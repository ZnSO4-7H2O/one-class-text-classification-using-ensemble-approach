{"title": "Deep Learning for Explicitly Modeling Optimization Landscapes", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated.", "text": "trivial optimization problems structure solutions exhibit complex interdependencies input parameters. decades research stochastic search techniques shown beneﬁt explicitly modeling interactions sets parameters overall quality solutions discovered. demonstrate novel method based learning deep networks model global landscapes optimization problems. represent search space concisely accurately deep networks must encode information underlying parameter interactions contributions quality solution. networks trained networks probed reveal parameter combinations high expected performance respect optimization task. estimates used initialize fast randomized local search algorithms turn expose information search space subsequently used reﬁne models. demonstrate technique multiple optimization problems arisen variety real-world domains including packing graphics scheduling layout compression. problems include combinatoric search spaces discontinuous highly non-linear spaces span binary higher-cardinality discrete well continuous parameters. strengths limitations extensions approach extensively discussed demonstrated. number researchers independently started employing probabilistic models guide heuristic stochastic based-search algorithms. idea simple knowledge search landscape ascertained analyzing points encountered guide look next. sharply contrasted procedure many successful randomized hill-climbing algorithms made small perturbations stochastically hopes better solution close neighbor current best solution. simplest instantiation probabilistic methods explicitly maintain statistics search space creating models good solutions found far. models sampled generate next query points evaluated. sampled solutions used update model cycle continued. probabilistic models optimization motivated three goals optimization method method incorporate simple learning hillclimbing method explain genetic algorithms might work. detail third motivation warranted maintaining population points genetic algorithms viewed creating implicit probabilistic models solutions seen search. attempt implicitly capture dependencies parameters solution quality distribution parameter settings contained population solutions. exploration proceeds generating samples evaluate process applying randomized recombination/crossover operators pairs high-performance members population. high-performance selected recombination parameter settings found poor-performing solutions explored further. recombination operator combines parents candidate solutions producing children transfer sets parameters parent children serves keep sets parameters together next candidate solutions explored. terms sampling viewed simple approach sampling population’s statistics. though similar intent implicit manner sampling distribution solutions contrasts approach presented paper. here explicit steps taken model parameters interdependencies contribute quality candidate solutions. goal paper succinctly stated follows algorithm possible simultaneously learn deep-neural network based approximation evaluation function. then deep-network inversion intelligent perturbations some/all samples generated search algorithm made. perturbations solutions increased probability higher scores. within literature ﬁrst attempts using population level statistics bit-based simulated crossover operator instead combining pairs solutions population-level statistics used generate solutions. operator worked follows. position number population members contain position counted. member’s contribution weighted ﬁtness respect target optimization function. process used count number zeros. instead using traditional crossover operators generate solutions generated query points stochastically assigning bit’s value probability seen value previous population important point note used population statistics generate solutions. extending idea incorporating hebbian learning another early probabilistic optimization population-based incremental learning algorithm rather based population-genetics lineage pbil stems back early reinforcement learning. pbil akin cooperative system discrete learning automata automata choose actions independently automata receive common reinforcement dependent upon actions unlike previous studies learning automata commonly addressed optimization noisy small environments pbil used explore large deterministic spaces. pbil algorithm easily described binary alphabet works follows. algorithm maintains real-valued probability vector specifying probability generating position probability vector sampled repeatedly generate candidate solutions. candidates evaluated respect optimization function best solution kept discarded. probability vector moved towards best solution simple hebbian-like updates. cycle repeated. note probabilistic model created pbil extremely simple inter-parameter dependencies captured; modeled independently. entire probability model single vector. although simple probabilistic model used pbil successful compared variety standard genetic algorithm hillclimbing procedures numerous benchmark real-world problems. theoretical analysis pbil found immediate improvements pbil mechanisms capture inter-parameter dependencies. mutual information maximization input clustering ﬁrst this. mimic captured heuristically chosen pairwise dependencies solution parameters. previously generated solutions pair-wise conditional probabilities calculated. mimic used greedy search generate chain variable conditioned previous variable. ﬁrst variable chain chosen variable lowest unconditional entropy deciding subsequent variable chain mimic selected variable lowest conditional entropy extended solely-unconditional model pbil maintain pair-wise dependencies. mimic’s probabilistic model extended larger class dependency graphs trees variable conditioned parent. shown created optimal tree-shaped network maximum-likelihood model data experimental comparisons mimic’s chain-based probabilistic models typically performed signiﬁcantly better pbil’s simpler models. tree-based graphs performed signiﬁcantly better mimic’s chains. trend indicated accurate probabilistic models increased probability generating candidate solutions promising regions search space natural extension pair-wise modeling modeling arbitrary dependencies. bayesian networks popular method efﬁciently representing dependencies bayesian networks directed acyclic graphs variable represented vertex dependencies variables encoded edges. numerous researchers taken step combining full bayesian networks stochastic search overview alternate model building approach termed stage presented stage attempts user-supplied features state space single value. value represents quality solutions found thus far. value function used select next point initialize search. unlike algorithms described above attempt automatically model effects parameter combinations overall solution quality hand-created features related optimization function used. hand-crafted features approaches shares many important characteristics methods used paper. next section describe deep-opt algorithm give details probabilistic model created sampled. also describe model integrated fast-search heuristics following work section examine performance deep-opt wide variety test problems. many interesting alternatives extensions approach used here; experiments three presented section three alternatives help increase problems tackled address limits approach described section finally discussion results suggestions future work presented section optimization probabilistic modeling high level simply explained figure shown ﬁgure previous work started large candidate solutions evaluated respect objective function problem solved poor-performing candidate solutions discarded. remaining solutions usually small subset better performing members original modeled. model stochastically sampled solutions evaluated process continued. interest concrete simple example probabilistic model provided figure using pbil’s independent-parameter model represent different populations model created sampled generate candidate solutions evaluate next. idea work? solutions represented high evaluations therefore subsequently generated sampling create random solutions uniform distribution. termination condition create probabilistic model stochastically sample generate candidate solutions. evaluate candidate solutions. update high-evaluation solutions figure modeling different binary-populations samples each. pbil’s probabilistic model position modeled independently simply probability position. distribution easily sampled generate points evaluate. statistics multiple good solutions high-performing well. large number solutions generated evaluated lower-performing ones discarded procedure repeated note although pbil’s probabilistic model extremely simple inter-parameter dependencies modeled mentioned earlier even proved effective many standard optimization tasks. despite successes however shown figure severe limitations population sets although different represented probabilistic vector; thereby demonstrating inadequacy models need powerful representations candidate solution’s statistics. thus models mimic optimal-dependency-trees pair-wise higher-order dependencies represented introduced. problem complexity increased ﬂexible powerful models improved quality solutions generated. paper neural network create mapping solutions sampled point score determined evaluation function. context describing algorithm also expose four largest differences approach approaches used earlier. primary differences using deep neural network priori specify form dependencies probabilistic model although architecture neural network used modeling manually speciﬁed actual dependencies network encodes need form parameters deeply tied architecture network tests described paper neural network architecture change; deep network capable modeling necessary statistics. network learned mapping input parameters evaluation procedure generating points evaluate necessary. procedure quite different straightforward sampling possible previous models pbil dependency trees. simplest model pbil samples easily generated biased-random sample generated position independently speciﬁed real-value position probability vector. models dependencies dependency-trees sampling simply conditioned variables parameter dependent neural networks however generating samples complex. based technique network inversion given trained network network inversion uses standard back-propagation modify inputs rather network’s weights. inputs modiﬁed match preset clamped outputs. method ﬁrst presented recently popularized within context texture style generation neural networks follows given trained network maps input parameters evaluation first weights network frozen; change sample generation process. second clamp output desired output maximization problems clamp output indicates would like generate solutions good best ones seen far. inputs initialized network performs forward propagation step error measured output. error metric standard least-mean squares error target output process similar standard training stochastic-descent back-propagation used. however unlike standard training errors propagated back inputs inputs modiﬁed weights. described procedure addresses following question which input produce output approximates given target vector error signal input tells input units change decrease error general modest learning-rates gradient descent algorithm found work best network-inversion process learning-rate networks trained well successfully model search landscape input values found procedure yield high-evaluation solutions tested actual evaluation function. candidate solutions generated evaluated next step? previous uses probabilistic models low-performance candidate solutions discarded high-performance solutions kept. interestingly actual evaluation high-performance whenever units changed outside bounds clipped within range. alternative approach could passed activations sigmoid functions; however would make setting values extrema slower. figure next-ascent stochastic hillclimbing implementation note mutation amount quite large; however based extensive empirical testing similar problems. dependent problem. experiments presented throughout paper ranged solutions used. contrast procedure create explicit mapping candidate solution score reﬂects fundamental difference approaches since explicit mapping created assumed model represents good solutions; represents high quality solutions. finally note contrast many previous studies represented solution vectors binary strings modeled binary parameters deep-neural networks used study naturally model real-values. extensions binary discrete parameters described section simplest implementation candidate solutions generated network inversion evaluated cycle continued. although method work drawbacks. first slow process; training full network sampled points evaluations expensive procedure sampling network. second post-processing step local optimization small changes made solutions generated yields improvement solutions found. interpolation extrapolation capabilities trained networks perfect; discrepancies estimated goodness candidate solution actual evaluation. early works probabilistic model-based optimization suggested probabilistic models methods initialize faster local-search optimization techniques. technique used here. simple next-ascent stochastic hillclimbing procedure shown figure repeatedly proven work well practice used conjunction optimization algorithms perform local optimization also surprisingly well used alone variety scenarios previous explorations probabilistic models instantiations weighted contribution member relative score. although changes models specifying well samples represented model create explicit mapping parameters scores. majority paper nash underlying search process; neural modeling wrap-around nash. importantly note nash initialized single candidate solution. candidate perturbed equal better solution found. well procedures described thus candidate solutions evaluated nash added pool solutions modeled networks trained next candidate solutions generated single best solution used initialize nash algorithm. nash proceeds normal recording candidate solutions evaluates augment next time step cycle continues. visual description algorithm shown figure section describe pragmatic considerations deployment steps step-by-step directions. many possible ways create maintain samples used training modeling networks. study size kept constant throughout run. although sufﬁciently large deep neural network able represent surface represented points seen would require extra computation valuable practice. overly precise models low-evaluation areas necessary. keep constant size every nash last unique solutions nash added pruned back removing members longest suggested previous studies algorithm progressing correctly average score solutions present increases time. full deep-opt algorithm shown figure respect step train deep neural network several points noted. validation vital good performance. training cycle samples drawn perturbing best solution added validation set. then step training correlation actual evaluation samples network’s predicted evaluation samples measured. correlation begins decrease training stopped next step algorithm begins. correlation negative training restarted random weights. implementation details speciﬁed figure ﬁrst step line several variants creating initial possible. simplest shown generating samples entirely randomly. fully random sampling gives broadest exploration search space. alternatively could performed single nash saved points explored. using however gives poor representation global landscape deep single path explored. third alternative sample number seed-points randomly also explore local neighborhoods making small perturbations seed points. gives cursory indication local landscape around seed points; empirically improved results problems tested. initialization method throughout paper. full analysis conducted measure effects alternate approaches. second seen large number parameters decisions made algorithm design. goal show using deep neural network capable modeling search space necessarily advocating particular network parameters. nonetheless make study complete need specify networks used. trials paper networks used. ﬁrst deep-opt- used -fully-connected-layer network hidden units layer. second deep-opt- used -layer fully-connected network hidden units layer skip connections every layer predecessors. networks single output estimate evaluation function estimated. weight decay used training variety networks tried ranging simple single layer networks even deeper ones. expected best performer dependent problem however provided good results across multiple problems. figure deep-opt creating model search landscape initialize fast search algorithms number-to-generate-from-model tests. kept samples size simplicity kept static tests paper. turning attention empirical tests next section present motivating example demonstrate local search algorithms utilize models created. examine simple problem i=). instantiate problem parameters take values note maximization problem parameter independent. though trivial problem serves demonstrate expected behavior. nash search initialized solution vector values parameters many local optima global optima. nonetheless ease problem even learning runs global optima. figure left column starting points parameters nash algorithm run. expected randomly distributed across input range. nash many parameters close optimal settings. middle figure shows restart nash. nash still initialized randomly expect largely distribution points beginning end. shown bottom restart nash. next repeat experiment deep-opt. difference initialization nash algorithm. samples generated nash added solutions modeled neural network. neural network model samples drawn. single best used initialize next nash run. expected figure ﬁrst looks similar earlier case model. information model yet. however nash restart distinct difference starting values many already high-evaluation regions. best solution found sampling many parameters right region search space. likelihood reaching global optimum increased nash trend even evident. sampling model works expected starting point hillclimbing already better region parameters closer global optima. though problem simple demonstrates modeling improve search results. interestingly early studies modeling sometimes poorer overall performance. why? probabilistic model improved exploration decreased samples started basin attraction local maxima seen previously. improvement slowed updates model happened similar candidate solutions. algorithm parameters particular size sample used modeling tuned settings shown figure slow convergence; vastly improved performance. effects output scaling also relevant observation. figure sines problem without learning. line underlying function maximized. points settings parameters beginning ending nash procedure histogram showing distribution points shown note beginning points uniformly distributed across full input space. results vastly improved hillclimbing though points made global optimum. nash middle nash bottom nash note graph although points represents parameters single solution string shown ’folded-over’ onto graph. possible parameter independent evaluated respect function. figure sines problem search-space modeling learning. note even beginning runs shown left column rows hillclimbing begins regions high performance thereby leading better performance overall hillclimbing search. since learning happened points randomly distributed beginning run. section examine beneﬁts probabilistic model select starting points nash optimization procedure number problems drawn literature real-world needs. described earlier model used samples generated best used initialize nash. ensure model actually providing useful information merely process examining samples beginning nash yielding improved performance three variants nash explored. though vary seemingly small implementation details effects performance dramatic. nash-v exactly nash shown figure nash-v beginning nash samples randomly generated evaluated. best found generated used initialize nash algorithm. learning used here. variant included test whether process generating multiple samples selecting best prior starting nash enough provide improvement ﬁnal result even modeling. nash-v beginning nash samples generated making small perturbations best solution found previous nash runs. samples evaluated. best sample found used initialize nash algorithm. variant tests whether neural network models actually capture shape search space whether forcing search around best solutions seen far. interesting note problems detailed below adding heuristics lead degradation performance. problems likely search space contain easily learnable trends– either large portions pocked local-optima information cannot correctly modeled networks used here. discussed problem descriptions. using problem-speciﬁc hand-crafted features done stage help. termination condition nash either evaluations performed evaluations conducted no-improvement. latter indicated search might trapped local maxima. approaches given total evaluations. number nash runs conducted within evaluations dependent problem quickly/often algorithm unable escape local optima. previous section used simple sines maximization problem illustrative example learning aids search. problem’s simplicity search algorithms perform well however introduction noise clearer separation performance emerges. version problem noisy-sines evaluation modiﬁed include signiﬁcant uniformly distributed random noise. uniformly chosen random noise added evaluation. large parts search space overall evaluation dominated noise performance algorithm judged best solution found underlying objective function algorithm privy underlying real function. results shown table algorithms tried best evaluation averaged trials listed ﬁrst row. last rows show signiﬁcance difference algorithm’s performance performance deep-opt--layers deep-opt--layers respectively. though problem shares part name stable marriage problem akin knapsack/packing problems. real versions problem arisen topics diverse processor scheduling intern group seating layouts. canonical version problem parties invited formal-seated party wedding reception. party variable size. member party must together tables capacity additional twist problem preference with expressed real value. full preference matrix preferences negative constrained magnitude. further requirement guest express preference every guest even guests. preferences symmetric. goal seating assignment keeps members group together seat people beyond capacity table maximizes summed happiness/preferences tables. size problems explored here reception tables capacity people. guest’s party randomly chosen people. preferences expressed value number groups encode solution vector group assigned parameters corresponding tables; total parameters evaluation time parameters sorted high low. based sorted list realv aluep arametergt assignments made order highest smallest group table note assignment occurred group unseated table could hold size group; otherwise parameter ignored next processed. encoding beneﬁt specifying groups’ preferences tables also able encode important particular group assigned particular table. unique problems created tested randomly generated complete preference matrices. random generation problems extremely large spread ﬁnal answers across problems. summarize results compared approaches gives numbers problems algorithm obtained highest evaluation results shown table next line table give number trials deep-opt--layers outperformed methods. last line deep-opt--layers. given graph vertices edges graph bandwidth problem label vertices graph unique integers difference labels connected vertices minimized. formally described label vertices graph distinct integers quantity max{|f vivj minimized details complexity problem found interest problem stems variety sources including constraint satisfaction minimizing propagation delay layout electronic cells. solution encoded follows vertex assigned real-valued parameter vertices sorted according respective assigned values. integers assigned vertices respective sort position. vertex integer assignment maximum difference assignments connected edges returned. results shown table particularly difﬁcult problem; ties shown parentheses. constraint satisfaction numerous real-world applications. recently used resource allocation scheduling. presented simplest form. simply stated problem real-value parameters range parameters assigned vertices graph. graph contains randomly chosen directed edges specify constraint origination-node must hold value greater destinationnode. optimization problem assign values nodes many constraints satisﬁed possible. constraint error absolute difference values. error minimized summed constraints. results shown table variant previous graph-based constraint satisfaction problem exact setup used section however node take letters a..p terms real-world application scheduling mentioned above version problem jobs enter system speciﬁc synchronized times. makes problem closer selection problem compared previous instantiation real value assigned node. though conceptually small difference encoding discretization enormous ramiﬁcations solution encoding. simplest encoding real-valued outputs node divide region evenly spaced regions assigned single letter. however reasons explained section encoding performs poorly. instead encoding amenable selection problems and/or discrete-parameters; encoding improves performance deep-opt well nash alone. encoding used similar reception-party-seating task described section vertex graph assigned real-valued parameters; corresponding single letter a..p maximum value found corresponding letter assigned vertex. node graph parameters used. graph nodes assigned values rest evaluation proceeds described section section highlights limitations deep-opt approach. number problems broadly encompassed task dimensional layout improve signiﬁcantly search space modeling. problems detailed here. goal planar layout graph’s nodes minimizes edge crossings. details. general edges drawn shape. simplicity implementation edges drawn straight lines termed rectilinear crossing number. tests node represented parameters small graphs tried nodes. yielded solution encoding real-values speciﬁed coordinates point plane. graph randomly chosen connections. randomly generated problem instantiations attempted. interesting ﬁndings nash- outperformed nash-. previous experiments reversed. nash- received higher score problems recall nash- initialized hillclimbing ﬁrst generating small number random candidate solutions selecting best one. contrast nash- perturbs current best solution determine best starting point nash. although left future exploration worth investigating insight gives search space? searching around current best yield good results randomly starting over search space less optima local optima spread apart deep etc? leave speculation ramiﬁcations nash- outperforming nash- future work. however deep-opt easily applied nash- nash- experiments section used wrap nash-. everything else parameters etc. remained same. problem paper parameters nash changed optimized problem. accordingly deep-opt also used parameters. parameters reset neither nash deep-opt would perform well shown here deep-opt under-performing nash alone. problem intensity target image pixels. goals triangles intensities approximate image. speciﬁcally triangle must specify three vertices intensity value. triangles drawn onto initially empty canvas. triangles overlap; intensities additive. triangles drawn resulting image scaled back appropriate space compared pixel-wise original image. distance minimized. particularly difﬁcult/interesting problem small. parameters nash deep-opt tested problem triangles trying approximate intensity based crop scream edvard munch. triangle encoded parameters coordinates three vertices intensity. triangles total parameters solution encoding. parameters images shown figure also attempted parameter settings. relative performance given table note overall performance virtually identical. averaged trials image. deep-opt described real-valued parameters. input parameters well target output scaled however wide variety problems employ discrete binary parameters. here present method tackle problems. ground discussion let’s examine simple dimensional checkerboard problem problem planar grid binary digits. goal digit opposite digits primary directions. globally optimal solutions also many locally optimal solutions single bit-ﬂip yield improved performance. first conducted experiment determine deep-opt described point would perform task. solution encoding bits. instantiation maximum possible evaluation task bits primary directions correctly inner square contributes point evaluation. average deep-opt based solution quality comparison three versions nash without learning average solution qualities respectively. difference deep-opt largest witnessed entire study. happen? initial attempt straight-forward method interpret real valued solution parameters binary used. real value assigned otherwise. scheme note small changes parameter’s real value often yield change solution string thereby evaluation. therefore many solution strings appeared evaluation though held different values real-valued vectors. ideally position favor solutions real value associated position possible this technique similar stochastic sigmoid units training neural network model search space recall candidate generation phase start input candidate solution vector number iterations back-driving network inputs modiﬁed following gradients needed transform candidate solution network computes value revised candidate solution input. discrete version instead treat real valued parameter probability. generate small number binary solutions strings parameter position-p probability assigned binary solutions network gradients computed solution strings note need evaluate actual real evaluation function network’s output measured error signal. although procedure adds processing time result vastly improved. trials perfect evaluations overall trials average overcomes previous limitations naive implementation binarizing real-value parameters. completeness also tried alternative procedure determine increasing learning rate generation process would cause enough moves across boundary achieve beneﬁts. variety larger learning rates tried case performance improve. generation binary strings outperformed versions increased learning rates. figure notice rapidly diminished difference evaluations best worst members search progresses. candidate solutions probabilistic model created. deep-opt used paper works back-driving neural network inputs change cause network yield output output node. recall evaluations solutions generated scaled training session values scaling evaluations ﬁxed range leads subtle complexity. note subset members change every training cycle nash completes hillclimbing newly found solutions appended solutions removed. often early parts search range actual evaluations present expands include higher evaluation members many randomly found initial members still present. opposite happens latter parts search; range values contracts search focuses primarily better solutions solutions close evaluations. this note trainingcyclet member real evaluation scaled trainingcyclet+ real evaluation scaled could either greater less networks continue training every cycle proven problem change evaluation seems quickly recovered; relative ordering samples change. however future interesting variants re-scaling done. require engineering setup problems always output values ﬁxed priori known range. expansion contraction dynamic process happens implicitly addition members search progresses. returning sine problem witness convergence minimum maximum values represented items added figure important trend minimum maximum values rapidly converge. necessarily imply candidate solutions similar mean close performance rate convergence corresponds exploitation exploration trade-off. imagine instead scaling target outputs values range scaled approach highest scoring value network back-driven still driven solutions produce output. semantically attempts create solutions explicitly better than equal seen far. success approach pinned network’s successful extrapolation underlying search surface regions better performance. many versions tried various experiments. although results preliminary setting higher little effect results compared setting setting range often hurt performance. exploration left future work. paper coupled deep-net modeling extremely simple fast localized search algorithm nash. important also consider possibility using alternate search algorithms modeling procedures. obvious alternative search heuristics simulated annealing tabu search easily substituted often used search neighborhoods around single point manner similar nash. although delve general debate merits simple techniques sophisticated search techniques active area discussion several decades interesting consider probabilistic models different search paradigms genetic algorithms. recall that unlike nash genetic algorithms work population points. members population created recombination operators combine elements candidate solutions. newly created solutions randomly perturbed reveal ’children’ solutions candidate solutions evaluate next numerous variations task-speciﬁc operators possible explored research literature. next perform tests using simple-ga parameters shown table typical used static optimization problems literature. learn please test population sizes equivalent number function evaluations previous runs hillclimbing additionally previous runs restarted evaluations. standard initial population comprised candidate solutions randomly generated. deep-opt-ga initial population candidate solutions entirely generated back-driven neural network model. approach tested real-valued constraints problems described section results shown table deep-opt-ga outperformed random initialization instantiations attempted sets trials population size make signiﬁcant difference. although directly relevant effects deep-opt interesting note results compare hillclimbing runs described section even simplest nash likely operators operator application rates yield improved optimization algorithms problems. intent modestsimply show deep-opt framework easily wrapped around multiple-point search-based algorithms well single-point search based algorithms hillclimbing. performance better models tested; though models consistently improved learning. summarize ﬁndings section using models guide search help even search heuristics operate single point population-based genetic algorithms. presented novel method incorporate deep-learning stochastic optimization. next instantiation intelligent model-based stochastic optimization follows tradition probabilistic model based optimization approaches last decades research. important aspect work priori information problem minimal setting form model. study multi-layer feed-forward networks hidden layers used problems problem-speciﬁc modiﬁcations. judicious early-stopping training potential downsides overtraining overcome. consideration discuss paper speed optimization. every hillclimbing network trained sampled; time consuming procedure problems large solution-encoding made attempts optimize this; however future methods employ fewer training steps hillclimbing runs explored. likely achieve similarly positive results. many advances rapidly evolving ﬁeld deep learning directly incorporated work continuously training networks core learning system. outside deep-learning advancements three avenues immediate interest future explorations given below. first overarching goal paper concretely demonstrate integration neural network learning optimization promote system ﬁnalized optimization system. make ﬁnalized optimization system exploration algorithm’s robustness behavior warranted. example many parameters system. study found results sensitive size decision restart training network scratch happens samples added network fails accommodate learning scaling issues mentioned section and/or weights network grown large. beneﬁcial effects regularization especially pronounced application networks constantly incrementally trained changing data. also found class problems observed beneﬁt using modeling problems easy hard alternate representation needed? second alternative using network back-driving technique generate candidate initialization points simply network proxy evaluation function hillclimbing. approach hillclimbing conducted directly model’s output. every perturbation candidate-solution passed network measure it’s estimated performance. this unlike network back-driving take advantage fact network differentiable rather uses proxy real evaluation function. however reveal parts search space back-driving not. third perhaps speculative direction measure transferable features learned different instantiations problem. example respect triangle-covering problem presented section learned evaluate well triangles reproduces image learning evaluations next image easier? imagine level primitives draw triangles indeed learned network reusable. even little transference current implementation exploring problem transference enormous potential make system automatically intelligent time. references baluja population-based incremental learning. method integrating genetic search based function optimization competitive learning cmu-cs--. carnegie mellon university dept. computer science tech. rep. pelikan goldberg cantú-paz bayesian optimization algorithm proceedings annual conference genetic evolutionary computation-volume morgan kaufmann publishers inc. juels wattenberg stochastic hillclimbing baseline method evaluating genetic algorithms proceedings conference neural information processing systems vol. holland forrest when genetic algorithm outperform hill climbing advances neural information processing systems cowan tesauro alspector eds. morgan-kaufmann available http//papers.nips.cc/paper/ -when-will-a-genetic-algorithm-outperform-hill-climbing.pdf lobo bazargani when hillclimbers beat genetic algorithms multimodal optimization proceedings companion publication annual conference genetic evolutionary computation.", "year": 2017}