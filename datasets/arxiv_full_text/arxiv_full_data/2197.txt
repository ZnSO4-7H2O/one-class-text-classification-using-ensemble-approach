{"title": "Beyond Parity: Fairness Objectives for Collaborative Filtering", "tag": ["cs.IR", "cs.AI", "cs.LG", "stat.ML"], "abstract": "We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.", "text": "study fairness collaborative-ﬁltering recommender systems sensitive discrimination exists historical data. biased data lead collaborative-ﬁltering methods make unfair predictions users minority groups. identify insufﬁciency existing fairness metrics propose four metrics address different forms unfairness. fairness metrics optimized adding fairness terms learning objective. experiments synthetic real data show metrics better measure fairness baseline fairness objectives effectively help reduce unfairness. paper introduces measures unfairness algorithmic recommendation demonstrates optimize metrics reduce different forms unfairness. recommender systems study user behavior make recommendations support decision making. widely applied various ﬁelds recommend items movies products jobs courses. however since recommender systems make predictions based observed data easily inherit bias already exist. address issue ﬁrst formalize problem unfairness recommender systems identify insufﬁciency demographic parity setting. propose four unfairness metrics address different forms unfairness. compare fairness measures non-parity biased synthetic training data prove metrics better measure unfairness. improve model fairness provide fairness objectives optimized adding unfairness penalties regularizers. experimenting real synthetic data demonstrate fairness metric optimized without much degradation prediction accuracy trade-offs exist among different forms unfairness. focus frequently practiced approach recommendation called collaborative ﬁltering makes recommendations based ratings behavior users system. fundamental assumption behind collaborative ﬁltering users’ opinions selected aggregated provide reasonable prediction active user’s preference example user likes item many users like item also like item reasonable expect user also like item collaborative ﬁltering methods would predict user give item high rating. approach predictions made based co-occurrence statistics methods assume missing ratings missing random. unfortunately researchers shown sampled ratings markedly different properties users’ true preferences sampling heavily inﬂuenced social bias results missing ratings cases others. non-random pattern missing observed rating data potential source unfairness. purpose improving recommendation accuracy collaborative ﬁltering models side information address problem imbalanced data work test properties effectiveness metrics focus basic matrix-factorization algorithm ﬁrst. investigating models could reduce unfairness direction future research. throughout paper consider running example unfair recommendation. consider recommendation education unfairness occur areas current gender imbalance science technology engineering mathematics topics. societal cultural inﬂuences fewer female students currently choose careers stem. example women accounted bachelor’s degrees awarded computer science underrepresentation women causes historical rating data computer-science courses dominated men. consequently learned model underestimate women’s preferences biased toward men. consider setting which even ratings provided students accurately reﬂect true preferences bias ratings reported leads unfairness. remainder paper organized follows. first review previous relevant work section section formalize recommendation problem introduce four unfairness metrics give justiﬁcations examples. section show unfairness occurs data gets imbalanced present results successfully minimize form unfairness. finally section concludes paper proposes possible future work. machine learning widely applied modern society researchers begun identifying criticality algorithmic fairness. various studies considered algorithmic fairness problems supervised classiﬁcation aiming protect algorithms treating people differently prejudicial reasons removing sensitive features help alleviate unfairness often insufﬁcient. features often correlated unprotected attributes related sensitive features therefore still cause model biased moreover problems collaborative ﬁltering algorithms directly consider measured features instead infer latent user attributes behavior. another frequently practiced strategy encouraging fairness enforce demographic parity achieve statistical parity among groups. goal ensure overall proportion members protected group receiving positive classiﬁcations identical proportion population whole example case binary decision binary protected attribute constraint formalized kamishima evaluate model fairness based non-parity unfairness concept solve unfairness issue recommender systems adding regularization term enforces demographic parity. objective penalizes differences among average predicted ratings user groups. however demographic parity appropriate preferences unrelated sensitive features. tasks recommendation user preferences indeed inﬂuenced sensitive features gender race therefore enforcing demographic parity signiﬁcantly damage quality recommendations. address issue demographic parity hardt propose measure unfairness true positive rate true negative rate. idea encourages refer equal opportunity longer relies implicit assumption demographic parity target variable independent sensitive features. propose that binary setting given decision protected attribute true label constraints equivalent constraint upholds fairness simultaneously respects group differences. penalizes models perform well majority groups. idea also basis unfairness metrics propose recommendation. signiﬁcant impacts lives usage algorithmic recommendation setting consequences affect society generations. coupling importance application issue gender imbalance stem challenges retention students backgrounds underrepresented stem setting serious motivation advance scientiﬁc understanding unfairness—and methods reduce unfairness—in recommendation. section introduces fairness objectives collaborative ﬁltering. begin reviewing matrix factorization method. describe various fairness objectives consider providing formal deﬁnitions discussion motivations. consider task collaborative ﬁltering using matrix factorization users indexed items indexed user variable indicating group user belongs example indicate whether user identiﬁes woman non-binary gender identity. item indicate item group belongs example represent genre movie topic course. preference score user item. ratings viewed entries rating matrix matrix-factorization formulation builds assumption rating represented product vectors representing user item. additional bias terms users items assumption summarized follows d-dimensional vector representing user d-dimensional vector representing item scalar bias terms user item respectively. matrix-factorization learning algorithm seeks learn parameters observed ratings typically minimizing regularized squared reconstruction error strategies minimizing non-convex objective well studied general approach compute gradient gradient-based optimizer. experiments adam algorithm combines adaptive learning rates momentum. section describe process matrix factorization leads unfair recommendations even rating data accurately reﬂects users’ true preferences. unfairness occur imbalanced data. identify forms underrepresentation population imbalance observation bias. later demonstrate either leads unfair recommendation forms together lead worse unfairness. discussion running example course recommendation highlighting effects underrepresentation stem education. population imbalance occurs different types users occur dataset varied frequencies. example consider four types users deﬁned aspects. first individual identiﬁes gender. simplicity consider binary gender identities though example would also appropriate consider gender group women non-binary gender identities second group. second individual either someone enjoys would excel stem topics someone would not. population imbalance occurs stem education when systemic bias societal problems signiﬁcantly fewer women succeed stem converse societal unfairness succeed stem four-way separation user groups available recommender system instead know gender group user proclivity stem. observation bias related distinct form data imbalance certain types users different tendencies rate different types items. bias often part feedback loop involving existing methods recommendation whether algorithms humans. individual never recommended particular item likely never provide rating data item. therefore algorithms never able directly learn preference relationship. education example women rarely recommended take stem courses signiﬁcantly less training data women stem courses. simulate types data bias stochastic block models create block model determines probability individual particular user group likes item particular item group. group ratios non-uniform leading population imbalance. second block model determine probability individual user group rates item item group. non-uniformity second block model lead observation bias. section present four unfairness metrics preference prediction measuring discrepancy prediction behavior disadvantaged users advantaged users. metric captures different type unfairness different consequences. describe mathematical formulation metric justiﬁcation examples consequences metric indicate. consider binary group feature refer disadvantaged advantaged groups represent women education example. average predicted score item disadvantaged users average predicted score advantaged users average ratings disadvantaged advantaged users respectively. precisely quantity computed value unfairness occurs class user consistently given higher lower predictions true preferences. errors prediction evenly balanced overestimation underestimation classes users direction magnitude error value unfairness becomes small. value unfairness becomes large predictions class consistently overestimated predictions class consistently underestimated. example course recommender value unfairness manifest male students recommended stem courses even interested stem topics female students recommended stem courses even interested stem topics. absolute unfairness unsigned captures single statistic representing quality prediction user type. user type small reconstruction error user type large reconstruction error type user unfair advantage good recommendation user type poor recommendation. contrast value unfairness absolute unfairness consider direction error. example female students given predictions points true preferences male students given predictions points true preferences absolute unfairness. conversely female students given ratings points either direction male students rated within point true preferences absolute unfairness high value unfairness low. underestimation unfairness important settings missing recommendations critical extra recommendations. example underestimation could lead student recommended explore topic would excel overestimation unfairness important settings users overwhelmed recommendations providing many recommendations would especially detrimental. example users must invest large amounts time evaluate recommended item overestimating essentially costs user time. thus uneven amounts overestimation could cost type user time other. finally non-parity unfairness measure based regularization term introduced kamishima computed absolute difference overall average ratings disadvantaged users advantaged users metrics straightforward subgradient optimized various subgradient optimization techniques. augment learning objective adding smoothed variation fairness metric based huber loss outer absolute value replaced squared difference less solve local minimum smoothed penalty helps reduce discontinuities objective making optimization efﬁcient. also straightforward scalar trade-off term weight fairness loss. experiments equal weighting omit term experiments synthetic data based simulated course-recommendation scenario real movie rating data experiment investigate whether learning objectives augmented unfairness penalties successfully reduce unfairness. synthetic experiments generate simulated course-recommendation data block model described section consider four user groups three item groups {fem stem masc}. user groups thought women enjoy stem topics women enjoy stem topics enjoy stem topics item groups thought courses tend appeal figure average unfairness scores standard matrix factorization synthetic data generated different underrepresentation schemes. metric four sampling schemes uniform biased observations biased populations biases reconstruction error ﬁrst four unfairness metrics follow trend non-parity exhibits different behavior. also consider observation block models uniform observation probability across groups ouni unbalanced observation probability inspired students often encouraged take certain courses deﬁne different user group distributions four groups exactly quarter population imbalanced setting population heavy imbalance inspired severe gender imbalances certain stem areas today. experiment select observation matrix user group distribution generate users items sample preferences observations preferences block models. training ratings evaluate remaining entries rating matrix comparing predicted rating true expected rating using standard matrix factorization measure various unfairness metrics different sampling conditions. average random trials plot average score fig. label settings follows uniform user groups uniform observation probabilities uniform groups biased observation probabilities biased user group populations uniform observations biased populations biased observations statistics demonstrate type underrepresentation contributes various forms unfairness. metrics except parity strict order unfairness uniform data table average error unfairness metrics synthetic data using different fairness objectives. best scores statistically indistinguishable best printed bold. represents different unfairness penalty column measured metric expected value unseen ratings. fair; biased observations next fair; biased populations worse; biasing populations observations causes unfairness. squared rating error also follows trend. contrast non-parity behaves differently heavily ampliﬁed biased observations seems unaffected biased populations. note though non-parity high observations imbalanced imbalance observations actually expect non-parity labeled ratings high non-parity score necessarily indicate unfair situation. unfairness metrics hand describe examples unfair behavior rating predictor. tests verify unfairness occur imbalanced populations observations even measured ratings accurately represent user preferences. before generate rating data using block model imbalanced setting user populations imbalanced sampling rate skewed. provide sampled ratings matrix factorization algorithms evaluate remaining entries expected rating matrix. two-dimensional vectors represent users items regularization term optimize iterations using full gradient. generate three datasets measure squared reconstruction error unfairness metrics. results listed table metric print bold best average score scores statistically signiﬁcantly distinct according paired t-tests threshold results indicate learning algorithm successfully minimizes unfairness penalties generalizing unseen held-out user-item pairs. reducing unfairness metric lead signiﬁcant increase reconstruction error. complexity computing unfairness metrics similar error computation linear number ratings adding fairness term approximately doubles training time. implementation learning fairness terms takes longer loops backpropagation introduce extra overhead. example synthetic data users items takes seconds train matrix factorization model without unfairness term seconds value unfairness. optimizing metric leads improved performance trends worth noting. optimizing unfairness metrics almost always reduces forms unfairness. exception optimizing absolute unfairness leads increase underestimation. value unfairness closely related underestimation overestimation since optimizing value unfairness even effective reducing underestimation overestimation directly optimizing them. also optimizing value overestimation effective reducing absolute unfairness directly optimizing finally optimizing parity unfairness leads increases unfairness metrics except absolute unfairness parity itself. relationships among metrics suggest need practitioners decide types fairness important applications. movielens million dataset contains ratings users movies. users annotated demographic variables including gender movies annotated genres. manually selected genres feature different forms genres selected action crime musical romance sci-ﬁ. selected genres noticeable gender effect data. women rate musical romance ﬁlms higher frequently men. women score action crime sci-ﬁ ﬁlms equally rate much frequently. table lists statistics detail. ﬁltering genre rating frequency users movies dataset. trials randomly split ratings training testing sets train objective function training evaluate metric testing set. average scores listed table bold scores indicate statistically indistinguishable best average score. real data results show optimizing unfairness metric leads best performance metric without signiﬁcant change reconstruction error. synthetic data optimizing value unfairness leads decrease underoverestimation. optimizing non-parity causes increase change almost unfairness metrics. paper discussed various types unfairness occur collaborative ﬁltering. demonstrate forms unfairness occur even observed rating data correct sense accurately reﬂects preferences users. identify forms data bias lead unfairness. demonstrate augmenting matrix-factorization objectives unfairness metrics penalty functions enables learning algorithm minimize them. experiments synthetic real data show minimization forms unfairness possible signiﬁcant increase reconstruction error. also demonstrate combined objective penalizes overestimation underestimation. minimizing objective leads small unfairness penalties forms unfairness. using combined objective good approach practitioners. however single objective best unfairness metrics remains necessary practitioners consider precisely form fairness important application optimize speciﬁc objective. future work work paper focused improving fairness among users model treats different groups users fairly address fair treatment different item groups. model could biased toward certain items e.g. performing better prediction items others terms accuracy overunderestimation. achieving fairness users items important considering items also suffer discrimination bias example courses taught instructors different demographics. experiments demonstrate minimizing empirical unfairness generalizes generalization dependent data density. ratings especially sparse empirical fairness always generalize well held-out predictions. investigating methods robust data sparsity future work. moreover fairness metrics assume users rate items according true preferences. assumption likely violated real data since ratings also inﬂuenced various environmental factors. e.g. education student’s rating course also depends whether course inclusive welcoming learning environment. however addressing type bias require additional information external interventions beyond provided rating data. finally investigating methods reduce unfairness directly modeling two-stage sampling process used generate synthetic biased data. hypothesize explicitly modeling rating observation probabilities separate variables able derive principled probabilistic approach address forms data imbalance.", "year": 2017}