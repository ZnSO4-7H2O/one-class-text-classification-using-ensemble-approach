{"title": "Learning a Recurrent Visual Representation for Image Caption Generation", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.", "text": "paper propose bi-directional representation capable generating novel descriptions images visual representations descriptions. critical tasks novel representation dynamically captures visual aspects scene already described. word generated read visual representation updated reﬂect information contained word. accomplish using recurrent neural networks long-standing problem rnns weakness remembering concepts iterations recurrence. instance language models often difﬁcultly learning long distance relations without specialized gating units sentence generation novel dynamically updated visual representation acts long-term memory concepts already mentioned. allows network automatically pick salient concepts convey spoken. demonstrate representation used create visual representation written description. demonstrate method numerous datasets. include pascal sentence dataset flickr flickr microsoft coco dataset generating novel image descriptions demonstrate state-of-the-art results measured bleu meteor pascal surprisingly achieve performance slightly humans measured bleu meteor coco dataset. qualitative results shown generation novel image captions. also evaluate bi-directional ability algorithm image sentence retrieval tasks. since require ability generate novel sentences numerous previous papers evaluated task. show results better comparable previous state-of-the-art results using similar visual features. paper explore bi-directional mapping between images sentence-based descriptions. propose learning mapping using recurrent neural network. unlike previous approaches sentences images common embedding enable generation novel sentences given image. using model also reconstruct visual features associated image given visual description. novel recurrent visual memory automatically learns remember long-term visual concepts sentence generation visual feature reconstruction. evaluate approach several tasks. include sentence generation sentence retrieval image retrieval. state-ofthe-art results shown task generating novel image descriptions. compared human generated captions automatically generated captions preferred humans time. results better comparable state-of-the-art results image sentence retrieval tasks methods using similar visual features. good image description often said paint picture mind’s eye. creation mental image play signiﬁcant role sentence comprehension humans fact often mental image remembered long exact sentence forgotten role visual memory play computer vision algorithms comprehend generate image descriptions? recently several papers explored learning joint feature spaces images descriptions approaches project image features sentence features common space used image search ranking image captions. various approaches used learn projection including kernel canonical correlation analysis recursive neural networks deep neural networks approaches project semantics visual features figure illustration model. shows full model used training. show parts model needed generating sentences visual features generating visual features sentences respectively. task building visual memory lies heart long-standing ai-hard problems grounding natural language symbols physical world semantically understanding content image. whereas learning mapping image patches single text labels remains popular topic computer vision growing interest using entire sentence descriptions together pixels learn joint embeddings viewing corresponding text images correlated kcca natural option discover shared features spaces. however given highly non-linear mapping ﬁnding generic distance metric based shallow representations extremely difﬁcult. recent papers seek better objective functions directly optimize ranking directly adopts pre-trained representations simplify learning combination good distance metric possible perform tasks like bi-directional image-sentence retrieval. however many scenarios also desired generate novel image descriptions hallucinate scene given sentence description. numerous papers explored area generating novel image descriptions papers various approaches generate text using pre-trained object detectors templatebased sentence generation retrieved sentences combined form novel descriptions recently purely statistical models used generate sentences based sampling recurrent neural networks also uses model significantly different model. speciﬁcally attempt reconstruct visual features similar contextual synthesizing images sentences recent paper zitnick uses abstract clip images learn visual interpretation sentences. relation tuples extracted sentences conditional random ﬁeld used model visual scene. numerous papers using recurrent neural networks language modeling build directly rnns learn word context. several models sources contextual information help inform language model despite success rnns still difﬁculty capturing long-range relationships sequential modeling solution long short-term memory networks gates control gradient back-propagation explicitly allow learning long-term interactions. however main focus paper show hidden layers learned translating multiple modalities already discover rich structures data learn long distance relations automatic data-driven manner. section describe approach using recurrent neural networks. goals twofold. first want able generate sentences given visual observations features. speciﬁcally want compute probability word generated time given previously generated words observed visual features second want enable capability computing likelihood visual features given spoken read words generating visual representations scene performing image search. accomplish tasks introduce latent variables encodes visual interpretation previously generated read words wt−. demonstrate later latent variables play critical role acting long-term visual memory words previously generated read. using goal compute combining likelihoods together global objective maximize want maximize likelihood word observed visual features given previous words visual interpretation. note previous papers objective compute model structure recurrent neural network model structure builds prior models proposed mikolov proposed language model shown green boxes figure word time represented vector using representation. size word vocabulary entry value depending whether word used. output contains likelihood generating word. recurrent hidden state provides context based previous words. however typically models shortrange interactions problem vanishing gradients simple effective language model shown provide useful continuous word embedding variety applications following mikolov added input layer shown white figure layer represent variety information topic models parts speech application represents observed visual features. assume visual features constant. visual features help inform selection words. instance detected word likely spoken. note unlike necessary directly connect since static application. represented dynamic information parts speech needed direct access. also found connecting half units provided better results since allowed different units specialize modeling either text visual features. main contribution paper addition recurrent visual hidden layer blue boxes figure recurrent layer attempts reconstruct visual features previous words i.e. visual hidden layer also used help predicting next word. network compare visual memory figure illustration hidden units activations time notice visual hidden units exhibit long-term memory temporal stability units hidden units change signiﬁcantly time step. already said currently observes predict next. beginning sentence represents prior probability visual features. words observed visual feature likelihoods updated reﬂect words’ visual interpretation. instance word sink generated visual feature corresponding sink increase. features correspond stove refrigerator might increase well since highly correlated sink. critical property recurrent visual features ability remember visual concepts long term. property arises model structure. intuitively expect visual features shouldn’t estimated sentence ﬁnished. used estimate generates sentence token. however model force estimate every time step help remembering visual concepts. instance word generated increase likelihood visual feature corresponding cat. assuming visual feature active network receive positive reinforcement propagate memory time instance next. figure shows illustrative example hidden units observed visual hidden units exhibit longer temporal stability. note network structure predict visual features sentences generate sentences visual features. generating sentences known ignored. predicting visual features sentences known ignored. property arises fact words units separate model halves predicting words visual features respectively. alternatively hidden units connected directly property would lost network would normal autoencoder language model typically words. word predicted independently approach computationally expensive. instead adopted idea word classing factorized distribution product terms probability word probability class. class label word computed unsupervised manner grouping words similar frequencies together. generally approach greatly accelerates learning process little loss perplexity. predicted word likelihoods computed using standard soft-max function. epoch perplexity evaluated separate validation learning reduced perplexity decrease. order reduce perplexity combine model’s output output maximum entropy language model simultaneously learned training corpus. experiments many words look back predicting next word used maximum entropy model three. natural language processing task pre-processing crucial ﬁnal performance. sentences following steps feeding model. stanford corenlp tool tokenize sentences. lower case letters. learning learning backpropagation time algorithm speciﬁcally network unrolled several words standard backpropagation applied. note reset model end-ofsentence encountered prediction cross sentence boundaries. shown beneﬁcial online learning weights recurrent units output words. weights rest network sentence batch update. activations units computed using sigmoid function clipping except word predictions soft-max. found rectiﬁed linear units unbounded activations numerically unstable commonly blew used recurrent networks. word image representations error predicting words directly backpropagated imagelevel features. however deep convolution neural networks require large amounts data train largest sentence-image dataset images therefore instead training scratch choose ﬁnetune pre-trained -class imagenet model avoid potential over-ﬁtting. experiments used full-connected layer output visual input model. section evaluate effectiveness bidirectional model multiple tasks. begin describing datasets used training testing followed baselines. ﬁrst evaluations measure model’s ability generate novel descriptions images. since model bi-directional evaluate performance sentence retrieval image retrieval tasks. addition results please supplementary material. flickr datasets consists images collected flickr respectively. images depict humans participating various activities. image also paired sentences. datasets standard training validation testing splits. coco microsoft coco dataset contains training images validation images human generated descriptions. images collected flickr searching common object categories typically contain multiple objects significant contextual information. downloaded version contains annotated training images validation images experiments. table results novel sentence generation pascal flickr flickr coco. results measured using perplexity bleu meteor available results midge babytalk provided. human agreement scores shown last row. text details. image features model image features feeding hidden layer inspired described section connected visual features used layer output bvlc reference relus. network trained imagenet -way classiﬁcation task experimented layers perform well. image features fine-tuned model architecture rnn+if error back-propagated convolution neural network initialized weights bvlc reference net. initialized pre-trained language model. randomly initialized weights ones visual features hidden layers pre-trained found initial gradients noisy cnn. weights hidden layers also pre-trained search space becomes limited. current implementation takes seconds learn mini-batch size tesla gpu. also crucial keep track validation error avoid overﬁtting. observed ﬁnetuning strategy particularly helpful coco give much performance gain flickr datasets before overﬁts. flickr datasets provide enough training data avoid overﬁtting. ﬁrst experiments evaluate model’s ability generate novel sentence descriptions images. experiment image-sentence datasets described previously compare baselines previous papers since pascal limited amount training data report results trained coco tested pascal standard train-test splits flickr datasets. coco train validate training test validation since testing available. generate sentence ﬁrst sample target sentence length multinomial distribution lengths learned training data ﬁxed length sample random sentences lowest loss output. choose three automatic metrics evaluating quality generated sentences perplexity bleu meteor perplexity measures likelihood generating testing sentence based number bits would take encode lower value better. bleu meteor originally designed automatic machine translation rate quality translated sentences given several references sentences. treat sentence generation task translation images sentences. bleu took geometric mean scores -gram -gram used ground truth length closest generated sentence penalize brevity. meteor used latest version bleu meteor higher scores better. reference also report consistency human annotators http//www.cs.cmu.edu/˜alavie/meteor/ used sentences references system evaluation leave sentences human consistency. unfair difference usually around .∼.. results shown table approach signiﬁcantly improves midge babytalk pascal dataset measured bleu meteor. several qualitative results three algorithms shown figure approach generally provides naturally descriptive sentences mentioning image black white double decker. midge’s descriptions often shorter less detail babytalk provides long often redundant descriptions. results flickr flickr also provided. coco dataset contains images high complexity provide perplexity bleu meteor scores. surprisingly bleu meteor scores slightly lower human scores image features signiﬁcantly improves performance using language model. fine-tuning full approach provide additional improvements datasets. future reference ﬁnal model gives bleu- bleu- compared human consistency known automatic measures roughly correlated human judgment also important evaluate generated sentences using human studies. evaluated generated sentences coco asking human subjects judge whether better worse quality human generated ground truth caption. subjects asked rate image majority vote recorded. case winners half vote. prefer automatically generated captions human captions without ﬁne-tuning respectively. less subjects rated captions same. impressive result given used image-level visual features complex images coco. model bi-directional. generate image features sentences sentences image features. evaluate ability both measure performance retrieval tasks. retrieve images given sentence description retrieve description given image. since previous methods capable retrieval task also helps provide experimental comparison. following methods adopted protocols using multiple image descriptions. ﬁrst treat sentences individually. scenario rank retrieved ground truth sentences used evaluation. second case treat sentences single annotation concatenate together retrieval. figure qualitative results sentence generation pascal dataset. generated sentences shown approach midge babytalk reference human generated caption shown black. given image since shorter sentences naturally higher probability generated followed normalized probability dividing total probability summed entire retrieval set. second could rank based reconstruction error image’s visual features reconstructed visual features better performance average reconstruction error time steps rather error sentence. tables report retrieval results using text likelihood term combination visual feature reconstruction error evaluation metrics adopted previous papers tasks sentence retrieval image retrieval. used measurements recall rates ground truth sentences images higher corresponds better retrieval performance. also report median/mean rank retrieved ground truth sentences images lower med/mean implies better performance. flickr several different evaluation methodologies proposed. report three scores flickr corresponding methodologies proposed measured mean achieve state-of-the-art results pascal image sentence retrieval shown tables flickr approach achieves comparable better results methods except recently proposed deepfe however deepfe uses different features based smaller image regions. features used approach achieve better results. believe contributions complementary using better features approach also show improvement. general ranking based text visual features outperforms using text please supplementary material retrieval results coco. image captions describe objects image relationships. area future work examine sequential exploration image relates image descriptions. many words correspond spatial relations current model difﬁcultly detecting. demonstrated recent paper better feature localization image greatly improve performance conclusion describe ﬁrst bi-directional model capable generating novel image descriptions visual features. unlike many previous approaches using rnns model capable learning long-term interactions. arises using recurrent visual memory learns reconstruct visual features words read generated. demonstrate state-of-the-art results task sentence generation image retrieval sentence retrieval numerous datasets.", "year": 2014}