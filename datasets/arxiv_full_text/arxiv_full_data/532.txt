{"title": "Sequence Training and Adaptation of Highway Deep Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Highway deep neural network (HDNN) is a type of depth-gated feedforward neural network, which has shown to be easier to train with more hidden layers and also generalise better compared to conventional plain deep neural networks (DNNs). Previously, we investigated a structured HDNN architecture for speech recognition, in which the two gate functions were tied across all the hidden layers, and we were able to train a much smaller model without sacrificing the recognition accuracy. In this paper, we carry on the study of this architecture with sequence-discriminative training criterion and speaker adaptation techniques on the AMI meeting speech recognition corpus. We show that these two techniques improve speech recognition accuracy on top of the model trained with the cross entropy criterion. Furthermore, we demonstrate that the two gate functions that are tied across all the hidden layers are able to control the information flow over the whole network, and we can achieve considerable improvements by only updating these gate functions in both sequence training and adaptation experiments.", "text": "highway deep neural network type depthgated feedforward neural network shown easier train hidden layers also generalise better compared conventional plain deep neural networks previously investigated structured hdnn architecture speech recognition gate functions tied across hidden layers able train much smaller model without sacriﬁcing recognition accuracy. paper carry study architecture sequence-discriminative training criterion speaker adaptation techniques meeting speech recognition corpus. show techniques improve speech recognition accuracy model trained cross entropy criterion. furthermore demonstrate gate functions tied across hidden layers able control information whole network achieve considerable improvements updating gate functions sequence training adaptation experiments. although tremendously successful ﬁeld speech processing neural network models usually criticised lack structure less interpretable less adaptable. furthermore neural network acoustic models much larger conventional models using gaussian mixtures make challenging deploy models resource constrained platforms embedded devices. recently works overcome limitations. example investigated stimulated learning deep feedforward neural networks make interpretable gain insight behaviour networks. hand order address size neural network acoustic models small-footprint neural network models received considerable research efforts using low-rank matrices teacher-student style training structured linear layers small-footprint models superior several aspects. apart requiring lower computational cost taking less memory applicable low-resource languages amount training data usually much smaller. furthermore smaller number model parameters models adaptable target domains environments speakers different training condition. previously proposed small-footprint acoustic model using highway deep neural network hdnn type network shortcut connections hidden layers compared plain networks skip connections hdnns equipped gate functions transform carry gate control facilitate information whole network. particular transform gate used scale output hidden layer carry gate used pass input directly elementwise rescaling. gate functions train deep networks speed convergence experimentally validated furthermore speech recognition recognition accuracy retained simply increasing depth network number hidden units hidden layer signiﬁcantly reduced. result networks became much thinner deeper much smaller number model parameters. contrast training plain feedforward neural networks depth width encounter difﬁculty train highway networks using standard stochastic gradient decent algorithm without pretraining however study focused cross entropy training networks previously paper investigate previous observations still hold case sequence training. understand effect gate functions hdnns performed ablation experiments disabled update model parameters hidden layers and/or classiﬁcation layer sequence training. based experiments using meeting transcription corpus observed updating parameters gate functions able retain improvement sequence training supports argument gate functions manipulate behaviour hidden layers nonlinear feature extractor. since number model parameters gate functions relatively small study speaker adaptation techniques unsupervised fashion tune gate functions using speaker dependent data. using seed models sequence training able obtain consistent improvement speaker adaptation. overall small-footprint hdnn acoustic model million model parameters achieved slightly better results compared baseline million parameters hdnn model million parameters obtained slightly lower accuracy compared baseline. facilitate discussion divide parameters standard neural network acoustic model sets represents model parameters classiﬁer denotes model parameters hidden layers feature extractor. given input acoustic frame time step feature extractor transforms input another feature representation denotes hidden activations l-th layer parameterised denotes activation function sigmoid tanh; transform gate scales original hidden activations; carry gate scales input passing directly next hidden layer; denotes elementwise multiplication; outputs constrained within sigmoid function gates parameterised respectively. following previous work parameters gate functions across hidden layers signiﬁcantly save model parameters. work bias vector gate functions. since parameters layerindependent denote look speciﬁc roles model parameters sequence training model adaptation experiments. weight matrix l-th layer compute ˜wlhl− all. trick leverage power gpus computing large matrix-matrix multiplications efﬁciently minibatch mode speed training signiﬁcantly. index hidden markov model state denotes ground truth label however state-of-the-art speech recognition systems usually built sequence-training techniques loss function deﬁned sequence level. approaches well understood neural network acoustic models instance denote sequence acoustic frames sequence labels length signal loss function scalable minimum bayesian risk criterion deﬁned measures state level distance ground truth predicted labels; denotes hypothesis space represented denominator lattice word-level transcription; acoustic score scaling parameter. however applying sequence training criterion without regularisation lead overﬁtting observed address problem interpolate smbr loss function loss used smoothing parameter. paper focus smbr criterion since achieve comparable slightly better results compared maximum mutual information minimum pone error criterion experimental section also study effect regularisation term different model parameter sets highway neural network acoustic models. table comparison hdnn system smbr training. systems built using kaldi toolkit networks pre-trained using restricted bolzman machines. results shown terms word error rates denote size hidden units number layers. indicates million model parameters. adaption standard feedforward neural networks challenging large number unstructured model parameters amount adaptation data usually much smaller. traditional approaches include input output layer adaptation recently researchers incorporate speaker dependent model parameters model space manipulate behaviour transform output network. techniques belong category include speaker code lhuc multiple basis neural networks hdnn architecture studied paper structured sense parameters gate functions layer-independent demonstrated experimental section able control behaviour hidden layers. motivates investigate adaptation highway gates ﬁning tune model parameters. although number parameters gate functions still much larger compared amount adaptation data per-speaker level size gate functions controllable reduce number hidden units without sacriﬁcing accuracy increasing depth neural network another adaptation approach input feature augmentation method using i-vectors complimentary study investigated paper. experiments performed individual headset microphone subset meeting speech transcription corpus amount training data around hours corresponding roughly million frames. used -dimensional fmllr adapted features vectors nortable results switching update different model parameters sequence training. denotes model parameters hidden layers denotes parameters gate functions parameters softmax layer. malised per-speaker level spliced context window frames number tied states hdnn models trained using cntk toolkit results obtained using kaldi decoder also used kaldi toolkit compute alignment lattices sequence training well feature transforms. momentum epoch training used sigmoid activation networks. weights hidden layer hdnns randomly initialised uniform distribution range bias parameters initialised used trigram language model decoding. order make experimental results comparable used training cross-validation sets split alignment hdnn plain systems. systems used decision tree state tying. showed smaller hdnn acoustic model comparable much larger plain model terms accuracy trained criterion performed much better compared dnns similar size. experiment investigate observation still holds sequence training. performed smbr update iterations learning rate frame cntk kaldi systems. regularisation parameter avoid overﬁtting cntk systems followed recipe kaldi systems using slightly different regularisation technique. baseline systems reproduced fig. convergence curves smbr training without regularisation. regularisation term stabilise convergence updating model parameter role diminishing updating gate functions only. using up-to-date kaldi toolkit obtained slightly better results compared cntk train plain models converge random initialisation thin deep networks reported previously table shows sequence training results plain hdnn systems which sequence training improves recognition accuracy comparably hdnn systems improvements consistent eval sets shown table again hdnn model around million model parameters plain system million model parameters terms recognition accuracy. follows present results eval set. previous experiments updated model parameters hdnns sequence training. look effect speciﬁc parameter performed ablation experiments switched update model parameters. results given table show updating parameters gates retain improvement given sequence training updating close optimum. note that accounts small fraction total number parameters e.g. hdnn-hl system hdnn-hl system. however results demonstrate gate functions largely manipulate behaviour neural network feature extractor. investigated effect regularisation term sequence training. performed experiments without regularisation system settings i.e. update model parameters; update gate functions. motivation validate updating gate parameters resistant overﬁtting. results given table switching regularisation term achieve even slightly lower updating gate functions only. however updating model parameters regularisation term turned important stabiliser convergence. figure shows convergence curves system settings. overall gate functions largely control behaviour highway networks prone overﬁtting model parameters switched update. observations sequence training experiments inspired study speaker adaptation gate functions control behaviour feature extractor relatively small number model parameters. paper term speaker adaptation config. unsupervised adaptation results different number adaptation iterations. speaker-independent models trained smbr used criterion adaptation experiments. vention though speaker deﬁned cluster acoustic frames granularity. ﬁrstly performed experiments unsupervised speaker adaptation setting decoded evaluation using speakerindependent models used pseudo labels tune parameters second pass. evaluation around hours audio number speakers average speaker around minutes speech corresponds thousand frames. compared size hdnns amount adaptation data still small e.g. size hdnn-hl system around million. learning rate sample updated iterations. table shows adaptation results observe small consistent reduction different model conﬁgurations speaker adapted features using fmllr. notably improvements consistent seed models speaker-independent models trained using either smbr criterion. updating model parameters yields smaller improvements shown table. speaker adaptation sequence training hdnn system million model parameters works slightly better baseline million parameters hdnn model million parameters achieves slightly higher compared baseline figure show adaptation results different number iterations. observe best results achieved adaptation iterations thought updating gate functions yield overﬁtting. validate this also experiments adaptation iterations still observe overﬁtting. observation line sequence training experiments demonstrating table results unsupervised speaker adaptation. here updated using criterion speakerindependent models trained either smbr. denotes speaker-dependent models. previous experiments studied unsupervised adaptation condition obtained labels adaptation ﬁrst-pass decoding. order evaluate impact accuracy labels adaptation method performed diagnostic experiments used oracle labels adaptation. obtained oracle labels force alignment using model trained criterion word level transcriptions. also ﬁxed alignment adaptation experiments order compare results different seed models. figure shows adaptation results oracle labels demonstrates signiﬁcant reduction achieved supervision labels accurate. therefore gate functions large capacity adaptation high quality pseudo labels. study aspect future shall investigate supervised adaptation highway networks. highway deep neural networks structured depth-gated feedforward neural networks. paper studied sequence training adaptation networks acoustic modelling. particular investigated roles parameters hidden layers gate functions classiﬁcation layer case sequence training. show gate functions accounts small fraction whole parameter able control information adjust behaviour neural network feature extractors. demonstrated sequence training adaptation experiments which considerable improvements achieved updating gate functions. techniques obtained comparable slightly lower wers much smaller acoustic models compared strong baseline conventional acoustic model sequence training. since number model parameters still relative large compared speaker-level adaptation data adaptation technique applicable domain adaptation scenarios amount adaptation data relatively large. future shall also investigate model compression techniques improve results small-footprint acoustic models. sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets proc. icassp. sindhwani sainath kumar structured transforms small-footprint deep learning proc. nips moczulski denil appleyard freitas acdc structured efﬁcient linear layer proc. iclr kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization proc. interspeech abdel-hamid jiang fast speaker adaptation hybrid nn/hmm model speech recognition based discriminative learning speaker code proc. icassp. ieee eversole seltzer huang guenter kuchaiev zhang seide wang introduction computational networks computational network toolkit tech. rep. microsoft research tech. rep. povey ghoshal boulianne burget glembek goel hannemann motlıcek qian schwarz silovsk´y semmer vesel´y kaldi speech recognition toolkit proc. asru", "year": 2016}