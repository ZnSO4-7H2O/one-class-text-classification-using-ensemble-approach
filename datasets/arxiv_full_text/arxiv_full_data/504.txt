{"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "tag": ["cs.NE", "cs.CL", "cs.CV"], "abstract": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.", "text": "module memory module improve question answering. propose input module uses level encoder sentence reader input fusion layer allow information sentences. memory propose modiﬁcation gated recurrent units formulation incorporates attention gates computed using global knowledge facts. unlike before dmn+ model require supporting facts labeled training. model learns select important facts larger set. addition introduce input module represent images. module compatible rest architecture output memory module. show changes memory module improved textual question answering also improve visual question answering. tasks illustrated fig. general architecture question answering composed modules allow different aspects input representations memory components analyzed improved independently. modules depicted fig. follows neural network architectures memory attention mechanisms exhibit certain reasoning capabilities required question answering. architecture dynamic memory network obtained high accuracy variety language tasks. however shown whether architecture achieves strong results question answering supporting facts marked training whether could applied modalities images. based analysis propose several improvements memory input modules. together changes introduce novel input module images order able answer visual questions. dmn+ model improves state visual question answering dataset babi-k text question-answering dataset without supporting fact supervision. neural network based methods made tremendous progress image text classiﬁcation however recently progress made complex tasks require logical reasoning. success based part addition memory attention components complex neural networks. instance memory networks able reason several facts written natural language triplets. attention mechanisms successful components machine translation image captioning models dynamic memory network example neural network model memory component attention mechanism. yields state results question answering supporting facts marked training sentiment analysis part-of-speech tagging. input module module processes input data question asked vectors termed facts represented total number facts. vectors ordered resulting additional information used later components. text kumar module consists input words. hidden state words question. episodic memory module episode memory aims retrieve information required answer question input facts. improve understanding question input especially questions require transitive reasoning episode memory module pass input multiple times updating episode memory after pass. refer episode memory summary relevant input pass relevance inferred question previous episode memory mt−. memory update mechanism responsible generating episode memory based upon contextual vector previous episode memory mt−. ﬁnal pass episodic memory contain information required answer question answer module answer module receives generate model’s predicted answer. simple answers single word linear layer softmax activation used. tasks requiring sequence output used decode concatenation vectors ordered tokens. cross entropy error answers used training propose compare several modeling choices crucial components input representation attention mechanism memory update. ﬁnal dmn+ model obtains highest accuracy babi-k dataset without supporting facts dataset several design choices motivated intuition accuracy improvements dataset. speciﬁed kumar single used process words story extracting sentence representations storing hidden states produced sentence markers. also provides temporal component allowing sentence know content sentences came them. whilst input module worked well babi-k supporting facts reported kumar perform well babi-k without supporting facts speculate main reasons performance disparity exacerbated removal supporting facts. first allows sentences context sentences them them. prevents information propagation future sentences. second supporting sentences away word level allow distant sentences interact word level gru. input fusion layer dmn+ propose replacing single different components. ﬁrst component sentence reader responsible encoding words sentence embedding. second component input fusion layer allowing interactions sentences. resembles hierarchical neural auto-encoder architecture allows content interaction sentences. adopt bi-directional input fusion layer allows information past future sentences used. gradients need propagate words sentences fusion layer also allows distant supporting sentences direct interaction. fig. shows illustration input module positional encoder used sentence reader bi-directional adopted input fusion layer. sentence encoding output encoding scheme taking word tokens length sentence. sentence reader could based variety encoding schemes. selected positional encoding described sukhbaatar allow comparison work. grus lstms also considered required computational resources prone overﬁtting auxiliary tasks reconstructing original sentence used. input fact timestep −→fi hidden state forward timestep ←−fi hidden state backward timestep allows contextual information future past facts impact ←→fi explored variety encoding schemes sentence reader including grus lstms positional encoding scheme described sukhbaatar simplicity speed selected positional encoding scheme. grus lstms also considered required computational resources prone overﬁtting auxiliary tasks reconstructing apply visual question answering introduce input module images. module splits image small local regions considers region equivalent sentence input module text. input module composed three parts illustrated fig. local region feature extraction visual feature embedding input fusion layer introduced sec. local region feature extraction extract features image convolutional neural network based upon vgg- model ﬁrst rescale input image take output last pooling layer dimensionality pooling layer divides image grid resulting local regional vectors visual feature embedding task involves image features text features linear layer tanh activation project local regional vectors textual feature space used question vector input fusion layer local regional vectors extracted global information available them. without global information representational power quite limited simple issues like object scaling locational variance causing accuracy problems. solve this input fusion layer similar textual input module described sec. first produce input facts traverse image snake like fashion seen figure apply bi-directional input facts produce soft attention soft attention produces contextual vector weighted summation sorted list vectors corresponding attention gates i←→f method advantages. first easy compute. second softmax activation spiky approximate hard attention function selecting single fact contextual vector whilst still differentiable. however main disadvantage soft attention summation process loses positional ordering information. whilst multiple attention passes retrieve information inefﬁcient. attention based complex queries would like attention mechanism sensitive position ordering input facts would advantageous situation except cannot make attention gate equation propose modiﬁcation architecture embedding information attention mechanism. update gate equation decides much dimension hidden state retain much updated transformed input current timestep. computed using current input hidden state previous timesteps lacks knowledge question previous episode memory. scalar generated important consideration using softmax activation opposed vector generated using sigmoid activation. allows easily visualize attention gates activate input later shown visual fig. though explored replacing softmax activation equation sigmoid activation would result produce contextual vector used updating episodic memory state ﬁnal hidden state attention based gru. episode memory updates episodic memory module depicted fig. retrieves information input facts provided focusing attention subset facts. implement attention associating sini fact scalar value attention gate pass computed allowing interactions fact question representation episode memory state. implemented kumar involved complex interactions within containing additional terms mt−]. initial analysis found additional terms required. attention mechanism attention gate attention mechanism extract contextual vector based upon current focus. focus types attention soft attention attention based gru. latter improves performance hence ﬁnal modeling choice dmn+. update episode memory newly constructed contextual vector producing initial hidden state question vector used purpose. episodic memory pass computed work sukhbaatar suggests using different weights pass episodic memory advantageous. model contains weights episodic passes input referred tied model weights table concatenation operator rnh×nh hidden size. untying weights using relu formulation memory update improves accuracy another shown table last column. ﬁnal output memory network passed answer module original dmn. related major lines recent work memory attention mechanisms. work visual textual question answering have developed separate communities. neural memory models earliest recent work memory component applied language processing memory networks adds memory component question answering simple facts. similar dmns also input scoring attention response mechanisms. however unlike input module computes sentence representations independently hence cannot easily used tasks sequence labeling. like original memory network requires supporting facts labeled training. end-toend memory networks limitation. contrast previous memory models variety different functions memory attention retrieval representations dmns shown neural sequence models used input representation attention response mechanisms. sequence models naturally capture position temporality inputs transitive reasoning steps. neural attention mechanisms attention mechanisms allow neural network models question selectively attention speciﬁc inputs. beneﬁt image classiﬁcation generating captions images among others mentioned below machine translation recent neural architectures memory attention proposed include neural turing machines neural gpus stack-augmented rnns question answering question answering involving natural language solved variety ways cannot justice. potential input large text corpus becomes combination information retrieval extraction neural approaches include reasoning knowledge bases directly sentences trivia competitions visual question answering comparison still relatively young task feasible objects identiﬁed high accuracy. ﬁrst large scale database unconstrained questions images introduced antol datasets existed include openended free-form questions general images others small viable deep learning approach model also attention component stacked attention network work also uses based features. however unlike input fusion layer single layer neural network features patch dimensionality question vector. hence model cannot easily incorporate adjacency local information hidden state. model also uses neural modules albeit logically inspired ones andreas evaluate knowledgebase reasoning visual question answering. compare directly method latter task dataset. related visual question answering task describing images sentences socher used deep learning methods images sentences space order describe images sentences images best visualize sentence. ﬁrst work modalities joint space deep learning methods could select existing sentence describe image. shortly thereafter recurrent neural networks used generate often novel sentences based images evaluating textual question answering babi-k english synthetic dataset features different tasks. example composed facts question answer supporting facts lead answer. dataset comes sizes referring number training examples task babi-k babi-k. experiments sukhbaatar found lowest error rates smaller babi-k dataset average three times higher babi-k. dataset question answering real-world images consists training images test images. based upon images training questions test questions generated. following previously deﬁned experimental method exclude multiple word answers resulting dataset covers original data. evaluation method uses classiﬁcation accuracy single words. development dataset model analysis visual question answering dataset constructed using microsoft coco dataset contained training/validation images test images. image several related questions question answered multiple people. dataset contains training questions validation questions testing. testing data split test-development test-standard testchallenge antol evaluation test-standard test-challenge implemented submission system. test-standard evaluated times test-challenge evaluated competition. best knowledge largest complex image dataset visual question answering task. original architecture presented kumar without modiﬁcations. replaces input module input fusion layer based upon replaces soft attention mechanism attention based proposed sec. finally dmn+ based upon untied model using unique weights pass linear layer relu activation compute memory update. report performance model variations table large improvement accuracy babi-k textual daquar visual datasets results updating input module seen comparing odmn dmn. datasets input fusion layer improves interaction distant facts. visual dataset improvement purely providing contextual information neighboring image patches allowing handle objects varying scale questions locality aspect. textual dataset improved interaction sentences likely helps path ﬁnding required logical reasoning multiple transitive steps required. addition attention helps answer questions complex positional ordering information required. change impacts textual dataset questions visual dataset likely require form logical reasoning. finally untied model dmn+ overﬁts tasks compared average error rate decreases. experimental results combination proposed model changes results culminating dmn+ achieves highest performance across visual textual datasets. trained models using adam optimizer learning rate batch size training runs epochs early stopping validation loss improved within last epochs. model epoch lowest validation loss selected. xavier initialization used weights except word embeddings used rates various model architectures table test error babi-k dataset accuracy performance daquar-all visual dataset. skipped babi questions achieved error across models. swer module keeping input probability last training data task chosen validation set. tasks three passes used episodic memory module allowing direct comparison state methods. finally limited input last sentences tasks except limited input last sentences similar sukhbaatar tasks accuracy stable across multiple runs. particularly problematic solve this repeated training times using random initializations evaluated model achieved lowest validation loss. text results compare best performing approach dmn+ state question answering architectures memory network neural reasoner framework neither approach supporting facts training. end-to-end memory network form memory network tested textual question answering language modeling. model features explicit memory recurrent attention mechanism. select model paper achieves lowest mean error babi-k dataset. model utilizes positional encoding input rnn-style tied weights episode module relu non-linearity task supporting facts supporting facts argument relations yes/no questions counting lists/sets simple negation basic coreference time reasoning basic induction positional reasoning size reasoning path ﬁnding mean error failed tasks table test error rates various model architectures tasks babi english dataset. end-to-end memory network results sukhbaatar neural reasoner original auxiliary task peng dmn+ achieve error babi question sets neural reasoner framework end-to-end trainable model features deep architecture logical reasoning interaction-pooling mechanism allowing interaction multiple facts. neural reasoner framework tested challenging question types time. table compare accuracy question answering architectures mean error error individual tasks. dmn+ model reduces mean error compared end-to-end memory network achieving state babi-k dataset. notable deﬁciency model basic induction. sukhbaatar untied model using summation memory updates able achieve near perfect error rate memory update replaced linear layer relu activation end-to-end memory network’s overall mean error decreased error rose sharply. model experiences difﬁculties suggesting complex memory update component prevent convergence certain simpler tasks. neural reasoner model outperforms end-to-end memory network positional reasoning. likely positional reasoning task involves minimal supervision sentences input yes/no answers supervision unique examples removing duplicates initial classes utilize full connected image feature classiﬁcation perform reasoning multiple small image patches. approach small image patches rest fully-connected whole image feature approach. here show quantitative qualitative results table fig. respectively. images fig. illustrate attention gate selectively activates relevant portions image according query. table method outperforms baseline state-of-the-art methods across question domains test-dev test-std especially questions achieves wide margin compared architectures likely small image patches allow ﬁnely detailed reasoning image. however granularity offered small image patches always offer advantage. number questions solvable architectures potentially counting objects simple task object crosses image patch boundaries. proposed modules framework achieve strong results without supervision supporting facts. improvements include input fusion layer allow interactions input facts novel attention based allows logical reasoning ordered inputs. resulting model obtains state results dataset babi-k text question-answering dataset proving framework generalized across input domains. training examples. peng auxiliary task reconstructing original sentences question representations. auxiliary task likely improves performance preventing overﬁtting. dataset question answered multiple people answers same generated answers evaluated using human consensus. predicted answer question target answer accuracy accv indicator function. simply answer accurate least people provide exact answer. training details adam optimizer learning rate batch size training runs epochs early stopping validation loss improved last epochs. weight initialization sampled random uniform distribution range word embedding hidden layers vectors size apply dropout initial image output convolutional neural network well input answer module keeping input probability results analysis dataset composed three question domains yes/no number other. enables analyze performance models various tasks require different reasoning abilities. figure examples qualitative results attention vqa. original images shown left. right show attention gate activates given pass image query. white regions active. answers given dmn+. merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. emnlp", "year": 2016}