{"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the art results on the WMT English$\\rightarrow$German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.", "text": "work propose simple effective technique using unsupervised pretraining improve seqseq models. proposal initialize encoder decoder networks pretrained weights language models. pretrained weights ﬁne-tuned labeled corpus. ﬁne-tuning phase jointly train seqseq objective language modeling objectives prevent overﬁtting. benchmark method machine translation english→german abstractive summarization daily mail articles. main result seqseq model pretraining exceeds strongest possible baseline neural machine translation phrasebased machine translation. model obtains improvement bleu previous best models wmt’ wmt’ english→german. human evaluations abstractive summarization model outperforms purely supervised baseline terms correctness avoiding unwanted repetition. also perform ablation studies understand behaviors pretraining method. study conﬁrms among many possible choices using language model seqseq attention proposal works best. study also shows that translation main gains come improved generalization pretrained features. summarization pretraining encoder gives large improvements suggesting gains come improved optimization encoder unrolled hundreds timesteps. tasks proposed method always improves generalization test sets. work presents general unsupervised learning method improve accuracy sequence sequence models. method weights encoder decoder seqseq model initialized pretrained weights language models ﬁne-tuned labeled data. apply method challenging benchmarks machine translation abstractive summarization signiﬁcantly improves subsequent supervised models. main result pretraining improves generalization seqseq models. achieve state-of-theart results english→german task surpassing range methods using phrase-based machine translation neural machine translation. method achieves signiﬁcant improvement bleu previous best models wmt’ wmt’ english→german. also conduct human evaluations abstractive summarization method outperforms purely supervised learning baseline statistically signiﬁcant manner. models sequence extremely effective variety tasks require mapping variable-length input sequence variable-length output sequence. main weakness sequence sequence models deep networks general lies fact figure pretrained sequence sequence model. parameters encoder blue parameters decoder. parameters shaded pretrained either source side target side language model. otherwise randomly initialized. following section describe basic unsupervised pretraining procedure sequence sequence learning modify sequence sequence learning effectively make pretrained weights. show several extensions improve basic model. sequence sequence sequence learning encoder used represent hidden vector given decoder produce output sequence. method based observation without encoder decoder essentially acts like language model y’s. similarly encoder additional output layer also acts like language model. thus natural trained languages models initialize encoder decoder. therefore basic procedure approach pretrain seqseq encoder decoder networks language models trained large amounts unlabeled text data. seen figure parameters shaded boxes pretrained. following describe method detail using first monolingual datasets collected source side language target side language. language model trained dataset independently giving trained source side corpus trained target side corpus. language models trained multilayer seqseq model constructed. embedding ﬁrst lstm layers encoder decoder initialized pretrained weights. even efﬁcient softmax decoder initialized softmax pretrained target side seqseq model initialized ﬁne-tuned labeled dataset. however procedure lead catastrophic forgetting model’s performance language modeling tasks falls dramatically ﬁne-tuning hamper model’s ability generalize especially trained small labeled datasets. ensure model overﬁt labeled data regularize parameters pretrained continuing train monolingual language modeling losses. seqseq language modeling losses weighted equally. chine translation abstractive summarization. task compare previous best systems. also perform ablation experiments understand behavior component method. machine translation dataset evaluation machine translation evaluate method english→german task used training dataset slightly smaller dataset. because dataset noisy examples used language detection system ﬁlter training examples. sentences pairs either source english target german thrown away. resulted around million training examples. following sennrich subword units merge operations giving vocabulary size around validation concatenated newstest newstest test sets newstest newstest. evaluation validation case-sensitive bleu tokenized text using multi-bleu.perl. evaluation test sets case-sensitive bleu detokenized text using mteval-va.pl. monolingual training datasets news crawl english german corpora billion tokens. experimental settings language models trained fashion used layer dimensional lstm hidden state projected units trained week tesla gpus. seqseq model layer model second third layers hidden units. monolingual objectives residual connection modiﬁed attention used. used adam optimizer train asynchronous gpus speed. used learning rate multiplied every steps initial steps gradient clipping norm dropout non-recurrent connections used early stopping validation perplexity. beam size used decoding. ensemble con. improvements model pretraining monolingual language modeling losses provide vast majority improvements model. however early experimentation found minor consistent improvements additional techniques residual connections multi-layer attention residual connections described input vector decoder softmax layer random vector high level layers lstm randomly initialized. introduces random gradients pretrained parameters. avoid this residual connection output ﬁrst lstm layer directly input softmax multi-layer attention models attention mechanism model attends ﬁrst layer concretely given query vector decoder encoder states encoder states ﬁrst layer last layer compute attention context vector follows system phrase based supervised edit distance transducer edit distance transducer backtranslation backtranslation backtranslation pretraining pretrained seqseq pretrained seqseq figure english→german ablation study measuring difference validation bleu various ablations full model. negative worse. full model uses trained monolingual data initialize encoder decoder plus language modeling objective. results table shows results method comparison baselines. method achieves state-of-the-art single model performance newstest newstest signiﬁcantly outperforming competitive semi-supervised backtranslation technique equally impressive fact best single model outperforms previous state ensemble models. ensemble models matches exceeds ablation study order better understand effects pretraining conducted ablation study modifying pretraining scheme. primarily interested varying pretraining scheme monolingual language modeling objectives techniques produce largest gains model. figure shows drop validation bleu various ablations compared full model. full model uses trained monolingual data initialize encoder decoder addition language modeling objective. follow pretrain much possible beneﬁts compound given drops pretraining pretraining encoder additive estimate drop pretraining decoder side however actual drop much larger drop additive estimate. pretraining softmax important pretraining embeddings ﬁrst lstm layer gives large drop bleu points. language modeling objective strong regularizer drop bleu points pretraining entire model using objective using objective without pretraining. pretraining unlabeled data essential learning extract powerful features model initialized pretrained source part target part parallel corpus drop performance large pretraining all. however performance remains strong pretrained large nonnews wikipedia corpus. understand contributions unsupervised pretraining supervised training track performance pretraining function dataset size. this trained model without pretraining random subsets english→german corpus. models additional objective. results summarized figure labeled data used pretrained pretrain model bleu points. however grows less data available. trained labeled data becomes bleu points. demonstrates abstractive summarization dataset evaluation low-resource abstractive summarization task cnn/daily mail corpus following nallapati modify data collection scripts restore bullet point summaries. task predict bullet point summaries news article. dataset fewer document-summary pairs. compare nallapati used anonymized corpus. however ablation study used non-anonymized corpus. evaluate system using full length rouge anonymized corpus particular considered highlight separate sentence following nallapati setting used english gigaword corpus larger unlabeled monolingual corpus although data used task english. encourage future researchers nonanonymized version realistic summarization setting larger vocabulary. numbers non-anonymized test rouge- rouge- rouge-l. consider highlights separate sentences. figure summarization ablation study measuring difference validation rouge various ablations full model. negative worse. full model uses trained unlabeled data initialize encoder decoder plus language modeling objective. predict entire summary. language model trained used initialize encoder decoder since source target languages same. however encoder decoder tied. one-layer lstm size trained similar fashion jozefowicz seqseq model settings machine translation experiments. differences layer model second layer having hidden units learning rate multiplied every steps initial steps. results table summarizes results anonymized version corpus. pretrained model able match previous baseline seqseq nallapati interestingly pretrained wordvec vectors initialize word embeddings. show ablation study pretraining embeddings gives large improvement. furthermore model unidirectional lstm bidirectional lstm. also longer context tokens whereas used context tokens memory issues. ablation study performed ablation study similar performed machine translation model. results reported figure report drops rouge- rouge- rouge-l nonanonymized validation set. marization model. interpretation pretraining enables gradient much back time randomly initialized weights. also explain pretraining parallel corpus worse pretraining larger monolingual corpus. language modeling objective strong regularizer model without objective signiﬁcant drop rouge scores. human evaluation rouge able capture quality summarization also performed small qualitative study understand human impression summaries produced different models. took random documents compared performance pretrained non-pretrained system. document gold summary system outputs presented human evaluator asked rate system output scale best score. system outputs presented random order evaluator know identity either output. evaluator noted repetitive phrases sentences either system outputs. unwanted repetition also noticed nallapati table show results study. cases pretrained system outperforms system without pretraining statistically significant manner. better optimization enabled pretraining improves generated summaries decreases unwanted repetition output. table count often pretrain system achieves higher equal lower score pretrained system side-byside study human evaluator gave system score sign statistical test gives p-value rejecting null hypothesis difference score obtained either system. table count often pretrain pretrain systems contain repeated phrases sentences outputs side-by-side study. mcnemar’s test gives p-value rejecting null hypothesis systems repeat proportion times. pretrained system clearly repeats less system without pretraining. acoustic models. recent acoustic models found pretraining unnecessary probably reconstruction objective deep belief networks easy. contrast pretraining language models next step prediction signiﬁcantly improves seqseq challenging real world datasets. despite appeal unsupervised learning widely used improve supervised training. radford amongst rare studies showed beneﬁts pretraining semi-supervised learning setting. methods similar except decoder network thus could apply seqseq learning. similarly zhang zong found useful additional task sentence reordering sourceside monolingual data neural machine translation. various forms transfer multitask learning seqseq framework also ﬂavors algorithm perhaps closely related method work gulcehre combined language model already trained seqseq model ﬁne-tuning additional deep output layers. empirically method produces small improvements supervised baseline. suspect method produce signiﬁcant gains models trained independently ﬁne-tuned combined seqseq model last layer wasting beneﬁt level features using decoder side. venugopalan addressed still experienced minor improvements. using related approach principle work chen proposed two-term theoretically motivated unsupervised objective unpaired input-output samples. though apply method seqseq learning framework modiﬁed case ﬁrst term pushes output highly probable scoring model second term ensures output depends input. seqseq setting interpret ﬁrst term pretrained language model scoring output sequence. work fold pretrained language model decoder. believe using pretrained language model scoring less efﬁcient using pretrained weights. labeled examples satisﬁes second term. connections provide theoretical grounding work. experiments benchmark method machine translation unsupervised methods shown give promising results backtranslation trained model used decode unlabeled data yield extra labeled data. argue method natural analogue tasks summarization. note technique complementary ours lead additional gains machine translation. method using autoencoders cheng promising though argued autoencoding easy objective language modeling force unsupervised models learn better features. presented novel unsupervised pretraining method improve sequence sequence learning. method generalization optimization. scheme involves pretraining language models source target domain initializing embeddings ﬁrst lstm layers softmax sequence sequence model weights language models. using method achieved state-of-the-art machine translation results wmt’ wmt’ english german. advantage technique ﬂexible applied large variety tasks. ondˇrej bojar rajen chatterjee christian federmann barry haddow matthias huck chris hokamp philipp koehn varvara logacheva christof monz matteo negri matt post carolina scarton lucia specia marco turchi. findings workshop statistical machine translation. proceedings tenth workshop statistical machine translation. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp. dahl deng acero. context-dependent pre-trained deep neural networks ieee large-vocabulary speech recognition. transactions audio speech language processing orhan firat baskaran sankaran yaser al-onaizan fatos yarman-vural kyunghyun cho. zero-resource translation multilingual neural machine translation. arxiv preprint arxiv.. goodfellow mehdi mirza xiao aaron courville yoshua bengio. eminvestigation catastrophic forgetting pirical arxiv preprint gradient-based neural networks. arxiv.. caglar gulcehre orhan firat kelvin kyunghyun loic barrault huei-chi fethi bougares holger schwenk yoshua bengio. using monolingual corpora neural machine translation. arxiv preprint arxiv.. s´ebastien jean orhan firat kyunghyun roland memisevic yoshua bengio. montreal neural machine translation systems wmt’. proceedings tenth workshop statistical machine translation. courtney napoles matthew gormley benjamin provan durme. annotated gigaword. ceedings joint workshop automatic knowledge base construction web-scale knowledge extraction. acl. hasim andrew senior franc¸oise beaufays. long short-term memory recurrent neural network architectures large scale acoustic modeling. interspeech. felix stahlberg hasler bill byrne. edit distance transducer action university cambridge english-german system wmt. proceedings first conference machine translation pages berlin germany. association computational linguistics. subhashini venugopalan lisa anne hendricks raymond mooney kate saenko. improving lstm-based video description linguisarxiv preprint knowledge mined text. arxiv.. philip williams rico sennrich maria nadejde matthias huck barry haddow ondˇrej bojar. edinburgh’s statistical machine translation proceedings first systems wmt. conference machine translation pages berlin germany. association computational linguistics. wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig. achieving human parity conversational speech recognition. abs/.. source document like phone booths typewriters record stores vanishing breed another victim digital camelot music virgin megastores wherehouse music tower records gone corporate america largely abandoned brick mortar music retailing scattering independent stores many scruffy urban neighborhoods necessarily thing harder spotify place physical music many remaining record stores succeeding even thriving catering passionate core customers collectors saturday hundreds music retailers hold events commemorate record store annual celebration well neighborhood record store many stores host live performances drawings book signings special sales rare autographed vinyl happenings even serve beer diehard customers places mere stores cultural institutions celebrate music history display artifacts nurture local music scene also employ knowledgeable clerks happy debate relative merits blood tracks blonde blonde maybe like jack black high ﬁdelity mock lousy taste music music geek drop might think twice asking stock called love ground truth summary saturday record store celebrated music stores around world many stores host live performances drawings special sales rare vinyl pretrain corporate america largely abandoned brick brick mortar music many remaining record stores succeeding even thriving catering passionate core customers pretrained hundreds music retailers hold events commemorate record store many stores host live performances drawings book signings special sales rare autographed vinyl source document look small boast social media trigger whirlwind spins real life grief texas veterinarian found shooting kristen lindsey allegedly shot arrow back orange tabby head posted proud photo week facebook smiling dangled limp body arrow shaft lindsey added comment afﬁliate kbtx reported ﬁrst kill good feral tomcat arrow head year award gladly accepted callers rang phones washington county animal clinic lindsey worked vent outrage trafﬁc crashed website high price public shaming internet animal rescuer said lindsey prey probably feral elderly couple called tiger gone missing wednesday lindsey posted photo slain able conﬁrm claim ﬁrestorm grew lindsey wrote comments underneath post lose psshh like someone would awesome prediction wrong clinic ﬁred lindsey covered name marquee duct tape publicly distanced actions goal black hope people reasonable understand actions anyway portray washington animal clinic said bruce buenger heart soul place clinic told wbtx lindsey available comment reaching removed controversial post eventually shut facebook page callers also complained brenham police department washington county animal control facebook post went viral sheriff ofﬁce austin county apparently shot investigating lindsey could face charges dispatchers overloaded calls sheriff posted facebook asking please take easy dispatchers soon investigation complete post relevant information page post read animal rights activists pushing charges animal cruelty must taken seriously guilty parties punished fullest extent said advocacy activist becky robinson organization alley allies offering reward evidence leading arrest conviction person shot others stood lindsey amazing caring said customer shannon stoddard good maybe choice posting something facebook good think judged dropped balloons animal clinic lindsey thank note jeremy grisham contributed report ground truth summary kristen lindsey since removed post holding dead arrow employer ﬁred sheriff ofﬁce investigating activist offers reward pretrain kristen lindsey allegedly shot arrow back orange orange tabby head good good tomcat arrow head year award pretrained lindsey lindsey texas veterinarian shot arrow back orange tabby head posted photo smiling dangled limp body arrow shaft lindsey could face charges sheriff department says table pretrained model outputs highly relevant summary makes mistake feline executioner’s name. pretrain model degenerates irrelevant details repeats itself. source document eugenie bouchard poor form continued seed beaten american lauren davis second round family circle charleston wednesday davis lost career meeting bouchard control time world davis nine ﬁnal games match broke bouchard serve twice ﬁnal pull upset eugenie bouchard ﬁres serve second round match family circle bouchard shows frustrations straight sets defeat lauren davis wednesday never beaten came knowing nothing lose said davis ranked world bouchard semi-ﬁnalist last year struggled lead charlston green clay event losing three last matches lower ranked opponents davis used precise groundstrokes keep bouchard heels throughout second davis broke bouchard serve love take lead year canadian double fault sailed forehand long fall behind games later davis backhand send back bouchard serve winner left sideline lead davis hits forehand impressive canadian seed bouchard struggled recently time slumping fourth defeat matches match ended bouchard forehand davis waving cheering crowd bouchard said soon deﬁnitely anger also kind confusion slash like quest wrong like kind searching feeling like know something know something right want deﬁnitely little slow today overpowered usually dominating deﬁnitely know good ground truth summary eugenie bouchard suffered fourth defeat matches canadian seed lost lauren davis family circle world davis second round charleston davis nine ﬁnal games match seal victory click latest news charleston pretrain bouchard beat american lauren davis second round family circle charleston wednesday bouchard lost career meeting bouchard control time world bouchard nine ﬁnal games match broke bouchard serve twice ﬁnal pull upset pretrained eugenie bouchard beaten american lauren davis second round davis lost career meeting bouchard control time world davis backhand send back bouchard serve winner left sideline source document mike rowe coming river near sometimes hear person makes feel good humanity rowe says thursday episode somebody rowe meets chad pregracke founder living lands waters pregracke wants clean nation rivers piece detritus time quota always read mike rowe facebook post break litter habit since founded nonproﬁt ripe pregracke volunteers collected million pounds trash u.s. waterways efforts helped earn hero year award along numerous honors wherever matter stream creek lake whatever needs cleaned organize told anderson cooper pregracke also gives rowe tour foot solar powered barge living lands waters staff calls home lengthy cleanups part home part ofﬁce part dumpster seven bedrooms bathrooms classroom kitchen happens made recycled strip club according organization latest annual report pregracke made mission remove pounds trash like help achieve goal visit website learn help livinglandsandwaters.org involved ground truth summary chad pregracke hero year mike rowe visited pregracke episode somebody pretrain rowe meets chad pregracke founder living lands waters pregracke volunteers collected million pounds trash u.s. waterways pretrained rowe founder living lands waters pregracke also gives rowe tour foot barge living lands waters gets source mayor bloomberg told reporters that court order city suspended reopening public space protesters informed however local laws allow re-install camping shops sleeping bags. ground truth b¨urgermeister bloomberg stellt presse klar aufgrund dieser richterlichen anordnung erneute ¨offnung platzes f¨ur publikumsverkehr demonstranten aufgehoben worden demonstranten wies darauf dass stadtgesetze ihnen nicht erlaubten sich erneut zelten schlafs¨acken diesem einzurichten pretrain b¨urgermeister bloomberg sagte reportern dass stadt aufgrund dieser gerichtsentscheidung wiederer¨offnung ¨offentlichen raumes information demonstranten ausgesetzt habe dass lokalen gesetze ihnen nicht erlauben campingpl¨atzen schlafs¨acken installieren pretrained b¨urgermeister bloomberg erkl¨arte gegen¨uber journalisten dass stadt aufgrund dieser gerichtsentscheidung wiederer¨offnung ¨offentlichen raums ausgesetzt habe dass demonstranten dar¨uber informiert wurden dass ¨ortlichen gesetze ihnen nicht erlauben w¨urden sich campingpl¨atzen schlafs¨alen installieren table pretrain model makes complete mistranslation outputting information demonstranten ausgesetzt habe. translates reopening public space information protesters suspended instead informing protesters. furthermore wrongly separated sentences ﬁrst sentence extra words second sentence left without subject. pretrained model make mistakes. however models make vocabulary mistake installieren typically used refer installing software. human evaluator ﬂuent german english said pretrained version better. source february temperatures cause losses millions agricultural sector limited possibilities state economy grow causing contraction economic activity general percent ﬁrst half year mainly supported historic fall cent agriculture affected dynamics economic sectors. ground truth niedrigen temperaturen februar verursachten nicht verluste millionenh¨ohe landwirtschaft sondern steckten dar¨uber hinaus wachstum staatswirtschaft enge grenzen verursachten ersten vierteljahr einen allgemeinen r¨uckgang wirtschaftst¨atigkeit prozent dieser geht haupts¨achlich historischen abbau landwirtschaftlichen entwicklung prozent zur¨uck sich bremsend weitere wirtschaftssektoren auswirkte pretrain niedrigen temperaturen februar f¨uhrten nicht verlusten f¨ur landwirtschaft sondern beschr¨ankten m¨oglichkeiten staatlichen wirtschaft wachsen wachsen wodurch wirtschaftst¨atigkeit insgesamt prozent ersten h¨alfte jahres haupts¨achlich durch historischen r¨uckgang landwirtschaft beeinﬂusst wurde dynamik anderer wirtschaftssektoren betraf pretrained niedrigen temperaturen februar f¨uhrten nicht verlusten millionen f¨ur agrarsektor sondern beschr¨ankten m¨oglichkeiten staatswirtschaft wachsen einer schrumpfung wirtschaftst¨atigkeit allgemeinen prozent ersten h¨alfte jahres f¨uhrte haupts¨achlich durch historischen einbruch prozent landwirtschaft dynamik anderer wirtschaftsbereiche beeinﬂusst hatte table human evaluator noted pretrained version better correctly captures meaning sentence structure middle. pretrain model misses translating word million repeats wachsen wachsen puts verb beeinﬂusst wurde unnatural position. however pretrained model makes mistake percentage source facilitate inception second world allowed bankers politicians create latent conﬂict situation saddling germany huge reparations thereby making radicalist example impoverished masses remained introduce sufﬁciently convincing culprit leader simple solution also creating multi-racial czechoslovakia strong german minority play indeed role ﬁfth colony ignited. ground truth zweiten weltkrieg einfacher entfachen k¨onnen ließen banker durch politik eine latente konﬂiktsituation schaffen indem deutschland gigantischen kriegsreparationen belegten dadurch schufen voraussetzung verarmter massen dass deutschen noch einen ausreichend starken f¨uhrer unterjubeln mussten schuldigen benannte einfache l¨osungen anbot weiterer faktor schaffung vielv¨olkerstaates tschechoslowakei einer starken deutschen minderheit funktion einer f¨unften kolonne einnehmen sollte auch einnahm kriegsbrand entfachen pretrain gr¨undung zweiten weltkriegs erleichtern erm¨oglichte bankern politikern eine latente konﬂiktlage schaffen indem deutschland enormen reparationsforderungen konfrontierte wodurch radikalislamistisches beispiel verarmten massen entstand blieb einen ausreichend aussagekr¨aftigen schuldigen einen f¨uhrer einer einfachen l¨osung etablieren w¨ahrend gleichzeitig eine multi-ethnische tschechoslowakei einer starken deutschen minderheit spielen tats¨achlich rolle einer f¨unften kolonie pretrained einf¨uhrung zweiten weltkrieges erleichtern ließen banker politiker eine latente konﬂiktlage schaffen indem deutschland riesigen reparationszahlungen belieferten wodurch radikalislamistisches beispiel verarmten massen entstand blieb einen ausreichend ¨uberzeugenden schuldigen einen f¨uhrer einer einfachen l¨osung pr¨asentieren w¨ahrend gleichzeitig eine multiethnische tschechoslowakei einer starken deutschen minderheit geschaffen wurde spielen tats¨achlich rolle einer f¨unften kolonie sobald krieg entfacht worden table example english source poorly worded. models output poor translations evaluator noted pretrained version still better pretrain version. interestingly models mistranslate radical radikalislamistisches means radical islam probably bias training data. source total vote count also done ordinary calculation established difference winner candidate placed second position equal less percentage point long request representative political party whose candidate came second position case excluded electoral boxes considered partial recount. ground truth stimmenausz¨ahlung kann auch ihrer gesamtheit erfolgen wenn nach abschluss ordentlichen berechnung festgestellt wird dass unterschied zwischen mutmaßlichen gewinner kandidaten zweiten platz gleich oder geringer prozent vorausgesetzt liegt ausdr¨ucklicher antrag einem vertreter partei deren kandidat zweiter geworden diesem fall w¨urden wahlpakete einer teilweisen ausz¨ahlung ausgesetzt wurden ausgeschlossen pretrain gesamte stimmenanzahl wird auch dann erreicht wenn ende ordentlichen berechnung festgestellt wird dass unterschied zwischen sieger kandidaten zweiten position liegt gleich oder weniger einen prozentpunkt betr¨agt vorausgesetzt dass vertreter partei deren kandidat zweiten position fall wahlunterlagen w¨ahrend teilweisen r¨uckz¨ahlung ber¨ucksichtigt wurden ausgeschlossen werden gesamtzahl stimmzettel wird auch dann durchgef¨uhrt wenn ende ordentlichen berechnung festgestellt wird dass unterschied zwischen gewinner zweiten platz platzierten kandidaten gleich oder weniger einen prozentpunkt betr¨agt solange einen antrag vertreters politischen partei gibt dessen kandidat zweite position wahlzettel ausklammert w¨ahrend teilz¨ahlung ber¨ucksichtigt wurden table another example english source poorly worded. models structure right variety problematic translations. models miss meaning total vote count. also translate electoral boxes poorly pretrain model calls electoral paperwork pretrained model calls ballots. failures poorly worded english source. human evaluator found equally poor.", "year": 2016}