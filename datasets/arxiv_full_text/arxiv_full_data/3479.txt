{"title": "Deep Incremental Boosting", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "This paper introduces Deep Incremental Boosting, a new technique derived from AdaBoost, specifically adapted to work with Deep Learning methods, that reduces the required training time and improves generalisation. We draw inspiration from Transfer of Learning approaches to reduce the start-up time to training each incremental Ensemble member. We show a set of experiments that outlines some preliminary results on some common Deep Learning datasets and discuss the potential improvements Deep Incremental Boosting brings to traditional Ensemble methods in Deep Learning.", "text": "paper introduces deep incremental boosting technique derived adaboost speciﬁcally adapted work deep learning methods reduces required training time improves generalisation. draw inspiration transfer learning approaches reduce start-up time training incremental ensemble member. show experiments outlines preliminary results common deep learning datasets discuss potential improvements deep incremental boosting brings traditional ensemble methods deep learning. adaboost considered successful ensemble method commonly used combination traditional machine learning algorithms especially boosted decision trees main principles behind additional emphasis given so-called hard classify examples training set. deep neural networks also great success many visual problems number benchmark datasets area state-of-the-art results held deep learning algorithm ideas transfer learning found applications deep learning; example convolutional neural networks sub-features learned early training process carried forward order improve generalisation problem domain also shown transfer learning methods reduce warm-up phase training randomly-initialised would re-learn basic feature selectors scratch. paper explore synergy adaboost transfer learning accelerate initial warm-up phase training round boosting. proposed method named deep incremental boosting exploits additional capacity embedded round boosting increases generalisation without adding much training time. tested deep learning benchmarks method able beat traditional boosted cnns benchmark datasets shorter training time. paper structured follows. section presents overview prior work development based. section presents learning algorithm. section reports methodology preliminary experimentation results. section provides examples state-of-the-art models used base classiﬁers deep incremental boosting. lastly section makes conclusions experiments shows possible avenues development. adaboost well-known ensemble method proven track record improving performance. based principle training ensemble members rounds round increasing importance training examples misclassiﬁed previous round. ﬁnal ensemble aggregated using weights α..n calculated training. algorithm shows common adaboost.m variant. variant generally considered better multi-class problems used experimentation however changes apply adaboost.m applied variant adaboost. analysed low-layer features deep networks transferable considered general problem domain image recognition speciﬁcally found that example ﬁrst-layer tends learn ﬁlters either similar gabor ﬁlters color blobs. studies transfer learning unsupervised setting deep neural networks also reached similar conclusion. supervised deep learning contexts transfer learning achieved setting initial weights layers deep neural network previously-trained network. ﬁndings generality ﬁrst layers ﬁlters traditionally applied mostly ﬁrst layers. training continued dataset beneﬁt already-learned initial features provide much better starting position randomly initialised weights generalisation power improved time required train network reduced. traditional adaboost methods related variants re-train classiﬁer scratch every round. this combined weighted re-sampling training appears ﬁrst glance elements create diversity ﬁnal ensemble necessary re-initialize network scratch every round. already previously shown weights transferred networks particular subsets network accelerate initial training phase. case convolutional neural networks particular approach particularly fruitful lower layers tend consistently develop similar features. intuition subsequent round adaboost increases importance given errors made previous round network given round repurposed round learn newly resampled training set. conjecture given conjecture provided dataset mostly similar deﬁnition classiﬁer classiﬁes better randomly still perform better randomly dataset xt+. assumption weights structure classiﬁer correctly classiﬁes training differ greatly classiﬁer correctly classiﬁes training provided sets mostly similar. every subsequent round struture network copied extended additional hidden layer given position network layers copied network. preserve knowledge captured previous round allow additional capacity learn corrections xt+−xt. network trained iterations network doesn’t re-learn basic features already incorporates knowledge dataset gradients lower layers smaller learning concentrated newly added hidden layer also means classiﬁers require smaller number epochs converge many weights network already starting favourable position dataset. pick original training distribution create untrained classiﬁer additional layer shape lnew copy weights bottom layers train classiﬁer current subset weights normalisation factor distribution experiment repeated times adaboost.m deep incremental boosting using weight initialisations possible ﬂuctuation favourable random starting conditions neutralised. variant ﬁxed rounds boosting. trained ensemble member using adam used hold-out validation select best model. mnist common computer vision dataset associates pre-processed images hand-written numerical digits class label representing digit. input features pixel values images grayscale outputs numerical value cifar- dataset contains small images categories objects. ﬁrst introduced images pixels format. output categories airplane automobile bird deer frog horse ship truck. classes completely mutually exclusive translatable -vs-all multiclass classiﬁcation. samples training instances validation test another sets perfect class balance. cifar- dataset contains small images categories objects grouped super-classes. ﬁrst introduced image format cifar-. class labels provided classes well super-classes. super-class category includes ﬁne-grained class labels samples training instances validation test another sets perfect class balance. preliminary results table deep incremental boosting able generalise better adaboost.m. also adaboost.m larger cnns size largest used deep incremental boosting found classiﬁcation performance gradually getting worse weak learners overﬁtting training set. therefore assume additional capacity alone sufﬁcient justify improved generalisation speciﬁcally transferred weights previous round layer learning corrections training set. table deep incremental boosting best validation error reached much earlier last boosting round. conﬁrms observation section learning would converge earlier epoch subsequent rounds based used shorter training schedule subsequent rounds means able save considerable time compared original adaboost even though trained network larger number parameters. summary improved training times provided table base classiﬁers used experimentation section convenient large numbers repetitions lock-stepped random initialisations train relatively quickly. longest base classiﬁer train used cifar took hours. however models give results still state-of-the-art experimented complicated models applied deep incremental boosting. best result mnist doesn’t involve data augmentation manipulation obtained applying network network paper full model described able reproduce. goal train ensembles quickly reduced training schedule epochs applied adam update rule also sped training signiﬁcantly. network total million weights however signiﬁcantly higher number computations. table shows that although remaining examples learned able improve adaboost longer offers beneﬁts. addition this training time reduced signiﬁcantly compared adaboost. published models achieve state-of-the-art performance cifar- cifar make hold-out validation set. instead additional examples additional training data. order reproduce similar test error results principle applied experimental run. efﬁcient model allconvolutional networks proposed stateof-the-art results cifar- dataset replaces max-pooling additional convolution stride fully-connected layer convolutions instead convolutions reduce dimensionality output possible perform global average pooling. based larger model architecture order make computations feasible ensemble modify slightly. ﬁnal structure network follows network million weights considerably harder train original experiment. results reported table including training time comparison vanilla adaboost. original member trained epochs round deep incremental boosting ﬁrst trained epochs. additional layer created memory limitations improvement dramatic seen original experiments. however time improvement alone sufﬁcient justify using method. paper introduced algorithm called deep incremental boosting combines power adaboost deep neural networks transfer learning principles boosting variant able improve generalisation. tested algorithm compared adaboost.m deep neural networks found generalises better benchmark image datasets supporting claims. ﬁnal observation made fact still using entire ensemble test time. certain situations shown small model trained replicate bigger without signiﬁcant loss generalisation future work investigate possibility modify deep incremental boosting ﬁnal test-time deep neural network necessary.", "year": 2017}