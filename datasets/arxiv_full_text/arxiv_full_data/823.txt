{"title": "Scalable Recollections for Continual Lifelong Learning", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Given the recent success of Deep Learning applied to a variety of single tasks, it is natural to consider more human-realistic settings. Perhaps the most difficult of these settings is that of continual lifelong learning, where the model must learn online over a continuous stream of non-stationary data. A continual lifelong learning system must have three primary capabilities to succeed: it must learn and adapt over time, it must not forget what it has learned, and it must be efficient in both training time and memory. Recent techniques have focused their efforts largely on the first two capabilities while the third capability remains largely unexplored. In this paper, we consider the problem of efficient and effective storage of experiences over very large time-frames. In particular we consider the case where typical experiences are n bits and memories are limited to k bits for k << n. We present a novel scalable architecture and training algorithm in this challenging domain and provide an extensive evaluation of its performance. Our results show that we can achieve considerable gains on top of state-of-the-art methods such as GEM.", "text": "given recent success deep learning applied variety single tasks natural consider human-realistic settings. perhaps difﬁcult settings continual lifelong learning model must learn online continuous stream non-stationary data. continual lifelong learning system must three primary capabilities succeed must learn adapt time must forget learned must efﬁcient training time memory. recent techniques focused efforts largely ﬁrst capabilities third capability remains largely unexplored. paper consider problem efﬁcient effective storage experiences large time-frames. particular consider case typical experiences bits memories limited bits present novel scalable architecture training algorithm challenging domain provide extensive evaluation performance. results show achieve considerable gains state-of-the-art methods gem. long-held dream community build machine capable operating autonomously long periods even indeﬁnitely. machine must necessarily learn adapt crucially manage memory learned effectively tasks encounter. spectrum learning scenarios available depending problem requirements. lifelong learning machine presented sequence tasks must knowledge learned previous tasks perform better next. resource-constrained lifelong learning setting machine constrained small buffer previous experiences. approaches lifelong learning assume task examples chosen distribution instead machine given sequence examples without batching called continual learning. paper focus challenging continual learning scenario. continual learning three main requirements continually learn non-stationary environment retain memories useful manage time memory resources long period time. neural network research focused paper consider well investigate role efﬁcient experience storage avoiding catastrophic forgetting problem makes challenging. experience memory inﬂuential many recent approaches. example experience replay includes storage incoming experiences training later stabilizing component enabled deep learning atari games episodic storage mechanisms also earliest solutions catastrophic forgetting problem supervised learning setting unlike approaches simply focus forgetting representations tasks episodic storage techniques achieve superior performance ability continually improve tasks time useful information learned later techniques stored experiences stabilize learning. however consider agents must operate independently world long time. scenario assuming kind high-dimensional data make human experience efﬁcient storage experiences becomes important factor. storing full experiences memory methods causes storage costs scale linearly number experiences stored. truly scale learning massive number experiences non-stationary environment incremental cost adding experience memory must sub-linear number experiences. paper present scalable experience memory module learns improve time. experiments demonstrate empirically scalable recollection module achieves sub-linear scaling number experiences provides useful basis realistic continual learning system. related work storing parameters instead experiences. method complementary recent work leveraging episodic storage stabilize learning recently proposed methods lifelong learning don’t store experiences instead recording parameters network model task creates linear scaling respect number tasks. experiments settings long-term interest continual learning storage cost extra model parameters task signiﬁcantly exceeds task size corresponding experience buffer. addition approaches make simplifying often unrealistic assumption data stream batched coherent tasks. generative models support lifelong learning. pseudorehearsals related approach preventing catastrophic forgetting unlike recollection module require explicit storage patterns. instead learns generative experience model alongside main model. generative model produces pseudoexperiences combined batches real experiences training help network remember predict examples. since true labels pseudo-experiences assumed unavailable main model’s representation used create target label them. simple learning problems crude approximations real data randomly generated data appropriate distribution sufﬁcient. however complex problems like found computer vision highly structured high dimensional inputs reﬁned approximations needed stimulate network relevant representations. best knowledge ﬁrst consider variational autoencoders method creating pseudo-experiences. recent work considers problem generative lifelong learning variational autoencoder introducing modiﬁed training objective. potentially complementary contributions paper. deep generative adversarial networks also considered lifelong learning setting mechanism creating pseudo-experiences unfortunately continual learning gans remains significant research challenge gans known demonstrate instability even typical ofﬂine training. distilling knowledge using current data. view taken computer vision input generation challenging problem right side-stepped using data current task inputs prevent forgetting. demonstrated strategy works best inputs task task drawn similar distribution. unfortunately using current data creates large bias renders approach unsuitable truly non-stationary problems. contrast approach uses novel pseudo-experience generator module leverages episodic storage efﬁciently model distribution experiences encountered without introducing signiﬁcant bias. asynchronous learning methods. asynchronous methods entail multiple agents accumulating experiences like become popular fast wall clock time performing realistic continual learning perspective agent. also efﬁcient episodic storage based techniques terms number experiences needed achieve good performance shown biological inspiration comparisons. interestingly idea scalable experience storage biologically inspired motivation relating back pioneering work mcclelland hypothesized complementary dynamics hippocampus neocortex. theory hippocampus responsible fast learning providing plastic representation retaining short term memories. neocortex responsible reasoning would otherwise suffer result catastrophic forgetting hippocampus also plays role generating approximate recollections interleave incoming experiences stabilizing learning neocortex. approach follows hippocampal memory index theory approximating role hippocampus modern deep neural network model. theory suggests lifelong systems need mechanism pattern completion pattern separation theory literally store previous experiences rather compact experience indexes used retrieve experience association cortex modeled auto-encoder. generative knowledge distillation. goal storing recollections facilitate scalable knowledge distillation previously seen tasks learning task early work recognized value augmenting real data synthetic data previously seen tasks. additionally unlabelled data widely used knowledge distillation. generative models also used sole source distillation context language models general case separate input output generate example. achieving high quality purely generative distillation goal obtain form general purpose knowledge transfer. result work related motivation techniques look preserve knowledge transforming network architecture storing experiences. recent work continual lifelong learning deep neural networks focused resource constrained lifelong learning problem promote stable learning relatively small diversity prior experiences stored memory. work complete picture also considering relationship ﬁdelity prior experiences stored memory. achieve considering additional resource constraint number bits storage allowed experience. core approach architecture supports scalable storage retrieval experiences shown figure three primary components encoder index buffer decoder. experience received encoder compresses sequence latent codes codes concantenated compressed binary code index shown decimal ﬁgure. compressed code stored index buffer. path shown blue. experiences retrieved index buffer passing latent code decoder create approximate reconstruction original input. path shown ﬁgure. recollection module used many ways continual learning setting. algorithm show approach later experiments. module integrated state-of-the-art techniques like experience replay gradient episodic memory setting model must learn tasks sequentially dataset every step receives triplet representing input task label correct output. intuitively algorithm proceeds phases. ﬁrst phase ensure recollection module stabilized forgetting second stabilize predictive model recollection module consists memory buffer encoder decoder decψ. recollection module achieve stabilization novel extension experience replay incoming example received ﬁrst read multiple batches recollections index buffer using current decoder. perform steps optimization encoder/decoder parameters interleaving current example different batch past recollections step. ﬁrst step error recollections respect parameters used generate them. however proceed future steps recollections stabilize recollection module itself making sure forget reconstruct recollections past experiences. surprisingly show experiments strategy effective replaying real inputs cases. recollection module trained loss function learning rate predictive model trained recollection sample sets using loss function learning rate finally sample written index buffer. details train scalable recollections found algorithm appendix differences algorithms contained different ways utilizing episodic memory inherent lifelong learning algorithm. project function within solves quadratic program explained resource constrained setting study paper upper limit number allowable episodic memories experience replay maintain buffer using reservoir sampling. follow prior work keep equal number recent examples task. experience replay training compute main model gradients recollection sample sets update main model parameters encode recollection sample store index buffer demonstrate experiments assumption transfer learning beneﬁcial harmful buffer capable using smaller latent codes maintain ﬁxed memory reconstruction error time. allows algorithm achieve sub-linear scaling. assume non-stationary learning problem stationary features shared across time that example environment adversarial learner. safe assumption practical continual learning problems. role scalable recollections module efﬁciently facilitate transfer knowledge neural models. section argue recently proposed discrete latent variable variational autoencoders ideally suited implement encoder/decoder functionality recollection module. typical experience storage strategies store full experiences expensive. example store cifar images color channels -bits pixel channel incur cost image stored bits. deep non-linear autoencoders natural choice compression problems. autoencoder continuous latent variable size assuming standard -bit representations used modern hardware storage cost bits latent representation. unfortunately continuous variable autoencoders -bits network parameters incur unnecessary storage cost many problems especially constrained-resource settings. solution combines beneﬁt training ability explicitly control precision recently proposed categorical latent variables consider bottleneck representation encoder decoder categorical latent variables containing dimensions representing encoding categorical variable. compressed binary representation bits. order model autoencoder discrete latent variables follow success recent work employ gumbelsoftmax function. gumbel-softmax function leverages gumbel-max trick provides efﬁcient draw samples categorical distribution class probabilities representing output encoder equation sample drawn gumbel calculated drawing uniform computing gi=-log). function quantizes input vector. softmax function used differentiable approximation argmax generate d-dimensional sample vectors temperature which gumbel-softmax distribution smooth therefore well-deﬁned gradient respect parameters forward propagation categorical autoencoder send output encoder sampling procedure equation create categorical variable. however backpropagation replace non-differentiable categorical samples differentiable approximation training using gumbel-softmax estimator equation although past work found value varying training still able strong results keeping ﬁxed across experiments. across experiments generator model includes three convolutional layers encoder three deconvolutional layers decoder. figure empirically demonstrate autoencoders categorical latent variables achieve signiﬁcantly storage compression input observations average distortion autoencoders continuous variables. detail provided experiment appendix figure comparison relationship average reconstruction distance mnist training sample compression continuous latent variable categorical latent variable autoencoders. recollection module must provide means compressing storage experiences scalable also mechanism efﬁciently sampling recollections truly representative prior experiences. figure comparison generative transfer learning performance using teacher student model mnist using code sampling recollection module sampling. ﬁrst consider typical method sampling variational autoencoder refer code sampling latent variable selected randomly. obviously increasing capacity autoencoder able achieve lower reconstruction distortion. however interestingly increasing autoencoder capacity increases modeling power also increases chance randomly sampled latent code representative seen training distribution. instead maintain index buffer indexes associated prior experiences. call sampling index buffer buffer sampling. table shows comparison code buffer sampling different latent variable representation sizes. reconstruction distortion error reconstructing recollection using decoder. nearest neighbor distortion distance sampled code nearest neighbor training set. reconstruction distortion buffer approach yields significantly smaller nearest neighbor distortion. means buffer sampling produces representative sample simple code sampling. much matter practice? figure demonstrates utility knowledge distillation experiment. compare representation sizes approaches task distilling teacher model student model architecture latent codes. student trained reconstructed data using teacher output label. best learning curve obtained using buffer sampling. would like emphasize results byproduct increased model capacity associated buffer small table comparing nearest training example distance code sampling buffer sampling based recollections. report averages across random samples. reconstruction distortion autoencoder measured test inﬂuenced sampling strategy. representation buffer signiﬁcantly outperforms representation code sampling despite fewer total bits storage including model parameters buffer. consider types resource constrained settings experiments. incremental storage constraint view initial size system sunk cost isolate effect incremental scaling increasing number experiences. total storage constraint consider bits storage used part constraint. informative lifelong learning models perform settings given ﬁnite learning interval. perform experiments datasets include tasks training. previous work measure retention metric. performance tasks sequential training completed every task. mnist-rotations dataset considers task random rotation degrees degrees input space digit mnist. incremental cifar- multi-task split cifar- image recognition dataset considering course grained labels task trained sequence. also test recollection module omniglot character recognition dataset considering alphabets task. even challenging setting explored prior work containing tasks fewer examples class. model experiments resnet- model cifar omniglot well layer hidden units mnist-rotations. cifar- employed test efﬁcacy transfer learning cifar- since dataset represents images similar structure drawn disjoint labels. details found experiments appendix allow effective storage full experience less class. experience-storage-based solutions still perform well regime given signiﬁcant boost scalable recollection module. example achieve considerably improved performance results reported benchmark. note keeps buffer recent items computing fisher information. also note large incremental resource expense parameter fisher information storage task equal storing real buffer thousand examples. next turn incremental cifar-. setting poses signiﬁcant challenge scalable recollection module since cifar tiny image domain known particularly difﬁcult setting performance table consider performance incremental cifar- small incremental resource constraint including couple settings even less incremental memory allowance number classes. real storage performs relatively well number examples greater number classes otherwise suffers biased sampling towards subset classes. seen decreased performance small buffer sizes compared using buffer learning online. consistently tuning recollection module approximate recollections reasonably sized index buffer results improvements real storage incremental resource cost. validate ﬁndings tried omniglot dataset attempting learn continually difﬁcult incremental task setting. incremental resource constraint full examples replay achieves ﬁnal retention accuracy contrast recollection module achieves accuracy. incremental resource constraint full examples replay achieves accuracy improved accuracy taking three gradient descent steps example. recollection module achieves better performance accuracy step example accuracy three steps example. table retention results incremental cifar- effective buffer sizes incremental storage resource constraint. requires buffer sizes multiple initial acquisition. demonstrate empirically figure ﬁrst training models incremental cifar- training million training examples cifar-. number training examples seen cifar- examples seen cifar. recollection module allow experience replay generalize effectively real storage initial learning also retains knowledge much gracefully time. provide detailed chart figure appendix includes learning larger real storage buffer sizes comparison. example times larger real storage buffer loses knowledge signiﬁcantly faster scalable recollections despite better performance originally trained incremental cifar-. model replay real storage replay recollections transfer replay recollections cifar- transfer real storage recollections transfer recollections cifar- transfer icarl complex input space scratch. demonstrate table transfer learning provides solution problem. employing unlabeled background knowledge able perform much better onset small autoencoder. explore total resource constraint examples smallest explored demonstrate able achieve state results initializing autoencoder representation learned cifar-. figure appendix also demonstrate positive inﬂuence transfer learning lifelong training autoencoder drives efﬁciency gain see. cifar- drawn larger database cifar- non-overlapping. validate recollection module training procedure demonstrating recollections generated autoencoder model actually effective preventing catastrophic forgetting model. achieved literature leveraging difference model parameters across time scales relevance problem. fact recently successful strategies preventing catastrophic forgetting relied saving models speciﬁc prior task work explore strategy described section generic reliant human deﬁned task boundaries function correctly. shown figure continual learning cifar- effective incremental buffer size average items class recollection module similarly effective real storage stabilizing lifelong autoencoder. negative effects less effective synthetic examples apparently drowned positive effects larger diversity stored examples. another case recollection module distillation transfer knowledge teacher model student model. experiments train teacher model lenet convolutional neural network architecture popular mnist benchmark achieving accuracy test set. would like test whether recollections drawn proposed recollection module sufﬁcient input representations teacher neural network convey function seperate student neural network size. table validate effectiveness technique comparing episodic storage baselines interest. baselines consider training number randomly sampled real examples using real input teacher’s output vector target using random sampling select subset real examples store. training large number memories complete knowledge transfer recollection compression clearly shows dividens random sampling baselines. impressive particularly results stricter total storage resource constraint setting sample basis compression actually account autoencoder model capacity. also would like validate ﬁndings complex setting consider distillation outputs task resnet- teacher model gets accuracy omniglot. test performance million training episodes enough achieve teacher performance using real training examples. however sampling diversity restricts learning signiﬁcantly example achieving accuracy sampling sampling sampling. contrast recollection module much effective achieving accuracy total resource compression accuracy compression compression. tron student hidden layers hidden units. mlps recollection module comparable performance real examples using much less storage compression scales much better sampling real inputs. table empirically demonstrate simple heuristics discussed appendix able recollection module achieve better sample efﬁciency knowledge transfer random sampling real examples. interesting result implying teacher model paired recollection module mastered skill sample efﬁcient conveying knowledge student model humans labelling random unlabelled data. proposed experimentally validated general purpose scalable recollection module designed scale long time-frames. demonstrated superior performance state-of-the-art approaches lifelong learning using small incremental storage footprints. increases dramatically boosted unsupervised recollection module pre-training. demonstrated module self improving leading diminished incremental storage needs experiences. shown vaes categorical latent variables signiﬁcantly outperform continuous latent variables lossy compression. also shown maintaining explicit buffer capturing distribution previously seen samples generating realistic recollections needed effectively prevent forgetting. applications large problems online learning left future work. however extensions straightforward unlike techniques recollection module rely data presented sequence tasks scalable recollection module bridges existing generative models based pseudo-rehearsal strategies full experience storage techniques. shown capability easily used augment existing techniques achieve state-of-the-art performance much reduced cost. increasing importance applications start scale true lifelong autonomous learning. references maruan al-shedivat trapit bansal yuri burda ilya sutskever igor mordatch pieter abbeel. continuous adaptation meta-learning nonstationary competitive environments. arxiv preprint arxiv. yoshua bengio j´erˆome louradour ronan collobert proceedings jason weston. curriculum learning. annual international conference machine learning charles blundell benigno uria alexander pritzel yazhe avraham ruderman joel leibo jack daan wierstra demis hassabis. model-free episodic control. arxiv preprint arxiv. cristian bucilu rich caruana alexandru niculescumizil. model compression. proceedings sigkdd international conference knowledge discovery data mining chrisantha fernando dylan banarse charles blundell yori zwols david andrei rusu alexander pritzel daan wierstra. pathnet evolution channels gradient descent super neural networks. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems diederik kingma salimans rafal jozefowicz chen ilya sutskever welling. improved variational inference inverse autoregressive ﬂow. advances neural information processing systems james kirkpatrick razvan pascanu neil rabinowitz joel veness guillaume desjardins andrei rusu kieran milan john quan tiago ramalho agnieszka grabskabarwinska overcoming catastrophic forgetting neural networks. proceedings national academy sciences brenden lake ruslan salakhutdinov jason gross joshua tenenbaum. shot learning simple visual concepts. proceedings cognitive science society volume james mcclelland bruce mcnaughton randall o’reilly. complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory. psychological review volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous internamethods deep reinforcement learning. tional conference machine learning alexander pritzel benigno uria sriram srinivasan adri`a puigdom`enech badia oriol vinyals demis hassabis daan wierstra charles blundell. neural international conference maepisodic control. chine learning matthew riemer elham khabiri richard goodwin. representation stability regularizer improved text analytics transfer learning. arxiv preprint arxiv. andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv preprint arxiv. thrun. lifelong learning perspective mobile robot control. proceedings ieee/rsj/gi international conference intelligent robots systems volume cheng zhang hedvig kjellstrom stephan mandt. stochastic learning imbalanced data determinantal point processes mini-batch diversiﬁcation. arxiv preprint arxiv. convolutional layer kernel size vary size categorical latent variable across experiments turn model number ﬁlters convolutional layer keep number hidden variables consistent intermediate layers network. practice implies number ﬁlters layer equal cl/. note discrete autoencoder stochastic deterministic report stochastic pass data experimental trial. knowledge distillation experiments report average result runs. mnist omniglot follow prior work consider images channel -bits pixel. mnist omniglot images originally larger others found sampling effect performance models using learn. detail architecture used experiments provided categorical latent variables table continuous latent variables table architecture learning rate reporting option achieves best training distortion. distortion pixels normalized dividing take mean vector absolute value reconstruction real sample difference report mean samples training set. compression ratio size mnist image size latent variables assuming bits ﬂoating point numbers continuous case binary representation categorical variables. jpeg data points collected using pillow python package using quality subtracted header size form jpeg size relatively fair accounting compression large data images size. jpeg compression computed average ﬁrst mnist training images. recollections experiments used autoencoder variables size buffer variables size buffer. experience replay used learning rate predictive model learning rate autoencoder. size buffer experienced over-ﬁtting buffer used batch size size buffer used batch size categorical latent variable autoencoders following sizes incremental cifar- variables effective buffer size variables effective buffer size variables effective buffer size variables effective buffer size variables effective buffer size predictive model trained learning rate replay experiments experiments. learning rate autoencoder buffer size regardless lifelong learning model buffer size gem. autoencoder learning rate replay buffer sizes well buffer size memory strength parameter real storage experiments experiments recollection. experiments replay mnist-rotations able good results even training autoencoder online leveraging recollection buffer stabilize training. could seen improvements show cifar- using buffer stabilization. incremental omniglot learning rate effective buffer size experiments leveraged categorical latent variable autoencoder variables. effective buffer size experiments utilized categorical latent variables consisting variables. follow multi-task training testing splits omniglot established transfer learning experiments cifar- replay learning rate used resnet- reasoning model learning rate used discrete autoencoder generator. used learning rate resnet model learning rate autoencoder memory strength experiment without transfer learning instead used higher learning rate replay autoencoder. used learning rate autoencoder transfer learning experiments. provide detailed version figure main text figure includes learning larger real storage buffer sizes comparison. example times larger real storage buffer looses knowledge signiﬁcantly faster scalable recollections despite better performance originally training incremental cifar-. blue show online training model random initialization buffer orange show ofﬂine training random initialization full data storage trained iterations. predictably access unlimited storage tasks simultaneously means performance ofﬂine-random-full consistently better onlinerandom-nobuffer. demonstrate value transfer figure retention performance cifar- prolonged training cifar-. compare recollections full storage replay buffer strategies listed effective incremental buffer size. good representation online setting show green online model replay buffer representation initialized training iterations cifar- note onlinepretrain-nobuffer performs comparably best randomly initialized model access tasks simultaneously unlimited storage fact performs considerably better ﬁrst tasks number prior experiences much greater number experiences. improvements transfer learning substantial effect stabilizing achieve improvement retention accuracy real storage online model improvement initialization cifar-. alongside teacher model train variational autoencoder model discrete latent variables. model trained epochs. ﬁnal pass data forward propogate training example store latent code index buffer. buffer eventually grows size training complete index buffer used statistical basis sampling diverse recollections train student network. logical effective strategy training student model sample randomly buffer thus capture full distribution. distillation experiments setting learning rate reporting best result. found higher learning rate beneﬁcial setting number examples lower learning rate beneﬁcial setting larger number examples. categorical latent variable autoencoders explored following representation sizes variables compression variables compression variables compression. code sampling baselines used numpy random integer function generate discrete latent variable. learning rate resnet- reasoning model experiments. trained discrete autoencoder models following representation sizes variables size compression variables size compression variables size compression. follow multi-task training testing splits omniglot established first consider dynamics balancing resources simple setting incremental storage constraint incoming data without regard size model used compress decompress recollections. refer total storage constraint incoming examples average storage rate limit γ/n. deﬁne probability incoming example stored memory. thus expected number bits required example stored ρssbe assuming simple binary encoding. treat ﬁxed deﬁne following optimization procedure search combination maximizes capacity fulﬁlling incremental resource storage constraint yields approximate solution seen equation inherent trade-off diversity experiences store governed distortion achieved related capacity. optimal trade-off likely problem dependent. work takes ﬁrst step trying understand relationship. example demonstrate deep neural networks improved stabilization resource constrained settings allowing degree distortion. increased ability capture diversity data incremental resource constraint. ways incremental storage constraint setting described previous section rigorous setting comparing recollections selected subset full inputs. another important factor number parameters model used compression decompression. generally also degree function example experiments number hidden units layer used bottleneck layer. fully connected layers yields such revise equation setting rigorous comparing lossless inputs somewhat harsh restriction measure lifelong learning systems. assumed compression model’s parameters largely transferable across tasks. degree parameters viewed sunk cost standpoint continual learning. experiments also look transferring representations related tasks build greater understanding trade-off. random sampling buffer effective would like maximize efﬁciency distilling knowledge teacher model student model. motivates automated curriculum learning setting recently explored multi-task learning rather automated generative curriculum learning case. tried simple reinforcement learning solutions rewards based unsuccessful initial experiments difﬁculty navigating complex continuous action space. also tried active learning formulation proposed gans learn best latent code sample given time. limited success strategy well tends learn emphasize regions latent space optimize incorrectness longer capture distribution inputs. table generative knowledge distillation active diverse sampling experiments teacher student model mnist. real input baselines randomly sampled. designing generative sampling heuristics. inspired ﬁndings instead employ simple sampling heuristics design curriculum prototypical qualities like responsiveness student depth coverage. model responsiveness student active sampling focusing examples student good performance. randomly sample latent codes using recollection buffer choose difﬁcult current student backpropagation cheaply forward propagating student each. sampling recollection buffer able ensure chosen difﬁcult samples still representative training distribution. experiments sampling roughly equates sampling difﬁcult class student model point time. model depth coverage sampling bigger batch random examples adding ﬁltering step considering difﬁculty. would like perform diverse sampling promotes subset diversity ﬁlter examples examples. approach achieving determinantal point process recently proposed selecting diverse neural network mini-batches product inputs measure similarity recollections found achieve effective performance diverse sampling step. however follow process sampling based squared similarity matrix outlined appendix found squared similarity matrix equally effective determinant signiﬁcantly scalable large matrices. also experiments. algorithm trying landmark point maximizes determinant ﬁnding point minimizes squared similarities msss algorithm initially randomly chooses points dataset computes similarities sampled points subset selected randomly remaining data points. point smallest squared similarities picked next landmark data point. procedure repeated total landmark points picked.", "year": 2017}