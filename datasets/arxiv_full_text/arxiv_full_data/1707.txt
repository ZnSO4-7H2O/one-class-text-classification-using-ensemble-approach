{"title": "\"What is Relevant in a Text Document?\": An Interpretable Machine  Learning Approach", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.", "text": "what relevant text document? interpretable machine learning approach leila arras franziska horn gr´egoire montavon klaus-robert m¨uller∗ wojciech samek∗ machine learning group fraunhofer heinrich hertz institute berlin germany machine learning group technische universit¨at berlin berlin germany department brain cognitive engineering korea university seoul korea text documents described number abstract concepts semantic category writing style sentiment. machine learning models trained automatically documents abstract concepts allowing annotate large text collections could processed human lifetime. besides predicting text’s category accurately also highly desirable understand categorization process takes place. paper demonstrate understanding achieved tracing classiﬁcation decision back individual words using layer-wise relevance propagation recently developed technique explaining predictions complex non-linear classiﬁers. train word-based models convolutional neural network bag-of-words classiﬁer topic categorization task adapt method decompose predictions models onto words. resulting scores indicate much individual words contribute overall classiﬁcation decision. enables distill relevant information text documents without explicit semantic information extraction step. word-wise relevance scores generating novel vector-based document representations capture semantic information. based document vectors introduce measure model explanatory power show that although models perform similarly terms classiﬁcation accuracy latter exhibits higher level explainability makes comprehensible humans potentially useful applications. number real-world problems related text data studied framework natural language processing example problems include topic categorization sentiment analysis machine translation structured information extraction automatic summarization. overwhelming amount text data available internet various sources user-generated content digitized books methods automatically intelligently process large collections text documents high demand. several text applications machine learning models based global word statistics like tfidf linear classiﬁers known perform remarkably well e.g. unsupervised keyword extraction document classiﬁcation however recently neural network models based vector space representations words shown great beneﬁt large number tasks. trend initiated seminal work introduced word-based neural networks perform various tasks language modeling chunking named entity recognition semantic role labeling. number recent works also reﬁned basic neural network architecture incorporating useful structures convolution pooling parse tree hierarchies leading improvements model predictions. overall models permitted assign automatically accurately concepts entire documents sub-document levels like phrases; assigned information mined large scale. parallel techniques developed context image categorization explain predictions convolutional neural networks related models. techniques able associate prediction model meaningful pattern space input features perform decomposition onto input pixels model output paper make layer-wise relevance propagation technique already substantially tested various datasets models present work propose method identify words text document important explain category associated approach consists using classiﬁer predict categories accurately possible second step decompose prediction onto input domain thus assigning word document relevance score. model study word-embedding based convolutional neural network train text classiﬁcation task namely topic categorization newsgroup documents. second model consider classical bag-of-words support vector machine classiﬁer. relevances validated document level building document heatmap visualizations dataset level compiling representative words text category. also shown quantitatively better identiﬁes relevant words sensitivity analysis. novel generating vector-based document representations introduced veriﬁed document vectors present semantic regularities within original feature space akin word vector representations. measure model explanatory power proposed shown models neural network bow/svm classiﬁer although presenting similar classiﬁcation performance largely diﬀer terms explainability. work organized follows. section describe related work explaining classiﬁer decisions respect input space variables. section introduce neural network model document classiﬁcation well decomposition procedure associated predictions. describe relevance scores used identify important words documents introduce novel condensing semantical information text document single document vector. likewise section introduce baseline model document classiﬁcation well gradientbased alternative assigning relevance scores words. section deﬁne objective criteria evaluating word relevance scores well assessing model explanatory power. section introduce dataset experimental setup present results. finally section concludes work. explanation individual classiﬁcation decisions terms input variables studied variety machine learning classiﬁers additive classiﬁers kernel-based classiﬁers hierarchical networks model-agnostic methods explanations relying random sampling also proposed despite generality latter however incur additional computational cost need process whole sample provide single explanation. methods speciﬁc deep convolutional neural networks used computer vision authors proposed network propagation technique based deconvolutions reconstruct input image patterns linked particular feature activation prediction. work aimed revealing salient structures within images related speciﬁc class computing corresponding prediction score derivative respect input image. latter method reveals sensitivity classiﬁer decision local variation input image related sensitivity analysis contrast method corresponds full decomposition classiﬁer output current input image. based layer-wise conservation principle reveals parts input space either support speak speciﬁc classiﬁcation decision. note framework applied various models kernel support vector machines deep neural networks refer reader context neural networks text classiﬁcation proposed extract salient sentences text documents using loss gradient magnitudes. order validate pertinence sentences extracted neural network classiﬁer latter work proposed subsequently sentences input external classiﬁer compare resulting classiﬁcation performance random heuristic sentence selection. work also employs gradient magnitudes identify salient words within sentences analogously method proposed computer vision however analysis based qualitative interpretation saliency heatmaps exemplary sentences. addition heatmap visualizations provide classiﬁer-intrinsic quantitative validation word-level relevances. furthermore extend previous work adding bow/svm baseline experiments proposing criterion assessing model explanatory power. section describe method identifying words text document relevant respect given category classiﬁcation problem. this assume given vector-based word representation neural network already trained accurately documents actual category. method divided four steps compute input representation text document based word vectors. forward-propagate input representation convolutional neural network output reached. backward-propagate output network using layer-wise relevance propagation method input reached. pool relevance scores associated input variable network onto words belong. result four-step procedure decomposition prediction score category onto words documents obtained. decomposed terms called relevance scores. relevance scores viewed highlighted text used form list top-words document. whole procedure also described visually figure detail section method speciﬁc network architecture predeﬁned choices layers method principle extended architecture composed similar larger number layers. section introduce diﬀerent methods serve baselines comparison. baseline convolutional neural network model bow/svm classiﬁer procedure adapted accordingly baseline relevance decomposition procedure gradient-based sensitivity analysis technique assigns sensitivity scores individual words. vectorbased document representation experiments also compare uniform tfidf baselines. prior training neural network using prediction explanation ﬁrst derive numerical representation text documents serve input neural classiﬁer. individual word document vector embedding concatenate embeddings form matrix size number words document times dimension word embeddings. distributed representation words learned scratch ﬁne-tuned simultaneously classiﬁcation task interest. present work pre-training shown that even without ﬁne-tuning leads good neural network classiﬁcation performance variety tasks like e.g. natural language tagging sentiment analysis shallow neural network model learning word embeddings unlabeled text sources continuous bag-of-words model similar log-bilinear language model ignores order context words. cbow model objective predict target middle word average embeddings context words surrounding figure diagram cnn-based interpretable machine learning system consisting forward processing computes input document high-level concept redistribution procedure explains prediction terms words. middle word means direct products word embeddings. training word embeddings context words target words learned separately. training completed context word embeddings retained applications. cbow objective simple maximum likelihood formulation maximizes training data logarithm probabilities form softmax normalization runs words vocabulary number context words training text window represents target word position training data wt−nt+n represent corresponding context words. present work utilize pre-trained word embeddings obtained cbow architecture negative sampling training procedure refer embeddings wordvec embeddings. model classifying text documents word-embedding based convolutional neural network model similar proposed sentence classiﬁcation slight variant model introduced semantic role labeling. architecture depicted figure composed several layers. indicates position within text sequence designates feature delay range ﬁlter size one-dimensional convolutional operation convolutional operation yields features maps length apply relu nonlinearity element-wise. note trainable parameters depend position text document hence convolutional processing equivariant physical dimension. figure next layer computes dimension previous representation layer creates invariance position features document. finally pooled features endmost logistic classiﬁer unnormalized log-probability classes indexed variable given layer-wise relevance propagation recently introduced technique estimating elements classiﬁer input important achieve certain classiﬁcation decision. applied bag-of-words classiﬁers well layer-wise structured neural networks. every input data point possible target class delivers scalar relevance value input variable hereby indicating whether corresponding part input contributing speciﬁc classiﬁer decision input variable rather uninvolved irrelevant classiﬁcation task all. main idea behind redistribute possible target class separately output prediction score causes classiﬁcation back input space backward propagation procedure satisﬁes layer-wise conservation principle. thereby intermediate classiﬁer layer input layer gets allocated relevance values relevances layer equal classiﬁer prediction score considered class. denoting neurons layers presented previous section associate respectively relevance score accordingly layer-wise conservation principle written runs neurons given layer network. formalize redistribution process layer another introduce concept messages ra←b indicating much relevance circulates given neuron neuron next lower-layer. express b∈upper ra←b upper denotes upper-layer neurons connected bootstrap propagation algorithm top-layer relevance vector kronecker delta function target deﬁne lower-layer neurons proportions representing contribution neuron upper-layer neuron value forward propagation incremented small stabilizing term prevents denominator nearing zero hence avoids large positive negative relevance messages. winner-take-all redistribution analogous rule used training backpropagating gradients i.e. neuron maximum value pool granted relevance upper-layer neuron. finally convolutional layer weighted redistribution formula relevance redistributed onto individual components wordvec vector associated word form single input neuron relevances rit. obtain word-level relevance value pool relevances dimensions wordvec vector compute vector summary consists additive composition semantic representation relevant words document. note resulting document vector lies semantic space wordvec vectors. ﬁned-grained extraction technique apply word-level pooling intermediate step extracts relevant subspace word last approach particularly useful address problem word homonymy thus result even ﬁner semantic extraction document. remaining refer semantic extraction deﬁned word-level extraction element-wise extraction. cases call vector document summary vector. sensitivity analysis. sensitivity analysis assigns scores input variables representing steepness decision function input space. partial derivatives straightforward compute using standard gradient propagation readily available neural network implementations. hereby note sensitivity analysis redistributes quantity redistributes however local steepness information relatively weak proxy actual function value real quantity interest estimating contribution input variables w.r.t. current classiﬁer’s decision. note relevance scores obtained signed obtained positive. bow/svm. baseline model bag-of-words linear classiﬁer used predict document categories. model text document ﬁrst mapped vector dimensionality size training data vocabulary entry computed term frequency inverse document frequency score corresponding word. subsequently vectors normalized unit euclidean norm. second step using vector representations documents maximum margin separating hyperplanes learned separate classes classiﬁcation problem ones. result obtain class linear class speciﬁc weights bias. prediction score form order obtain decomposition prediction score class onto input variables simply compute bc/d number non-zero entries respectively sensitivity analysis redistribution prediction score squared gradient reduces note bow/svm model linear predictor relying directly word frequency statistics lacks expressive power comparison model additionally learns intermediate hidden layer representations convolutional ﬁlters. moreover model take advantage semantic similarity encoded distributed wordvec representations bow/svm model words equidistant bag-of-words semantic space. experiments show limitations lead bow/svm model sometimes identify spurious words relevant classiﬁcation task. analogy semantic extraction proposed section model build vectors representing documents leveraging word relevances obtained bow/svm model. uniform/tfidf based document summary vector. place word-level relevance resp. uniform weighting. corresponds build document vector average wordvec word embeddings ﬁrst case take document representation binary bag-of-words vector second case. moreover replace inverse document frequency score tfidf score. correspond tfidf weighting either wordvec vectors one-hot vectors representing words. section describe evaluate compare outcomes algorithms assign relevance scores words intrinsic validation. furthermore propose measure model explanatory power based extrinsic validation procedure. latter used analyze compare relevance decompositions explanations obtained neural network bow/svm classiﬁer. types evaluations carried section evaluation good method identiﬁes relevant words text documents performed qualitatively e.g. document level inspecting heatmap visualization document reviewing list relevant words document. similar analysis also conducted dataset level e.g. compiling list relevant words category across documents. latter allows identify words representatives document category eventually detect potential dataset biases classiﬁer speciﬁc drawbacks. however order quantitatively compare algorithms regarding identiﬁcation relevant words need objective measure quality explanations delivered relevance decomposition methods. adopt idea word considered highly relevant classiﬁcation document removing classifying modiﬁed document results strong decrease classiﬁcation score idea extended sequentially deleting words relevant least relevant round. result graph prediction scores function number deleted words. experiments employ approach track changes classiﬁcation performance successively deleting words according relevance value. comparing relative impact classiﬁcation performance induced diﬀerent relevance decomposition methods estimate appropriate methods identifying words really important classiﬁcation task hand. described procedure constitutes intrinsic validation rely external classiﬁer. although intrinsic validation used compare relevance decomposition methods given model approach suited compare explanatory power diﬀerent models since latter requires common evaluation basis. furthermore even would track classiﬁcation performance changes induced diﬀerent models using external classiﬁer would necessarily increase comparability removing words document aﬀect diﬀerent classiﬁers diﬀerently graphs comparable. therefore propose novel measure model explanatory power depend classiﬁcation performance change word relevances. hereby consider model explainable model word relevances semantic extractive i.e. helpful solving semantic related task classiﬁcation document summary vectors. normalize document summary vectors unit euclidean norm perform k-nearest-neighbors classiﬁcation half vectors using half summary vectors neighbors diﬀerent hyperparameters repeat step random data splits average classiﬁcation accuracies finally report maximum accuracy explanatory power index higher value explanatory power model corresponding document summary vectors have. proposed evaluation procedure common external classiﬁer enables unbiasedly objectively compare diﬀerent models terms density local neighborhood structure semantic information extracted summary vectors input feature space. indeed recall summary vectors constructed semantic space wordvec embeddings summary vectors obtained live bag-of-words space. section summarizes experimental results. ﬁrst describe dataset experimental setup training procedure classiﬁcation accuracy models. consider four models three cnns diﬀerent ﬁlter sizes bow/svm classiﬁer. then demonstrate used identify relevant words text documents. compare heatmaps best performing model bow/svm classiﬁer report representative words three exemplary document categories. results demonstrate qualitatively model produces better explanations bow/svm classiﬁer. move evaluation document summary vectors show projection document vectors computed scores groups documents according topics since worse results obtained using scores uniform tfidf weighting indicates explanations produced semantically meaningful latter. finally conﬁrm quantitatively observations made before namely decomposition method provides better explanations model outperforms bow/svm classiﬁer terms explanatory power. experiments consider topic categorization task employ freely available newsgroups dataset consisting newsgroup posts evenly distributed among twenty ﬁne-grained categories. precisely news-bydate version already partitioned training test documents corresponding diﬀerent periods time. ﬁrst preprocessing step remove headers documents tokenize text nltk. then ﬁlter tokenized data retaining tokens composed following four types characters alphabetic hyphen apostrophe containing newsgroups dataset available http//qwone.com/%ejason/newsgroups/ natural language toolkit available http//www.nltk.org lastly build neural network input horizontally concatenating pretrained word embeddings according sequence tokens appearing preprocessed document. particular take -dimensional freely available wordvec embeddings out-of-vocabulary words simply initialized zero vectors. input normalization subtract mean divide standard deviation obtained ﬂattened training data. train neural network minimizing cross-entropy loss mini-batch stochastic gradient descent using l-norm dropout regularization. tune model hyperparameters -fold cross-validation case employing random documents ﬁxed validation model. however hyperparameters perform extensive grid search stopped tuning obtained models reasonable classiﬁcation performance purpose experiments. table summarizes performance trained models. herein respectively denote neural networks convolutional ﬁlter size equal linear performs neural networks i.e. non-linear structure models yield considerable advantage toward classiﬁcation accuracy. similar results also reported previous studies observed document classiﬁcation convolutional neural network model starts outperform tfidf-based linear classiﬁer datasets order millions documents. explained fact topic categorization tasks diﬀerent categories separated linearly high-dimensional bag-of-words bag-of-n-grams space thanks suﬃciently disjoint sets features. figure compiles resulting heatmaps obtain exemplary sci.space test document correctly classiﬁed best performing neural network model cnn. note model relevance values computed bag-of-words feature i.e. words relevance irrespectively context document whereas classiﬁer visualize relevance value word position. hereby consider target class decomposition classes sci.space sci.med. observe model considers insigniﬁcant words like relevant target class sci.med time mistakenly estimates words like sickness mental distress negatively contributing class hand heatmap consistently sparse concentrated semantically meaningful words. sparsity property attributed maxpooling non-linearity feature neural network selects ﬁrst relevant feature occurs document. seen signiﬁcantly simpliﬁes interpretability results human. another disadvantage model relies entirely local global word statistics thus assign relevances proportionally tfidf features neural network model beneﬁts knowledge encoded wordvec embeddings. instance word weightlessness highlighted model target class sci.space word occur training data thus simply ignored classiﬁer. neural network however able detect attribute relevance unseen words thanks semantical information encoded pre-trained wordvec embeddings. dataset-wide analysis determine words identiﬁed constituting class representatives. purpose class target class relevance decomposition conduct test documents subsequently sort words appearing test data decreasing order obtained wordlevel relevance values retrieve thirty relevant ones. result list words identiﬁed highly supportive classiﬁer decision toward considered class. figures list relevant words diﬀerent target classes well corresponding word-level relevance values model. underlining indicate words occur training data. interestingly observe class-characteristical words identiﬁed neural network model correspond words even appear training data. contrast words simply ignored model occur bagof-words vocabulary. similarly previous heatmap visualizations class-speciﬁc analysis reveals classiﬁer occasionally assigns high relevances semantically insigniﬁcant words like example pronoun target class sci.med names henry nicho target class sci.space former case high relevance high term frequency word whereas latter case explained high inverse document frequency classbiased occurrence corresponding word training data contrary neural network model seems less aﬀected word counts regularities systematically attributes highest relevances words semantically related considered target class. results demonstrate that subjectively neural network better suited identify relevant words text documents bow/svm model. wordvec embeddings known exhibit linear regularities representing semantical relationships words explore whether regularities transferred document representation denoted document summary vector building vector weighted combination wordvec embeddings combination one-hot word vectors compare weighting scheme based relevances following baselines relevance tfidf uniform weighting two-dimensional projection summary vectors obtained resp. model well corresponding tfidf/uniform weighting baselines shown figure visualizations group newsgroups test documents top-level categories color document according true category model observe two-dimensional projection reveals clear-cut clustered structure using element-wise weighting semantic extraction regularity observed uniform tfidf weighting. figure heatmaps document sci.space model. positive relevance mapped negative blue. color opacity normalized maximum absolute relevance document. target class corresponding classiﬁcation prediction score indicated left. word-level weightings well element-wise weighting present also form bundled layout dense well-separated case element-wise lrp. model two-dimensional visualization summary vectors exhibits partly cross-shaped layout weighting particular structure observed tfidf uniform semantic extraction. analysis conﬁrms observations made last section namely neural network outperforms bow/svm classiﬁer terms explainability. figure furthermore suggests provides semantically meaningful semantic extraction baseline methods. next section conﬁrm observations quantitatively. figure projection summary vectors newsgroups test documents. lrp/sa based weightings computed using model’s predicted class colors denote true labels. order quantitatively validate hypothesis able identify words either support inhibit speciﬁc classiﬁer decision conduct several word-deleting experiments models using scores relevance indicator. speciﬁcally accordance word-level relevances delete sequence words document re-classify documents missing words report classiﬁcation accuracy function number deleted words. hereby word-level relevances computed original documents deleting experiments consider newsgroups test documents length greater equal tokens amounts test documents delete words. deleting word simply corresponding word embedding zero input. moreover order assess pertinence decomposition method opposed alternative relevance models additionally perform word deletions according word relevances well random deletion. latter case sample random sequence words document delete corresponding words successively document. repeat random sampling times report average results additionally perform biased random deletion sample among words comprised wordvec vocabulary ﬁrst deletion experiment start subset test documents initially correctly classiﬁed models successively delete words decreasing order lrp/sa wordlevel relevance. ﬁrst deletion experiment lrp/sa relevances computed true document class target class relevance decomposition. second experiment perform opposite evaluation. start subset initially falsely classiﬁed documents delete successively words increasing order relevance considering likewise true document class target class relevance computation. third experiment start initially falsely classiﬁed documents delete words decreasing order relevance considering classiﬁer’s initially predicted class target class relevance decomposition. figure summarizes resulting accuracies deleting words resp. input documents note report results bow/svm model focus comparison diﬀerent models. successive deleting either positiverelevant words decreasing order relevance negative-relevant words increasing order relevance conﬁrm extremal relevance values capture pertinent information respect classiﬁcation problem. indeed deletion experiments observe pregnant decrease resp. increase classiﬁcation accuracy using relevance model. additionally note contrast largely unable provide suitable information linking words speak speciﬁc classiﬁcation decision. instead appears lowest relevances likely identify words impact classiﬁer decision deletion scheme even less impact classiﬁcation performance random deletion deleting words increasing order relevance shown second deletion experiment. figure word deletion experiments model. lrp/sa target class either true document class words deleted decreasing resp. increasing order lrp/sa relevance else target class predicted class case words deleted decreasing order relevance. random deletion reported average runs. words initially correctly classiﬁed documents considering well relevance model shown ﬁrst deletion experiment. indicates networks greater ﬁlter sizes sensitive single word deletions presumably deletions meaning surrounding words becomes less obvious classiﬁer. also provides weak evidence that behave similarly learned ﬁlters focus isolated words additionally consider bigrams trigrams words results diﬀer model ﬁrst deletion experiment. order quantitatively evaluate compare models combination relevance decomposition explanation technique apply evaluation method described section compute accuracy external classiﬁer classiﬁcation document summary vectors experiments remove test documents empty contain word preprocessing maximum mean accuracy obtained varying number neighbors reported several models explanation techniques table table results averaged random data splits. semantic extraction method report explanatory power index corresponding maximum mean accuracy obtained varying number neighbors corresponding standard deviation multiple data splits hyperparameter maximum accuracy. pairwise comparing best based weighting schemes corresponding tfidf baseline result table element-wise weighted combinations wordvec vectors statistical signiﬁcantly better tfidf weighting word embeddings signiﬁcance level similarly bag-of-words space combination one-hot word vectors signiﬁcantly better corresponding tfidf document representation signiﬁcance level lastly best explanatory power index signiﬁcantly higher best based explanation signiﬁcance level figure provides consistently better semantic extraction baseline methods model higher explanatory power bow/svm classiﬁer since produces semantically meaningful summary vectors classiﬁcation. altogether good performance qualitatively well quantitatively element-wise combination wordvec embeddings according relevance illustrates usefulness extracting vector-based document representation presenting semantic neighborhood regularities feature space presume potential applications relevance information e.g. aggregating word representations sub-document representations like phrases sentences paragraphs. demonstrated qualitatively quantitatively constitutes useful tool ﬁnegrained analysis document level dataset-wide introspection across documents identify words important classiﬁer’s decision. knowledge enables broaden scope applications standard machine learning classiﬁers like support vector machines neural networks extending primary classiﬁcation result additional information linking classiﬁer’s decision back components input case words document. furthermore based relevance introduced condensing semantic information contained word embeddings document vector representation used nearest neighbors classiﬁcation leads better performance standard tfidf weighting word embeddings. resulting document vector basis measure model explanatory power proposed work semantic properties could beyond applications various visualization search tasks document similarity expressed product vectors. work ﬁrst step toward applying decomposition domain expect technique also suitable various types applications based neural network architectures character-based recurrent network classiﬁers types classiﬁcation problems generally could contribute design accurate eﬃcient classiﬁers inspecting leveraging input space relevances also analysis intermediate relevance values classiﬁer hidden layers. work supported german ministry education research berlin data center bbdc funding mark dfg. thanks partial funding national research foundation korea funded ministry education science technology program. correspondence addressed conceived theoretical framework conceived designed experiments performed experiments wrote manuscript revised manuscript figure design final drafting equally.", "year": 2016}