{"title": "Online Continuous Submodular Maximization", "tag": ["stat.ML", "cs.AI", "cs.DS", "cs.LG"], "abstract": "In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We show that it achieves a regret bound of $O(\\sqrt{T})$ (where $T$ is the horizon of the online optimization problem) against a $(1-1/e)$-approximation to the best feasible solution in hindsight. However, in many scenarios, only an unbiased estimate of the gradients are available. For such settings, we then propose an online stochastic gradient ascent algorithm that also achieves a regret bound of $O(\\sqrt{T})$ regret, albeit against a weaker $1/2$-approximation to the best feasible solution in hindsight. We also generalize our results to $\\gamma$-weakly submodular functions and prove the same sublinear regret bounds. Finally, we demonstrate the efficiency of our algorithms on a few problem instances, including non-convex/non-concave quadratic programs, multilinear extensions of submodular set functions, and D-optimal design.", "text": "paper consider online optimization process objective functions convex instead belong broad class continuous submodular functions. ﬁrst propose variant frank-wolfe algorithm access full gradient objective functions. show achieves regret bound regret albeit weaker /-approximation best feasible solution hindsight. also generalize results γ-weakly submodular functions prove sublinear regret bounds. finally demonstrate eﬃciency algorithms problem instances including non-convex/non-concave quadratic programs multilinear extensions submodular functions d-optimal design. past years data necessitated scalable machine learning techniques process unprecedentedly growing amount data including data generated users wearable devices monitoring sensors time practically impossible exact mathematical model data generating processes. thus optimization techniques applied data robust imperfect even fundamentally unavailable knowledge. robust approach optimization many ﬁelds including artiﬁcial intelligence statistics machine learning look optimization process learns experience aspects problem observed. framework formally known online optimization performed sequence consecutive rounds. round learner/algorithm choose action environment/adversary reveals reward function. goal minimize regret metric borrowed game theory measures diﬀerence accumulated reward received algorithm best ﬁxed action hindsight. objective functions concave feasible forms convex body problem extensively studied machine learning community name online convex optimization well known algorithm incurs regret worst case also several algorithms match lower bound online gradient descent regularized-follow-the-leader even though optimizing convex/concave functions done eﬃciently problems statistics artiﬁcial intelligence non-convex. examples include training deep neural networks learning latent variables non-negative matrix factorization bayesian inference clustering among many others. result burst recent research directly optimize functions. fact general np-hard compute global optimum non-convex function non-convex optimization algorithms focus ﬁnding local optimum. naturally online non-convex optimization needs deﬁne appropriate notion regret related convergence local optimum work consider rich subclass non-convex/non-concave reward functions called continuous submodular functions recently established oﬄine setting ﬁrst order methods provide tight approximation guarantees best knowledge work ﬁrst systematically studies online continuous submodular maximization problem provides no-regret guarantees along developing eﬃcient algorithms. contributions summary monotone continuous dr-submodular reward functions subject general convex body propose algorithms sublinear regret bounds depending side information available regarding gradients. furthermore function monotone partial order deﬁned lattice power equipped union intersection instance lattice. fact submodular functions lattice precisely submodular functions extensively studied past denote bounded unbounded integer lattices equipped entrywise maximum minimum construction corresponds submodular functions integer lattices dr-submodular functions. lattice equipped commutative associative binary operations connected absorption lattice deﬁne dr-submodular function revealed algorithm receives reward goal minimize regret typically deﬁned diﬀerence total award algorithm accumulated best ﬁxed decision hindsight. note even oﬄine setting maximizing monotone dr-submodular function subject convex constraint done approximately polynomial approximation ratio. deterministic setting full access gradients ft’s possible best polynomial-time approximation guarantee oﬄine setting using variant frank-wolfe algorithm unless contrast stochastic situations unbiased estimates gradients given best known approximation guarantee using stochastic gradient ascent. also known stochastic gradient ascent cannot achieve better approximation guarantee general regret. algorithm based frank-wolfe variant proposed maximizing monotone continuous dr-submodular functions idea meta-actions proposed unlike consider general convex body algorithms online algorithms. precise consider ﬁrst iteration ﬁrst objective function online optimization setting. note remains unknown algorithm commits choice. oﬄine setting could used frank-wolfe variant proposed iterations order maximize iteration would found oﬀ-the-shelf online linear maximization algorithm regularized-follow-theintend mimic. thus maximize unknown linear objective function online linear maximization problem simply function revealed algorithm knows linear objective function corresponding inner product vk∇f. simply feed online algorithm reward vk∇f. subsequent function repeat process. note rftl algorithm regret bounded eθ∼d] every continuous drsubmodular parameter sampled distribution eθ∼d]. instead stochastic terms provide unbiased estimates gradients. another disadvantage meta-frank-wolfe algorithm requires gradient queries function even prohibitive. subsection show online gradient ascent design algorithm sublinear regret robust stochastic gradients functions monotone continuous dr-submodular. first shown hassani direct usage unbiased estimates gradients frank-wolfe-type algorithms lead arbitrarily solutions context stochastic submodular maximization. happens non-vanishing variance gradient approximations. result techniques developed online optimization algorithm access unbiased estimates gradients handle stochastic noise gradient consider gradient ascent method. theorem show -regret online gradient ascent bounded γ-weakly dr-submodular functions. particular special case /-regret online gradient ascent bounded continuous dr-submodular functions. precise description online gradient ascent presented algorithm stochastic version presented algorithm random. objective function random samples points constraint selects maximizes would like emphasize random infeasible online setting since online algorithms make decisions objective function revealed. figure legends subﬁgures write meta-fw meta-frank-wolfe online gradient ascent surrga surrogate gradient ascent. results multilinear extension presented figs. present -regret versus number iterations fig. fig. shows -regret evolves non-convex/non-concave quadratic programming. figs. d-optimal experiment design problem. fig. shows -regret versus ﬁrst experiment consider sequence multilinear extensions weighted coverage functions recall functions concave lower bound. thus introduce another baseline surrogate gradient ascent uses supergradient ascent maximize concave lower bound fig. regret random surrogate gradient ascent uninﬂuenced stochastic gradient oracle since rely exact gradient original objective function. meta-frankwolfe online gradient ascent incur higher regret fig. fig. addition stochastic gradient oracle impact upon meta-frank-wolfe online gradient ascent. agrees theoretical guarantee online gradient ascent result states frank-wolfe-type algorithms robust stochastic noise gradient oracle. quadratic programming problems objective functions form linear equality and/or inequality constraints. matrix indeﬁnite objective function becomes non-convex non-concave. constructed linear inequality constraints entry rm×n sampled uniformly random addition require variable reside positive cuboid. formally constraint positive polytope ensure gradient non-negative −hu. without u)hx; loss generality assume constant term thus function result illustrated fig. observed uniformly number inequality constraints polytope shifted avoid since function undeﬁned fig. illustrate function value attained algorithms varies experiences iterations; ﬁxed experiments. observe meta-frank-wolfe outperforms baselines. addition meta-frank-wolfe achieves better performance step size second experiments show function values attained algorithms iteration ranging meta-frank-wolfe. recall number frank-wolfe steps meta-frank-wolfe. result presented fig. since parameter online gradient ascent regret online gradient ascent remains constant varies. regret meta-frank-wolfe reduced increases. agrees intuition frank-wolfe steps yield better performance. submodular functions. submodularity structural property often associated functions found far-reaching applications statistics artiﬁcial intelligence including active learning viral marketing network monitoring document corpus summarization crowd teaching feature selection interpreting deep neural networks however submodularity goes beyond functions extended continuous domains maximizing submodular function inherently related continuous relaxation multilinear extension example dr-submodular function. variant frank-wolfe algorithm called proximation optimum multilinear extension submodular function generally monotone smooth submodular function subject polytope also known ﬁnding better approximation guarantee impossible reasonable complexitytheoretic assumptions recently bian generalized line work hassani studied applicability gradient ascent algorithms stochastic continuous submodular maximization setting objective function deﬁned terms expectation. proved gradient methods achieve approximation guarantee monotone dr-submodular functions subject general convex body. also known gradient methods cannot achieve better guarantee general furthermore also shown continuous greedy algorithms robust stochastic settings provide arbitrarily poor solutions general even though focus paper mention continuous submodular minimization also studied recently online optimization. work online optimization considers convex concave functions. protocol online convex optimization ﬁrst deﬁned zinkevich inﬂuential paper proposed online gradient descent method regret bound. result later improved regret hazan showed strongly convex functions. kalai vempala developed another class algorithms termed follow-the-leader idea ﬁnding point minimizes accumulated objective functions revealed far. however simple situations regret grows linearly circumvent issue kalai vempala introduced random perturbation regularization proposed follow-the-perturbed-leader algorithm following early work addition shalev-shwartz singer abernethy designed regularized-follow-the-leader algorithm. comprehensive survey found regret strongly convex loss functions. furthermore showed algorithm conditional gradient algorithm online convex optimization problem polyhedral sets. single linear optimization step performed iteration algorithm achieves regret bound convex losses regret bound strongly convex losses. schapire proposed general methodology devising online learning algorithms based drifting-games analysis. hazan goes beyond convexity considered regret minimization repeated games non-convex loss functions. introduced objective termed local regret proposed online non-convex optimization algorithms achieve optimal guarantees objective. work contrast considers nonconvex objective functions approximately maximized. notion α-regret design algorithms compete best ﬁxed oﬄine approximate solution tight regret bounds. online submodular optimization. existing work considered online submodular optimization discrete domain. streeter golovin golovin proposed online optimization algorithms submodular functions cardinality matroid constraints respectively. work studies online submodular optimization continuous domains. point online algorithm proposed golovin relies multilinear continuous relaxation simply instance general class dr-submodular functions consider here. paper considered online optimization process objective functions continuous dr-submodular. proposed online optimization algorithms meta-frank-wolfe online gradient ascent no-regret guarantees. also evaluated performance algorithms practice. results", "year": 2018}