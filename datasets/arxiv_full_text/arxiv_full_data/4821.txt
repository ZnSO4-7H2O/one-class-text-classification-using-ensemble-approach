{"title": "On the Latent Variable Interpretation in Sum-Product Networks", "tag": ["cs.AI", "cs.LG", "62"], "abstract": "One of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent variables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and to efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables corresponding to the LVs' states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in SPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for introducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish the probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on these results, we find a sound derivation of the EM algorithm for SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm, when applied to selective SPNs, and in particular when applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets.", "text": "abstract—one central themes sum-product networks interpretation nodes marginalized latent variables interpretation yields increased syntactic semantic structure allows application algorithm efﬁciently perform inference. literature interpretation justiﬁed explicitly introducing indicator variables corresponding lvs’ states. however pointed paper approach conﬂict completeness condition spns fully specify probabilistic model. propose remedy problem modifying original approach introducing call augmentation. discuss conditional independencies augmented spns formally establish probabilistic interpretation sum-weights give interpretation augmented spns bayesian networks. based results sound derivation algorithm spns. furthermore viterbi-style algorithm proposed literature never proven correct. show indeed correct algorithm applied selective spns particular applied augmented spns. theoretical results conﬁrmed experiments synthetic data real-world datasets. distribution latent marginalized variable second interpretation interpretation yields syntactically well-structured model. example following interpretation clear draw samples ppxq using ancestral sampling. structure also semantic nature instance represents clustering class variable. furthermore interpretation allows application algorithm essentially maximum-likelihood learning missing data enables advanced bayesian techniques mixture models seen special case spns single node corresponds single generally spns arbitrarily many nodes corresponding leading hierarchically structured model. interpretation spns justiﬁed explicitly introducing model using so-called indicator variables corresponding lvs’ states. however shown paper justiﬁcation actually simplistic since potentially conﬂict completeness condition leading incompletely speciﬁed model. remedy propose augmentation additionally also introduces so-called twin nodes order completely specify model. investigate independency structure model resulting augmentation parallel local independence assertions bayesian networks allows deﬁne representation augmented spn. using interpretation differential approach augmented spns give sound derivation algorithm spns. abilistic model combining domains deep learning graphical models main advantages many interesting inference scenarios expressed single forward and/or backward passes i.e. inference scenarios computational cost linear spn’s representation size. spns shown convincing performance applications image completion computer vision classiﬁcation speech language modeling since proposition central themes spns interpretation hierarchically structured latent variable models. essentially approach interpretation mixture models. consider example gaussian mixture model components random variables engineering university washington. e-mail rcgcs.washington.edu pedrodcs.washington.edu pernkopf signal processing speech communication graz university technology. e-mail pernkopftugraz.at i.e. ﬁnding probability maximizing assignment rvs. using results form ﬁrst point problem generally np-hard spns. proposed solution found efﬁciently maximizing model lvs. proposed algorithm replaces nodes nodes recovers solution using viterbistyle backtracking. however shown algorithm delivers correct solution. paper show algorithm indeed correct applied selective spns particular since augmented spns selective algorithm obtains solution augmented spns. however applied non-augmented spns algorithm still returns solution augmented implicitly assumes weights twin sums deterministic i.e. except single leads phenomenon inference call low-depth bias i.e. shallow parts preferred backtracking. main contribution paper provide sound theoretical foundation interpretation spns related concepts i.e. algorithm inference. theoretical ﬁndings conﬁrmed experiments synthetic data real-world datasets. paper organized follows remainder section introduce notation review spns discuss related work. section propose augmentation spns show soundness hierarchical model give interpretation furthermore discuss independency properties augmented spns interpretation sum-weights conditional probabilities. algorithm spns derived section section discuss inference spns. experiments presented section section concludes paper. proofs theoretical ﬁndings deferred appendix. denoted upper-case letters values denoted valpxq corresponding lower-case letters denote elements valpxq e.g. element valpxq. sets denoted boldface letters valpxnq corresponding lower-case boldface letters elements valpxq e.g. element valpxq. subset xrys denotes projection onto elements valpxq interpreted complete evidence assigning ﬁxed value. partial evidence represented subset valpxq element sigma-algebra induced valpxqu being borel-sets discrete choice yields power-set valpxq. example partial evidence discrete valpxq represents evidence takes states real-valued represents evidence takes value smaller formally speaking partial evidence used express domain marginalization maximization particular evidence elements denoted using boldface notation e.g. deﬁne txrys furthermore symbolize combination complete partial evidence i.e. complete evidence partial evidence xzx. given node directed graph chpnq papnq children parents respectively. furthermore descpnq descendants recursively deﬁned containing child descendant. similarly deﬁne ancpnq ancestors recursively deﬁned containing parent ancestor. spns deﬁned follows. deﬁnition sum-product network tuple connected rooted acyclic directed graph non-negative parameters. graph contains three types nodes distributions sums products. leaves distributions internal nodes either sums products. distribution node valpyq distribution function subset i.e. either mixed distribution function node computes non-negative weight associated edge contains weights outgoing sum-edges. product node sets spsq ppsq contain nodes product nodes respectively. symbols nodes spns denotes distribution denotes denotes product. symbols denote generic nodes indicate child parent relationship another node respectively. distribution deﬁned normalized output i.e. pspxqspxq. node deﬁne sub-spn rooted deﬁned graph induced descendants corresponding parameters. inference unconstrained spns generally intractable. however efﬁcient inference spns enabled structural constraints completeness decomposability complete sums holds shown integrating spxq arbitrary sets i.e. marginalization reduces corresponding integrals input distributions evaluating sums products usual way. property known validity spns efﬁcient inference. paper consider complete decomposable spns. without loss generality assume locally normalized sum-weights i.e. ﬁnitely many states so-called indicator variables input distributions ﬁnitestate state valpxq introduce λxxpxq assigning probability mass complete decomposable represents network polynomial used differential approach inference assume evidence evaluated spn. derivatives function respect details) yield representing inference scenario modiﬁed evidence i.e. evidence modiﬁed computationally attractive feature differential approach evaluated valpxq simultaneously using single back-propagation pass evidence evaluated. similarly second derivatives otherwise. furthermore differential approach generalized spns arbitrary input distributions i.e. spns countably inﬁnite uncountably many states details). spns related negation normal forms potential deep network representation propositional theories like spns structural constraints nnfs enable certain polynomial-time queries represented theory. particular notions smoothness decomposability determinism nnfs translate notions completeness decomposability selectivity spns respectively. work nnfs concept network polynomials multilinear representation ﬁnitely many states cast intermediate d-dnnf representation order generate arithmetic circuit representing network polynomial. restricted sums products equivalent spns slightly different syntax. learned optimizing objective trading log-likelihood training inference cost measured worst-case number arithmetic operations required inference learned models still represent context-speciﬁc independencies similar approach learning markov networks represented followed spns ﬁrst time proposed represented distribution deﬁned background graphical model more directly normalized output network. work spns applied image data generic architecture reminiscent convolutional neural networks proposed. structure learning algorithms restricted image domain proposed discriminative learning spns optimizing conditional likelihood proposed furthermore growing body literature theoretical aspects spns relationship types probabilistic models. families functions identiﬁed efﬁciently representable deep shallow spns considered shallow three layers. shown spns w.l.o.g. assumed locally normalized notion consistency allow exponentially compact models decomposability. results independently found furthermore sound derivation inference mechanisms generalized spns given i.e. spns inﬁnitely many states. representation spns found associated nodes model organized layer bipartite structure. actual structure captured structured conditional probability tables using algebraic decision diagrams. recently notion spns generalized sumproduct functions arbitrary semirings yields general unifying framework learning inference subsuming among others spns probabilistic modeling nnfs logical propositions function representations integration optimization. pointed node interpreted marginalized similar example section node postulates discrete whose states correspond children state product introduced children switched on/off corresponding illustrated fig. fig. still computes value fig. since setting corresponds marginalizing interpreted latent marginalized however regard larger structural context fig. recognize justiﬁcation actually simplistic. explicitly introducing renders ancestor incomplete descendant thus scope note setting incomplete generally correspond graphical representations spns depicted nodes containing small circle general distributions nodes containing gaussian-like products nodes symbols. empty nodes arbitrary type. deﬁnition twin-weights result algorithm augmentspn shown fig. called augmented denoted augpsq. within context called former child introduced product node λzsk respectively. node introduced called twin node respect denote original spn. steps augmentspn introduce links interconnected node child. link single parent namely simply copies former child steps introduce corresponding associated proposed fig. discussion above render nodes incomplete. sums clearly conditioning sums scpsq. thus necessary introduce twin node steps treat problem. following proposition states soundness augmentation. fig. problems occurring introduced. excerpt containing corresponding introducing renders incomplete assuming descpnq. remedy extending further introducing twin node marginalization. furthermore note also corresponds know probability distribution state corresponding namely weights know distribution state corresponding intuitively recognize state irrelevant case since inﬂuence resulting distribution model nevertheless probabilistic model completely speciﬁed unsatisfying. remedy problems shown fig. introduce twin node whose children corresponding twin connected child additional product node interconnected since product node scope scpnq rendered complete now. furthermore takes state corresponding speciﬁed conditional distribution namely weights twin node. clearly given network depicted fig. still computes function network fig. since constantly outputs long normalized weights weights used twin node basically assume arbitrary normalized weights cause constantly output where however natural choice would uniform weights although choice weights crucial evaluating evidence plays role inference section formalize explicit introduction denoted augmentation. spsq assume arbitrary ﬁxed ordering children chpsq |chpsq|. probability space valpzsq state corresponds child call associated sets nodes deﬁne distinguish refer former model rvs. node deﬁne ancestors/descendants proposition states marginal distribution augmented distribution represented original completely speciﬁed probabilistic model thus augmentation provides sound generalize interpretation mixture models general spns. example augmentation shown fig. note understand augmentation mainly theoretical tool establish work interpretation spns. cases neither necessary advisable explicitly construct augmented spn. interesting question sizes original augmented relate other. lower bound ωp|s|q holding e.g. spns single node. asymptotic upper bound op|s|q. this note introduction links twin sums cause linear increase spn’s size. number edges introduced connecting twins links conditioning sums bounded since number twins links bounded |s|. therefore op|s|q. asymptotic upper bound indeed achieved certain types spns consider e.g. chain consisting nodes distribution nodes. parent distribution parent last distributions. preceding sums conditioning sums yielding introduced edges i.e. case indeed grows quadratically |s|. intuitively conﬁgured isolates computational structure selected edges survive conﬁgured equipped weights augmented spn. therefore conﬁgured general locally normalized. note following properties conﬁgured spns. fig. augmentation spn. example containing nodes augmented containing corresponding links twin nodes nodes introduced augmentation smaller circles used. contained scope corresponding node. model unconnected among other respectively. constrain twin-weights equal sum-weights becomes independent special choice twin weights effectively removes edges recovering structure next section augmented interpretation derive algorithm spns. i.e. independent given show sum-weights conditional distribution conditioned event select path problem original interpretation conditional distribution speciﬁed complementary event. here show twin-weights precisely conditional distribution. requires event select path twin indeed complementary event select path shown following lemma. using result deﬁne representing augmented follows node connect parents scpsq children obtain representation augmented serving useful tool understand spns context probabilistic graphical models. example interpretation shown fig. algorithm general scheme maximum likelihood learning complete evidence missing thus augmented spns amenable associated nodes. moreover twin-weights kept ﬁxed applied augmented spns actually optimizes weights original spn. approach already pointed suggested evidence marginal posteriors given ppzs eqwsck bspeq used updates. updates however cannot correct ones actually leave weights unchanged. here using augmented spns formally derive standard updates sum-weights input distributions chosen exponential family. updates weights assume dataset tepq eplqu i.i.d. samples eplq combination complete partial evidence model section zspsq consider arbitrary node shows weights interpreted conditional probabilities interpretation followed renormalization. make event explicit introducing switching parent twin exists assumes states valpysq y¯su twin exist takes single value valpysq tysu. clearly observed renders independent switching parent explicitly introduced augmented depicted fig. simply introduce λysys λysy¯s switch on/off output respectively. easy constantly i.e. marginalized augmented performs exactly computations before. furthermore easy completeness decomposability augmented maintained fig. explicitly introducing switching parent augmented spn. part augmented containing node three children twin. explicitly introduced switching parent using λysys λysy¯s simplicity derive updates univariate input distributions i.e. distributions |scpdyq| similar updates rather easily derived also multivariate input distributions. so-called distribution selectors introduced derive differential approach generalized spns. similar switching parents nodes render respective model independent remaining rvs. formally input distributions scope txu. assume arbitrary ﬁxed ordering rdxs index ordering. discrete states. so-called gated obtained replacing distribution product node introduced product denoted gate. shown rendered independent conditioned moreover conditional distribution given therefore incorporated family interpretation. input distribution chosen exponential family natural parameters m-step given expected sufﬁcient statistics depending type evaluating less demanding. simple practical case gaussian interval permitting closed form solution integrating gaussian’s statistics θpxq using truncated gaussians algorithm spns sum-weights input distributions summarized fig. section empirically verify derivation show standard successfully trains spns suitable structure hand. note recently zhao poupart derived concave-convex procedure yield sum-weight updates algorithm presented result surprising cccp rather different approaches general. i.e. generally np-hard inherently harder using result follows inference np-hard also spns. particular theorem shows decision version np-complete naive bayes model class variable marginalized. naive bayes represented augmentation single node representing class variable. therefore spns generally np-hard. since augmented representing naive bayes model corresponds inference original i.e. mixture model follows also inference generally np-hard spns. proof tailored spns found theorem shows maximizes probability corresponding selective spn. proof also shows actually maximizing assignment. product maximizing assignment given combining maximizing assignments children. maximizing assignment given maximizing assignment single child whose weighted maximum maximal among children. children’s maxima readily given upwards pass mpn. thus ﬁnding maximizing assignment node selective recursively reduces ﬁnding maximizing assignments children node; accomplished viterbi-like backtracking procedure. algorithm denoted mpeselective shown fig. denotes queue nodes denote en-queue de-queue operations respectively. note theorem already derived special case namely arithmetic circuits representing network polynomials discrete direct corollary theorem inference tractable augmented spns since augmented spns selective spns easily seen augmentspn exactly causing child non-zero. therefore mpeselective augmented spns order solution model lvs. note solution augmented general correspond solution original discarding states lvs. however procedure frequently used approximation models tractable model model alone. solution model lvs. states assigned max-backtracking sumchildren states one-to-one correspondence. states whose sums visited backtracking assigned again causes confusion since appear undeﬁned contexts illustrations section however since algorithm used approximation model discarding states situation paid attention. nevertheless show here applying mpeselective original spns effectively simulates mpeselective corresponding augmented spn. thereby however deterministic twin-weights implicitly assumed i.e. twin-weights except single this modify mpeselective applied original returning solution corresponding augmented spn. first note augmented every twin node simply outputs maximal twin-weight among children whose states contained evidence twin node maximal weight denoted ˆw¯s. effect twin nodes simulated original replacing weight original ˜wsc. ˜wsc product runs twins sums conditioning sum. using corrected weights node corresponding gets input augmented i.e. twin nodes simulated. identify maximizing states whose sums visited backtracking states sums visited given child correspond maximal twin-weight w¯s. pseudo-code somewhat technical modiﬁcation mpeselective found fig. low-depth bias using structure introduced augmentation depicted small nodes edges. deterministic twin-weights used state corresponding preferred since probabilities dampened weights respectively. deterministic. therefore although model completely speciﬁed shown viterbilike algorithm recovers solution nevertheless corresponds inference augmented special twin-weights i.e. deterministic weights. however using deterministic twin-weights rather unnatural choice since prefers arbitrary state others cases actually rendered irrelevant. case inference also bias towards less structured sub-models call lowdepth bias. illustrated fig. shows three augmented twin nodes corresponding respectively. twin-weights deterministic selection state biased towards state corresponding distribution assuming independence among comes fact values dampened weights respectively generally smaller therefore using deterministic weights twin nodes introduce bias towards selection subspns less deep less structured. using uniform weights twin nodes somewhat fairer since case gets dampened uniform weights extend opposite choice deterministic twin-weights former represent strongest possible dampening twin-weights therefore actually penalize less structured distributions. investigating effects subject future work. fig. normalized log-likelihood em-iterations averaged datasets random initializations. training set. test set; curves outside displayed region better readability curves. start approximately nats decreased approximately nats. training i.e. derived algorithm showed monotonicity experiments. moreover seen fig. training log-likelihood actually increased iterations. curves missing data scenarios similar. gives afﬁrmative evidence question fig. shows log-likelihood test set. note optimizing parameter sets severe overﬁtting achieving extremely high likelihoods training achieved extremely poor likelihoods test set. also parameter sets tend overﬁt although strong regarding question closer inspected test loglikelihood original parameters used initialization i.e. parameters obtained post-trained using table summarizes results. parameter sets including gaussian variances optimized test log-likelihood increased time i.e. datasets. furthermore oracle knowledge ideal number iterations average log-likelihood increased relative original parameters. improvement happens ﬁrst iteration yielding improvement. results indicate parameters obtained slightly underﬁt given datasets. similar fig. parameter sets including gaussian variances spns applied image data generic architecture reminiscent convolutional neural networks proposed. refer architecture architecture. standard used experiments reasons first explicitly constructing proposed structure train standard hardly possible current hardware since number nodes grows oplq square-length modeled image domain pixels instead sparse hard algorithm used virtualizes structure i.e. products generated details). second using standard seemed unsuited train large dense spns either trapped local optima gradient vanishing phenomenon. used datasets structures obtainable datasets comprise caltech- face images i.e. total datasets. input distributions spns single-dimensional gaussians means averages -quantiles variances constantly iterations various settings update combination three different types parameters i.e. sum-weights gaussian means gaussian variances. parameters types encoded string letters original parameters initialization obtained random initialization sumweights drawn dirichlet distribution uniform hyper-parameter gaussian means uniformly drawn gaussian variances parameters actually updated initialized randomly; otherwise original parameters used kept ﬁxed. thus total times yielding em-iterations. avoid pathological solutions used lower bound gaussian variances. iteration observed decreasing likelihood log-likelihoods sum-weights trained using random initialization. percentage data sets log-likelihood larger original parameters. pos. neg. relative log-likelihood w.r.t. original parameters data sets data sets wmv) prone overﬁtting datasets decreased test log-likelihood however remaining datasets test loglikelihood could improved substantially least average. turn question pointed above hard variant used time ﬁnds effective structure. optimizing using random initialization amounts using oracle structure obtained discarding learned parameters. dataset selected random initialization yielded highest likelihood training iteration compared log-likelihoods log-likelihoods obtained original parameters. results summarized table data sets log-likelihood training larger original parameters. also case individual random start every random restart always yielded higher training log-likelihood original parameters. thus considering actual optimization objective likelihood training successfully trains spns given suited oracle structure. furthermore seen table also prone overﬁtting algorithm datasets delivered higher test log-likelihood original parameters using oracle knowledge ideal number iterations illustrate correctness mpeselective applied augmented spns generated spns using architecture arranging binary grid respectively. inputs used indicator variables representing states. sum-weights drawn dirichlet distribution uniform α-parameters networks drew independent parameters sets. mpeselective augmented equipped uniform twin-weights deterministic twin-weights. uniform twin-weights denote result obtained mpeselective mpeuni. deterministic twin-weights denote result mpedet. described section mpedet corresponds essentially result mpeselective applied original assignment log-likelihoods evaluated augmented deterministic weights augmented uniform weights original additionally found ground truth assignments augmented spns original using exhaustive enumeration. results relative ground truth solutions shown tables seen mpeuni always ﬁnds solution augmented uniform twin-weights mpedet always ﬁnds solution augmented spns deterministic twin-weights. gives empirical evidence correctness mpeselective inference augmented spns. furthermore wanted investigate quality algorithms serving approximation inference original spns. spns considered here mpedet delivered average slightly better approximations mpeuni. however results interpreted caution rather similar nature distributions considered here. closer investigating approximate spns interesting direction subject future research. paper revisited interpretation spns hierarchically structured model. pointed original approach explicitly incorporate produce sound probabilistic model. remedy proposed augmentation spns proved soundness model. within augmented spns investigated independency structure represented showed sum-weights interpreted structured cpts within using augmented spns derived algorithm sum-weights singledimensional input distributions exponential families. mpe-inference generally np-hard spns showed viterbi-style backtracking algorithm recovers solution selective spns particular augmented spns. experiments give empirical evidence supporting theoretical results. furthermore showed standard successfully train generative spns given suitable network structure hand. complete decomposable spxq spxq immediate computing spxq valpxq done marginalizing i.e. setting λzsk case easy none structural changes modiﬁes output i.e. outputs agree i.e. spxq spxq. remains show complete decomposable root’s scope steps augmentspn introduce links representing private copies sum’s children clearly leave complete decomposable. steps introduced scope thus scope root. since done nodes introduced root’s scope. steps cannot render products nondecomposable since would imply reachable distinct children product contradiction fact decomposable before. however shown fig. steps render ancestor sums incomplete. treated steps twin introduced clearly complete scope tzu. furthermore incompleteness conditioning caused links scope. scope links augmented step deleting links scopes remains same since complete left child. thus also scope ancestor remains same. graph rooted acyclic since root cannot link deleting nodes edges cannot introduce cycles. deleted also link deleted internal nodes left leaves. roots same point xyyyz scope root. also complete decomposable whenever link deleted corresponding node twin node remain trivially complete since left single child. furthermore completeness decomposability ancestor left intact since neither changes scope. according point scope since scpnq disconnected deleted links descendants i.e. descendants disconnected conﬁguration. since present must still reachable root. therefore also descendants reachable i.e. input ﬁxed links deleted conﬁgured evaluate zero augmented outputs sums twin sums therefore therefore also output nodes remains same. includes root therefore proof lemma must contain either since scope root proposition show denote paths length root node scpnq. paths constructed extending path child path’s last node scope. smallest number path containing induction step show given |πk´| also |πk| nk´q single path πk´. product node single child scpcq decomposability. node must ancspsqztsu therefore single child conﬁgured spn. therefore single extend path therefore |πk| single path either lead since descp¯sq descpsq contains single path them both. want compute spzs i.e. marginalize according proposition equals zpzs according proposition sub-spn rooted former child since locally normalized subspn also locally normalized since scope former child sub-set marginalized λzsk link outputs since λzsk outputs slight abuse notation actually suprema sets supc deﬁne supremum empty used fact support node partitioned supports children selective sums whenever single child prove theorem using inductive argument. theorem clearly holds deﬁnition. consider product assume theorem holds chpˆpq. theorem also holds since would like thank anonymous reviewers constructive comments. work supported austrian science fund austrian science fund p-n. research partly funded grant n--- afrl contract fa---. z¨ohrer peharz pernkopf representation learning single-channel source separation bandwidth extension ieee/acm transactions audio speech language processing vol. trapp peharz skowron madl pernkopf trappl structure inference sum-product networks using inﬁnite sum-product trees nips workshop practical bayesian nonparametrics robert peharz received degree computer engineering ph.d degree electrical engineering graz university technology. main research interest lies machine learning particular probabilistic modeling applications signal processing speech audio processing computer vision. currently research medical university graz applying machine learning techniques detect early markers neurological conditions infants. funded biotechmed-graz cooperation interdisciplinary network major universities graz focus basic bio-medical research technological development medical applications. robert gens received s.b. degree electrical engineering computer science massachusetts institute technology cambridge m.sc. degree computer science engineering university washington seattle summer research intern microsoft research redmond usa. currently ph.d. student computer science engineering university washington seattle usa. supported google ph.d. fellowship deep learning. gens recipient outstanding student paper award neural information processing systems conference franz pernkopf received degree electrical engineering graz university technology austria summer earned ph.d degree university leoben austria awarded erwin schr¨odinger fellowship. research associate department electrical engineering university washington seattle currently associate professor laboratory signal processing speech communication graz university technology austria. research interests include machine learning discriminative learning graphical models feature selection ﬁnite mixture models imagespeech processing applications. fei-fei fergus perona learning generative visual models training examples incremental bayesian approach tested object categories computer vision image understanding vol. pedro domingos professor computer science university washington author master algorithm. winner sigkdd innovation award highest honor data science. fellow association advancement artiﬁcial intelligence received fulbright scholarship sloan fellowship national science foundations career award numerous best paper awards. received ph.d. university california irvine author co-author technical publications. held visiting positions stanford carnegie mellon mit. co-founded international machine learning society research spans wide variety topics machine learning artiﬁcial intelligence data science including scaling learning algorithms data maximizing word mouth social networks unifying logic probability deep learning.", "year": 2016}