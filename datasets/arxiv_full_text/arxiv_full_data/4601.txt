{"title": "Discovering the Hidden Structure of Complex Dynamic Systems", "tag": ["cs.AI", "cs.LG"], "abstract": "Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no expert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial computational aspects of learning the structure of dynamic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the Structural Expectation Maximization (SEM) algorithm. The main computational cost of the SEM algorithm is the gathering of expected sufficient statistics. We propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden variable typically results in a violation of the Markov property. Thus, our algorithm searches for such violations in the data, and introduces hidden variables to explain them. We provide empirical results showing that the algorithm is able to learn the dynamics of complex systems in a computationally tractable way.", "text": "dynamic bayesian networks provide compact natural representation complex dynamic systems. however many cases pert available learning provides alternative constructing paper address crucial compu­ tational aspects learning structure namic systems particularly relevant variables partially entirely structural maximization gorithm. main computational algorithm gathering ficient statistics. tion scheme allows sufficient statistics computed efficiently. also investigate fundamental tence hidden variables expensive observation hidden variable typically markov property. thus algorithm searches violations troduces hidden variables provide empirical results rithm able learn dynamics complex systems computationally real world complex systems tured namic bayesian tational language modeling tems. representing variables capture system dynamics external less structured models dbns inference robust eters. recent work made significant learning networks example cently presence using structurol rithm fluences additional ences possibly well. thus extra variable account paper extend technique structure learning. parametric denote random variable attribute represents nite) trajectories distribution assumptions. future precisely pendence sumption specifying second assumption process stationary i.e. bilistic fragments. specifies distribution second transition network transition transition nodes xnxi resents represents network ents) bility graph conditional specifies state another defined value variables consider simplicity single section observable. learning friedman learning briefly ysis. given find network tion best match defined using scoring scoring several literature. bayesian information bayesian gradually structure adding mations occasional plateaux. greatly network term determines ents particular family affects quence family determine traditionally adjusting friedman's tends structure rithm e-step computing expected structure parameters puted according candidate pletes performs complete-data inner loop. number steps stops structure complete network friedman rules network must higher score original. true even though expected ating structure structure. main difficulty tions longer consequence separate components ilies. problem rithm executes current \"filling ues. m-step real maximum likelihood precisely score expectation completions completion scores make change network expected least much. crucial score that like score complete composes expected expected states case structure ability re-evaluate changes restrict clearly shows discussion ability ment applying pected statistics. sufficient probabilistic inference jectory distribution affected typically unfortunately toriously expensive point paper learning message straightforward rithm essentially small dbns obtain joint posterior extract cliques straightforward approach compute performing tailored expensive \"distant\" performed statistic serious seek statistics search violations erty. case contains slices almost propose spirit boyen koller individual marginalizing able present. process form marginalization tion requires cient statistics approach events extract example event could extract multiply marginals pected event course would lose correlations. uniformly sufficient lated reveal thus evidence give information correlations lose correlation. hard learn general proximation). indeed enough information able ability distribution tested better-understood mation. ated long sequence network estimate attempted data given correct sions experiment message approximation seen table approximation degrade approximation could explained computation occurrence order avoid approximating ever extension cliques potentially need compute. experiments error introduced large enough worth significant overhead. riously cues indicate variables. ignoring able often lead non-markovian duced loss information \"forgotten\" non-markovian dication process past. solutions trum. friedman solution works stages. search stage. selected based current statistics inference ically pot; potential work structure. consider pot; removing number required small uses heuristics procedure rithm therefore stage. repeats potential iterated general start learning slices structure using parametric passing algorithm fhmm best viewed hidden variables others; hidden variables shown good candidate several multiple periments binary hidden variables. small number observables variables.) also introduction hidden variables standard search disappointingly ables improve attribute work structure quite natural. decoupled tence arcs except able represents segment. duced several markovian ingly resents slices); troduced resents spond longer-range bach chorales data proposed part santa competition learning series melodic model five discrete attributes pitch duration fermata last three represent training notes long total training transitions; total test size chosen random first column table shows results data set. instances learning algorithms instances sleep apnea data also proposed part santa competition shenfeld ical parameters nea. data contains single various tient. ets. partitioned subsequences test sequence likelihoods again correlation assumption. interesting poral granularity stock market constructed selves handful chips since usually relations stock recorded covered trading middle period training transitions data giving test. log-likelihood table here second iteration duction basic score. somewhat different appear within natural ferent within move together. detecting dencies discover another intuition work numerical better nificantly observable search. structure tion hidden variables extent. compelling true network. present paper combine first deals search techniques presence mate inference boyen koller approximate inference metric jordan issues particular large number different methods based solution mation approximate tional clearly problem algorithm discovers interactions relations extreme covery influences current requires time scale model. hidden vari­ able evolving algorithm addressed markov property larities. sequence interactions believe learning servable processes thermore observations.", "year": 2013}