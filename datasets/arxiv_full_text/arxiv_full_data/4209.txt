{"title": "Belief Tree Search for Active Object Recognition", "tag": ["cs.AI", "cs.CV"], "abstract": "Active Object Recognition (AOR) has been approached as an unsupervised learning problem, in which optimal trajectories for object inspection are not known and are to be discovered by reducing label uncertainty measures or training with reinforcement learning. Such approaches have no guarantees of the quality of their solution. In this paper, we treat AOR as a Partially Observable Markov Decision Process (POMDP) and find near-optimal policies on training data using Belief Tree Search (BTS) on the corresponding belief Markov Decision Process (MDP). AOR then reduces to the problem of knowledge transfer from near-optimal policies on training set to the test set. We train a Long Short Term Memory (LSTM) network to predict the best next action on the training set rollouts. We sho that the proposed AOR method generalizes well to novel views of familiar objects and also to novel objects. We compare this supervised scheme against guided policy search, and find that the LSTM network reaches higher recognition accuracy compared to the guided policy method. We further look into optimizing the observation function to increase the total collected reward of optimal policy. In AOR, the observation function is known only approximately. We propose a gradient-based method update to this approximate observation function to increase the total reward of any policy. We show that by optimizing the observation function and retraining the supervised LSTM network, the AOR performance on the test set improves significantly.", "text": "dynamic programming monte carlo planning however methods require model object computationally heavy test time. propose method reduces optimal action selection simple classiﬁcation beliefs test time require planning. show proposed method generalizes well novel views familiar objects also novel objects. moreover show active perception paradigm used improve accuracy object recognition system selecting images likely result higher rewards training. ﬁrst contribution paper formulate pomdp problem adapt belief tree search algorithm discover near-optimal values objects poses training set. infer policy values train lstm network predict best action given current objects belief. test time actions suggested lstm explore objects. show supervised approach generalizes well explore novel objects novel views familiar objects results higher accuracy compared reinforcement learning guided policy search methods. second contribution derive update rule learn parameters pomdp likelihood function goal maximizing total reward. update rule emphasizes views objects produce higher rewards future. show retraining likelihood function using proposed method performance system signiﬁcantly improves. next section review previous approaches aor. present algorithm observation function update rule. results section report details implementation methods performance germs dataset. germs proved challenging dataset improve state-of-the-art performance dataset. ﬁnal section concluding remarks. large category models minimize predicted label uncertainty best next-action planning models predict object label probabilities using current view search best next action minimizes expected entropy object labels. methods learning object appearance performed ﬁtting generative model ofﬂine best action selection carried online test time. uncertainty measures conditional entropy mutual information computationally expensive evaluate abstract— active object recognition approached unsupervised learning problem optimal trajectories object inspection known discovered reducing label uncertainty measures training reinforcement learning. approaches guarantees quality solution. paper treat partially observable markov decision process near-optimal policies training data using belief tree search corresponding belief markov decision process reduces problem knowledge transfer near-optimal policies training test set. train long short term memory network predict best next action training rollouts. proposed method generalizes well novel views familiar objects also novel objects. compare supervised scheme guided policy search lstm network reaches higher recognition accuracy compared guided policy method. look optimizing observation function increase total collected reward optimal policy. observation function known approximately. propose gradient-based method update approximate observation function increase total reward policy. show optimizing observation function retraining supervised lstm network performance test improves signiﬁcantly. active object recognition refers problem predicting object label images able change pose object relative camera increasing prediction certainty. robot rotating in-hand object reﬁne label prediction accuracy example system. ambiguity object recognition exists similar views different objects. aims ﬁnding optimal sequence actions decreases label ambiguity improves object recognition performance smaller number steps. despite wide application performance improvement capacity applied widely remained secluded main-stream computer vision progress recent years. existing approaches change sensor position reduce ambiguity label prediction methods rely uncertainty object label greedy best next action selection test time decrease label probability entropy. method optimal action selection time using tree search monte carlo pomdp planning. proposed clustering beliefs belief tree search algorithm reduce width tree. despot uses sampling observations reduce width belief tree optimal action selection. pomcp adapts monte carlo sampling upper conﬁdence trees algorithm efﬁcient pomdp planning. based method desirable properties acting belief space performance guarantees reachable space beliefs. important features proposed approach freedom maintaining object model. method acts belief space objects requires black simulator training returns belief resulting performing different actions objects. another important property method test time next action predicted simple classiﬁcation current belief. show approach effective learning object exploration policies compared reinforcement learning actor-critic methods. next section describes proposed approach details. active object recognition formulated pomdp problem denoted tuple saot states actions object examination observations don’t perceive identity object directly rather collect information observations. transition function marks transition different states observation function relates observations different object identities probability observing taking action object labels reward function determines reward taking action object label finally reward discount factor. note pomdp transition function reduces identity function. observation space hand includes images objects prohibitively large apply value iteration techniques another approach monte carlo planning builds search tree one-step action selection however method suffers curse history exacerbated high dimension observations. seek solution pomdp compactly represents history allows tractable search ﬁnding optimal actions. second category models techniques reinforce neurally fitted q-iteration good policy action-value function object exploration parametric function encodes object exploration policy action-values learned ofﬂine using exploration policy collecting rewards depend label prediction accuracy. model updates parameters policy action-value function maximize total expected reward. method suffer high variance prediction sampling actions guarantee convergence local optimum require training explore discover optimal sequences actions. recently deep convolutional neural networks applied tool modeling object appearance along action-value prediction malmir trained deep using update rule work layer dirichlet distribution embedded network modeling distribution beliefs different object-action pairs. johns used deep cnns entropy regression action prediction next view points finding optimal trajectory object inspection approximated maximizing cross entropy adjacent views pairs. haque trained lstm networks reinforce algorithm recognize subjects point-clouds jayaraman grauman modeled object exploration policy neural network trained using classiﬁcation accuracy reward found predicting next state environment based current state action improves overall accuracy. methods show improved performance random exploration strategy non-active methods. however suffer problem previous methods lack guarantee performance even training set. approaches optimal exploration policies. atanasov adapted active hypothesis testing approach camera viewpoint selection object segmentation. approach learns model object appearance uses planning sequence actions minimizes cost motor movements object classiﬁcation view-point prediction. dynamic programming approach used discover best sequence actions minimizes cost. method depends representation object appearance efﬁcient planning method acts belief space completely independent object representation classiﬁcation. patten monte carlo planning active exploration perception outdoor objects. method uses rollouts depends predicting point-cloud objects different actions. compared method method intuitive doesn’t require model objects. adapt algorithm approximately optimal value image training set. algorithm depicted pseudo code style algorithm using values training active object recognition system reduces supervised learning knowledge transfer problem. node tree algorithm expands actions receives observations. beliefs calculated using belief already expanded belief height tree algorithm sets value equal backtracks. otherwise search continues children algorithm builds belief tree sampling images training maintaining δ-packing level tree. belief tree root denotes possible actions observations encountered inspecting object initial belief belief tree captures possible actions observations construction full belief tree prohibitive case active object recognition size observation space extremely large. modiﬁcation made algorithm compared theorem root node calculate value reduce overﬁtting values speciﬁc beliefs. algorithm beliefs similar result vastly different rewards considered calculating value happens classiﬁer uncertain examples beliefs close reﬂected calculated values. figure depicts proposed algorithm graphical form. deﬁned equivalent belief solving planning problem using belief tree search algorithm. algorithm constructs search tree given belief different branches represent actions resulting observations. node tree represents belief underlying pomdp states edge captures action resulting observation. algorithms starts root exhaustively performs action collect observations form beliefs. beliefs added children root process iterates beliefs stop states reached leaves. values backtracked leaves root estimate value belief tree search used online planning pomdps dynamics environment known. however adapt calculate optimal values images training set. plain belief tree search algorithm computationally intractable belief since examines observations action. instead adapt algorithm theorem sacriﬁces optimality predicted values exchange computational tractability. algorithm utilizes smoothness optimal value function cluster belief space. speciﬁcally given node tree represents approximate value beliefs δ-neighborhood represent clustering belief space width belief tree decreases manageable size. another approximation algorithm algorithm calculates approximate optimal value given belief reachable space reachable space deﬁned beliefs |s|-dimensional belief simplex. changing observation function parameters result increased likelihood images corresponding object label. theorem presents gradient ascent update rule parameters observation function goal increasing total reward. theorem given policy corresponding value function gradient total reward respect parameters observation function given proof. supplementary notes. intuitively speaking update rule weights parameter value belief reached observing corresponding changing observation function parameters changes belief dynamics transition probabilities depend updating observation function belief reached solved approximately using theorem practice evaluation computationally intractable. results section describe simple procedure updating observation function based value weighted updates. section adapt proposed method algorithm active recognition germs medium size dataset images different object collected robot. robot grabs object different orientations examines object rotating front camera. goal recognize object test orientations given in-hand orientations. germs proved challenging dataset since separation objects dataset requires extraction small visual cues categorization. extract visual features germs images using resnet deep model softmax layer trained features predict object label. convert train test images belief vectors train method belief space. normalize output softmax layer class germs important understand observation function pomdp approximation actual likelihood values object views different classes depends model data model likelihood. example borotsching gaussian mixture eigenspace model likelihood images view sphere object different classes malmir approximate observation function using deep convolutional networks calculating observation function value image usually requires feature extraction image density estimation different classes. assume parametric observation function function different values parameters changes approximation observation function. improve observation function using different estimation changes feature extraction density estimation. improved observation function results pomdp different environment dynamics. ideal case observation function value given image correct object label labels case pomdp reduces trivial mdp. given observation function theorem ﬁnds image values policies arbitrarily close optimal policy. however values depend pomdp dynamics e.g. observation function. propose improve probability sequence policy probability action state policy implement guided drawing sequences actions stochastic policy acquired performing softmax action-values. gradients multiplied importance applied network. figure shows comparison mean performance gnfq germs test data. approaches show interval performance shaded area. comparison report performance random policy time step action taken explore object. plain algorithm fails perform better gnfq performs better random. advantage gnfq signiﬁcant ﬁrst action gradually decreases next four actions. last action majority evidence accumulated three methods perform similar. actor-critic policy learning method updates policy parameters using gradients expected reward reduce variance gradient estimation predicted value decreased reward calculate likelihood image algorithm calculate value near-optimal policy image train set. order values planning proposes sampling approach repeatedly executes algorithm different simulations augments tree newly discovered beliefs ﬁnally uses action-values root resulting tree planning. proposed algorithm similar proposed approach however make similar belief vectors dataset simulations. found action-values root tree effective aor. extract action-values training image transfer knowledge action-values test set. compare three approaches learning policy action-values. ﬁrst second approaches neurally fittred q-learning actor critic approaches guided probabilistic policy uses action-values bts. show guiding approaches results slight improvement average performance test compared plain version. third approach lstm network learn predict best action action-values. show lstm network superior performance approaches. guided neurally fitted q-learning neurally fitted q-learning trains neural network predict action values using reward signal environment algorithm successfully applied reinforcement learning benchmarks playing atari games active object recognition heart approach iterative update rule network parameters network outputs action-values action state above gradient operator right side applies previously observed plain algorithm fail discover optimal policies active object recognition instead employ n-step extension algorithm proposed update rule applied action sequences length n-step speeds learning updating actionvalues iteration compared single action-value update original nfq. experiments reported obtained using -step sequences actions. improve performance employing importance sampling framework policy improvement idea behind approach auxiliary policy acquire sequences actions states update parameters target policy using sequences. order obtain unbiased action state ﬁgure compare performance -step actor-critic method without guiding. used guiding scheme described above multiplying gradient terms importance plain fails perform better random shows higher performance second third actions. transfer knowledge near-optimal values training test train neural network directly predict action-values given belief. stack lstm layers units layer followed softmax layer predicts action values training sequences produced following probabilistic policy derived action-values. belief vector target action highest action-value. found crucial performance sequences data supervised action prediction. figure compares performance lstm random policies. lstm clear advantage performance random policies. moreover variance learned policies signiﬁcantly smaller compared actor-critic methods. figure compares average performance lstm previous methods. supervised learning action-value prediction clearly superior policy learning methods. table shows comparison methods details. section test generalization proposed method novel object. goal experiment understand much object inspection knowledge transferrable amongst different objects germs. purpose objects germs training lstm method used rest objects testing. results averaged different experiments shown ﬁgure overall variance results high large variations accuracy germs objects. proposed method achieves better performance compared random. random strategy difﬁculty ﬁnding informative moves compared novel views experiment. section retrain observation function using proposed gradient update rule order implement update rule adapt sampling strategy generating rollouts using policy derived action-values. denote belief vector corresponding image denote beliefs actions resulted rollouts. retraining weight calculated using sample average rollouts retrain softmax using weighting cross entropy cost image using retraining algorithm resulting beliefs train lstm resulting action-values. show performance retrained lstm lstm-i ﬁgure retrained lstm achieves signiﬁcant improvement lstm. repeat procedure much improvement achieved. practice observed performance lstm starts decline second iteration. performance retrained lstm shown table mainstream computer vision machine learning communities despite potential improving recognition performance. progress slow reliance models semi supervised heuristic methods object inspection policy discovery. work proposed method learns object exploration policy supervised manner using training data. proposed method desirable properties example fast test time generalize novel objects since require generative model objects. recently large interest attention models mostly reasons besides object recognition. models rely reinforcement learning variation inference guide system discover suitable policies. contrast proposed beneﬁts reduced training time free local optima large problem approaches. reducing optimal policy inference supervised learning problem recent advances supervised visual recognition learning policies test data. developed weighting scheme training single images emphasizes images result higher value exploration. weight image denotes useful image achieving correct classiﬁcation future. weighting scheme potentially reduces overﬁtting single image classiﬁcation model images discriminative information. complex background germs images high chance overﬁtting background cues training. performing opportunity calculate value image direct single-image classiﬁcation model invest informative views objects. atanasov sankaran pappas daniilidis. nonmyopic view planning active object classiﬁcation pose estimation. ieee transactions robotics mnih kavukcuoglu. multiple object recognition browatzki tikhanoff metta b¨ulthoff wallraven. active object recognition humanoid robot. robotics automation ieee international conference pages ieee denzler brown niemann. optimal camera parameter selection state estimation applications object recognition. pattern recognition pages springer jayaraman grauman. look-ahead leap end-toend active recognition forecasting effect motion. european conference computer vision pages springer malmir sikka forster fasel movellan cottrell. deep active object recognition joint label action prediction. computer vision image understanding pages malmir sikka forster movellan cottrell. deep q-learning active recognition germs baseline performance standardized dataset active learning. proceedings british machine vision conference pages pages bmva mnih badia mirza graves lillicrap harley silver kavukcuoglu. asynchronous methods deep international conference machine reinforcement learning mnih heess graves kavukcuoglu. recurrent models visual attention. ghahramani welling cortes lawrence weinberger editors advances neural information processing systems pages curran associates inc.", "year": 2017}