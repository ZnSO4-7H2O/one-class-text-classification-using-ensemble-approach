{"title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).", "text": "domain adaptation important open problem deep reinforcement learning many scenarios interest data hard obtain agents learn source policy setting data readily available hope generalises well target domain. propose multi-stage agent darla learns learning act. darla’s vision based learning disentangled representation observed environment. darla able acquire source policies robust many domain shifts even access target domain. darla signiﬁcantly outperforms conventional baselines zero-shot domain adaptation scenarios effect holds across variety environments base algorithms introduction autonomous agents learn maximise future expected rewards choosing based incoming sensory observations reinforcement learning early approaches scale well environments large state spaces high-dimensional observations commonly used workaround embed observations lower-dimensional space typically hand-crafted and/or privileged-information features. recently advent deep learning successful combination enabled end-to-end learning embeddings directly inputs sparking success wide variety previously challenging domains despite seemingly universal efﬁcacy deep however fundamental issues remain. include data inefﬁciency reactive nature general brittleness learnt policies changes input data distribution lack model interpretability paper focuses outstanding issues ability agents deal changes input distribution form transfer learning known domain adaptation domain adaptation scenarios agent trained particular input distribution speciﬁed reward structure placed setting input distribution modiﬁed reward structure remains largely intact develop agent learn robust policy using observations rewards obtained exclusively within source domain. here policy considered robust generalises minimal drop performance target domain without extra ﬁne-tuning. past attempts build agents strong domain adaptation performance highlighted importance learning good internal representations observations typically approaches tried align source target domain representations utilising observation reward signals domains many scenarios robotics reliance target domain information problematic data expensive difﬁcult obtain furthermore target domain simply known advance. hand policies learnt exclusively source domain using existing deep approaches constraints nature learnt representations often overﬁt source input distribution resulting poor domain adaptation performance propose tackling issues focusing instead learning representations capture underlying low-dimensional factorised representation world therefore task domain speciﬁc. many natdarla develops vision learning parse world terms basic visual concepts objects positions colours etc. utilising stream unlabelled observations unlike human babies ﬁrst months life second stage agent utilises disentangled visual representation learn robust source policy. stage three demonstrate darla source policy robust domain shifts leading signiﬁcantly smaller drop performance target domain even policy ﬁnetuning allowed effects hold consistently across number different environments algorithms formalise domain adaptation scenarios reinforcement learning setting. denote source target domains respectively. domain corresponds deﬁned tuple state space action space transition function reward function domain adaptation scenarios states source target domains quite different action spaces shared transitions reward functions structural similarity. example consider domain adaptation scenario jaco robotic mujoco simulation source domain real world setting target domain. state spaces source target domains differ signiﬁcantly perceptual-reality domains however share action spaces since policy learns control actuators within arm. finally source target domain transition reward functions share structural similarity since domains transitions states governed physics world performance task depends relative position arm’s effectors respect object interest. figure schematic representation darla. yellow represents denoising autoencoder part model blue represents β-vae part model grey represents policy learning part model. uralistic domains video game environments simulations world well described terms structure. examples factors variation object properties like colour scale position; examples correspond general environmental factors geometry lighting. think factors high-level parameters used world graphics engine generate particular natural visual scene learning project observations factorised description world addressed large body literature disentangled representation learning disentangled representations deﬁned interpretable factorised latent representations either single latent group latent units sensitive changes single ground truth factors variation used generate visual world invariant changes factors theoretical utility disentangled representations supervised reinforcement learning described however knowledge empirically validated date. demonstrate disentangled representations improve robustness algorithms domain adaptation scenarios introducing darla agent capable learning robust policy source domain achieves signiﬁcantly better out-of-the-box performance domain adaptation scenarios compared various baselines. darla relies learning latent state representation shared source target domains learning disentangled representation environment’s generative factors. crucially darla require target domain data form representations. approach utilises three stage pipeline learning learning transfer. ﬁrst stage natural world mdps sampled deﬁne terms state space contains possible conjunctions high-level factors variation necessary generate naturalistic observation natural world whose state space corresponds subset simple terms assume exists shared underlying structure mdps sampled contend reasonable assumption permits inclusion many interesting problems including being able characterise reality introduce notation state space variables principle used interchangeably within source target domain mdps agent observation state space agent’s internal latent state space consists observations generated true world simulator sam∼ sim. pled data generative factors i.e. sampled distribution process using newly introduced notation domain adaptation scenarios described different sampling processes source target domains respectively using generate different agent observation sim. intuitively constates sider source domain oranges appear blue rooms apples appear rooms target domain object/room conjunctions reversed oranges appear rooms apples appear blue rooms. true data generative factors variation remain room colour object type particular source target distributions differ. typically deep agents operating learn end-to-end mapping observations actions actions derived). process doing agent implicitly learns function maps typically high-dimensional observations typically low-dimensional latent states followed policy function maps latent actions states context domain adaptation agent learns naive latent state map→ ping function source domain using reward signals shape representation learning likely overﬁt source domain generalise well target domain. returning intuitive example imagine agent learnt policy pick oranges avoid apples source domain. source policy likely based entangled latent state space object/room conjunctions oranges/blue good apples/red since arguably efﬁcient representation maximising expected rewards source task absence extra supervision signals suggesting otherwise. source policy based entangled latent representation generalise well target domain without ﬁne-tuning since therefore crucially hand since sampled natural world state space source target domains respectively possible learn latent state mapping function projects agent observation state space latent state space expressed terms factorised data generative factors representative consider intuitive natural world i.e. example maps agent observations factorised disentangled representation expressed terms data generative factors disentangled latent state mapping function directly generalise source target domains hence darla based idea good quality learnt exclusively source domain zero-shot-generalise target domains therefore source policy also generalise target domains box. next describe stages darla pipeline allow learn source policies robust domain adaptation scenarios despite trained knowledge target domains learn task inferring factorised generative factors observations goal extensive disentangled factor learning literature hence stage learn mapping using unsupervised model learning disentangled factors utilises observations collected agent random policy visual pre-training note require sufﬁcient variability factors conjunctions order learn agent learnt world stage terms natural data generative factors exposed source domain agent tasked learning source policy phase; transfer ﬁnal step test well policy learnt source domain generalises target domain zero-shot domain adaptation setting i.e. agent evaluated target domain without retraining. compare performance policies learnt disentangled latent state various baselines latent state mapping function projects agent observations entangled latent state representations learning disentangled representations order learn darla utilises β-vae state-of-the-art unsupervised model automated discovery factorised latent representations image data. β-vae modiﬁcation variational autoencoder framework controls nature learnt latent representations introducing adjustable hyperparameter balance reconstruction accuracy latent channel capacity independence constraints. maximises objective parametrise distributions encoder decoder respectively. well-chosen values usually larger typically result disentangled latent representations limiting capacity latent information channel hence encouraging efﬁcient factorised encoding increased pressure match isotropic unit gaussian prior cost increasing crucial information scene discarded latent representation particularly information takes small proportion observations pixel space. encountered issue tasks discussed section shortcomings calculating log-likelihood term eqφ] per-pixel basis known addressed past calculating reconstruction cost abstract high-level feature space given another neural network model pre-trained alexnet practice found pre-training denoising autoencoder data visual pre-training worked best reconstruction targets β-vae match β-vaedae model trained according rw×h×c function maps images pixel space dimensionality high-level feature space dimensionality given stack pre-trained layers certain layer depth. note replacing pixel based reconstruction loss high-level feature reconstruction loss longer optimising variational lower bound β-vaedae loses equivalence variational autoencoder framework proposed setting interpret mixing coefﬁcient balances capacity latent channel β-vaedae pressure match high-level features within pre-trained dae. deep network variant q-learning algorithm utilises deep learning. uses neural network parametrise approximation action-value function using parameters asynchronous advantage actor-critic asynchronous implementation advantage actor-critic paradigm separate threads parallel perform updates shared parameters. different threads hold instance environment different exploration policies thereby decorrelating parameter updates without need experience replay. therefore online algorithm whereas learns policy ofﬂine resulting different learning dynamics bemodel-free episodic control proposed complementary learning system algorithms described above. algorithm relies near-determinism state transitions rewards environments; settings holds exploit properties memorise action high returns similar situations past. since simplest form relies lookup table learns good policies much faster value-function-approximation based deep algorithms like trained gradient descent cost generality also compared approach unreal recently proposed algorithm also attempts utilise unsupervised data environment. unreal agent takes base lstm agent augments number unsupervised auxiliary tasks make rich perceptual data available agent besides extrinsic reward signals. auxiliary learning tends improve representation learnt agent. sec. supplementary materials details algorithms above. tasks evaluate performance darla different task environment setups probe subtly different aspects domain adaptation. reminder sec. deﬁned state space contains possible conjunctions high-level factors variation necessary generate naturalistic observation domain adaptation scenarios agent observation states generated simt according source target domains respectively sampled distributions processes according deepmind test version domain adaptation setup source target domain observation simulators equal processes used sample different jaco matching mujoco simulation environment domain adaptation scenarios simulation simulation simulation reality simsim domain adaptation setup relatively similar deepmind i.e. source target domains differ terms processes however signiﬁcant point difference. deepmind values factors target domain previously seen source domain; however conjunctions figure deepmind transfer task setup. different conjunctions {room object object} used different parts domain adaptation curriculum. stage used minimal spanning objects rooms whereby object seen room. note extrinsic reward signal notion ‘task’ phase. stage agents taught pick cans balloons avoid hats cakes. objects always presented pairs hat/can cake/balloon. agent never hat/can pair pink room. novel room/object conjunction presented target domain adaptation condition ability agent transfer knowledge objects’ value novel environment tested. β-vae reconstructions using frames deepmind increased necessary disentangle data generative factors variations model lost information objects. fig. model appropriately capturing objects. left sample frames mujoco simulation environments used vision source policy training middle simsim domain adaptation test right simreal domain adaptation test factor values different. simsim contrast novel factor values experienced target domain hence deepmind considered assessing domain interpolation performance whereas simsim tests domain extrapolation. simreal setup hand based identical processes different observation simulators sims simt corresponding mujoco simulation real world results so-called ‘perceptual reality gap’ details tasks given below. deepmind ﬁrst person game environment rich visuals realistic physics. used standard seekavoid object gathering setup room initialised equal number randomly placed objects different types. object varieties ‘good’ ‘bad’ full state space consisted conjunctions room types four object types source domain contained environments hats/cans presented green room balloons/cakes presented either green pink room. target domain contained hats/cans presented pink room. domains cans balloons rewarded objects. learn used β-vaedae learn disentangled latent state representation includes room object generative factors variation within deepmind lab. high-level feature space pre-trained within β-vaedae framework instead pixel space vanilla βvae found objects failed reconstruct using values necessary disentangle generative factors variation within deepmind β-vaedae trained observations collected agent simple wall-avoiding policy order enable model learn important expose agent least minimal environments span range values factor extraneous correlations added different factors. section supplementary materials details β-vaedae training. learn agent trained algorithms detailed section seek-avoid task using source domain conjunctions object/room shown fig. pre-trained β-vaedae stage used ‘vision’ part various algorithms learn source policy picks balloons avoids cakes green pink rooms picks cans avoids hats green rooms. section supplementary materials details various versions darla tried based different base algorithm. transfer tested ability darla transfer seek-avoid policy learnt source domain stage using domain adaptation condition illustrated figure agent continue picking cans avoid hats pink room even though objects seen green room source policy training. optimal policy maintains reward polarity source domain details appendix a... setup deepmind domain adaptation task object environment factors supposed independent. order ensure β-vaedae learns factorised representation reﬂects ground truth independence present observations possible conjunctions room individual object types. used frames camera facing robotic jaco matching rendered camera view mujoco physics simulation environment investigate performance darla domain adaptation scenarios simulation simulation simulation reality simreal setup particular importance since progress deep brought control tasks simulation translated well reality despite various attempts solving control problems reality hard sparse reward signals expensive data acquisition attendant danger breaking robot exploration. simsim simreal trained agent perform object reaching policy goal place effector close object possible. conceptually reaching task simple hard control problem since requires correct inference object positions velocities visual inputs. learn β-vae trained observations collected mujoco simulations factors variation order enable model learn reaching policy applied phantom obf; learn feedforward-ac based agent vision module pre-trained stage taught source reaching policy towards real object simulation example frame sec. supplementary materials fuller description agent). source domain agent trained distribution camera angles positions. colour tabletop rests object colour sampled anew every episode. transfer simsim target domain agent faced distribution camera angles positions little overlap source domain distributions well completely held object colours simreal target domain camera position angle well tabletop colour object colour sampled distributions seen source domain target domain real world. many details present real world shadows specularity multiple light sources modelled simulation; disentangled β-vae entangled β-vae denoising autoencoder apart nature learnt representations darla versions baselines equivalent throughout three stages proposed pipeline terms architecture observed data distribution figs. display degree disentanglement learnt vision modules darla darlaent deepmind mujoco. darla’s vision learnt independently represent environment variables object-related variables deepmind disentangling also evident mujoco. fig. left shows darla’s single latent units learnt represent different aspects jaco object camera. contrast representations learnt darlaent latent responsible changes environment objects deepmind mixture camera object and/or movements mujoco. table fig. shows average performance terms rewards episode various agents target domain ﬁne-tuning source policy seen darla able zero-shot-generalise signiﬁcantly better darlaent darladae highlighting importance learning disentangled representation unsupervised stage darla pipeline. particular also demonstrates improved domain transfer performance simply function increased exposure training observations darlaent darladae exposed data. results mostly consistent across target domains cases darla signiﬁcantly better second-best-performing agent. holds simreal task able perform zero-shot policy transfer highly valuable particular difﬁculties gathering data real world. darla’s performance particularly surprising actually preserves less information observations darlaent darladae. nature β-vae achieves disentangling; disentangled model utilised signiﬁcantly higher value hyperparameter entangled model constrains caphysics engine also perfect model reality. thus simreal tests ability agent cross perceptual-reality generalise source policy real world details appendix a... results evaluated robustness darla’s policy learnt source domain various shifts input data distribution. particular used domain adaptation scenarios based deepmind seek-avoid task jaco reaching task described sec. task compared darla’s performance various baselines. evaluated importance learning ‘good’ vision stage pipeline maps input observations disentangled representations order this darla encoders pipeline different vision models figure table zero-shot performance source policy target domains within deepmind jaco/mujoco environments. baseline agent refers vanilla dqn/ac/ec agents. main text detailed model descriptions. figure correlation zero-shot performance transfer performance deepmind task obtained based darla level disentanglement measured transfer/disentanglement score pacity latent channel. indeed darla’s β-vae utilises possible gaussian latents store observation-speciﬁc information mujoco/jaco whereas darlaent utilises environments furthermore examined happens darla’s vision allowed ﬁne-tuned gradient updates learning source policy stage pipeline. denoted darlaft table fig. exhibits signiﬁcantly worse performance darla zero-shot domain adaptation using acbased agent tasks. suggests favourable initialisation make subsequent overﬁtting source domain on-policy however off-policy dqn-based ﬁne-tuned agent performs well. leave investigation curious effect future work. finally compared performance darla unreal agent architecture. despite also exploiting unsupervised data available source domain unreal performed worse baseline deepmind domain adaptation task. demonstrates unsupervised data panacea transfer performance; must utilised careful structured manner conducive learning disentangled latent states order quantitatively evaluate hypothesis disentangled representations essential darla’s performance domain adaptation scenarios trained various darlas different degrees learnt disentanglement varying stage pipeline. calculated correlation performance ec-based darla deepmind domain adaptation task transfer metric approximately measures quality disentanglement darla’s latent representations shown chart fig. seen strong positive correlation level disentanglement darla’s zeroshot domain transfer performance shown robust utility disentangled representations agents domain adaptation note evidence provide important additional beneﬁt. found signiﬁcantly improved speed learning source domain itself function disentangled model was. gain data efﬁciency disentangled representations source policy learning main focus paper leave main text; however provide results discussion section supplementary materials. conclusion demonstrated beneﬁts using disentangled representations deep setting domain adaptation. particular proposed darla multi-stage agent. darla ﬁrst learns visual system encodes observations receives environment disentangled representations completely unsupervised manner. uses representations learn robust source policy capable zero-shot domain adaptation. demonstrated efﬁcacy approach range domains task setups naturalistic ﬁrstperson environment simulated graphics physics engine crossing simulation reality also shown effect disentangling consistent across different algorithms achieving signiﬁcant improvements baseline algorithms best knowledge ﬁrst comprehensive empirical demonstration strength disentangled representations domain adaptation deep setting. references abadi martin agarwal ashish paul barham. tensorﬂow large-scale machine learning heterogeneous distributed systems. preliminary white paper blundell charles uria benigno pritzel alexander yazhe ruderman avraham leibo joel jack wierstra daan hassabis demis. model-free episodic control. arxiv chen duan houthooft rein schulman john sutskever ilya abbeel pieter. infogan interpretable representation learning information maximizing generative adversarial nets. arxiv cheung brian levezey jesse bansal arjun olshausen bruno discovering hidden factors variation deep networks. proceedings international conference learning representations workshop track daftry shreyansh bagnell andrew hebert martial. learning transferable policies monocular reactive coninternational symposium experimental robotics trol. gupta abhishek devin coline yuxuan abbeel pieter levine sergey. learning invariant feature spaces transfer skills reinforcement learning. iclr heess nicolas wayne gregory silver david lillicrap timothy erez tassa yuval. learning continuous control policies stochastic value gradients. nips higgins irina matthey loic arka burgess christopher glorot xavier botvinick matthew mohamed shakir lerchner alexander. beta-vae learning basic visual concepts constrained variational framework. iclr jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. iclr lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. corr mcclelland james mcnaughton bruce oreilly randall complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory. psychological review mnih volodymyr badia adri puigdomnech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. icml https//arxiv. org/pdf/..pdf. niekum scott chitta sachin barto andrew marthi bhaskara osentoski sarah. incremental semantically grounded learning demonstration. robotics science systems norman kenneth o’reilly randall modeling hippocampal neocortical contributions recognition memory complementary-learning-systems approach. psychological review pathak deepak kr¨ahenb¨uhl philipp donahue jeff darrell trevor efros alexei context encoders feature learning inpainting. corr abs/. http //arxiv.org/abs/.. rajendran janarthanan lakshminarayanan aravind khapra mitesh prasanna ravindran balaraman. attend adapt transfer attentive deep architecture adaptive iclr transfer multiple sources domain. tobin josh fong rachel alex schneider jonas zaremba wojciech abbeel pieter. domain randomization transferring deep neural networks simulation real world. arxiv tulving endel hayman macdonald carol longlasting perceptual priming semantic learning amnesia case experiment. journal experimental psychology learning memory cognition tzeng eric devin coline hoffman judy finn chelsea abbeel pieter levine sergey saenko kate darrell trevor. adapting deep visuomotor representations weak pairwise constraints. wafr vincent pascal larochelle hugo lajoie isabelle bengio yoshua manzagol pierre-antoine. stacked denoising autoencoders learning useful representations deep network local denoising criterion. nips whitney william chang michael kulkarni tejas tenenbaum joshua understanding visual concepts continuation learning. arxiv http//arxiv. org/pdf/..pdf. supplementary materials reinforcement learning paradigm reinforcement learning paradigm consists agent receiving sequence observations function environment states accompanied rewards conditional actions chosen time step assume interactions modelled markov decision process deﬁned tuple transition function models distribution possible next states given action taken state accompanied transition reward signal rt+. goal agent learn policy probability distribution actions maximises expected return i.e. discounted γτ−rt+τ time step episode ends discount factor progressively down-weights future rewards. given policy deﬁne value function described source episode deepmind agent presented three possible room/object type conjunctions chosen random. marked setup seek-avoid style task object types room gave reward gave reward agent allowed pick objects seconds episode would terminate would begin; agent able pick ‘good’ objects less seconds episode begun immediately. agent spawned random location room start episode. described source task consisted agent learning control simulated order reach toward object. shaping reward used maximum value centre object fell pinch grip sites effector within distance two. distances dimensions counted double compared distances dimension. episode object placed random drop point within area random initial start position high work-space independent object’s position. episode lasted steps seconds control step observations sampled randomly across episodes. overall million frames dimensions used stage curriculum. episode camera position orientation randomly sampled isotropic normal distribution centred around approximate position orientation real camera standard deviation precise measurements used match two. work-space table colour sampled uniformly around midpoint independently channel; object colours sampled uniformly random space rejecting colours fell within ball around held-out intensities latter used simulated transfer experiments i.e. simsim experiments. additionally gaussian noise standard deviation added observations simsim task. real jaco mujoco simulation counterpart nine joints could independently take different actions simulation gaussian noise standard deviation added discrete velocity output; delays real setup observations action execution simulated randomly mixing velocity outputs previous steps instead emitting last output directly. speed ranges jaco arm’s speed joints starting base ﬁngers could full range. safety reasons speed ranges reduced factor evaluating agents jaco without signiﬁcant performance degradation. denoising autoencoder used model provide feature space β-vae reconstruction loss computed trained occlusion-style masking noise vein learn semantic representation input frames. concretely values independently sampled width height input frames. four values determined corners rectangular mask applied; pixels fell within mask zero. architecture consisted four convolutional layers kernel size stride height width dimensions. number ﬁlters learnt layer respectively. bottleneck layer consisted fully connected layer size neurons. followed four deconvolutional layers kernel sizes strides ﬁlters. padding algorithm used ‘same’ tensorflow relu non-linearities used throughout. speciﬁcally input passed β-vae sampled reconstruction passed pre-trained designated layer. distance representation representation original input passed layers computed formed train. loss β-vae part β-vaedae weights remained frozen throughout. β-vae architecture consisted encoder four convolutional layers kernel size stride height width dimensions. number ﬁlters learnt layer respectively. followed fully connected layer size neurons. latent layer comprised neurons parametrising independent gaussian distributions. decoder architecture simply reverse encoder utilising deconvolutional layers. decoder used gaussian number output channels number channels input frames had. padding algorithm used ‘same’ tensorflow. relu non-linearities used throughout. model trained loss given speciﬁcally disentangled model used darla trained hyperparameter value layer used compute perceptual similarity loss last deconvolutional layer. entangled model used darlaent trained hyperparameter value last deconvolutional layer used compute perceptual similarity loss. mujoco/jaco tasks standard β-vae used rather β-vaedae used deepmind lab. architecture encoder decoder latent size exactly described previous section a... disentangled βvae darla entangled model darlaent corresponding standard baseline model darladae trained denoising autoencoder occlusion-style masking noise described appendix section a... architecture used matched exactly β-vae described appendix section however stochastic nodes replaced deterministic neurons. representations taken passing layer passing following non-linearity. also brieﬂy experimented taking loss post-activation signiﬁcant difference. convolutional part q-net replaced encoder β-vaedae stage frozen. takes four consecutive frames input order capture aspect environment dynamics agent’s state. order match setup pre-trained vision stack passed observation frame pre-trained model concatenated outputs together form k-dimensional input policy network. case size darla well darlaent darladae darlaft. frozen convolutional stack ‘policy’ layers neurons used ﬁnal linear layer neurons corresponding size action space deepmind task. relu non-linearities used throughout. hyperparameters reported episodic control episodic controller-based darla used mostly hyperparameters original paper explored following hyperparameter settings number nearest neighbours return horizon kernel type {inverse gaussian} kernel width tried training without peng’s practice found none explored hyperparameter choices signiﬁcantly inﬂuenced results experiments. ﬁnal hyperparameters used experiments reported paper following number nearest neighbours return horizon kernel type inverse kernel width peng’s darla mujoco/jaco based feedforward closely followed simulation training setup feed-forward networks using visualinput only. place usual conv-stack however used encoder β-vae described appendix a... followed linear layer units relu non-linearity collection linear softmax layers independent policy outputs well single value output layer outputted value function. figure traversals latent corresponding room background models different transfer metric scores note entangled model many objects appear blue changes shape addition background changing. model middling transfer score object type background alter; whereas disentangled model little apart background changes. order choose optimal value β-vae -dae models evaluate ﬁtness representations learnt stage pipeline used visual inspection heuristic described heuristic involved clustering trained β-vae based models based number informative latents cluster examined degree learnt disentanglement running inference number seed images traversing latent unit z{i} time three standard deviations away average inferred mean keeping latents z{\\i} ﬁxed inferred values. allowed visually examine whether individual latent unit learnt control single interpretable factor variation data. similar heuristic rigueur method exhibiting disentanglement disentanglement literature case deepmind able ground truth labels corresponding factors variation object type background design proxy disentanglement metric proposed procedure used unlike uses lstm core encode history therefore longer term memory permitting perform better partially observed environments. version used paper deepmind task policy additionally takes last action last reward inputs along observation unreal agent takes base lstm agent augments number unsupervised auxiliary tasks make rich perceptual data available agent besides extrinsic reward signals. auxiliary learning tends improve representation learnt agent. training base agent observations rewards actions stored replay buffer used auxiliary learning tasks. tasks include pixel control agent learns control environment training auxiliary policies maximally change pixel intensities different parts input; reward prediction given replay buffer observations within short time period extrinsic reward agent predict reward obtained next unobserved timestep using sequence three preceding steps; value function replay extra training value function promote faster value iteration. simplest form lookup table states actions denoted state picks action highest value. episode discounted regeneralise policy novel states uses non-parametric nearest neighbours search states smallest distance novel state like takes concatenation four frames input. algorithm proposed model fast hippocampal instance-based learning brain deep algorithms described analogous slow cortical learning relies generalised statistical summaries input distribution source task performance results focus paper primarily zero-shot domain adaptation performance. however also interesting analyse effect darla approach source domain policy performance. order compare models’ behaviour source task examined training curves noted particular their train model consideration observations learn described stage darla pipeline. learn linear model representations corresponds possible rooms corresponds possible objects. therefore learning low-vc dimension classiﬁer predict room object class latent representation model. crucially linear model trained subset cartesian product e.g. practice utilised softmax classiﬁer trained using backpropagation cross-entropy loss keeping unsupervised model ﬁxed. trained linear model accuracy evaluated held subset cartesian product although procedure measures disentangling linearity latents object type room background nevertheless found metric highly correlated disentanglement determined visual inspection variant q-learning algorithm utilises deep learning. uses neural network parametrise approximation action-value function using parameters parameters updated minimising mean-squared error -step lookaasynchronous advantage actor-critic asynchronous implementation advantage actor-critic paradigm separate threads parallel perform updates shared parameters. different threads hold instance environment different exploration policies thereby decorrelating parameter updates without need experience replay. uses neural networks approximate policy value functions using parameters using nstep look-ahead loss algorithm trained using advantage actor-critic loss function entropy regularisation penalty es∼π entropy. parameter updates performed every tmax actions terminal purposes metric utilised rooms single objects denote subscript e.g. observation baseline algorithms could ﬁne-tuned source task able achieve higher asymptotic performance. particularly notable deepmind lab. however cases darla able learn reasonable policies source task order lower ﬁne-tuned models arguably worthwhile sacriﬁce subsequent median improvement target domain performance noted main text. vision module boosted source task learning speed allowed agent asymptote level baseline algorithms. discussed main text comes cost signiﬁcantly reduced domain transfer performance however ﬁnetuning appears offer best worlds. perhaps relevantly paper even solely examining source task performance darla outperforms darlaent darladae asymptotic performance data efﬁciency suggesting disentangled representations wider applicability beyond zero-shot domain adaptation focus paper. figure source task performance training curves unreal. darla shows accelerated learning task compared architectures. results show average standard deviation random seeds using workers.", "year": 2017}