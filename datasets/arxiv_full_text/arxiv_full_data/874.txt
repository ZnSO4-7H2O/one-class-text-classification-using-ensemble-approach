{"title": "Distillation as a Defense to Adversarial Perturbations against Deep  Neural Networks", "tag": ["cs.CR", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.", "text": "left image correctly classiﬁed trained car. right image crafted adversarial sample algorithm correct left image. altered image incorrectly classiﬁed dnn. misclassiﬁcation dangerous consider deep learning commonly used autonomous cars systems based dnns used recognize signs vehicles road perturbing input systems slightly altering car’s body instance prevents dnns classifying moving vehicule correctly might stop eventually involved accident potentially disastrous consequences. threat real adversary proﬁt evading detection input misclassiﬁed. attacks commonly occur today non-dl classiﬁcation systems thus adversarial samples must taken account designing security sensitive systems incorporating dnns. unfortunately effective countermeasures available today. previous work considered problem constructing defenses solutions proposed deﬁcient require making modiﬁcations architecture partially prevent adversarial samples effective distillation training procedure initially designed train using knowledge transferred different dnn. intuition suggested distillation formally introduced motivation behind knowledge transfer operated distillation reduce computational complexity architectures transferring knowledge larger architectures smaller ones. facilitates deployment deep learning resource constrained devices cannot rely powerful gpus perform computations. formulate variant distillation provide defense training instead transferring knowledge different architectures propose knowledge extracted improve resilience adversarial samples. abstract—deep learning algorithms shown perform extremely well many classical machine learning problems. however recent studies shown deep learning like machine learning techniques vulnerable adversarial samples inputs crafted force deep neural network provide adversary-selected outputs. attacks seriously undermine security system supported sometimes devastating consequences. example autonomous vehicles crashed illicit illegal content bypass content ﬁlters biometric authentication systems manipulated allow improper access. work introduce defensive mechanism called defensive distillation reduce effectiveness adversarial samples dnns. analytically investigate generalizability robustness properties granted defensive distillation training dnns. also empirically study effectiveness defense mechanisms dnns placed adversarial settings. study shows defensive distillation reduce effectiveness sample creation less studied dnn. dramatic gains explained fact distillation leads gradients used adversarial sample creation reduced factor also distillation increases average minimum number features need modiﬁed create adversarial samples dnns tested. deep learning demonstrated perform exceptionally well several categories machine learning problems notably input classiﬁcation. deep neural networks efﬁciently learn highly accurate models large corpus training samples thereafter classify unseen samples great accuracy. result dnns used many settings increasingly security-sensitive using deep learning algorithms designers systems make implicit security assumptions deep neural networks. however recent work machine learning security communities shown adversaries force many machine learning models including dnns produce adversary-selected outputs using carefully crafted inputs speciﬁcally adversaries craft particular inputs named adversarial samples leading models produce output behavior choice misclassiﬁcation. inputs crafted adding carefully chosen adversarial perturbation legitimate sample. resulting sample necessarily unnatural i.e. outside training data manifold. algorithms crafting adversarial samples designed minimize perturbation thus making adversarial samples hard distinguish legitimate samples. attacks based adversarial samples paper explore analytically empirically distillation defensive mechanism adversarial samples. knowledge extracted distillation reduce amplitude network gradients exploited adversaries craft adversarial samples. adversarial gradients high crafting adversarial samples becomes easier small perturbations induce high output variations. defend perturbations must therefore reduce variations around input consequently amplitude adversarial gradients. words defensive distillation smooth model learned architecture training helping model generalize better samples outside training dataset. test time models trained defensive distillation less sensitive adversarial samples therefore suitable deployment security sensitive settings. make following contributions paper articulate requirements design adversarial sample defenses. guidelines highlight inherent tension defensive robustness output accuracy performance dnns. introduce defensive distillation procedure train dnn-based classiﬁer models robust perturbations. distillation extracts additional knowledge training points class probability vectors produced back training regimen. departs substantially past uses distillation aimed reduce architectures improve computational performance rather feeds gained knowledge back original models. analytically investigate defensive distillation security countermeasure. show distillation generates smoother classiﬁer models reducing sensitivity input perturbations. smoother classiﬁers found resilient adversarial samples improved class generalizability properties. show empirically defensive distillation reduces success rate adversarial sample crafting ﬁrst trained mnist dataset second trained cifar dataset. empirical exploration distillation parameter space shows correct parameterization reduce sensitivity input perturbations factor successively increases average minimum number input features perturbed achieve adversarial targets ﬁrst second dnn. deep learning established technique machine learning. section provide rudiments deep neural networks necessary understand subtleties adversarial settings. formally describe attack methods context framework construct develop understanding vulnerabilities exploited attacks compare strengths training deploying architectures deep neural networks compose many parametric functions build increasingly complex representations high dimensional input expressed terms previous simpler representations practically speaking made several successive layers neurons building output layer. layers seen successive representations input data multidimensional vector corresponding parametric functions mentioned above. neurons constituting layers modeled elementary computing units applying activation function input. layers connected using links weighted vectors also referred network parameters figure illustrates architecture along notations used paper. numerical values weight vectors evaluated network’s training phase. phase architecture given large known input-output pairs uses series successive forward backward passes layers compute prediction errors made output layer corresponding gradients respect weight parameters weights updated using previously described gradients order improve prediction eventually overall accuracy network. training process referred backpropagation governed hyperparameters essential convergence model weight important hyper-parameter learning rate controls speed weights updated gradients. attacker’s goals quite diverse pointed previous work formalizing space adversaries deep learning classiﬁers range simple conﬁdence reduction source-target misclassiﬁcation paper considers source-target misclassiﬁcation also known chosen target attack following sections. potential examples adversarial samples realistic contexts could include slightly altering malware executables order evade detection systems built using dnns adding perturbations handwritten digits check resulting wrongly recognizing digits altering pattern illegal ﬁnancial operations prevent picked fraud detections systems using dnns. similar attacks occur today non-dnn classiﬁcation systems likely ported adversaries classiﬁers. explained later attack framework described section methods crafting adversarial samples theoretically require strong knowledge architecture. however practice even attackers limited capabilities perform attacks approximating target model crafting adversarial samples approximated model. indeed previous work reported adversarial samples dnns transferable model another skilled adversaries thus train dnns produce adversarial samples evading victim dnns. therefore throughout paper consider attacker capability accessing trained used classiﬁcation since transferability adversarial samples makes assumption acceptable. capability indeed take various forms including instance direct access network architecture implementation parameters access network oracle requiring adversary approximatively replicate model. note consider attacks training time paper leave considerations future work. describe precisely adversarial sample crafted adversaries. general framework introduce builds previous attack approaches split folds direction sensitivity estimation perturbation selection. attacks holding framework correspond adversaries diverse goals including goal misclassifying samples speciﬁc source class distinct target class. strongest adversarial goals attacks targeting classiﬁers test time several goals achieved adversary capability achieving goal. speciﬁcally consider sample trained resulting classiﬁer model goal adversary produce adversarial sample adding perturbation sample adversarial target output taking form indicator vector target class fig. legitimate adversarial samples datasets dataset legitimate samples correctly classiﬁed dnns found corresponding adversarial samples misclassifed dnns bottom row. network trained architecture together parameter values considered classiﬁcation function test phase begins network used unseen inputs predict outputs weights learned training hold knowledge applies unseen inputs. depending type output expected network either refer supervised learning network must learn association inputs outputs unsupervised learning network trained unlabeled inputs paper consider supervised learning speciﬁcally task classiﬁcation. goal training phase enable neural network extrapolate training data observed training correctly predict outputs unseen samples test time. adversarial deep learning shown previous work dnns deployed adversarial settings must take account certain vulnerabilities namely adversarial samples artifacts threat vector dnns exploited adversaries test time network training completed. crafted adding carefully selected perturbations legitimate inputs property provoke speciﬁc behavior initially chosen adversary. instance adversaries alter samples misclassiﬁed case adversarial samples crafted experiments presented section illustrated figure note noise introduced perturbation added craft adversarial sample must small enough allow human still correctly process sample. fig. adversarial crafting framework existing algorithms adversarial sample crafting succession steps direction sensitivity estimation perturbation selection. step evaluates sensitivity model input point corresponding sample step uses knowledge select perturbation affecting sample classiﬁcation. resulting sample misclassiﬁed model adversarial target class instead original class adversarial sample found. steps repeated updated input several approaches adversarial sample crafting proposed previous work construct framework encompasses approaches future work build allows compare strengths weaknesses method. resulting crafting framework illustrated figure broadly speaking adversary starts considering legitimate sample assume adversary capability accessing parameters targeted model replicating similar architecture therefore access parameter values. adversarial sample crafting two-step process terms step identiﬁes directions data manifold around sample model learned sensitive likely result class change step exploits knowledge effective adversarial perturbation. steps repeated necessary replacing x+δx starting iteration sample satisﬁes adversarial goal classiﬁed deep neural networks target class speciﬁed adversary using class indicator vector note that mentioned previously important total perturbation used craft adversarial sample legitimate sample minimized least approximatively. essential adversarial samples remain undetected notably humans. crafting adversarial samples using large perturbations would trivial. therefore deﬁnes norm appropriate describe differences points input domain model adversarial samples formalized solution following optimization problem s.t. models make problem non-linear non-convex making closed-solution hard cases. describe details attack framework approximating solution optimization problem using previous work illustrate steps. direction sensitivity estimation step considers sample m-dimensional input. goal dimensions produce expected adversarial behavior smallest perturbation. achieve this adversary must evaluate sensitivity trained model changes made input components building knowledge network sensitivity done several ways. goodfellow introduced fast sign gradient method computes gradient cost function respect input neural network. finding sensitivities achieved applying cost function inputs labeled using adversarial target labels. papernot took different approach introduced forward derivative jacobian thus directly providing gradients output components respect input component. approaches deﬁne sensitivity network given input dimensions miyato introduced another sensitivity estimation measure named local distribution smoothness based kullback-leibler divergence measure difference probability distributions. compute approximation network’s hessian matrix. however present results adversarial sample crafting instead focus using local distribution smoothness training regularizer improving classiﬁcation accuracy. perturbation selection adversary must knowledge network sensitivity input variations evaluate dimensions likely produce target misclassiﬁcation minimum total perturbation vector techniques takes different approach here depending distance metric used evaluate minimum perturbation goodfellow choose perturb input dimensions small quantity direction sign gradient computed. effectively minimizes euclidian distance original adversarial samples. papernot take different approach follow complex process involving saliency maps select limited number input dimensions perturb. saliency maps assign values combinations input dimensions indicating whether contribute adversarial goal perturbed. effectively diminishes number input features perturbed craft samples. amplitude perturbation added input dimensions ﬁxed parameter approaches. depending input nature method suitable guarantee existence adversarial samples crafted using acceptable perturbation acceptable perturbation deﬁned terms distance metric input dimensions depending problem nature different metrics apply different perturbation shapes acceptable not. describe approach distillation introduced hinton distillation motivated goal reducing size architectures ensembles architectures reduce computing ressource needs turn allow deployment resource constrained devices like smartphones. general intuition behind technique extract class probability vectors produced ﬁrst ensemble dnns train second reduced dimensionality without loss accuracy. intuition based fact knowledge acquired dnns training encoded weight parameters learned also encoded probability vectors produced network. therefore distillation extracts class knowledge probability vectors transfer different architecture training. perform transfer distillation labels inputs training dataset second using classiﬁcation predictions according ﬁrst dnn. beneﬁt using class probabilities instead hard labels intuitive probabilities encode additional information class addition simply providing sample’s correct class. relative information classes deduced extra entropy. perform distillation large network whose output layer softmax ﬁrst trained original dataset would usually done. example network depicted figure softmax layer merely layer considers vector outputs produced last hidden layer named logits normalizes probability vector ouput assigning probability class dataset input within softmax layer given neuron corresponding class indexed computes component following output vector logits corresponding hidden layer outputs classes dataset parameter named temperature shared across softmax layer. temperature plays central role underlying phenomena distillation show later section. context distillation refer temperature distillation temperature. constraint training ﬁrst high temperature larger used softmax layer. high temperature forces produce probability vectors relatively large values class. indeed high temperatures logits vector become negligible compared temperature therefore components probability vector expressed equation converge higher temperature softmax ambiguous probability distribution close whereas smaller temperature softmax discrete probability distribution close remainder close probability vectors produced ﬁrst used label dataset. labels called soft labels opposed hard class labels. second network less units trained using newly labelled dataset. alternatively second network also trained using combination hard class labels probability vector labels. allows network beneﬁt labels converge towards optimal solution. again second network trained high softmax temperature identical used ﬁrst network. second model although smaller size achieves comparable accuracy original model less computationally expensive. temperature back test time produce discrete probability vectors classiﬁcation. armed background dnns adversarial settings introduce defensive mechanism reduce vulnerabilities exposing dnns adversarial samples. note previous work combating adversarial samples proposed regularizations dataset augmentations. instead take radically different approach distillation training technique described previous section improve robustness dnns. describe adapt distillation defensive distillation address problem vulnerability adversarial perturbations. provide justiﬁcation approach using elements learning theory. formalize discussion defenses adversarial samples propose metric evaluate resilience dnns adversarial noise. build intuition metric namely robustness network brieﬂy comment underlying vulnerabilities exploited attack framework presented above. formulate requirements defenses capable enhancing classiﬁcation robustness. norm must speciﬁed accordingly context. higher average minimum perturbation required misclassify sample data manifold robust adversarial samples. impact architecture techniques introducing limited modiﬁcations architecture preferred approach introducing architectures studied literature requires analysis behaviors. designing architectures benchmarking approach left future work. maintain accuracy defenses adversarial samples decrease dnn’s classiﬁcation accuracy. discards solutions based weight decay regularization cause underﬁtting. maintain speed network solutions signiﬁcantly impact running time classiﬁer test time. indeed running time test time matters usability dnns whereas impact training time somewhat acceptable viewed ﬁxed cost. impact training nevertheless remain limited ensure dnns still take advantage large training datasets achieve good accuracies. instance solutions based jacobian regularization like double backpropagation using radial based activation functions degrade training performance. defenses work adversarial samples relatively close points training dataset indeed samples away training dataset like produced irrelevant security easily detected least humans. however limiting sensitivity inﬁnitesimal perturbation provides constraints near training examples solve adversarial perturbation problem. also hard expensive make derivatives smaller limit sensitivity inﬁnitesimal perturbations. show approach description defense technique require modiﬁcation neural network architecture overhead training overhead test time. evaluation conducted section also show defense technique remaining defense requirements evaluating accuracy dnns without defense deployed studying generalization capabilities networks show defense impacted adversarial samples. fig. visualizing hardness metric representation illustrates hardness metric radius disc centered original sample going closest adversarial sample among possible adversarial samples crafted sample inside disc class output classiﬁer constant. however outside disc samples classiﬁed differently framework discussed previously underlined fact attacks based adversarial samples primarily exploiting gradients computed estimate sensitivity networks input dimensions. simplify discussion refer gradients adversarial gradients remainder document. adversarial gradients high crafting adversarial samples becomes easier small perturbations induce high network output variations. defend perturbations must therefore reduce variations around input consequently amplitude adversarial gradients. words must smooth model learned training helping network generalize better samples outside training dataset. note adversarial samples necessarily found nature adversarial samples speciﬁcally crafted break classiﬁcation learned network. therefore necessarily extracted input distribution architecture tries model training. robustness informally deﬁned notion robustness adversarial perturbations capability resist perturbations. words robust display good accuracy inside outside training dataset well model smooth classiﬁer function would intuitively classify inputs relatively consistently neighborhood given sample. notion neighborhood deﬁned norm appropriate input domain. previous work formalized close deﬁnition robustness context machine learning techniques intuition behind metric robustness achieved ensuring classiﬁcation output remains somewhat constant closed neighborhood around given sample extracted classiﬁer’s input distribution. idea illustrated figure larger neighborhood inputs within natural distribution samples inputs considered otherwise ideal robust classiﬁer would constant function merit robust adversarial perturbations interesting classiﬁer. extend deﬁnition robustness introduced adversarial behavior source-target class pair fig. overview defense mechanism based transfer knowledge contained probability vectors distillation ﬁrst train initial network data softmax temperature probability vector includes additional knowledge classes compared class label predicted network train distilled network temperature data introduce defensive distillation technique propose defense dnns used adversarial settings adversarial samples cannot permitted. defensive distillation adapted distillation procedure presented section suit goal improving classiﬁcation resilience face adversarial perturbations. intuition knowledge extracted distillation form probability vectors transferred smaller networks maintain accuracies comparable larger networks also beneﬁcial improving generalization capabilities dnns outside training dataset therefore enhances resilience perturbations. note throughout remainder paper assume considered dnns used classiﬁcation tasks designed softmax layer output layer. main difference defensive distillation original distillation proposed hinton keep network architecture train original network well distilled network. difference justiﬁed resilience instead compression. resulting defensive distillation training procedure illustrated figure outlined follows input defensive distillation training algorithm samples class labels. speciﬁcally sample denote discrete label also referred hard label. indicator vector non-zero element corresponds correct class’ index indicates sample class index given training train deep neural network softmax output layer temperature discussed before probability vector class possible labels. precisely model parameters output probability distribution label label class gives probability label simplify notation later denote probability input class according model parameters form training consider samples form instead using hard class label soft-target encoding belief probabilities label class. again beneﬁt using soft-targets training labels lies additional knowledge found probability vectors compared hard class labels. additional entropy encodes relative differences classes. instance context digit recognition developed later section given image handwritten digit model evaluate probability class probability label indicates structural similarity training network explicit relative information classes prevents models ﬁtting tightly data contributes better generalization around training points. note knowledge extraction performed distillation controlled parameter softmax temperature described section high temperatures force dnns produce probabilities vectors large values class. sections make intuition precise theoretical analysis empirical evaluation. output. however forces make overly conﬁdent predictions sample class. argue fundamental lack precision training architecture remains unconstrained weights updated. move explain defensive distillation solves issue distilled model trained. mentioned before original training dataset distilled model trained using samples labeled soft-targets instead. constructed step defensive distillation. words label longer indicator vector corresponding hard class label rather soft label input probability vector therefore trained step solving following optimization problem note difference using soft labels trivial anymore components double null. instead using probabilities ensures training algorithm constrain output neurons proportionally likelihood updating parameters argue contributes improving generalizability classiﬁer model outside training dataset avoiding situations model forced make overly conﬁdent prediction class sample includes characteristics classes note model theoretically eventually converge towards indeed locally point optimal solution model this observe training aims minimize cross entropy equal shannon entropy denotes kullback-leibler divergence. note quantity minimized divergence equal true therefore ideal training procedure would result model converging ﬁrst model however empirically case training algorithms approximate solution training optimization problem often non-linear non-convex. furthermore training algorithms access ﬁnite number samples. thus observe empirically better behavior adversarial settings model model conﬁrm result section explore analytically impact defensive distillation training resilience adversarial samples. stated above intuition probability vectors produced model encode supplementary entropy classes beneﬁcial training distilled model proceeding further note purpose section provide deﬁnitive argument using defensive distillation combat adversarial perturbations rather view initial step towards drawing connection distillation learning theory robustness future work build upon. analysis distillation split three folds studying network training model sensitivity generalization capabilities dnn. note training looking converge towards function resilient adversarial noise capable generalizing better. existence function guaranteed universality theorem neural networks states enough neurons enough training points approximate continuous function arbitrary precision. words according theorem know exists neural network architecture converges trained sufﬁcient number samples. result mind natural hypothesis distillation helps convergence models towards optimal function instead different local optimum training. precisely understand effect defensive distillation adversarial crafting need analyze depth training process. throughout analysis frequently refer training steps defensive distillation described section iii. start considering training procedure ﬁrst model corresponds step defensive distillation. given batch samples labeled correct classes training algorithms typically solve following optimization problem parameters model component sample hypothesis i.e. model parameters consider log-likelihood average entire training roughly speaking goal optimization adjust weights model push towards however readers notice since indicator vector input class equation simpliﬁed element indicator vector equal words index sample’s class. means performing updates training algorithm constrain output neuron different corresponding probability give studied impact defensive distillation optimization problems solved training investigate adversarial perturbations harder craft dnns trained defensive distillation high temperature. goal analysis provide intuition distillation high temperatures improves recall example handwritten digit recognition. suppose given sample hand-written writing looks like assume model assigns probability probability given sample input. indicates look similar intuitively allows model learn structural similarity digits. contrast hard label leads model believe misleading since sample poorly written. example illustrate need algorithms ﬁtting tightly particular samples turn prevent models overﬁtting offer better generalizations. recent breakthrough computational learning theory connection learnability stability. ﬁrst present elements stable learning theory facilitate discussion. shalev-schwartz proved learnability equivalent existence learning rule simultaneously asymptotic empirical risk minimizer stable. precisely learning problem input space output space hypothesis space instance loss function maps pair positive real loss. given training deﬁne empirical loss denote hypothesis minimal empirical risk minw∈h ready present following deﬁnitions deﬁnition learning rule asymptotic empirical risk minimizer rate function every training size theorem learning rule asymptotic empirical risk minimizer stable generalizes means generalization error minh∈h rate converges independent data generating distribution link theorem back discussion. observe that appropriately setting temperature follows datasets differing training item generated training sets satisfy strong stability condition. turn means statistically close. model’s sensitivity input variation quantiﬁed jacobian. ﬁrst show amplitude jacobian’s components naturally decrease temperature softmax increases. derive expression component jacobian model temperature inputs softmax layer—also referred logits—and simply outputs last hidden layer model sake notation clarity write dependency simply write zn−. also write last equation yields increasing softmax temperature ﬁxed values logits reduce absolute value components model jacobian matrix components inversely proportional temperature logits divided temperature exponentiated. simple analysis shows using high temperature systematically reduces model sensitivity small variations inputs defensive distillation performed training time. however test time temperature decreased back order make predictions unseen inputs. intuition affect model’s sensitivity weights learned training modiﬁed change temperature decreasing temperature makes class probability vector discrete without changing relative ordering classes. smaller sensitivity imposed using high temperature encoded weights training thus still observed test time. explanation matches intuition experiments detailed later section formal analysis needed. plan pursue future work. distillation generalization capabilities dnns provide elements learning theory analytically understand impact distillation generalization capabilities. formalize intuition models beneﬁt soft labels. motivation stems fact probability vectors encode model knowledge regarding correct class also encodes knowledge classes likely relatively other. table overview architectures architectures based succession layers. however mnist architecture uses less units layers cifar architecture input composed less features. moreover deduce objective function defensive distillation approach minimizes empirical risk. combining results together theorem allows conclude distilled model generalizes well. conclude discussion noting strictly prove distilled model generalizes better model trained without defensive distillation. right indeed property difﬁcult prove dealing dnns non-convexity properties optimization problems solved training. deal lack convexity approximations made train architectures model optimality cannot guaranteed. best knowledge difﬁcult argue learnability dnns ﬁrst place good learnability results known. however believe argument provides readers intuition distillation help generalization. defensive distillation improve resilience adversarial samples retaining classiﬁcation accuracy? result distillation reduces success rate adversarial crafting ﬁrst dataset second dataset. distillation negligible existent degradation model classiﬁcation accuracy settings. indeed accuracy variability defensive distillation reduce sensitivity inputs? result defensive distillation reduces sensitivity input perturbations experiments show performing distillation high temperatures lead decreases amplitude adversarial gradients factors defensive distillation lead robust dnns? result defensive distillation impacts average minimum percentage input features perturbed achieve adversarial targets dnns distillation increases robustness ﬁrst second ﬁrst network metric increases input features second network metric increases dataset description experiments described section performed canonical machine learning datasets mnist cifar datasets. mnist dataset collection black white images handwritten digits pixel encoded real number samples split training samples test classiﬁcation goal determine digit written. classes therefore range cifar dataset collection color images. pixel encoded color components preprocessing values test set. samples split training samples test samples. images classiﬁed mutually exclusive classes airplane automobile bird deer frog horse ship truck. representative samples dataset shown figure architecture characteristics implement deep neural network architectures whose speciﬁcities described table training hyper-parameters included table ﬁrst architecture layer architecture trained mnist dataset second architecture layer architecture trained cifar dataset. architectures based convolutional neural networks widely studied literature. momentum parameter decay ensure model convergence dropout prevent overﬁtting. performance consistent dnns evaluated datasets before. mnist architecture constructed using convolutional layers ﬁlters followed pooling layer convolutional layers ﬁlters followed pooling layer fully connected layers rectiﬁed linear units softmax layer classiﬁcation classes. experimental trained batches samples learning rate epochs. resulting achieves correct classiﬁcation rate data comparable state-of-the-art accuracy. cifar architecture succession convolutional layers ﬁlters followed pooling layer convolutional layers ﬁlters followed pooling layer fully connected layers rectiﬁed linear units softmax layer classiﬁcation. trained batches samples cifar dataset learning rate momentum epochs dropout rate architecture achieves accuracy cifar test comparable state-of-the-art performance unaugmented datasets. train dnns theano designed simplify large-scale scientiﬁc computing lasagne simpliﬁes design implementation deep neural networks using computing capabilities offered theano. setup allows efﬁciently implement network training well computation gradients needed craft adversarial samples. conﬁgure theano make computations ﬂoat precision accelerated using graphics processing. indeed machines equipped nvidia tesla gpus. adversarial crafting implement adversarial sample crafting detailed adversarial goal alter sample originally classiﬁed source class perturbed sample classiﬁed distinct target class achieve goal attacker ﬁrst computes jacobian neural network output respect input. perturbation constructed ranking input features perturbed using saliency based previously computed network jacobian giving preference features likely alter network output. feature perturbed mnist architecture cifar dataset. note attack implemented evaluation based perturbing pixels large amount previous attacks based perturbing pixels small amount. discuss section impact defense crafting algorithms algorithm conﬁrm analytical results presented preceding sections. steps repeated several times resulting sample classiﬁed target class stop perturbation selection number features perturbed larger justiﬁed larger perturbations would detectable humans potential anomaly detection systems. method previously reported achieve success rate used craft adversarial samples altering samples mnist test average distortion input features altering maximum features also yields high adversarial success rate cifar test set. note throughout evaluation number features altered producing adversarial samples compare original samples. impact adversarial crafting architectures corresponding mnist cifar datasets consider original trained model fcif well distilled model obtain distilled models training defensive distillation class knowledge transfer temperature resulting classiﬁcation accuracy classiﬁcation mnist model accuracy cifar model comparable non-distilled models. second experiments measured success rate adversarial sample crafting samples randomly selected dataset. considered sample crafting algorithm craft adversarial samples corresponding classes distinct sample’ source class. thus craft total samples model. architectures trained mnist data using defensive distillation reduces success rate adversarial sample crafting original model distilled model thus resulting decrease. similarly models trained cifar data using distillation reduces success rate adversarial sample crafting original model distilled model represents decrease. distillation temperature next experiments measure temperature impacts adversarial sample generation. note softmax layer’s temperature test time i.e. temperature matters training. objective identify optimal training temperature resulting resilience adversarial samples dataset. adversarial sample crafting experiment architectures vary distillation temperature time. number adversarial targets successfully reached following distillation temperatures measured. figure plots success rate adversarial samples respect temperature fig. exploration temperature parameter space targets mnist cifar based models several distillation temperatures plot percentage targets achieved crafting adversarial sample altering features. baselines models trained without distillation dashes. note horizontal logarithmic scale. architectures provides exact ﬁgures. words rate plotted number adversarial sample targets reached. interesting observations made increasing temperature generally speaking make adversarial sample crafting harder elbow point rate largely remains constant observation validates analytical results section showing distilled network resilience adversarial samples success rate adversarial crafting reduced without distillation distillation mnist based without distillation distillation cifar dnn. temperature corresponding curve elbow linked role temperature plays within softmax layer. indeed temperature used divide logits given inputs softmax layer order provide discreet smoother distributions probabilities classes. thus make hypothesis curve’s elbow reached temperature increasing would make distribution smoother probabilities already close number classes. conﬁrm hypothesis computing average maximum probability output cifar equal thus elbow point correspond probabilities near classiﬁcation accuracy next experiments sought measure impact approach accuracy. knowledge transfer temperature used previous experiments compute variation classiﬁcation accuracy models fcif respectively trained without distillation distillation temperature model accuracy computed using samples corresponding test recall baseline rate meaning accuracy rate corresponding training fig. inﬂuence distillation accuracy plot accuracy variations architectures training without defensive distillation. rates evaluated corresponding test various temperature values. observe variations accuracy introduced distillation moderate. instance accuracy mnist based model degraded less temperatures instance accuracy would state recently. similarly accuracy cifar based model degraded also potentially improves variations positive notably cifar model although providing quantitative understanding potential accuracy improvement outside scope paper believe stems generalization capabilities favored fig. exploration impact temperature amplitude adversarial gradients illustrate adversarial gradients vanish distillation performed higher temperatures. indeed temperature considered draw repartition samples ranges mean adversarial gradient amplitudes associated distinct color. data collected using samples cifar test corresponding model. summarize distillation improves resilience dnns adversarial perturbations also without severely impacting classiﬁcation correctness thus defensive distillation matches second defense requirement section deploying defensive distillation defenders empirically temperature value offering good balance robustness adversarial perturbations classiﬁcation accuracy. case mnist model instance temperature would according figure distillation sensitivity second battery experiments sought demonstrate impact distillation dnn’s sensitivity inputs. hypothesis defense mechanism reduces gradients exploited adversaries craft perturbations. conﬁrm hypothesis evaluate mean amplitude gradients models trained without defensive distillation. experiment split samples cifar test bins according mean value adversarial gradient amplitude. train varying temperatures plot resulting frequencies figure note distillation reduces average absolute value adversarial gradients instance mean adversarial gradient amplitude without distillation larger samples among samples considered whereas case samples distillation performed temperature similarly samples corresponding mean adversarial gradient amplitude smaller model trained without distillation whereas vast majority samples namely samples mean adversarial gradient amplitude smaller model trained defensive distillation temperature generally speaking observe largest frequencies samples shifts higher mean amplitudes adversarial gradients smaller ones. amplitude adversarial gradients smaller means model learned training smoother around points distribution considered. turns means evaluating sensitivity directions complex crafting adversarial samples require adversaries introduce perturbation original samples. another observation overtraining help overﬁtting adversarial gradients progressively increase amplitude early stopping similar techniques help prevent exploding. discussed section case training epochs sufﬁcient distilled models achieve comparable accuracies original models ensured adversarial gradients explode. experiments show distillation smoothing impact classiﬁcation models learned training. indeed gradients characterizing model sensitivity input variations reduced factors larger defensive distillation applied. lastly explore interplay smoothness classiﬁers robustness. intuitively robustness average minimal perturbation required produce adversarial sample distribution modeled inputs drawn distribution architecture trying model ∆adv deﬁned equation minimum perturbation required misclassify sample classes. evaluate whether distillation effectively increases robustness metric evaluation architectures. without similarly cifar architecture model trained without distillations displays robustness whereas model trained defensive distillation robustness represents increase result suggests indeed distillation able provide sufﬁcient additional knowledge improve generalization capabilities dnns outside training manifold thus developing robustness perturbations. distillation conﬁdence next investigate impact distillation temperature classiﬁcation conﬁdence. hypothesis distillation also impacts conﬁdence class predictions made distilled model. correct class sample resulting conﬁdence values shown table lowest conﬁdence highest monotonically increasing trend suggests distillation indeed increase predictive conﬁdence. note similar analysis mnist inconclusive conﬁdence values already near leaves little opportunity improvement. preceding analysis distillation shows increase resilience dnns adversarial samples. training extracts knowledge learned classes probability vectors produced dnn. resulting models stronger generalizations capabilities outside training set. limitation defensive distillation applicable models produce energy-based probability distribution temperature deﬁned. indeed paper’s implementation distillation dependent engergy-based probability distribution reasons softmax produces probability vectors introduces temperature parameter. thus using defensive distillation machine learning models different dnns would require additional research efforts. however note many machine learning models unlike dnns don’t model capacity able resist adversarial examples. instance goodfellow showed shallow models like linear models also vulnerable adversarial examples unlikely hardened them. defense specialized dnns guaranteed universal approximation property least able represent function correctly processes adversarial examples thus signiﬁcant step towards building machine learning models robust adversarial samples. fig. quantifying impact distillation temperature robustness plot value robustness described equation several temperatures compare baseline robustness value models trained without distillation. exhaustively searching perturbations possible sample underlying distribution modeled approximate metric compute metric samples test model. results computation following quantity values evaluated considering possible adversarial targets corresponding sample using number features altered creating corresponding adversarial samples distance measure evaluate minimum perturbation required create mentionned adversarial sample. figure plot evolution robustness metric respect increase distillation temperature architectures. temperature increases robustness network deﬁned here increases. mnist architecture model trained without distillation displays robustness whereas model trained distillation displays robustness increase note that perturbations large enough potentially change true class could detected anomaly detection process. fact empirically shown previous work humans begin misclassify adversarial samples perturbations larger figure desirable adversary produce adversarial samples identiﬁable humans. furthermore changing additional features hard depending input nature. evaluation easy change feature images. however input spam email would become challenging adversary alter many input features. thus making dnns importance. robust number modiﬁed features. metrics suitable compare samples like norms. using different metrics produce different distortions pertinent application domains different computer vision. instance crafting adversarial samples real malware evade existing detection methods require different metrics perturbations future work investigate various distance measures. question also whether probabilities used transfer knowledge paper could replaced soft class labels. n-class classiﬁcation problem soft labels obtained replacing target value correct class target value incorrect classes empirically observed replacing target improvements neural network’s robustness signiﬁcant soft labels. speciﬁcally trained mnist used section using soft labels. misclassiﬁcation rate adversarial samples crafted using mnist test data attack parameters section whereas distilled model studied section misclassiﬁcation rate smaller believe relative information classes encoded probability vectors soft class labels. inspired early public preprint paper warde-farley goodfellow independently tested label smoothing found partially resists adversarial examples crafted using fast gradient sign method possible interpretation conﬂicting results label smoothing without distillation smart enough defend simple inexpensive methods adversarial example crafting powerful iterative methods used paper future work also evaluate performance defensive distillation face different perturbation types. instance defensive distillation good defense attack studied could still vulnerable attacks based l-bfgs fast gradient sign method genetic algorithms however techniques preliminary results promising worthy exploration; seems likely distillation also beneﬁcial defensive impact techniques. paper compare defense technique traditional regularization techniques adversarial examples traditional overﬁtting problem fact previous work showed wide variety traditional regularization methods including dropout weight decay either fail defend adversarial examples seriously harming accuracy original task finally would like point defensive distillation create additional attack vectors words start arms race defenders attackers. indeed attacks designed approximately optimal regardless targeted model. even attacker knows defensive distillation used clear could exploit adapt attack. increasing conﬁdence estimates across model’s input space defensive distillation lead strictly better models. machine learning security active research area security community attacks organized taxonomies according adversarial capabilities biggio studied binary classiﬁers deployed adversarial settings proposed framework secure work consider deep learning models rather binary classiﬁers like support vector machines logistic regression. generally attacks machine learning models partitioned execution time training test time model used make predictions. previous work studying dnns adversarial settings focused presenting novel attacks dnns test time mainly exploiting vulnerabilities adversarial samples attacks discussed depth section papers offered suggestions defenses investigation left future work authors whereas proposed evaluated full defense mechanism improve resilience dnns adversarial perturbations. nevertheless attempts made making resilient adversarial perturbations. goodfellow showed radial basis activation functions resistant perturbations deploying requires important modiﬁcations existing architecture explored denoising auto-encoders type architecture intended capture main factors variation data showed remove substantial amounts adversarial noise however resulting stacked architecture evaded using adversarial samples. authors therefore proposed architecture deep contractive networks based imposing layer-wise penalty deﬁned using network’s jacobian. penalty however limits capacity deep contractive networks compared traditional dnns. work investigated distillation technique previously used reduce dimensionality defense adversarial perturbations. formally deﬁned defensive distillation evaluated standard architectures. using elements learning theory analytically showed distillation impacts models learned deep neural network architectures training. empirical ﬁndings show defensive distillation signiﬁcantly reduce successfulness attacks dnns. reduces success adversarial sample crafting rates smaller mnist dataset smaller cifar dataset maintaining accuracy rates original dnns. surprisingly distillation simple implement introduces little overhead training. hence work lays foundation securing systems based deep learning. future work investigate impact distillation models adversarial sample crafting algorithms. notable endeavor extend approach outside scope classiﬁcation tasks. trivial requires ﬁnding substitute probability vectors used defensive distillation similar properties. lastly explore different deﬁnitions robustness measure aspects resilience adversarial perturbations. authors would like thank damien octeau goodfellow ulfar erlingsson insightful comments. research sponsored army research laboratory accomplished cooperative agreement number wnf--- views conclusions contained document authors interpreted representing ofﬁcial policies either expressed implied army research laboratory u.s. government. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation sainath a.-r. mohamed kingsbury ramabhadran deep convolutional neural networks lvcsr acoustics speech signal processing ieee international conference sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks international conference learning representations dahl stokes deng large-scale malware classiﬁcation using random projections neural networks acoustics speech signal processing ieee international conference papernot mcdaniel fredrikson celik swami limitations deep learning adversarial settings proceedings ieee european symposium security privacy. szegedy zaremba sutskever bruna erhan goodfellow fergus intriguing properties neural networks proceedings international conference learning representations. computational biological learning society goodfellow shlens szegedy explaining harnessing adversarial examples proceedings international conference learning representations. computational biological learning society biggio fumera pattern recognition systems attack design issues research challenges international journal pattern recognition artiﬁcial intelligence vol. biggio corona maiorca nelson evasion attacks time machine learning fogla evading network anomaly detection systems formal reasoning practical techniques proceedings conference computer communications security. rigazio towards deep neural network architectures robust adversarial examples proceedings international conference learning representations. computational biological learning society rumelhart hinton williams learning representations back-propagating errors cognitive modeling vol. bergstra bengio random search hyper-parameter optimization journal machine learning research vol. glorot bordes bengio domain adaptation largescale sentiment classiﬁcation deep learning approach proceedings international conference machine learning masci meier cires¸an stacked convolutional autoencoders hierarchical feature extraction artiﬁcial neural networks machine learning–icann springer erhan bengio courville p.-a. manzagol vincent bengio unsupervised pre-training help deep learning? journal machine learning research vol. miyato maeda koyama distributional smoothing nguyen yosinski clune deep neural networks easily fooled high conﬁdence predictions unrecognizable images computer vision pattern recognition ieee cybenko approximation superpositions sigmoidal function mathematics control signals systems vol. bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio theano math expression compiler proceedings python scientiﬁc computing conference vol. austin battenberg dieleman nouri olson oord raffel schlter kaae snderby lasagne lightweight library build train neural networks theano available https//github.com/lasagne/lasagne biggio rieck ariu wressnegger poisoning behavioral malware clustering proceedings workshop artiﬁcial intelligent security workshop. warde-farley goodfellow adversarial perturbations deep neural networks advanced structured prediction hazan papandreou tarlow eds.", "year": 2015}