{"title": "ResBinNet: Residual Binary Neural Network", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Recent efforts on training light-weight binary neural networks offer promising execution/memory efficiency. This paper introduces ResBinNet, which is a composition of two interlinked methodologies aiming to address the slow convergence speed and limited accuracy of binary convolutional neural networks. The first method, called residual binarization, learns a multi-level binary representation for the features within a certain neural network layer. The second method, called temperature adjustment, gradually binarizes the weights of a particular layer. The two methods jointly learn a set of soft-binarized parameters that improve the convergence rate and accuracy of binary neural networks. We corroborate the applicability and scalability of ResBinNet by implementing a prototype hardware accelerator. The accelerator is reconfigurable in terms of the numerical precision of the binarized features, offering a trade-off between runtime and inference accuracy.", "text": "recent efforts training light-weight binary neural networks offer promising execution/memory efﬁciency. paper introduces resbinnet composition interlinked methodologies aiming address slow convergence speed limited accuracy binary convolutional neural networks. ﬁrst method called residual binarization learns multi-level binary representation features within certain neural network layer. second method called temperature adjustment gradually binarizes weights particular layer. methods jointly learn soft-binarized parameters improve convergence rate accuracy binary neural networks. corroborate applicability scalability resbinnet implementing prototype hardware accelerator. accelerator reconﬁgurable terms numerical precision binarized features offering trade-off runtime inference accuracy. convolutional neural networks shown promising inference accuracy learning applications various domains. models generally over-parameterized facilitate convergence training phase denil line optimization methodologies tensor decomposition zhang parameter quantization sparse convolutions binary networks rastegari proposed reduce complexity neural networks efﬁcient execution. among works binary neural networks result particular beneﬁts reduce memory footprint factor compared full-precision model; speciﬁcally important since memory access plays essential role execution cnns resource-constrained devices. binary networks replace costly multiplications simple xnor operations umuroglu reducing execution time energy consumption signiﬁcantly. considering prior exist major challenges associated binary neural networks. first convergence rate existing solutions training binary cnns considerably slower full-precision counterparts. second order achieve comparable classiﬁcation accuracy binarized neural networks often compensate numerical precision loss employing high dimensional feature maps wide topology turn reduces effective compression rate. result full-precision networks often surpass binary networks terms convergence rate ﬁnal achievable accuracy. paper propose resbinnet novel solution increasing convergence rate ﬁnal accuracy binary networks. global resbinnet depicted figure ﬁrst phase call soft binarization includes methodologies propose address aforementioned challenges training binary cnns. first introduce residual binarization scheme allows number possible values activation units reconﬁgurable runtime. purpose learn multi-level residual representation features within adaptively increase numerical precision activation units. second introduce novel weight binarization approach called temperature adjustment aims gradually enforce binarization constraints weight parameters throughout training phase. interlinked methods signiﬁcantly improve convergence rate ﬁnal accuracy resbinnet compared prior art. soft training phase ﬁnished convert weights actual binary figure global resbinnet binary training. residual binarization learns multi-level representation feature maps. temperature adjustment performs change variable trainable weights gradually pushes towards binary values training phase. resbinnet designed fulﬁll certain goals enable reconﬁgurability binary neural networks; words number residual binary representatives adjustable offer trade-off inference accuracy computation time. multi-level binarized features compatible xnor multiplication approach proposed existing literature. resbinnet speed convergence rate binarized cnns. current hardware accelerators binary cnns able beneﬁt resbinnet minimum modiﬁcation design. summary contributions paper follows remainder paper organized follows section describe residual binarization method binarizing activations. section explains temperature adjustment technique binarizing weights. section discuss particular resbinnet operations efﬁciently implemented existing hardware accelerators. experiments discussed section finally discuss related work conclusion sections binarization scheme converts value binarized estimation take possible values representation allows represent using single particular given layer within store single full-precision value representative features reduce memory footprint storing bits instead feature. assuming weights input features layer binarized product feature vector weight vector efﬁciently computed using xnor-popcount operations previously suggested rastegari scalar values corresponding sign vectors. binary representations denote {bxbw} simply computed encoding sign vectors binary vectors. product computed figure schematic computing levels residual binary estimates deeper levels estimation becomes accurate. ﬁgure drop subscript represent simplicity. figure illustration binarized activation function. conventional -level binarization. residual binarization levels. note parameters universal across features particular layer. multi-level residual binarization imposing binary constraints weights activations neural network inherently limits model’s ability provide inference accuracy ﬂoatingpoint counterpart achieve. address issue propose multi-level binarization scheme residual errors sequentially binarized increase numerical precision estimation. figure presents procedure compute estimate input level graph computes corresponding estimate taking sign input level) multiplying parameter adding estimate previous level. addition computes residual error feeds input next level. estimates deeper levels therefore accurate representations input note estimate level represented using stream bits corresponding signs inputs ﬁrst levels. residual binary activation function similar previous works sign function activation function paper residual binarization account activation function. difference approach single-bit approach shown figure level separate full-precision representative learned training phase. setting gradients backward propagation computed conventional single-bit binarization rastegari regardless number residual levels. forward propagation however computed results approach provide accurate approximation. instance employ levels residual binarization activation function take different values. general total number possible values activation functions l-level residual binarization scheme multi-level xnor-popcount resbinnet product l-level residual-binarized feature vector vector binary weights rendered using subsequent xnor-popcount γeisei γwsw correspond sign {bei binary representations corresponding {sei respectively. note subsequent xnor-popcount operations performed sequentially thus memory used operating reused operating bei+. result actual memory footprint multi-level residual binarization single-level binarization provided streams processed sequentially. residual encoding order convert matrix-vector multiplications xnor-popcount operations need encode feature stream binary values {bei|i pseudo code operation call residual encoding presented algorithm approximating weights neural network binary values often results loss accuracy pertinent model. section explain methodology minimize approximation error training trained weights exhibit lower binarization errors. denote parameter within certain layer neural network. instead directly using compute layer’s output perform change variable compute output using here bounded monotonically-increasing function hyperbolic tangent function applied element-wise. parameter trainable scalar adjusts maximum minimum values elements take. parameter call temperature henceforth controls slope function note across elements parameter certain layer. particularly important maintain simplicity ﬁnal binary accelerator discuss section effect binarization figure illustrate effect parameters nonlinear change variable anh. note acts semi-binarized parameter soft training phase. increase temperature parameter becomes closer binary sign function meaning pertinent exhibit less error approximated note trainable parameters setting. parameter used forward propagation phase soft training backward propagation step updated. effect training gradients training loss function respect respectively words magnitude gradient certain element close actually ﬂows controlled pertinent gradient ﬁltered out; otherwise gradients effect temperature gradients figure illustrates temperature parameter affect gradient ﬁltering term training. increase temperature elements closer receive ampliﬁed gradients elements closer binary regime encounter damped gradients. means increasing temperature parameter push weights binary regime bigger force; therefore neural network trained high temperature values exhibits smaller binarization error. figure example change variable tanh nonlinearity. effect temperature parameter higher values provide better soft-binary estimations. effect bounding parameter trainable value weight matrix effect temperature parameter gradient ﬁltering term temperature adjustment setting high temperature beginning training eliminate gradients preventing training loss optimized. address problem start soft binarization phase temperature slightly increase mini-batch. approach gradually adapts weights binary values training. figure presents example histogram semi-binarized weights different training epochs. seen distribution gradually shifted towards binary values training proceeds. soft binarization parameter used initial point existing hard binarization schemes method proposed illustrate section soft binarization methodology signiﬁcantly increases convergence rate binary cnns. section show modiﬁcations required incorporate residual binarization existing hardware accelerators binary cnns minimal. resbinnet provides trade-off inference accuracy execution time keeping implementation cost almost intact; result resbinnet readily offers users decision latency learning application compromising inference accuracy. example consider fpga accelerator binary cnns proposed refer reader mentioned paper details original design. here describe modiﬁcations integrated speciﬁc components design accommodate residual binarization. modiﬁed accelerator publicly available github repository resbinnet. figure depicts schematic view original hardware modiﬁed accelerator. note original implementation layer takes single binary vector computes single output vector bout modiﬁed version processes streams binary vectors desired number residual levels. matrix-vector multiplication original modiﬁed accelerators matrix-vector multiplication unit computationally intensive among operations. original design unit takes binary vector outputs full-precision vector accommodate residual binarization modify module follows xnor-popcount operation sequentially performed stream binary vectors bini. xnor-popcount results different γiyi. note computation overhead summation negligible compared xnor-popcount operation thus runtime multilevel xnor-popcount l-level residual representations approximately times runtime conventional xnor-popcount original design. maximize beneﬁts binarization actual hardware implementation resbinnet properties within layer values used activations same. values ﬁxed inference phase. contrast approach employs different scaling factors activations certain layer ﬁxed computed inference phase. argue computing adaptive scaling factors imposes signiﬁcant runtime area overhead diminishes beneﬁts binarization. batch-normalization activation function batch-normalization inference phase viewed multiplying vector constant vector subtracting vector obtain normalized vector ynorm. original design require multiplication step since sign ynorm matters compute output activation function design multiplication step necessary since value ynorm affects output activation function encoded using algorithm sent next layer used input. max-pooling original implementation simply computes boolean binary values perform pooling features within window. resbinnet however features represented l-bit residual representations. result performing boolean binary encodings longer equivalent performing max-pooling features. nevertheless pooling operation performed encoded values directly. assume full-precision values l-level binary encodings respectively. considering ordered positive values easily conclude implement using keras library tensorﬂow backend. synthesis reports fpga accelerator gathered using vivado design suite temperature adjustment hard tanh nonlinearity gradually increase temperature incrementing epoch. evaluate resbinnet comparing accuracy number training epochs size network execution time. proof-of-concept evaluations performed four datasets cifar- svhn mnist ilsvrc- table presents architecture trained neural networks ﬁrst three tasks. architectures picked network imagenet classiﬁcation task inspired alexnet architecture obtained table network architectures evaluation benchmarks. denotes convolution output channels stands pooling represents batch normalization means dense layer outputs. residual binarizations shown using effect model size accuracy discussed ﬁnal accuracy binary particular application correlated shape network. instance authors paper report accuracy mnist architecture table varies range number neurons hidden layers varied similarly architecture cifar- smaller version architecture originally trained using smaller architecture drops accuracy evaluations show resbinnet reduce accuracy drop using residual binary levels activations smaller model. effect number epochs accuracy compared full-precision neural networks binarized cnns usually need training epochs achieve accuracy. example cifar- architecture trained epochs fullprecision version network achieve comparable accuracy roughly iterations here argue soft binarization resbinnet signiﬁcantly reduce number training epochs binary cnns. table compares resbinnet prior arts namely binarynet finn. baselines training methodology network architectures finn considerably smaller leads lower accuracy rates finn. ﬁrst three benchmarks evaluate resbinnet using small architectures finn. higher accuracy binarynet benchmarks compared approach direct result employing large architecture training many epochs. ilsvrc- classiﬁcation task train network architecture binarynet baseline. training resbinnet consists soft binarization phase single ﬁne-tuning epoch. output soft-binarization still full-precision accuracy reported table. since full-precision values close actual binary values hard-binarized network recover accuracy retraining epochs note ﬁne-tuning phase uses algorithm baselines. comparison resbinnet baseline architecture followed cifar- resbinnet achieves higher accuracy level residual binarization. argue that even -level binarization accuracy viable ﬁne-tune soft-binarized model epochs addition accuracy resbinnet improved higher number residual levels. svhn mnist resbinnet achieves higher accuracy even fewer epochs compared cifar-. ﬁnal accuracy convergence speed also exhibit improvement number residual levels increased imagenet report accuracy binarynet since original paper evaluate task. seen resbinnet obtains higher accuracy even single-level residual binarization. figure resource utilization overhead resbinnet different residual levels versus baseline design) implemented xilinx evaluation kit. latencyaccuracy trade-off offered resbinnet different residual levels. resource utilization resbinnet evaluated figure compares utilization different resources fpga resource compare baseline different number residual binarization levels resbinnet. asides utilization required full-precision multiplications batch normalization three resources show modest increase utilization meaning residual binarization method offers scalable design real-world systems. figure compares latency resbinnet baseline accelerator. particular consider multi-level residual binarization residual levels denoted respectively. numbers bars show accuracy corresponding binarized cifar- task. seen resbinnet enables users achieve higher accuracy tolerating higher latency almost linear respect number residual levels. training cnns binary weights and/or activations subject recent works rastegari courbariaux umuroglu authors binaryconnect suggest probabilistic methodology leverages full-precision weights generate binary representatives forward pass back-propagation full-precision weights updated. ﬁrst work attempting binarize weight activations cnn. work authors also suggest replacing costly products xnor-popcount operations. xnor-net proposes scale factors training results improved accuracy. authors xnor-net provide hardware accelerator binarized cnn. although xnor-net achieves higher accuracy compared prior works sacriﬁces simplicity hardware accelerator binary cnns reasons utilizes multiple scaling factors parameter would increase memory footprint logic utilization. requires online computation scaling factors activations would signiﬁcant number full-precision operations execution phase. aforementioned works propose optimization solutions enable binarized values cnns which turn enable design simple efﬁcient hardware accelerators. downside works that aside changing architecture offer reconﬁgurability designs. another track research reconﬁgurability accelerators investigated. line research focuses using adaptive bit-width representations compressing parameters and/or simplifying pertinent arithmetic operations proposed solutions however enjoy simpliﬁed xnor-popcount operations binarized cnns. among aforementioned works uniﬁed solution reconﬁgurable binarized missing. best knowledge resbinnet ﬁrst offer solution reconﬁgurable time enjoys beneﬁts binarized cnns. goal design resbinnet remain consistent existing optimization solutions. binarized cnns trained framework compatible existing accelerators designed compressed weights samragh in-memory computation imani binary neural networks umuroglu paper introduces resbinnet novel reconﬁgurable binarization scheme aims improve convergence rate ﬁnal accuracy binary cnns. proposed training twofold ﬁrst phase called soft binarization introduce distinct methodologies designed binarizing weights feature within cnns namely residual binarization temperature adjustment. residual binarization learns multi-level representation features provide arbitrary numerical precision inference. temperature adjustment gradually imposes binarization constraints weights. second phase call hard binarization model ﬁne-tuned training epochs. experiments demonstrate joint residual binarization temperature adjustment improves convergence rate accuracy binarized cnn. argue resbinnet methodology adopted current hardware accelerators requires minimal modiﬁcation existing binarized solutions. developers integrate approaches proposed paper deep learning systems provide users trade-off application latency inference accuracy. matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems matthieu courbariaux itay hubara daniel soudry el-yaniv yoshua bengio. binarized neural networks training deep neural networks weights activations constrained or-. arxiv preprint arxiv. song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. song xingyu huizi jing ardavan pedram mark horowitz william dally. efﬁcient inference engine compressed deep neural network. proceedings international symposium computer architecture ieee press geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. itay hubara matthieu courbariaux daniel soudry el-yaniv yoshua bengio. quantized neural networks training neural networks precision weights activations. arxiv preprint arxiv. mohsen imani saransh gupta tajana rosing. ultra-efﬁcient processing in-memory data intensive applications. proceedings annual design automation conference yeseong mohsen imani tajana rosing. orchard visual object recognition accelerator based approximate in-memory processing. international conference computer-aided design yong-deok eunhyeok park sungjoo taelim choi yang dongjun shin. compression deep convolutional neural networks fast power mobile applications. arxiv preprint arxiv. baoyuan wang hassan foroosh marshall tappen marianna pensky. sparse convolutional neural networks. proceedings ieee conference computer vision pattern recognition mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. european conference computer vision springer mohammad samragh razlighi mohsen imani farinaz koushanfar tajana rosing. looknn neural network multiplication. design automation test europe conference exhibition ieee mohammad samragh mohammad ghasemzadeh farinaz koushanfar. customizing neural networks efﬁcient fpga implementation. field-programmable custom computing machines ieee annual international symposium ieee yaman umuroglu nicholas fraser giulio gambardella michaela blott philip leong magnus jahre kees vissers. finn framework fast scalable binarized neural network inference. proceedings acm/sigda international symposium field-programmable gate arrays jiaxiang cong leng yuhang wang qinghao jian cheng. quantized convolutional neural networks mobile devices. proceedings ieee conference computer vision pattern recognition xiangyu zhang jianhua xiang ming kaiming jian sun. efﬁcient accurate approximations nonlinear convolutional networks. proceedings ieee conference computer vision pattern recognition shuchang zhou yuxin zekun xinyu zhou yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. arxiv preprint arxiv.", "year": 2017}