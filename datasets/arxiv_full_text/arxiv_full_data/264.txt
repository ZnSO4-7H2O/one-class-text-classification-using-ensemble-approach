{"title": "High-Dimensional Vector Semantics", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper we explore the \"vector semantics\" problem from the perspective of \"almost orthogonal\" property of high-dimensional random vectors. We show that this intriguing property can be used to \"memorize\" random vectors by simply adding them, and we provide an efficient probabilistic solution to the set membership problem. Also, we discuss several applications to word context vector embeddings, document sentences similarity, and spam filtering.", "text": "paper explore \"vector semantics\" problem perspective \"almost orthogonal\" property high-dimensional random vectors. show intriguing property used \"memorize\" random vectors simply adding them provide eﬃcient probabilistic solution membership problem. also discuss several applications word context vector embeddings document sentences similarity spam ﬁltering. many natural language processing tasks words documents represented using \"bag words\" model. model document represented high-dimensional vector components corresponding frequency particular word document references within). example assuming english vocabulary words document represented dimensional vector component frequency word document. vector representation particularly useful text classiﬁcation tasks similarity documents simply estimated using product vectors. vectors normalized product equal cosine angle vectors therefore parallel vectors similar documents are. problem words represented high-dimensional vectors \"meaning\" computed context modeled using distribution words around references within). several computational methods based pointwise mutual information neural networks matrix factorizations agglomerative clustering developed compute \"meaning\" words. eﬀorts culminated identifying words share semantic syntactic properties solving complex problems like estimating similarity pairs words example pairs roughly recover queen king woman simply using linear vector algebra approach d-dimensional sparse random vector called random label assigned diﬀerent word text data. labels small number randomly distributed rest next step given word labels words context window added context vector. approach motivated earlier observation high-dimensional spaces many \"almost orthogonal\" directions dimensionality space inspired ideas explore \"vector semantics\" problem perspective \"alorthogonal\" property high-dimensional random vectors. exactly extend theoretical justiﬁcation method providing probabilistic solution membership problem discuss several potential applications word context vector embeddings document sentences similarity spam ﬁltering. contrary \"expensive\" machine learning methods method simple even require \"learning\" process however exhibits similar properties. means relatively large dimensionality probability random vectors \"almost orthogonal\" quite high. example have general show high dimensional space exponentially large number \"almost orthogonal\" randomly chosen vectors following random indexing approach next section show \"intriguing\" property used \"memorize\" random vectors simply adding them. consider following membership problem given random vectors random vector want check typical binary decision problem answer true false. normally solution requires calculation product vector answer true otherwise answer false. thus solution membership problem practically binary classiﬁer also acts \"set ﬁlter\" membership problem also reformulated \"query\" problem asking return vector similar \"query\" vector case answer obtained taking products searching index product highest value statistically membership problem requires average operations order provide correct answer \"query\" problem requires operations sorting procedure. however show probabilistically membership problem solved using operation order illustrate numerically result consider plot value product cases \"query\" vector member respectively nonmember results shown figure also included distributions distinct member non-member situations. since components vectors bernoulli distributed distribution values binomial also approximate binomial distribution normal distribution mean members respectively non-members standard deviation =k/d. therefore normal distributions k/d) respectively k/d). estimate classiﬁcation precision recall using overlap normal distributions function standard deviation =k/d shown figure perfect agreement analytical estimation interesting precision means large dimension solve membership problem high probability relatively large sets example solve high probability membership problems members quite impressive considering simplicity method. word-context vectors consider vocabulary unique words. word associate randomly drawn vector thus associate vocabulary \"almost orthogonal\" random vectors. thus given document word calculate context windows positions appears document since vectors \"almost orthogonal\" according previously obtained result \"set membership problem\" context accommodate quite large number vectors \"set ﬁltering\" properties signiﬁcantly deteriorate. example let’s assume context word wim} word appears number times. then context vector word consider example using book adventures sherlock holmes arthur conan doyle downloaded gutenberg project document processed removing \"stop words\" bring meaningful information contexts also eliminate non-alphanumeric words remaining words lemmatized. resulted corpus words vocabulary unique words. figure show distribution number words unique number words context word resulted vocabulary window length words context number words larger thus using random vectors dimensionality still quite high precision cases. interesting similarities derived resulted word-context vectors also relevant examples word-context vector operations accent german proﬁcient; acid pungent hydrochloric; aged grizzle middle. expectation product number words sentences common ζγi)−. also variance product νni/d. thus reasonable high dimensionality obtain good precision example let’s consider book adventures sherlock holmes following naive question \"who woman irene photograph special connection sherlock?\" question naive processing \"query\" sentence following words actually used search process woman irene photograph special connection sherlock also independent eachother since \"sum\" used. ﬁrst three sentences returned using method described \"and speaks irene adler refers photograph always honourable title woman.\"; \"and woman woman late irene adler dubious questionable memory.\"; \"the photograph irene adler evening dress letter superscribed sherlock holmes esq.\" also used product without normalization results ﬁrst three returned sentences also interesting sherlock holmes always woman.\"; \"and speaks irene adler refers photograph always honourable title woman.\"; \"and woman woman late irene adler dubious questionable memory.\" spam ﬁlters built order protect email users spam phishing messages. spam ﬁlters word-based ﬁlters simply block email contains certain words phrases. another approach based machine learning techniques bayesian classiﬁers must trained large sets already classiﬁed spam non-spam messages. discuss diﬀerent previously described applications associate vocabulary \"almost orthogonal\" random vectors assume message represented words equivalently \"almost orthogonal\" vectors representing words. therefore assume messages already classiﬁed associated vectors order evaluate simple method ling-spam corpus described paper ref. data contains four subdirectories corresponding four versions corpus bare lemmatiser disabled stop-list disabled; lemm lemmatiser enabled stop-list disabled; lemm-stop lemmatiser enabled stop-list enabled; stop lemmatiser disabled stop-list enabled. experiment used ﬁles ﬁrst subdirectory \"bare lemmatiser disabled stoplist disabled\". directory contains subdirectories corresponding partitions corpus used -fold cross validation experiment. repetition part reserved testing used training. subdirectories contains spam legitimate messages. total number ﬁles files whose names form \"spmsg*.txt\" spam messages. ﬁles legitimate messages. preprocessed messages using spacy python library messages processed removing \"stop words\" remaining words lemmatized resulting vocabulary unique words. results -fold cross validation shown figure three diﬀerent vector dimensionality values three cases average values quite close indicating decreasing dimensionality slight eﬀect classiﬁcation precision recall. also described method based \"almost orthogonal\" random vectors gives better results bayesian approach described ref. even though case learning involved. paper explored diﬀerent approach \"vector semantics\" problem based \"almost orthogonal\" property high-dimensional random vectors. shown \"almost orthogonal\" property used \"memorize\" random vectors simply adding them provided eﬃcient probabilistic solution membership problem. also discussed several applications word context vector embeddings document sentences similarity spam ﬁltering. easily extend approach problems like example sentiment analysis. contrary \"expensive\" machine learning methods method simple even require \"learning\" process however exhibits similar properties.", "year": 2018}