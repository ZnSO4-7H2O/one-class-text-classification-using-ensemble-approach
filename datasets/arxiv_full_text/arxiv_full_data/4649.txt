{"title": "Exploring compact reinforcement-learning representations with linear  regression", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.", "text": "paper presents algorithm online linear regression whose eﬃciency guarantees satisfy requirements kwik framework. algorithm improves complexity bounds current state-of-the-art procedure setting. explore several applications algorithm learning compact reinforcement-learning representations. show kwik linear regression used learn reward function factored probabilities action outcomes stochastic strips object oriented mdps none proven eﬃciently learnable setting before. also combine kwik linear regression kwik learners learn larger portions models including experiments learning factored transition reward functions together. linear regression decades powerful tool kits machine-learning researchers. ﬁeld reinforcement learning certainly made linear regression approximating value functions using online regression learn parameters model limited environments linear dynamics often unable make guarantees behavior resulting learning agent without strict assumptions. great hindrances applying linear regression learn models environment computational sample eﬃciency guarantees online regression learners port reinforcement-learning recently introduction kwik framework provided characterization suﬃcient conditions model-learning algorithm induce sample-eﬃcient behavior reinforcement-learning agent. ﬁrst algorithms developed framework kwik linear regression algorithm used learn transition function linear dynamics. paper present algorithm improves sample computational bounds previous algorithm apply stable learning problems reinforcement-learning agents employ compact representations. speciﬁcally kwik linear regression learn reward function factored transition probabilities domains encoded using stochastic strips object oriented mdps note learning parameters typically associated linear regression—this paper shows kwik-lr used help learn models beyond standard usage learning linear dynamics. kwik guarantees agents using algorithm settings guaranteed make polynomial number sub-optimal steps high probability. present algorithms theoretical arguments eﬀect including general reinforcementlearning algorithm agents need learn probabilities action outcomes eﬀects ambiguous sample data. experimental evidence also presented benchmark problems application areas mentioned above. major contributions improved simpliﬁed algorithm kwik linear regression sample-eﬃcient algorithms kwik-lr eﬃciently learn portions compact reinforcementlearning representations none previously shown eﬃciently learnable. also discuss extensions including combining kwiklr kwik algorithms learn parameters compact representations. kwik framework studying supervised learning algorithms designed unify analysis model-based reinforcement-learning algorithms. formally kwik learner operates input space output space every timestep input chosen presented learner. learner make accurate prediction input returning allowing true noisy version algorithm said kwik high probability returned agent’s lifetime bounded polynomial function size input problem. shown model-based reinforcement-learning setting underlying model learner kwik possible build agent around driving exploration areas using r-max style manipulation value function. agent will high probability take polynomial number suboptimal actions. paper deals compact representations parameter sizes smaller enumerated state space call agents satisfy conditions require superpolynomial computation planning pac-compact-mdp. ﬁrst uses kwik framework analysis online linear regression algorithm used learn linear transitions continuous state mdps algorithm uses least squares estimate weight vector inputs output known high certainty. certainty measured terms representing number proximity previous samples current point appropriateness previous samples making least squares estimate. certainty either measure algorithm reports work present kwik-lr algorithm sample computationally eﬃcient previous work. first though deﬁne notation. solution system. however solving system directly problematic because rankdeﬁcient least-squares solution unique even solution information conﬁdence. avoid ﬁrst problem regularization i.e. augmenting system arbitrary vector. regularization certainly distorts solution gives measure conﬁdence distortion large predictor conﬁdence output hand distortion important consequences. first choice little eﬀect second ﬂuctuations caused using instead also minor. algorithm describes method kwik-learning linear model. notice avoids problem storing grow without bound quantities suﬃcient calculating predictions updated incrementally algorithm kwik shown next. consider ﬁrst term constant number timesteps kqt~xtk show n/α. proceed showing traces matrices positive monotonically decreasing decrease considerably kqt~xtk large. trace lowerbound update note weighted i=ηi. kq~xk prediction error otherwise index kq~xk number inputs falling range mk+. bound squared weights noise terms markov decision process characterized quintuple ﬁnite states; ﬁnite actions; reward function agent; transition function; ﬁnally discount rate future rewards. factored-state markov decision process structured cartesian product smaller components function local-scope function deﬁned subspace state space index set. make standard assumption exist sets size ~xt+ depends transition probabilities carried reward-learning experiments stocks domain sectors stocks sector. rewards uniformly random interval owning rising stock random owning decreasing stock. compared algorithms algorithm algorithm modiﬁed output rmax unknown states; previous state-of-the-art kwik-lr algorithm modiﬁed output rmax unknown states; tabular reward learner; demonstrate need eﬃcient exploration linear regression without exploration linear regression epsilon-greedy exploration. algorithm times steps updating model every steps. stocks domain rmax used algorithm second approach third one. values learned policies shown figure curves except diﬀer signiﬁcantly conﬁdence level. notice take longer learn model takes longer fails explore correct model. consider another novel application kwik linear regression—learning action-eﬀect probabilities environments eﬀects ambiguous. speciﬁcally consider environments actions encoded stochastic action schemas rather travel) eﬀects actions stochastic. instance action travel result eﬀect probability eﬀect probability formally every action observe sums unknown quantities quantities themselves. requires solution online linear regression problem suitable encoding reward model. total number parameters describing reward model consider indicator function indices corresponding elsewhere. solution algorithm modiﬁcation algorithm initialize unknown rewards constant suﬃciently high algorithm outputs optimistic reward estimates unknown states otherwise gives near-accurate predictions high probability. property follows standard arguments unknown states noise term prediction error bounded azuma’s inequality high enough second term positive dominates. form optimistic initialization proven consistently better r-max mdps known states kwik guarantees suﬃce ensure near-optimal behavior. combines kwik model-learner r-max style exploration algorithm pac-compact-mdp—the ﬁrst eﬃcient algorithm task. section combine algorithm another kwik learner learns transition dependency structure probabilities learn full fmdp model. form possible eﬀect. action taken eﬀects occurs according probability distribution induced pis. schemas also contain conditional eﬀect distributions nature determined speciﬁc language used form generalization used encode many diﬀerent types environments including stochastic strips object oriented mdps typed dynamics learning probabilities non-trivial given state action pair eﬀects partitioned equivalence classes {{ωi ...} contains eﬀects identical given state instance action possible eﬀects holding holding taken state already held cannot tell actually occurred. notice probability equivalence class equal probabilities eﬀects contains hence link linear regression. standard counting method learning probabilities cannot used setting unclear eﬀect’s count holdings) increment. however kwik-lr used learn probabilities. standard linear regression could also used transition data available batch form agent using kwiklr learn probabilities online maintain pac-compact-mdp guarantee needed eﬀective exploration. algorithm presents pac-compactmdp algorithm agent given full actionoperator speciﬁcations except probabilities. since focus learning probabilities assume known pis. also assume preconditions reward structure problem known. discuss methods relaxing assumptions section algorithm used several representations including mentioned above. intuitively algorithm computes optimistically optimal action using planner gathers experience indicating equivalence classes actually occurred. instance holding occurred instead holding. equivalence class contains eﬀects indicator vector created instance agent holding anything holding occurred holding vector would state already held would always come note equivalence class induces unique used update kwik-lr learner output associated actually happened given possible action equivalence class state/action space kwik-lr agent queried determine probability equivalent transition though return identifying transitions equivalence partitions unlearned probability. planning algorithm done constructing transition model grounded state space kwik-lr determines probability possible next states modiﬁed version value iteration used plan optimal next action. modiﬁcation every iteration every state-action pair eﬀects known probabilities ωj...} unknown probabilities ωl...} eﬀect leads highest value next state considered probability value iteration optimistic—it considers rosiest models consistent learned. note planning state space require exponential computation time often unavoidable since model still learned sample eﬃcient manner satisﬁes conditions pac-compact-mdp. strips domains made objects predicates actions actions conjunctive preconditions eﬀects speciﬁed delete lists specify predicates added deleted world state action occurs. stochastic strips operators generalize representation considering multiple possible action eﬀects speciﬁed hadd probi tuples table notice representation instance general action schemas deﬁned above. others looked learning similar operators work attempted heuristically learn full operators could give guarantees behavior algorithm identify eﬃcient algorithm learning probabilities algorithm make planning step well deﬁned consider stochastic strips rewards table familiar paint/polish domain stochastic strips setting. several ambiguous eﬀects. instance executing paint scratched painted object observing scratched painted cannot tell ﬁrst eﬀects occurred. used domain single object empirically test algorithm version algorithm kwik-lr instead attempting learn every possible equivalence class partition distribution separately focus learning transition probabilities learners given preconditions eﬀects action. discuss relaxing assumptions later. figure shows results averaged runs randomized initial states episode algorithm learns much faster effectively shares information partitions. object-oriented mdps consist objects actions take elements parameters condition-eﬀect pairs associated action. objects attributes objects attribute values given time constitute state action executed environment checks condition satisﬁed applies corresponding eﬀect updates attributes aﬀected objects. stochastic oomdp eﬀects generalize representation given condition induces single eﬀect distribution possible eﬀects actually occur single timestep. parlance action schemas deﬁned above pair action thought action preconditions speciﬁed again model viewed speciﬁc version action schemas presented above. previous work presented eﬃcient algorithm learning deterministic eﬀects. here demonstrate algorithm learning probabilities associated eﬀect stochastic setting possible eﬀects given condition conditions known advance. methods relaxing assumptions discussed later sections. demonstrate algorithm stochastic oomdps simple maze domain illustrated figure agent starts location goal arrive step cost arriving goal results reward agent’s actions executing action agent attempt move desired direction probability slip either side probability hits wall stays put. rule produces ambiguity eﬀects. plex domains. follow building blocks approach show combine fmdp rewardlearning algorithm implementation noisy-union algorithm learn transition structure transition probabilities reward function fmdp once. knowledge given agent number parents factor reward-function structure resulting algorithm date learns parameters eﬃciently. experience building block algorithms parallel. reward learner outputs optimistic approximation reward function given noisy-union learns transition structure probabilities. conducted experiments validate algorithm stocks domain. comparison also noisyunion algorithm rewards given priori. figure displays results show three quantities learned small overhead. note many settings could beneﬁt combining kwik-lr kwik learners diﬀerent parts model. instance stochastic strips oomdps preconditions actions conditional eﬀects learned using existing kwik adaptation noisy union long size bounded known constant. together learners could used learn eﬀects action operator problem consider. eﬀect distribution condition learning variations require possible eﬀects input. unfortunately relaxing assumption stochastic case unlikely since eﬀect learning problem known np-hard number possible eﬀects small constant size example imagine agent wall north east. attempts action could move west stay place. stays place might attempted move north north wall attempted move east east wall. figure presents results. previous experiment kwik-rl algorithm learns much faster partition learner sharing information equivalence partitions. discuss extensions learning compact representations discussed work. outline learn full fmdps larger parts strips oomdp models combining kwiklr kwik learners provide empirical support fmdp case. also discuss variation kwik-lr used learn stochastic action-schema outcomes enumeration techniques could used. assumptions often violated researchers concentrated heuristic solutions here suggest novel heuristic extends kwik-lr probability learning setting whole action schema needs learned. propose using sparsiﬁcation; consider possible eﬀects kwik-lr learn probabilities extra constraint number active probabilities small. minimization eﬃciently computed linear programming techniques column generation used keep number active constraints small cases. together learners discussed paper implementation extension would form complete solution action-schema learning problem probabilities preconditions learned kwik-learners eﬀects learned heuristically using sparsiﬁcation extension. remains matter future work compare system heuristic solutions problems. online linear regression also studied regret minimization framework applications restricted problems also exist diﬀerent types bounds. furthermore regret analysis seems lack modularity kwik framework. previous work linear regression model-based focussed learning linear transition functions continuous spaces. however approaches often lacked theoretical guarantees placed restrictions environment regarding noise reward structure paper improved current state-of-the-art algorithm linear regression used applications beyond standard linear transition models. theoretical results experiments illustrate potential kwik-lr model-based future intend identify compact models technique facilitate eﬃcient learning perform expanded empirical theoretical studies extensions mentioned above.", "year": 2012}