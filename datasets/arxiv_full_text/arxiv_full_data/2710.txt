{"title": "Hidden Physics Models: Machine Learning of Nonlinear Partial  Differential Equations", "tag": ["cs.AI", "cs.LG", "math.AP", "stat.ML"], "abstract": "While there is currently a lot of enthusiasm about \"big data\", useful data is usually \"small\" and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from {\\em small} data. In particular, we introduce \\emph{hidden physics models}, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier-Stokes, Schr\\\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.", "text": "currently enthusiasm data useful data usually small expensive acquire. paper present paradigm learning partial diﬀerential equations small data. particular introduce hidden physics models essentially data-eﬃcient learning machines capable leveraging underlying laws physics expressed time dependent nonlinear partial diﬀerential equations extract patterns high-dimensional data generated experiments. proposed methodology applied problem learning system identiﬁcation data-driven discovery partial diﬀerential equations. framework relies gaussian processes powerful tool probabilistic inference functions enables strike balance model complexity data ﬁtting. eﬀectiveness proposed approach demonstrated variety canonical problems spanning number scientiﬁc domains including navier-stokes schr¨odinger kuramotosivashinsky time dependent linear fractional equations. methodology provides promising direction harnessing long-standing developments classical methods applied mathematics mathematical physics design learning machines ability operate complex domains without requiring large quantities data. trillion sensors world today according estimates trillion cameras worldwide within next years collecting data either sporadically around clock. however scientiﬁc experiments quality error-free data easy obtain e.g. system dynamics characterized bifurcations instabilities hysteresis often irreversible responses. admittedly everyday applications scientiﬁc experiments volume data increased substantially compared even decade analyzing data expensive time-consuming. data-driven methods enabled past decade availability sensors data storage computational resources taking center stage across many disciplines science. highly scalable solutions problems object detection recognition machine translation text-to-speech conversion recommender systems information retrieval. solutions attain state-of-the-art performance trained large amounts data. however purely data driven approaches machine learning present difﬁculties data scarce relative complexity system. hence ability learn sample-eﬃcient manner necessity data-limited domains. less well understood leverage underlying physical laws and/or governing equations extract patterns small data generated highly complex systems. work propose modeling framework enables blending conservation laws physical principles and/or phenomenological behaviors expressed partial diﬀerential equations datasets available many ﬁelds engineering science technology. paper considered direct continuation preceding addressed problem inferring solutions time dependent nonlinear partial diﬀerential equations using noisy observations. here similar methodology employed deal problem learning system identiﬁcation data-driven discovery partial diﬀerential equations denotes latent solution nonlinear operator parametrized subset example dimensional burgers’ equation corresponds case λhhx−λhxx here subscripts denote partial diﬀerentiation either time space. given noisy measurements system typically interested solution distinct problems. ﬁrst problem inference ﬁltering smoothing states given ﬁxed model parameters said unknown hidden state system? question topic preceding paper authors introduce concept numerical gaussian processes address problem inferring solutions time dependent nonlinear partial diﬀerential equations using noisy observations. second problem learning system identiﬁcation data driven discovery partial diﬀerential equations stating parameters best describe observed data? assume observe snapshots {xn− hn−} system times respectively apart. main assumption small enough apply backward euler time stepping scheme equation obtain discretized equation build upon analytical property gaussian processes output linear system whose input gaussian distributed gaussian. speciﬁcally proceed placing gaussian process prior latent function i.e. here denotes hyper-parameters covariance function without loss generality gaussian process priors used work assumed squared exponential covariance function i.e. gaussian processes provide ﬂexible prior distribution functions enjoy analytical tractability. viewed prior one-layer feed-forward bayesian neural networks inﬁnite number hidden units gaussian processes among class methods known kernel machines analogous regularization approaches from theoretical point view kernel gives rise reproducing kernel hilbert space deﬁnes class functions represented kernel. particular squared exponential covariance function implies smooth approximations. systematic treatment kernelselection problem would like refer readers furthermore complex function classes accommodated employing nonlinear warping input space capture discontinuities call multi-output gaussian process hidden physics model because matrix covariance functions explicitly encodes underlying laws physics expressed equations given noisy data {xn− hn−} latent solution times respectively hyper-parameters covariance functions importantly parameters operators learned employing quasi-newton optimizer l-bfgs here total number data points moreover included capture noise data also learned minimizing negative marginal likelihood. implicit underlying assumption simply favor models training data best. fact induces automatic trade-oﬀ data-ﬁt model complexity. speciﬁcally minimizing term equation targets ﬁtting training data log-determinant term penalizes model complexity. regularization mechanism automatically meets occam’s razor principle encourages simplicity explanations. aforementioned regularization mechanism negative marginal likelihood eﬀectively guards overﬁtting enables learning unknown model parameters noisy observations. however theoretical guarantee negative marginal likelihood suffer multiple local minima. practical experience negative marginal likelihood seems indicate local minima devastating problem certainly exist. moreover highlighted that although pursued here fully bayesian robust estimate linear operator parameters obtained sampling procedures markov chain monte carlo chapter train model. furthermore computationally intensive part learning using negative marginal likelihood associated inverting dense covariance matrices scales cubically number training data eﬀectively addressed recent works cubic scaling still well-known limitation gaussian process regression. proposed framework provides general treatment time-dependent nonlinear partial diﬀerential equations fundamentally diﬀerent nature. generality demonstrated applying algorithm dataset originally proposed sparse regression techniques used discover partial diﬀerential equations time series measurements spatial domain. dataset covers wide range canonical problems spanning number scientiﬁc domains including regularization important even data abundant regimes witnessed recently growing literature discovering ordinary partial diﬀerential equations data using sparse regression techniques navier-stokes schr¨odinger kuramoto-sivashinsky equations. moreover data codes used manuscript publicly available github https//github.com/maziarraissi/hpm. burgers’ equation arises various areas applied mathematics including ﬂuid mechanics nonlinear acoustics dynamics traﬃc fundamental partial diﬀerential equation derived navier-stokes equations velocity ﬁeld dropping pressure gradient term. burgers’ equation despite relation much complicated navier-stokes equations exhibit turbulent behavior. however small values viscosity parameters burgers’ equation lead shock formation notoriously hard resolve classical numerical methods. space dimension equation reads unknown parameters. original data-set proposed contains time snapshots solution burgers’ equation gaussian initial condition propagating traveling wave. snapshots apart. spatial discretization snapshot involves uniform grid cells. depicted ﬁgure using snapshots data points respectively algorithm capable identifying correct parameter values relatively good accuracy. noted using data set. surprising performance achieved cost explicitly encoding underlying physical laws expressed burgers’ equation covariance functions hidden physics model systematic study performance method carry experiment illustrated ﬁgure every pair consecutive snapshots original dataset. still using number data points pair snapshots albeit diﬀerent locations. resulting statistics learned parameter values reported table clearly demonstrated table noise data leads less conﬁdence estimated values parameters. moreover recall main assumption work pair snapshots small enough employ backward euler scheme test importance assumption exact setup explained ﬁgure increase results reported table therefore important facts proposed methodology data less noise smaller snapshots enhance performance algorithm. figure burgers’ equation solution burgers’ equation depicted panel. white vertical lines panel specify locations randomly selected snapshots. snapshots apart plotted middle panel. crosses denote locations training data points. correct partial diﬀerential equation along identiﬁed ones reported lower panel. mathematical model waves shallow water surfaces could consider korteweg-de vries equation. equation also viewed burgers’ equation added dispersive term. equation several connections physical problems. describes evolution long one-dimensional waves many physical settings. physical settings include shallow-water waves weakly non-linear restoring forces long internal waves density-stratiﬁed ocean acoustic waves plasma acoustic waves crystal lattice. moreover equation governing equation string fermi-pasta-ulam problem continuum limit. equation reads unknown parameters. original dataset proposed contains soliton solution equation spatial points time-steps. snapshots apart. depicted ﬁgure using snapshots data points respectively algorithm capable identifying correct parameter values relatively good accuracy. particular original data set. level eﬃciency direct consequence equation covariance functions explicitly encode underlying physical laws expressed equation. sensitivity analysis reported results perform experiment illustrated ﬁgure every pair consecutive snapshots original dataset. still using number data points pair snapshots albeit diﬀerent locations. resulting statistics learned parameter values reported table clearly demonstrated table noise data leads less conﬁdence estimated values parameters. moreover test sensitivity results respect time snapshots exact setup explained ﬁgure increase results reported table results verify important facts proposed methodology data less noise smaller snapshots enhance performance algorithm. figure equation solution equation depicted panel. white vertical lines panel specify locations randomly selected snapshots. snapshots apart plotted middle panel. crosses denote locations training data points. correct partial diﬀerential equation along identiﬁed ones reported lower panel. kuramoto-sivashinsky equation similarities burgers’ equation. however presence second fourth order spatial derivatives behavior complicated interesting. kuramoto-sivashinsky canonical model pattern forming system spatio-temporal chaotic behavior. sign second derivative term acts energy source thus destabilizing eﬀect. nonlinear term however transfers energy high wave numbers stabilizing fourth derivative term dominates. ﬁrst derivation equation kuramoto study reaction-diﬀusion equations modeling belousov-zabotinskii reaction. equation also developed sivashinsky higher space dimensions modeling small thermal diﬀusive instabilities laminar ﬂame fronts small perturbations reference poiseuille layer inclined plane. space dimension also used model problem b´enard convection elongated used describe long waves interface viscous ﬂuids unstable drift waves plasmas. space dimension kuramoto-sivashinsky equation reads unknown parameters. original dataset proposed contains direct numerical solution kuramoto-sivashinsky equation spatial points time-steps. snapshots apart. depicted ﬁgure using snapshots data points respectively algorithm capable identifying correct parameter values relatively good accuracy. particular using total equation covariance functions explicitly encode underlying physical laws expressed kuramoto-sivashinsky equation. sensitivity analysis reported results perform experiment illustrated ﬁgure every pair consecutive snapshots original dataset. still using number data points pair snapshots albeit diﬀerent locations. resulting statistics learned parameter values reported table shown table noise data leads less conﬁdence estimated parameter values. moreover test sensitivity results respect time snapshots exact setup explained ﬁgure increase results reported table results indicate data less noise smaller snapshots enhance performance algorithm. figure kuramoto-sivashinsky equation solution kuramoto-sivashinsky equation depicted panel. white vertical lines panel specify locations randomly selected snapshots. snapshots apart plotted middle panel. crosses denote locations training data points. correct partial diﬀerential equation along identiﬁed ones reported lower panel. one-dimensional nonlinear schr¨odinger equation classical ﬁeld equation used study nonlinear wave propagation optical ﬁbers and/or waveguides bose-einstein condensates plasma waves. optics nonlinear term arises intensity dependent index refraction given material. similarly nonlinear term bose-einstein condensates result mean-ﬁeld interactions interacting n-body system. nonlinear schr¨odinger equation given involves linear operations. here real imaginary parts state system previous time step respectively. proceed placing independent gaussian processes i.e. here hyper-parameters kernels respectively. prior assumptions along equations enable encode underlying laws physics expressed nonlinear schr¨odinger equation resulting hidden physics model speciﬁc forms covariance functions involved model direct function prior assumptions well equations hyper-parameters along parameters learned minimizing negative marginal likelihood outlined section original data-set proposed contains time snapshots solution nonlinear schr¨odinger equation gaussian initial condition. snapshots apart. spatial discretization snapshot involves uniform grid elements. depicted ﬁgure using snapshots data points respectively algorithm capable identifying correct parameter original data set. performance achieved cost explicitly encoding underlying physical laws expressed nonlinear schr¨odinger equation covariance functions hidden physics model systematic study performance method carry experiment illustrated ﬁgure every pair consecutive snapshots original dataset. still using number data points pair snapshots. resulting statistics learned parameter values reported table clearly demonstrated table noise data leads less conﬁdence estimated values parameters. moreover recall main assumption work pair snapshots small enough employ backward euler scheme test importance assumption exact setup explained ﬁgure increase results reported table therefore important facts proposed methodology data less noise smaller snapshots enhance performance algorithm. figure nonlinear schr¨odinger equation solution nonlinear schr¨odinger equation depicted panels. black vertical lines panels specify locations randomly selected snapshots. snapshots apart plotted middle panels. crosses denote locations training data points. correct partial diﬀerential equation along identiﬁed ones reported lower panel. here real part imaginary part. navier-stokes equations describe physics many phenomena scientiﬁc engineering interest. used model weather ocean currents water pipe around wing. navierstokes equations full simpliﬁed forms help design aircraft cars study blood design power stations analysis dispersion pollutants many applications. consider navier-stokes equations dimensions given explicitly denotes x-component velocity ﬁeld y-component pressure. here unknown parameters. solutions navier-stokes equations searched divergence-free functions; i.e. extra equation continuity equation incompressible ﬂuids describes conservation mass ﬂuid. applying backward euler time stepping scheme navier-stokes equations obtain construction samples generated multioutput gaussian process satisfy continuity equation moreover independent place gaussian process prior i.e. lower triangular portion matrix covariance functions shown symmetry. hyper-parameters along parameters learned minimizing negative marginal likelihood outlined section data following exact instructions ones provided simulate navierstokes equations describing two-dimensional ﬂuid past circular cylinder reynolds number using immersed boundary projection method approach utilizes multi-domain scheme four nested domains successive grid twice large previous one. length time nondimensionalized cylinder unit diameter unit velocity. data collected ﬁnest solver uses rd-order runge kutta integration scheme time step veriﬁed yield well-resolved converged ﬁelds. simulations converge steady periodic vortex shedding snapshots saved every depicted ﬁgure using snapshots velocity ﬁeld data points respectively algorithm capable identifying correct parameter values relatively good accuracy. noted using snapshots total data points. surprising performance achieved cost explicitly encoding underlying physical laws expressed navier-stokes equations covariance functions hidden physics model sensitivity analysis reported results perform experiment illustrated ﬁgure pairs consecutive snapshots. still using number data points pair snapshots. resulting statistics learned parameter values reported table clearly demonstrated table noise data leads less conﬁdence estimated values parameters. moreover test sensitivity results respect time snapshots exact setup explained ﬁgure increase results reported table results verify important facts proposed methodology data less noise smaller snapshots enhance performance algorithm. particular results reported table indicate obtain accurate estimates reynolds number needs utilize smaller pair snapshots. verify validity conjecture decrease pair time snapshots employing exact setup explained ﬁgure results reported table clearly demonstrated table smaller leads accurate estimates reynolds number absence noise data. however smaller seems make algorithm susceptible noise data. worth emphasizing making data pressure vorticity ﬁelds. practice unlike velocity data) obtaining direct measurements pressure vorticity ﬁelds demanding impossible. method circumvents need data pressure simply prior assumption samples generated multi-output gaussian process satisfy continuity equation figure navier-stokes equations single snapshot vorticity ﬁeld solution navier-stokes equations ﬂuid past cylinder depicted panel. black panel speciﬁes sampling region. snapshots velocity ﬁeld apart plotted middle panels. black crosses denote locations training data points. correct partial diﬀerential equation along identiﬁed ones reported lower panel. here denotes x-component velocity ﬁeld y-component pressure vorticity ﬁeld. sense fractional operators often arise modeling anomalous diﬀusion processes non-local interactions. integer values model classical advection diﬀusion phenomena respectively. however fractional calculus setting assume real values thus continuously interpolate inherently diﬀerent model behaviors. proposed framework allows directly inferred noisy data opens path ﬂexible formalism model discovery calibraone obtain kn−n kn−n−. hyper-parameters along parameters learned minimizing negative marginal likelihood outlined section hidden physics model identify long celebrated relation brownian motion diﬀusion equation fokker-planck equation brownian motion simulated brownian motion evenly spaced time points generated histograms particle’s displacement. histograms apart. depicted ﬁgure using histograms bins algorithm capable identifying correct fractional order parameter values relatively good accuracy. moreover consider dimensional fractional equation fractional laplacian operator also deﬁned generator α-stable l´evy processes. motivated observation simulated αstable l´evy process employed hidden physics model resulting equation identify fractional order depicted ﬁgure using histograms bins algorithm capable identifying correct fractional order relatively good accuracy. figure fractional equation brownian motion single realization brownian motion depicted panel. histograms particle’s displacement apart plotted middle panel. correct partial diﬀerential equation along identiﬁed ones reported lower panel. stable distributions rich class probability distributions allow skewness heavy tails. stable distributions proposed model many types physical economic systems. particular argued observed quantities many small terms price stock noise communication system etc. hence stable model used describe systems. figure fractional equation α-stable l´evy process single realization α-stable l´evy process depicted panel. histograms particle’s displacement apart plotted middle panel. correct partial diﬀerential equation along identiﬁed ones reported lower panel. introduced structured learning machine explicitly informed underlying physics possibly generated observed data. exploiting structure critical constructing data-eﬃcient learning algorithms eﬀectively distill information data-scarce scenarios appearing routinely study complex physical systems. applied proposed framework problem identifying general parametric nonlinear partial diﬀerential equations noisy data. generality demonstrated using various benchmark problems diﬀerent attributes. work considered direct follow similar methodology employed infer solutions time-dependent nonlinear partial diﬀerential equations eﬀectively quantify propagate uncertainty noisy initial boundary data. ideas introduced papers provide natural platform learning noisy data computing uncertainty. perhaps pressing limitation work present form stems cubic scaling respect total number training data points. however ideas recursive kalman updates variational inference parametric gaussian processes used address limitation. moreover examples studied current work inspired pioneering work recently presented authors followed sparse regression approach full spatio-temporal time series measurements consisting thousands data points. contrast used much smaller datasets hundreds points snapshots systems. however unlike work dictionary possible terms involved partial diﬀerential equation. could possibly include dictionary formulation would make kernel evaluations expensive. moreover systems e.g. advection-diﬀusion-reaction system know terms equation i.e. advection diﬀusion typically reaction term unknown. case would seek obtain parameters front advection-diﬀusion discover functional form reaction term along parameters using methodology outline paper. comparison method require numerical diﬀerentiation kernels obtained analytically. moreover require regular lattice work scattered data. additional advantage approach estimate parameters appearing anywhere formulation partial diﬀerential equation method suitable parameters appearing coeﬃcients. example cannot estimate fractional order last example presented paper parameters partial diﬀerential equations involving term like sin) parameter. also treatment noise somewhat complex method involves sort ﬁltering e.g. singular value decomposition whereas method ﬁlter arbitrarily noisy data automatically gaussian process prior assumptions. believe methods used diﬀerent contexts eﬀectively anticipate beginning thinking formulating possibly simpler equations e.g. employing fractional operators naturally captured framework. work received support darpa equips grant muri/aro grant wnf--- afosr grant fa---. data codes used manuscript publicly available github https//github.com/maziarraissi/hpm.", "year": 2017}