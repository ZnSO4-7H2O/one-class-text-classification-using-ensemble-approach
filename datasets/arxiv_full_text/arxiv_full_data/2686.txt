{"title": "InfoVAE: Information Maximizing Variational Autoencoders", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "It has been previously observed that variational autoencoders tend to ignore the latent code when combined with a decoding distribution that is too flexible. This undermines the purpose of unsupervised representation learning. In this paper, we additionally show that existing training criteria can lead to extremely poor amortized inference distributions and overestimation of the posterior variance, even when trained to optimality. We identify the reason for both short-comings in the regularization term used in the ELBO criterion to match the variational posterior to the latent prior distribution. We propose a class of training criteria termed InfoVAE that solves the two problems. We show that these models maximize the mutual information between input and latent features, make effective use of the latent features regardless of the flexibility of the decoding distribution, and avoid the variance over-estimation problem. Through extensive qualitative and quantitative analyses, we demonstrate that our models do not suffer from these problems, and outperform models trained with ELBO on multiple metrics of performance.", "text": "previously observed variational autoencoders tend ignore latent code combined decoding distribution ﬂexible. undermines purpose unsupervised representation learning. paper additionally show existing training criteria lead extremely poor amortized inference distributions overestimation posterior variance even trained optimality. identify reason short-comings regularization term used elbo criterion match variational posterior latent prior distribution. propose class training criteria termed infovae solves problems. show models maximize mutual information input latent features make effective latent features regardless ﬂexibility decoding distribution avoid variance overestimation problem. extensive qualitative quantitative analyses demonstrate models suffer problems outperform models trained elbo multiple metrics performance. variational autoencoders shown great promise modeling complex distributions natural images text. directed graphical models represent joint distribution data hidden variables capturing latent factors variation. joint factored product prior latent variables conditional distribution visible variables given latent ones. typically conditional distribution chosen simple parameterized gaussian fully factored discrete distribution approaches highly effective relatively simple datasets mnist work well complex datasets lsun autoregressive density estimators hand shown perform well complex dataset imagenet. however autoregressive density estimators pixelrnn/pixelcnn explicitly include latent variables designed capture meaningful latent features often main purpose unsupervised learning promising line research combine models achieve best worlds autoregressive density estimator serves highly expressive conditional distribution latent variable variational autoencoder-style model however ﬁrst observed models shortcomings. conditional distribution model sufﬁciently expressive latent code ignored. model uses conditional distribution component model data effectively ignoring latent variables withtaking advantage mixture modeling capability vae. solution proposed restrict conditional distribution forced additional representational power provided latent variables. however difﬁcult limit capacity conditional distribution principled way. aside uninformative latent code problem show suffers additional problem overestimating variance latent features. extreme cases inference distribution input distribution inﬁnite variance distribution latent space. leads failed amortized inference mismatch samples generated reconstruction samples generated generative model. paper propose novel solution problems designing training objectives autoencoding latent variable models. first observe reason problems evidence lower bound training criterion itself. propose family objective term infovae. even though optimize lower bound likelihood show objective also learns correct generative model amortized inference. importantly objective suffer problems. also show experimentally choice model family mmdvae based maximum-mean discrepancy performs better elbo-vae almost every metric model performance likelihood sample quality etc. furthermore mmd-vae learns meaningful latent features good semi-supervised learning performance elbo-vae performs better random conditional distribution sufﬁciently ﬂexible. furthermmd-vae learns correct inference distribution elbo-vae severely over-estimates variance latent space. latent variable generative model deﬁnes joint distribution feature space input space usually assume simple prior distribution features gaussian uniform model data distribution complex conditional distribution implemented neural network parameterized natural training objective maximum likelihood however direct optimization likelihood intractable pθpdz requires integration classic approach deﬁne amortized inference distribution jointly optimize variational lower bound likelihood lelbo −dkl||p) eqφ] effect shall refer information preference problem ﬁrst formally studied uses bits-back coding argument show ignoring latent code lead concise encoding scheme. provide alternative interpretation shed light novel solution solve problem. marginal hence ﬁrst term addition second divergence best elbo achieve. second divergence latent code completely non-informative i.e. making independent incentive model learn otherwise undermining purpose learning latent variable model. elbo intuitive interpretation ﬁrst term regularization encourages posterior match prior second term reconstruction error probabilistic autoencoder. another limitation elbo training criterion might fail learn amortized inference distribution approximates true posterior ﬁrst provide intuition would happen. observed conditional distribution simple distribution gaussian family optimizing lelbo complex dataset imagenet results poor generative model. remedy expressive distribution family pixelrnn/pixelcnn however although models generate higher quality samples complex datasets approach suffers problem tends neglect latent code altogether. mutual information becomes ﬁrst term maximizes likelihood observing data point given inferred latent code consider ﬁnite dataset {x··· xn}. learn distributions disjoint support learn mapping support δ-distribution centered leading inﬁnitely large therefore training maximize encourage learning disjoint support note almost cases variational distribution family supported entire space therefore attempting learn disjoint supports push mass distributions away other. example maps gaussian means implication also different indicating we’ve failed learn good amortized inference model. undesirable result prevented lreg term counter-balance tendency learn disjoint support. however show elbo regularization term lreg sufﬁcient prevent ’exploding’ latent space problem. example mixture gaussians section prove lreg sufﬁcient prevent exploding latent space problem simple case mixture gaussians. discuss realistic datasets experiments section suppose ﬁtting dataset samples gaussians non-zero mean variance. elbo optimization tend data point distinct gaussian mean gaussian back highly concentrated gaussian around model tend push away reduce tail probability attempting learn bijection. show formally following proposition proposition dataset samples arbitrary dimensional gaussians lelbo maximized when tend respectively gives lelbo variational dklp) means amortized inference completely failed. model learns inference distribution pushes probability mass become inﬁnitely true posterior measured divergence. experimental section purely theoretical interest phenomenon occurs practice real world datasets. marginal inference distribution latent space. term directly computable must optimize likelihood free optimization techniques deﬁne following objective linfovae −λd||p) eqφ] special case previously proposed heuristically adversarial autoencoders chosen jensen shannon divergence approximated adversarial discriminator. however paper fully justify models proposition addition show entire class models suffer information preference problem rescaling also solve ’exploding’ latent space problem. proposition continuous spaces divergence mutual information constrained imax linfovae deﬁned globally optimized pdata pθ∀z imax. proposition implies infovae able learn correct model distribution pdata correct amortized inference addition maximizes mutual information completely avoids information preference problem. addition scale cost violating large enough prevent model learning disjoint respectively. effective strategy experiments show reasonably large value chosen exploding latent space problem occur practice. note adding scaling coefﬁcient kl||p) β-vae lβvae −λdkl||p) eqφ] cannot problem dklp) penalizes mutual information large value strongly encourages independent resulting severe under-utilization latent code stop gradient indicates term treated constant back propagation. note really divergence simply convenient loss function implement using standard automatic differentiation software whose gradient stein variational gradient true divergence. particular form infovae stein regularization different stein variational autoencoders proposed uses stein variational gradient posterior inference elbo regularization minimize dklp). model still suffers uninformative latent code problem since objective remains unchanged. maximum-mean discrepancy framework quantify distance distributions comparing moments. efﬁciently implemented using kernel trick. letting positive deﬁnite kernel dmmdp) epp))] eqp))] similar architecture also introduced metric match distributions named gmmn-ae proposed signiﬁcant difference gmmn-ae proposed mmd-vae model. gmmn-ae trains implicit generative model matches distribution hidden activations pretrained autoencoder. mmd-vae hand trained end-to-end fashion without pretraining. section perform extensive qualitative quantitative experiments binarized mnist dataset evaluate performance different divergences. also perform qualitative experiments challenging cifar dataset verify members infovae family suffer information preference problem elbo fails learn meaningful latent features. experiments choose prior gaussian zero mean identity covariance pixelcnn conditioned latent code ﬁnal layer outputs mean standard deviation factored gaussian distribution. infovae broad family models divergence d||p) used long optimized. divergence leads distinct model potentially different properties tradeoffs. particular consider compare three divergences. approach generalized f-divergences using method proposed however simple distribution gaussian preferable alternatives. fact adversarial training unstable slow even apply recent techniques stabilizing training stein variational gradient simple effective framework matching distribution descending variational gradient dkl||p). distribution small step size positive deﬁnite kernel function function deﬁnes transformation transformation induces distribution minimizes dkl||p) propose stein variational gradient regularize variational autoencoders using following process. mini-batch compute corresponding mini-batch features based mini-batch compute stein gradients empirical samples figure comparison numerical performance. evaluate determinant sample covariance cross entropy correct class distribution semi-supervised learning performance. ‘stein’ ‘mmd’ ‘adversarial’ ‘elbo’ corresponds latent code regularized respective methods ‘unregularized’ corresponds vanilla autoencoder without regularization latent dimensions. please refer section details meaning plots. mnist simpliﬁed version conditional pixelcnn architecture cifar public implementation pixelcnn++ either case convolutional encoder architecture generate latent code plug conditional input models. entire model trained adam pixelcnn imagenet take train convergence single titan cifar take days train convergence. make code public upon publication. consider multiple quantitative evaluations previously mentioned regularization methods models. consider quality samples generated training speed stability latent features semisupervised learning log-likelihoods samples separate test set. measure well approximates numerical metrics. ﬁrst statistic full data. even though also used training mmd-vae expensive train using full dataset mini-batches training. however evaluation full dataset obtain accurate estimates. second determinant covariance matrix estimated samples ideally standard gaussian identity matrix experiments plot determinant divided dimensionality latent space. measures average under/over estimation dimension learned covariance. results plotted figure achieves best performance except elbo. even though elbo achieves extremely error trivial elbo learned latent code contain information show later. note performances regularization stein regularization degrade latent code dimension increases. expected computing stein variational gradients ﬁxed batch size becomes inherently difﬁcult accurately estimate distance distributions dimensionality increases. performance stable stein strong tendency under-estimate entropy approximating distribution increasing dimensionality shown dramatically decreasing determinant covariance matrix adversarial training however relatively consistent performance across dimensionalities. sample distribution generative model true marginal pdata distribution different object classes also follow distribution classes original dataset. denote class distribution real dataset denote class distribution generated images computed pretrained classiﬁer. cross entropy loss measure deviation true class distribution. informative measure incorrect distribution highly unlikely cause classiﬁer generate correct label counts. furthermore highly sensitive missed over/under-represented modes. results metric plotted figure general stein regularization performs well small latent space dimensions whereas adversarial regularization performs better larger dimensions; regularization generally performs well cases performance stable respect latent code dimensions. general would prefer model stable trains quickly requires little hyperparameter tuning. figure plot change statistic number iterations. respect adversarial autoencoder becomes less desirable takes much longer converge sometimes converges poor results even consider power training techniques wasserstein gradient penalty also adversarial autoencoders need hyperparameter tuning; example need sufﬁciently small learning rate discriminator. evaluate quality learned features downstream tasks semi-supervised learning train directly learned latent features mnist images. state-of-the-art results pushed accuracy sample semi-supervised learning less improving semi-supervised learning performance separate problem paper simple baseline similar m+tsvm semi-supervised performance samples approximate metric verify informative meaningful latent features learned generative models. lower classiﬁcation error would suggest learned features contain information data latent code elbo objective contains almost information input semi-supervised learning error rate better random guessing; further demonstrates information preference problem discussed section suggesting infovae approaches able learn latent code informative even decoder powerful. consistent previous results stochastically binarized version ﬁrst used estimation likelihood achieved importance sampling. because accurate estimation importance sampling generally exponentially harder w.r.t. dimension latent code -dimensional latent features likelihood experiments. values shown table results slightly worse reported pixelrnn achieves likelihood however regularizations perform on-par superior compared elbo baseline. somewhat surprising explicitly optimize lower bound true likelihood unless using elbo objective. however note models attempt model pdata proved proposition expect high likelihood performance even optimize directly. another frequently used qualitative evaluation method visualize samples model produces. although visualizing subset samples cannot identify missing mode problem allow identiﬁcation problems metrics test log-likelihood test elbo values inception scores might uncover. samples generated different models trained mnist shown figure latent code dimensionality elbo stein achieve comparable sample quality adversarial autoencoders learn covers support shown numerical evaluation covariance determinant figure therefore adversarial autoencoders quality samples generated small. regions latent space small zero trained eqφ] generate realistic samples. latent feature dimensions obvious degradation sample quality stein regularized consistent previous observation stein variational gradients scale well adversarial training dimenfigure ancestral sample generated different models. generated samples latent code -dimensional. bottom generated samples latent code -dimensional figure cifar samples. plots samples elbo stein regularization dimensional latent features. samples generated stein regularization regularization coherent globally generated elbo regularization. also learn generate cifar images results shown figure cases model accurately matches samples generated stein regularization regularization coherent globally compared samples generated elbo regularization. finally demonstrate exploding latent space problem elbo described section train elbo infovae mnist different training sizes ranging images test images; dcgan architecture models. infovae mmd-vae choose cost ratio set. standard factored gaussian prior cov] det]) values zero give estimate underestimation variance theory match prior observed elbo variance signiﬁcantly over-estimated. especially severe training small elbo drive cov] overﬁt entire dataset. hand scale coefﬁcient dkl||p) infovae avoid problem. show samples generated models figure elbo generates sharp reconstructions poor samples sampled ancestrally ppθ. infovae hand generates samples consistent quality fact produces samples reasonable quality training dataset examples. infovae reduces problem latent variance over-estimation reduces over-ﬁtting. table likelihood estimates different models mnist dataset. mmd-vae achieves best results even though explicitly optimizing lower bound true likelihood. figure det]) elbo mmd-vae different training sizes overestimation det]) elbo applies training test set. infovae training divergence optimized almost zero. despite recent success variational autoencoders fail learn meaningful latent features perform amortized inference. trace issues back elbo learning criterion itself. based insights propose family variational autoencoders learn correct generative model ideal conditions well learning informative latent features. particular propose mmd-vae replacement original trained elbo. mmd-vae achieves similar better performance every quantitative qualitative metric experimented stable terms dimension latent variables. speciﬁc regularizations considered paper characteristic complete. would interesting whether divergences could exhibit interesting properties applied either latent data space. moreover notions mutual information evaluate consistency latent data variables could give rise paradigms learning latent variable generative models. figure samples generated elbo infovae training samples. samples generated elbo. even though elbo generates sharp reconstruction samples training model samples poor differ signiﬁcantly reconstruction samples indicating over-ﬁtting mismatch bottom samples generated infovae. reconstructed samples model samples look similar quality appearance suggesting better generalization latent space. chen duan houthooft rein schulman john sutskever ilya abbeel pieter. infogan interpretable representation learning information maximizing generative adversarial nets. arxiv preprint arxiv. dziugaite gintare karolina daniel ghahramani zoubin. training generative neural networks maximum mean discrepancy optimization. arxiv preprint arxiv. radford alec metz luke chintala soumith. unsupervised representation learning deep convoluarxiv preprint tional generative adversarial networks. arxiv. rasmus antti valpola harri honkala mikko berglund mathias raiko tapani. semi-supervised learning ladder network. corr abs/. http //arxiv.org/abs/.. rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. arxiv preprint arxiv. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei. imagenet large scale visual recognition challenge. corr abs/. http//arxiv.org/abs/.. salimans karpathy andrej chen kingma diederik pixelcnn++ improving pixelcnn discretized logistic mixture likelihood modiﬁcations. arxiv preprint arxiv. tishby naftali zaslavsky noga. deep learning information bottleneck principle. corr abs/. http//arxiv.org/abs/.. oord aaron kalchbrenner espeholt lasse vinyals oriol graves alex conditional image generation pixelcnn decoders. advances neural information processing systems fisher seff zhang yinda song shuran funkhouser thomas xiao jianxiong. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. zhao shengjia song jiaming ermon stefano. learnarxiv goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems gretton arthur borgwardt karsten rasch malte sch¨olkopf bernhard smola alex kernel method two-sample-problem. advances neural information processing systems gulrajani ishaan kumar kundan ahmed faruk taiga adrien visin francesco v´azquez david courville aaron pixelvae latent variable model natural images. corr abs/. http//arxiv.org/abs/.. higgins irina matthey loic arka burgess christopher glorot xavier botvinick matthew mohamed shakir lerchner alexander. beta-vae learning basic visual concepts constrained variational framework. kingma diederik rezende danilo jimenez mohamed shakir welling max. semi-supervised learning deep generative models. corr abs/. http//arxiv.org/abs/.. larsen anders boesen lindbo sønderby søren kaae winther ole. autoencoding beyond pixels using learned similarity metric. arxiv preprint arxiv. nowozin sebastian cseke botond tomioka ryota. f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems", "year": 2017}