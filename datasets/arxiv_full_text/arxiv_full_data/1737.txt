{"title": "Efficient Learning for Undirected Topic Models", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.", "text": "replicated softmax model well-known undirected topic model powerful extracting semantic representations documents. traditional learning strategies contrastive divergence inefﬁcient. paper provides novel estimator speed learning based noise contrastive estimate extended documents variant lengths weighted inputs. experiments benchmarks show estimator achieves great learning efﬁciency high accuracy document retrieval classiﬁcation. topic models powerful probabilistic graphical approaches analyze document semantics different applications document categorization information retrieval. mainly constructed directed structure like plsa accompanied vast developments deep learning several undirected topic models recently reported achieve great improvements efﬁciency accuracy. replicated softmax model kind typical undirected topic model composed family restricted boltzmann machines commonly learned like standard rbms using approximate methods like contrastive divergence however really designed rsm. different rbms binary input adopts softmax units represent words resulting great inefﬁciency sampling inside especially large vocabulary. systems usually require vocabulary sizes tens hundreds thousands thus seriously limiting application. dealing large vocabulary size inputs serious problem deep-learning-based systems. bengio pointed problem normalizing softmax probability neural language model morin bengio solved based hierarchical binary tree. similar architecture used word representations like directed tree structures cannot applied undirected models like stochastic approaches work well. instance dahl found several metropolis hastings sampling approaches approximate softmax distribution well although requires additional complexity computation. hyv¨arinen proposed ratio matching train unnormalized models dauphin bengio added stochastic approaches accommodate high-dimensional inputs. recently estimator noise contrastive estimate proposed unnormalized models shows great efﬁciency learning word representations paper propose efﬁcient learning strategy named α-nce applying basic estimator. different related efforts predicting single word method extends generate noise documents variant lengths. also enables weighted inputs improve modelling ability. usually used ﬁrst layer many deeper undirected models like deep boltzmann machines α-nce readily extended learn efﬁciently. suppose vocabulary size. document words word document equals word dictionary vector assigned element formed assigning hidden state document energy function parameters shared commonly referred word count vector document. probability document given free energy analytically integrated easily partition function normalization associated document length hidden state document conditionally independent conditional distributions derived +e−x equation softmax units describing multinomial distribution words equation serves efﬁcient inference words semantic meanings adopt probabilities hidden unit activated topic features. however gradient intractable combinatorial normalization term common strategies overcome intractability mcmcbased approaches contrastive divergence persistent require repeating gibbs steps generate model samples approximate gradient. typically performance consistency improve steps adopted. notwithstanding even gibbs step time consuming since multinomial sampling normally requires linear time computations. alias method speeds multinomial sampling constant time linear time required processing distribution. since changes every iteration methods cannot used. unlike retains adopted basic learning strategy. considering designed documents further modiﬁed novel heuristics developing approach partial noise uniform contrastive estimate noise contrastive estimate noise contrastive estimate similar another estimator training models intractable partition functions. solves intractability treating partition function additional parameter added makes likelihood computable. model cannot trained likelihood tends arbitrarily large setting huge numbers. instead learns model proxy classiﬁcation problem noise samples. given document collection {vd}td another collection {vn}tn distinguishes documents simply based bayes’ theorem assumed data samples matched model indicating pdata noise samples generated artiﬁcial distribution parameters learned minimizing cross-entropy function partial noise sampling different generates noise word requires estimator sample noise document level. intuitive approach sample empirical distribution times probabilﬁxed gutmann hyv¨arinen suggested choosing noise close data sufﬁcient learning result indicating full noise might satisfactory. proposed alternative partial noise sampling generate noise replacing part data sampled words. algorithm ﬁxed proportion remaining words named noise level pns. however traversing conditions guess remaining words requires computations. avoid this simply bound remaining words data noise advance noise derived readily sampling noise normally requires additional computational load. fortunately since ﬁxed sampling efﬁcient using alias method. also allows storing noise subsequent yielding much faster computation uniform contrastive estimate initially implemented found document lengths terribly biased log-ratio resulting parameters. therefore uniform contrastive estimate proposed accommodate variant document lengths adopts uniform probabilities classiﬁcation average modelling ability word-level. note necessarily integer allows choosing real-valued weights document -weighting typically deﬁned weighting vector ∈{vd}vik=vi∈v multiplied word dictionary. thus weighted input corresponding length derive newsgroups dataset collection usenet posts contains training testing instances. training testing sets labeled classes. removing stop words well stemming performed. imdb dataset benchmark sentiment analysis consists movie reviews taken imdb. dataset divided training instances testing instances. types labels positive negative given show sentiment. following stop words removed dataset. dataset randomly selected training validation idf-weight vector computed advance. addition replacing word count slightly improved modelling performance models. implemented α-nce according parameter settings using minibatches size initialized learning rate number hidden units ﬁxed models. although learning partition function separately every length nearly impossible also surprisingly found freezing constant function without updating never harmed actually enhanced performance. probably large number free parameters forced learn better constant. practise constant function learn real-valued weighted length evaluation efﬁciency evaluate efﬁciency learning used frequent words dictionaries sizes ranging datasets test computation time variant gibbs steps α-nce variant noise sample sizes. comparison mean running time minibatch clearly shown figure averaged datasets. typically α-nce achieves times speed-up compared although α-nce slower input dimension increases tends take much time multinomial sampling iteration especially gibbs steps used. contrast running time stays reasonable α-nce even larger noise size larger dimension applied. estimate log-probability word perplexity. however α-nce learns distinguishing data noise respective features parameters trained like feature extractor generative model. fair perplexity evaluate performance. reason evaluated modelling performance indirect measures. newsgroups trained rsms training reported results document retrieval document classiﬁcation. retrieval treated testing queries retrieved documents labels training cosine-similarity. precision-recall curves mean average precision metrics used evaluation. classiﬁcation trained softmax regression training checked accuracy testing set. dataset show modelling ability different estimators. imdb whole training used learning rsms l-regularized logistic regression trained labeled training set. error rate sentiment classiﬁcation testing reported compared several bow-based baselines. dataset show general modelling ability compared others. trained α-nce naturally ﬁxed vocabulary size posteriors hidden units used topic features. α-nce ﬁxed noise level newsgroups imdb. comparison trained gibbs steps. mance α-nce greatly outperforms retrieval tasks especially around large recall values. classiﬁcation results α-nce also comparable slightly better simultaneously gratifying -weighting inputs achieve best results retrieval classiﬁcation tasks -weighting known extract information better word count. addition naturally performs poorly compared others figure indicating variant document lengths actually bias learning greatly. hand table shows performance sentiment classiﬁcation model combinations reported previous efforts considered. clear α-nce learns better outperforms bow-based models lda. weighting inputs also achieve best performance. note also based indicating α-nce arguably reached limits learning bow-based models. future work extended powerful undirected topic models considering syntactic information word-order dependency relationship representation. α-nce used learn efﬁciently achieve better performance. choice noise level-α order decide best noise level learned rsms using α-nce different noise levels word count -weighting inputs datasets. figure shows α-nce learning partial noise outperforms full noise situations achieves better results retrieval classiﬁcation datasets. however learning tends become extremely difﬁcult noise becomes close data explains performance drops rapidly furthermore curves figure also imply choice might problem-dependent larger sets like imdb requiring relatively smaller nonetheless systematic strategy choosing optimal explored future work. practise range recommended. propose novel approach α-nce learning undirected topic models efﬁciently allowing large vocabulary sizes. estimator based adapted documents variant lengths weighted inputs. learn rsms α-nce classic benchmarks achieves efﬁciency learning accuracy retrieval classiﬁcation tasks. andrew maas raymond daly peter pham huang andrew christopher potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies-volume pages association computational linguistics. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pages frederic morin yoshua bengio. hierarchical probabilistic neural network language model. proceedings international workshop artiﬁcial intelligence statistics pages citeseer. training restricted boltzmann machines using approximations likelihood gradient. proceedings international conference machine learning pages acm.", "year": 2015}