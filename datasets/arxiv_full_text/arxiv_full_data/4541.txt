{"title": "Closing the Learning-Planning Loop with Predictive State Representations", "tag": ["cs.LG", "cs.AI"], "abstract": "A central problem in artificial intelligence is that of planning to maximize future reward under uncertainty in a partially observable environment. In this paper we propose and demonstrate a novel algorithm which accurately learns a model of such an environment directly from sequences of action-observation pairs. We then close the loop from observations to actions by planning in the learned model and recovering a policy which is near-optimal in the original environment. Specifically, we present an efficient and statistically consistent spectral algorithm for learning the parameters of a Predictive State Representation (PSR). We demonstrate the algorithm by learning a model of a simulated high-dimensional, vision-based mobile robot planning task, and then perform approximate point-based planning in the learned PSR. Analysis of our results shows that the algorithm learns a state space which efficiently captures the essential features of the environment. This representation allows accurate prediction with a small number of parameters, and enables successful and efficient planning.", "text": "central problem artiﬁcial intelligence planning maximize future reward uncertainty partially observable environment. paper propose demonstrate novel algorithm accurately learns model environment directly sequences action-observation pairs. close loop observations actions planning learned model recovering policy near-optimal original environment. speciﬁcally present eﬃcient statistically consistent spectral algorithm learning parameters predictive state representation demonstrate algorithm learning model simulated high-dimensional vision-based mobile robot planning task perform approximate point-based planning learned psr. analysis results shows algorithm learns state space eﬃciently captures essential features environment. representation allows accurate prediction small number parameters enables successful eﬃcient planning. planning sequence actions policy maximize future reward long considered fundamental problem autonomous agents. many years partially observable markov decision processes considered general framework single agent planning. pomdps model state world latent variable explicitly reason uncertainty action eﬀects state observability. plans pomdps expressed policies specify action take given possible probability distribution state. unfortunately exact planning algorithms value iteration computationally intractable realistic pomdp planning problems. arguably primary reasons ﬁrst curse dimensionality pomdp states optimal policy function curse history number distinct policies increases exponentially planning horizon. hope mitigate curse dimensionality seeking dynamical system model compact dimensionality mitigate curse history looking model susceptible approximate planning. pomdps yield representations least compact contrast latent-variable representations pomdps psrs ooms represent state dynamical system tracking occurrence probabilities future events conditioned past events tests histories observable quantities suggested learning psrs ooms easier learning pomdps. ﬁnal beneﬁt psrs ooms many successful approximate planning techniques pomdps used plan observable models minimal adjustment. accordingly models dynamical systems potential overcome curse dimensionality curse history quality optimized policy pomdp depends strongly accuracy model inaccurate models typically lead useless plans. specify model manually learn data diﬃculty learning common planning algorithms applied manually-speciﬁed models. unfortunately usually possible hand-specify accurate models small systems extensive goal-relevant domain knowledge. example recent extensions approximate planning techniques psrs applied models constructed hand part learning models planning partially observable environments hampered inaccuracy learning algorithms. example expectation-maximization avoid local minima scale large state spaces; although many learning algorithms proposed psrs ooms attempt take advantage observability state representation none shown learn models accurate enough planning. result successful attempts learning model directly data closing loop planning model. several researchers have however made progress inproblem planning using learned model. stance researchers obtained pomdp heuristically output model-free algorithm demonstrated planning small maze. another instance researchers used markov chain monte carlo inference learn factored dynamic bayesian network representation pomdp small synthetic network administration domain well perform online planning. cost mcmc sampler used approach still impractical larger models. ﬁnal example researchers learned linear-linear exponential family psrs agent traversing simulated environment found policy using policy gradient technique parameterized function learned staten input case learning planning algorithm subject local optima. addition authors determined learned model inaccurate support value-function-based planning methods current paper diﬀers previous examples planning learned models uses principled provably statistically consistent model-learning algorithm demonstrates positive results challenging high-dimensional problem continuous observations. particular propose novel consistent spectral algorithm learning variant psrs called transformed psrs directly execution traces. algorithm closely related subspace identiﬁcation learning linear dynamical systems spectral algorithms learning hidden markov models reduced-rank hidden markov models demonstrate algorithm able learn compact models diﬃcult realistic dynamical system without prior domain knowledge built model algorithm. finally perform point-based approximate value iteration learned compact models demonstrate greedy policy resulting value function works well original system. knowledge ﬁrst research combines achievements closing loop observations actions unknown domain human intervention beyond collecting transition data. predictive state representation compact complete description dynamical system represents state predictions observable experiments tests could perform system. speciﬁcally test length ordered sequence action-observation pairs akok executed observed given time. likewise history ordered sequence action-observation pairs executed observed prior given time. prediction test probability sequence observations generated given intervene take sequence actions observations produced dynamical system match speciﬁed test test said succeeded. idea behind that expected outcomes executing possible tests known everything know state dynamical system also known. psrs actions tests interventions observations. thus notationally convenient separate test observation component action component equations contain probabilities single vertical indicates conditioning double vertical indicates intervening. example probability observations test conditioned history given intervene execute actions contains probabilities success tests existence means knowing probabilities tests suﬃcient computing probabilities tests prediction vector suﬃcient statistic system. vector initial prediction outcomes tests given initial distribution histories allow initial distribution general; practice might correspond steady state distribution heuristic exploration policy distribution histories ﬁrst encounter system empty history probability test outcomes conditioned history observation given intervention choosing immediate next action appropriate actions test). faoq function needed update prediction test normalizing vector. specifying involves ﬁrst ﬁnding core tests called discovery problem ﬁnding parameters tests well initial state called learning problem. discovery problem usually solved searching linearly independent tests repeatedly performing singular value decompositions collections tests learning problem solved regression. equivalent dynamical systems linear dimension linear dimension dynamical system measure intrinsic complexity; speciﬁcally rank system-dynamics matrix dynamical system. since exist dynamical systems ﬁnite linear dimension cannot modeled pomdp ﬁnite number states example) pomdps hmms proper subset psrs transformed psrs generalization psrs maintain small number linear combinations test probabilities suﬃcient statistics dynamical system. transformed psrs thought linear transformations regular psrs. accordingly tpsrs include psrs special case since transformation identity matrix. main beneﬁt tpsrs given core tests parameter learning problem solved large step toward solving discovery problem achieved closed form. respect tpsrs closely related transformed representations ldss hmms found subspace identiﬁcation indicative events promised indicative events ensures rank equal linear dimension system. finally deﬁned initial prediction outcomes tests given initial distribution histories given action-observation pair represent probabilities triple indicative event immediate following observation subsequent test given appropriate actions deﬁned notations vector. generalize keep meaning next deﬁne |×|h| matrix entries contain joint probability every test every indicative event hj||τ vector linear function speciﬁes probability test given probabilities core tests vector contains probabilities core tests given history belongs indicative event because assumptions linear dimension deﬁne tpsr terms matrices additional matrix obeys condition invertible. words columns deﬁne n-dimensional subspace orthogonal column space natural choice given left singular vectors derivation equation makes equations given parameters calculate probability observations time given intervened actions initial state write product matot maot mt∞maot deﬁned above. build estimates repeatedly sample history distribution execute sequence actions record resulting observations. data gathering strategy implies must able arrange system state corresponding long sequence action-observation pairs subsequences pretending subsequence started reset. forced initial distribution histories equal steady state distribution policy generated data. approach called suﬃx-history algorithm method estimated matrices approximately correct since interventions take time aﬀect distribution histories future times; however approximation often good practice. computed generate singular value decomposition learn tpsr parameters plugging equation reference summarize compute empirical estimates hbpt aoh. compute matrix left parameters linear transform. although parameters estimated ﬁnite data sometimes lead negative probability estimates ﬁltering predicting avoided practice thresholding prediction vectors small positive probability. note learning algorithm presented distinct tpsr learning algorithm presented rosencrantz principal diﬀerence algorithms estimate joint probability past event current observation future event probability future event conditioned past event current observation. compensate rosencrantz later multiply estimate approximation probability current observation conditioned past event applied. rosencrantz also derive approximate probability current observation diﬀerently result regression instead directly empirical counts. finally rosencrantz make attempt multiply marginal probability past event although term learning strategy employed seen generalization al.’s spectral algorithm learning hmms psrs. note since hmms pomdps proper subset psrs algorithm paper learn back hmms pomdps form. general elliptical covariance matrix chosen spherical covariance projecting onto eigenvectors covariance matrix observations scaling square roots eigenvalues. indicator function particular observation. parameters tpsr deﬁned terms matrix obeys condition invertible terms matrices aoh. also deﬁne vector s.t. means must space ones vector since matrix features always ensure case requiring features constant. then finally deﬁne generalized tpsr parameters follows continuous observations kernel density estimation model observation probability density function fraction training data points kernel centers placing multivariate gaussian kernel point. estimator observation convex combination kernels; since kernel integrates estimator also integrates theory tells that correct kernel weights number kernel centers number samples inﬁnity kernel bandwidth goes present extensions learning algorithm preserve consistency relaxing requirement discrete indicative events tests. extensions make learning substantially easier many diﬃcult domains practice. particular large sets tests indicative events shorter vectors characteristic indicative features. matrices longer contain probabilities rather expected values features products features. special case features indicator functions tests histories recover tpsr matrices section consist probabilities. prove consistency estimation algorithm using general matrices inputs. following equations matrices characteristic indicative features respectively ﬁrst dimension equal number characteristic indicative features entry expectation indicative features given occurrence indicative events. entry weight tests calculating characteristic features. features generalize matrices backup steps ﬁnite heuristically-chosen belief points rather entire belief simplex. pbvi exploits fact value function pwlc. linear lower bound value function point used lower bound nearby points; insight allows value function approximated ﬁnite hyperplanes point. although pbvi designed pomdps approach generalized psrs formally given corresponds optimal value function least prediction vector obtain approximate value function previous value function apply recursive backup operator points maxα∈γt addition tractable much larger-scale planning problems exact value iteration pbvi comes theoretical guarantees form error bounds low-order polynomials degree approximation range reward values discount factor perseus variant pbvi updates value function small randomized subset large reachable belief points time step. updating subset belief points perseus achieve computational advantage plain pbvi domains. perseus paper speed simplicity implementation. introduced novel algorithm learning tpsrs directly data well kernel-based extension modeling continuous observations discussed plan learned model. first demonstrate viability approach planning challenging non-linear partially observable controlled domain learning model directly sensor inputs closing loop planning learned model. second unlike previous attempts learn psrs either lack planning results compare policies within learned system compare resulting policy bound best possible solution original system demonstrate policy close optimal. tion. robot simulated pixel color camera whose focal plane located unit front robot’s center rotation. robot’s visual ﬁeld azimuth elevation thus providing robot angular resolution pixel. images sensor zero estimator converges observation norm. kernel density estimator completely determined normalized vector kernel weights; therefore estimate vector accurately estimate observation converge observation well. hence goal predict correct expected value normalized kernel vector given past observations. continuous-observation case still write latent-state update form using matrix bao; however rather learning uncountably-many matrices separately learn base operator kernel center convex combinations base operators compute observable operators needed. details practical aspects learning procedure continuous observations section primary motivation modeling controlled dynamical system reasoning eﬀects taking sequence actions system. tpsr model augmented purpose specifying reward function taking action state state obtained executing action observing optimized exactly value function always piecewise linear convex state ﬁnitely many pieces ﬁnite-horizon planning problems. optimal action obtained taking instead equation exact value iteration pomdps tpsrs optimizes value function possible belief state vectors. computing exact value function problematic number sequences actions must considered grows exponentially planning horizon called curse history. approximate point-based planning techniques attempt calculate best sequence actions ﬁnite belief points. unfortunately high dimensions approximate planning techniques diﬃculty adequately sampling space possible beliefs. curse dimensionality. tpsrs often admit compact low-dimensional representation approximate point-based planning techniques work well models. figure learning autonomous robot domain. robot uses visual sensing traverse square domain multi-colored walls central obstacle. examples images recorded robot occupying diﬀerent positions environment shown bottom ﬁgure. to-scale -dimensional view environment. dimension learned subspace point embedding single history displayed color equal average color ﬁrst image highest probability test. points projected onto environment’s geometric space. ward units simultaneously rotate resulting unique actions. real world friction uneven surfaces factors confound precisely predictable movements. simulate uncertainty small amount gaussian noise added translation rotation components actions. robot allowed occupy real-valued pose environment allowed intersect walls. case collision interrupted current motion robot intersected obstacle simulating inelastic collision. learn model sample short trajectories containing action-observation pairs. generate trajectory starting uniformly randomly sampled position environment executing uniform random sequence actions. used ﬁrst trajectories generate kernel centers remaining estimate matrices aoh. deﬁne matrices need specify indicative features observation kernel centers characteristic features. gaussian kernels deﬁne indicative characteristic features similar manner gaussian kernels described observations; analysis allows arbitrary indicative characteristic features found gaussian kernels convenient eﬀective. note resulting features tests histories features; unlike kernel centers deﬁned observations need kernel width approach zero since attempting initial segment trajectories. choose kernel covariance using sequences observations described single observations section generate indicative features sequence three observations evaluating indicative kernel sequence normalizing vector features sums one. similarly deﬁne characteristic kernels centered sequence observations sample trajectories choose kernel covariance deﬁne characteristic feature vector evaluating kernel observation sequence normalizing. initial distribution therefore distribution obtained initializing uniformly taking random actions. finally deﬁne observation kernels centered single observation middle sample trajectories replace observation corresponding vector normalized kernel weights. larly element empirical expectation constructed compute matrix left singular vectors advantages subspace tuned selecting number singular vectors vectors correspond singular values greater cutoﬀ varies noise resolution data. however wish pick smaller singular vectors; produce compact tpsr possible loss prediction quality. chose smallest tpsr able produce high quality policies figure planning learned state space. value function computed embedded point; lighter indicates higher value. policies executed learned subspace. green magenta yellow paths correspond policy executed robot starting positions facing green magenta yellow walls respectively. paths taken robot geometric space executing policy. paths corresponds path color darker circles indicate starting ending position path tick-mark circles indicates robot’s orientation. mean number actions path randomly sampled start position target image ﬁrst mean number actions optimal solution found search robot’s conﬁguration space. second mean number actions taken executing policy computed perseus learned model last mean number actions required target random policy. graph indicates policy computed learned tpsr close optimal. every observation. next learned reward function linear regression histories embedded learned tpsr state space reward speciﬁed image followed embedded history. used reward function compute approximate value function using perseus algorithm discount factor prediction horizon steps embedded histories belief points. learned value function displayed figure approximate value function learned initial belief speciﬁed robot greedily chooses action maximizes expected value. initial beliefs computed starting incorporating random action-observation pairs. examples paths planned learned model presented figure paths shown geometric space figure note valid target positions environment since receive identical close-up image blue wall anywhere along corresponding edge environment. restrict training trajectories action middle time step then element empirical expectation product indicative feature characteristic feature element observation kernel vector. kernel function kernel normalization constant computed summing observation kernels given matrices compute tpsr parameters using equations section learned parameters tpsr model used prediction ﬁltering planning autonomous robot domain. ﬁrst evaluated model qualitatively projecting sets histories traincolored datapoint according average green blue components highest probability observation following projected history. features dimensional embedding clearly capture topology reward function encouraged robot navigate speciﬁc points environment therefore planning problem viewed solving shortest path problem. even though don’t encode intuition algorithm quantitatively evaluate performance policy original system. first randomly sampled initial histories environment asked robot plan path based learned policy. robot able reach goal trials. trials robot stuck repeatedly taking alternating actions whose eﬀects cancelled model assumptions would exact algorithm would need unlimited amount training data. results summarized figure indicate tpsr policy close optimal policy original system. think result remarkable especially given previous approaches encountered signiﬁcant diﬃculty modeling continuous domains domains similarly high levels complexity presented novel consistent subspace identiﬁcation algorithm simultaneously solves discovery learning problems tpsrs. addition provided extensions learning algorithm useful practice maintaining consistency characteristic indicative features require know relevant features tests histories rather sets core tests histories kernel density estimation used observable operators observations real-valued. also showed point-based approximate planning techniques used solve planning problem learned model. demonstrated representational capacity model eﬀectiveness learning algorithm learning compact model simulated autonomous robot vision data. closed loop successfully planning learned models using perseus approximately compute value function optimal policy navigation task. knowledge ﬁrst instance learning model simulated robot partially observable environment using consistent algorithm successfully planning learned model. compare policy generated model bound best possible value determine policy close optimal. believe spectral learning algorithm presented here subspace identiﬁcation procedures learning psrs general increase scope planning uncertainty autonomous agents previously intractable scenarios. believe improvement partly actual application believe could avoid getting stuck performing short lookahead simply randomizing policy; purposes comparison however report results greedy policy. modeling error observable operator models associated learning algorithm. neural computation. mccallum. reinforcement learning selective perception hidden state. thesis university rochester", "year": 2009}