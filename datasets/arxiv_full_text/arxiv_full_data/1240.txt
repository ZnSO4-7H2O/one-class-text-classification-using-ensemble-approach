{"title": "OnionNet: Sharing Features in Cascaded Deep Classifiers", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The focus of our work is speeding up evaluation of deep neural networks in retrieval scenarios, where conventional architectures may spend too much time on negative examples. We propose to replace a monolithic network with our novel cascade of feature-sharing deep classifiers, called OnionNet, where subsequent stages may add both new layers as well as new feature channels to the previous ones. Importantly, intermediate feature maps are shared among classifiers, preventing them from the necessity of being recomputed. To accomplish this, the model is trained end-to-end in a principled way under a joint loss. We validate our approach in theory and on a synthetic benchmark. As a result demonstrated in three applications (patch matching, object detection, and image retrieval), our cascade can operate significantly faster than both monolithic networks and traditional cascades without sharing at the cost of marginal decrease in precision.", "text": "focus work speeding evaluation deep neural networks retrieval scenarios conventional architectures spend much time negative examples. propose replace monolithic network novel cascade featuresharing deep classiﬁers called onionnet subsequent stages layers well feature channels previous ones. importantly intermediate feature maps shared among classiﬁers preventing necessity recomputed. accomplish this model trained end-to-end principled joint loss. validate approach theory synthetic benchmark. result demonstrated three applications cascade operate signiﬁcantly faster monolithic networks traditional cascades without sharing cost marginal decrease precision. last several years seen deep neural networks bringing tremendous rise performance variety recognition tasks. however often comes price high computational cost test time reduction recently become topic deep learning particularly retrieval scenarios large amount computational time spent negative examples varying difﬁculty. popular remedy cascade multiple classiﬁers increasing strength called stages recently pair independent dnns used cascade also region proposal network faster r-cnn essentially seen ﬁrst stage two-stage cascade. former case networks receive input build higher-level representation individually latter case stages ﬁnetuned share ﬁrst convolutional layers. expensive ones compute questionable whether powerful features always necessary. observation extreme cases sharing. intermediate representation reused representation presumably least powerful rebuilt following stage running time positive examples suffers. hand making ﬁrst stage representation last stage lead losing time easy negatives. figure feature sharing. two-stage onionnet. bottom corresponding nonsharing cascade. onionnet ﬁrst stage shares intermediate feature maps second stage without sharing stages independent evaluated fully recomputing certain features address proposing onionnet novel architecture next stage extends feature previous stage preventing repeated computation. crucially architecture ﬂexible next stage layers well feature channels reusing previous ones time. thus stages increasing depth only even classiﬁers depth increasing width still able share features. accomplish this model trained end-to-end principled joint loss. onionnet demonstrated three important tasks patch matching proposal-based object detection image retrieval. achieve substantial speed-up compared noncascaded baselines well non-sharing cascades marginal loss precision. main contributions show cascaded offer signiﬁcant computational beneﬁts compared monolithic architectures propose novel cascaded architecture promotes feature sharing leading additional computational advantages provide systematic study sheds light time cost behavior cascaded architectures. cascades sharing. whereas pioneering work viola jones stages distinct essentially trained hard negative mining soft cascades bourdev brandt trained single boosted classiﬁer weak learner cumulative score rejection threshold. motivated general idea stages building successively realize context dnns. zehnder share stages among several class-speciﬁc cascades multi-class detection stages independent. deep learning sharing idea faster r-cnn comes probably closest method. however training less principled ours using ’-step training algorithm learn shared features alternating optimization’. moreover architecture ﬂexible subsequent stages also feature channels besides layers. also note term ’cascades’ overloaded literature. several authors speak cascades describe sequence stages evaluated whole stage receives output another reﬁnes cascades early rejection negatives. conditional execution. approach also related conditional evaluation networks. hierarchical classiﬁcation hd-cnn class group specialist networks executed based prediction group classiﬁer sharing early layers. unlike approach hd-cnn aims precision rather speed. dynamic capacity network uses entropy-based attention mechanism apply expensive network salinient parts input image better prediction. work processes images whole concentrates feature sharing instead. model compression. research speeding evaluation dnns related general especially works exploring redundancy networks. knowledge distillation aims compress models student-teacher framework whereas matrix factorization methods replace weight matrices low-rank approximations. computational efﬁciency also incorporated beginning imposing e.g. special ﬁlter structure sparse ﬁlter connectivity sense also exploit redundancy present baselines assuming possible separate certain amount layers/channels individual stage still performs reasonably well task. however motivation different train cascade concurrently scratch full combined network last stage well. model cascade feed-forward dnns called stages evaluated sequentially test time. cascade conﬁdently discriminate input example early classiﬁer pipeline possible saving running time. deal gradually complicated examples later stages reﬁned operate higher level abstraction. motivation. multiple networks different sizes trained dataset similar objective raise question whether learned features something common. conﬁrm case different initializations network. major assumption feature maps particular layer computed smaller network approximately subsumed feature maps larger network. thus larger network seen envelope around smaller network adding feature channels layers reusing features smaller network. speciﬁcally convolutional layer larger network receives respective feature maps smaller networks additional input. observation shared don’t recomputed. cascade coined onionnet pictured onion next stage wrapping previous. depth width. natural constructing cascade might gradually increase depth only. principally similar training deeply supervised network proceeding deeper layers test time associated ’local companion output’ rejects example. however ﬁrst layers expensive compute large spatial size tending produce weak classiﬁers non-linearities instead assume likely early stages cascade don’t need many feature maps later ones leads construct cascade gradually increasing width possibly addition depth. making stage thin reduces burden signiﬁcantly permits cascade delay fully evaluating expensive lower layers necessary. two-stage onionnet cascade consists branches layer organization takes input terminated output layer. core idea branches linked every convolutional layer including ﬁnal one. feature maps ﬁrst stage used additional input following convolutional layer second stage round creating one-way dependence. network conversely output layer immediately preceeding forwarded applications onionnet designed replacement large monolithic network simple conﬁgure cascade keep effective number ﬁlters layer unchanged i.e. splitting although number feature maps preserved amount weights decreases missing connections denotes size ﬁlters same albeit less severe effect speed accuracy so-called ﬁlter groups arise splitting layers among multiple gpus imposing structure-induced regularization training stage assigned loss function evaluated output layer. whereas application dependent standard cross-entropy loss s-classes onionnet trained jointly single model combined loss ﬁxed hyperparameter stage access full training set. feature sharing branches weights receive backpropagation updates weights trained only. therefore major beneﬁt joint training cascade learns allocation features networks guided ratio individual losses. initial experiments stage-wise independent training observed decreased accuracy increased need technical tweaks properly converge. testing thresholds. desired true positive rates s-classes user interest care precisely control ﬁnal accuracy rather speed order choose thresholds principled manner curve computed s-class based score statistics complete training set. test example passes scores predeﬁned thresholds otherwise rejected sparse batches. examples test batch pass ﬁrst stage leaving irregular pattern holes unfortunately cannot easily skipped current backend work irregularly strided memory blocks. thus batch well table conﬁguration monolitic baselines best onionnet models parametric layers listed clarity layers parameters consistent paper introduced baseline. fully connected layers implemented convolutions. denotes convolutional layer output ﬁlters spatial size onionnets denotes pair convolutional layers shared feature maps reshufﬂed smaller contiguous blocks output scattered back. formalized obtaining single contiguous subsequence binary vector least amount move operations classic problem solved efﬁciently passes vector. application compare best-performing onionnet cascade respective monolithic baseline network conﬁgurations listed table designed keep effective number ﬁlters non-sharing cascade constitutes second baseline; stages share features number effective ﬁlters stage implementation done torch cudnn backend auto-tuning fastest convolution algorithms. mean running time executions nvidia titan black reported standard error. time solely forward pass e.g. preprocessing uploading batch. addition report mean percentage examples batch passing indicative strength ﬁrst stage. dnns applied comparing patches recently achieving state-of-the-art results. ultimate goal might learn embeddings deep descriptors comparing descriptor pairs using matching network particularly processing patch pairs jointly start shown best-performing solutions however quadratically many pairs joint network evaluated comparison architectures seem rather impractical. fortunately expected high number easy negative pairs makes natural application onionnet. holds especially true feature point matching images. setting. evaluate datasets. multi-view stereo correspondence dataset brown balanced dataset grayscale patches. three subsets; train notre dame test liberty yosemite reporting false positive rate recall local descriptor benchmark mikolajczyk schmid consists images sequences ground truth homographies. dataset table descriptor matching average liberty yosemite subsets tpr=. tpr=. s/baseline mean percentage examples passing running time normalized examples. expected unbalanced realistic proportion positives negatives. framework evaluation regions interest extracted using mser detector. fully demonstrate advantage experiment ch-deep model zagoruyko komodakis note method could also applied matching networks way. models trained scratch epochs binary hinge loss asgd learning rate weight decay momentum used train models batch size random ﬂipping. results. table lists results mvsd onionnet outperforms baselines terms running time better worse running time w.r.t. justiﬁed section figure plots running times averaged types transformations dataset. evaluation constrain choosing larger tprs plot shows realistic imbalance positive negative examples archive considerable speedup limited decrease precision starts show mostly higher transformation magnitude. likely caused discarding positives difﬁcult extreme deformation places near decision boundary. currently dominant paradigm object detection algorithm generate object proposals veriﬁed classiﬁer. object proposal algorithms typically tuned high recall often class-agnostic allows used off-the-shelf preprocessing step. ﬂexibility comes price classiﬁer process many proposals interest respect task-speciﬁc classes. example fast r-cnn sifts thousands proposals image motivated this propose construct classifer onionnet ﬁrst stage serves background classiﬁer leaving task identifying particular classes second stage. note task-speciﬁc scoring alternatively built proposal generator itself demonstrated several recent works setting. experiment fast r-cnn pascal precomputed selective search proposals available author’s webpage. baseline ’small’ alexnet network proposal cresetting s-tpr already made produce enough false negatives prescribed s-tpr never reached. mostly domain transfer particular threshold chosen based curve computed training different subset. ated replacing classiﬁer composed fully-connected layers onionnet width table classiﬁer layers initialized randomly models whole networks ﬁnetuned trainval subset iterations using cross-entropy loss learning rate dropping after iterations. bounding regression omitted implementation affect conclusions comparison though. report mean average precision ﬁxed results. results listed table onionnet able achieve gain speed w.r.t. baseline graceful degradation points better speed-up attained weight matrices note methods might combined basically orthogonal. also marginally outperform running time precision. motivated retrieval ephemeral datasets index building stage typical image retrieval heavy; consider e.g. robot actively searching particular object user wanting copy images cats camera. instead on-the-ﬂy retrieval casts problem classiﬁcation. however similar human search system need precisely label every object knows unless object searched for. demonstrate designing classiﬁer onionnet lead signiﬁcant decrease running time. setting. train validate ilsvrc although perfectly suited retrieval scenarios incomplete annotations choose scale concentration quantifying relative performance improvements. retrieving images certain class validation images rather classifying images. therefore pascal classiﬁcation task report mean average precision classes instead accuracy. test example considered retrieved true class predicted within top- softmaxed scores. ﬁxed s-classes partitioned s-classes k-means clustering class-averaged activation. experiments performed alexnet-like ’cnn-f’ baseline models trained scratch epochs -way cross-entropy loss learning rate weight decay epochs momentum used train models batch size reference achieves top- error crops standard ilsvrc classiﬁcation task. network proposal deep contains resp. convolutional resp. fully-connected ﬁlters table results. results listed table onionnet able running time giving points w.r.t. baseline also saves time w.r.t. non-sharing cascade fair result given relatively amount shared feature maps. better worse running time w.r.t. justiﬁed section section conduct analysis order gain insight properties onionnet. deﬁne multiple variants onionnet image retrieval network varying width depth extend notation superscripts that network width grade depth table image retrieval networks various width depth. ’full’ allows every example pass conﬁguration notation table further starts deeper ends shallower missing layers indicated networks contain extra max-pooling layer ﬁnal convolutional layer. denoted accordingly. first study ratio number ﬁlters allocated inﬂuences overall performance. second deﬁne theoretical time complexity compare empirical running time synthetic benchmark. analyze effect reducing depth width evaluating networks listed table results convey makes weaker measured stronger measured deactivated examples pass expected accuracy network highly dependent amount allocated ﬁlters parameters. regarding running time depth reduction brings less beneﬁt width lower layers expensive compute mimicking faster r-cnn-like cascade spare time. general neither sharing non-sharing cascades expected reach accuracy monolithic baseline non-zero false negative rate sharing cascades trade even accuracy speed parameter reduction shared features serving different objectives. therefore observed column ’full’ none onionnet cascades achieve conﬁrm effect joint learning increased importance retraining obtained improvement around points increase around points. measuring empirical running time makes practical comparison generality limited inherent sensitivity system implementation factors thus additionally investigate theoretical time complexity introduced total time complexity convolutional layers figure empirical running times time complexities rmbased cascades functions percentage examples passing sharing cascade compared corresponding non-sharing baseline monolithic baseline time cost behavior cascade best described function percentage examples batch passing plot costs networks respectively synthetic benchmark regulate necessary. disparity shows cascade actually useful disparity −tns reveals margin onionnet parameter reduction feature sharing. results prominent networks table shown figure time complexity. plots suggests onionnet always improves time cost tns. note higher values i.e. non-sharing cascades overperformed monolithic classiﬁers point. plots also convey large networks beneﬁt speed-up also notice networks unreduced width costly even small values despite heavy help feature sharing. running time. plots follow general trend time complexity plots although important differences. observe many conﬁgurations perform worse monolithic baseline large except conﬁguration largest networks nevertheless behavior smaller values i.e. reported applications appears still promising. identiﬁed causes inconsistency theory practice also reported nonlinear nonmonotonous relation data size convolution running time sizes ’gpu friendly’. overhead layer executions esp. cudnn kernel launches since cascades basically perform forward pass twice. figure empirical running times cascades based imagenet resnet- network created allocating feature channels convolutional layer leaving rest stages depth. notation identical figure conclusions section hold well. although overhead current solutions considered practice render small weak classiﬁers ill-suited cascaded solution general. similar reasons using stages turned impractical initial experiments. since actual beneﬁt cascade varies practice depends precision chosen true positive rate important identify sweet spots practical applications successfully demonstrated above. novel cascade feature-sharing deep classiﬁers proposed subsequent stages extended layers and/or feature channels intermediate computations reused. motivation speed evaluation preventing similar features recomputed make stage cascade equally deep. sharing reduction model parameters main causes achieved speed-up. factors account minor decrease precision though. demonstrated good speedups cascades three important tasks showed onionnet sharing bring gain atop fact encouraging applications seem require higher-level understanding even easy examples thus massive speed-ups simple stages sliding-window methods expected. much deeper expensive networks introduced believe method might gain signiﬁcance larger absolute running time savings; figure preliminary time cost analysis -layer residual network cascade.", "year": 2016}