{"title": "Inverting face embeddings with convolutional neural networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep neural networks have dramatically advanced the state of the art for many areas of machine learning. Recently they have been shown to have a remarkable ability to generate highly complex visual artifacts such as images and text rather than simply recognize them.  In this work we use neural networks to effectively invert low-dimensional face embeddings while producing realistically looking consistent images. Our contribution is twofold, first we show that a gradient ascent style approaches can be used to reproduce consistent images, with a help of a guiding image. Second, we demonstrate that we can train a separate neural network to effectively solve the minimization problem in one pass, and generate images in real-time. We then evaluate the loss imposed by using a neural network instead of the gradient descent by comparing the final values of the minimized loss function.", "text": "deep neural networks dramatically advanced state many areas machine learning. recently shown remarkable ability generate highly complex visual artifacts images text rather simply recognize them. work neural networks effectively invert low-dimensional face embeddings producing realistically looking consistent images. contribution twofold ﬁrst show gradient ascent style approaches used reproduce consistent images help guiding image. second demonstrate train separate neural network effectively solve minimization problem pass generate images real-time. evaluate loss imposed using neural network instead gradient descent comparing ﬁnal values minimized loss function. deep neural networks extremely powerful tool object recognition image segmentation recently also shown uncanny abilities generate images particular style transfer deep dream generative adversarial networks producing highly compelling results. work explore ability control images deep neural networks produce. purposes work facenet face-recognition network trained distinguish people test bench. address problem inverting network output embedding vector i.e. provided embedding vector generate realistic face image passed facenet produces interesting aspect problem fact space distinct acceptable solutions huge particular different orientations poses person theory produce embedding. furthermore space dominated space unacceptable solutions images glimpses faces various orientations simply random-noise looking solutions. unacceptable images mapped given embedding thus proper inversions particularly interesting ones. approach solve employ adversarial learning algorithms pair networks e.g. generator classiﬁer training parallel. however somewhat limits ability control produced generator. goal work produce consistent inverse solutions look like faces prescribed position orientation. paper perhaps somewhat surprisingly show several simple regularization techniques worked well enforcing consistency output images. rest section provide overview results. experiments facenet model mapping face image normalized -dimensional embedding vector. network trained embeddings different photographs person closer different person. network achieves comparable human-level face recognition performance contents paper roughly separated parts. first secs. introduce general problem face reconstruction propose loss function using gradient-descent style algorithm reconstruct highly recognizable faces using target embedding vector. orientation facial expression produced image match provided guiding image. main idea method based attaching additional regularization losses enforce face consistency orientation optimized embedding loss function. speciﬁcally total-variation loss laplacian pyramid graident normalization ensure image smooth. also distance intermediate layers guiding image enforce speciﬁc face orientation position. minimization combined loss function approached using gradient descent starting random noise apriori chosen initial state. second part outlined sec. introduce feed-forward neural network trained produce face images minimize loss function used previously iterative reconstruction. believe approach independent interest since allows solve minimization problem single step. finally experimental results presented sec. interesting observation made studying reconstructions might independent interest even faces look remarkably similar still recognized despite sharing virtually identical macro charactertics. show several examples phenomenon sec. face reconstruction minimization problem face reconstruction problem discussed sec. formalized follows. function deﬁned trained deep neural network mapping photo person lower-dimensional embedding following considering facenet deﬁnitions embedding vector unnormalized embedding obtained intermediate facenet node normalized embedding calulated applying softmax activation function unnormalized vector. naively given embedding reconstruction could accomplished ﬁnding image minimizing metric embedding space however since practical applications space possible images much greater dimension embedding space inverse arbitrary generally high-dimensional manifold containing rich variety images small fraction could considered realisitic face images. sensible deﬁnition face reconstruction problem could thus written regularizer vanishes images within subset realistic face images otherwise. since generally difﬁcult deﬁne solve minimization problem classes regularization functions favour face-like images. approaches characterizing based using single reference guiding image since trained convolutional neural network deﬁning contains lower-level edge shape ﬁlters well complex features relevant face recognition guiding image regularization function constructed using intermediate nodes network. example could naturally chosen regularizer weight vector activations speciﬁc network layer chosen amongst lowest network layers regularizer effectively pulls towards image higher layers regularizer introduces restrictions higher-order features without necesserily forcing speciﬁc textures colors advantage using single image condition reconstruction possibility enforce speciﬁc pose facial expression background. disadvantage course fact ﬁnal image contain facial features corresponding embedding guiding image. values guiding image regularizer weight produced image look realistic frequently consists numerous face fragments. contrast large reconstruction almost indistinguishable guiding image. tuning value however possible produce realistic looking faces barely facial features leaked guiding image appendix also discuss alternative approach regularizer uses collection images instead single guiding image. regularizer force speciﬁc facial features generally results lower-quality images. numerical optimization frequently produces noisy distorted images. problem alleviated introducing additional regularizers. total-variation regularizer seen penalize images high-frequency noise large gradients sharp boundaries. here operators shifting entire image pixel direction correspondingly constant parameter. choice optimization function strong impact produced face reconstructions. paper consider families loss functions deﬁned normalized unnormalized embedding spaces. ﬁrst based metric embedding space i.e. another approach shown frequently result higherquality images employs dot-product provided embedding chosen regularizer parameters minimization problem solved numerically using stochastic gradient descent adam another optimization method starting random noise guiding image entering without regularizers iterative process converges image within small neighborhood initial state performing optimization guiding image regularization alone also unsuccessful reconstructing realistic face image. signiﬁcant improvement observed total-variation regularizer introduced initial state reconstruction also shown play important role starting guiding image instead random noise frequently improved stability quality produced images interestingly using sufﬁciently high allowed generate realistic images facial features embedding facial expression guiding image. running algorithm sequence video frames able perform face transfer embedding onto face shown video. result particularly impressive given embedding produced single photo person. positive effect regularizer previously observed example ref. speculate attributed suppresion high-frequency harmonics leading search local minimum subspace smooth images. indeed ∂rtv/∂p shown proportional discrete laplacian operator deﬁned p)xy px+y px−y pxy+ pxy− pxy. gradient descent step size sufﬁciently small expression update ∂l/∂p viewed discretization dynamical system continuous step number regularizer plays role diffusion term figure original images face reconstructions obtained using different techniques guiding image; source embedding used reconstruction; minimizes starting random noise; total variation added regularization; total variation guiding image regularization added intermediate layer; metric guiding image initialization; embedding guiding image loss functions ultimately need minimize. notice shown appendix limit generally ﬁnite number local minima longer form high-dimensional submanifolds total-variation regularizer proved essential essential converging towards realistic face image however also blurs output images. reduced blurring effect produced reconstruction reducing iteration number. also used laplacian pyramid normalization applied intermediate gradients. improved overall contrast image. choice guiding image also proven important high quality face reconstruction. guiding image embedding corresponded people different gender nationality produced images could resemble guiding image facial features borrowed embedding effect less noticeable large values case reconstructions worse quality unstable i.e. could drastically different different random initial conditions problem guiding image choice solved either building classiﬁer predicts gender nationality face corresponding given embedding attempting produce reconstructions different guiding images choosing result smallest embedding space distance. reconstruction obtained using methods discussed sec. requires hundreds even thousands iterations. training feed-forward neural network capable reconstructing image single pass could signiﬁcant performance advantage. main idea behind training network based using loss function employed iterative face reconstruction. speciﬁcally training step given random input embedding network weights updated minimize loss calculated image produced network. figure original images face reconstructions generic male guiding image; generic female guiding image; image used calculating target embedding reconstruction guiding image reconstruction guiding image figure architecture feed-forward network taking embedding input producing output image loss function used train weights network depended obtained passing output image facenet model architecture feed-forward network taking embedding guiding image inputs. intermediate tensors obtained applying sequence convolutions stride concatenated intermediate tensors feed-forward network output image passed facenet order produce entering loss function. feed-forward network used face reconstruction took -dimensional vector input produced image channels within network fully-connected layer ﬁrst used transform embedding vector tensor tensor passed sequence relu deconvolutions took input tensor size producing tensor size ln+. experiments deconvolution kernel size except last equal. ﬁnal tensor cropped facenet dimensions using loss function used iterative reconstruction could train network produce face-like image desired facenet embedding. feed-forward network described sec. trained perform face reconstruction guiding images instead one. case sparse guiding image index passed network inputs. unfortunately ﬁnite capacity network need somehow encode guiding images network weights could demonstrated even dozens similar guiding images approaches alleviating restriction potentially performing face reconstruction arbitrary guiding image based passing guiding image inputs feed-forward neural network. experiments input guiding image ﬁrst padded size passed series convolutional layers stride obtained tensors size depth-concatenated tensors spatial dimensions produced series deconvolutions described above. words starting tensor generated embedding vector ﬁnal convolution guiding image deconvolution consumed tensor produced tensor concatenated ¯ln+ tensor obtained guiding image. last tensor ﬁnally transformed convolution operation produce output image. section explore ability generate face images using iterative reconstruction feed-forward networks. quality reconstruction measures quality loss function ﬁnding recognizable faces. satisﬁed loss function reasonably conﬁdent gradient descent loss function produces recognizable faces turn attention feed-forward networks trained optimum loss single pass. experiments want illustrate recognizability people choose famous people order maximize recognizability reconstructed images. guiding images publicly available cartoon images ref. well averaged images ref. experiments pre-normalized embedding input. even though original facenet trained differentiate normalized embeddings thus ignore difference norm optimizing match pre-normalized embedding produces better results. conjecture normalization favors smaller embedding values essentially results generic looking image illustrated fig. experiments product. product produces signiﬁcantly sharper slightly less recognizable images demonstrated figs. somewhat surprisingly using normalized distance results much worse reconstructions. figure impact changing weight. ﬁrst column contains original images whose embeddings used. highlight difference apply brightness correction. ﬁgure fig. show ﬁrst three images adjusted brighness intensity. distance embedding different values function gradient descent iteration. note recognizable images away target embedding. example fig. used activation target single intermediate layer. remainder section attach -distance loss multiple intermediate layers well laplacian pyramid gradient normalization change spectral characteristics image gradients. technique improves quality reconstructed images. ﬁgure fig. show face reconstructions multiple celebrities. guiding images either cartoon-like faces average faces ref. face transfer examples including face transfer video supplementary materials. turn attention total-variation regularizer. ﬁgures fig. show increasing weight affects image quality. demonstrate impact high perform normalization image results subdued images. interesting note embedding reconstructed image gets away target shown fig. image becomes recognizable. another observation might independent interest images right-most column fig. extremely similar exhibit traits person whose embedding minimize. trained embeddings single guiding image feed-forward network described sec. taking facenet embedding input producing image output succesfully converged. images produced network several embeddings never seen training shown fig. model observed converge faster network weights initialized follows. deconvolution weights chosen produce smooth spatial interpolation random xaiver ﬁlter-to-ﬁlter connections ﬁnal deconvolution tuned produce grayscale output image. figure impact reconstruction using target metric target embedding scaled given factor. notably result obtained metric target scaled similar result obtained dot-product loss remarkable seemingly similar images produced feed-forward network still recognizible reconstructions provided embeddings. demonstration fact subtle change facial features frequently sufﬁcient distinguish person another. case iterative reconstruction generally expectation reconstructed image smooth unless apply regularizer convolutional networks smoothness faciliatedby fact lots parameters shared across entire image. approach quantifying quality constructed feed-forward network average distance reconstructed target embeddings. note contrast iterative reconstruction feed-forward network unlikely match input embeddings almost exactly means adding small perturbation guiding image. indeed since additive perturbation expected strongly depend input embedding memorizing complicated dependence require much greater network information capacity producing accurate smooth reconstructions. hand ﬁnal embedding space loss calculated images produced feed-forward network used choosing optimal model parameters. table shows average values total loss function embedding space distance product normalized embeddings calculated several trained feed-forward networks distance optimization) embedding vectors. even though feed-forward network seems perform best largest number ﬁlters using ﬁlters already results greater average value generally obtained different real photos single person. extent trained feed-forward network optimizes loss function compared iterative reconstruction algorithm. figure shows scatter plot comparing values obtained using approaches. would expect average full loss function calculated feed-forward solutions embedding space loss random embeddings) factor greater average loss obtained iterative reconstruction embeddings. interestingly results iterative reconstructions percieved worse produced feed-forward network. time average embedding space distance input output signiﬁcantly smaller iterative reconstruction. feed-forward network embedding guiding image inputs experiments trained feed-forward network described sec. random embedding vectors frames short video clip treated independent guiding images. network trained used embedding vector seen training stage frame sequence produce animation. procedure generally sucessfull performing face transfer embedding target video clip. frames resulting animation shown fig. demonstrated facenet embedding loss coupled simple regularization functions used succesfully reconstruct realistic looking faces. gradient descent simple deep figure face reconstructions obtained using feed-forward network trained generic male image fig. arnold schwarzenegger albert einstein; barack obama; ronald reagan. uses dot-product embedding loss bottom uses distance. figure original guiding images face reconstructions obtained using feed-forward network takes guiding image inputs. network trained frames short video clip orientation; orientation. neural networks shown produce high-quality results. work deﬁnes distinct areas guide future research. hand explore better regularization functions might improve quality generated images combine multiple networks precise control reconstructions. example would interesting explore capability controlling facial expression generated image using network trained recognize facial expressions hand order achieve fast generation need train deep network would solve minimization problem pass. interesting able achieve without using adversarial learning. suggests embedding produced unrelated network mostly complete sense contains enough information reconstruct recognizable image matches original human understandable sense. interesting extension would employ advanced architectures including utilizing recurrent networks. also combining techniques adversarial learning promising direction. christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages florian schroff dmitry kalenichenko james philbin. facenet uniﬁed embedding face recognition clustering. proceedings ieee conference computer vision pattern recognition pages jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition pages table dependence average total loss average distance average product calculated feed-forward network number ﬁlters weight wtv. unnormalized embeddings target reconstruction respectively. tilde denotes normalized value. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages aravindh mahendran andrea vedaldi. understanding deep image representations inverting them. computer vision pattern recognition ieee conference pages ieee arxivcs.lg/.. publicdomainvectors.org. lisa debruine jones. http//faceresearch.org experiments faces voice preferences. schaaf hateren. modelling power spectra natural images statistics instead using single image regularization could consider collection photos faces share characteristics like position pose facial expression. given function modeling distribution images constant regularizer could example deﬁned otherwise. however since many numerical optimization methods perform better smooth functions practically suitable choice could rgauss modelled gaussian distribution activation space normalization constant goes nodes layer node activations average values standard deviations activations computed images lower layers rgauss expected penalize images colors textures inconsistent present majority images higher layers turn rgauss would give preference images right higher-order features. notice rgauss given also thought natural deﬁning metric activation space. unlike guiding image regularizer arbitrarily uses metric invariant linear rescaling individual activations. practical applications neural network activations almost independent network input. since corresponding terms dominate rgauss introduce small parameter smoothing regularization function preventing singularities maxn∈ face reconstruction experiments used ﬁnal form regularizer. contrast experiments single guiding image reconstructions produced regularizer inherit facial features speciﬁc pre-deﬁned image. however also tend less photo-realistic since average activations {vn} include traces numerous images. lower layers example {vn} described blurred image obtained superimposing images collection since generally ﬁnite number solutions expect ﬁnite number local minima furthermore noticing γnxny total-variation regularizer indeed suppresses higher harmonics ∂l/∂p. natural images typically characterized intensity power spectrum obeying approximate power face reconstruction algorithm could regularized produce images tuned spectrum performing laplacian pyramid decomposition image. original image expand operator reduce operator laplacian pyramid deﬁned sequence images ˆegk+ ˆrgk. normalization regularizer favour images desired power spectrum component norm alternative approach based normalizing laplacian pyramid components gradient updates themselves. case reconstruction expected respect particular symmetry optimization problem regularized enforce symmetry. special case horizontal mirror symmetry regularizer could read", "year": 2016}