{"title": "A survey on independence-based Markov networks learning", "tag": ["cs.AI", "cs.LG"], "abstract": "This work reports the most relevant technical aspects in the problem of learning the \\emph{Markov network structure} from data. Such problem has become increasingly important in machine learning, and many other application fields of machine learning. Markov networks, together with Bayesian networks, are probabilistic graphical models, a widely used formalism for handling probability distributions in intelligent systems. Learning graphical models from data have been extensively applied for the case of Bayesian networks, but for Markov networks learning it is not tractable in practice. However, this situation is changing with time, given the exponential growth of computers capacity, the plethora of available digital data, and the researching on new learning technologies. This work stresses on a technology called independence-based learning, which allows the learning of the independence structure of those networks from data in an efficient and sound manner, whenever the dataset is sufficiently large, and data is a representative sampling of the target distribution. In the analysis of such technology, this work surveys the current state-of-the-art algorithms for learning Markov networks structure, discussing its current limitations, and proposing a series of open problems where future works may produce some advances in the area in terms of quality and efficiency. The paper concludes by opening a discussion about how to develop a general formalism for improving the quality of the structures learned, when data is scarce.", "text": "abstract problem learning markov network structure data become increasingly important machine learning many application ﬁelds. markov networks probabilistic graphical models widely used formalism handling probability distributions intelligent systems. document focuses technology called independence-based learning allows learning independence structure markov networks data eﬃcient sound manner whenever dataset suﬃciently large data representative sample target distribution. analysis technology work surveys current stateof-the-art algorithms discussing limitations posing series open problems future work produce advances area terms quality eﬃciency. nowadays intelligent systems reason realistic domains storing knowledge world supporting eﬃcient inference even exceptions occur. referred literature reasoning uncertainty. popular approach taken reasoning uncertainty probabilistic models statistical analysis tool statistical inference. statistical inference process used drawing conclusions data calculating probability propositional sentences. example probabilistic model tabular probabilistic model function represented table assigns probability every possible complete assignment domain probabilities adds figure illustrates abstract tabular model domain binary variables xn−} consisting tuples possible conﬁguration variables. however tabular model presents computational semantic limitations. first storage requirements exponential number variables size respective domains. schlüter e-mail federico.schluterfrm.utn.edu.ar home page http//dharma.frm.utn.edu.ar/fschluter/ lab. dharma artiﬁcial intelligence dept information systems facultad regional mendoza national technological university argentina. domains variables continuous tables inﬁnite practice mathematical functions used. nonetheless work attention restricted discrete distributions continuous variables considered discrete variables. second queries interest usually involve variables cost computing marginal conditional probabilities would result exponential summations variable combinations. third representation clear semantics humans. common pattern human knowledge probabilistic judgments small number propositions. therefore conditional independences natural representing probability distributions. common people judge three-place relationship conditional dependency i.e. inﬂuences given using independences reduce exponential requirements tabular model. example making simple assumption variables figure mutually independent allows decomposing joint probability distribution decomposition requires polynomial number exponentially smaller tables rows. figure illustrates model assuming binary variables mutually independent consisting tables tuples each. address problems namely exponential storage requirements exponential cost computing marginal conditional probabilities lack explicitness model several researchers late created probabilistic graphical models simply graphical models well-established formalism representing compactly joint probability distributions. composed independence structure numerical parameters. structure encodes independences present distribution deﬁnes family probability distributions. numerical parameters deﬁnes unique distribution among family quantiﬁes relationships structure. representation explained detail section important types graphical models bayesian networks markov networks pearl well-known bayesian networks graphical models encoding distributions dependencies representable directed acyclic graph. markov networks encode distributions dependencies representable undirected graph. three inﬂuential textbooks topic published last three decades pearl lauritzen koller friedman many applications graphical models wide range ﬁelds recent years. examples present areas computer vision image analysis. besag gives examples archeology epidemiology; anguelov addressing problem segmenting scan data objects object classes; complete textbook presents exposition markov random ﬁelds image restoration edge detection low-level domain object matching recognition high-level domain. examples present area spatial data mining geostatistics presented textbook cressie markov random fields emphasized modeling spatial lattice data; recently work shekhar presents spatial analysis methods applications markov random fields wide range ﬁelds including biology spatial economics environmental earth science ecology geography epidemiology agronomy forestry mineral prospection. also examples disease diagnosis schmidt presents markov random fields based method detecting coronary heart disease processing ultrasound images echocardiograms. also area computational biology friedman proposes bayesian networks discovering interactions among genes. applications graphical models present evolutive optimization searching larrañaga lozano describes bayesian networks modeling probability distribution individuals high ﬁtness evolutive algorithms recently alden shakya santana proposing markov networks purpose. examples shown information retrieval metzler croft model term dependencies using markov random fields; malware propagation karyotis analyzes spatial contextual dependencies malware propagation also using markov random fields. many interesting examples could included list. table summarizes examples order help readers choose method better solution certain application. representation compact declarative model knowledge based graphs. hand models compact providing representation conditional independences present probability distribution eﬃcient computationally tractable. compact representation graphical models achieved exploiting principle property present many distributions variables tend interact directly others. hand since models graphical declarative human expert understand evaluate semantics properties. inference given graphical model fundamental highly non-trivial task compute marginal distributions variables. task usually called inference. marginalization possible compute conditionals posteriors make predictions. inference also sub-routine learning tasks therefore elementary sub-routine graphical models. however proven cooper exact inference markov random fields spatial spatial analysis methods biology spatial economics environmental earth science ecology geography epidemiology agronomy forestry mineral prospection. np-hard general. several methods working directly structure graphical models practice orders magnitude faster manipulating explicitly joint probability distribution. textbook koller friedman provides extensive discussion topic describes popular methods used variable elimination monte carlo methods loopy belief propagation. recent works treereweighted message-passing wainwright power minka generalized belief propagation yedidia variational message-passing winn bishop free open source library providing implementations various exact approximate inference methods graphical models published recently mooij learning constructing graphical models done either human expert learning automatically data. many algorithms model probability distribution historical data returning graphical model solution. really useful since experts knowledge always enough design proper model. therefore authors consider algorithms tool knowledge discovery. moreover constructing models speciﬁc problem possible data-driven approach using part model provided expert ﬁlling details automatically ﬁtting model data. large number success stories claimed using approach recent years resulted authors koller friedman claiming models produced process usually much better purely hand constructed. work speciﬁc problem learning independence structure markov networks reviewed. interesting problem resulted important contributions domain recent years although many core challenges remain unresolved intense deliberation. work focuses technology called independence-based learning allows infer independence structure markov networks data eﬃcient sound manner whenever data suﬃcient representative sample target distribution. analysis current state-of-the-art algorithms learning markov networks structure using technology presented discussing current limitations potential improving quality eﬃciency current approaches. rest document structured follows section presents overview markov networks representation. section discusses problem learning markov networks data. section provides review current independence-based markov network structure learning algorithms. finally section analyzes surveyed independence-based algorithms discusses relative advantage well disadvantages concluding series open problems remain domain independence-based structure learning markov networks. section provides overview representation speciﬁc type graphical models markov networks. graphical models general consist qualitative quantitative component representing probability distribution distribution given domain variables denoted xn−}. qualitative component independence structure model represents conditional independences among domain variables deﬁnes family probability distributions. quantitative component numerical parameters deﬁnes unique distribution among family quantiﬁes relationships structure. independence structure compact representation conditional independences present underlying distribution variables independent conditioned variables knowing value tells nothing already know values variables work conditional independence denoted denotes conditional dependence. structure markov network undirected graph nodes representing random variable domain. edges graph encode conditional independences among variables. figure shows examples undirected structures representing domains variables ﬁrst example figure irregular fig. examples undirected independence structure irregular graph diﬀerent grade connectivity distinct nodes regular lattice variables belong domain spatial problem. graph diﬀerent grade connectivity distinct nodes. second figure regular lattice variables belong domain spatial problem typically used representing images dimensional ising spin glasses models independence structure independences underlying distribution independences read graph vertex separation considering variable conditionally independent non-neighbor variables graph given neighbor variables. called local markov property. example figure variables conditionally independent given variables toroidal lattice figure conditionally independent non-adjacent variables given neighbor variables correctly representing probability distribution markov network must independences present proved pearl graph called independence-map distribution independences encoded graph exist underlying distribution using graph i-map guarantees nodes found separated correspond independent variables guarantee showed connected dependent. conversely d-map guaranteed nodes connected dependent distribution fully-connected graphs trivial i-maps empty graphs trivial d-maps. distribution said perfect-map i-map d-map. axiomatic characterization family relations isomorphic vertex separation graphs given concept graph-isomorphism. basically distribution graphisomorph independences among variables encoded undirected graph. necessary suﬃcient condition distribution graph-isomorph satisﬁes following axioms independences introduced pearl anset axioms learning bayesian networks omitted here. disjoint subsets variables domain stands single variable intersection axiom valid strictly positive probability distributions. list axioms represents relationships hold among independences encoded graph. summary distribution graph-isomorph exists graph perfect-map representing distribution graph i-map used. however independences underlying distribution encoded graph better model complexity accuracy used inference. assuming graphisomorphism important decision since existent distributions represented undirected graph. example distributions represented acyclic directed graph case bayesian networks correct model use. also distributions cannot encoded graph. section describes concept markov blanket central theoretical concept representation distributions introduced pearl markov blanket variable knowledge needed predict behavior variable. hence concept holds relevance wide variety applications local relationships variables signiﬁcant. markov blanket variable smallest variables shields probabilistic inﬂuence variables blanket. graphical view point markov blanket variable identical neighbors graph. textbook pearl proved formally that strictly positive distributions independence structure constructed piecing together markov blanket variables domain connecting edge every variables belongs markov blanket also proof stating every variable distribution graph isomorph therefore satisﬁes pearl’s axioms unique markov blanket. strictly positive distributions satisfy intersection axiom mechanism constructing structure holds positive distributions. section explains quantify relationships encoded although work addresses problem structure learning quantitative aspects markov networks brieﬂy explained better clarifying work. described factorization method constructing gibbs distribution arbitrary undirected graph provided pearl identiﬁcation maximal subgraphs whose nodes adjacent other called maximal cliques example graph figure shows maximal clique size among nodes corresponding variables maximal cliques size among nodes rest edges maximal cliques size figure size cliques clique cliques graph assign non-negative potential function measuring relative degree compatibility associated possible conﬁguration usually potential function represented table numerical parameter assigned possible complete assignment variables compose clique like tabular model showed figure including variables compose clique diﬀerence tabular model parameter values normalized. using hammersley-cliﬀord theorem possible prove general form gibbs distribution equation embodies conditional independences encoded graph form gibbs distribution presents diﬃculties. first diﬃcult discern meaning potential functions. second computational cost calculating partition function exponential requires exponential possible assignments complete variables. section discusses diﬃculties arise task learning markov networks historical information. task possible whenever size input dataset suﬃcient data representative sample underlying distribution conditions satisﬁed possible algorithms learn model representing exploring analyzing input dataset contains historical information commonly structured tabular format standard format machine learning. contains table column random variable rows datapoints complete assignment variables. example datapoint domain random binary variables algorithms discussed work ignore problem missing values solved known computationally challenging statistical techniques. learning markov network data problem consists learning structure parameters course best possible structure learned perfect-map model contains structure encoding dependences independences present however every model containing structure i-map good solution. closer perfect-map better structure learned better resulting markov network representing learning model large domains desirable property model sparsity since densely connected models require many parameters make exact even approximate inferences computationally intractable. evaluating merits model learning method important consider goal learning. clearly learning complete model ideal method computational spatial sampling limitations possible practice. reason less ambitious goals often considered practice three main goals learning discussed koller friedman density estimation common reason learning markov network inference task. formulating goal learning density estimation goal construct model deﬁned distribution close underlying distribution common metric evaluating quality approximation likelihood data however goal assumes overall distribution needed. speciﬁc prediction tasks goal predicting distribution particular variables given certain variables model used perform particular task model never evaluated predictions variables better optimize learning task improving quality answers goal large fraction work machine learning. example consider problem documents classiﬁcation given relevant words document variable labels topic document. another well-known example task image segmentation goal task prediction class labels pixels image given image features. knowledge discovery goal knowledge discovery learn correct structure underlying distribution. cases learned structure reveal important unknown properties domain. diﬀerent motivation learning distribution. examination learned structure show dependences among variables positive negative correlations. knowledge discovery application critical assess conﬁdence prediction taking account extent identiﬁed given available data number hypotheses would cause similar observed behavior. example medical diagnosis domain want learn structure model discover predisposing factors lead certain diseases symptoms associated diﬀerent diseases. markov network parameters estimation usually used choose value parameters ﬁtting model data tuning parameters manually often diﬃcult learned models often exhibit better performance. task shown np-hard problem barahona estimating parameters common method proposed maximum-likelihood estimation potentially using regularization additional parameter prior. unfortunately evaluating likelihood complete model requires every parameters proposed maximum-likelihood estimation process computation partition function used normalizing product possible value combinations variables domain showed equation although possible optimize maximum-likelihood closed form guaranteed global optimum found concave function. result approximations heuristics literature introduced minka vishwanathan reducing cost parameters estimation using iterative methods simple gradient ascent sophisticated optimization algorithms. unfortunately problem remains intractable practice partition function couples parameters across network requiring several inference steps network reducing cost parameters estimation solutions proposed. pseudolikelihood besag score matching hyvärinen dayan tractable approximate alternatives. loopy belief propagation method proposed pearl later yedidia variants introduced wainwright jordan uses approximate inference technique approximating gradient maximum likelihood function. another solution outperforming robustness loopy belief propagation provided ganapathi broad approaches learning structure markov networks data scorebased independence-based approaches. former intractable practice latter eﬃcient presents quality problems. approaches motivated distinct learning goals generally score-based approaches better suited density estimation goal tasks inferences predictions required. explained section score-based methods learn complete markov network overwhelmingly markov networks settings including image segmentation others exists particular inference task mind. instead independence-based methods better suited remaining goals speciﬁc prediction tasks knowledge discovery. hand independence-based algorithms commonly used tasks feature selection classiﬁcation since possible perform local discovery particular variables interest hand independence-based algorithms suited knowledge discovery tasks tasks understanding interactions among variables domain carries greatest importance structure viewed purely predictive tool example econometrics psychology sociology. since work focuses independence-based approach markov networks structure learning methods sections discuss detail state-of-the-art independence-based algorithms. score-based algorithms proposed learning structure bayesian networks works bacchus heckerman later proposed learning structure markov networks works della pietra mccallum algorithms approach problem optimization space complete models looking maximum score. goal score-based algorithms model maximizes score. traditional score-based algorithms perform global search learn potential functions accurately captures high-probability regions instance space complete models. standard approach learning structure markov networks score-based approach della pietra algorithm. algorithm learns structure inducing potential functions data. strategy based top-down search generalto-speciﬁc search. algorithm starts atomic potentials then creates candidate potentials ways. first potential currently model conjoined every potential model. second potential model composed atomic potential. then eﬃciency reasons parameters learned candidate potential assuming parameters potentials remain unchanged. setting parameters uses gibbs sampling inference. then candidate potential algorithm evaluates much adding potential would increase log-likelihood score used algorithm. potential maximizes measure added. candidate potential improves score model procedure ends. another algorithm using approach proposed mccallum similar algorithm proposed della pietra performing eﬃcient heuristic search space candidate structures automatically inducing potentials improve conditional log-likelihood. however reported davis domingos general-to-speciﬁc search methods ineﬃcient test many potential variations support data highly prone local optima. recently alternative approaches considered. approach höﬂing tibshirani ravikumar propose couple parameters learning potentials induction step using l-regularization forces numerical parameters zero. approach problem optimization problem providing large initial potential possible potentials interest. then learning model selection occurs selecting potentials non-zero parameters. eﬃciency reasons approaches höﬂing tibshirani ravikumar construct pairwise networks instead algorithm learn arbitrarily long potentials. practice however evaluated inducing potentials length recent alternative approach proposed davis domingos called bottomlearning markov networks algorithm. starts complete training example long potential markov network. then algorithm iterates potential generalizing potential match k-nearest previously unmatched examples dropping variables. generalized potential improves score model incorporated model. loop ends generalization improve score. however approaches often slow reasons. first size search space structures intractable number variables. second evaluating score step necessary compute score requiring estimation numerical parameters independence-based algorithms work performing succession statistical independence tests discovering independence structure graphical models. algorithms exploit semantics independence structure casting problem structure learning instance constraint satisfaction problem constraints independences present input dataset goal structure encoding independences. independence test consults data responding query conditional independence among input random variables given conditioning variables resulting independence assertion dependence assertion. computation cost statistical tests proportional number rows input dataset number variables involved. examples independence tests used practice mutual information cover thomas pearson’s agresti bayesian test margaritis continuous gaussian data partial correlation test spirtes independence tests compute statistical value triplet variables given input dataset decide independence dependence comparing threshold. instance p-value computed probability obtaining test statistic least extreme actually observed assuming null hypothesis true null hypothesis rejected p-value less signiﬁcance level often null hypothesis rejected result said statistically signiﬁcant. elegant eﬃcient scalable strategy used several independence-based algorithms literature called local-to-global strategy presented recent work aliferis generalization previous algorithms using strategy. algorithm shows outline theoretically sound straightforward procedure omitting third step edges orientation used learning bayesian networks strategy suggests construct independence structure dividing problem diﬀerent markov blanket learning problems markov blanket learned variable domain learning markov blanket generalized aliferis learning bayesian networks generalized local learning framework aliferis algorithms using local-to-global strategy learn locally markov blanket every variable domain construct global structure linking variables every member markov blanket using rule learning bayesian networks structure independence-based algorithms ﬁrst arose spirtes published well-known algorithms ﬁrst edition textbook. then independence-based algorithms appeared works feature selection induction markov blanket works bayesian markov networks structure learning. reason series independence-based algorithms markov blanket learning bayesian networks appeared koller-sahami algorithm koller sahami grow-shrink algorithm margaritis thrun incremental association markov blanket algorithm variants tsamardinos max-min parents children markov blanket algorithm tsamardinos hiton-pc/mb algorithm aliferis fast-iamb algorithm yaramakala margaritis parent-children markov blanket algorithm peña iterative parent children markov blanket desmarais summary important aspects algorithms shown table reproduced conclusions recent review markov blanket based feature selection written desmarais learning markov networks structure independence-based algorithms arose later bromberg published grow-shrink markov network algorithm grow-shrink inference-based markov network algorithm. then independence-based algorithms appeared markov networks structure learning particle filter markov network algorithm bromberg margaritis margaritis bromberg dynamic grow shrink inference-based markov network algorithm gandhi another approach proposed bromberg bromberg margaritis framework based argumentation improving reliability tests. section sound theory data eﬃcient making topology information poor time eﬃciency distinguish spouses parents/children distinguish children parents/children sound theory data eﬃcient compared previous algorithms much faster pcmb computing distinguish spouses parents/children distinguish children parents/children best trade-oﬀ among family algorithms several advantages independence-based algorithms. first learn structure without interleaving expensive task parameters estimation reaching sometimes polynomial complexities number statistical tests performed. complete model required parameters estimated given structure. another important advantage algorithms sound statistical tests outcomes correct structure found correctly represents underlying distribution. however correct following assumptions third condition soundness important problem independence-based algorithms. dataset used learning suﬃciently large representative sampling underlying distribution outcomes tests incorrect structures learned deemed unreliable. problem statistical tests unreliability exponentially exacerbated number variables involved good quality statistical tests require enough counts contingency tables exponentially number example cochran recommends test must deemed unreliable cells expected count less data points. another disadvantage independence-based algorithms guarantee quality complete model obtained learning structure ﬁrst ﬁtting parameters learned structure. approximation experimental results published literature independence-based methods learning complete models. section reviews independence-based structure learning algorithms markov networks appeared literature. review section covers series published algorithms tackle problem. grow-shrink markov network algorithm introduced bromberg ﬁrst independence-based structure learning algorithm markov networks literature. algorithm adaptation markov networks algorithm margaritis thrun learning markov blanket. gsmn algorithm learns global structure markov network following simple outline local-to-global algorithms showed algorithm using algorithm outlined algorithm discovering markov blanket variables. maintains called contains markov blanket input variable algorithm terminates. first line performs initialization phase sorts increasing association rest variables domain using unconditional test every variable {x}. then algorithm proceeds stages grow shrink phases using ordering. grow phase algorithm increases every variable found dependent conditioning current state phase contains members markov blanket potentially includes false positives non-members. false positives removed shrink main advantages gsmn sound eﬃcient. soundness gsmn proven theoretically authors guaranteeing correct independence structure found statistical tests reliable. algorithm eﬃcient polynomial number independence tests discovering structure test requiring polynomial time execution respect variables involved test size input dataset. disadvantage using unreliable statistical tests produce cascade errors incorrect outcomes also generate next incorrect tests grow shrink phases producing cumulatively errors stated spirtes important algorithms learning markov blanket variable bayesian networks incremental association markov blanket algorithm tsamardinos hiton algorithm aliferis algorithms proven empirically resilient errors statistical tests introducing simple variants. hand iamb algorithm introduce modiﬁcation interleaving initialization step ordering grow phase interleaving sorting step grow phase iamb maximizes accuracy reducing number false positives grow phase. hand hiton algorithm aims reduce data requirements iamb introducing additional modiﬁcation criteria used testing independence. grow shrink phases instead conditioning tentative markov blanket hiton tests independence conditioning subsets statistical tests reliable containing fewer variables modiﬁcation exploits strong union axiom pearl improving quality independence tests data scarce. disadvantage approach proposed hiton exponential cost general comparatively smaller size domain summary algorithms proven better quality designed learning structure bayesian networks works literature proposing theoretical adaptation ideas learning complete structure markov network empirically evaluating performance. grow shrink inference markov network algorithm presented bromberg algorithm works similar fashion gsmn algorithm using local-to-global strategy algorithm learning markov blanket variables algorithm interleaving inference step reduce number tests required learn markov blanket. using theorem inference called triangle theorem authors gsimn reduces number tests performed data without adversely aﬀecting quality learned structures. useful using large datasets distributed domains statistical tests expensive. gsimn tests independence data ﬁrst applies triangle theorem tests already done data order check independence assertion logically inferred. test cannot inferred done data stored. convenience algorithm determines visit ordering attempt maximize inferences. results obtained gsimn show savings running times gsmn obtaining comparable qualities. particle filter markov networks algorithm presented bromberg margaritis margaritis bromberg novel independence-based approach learning markov network structures. previous independence-based algorithms reviewed gsmn gsimn local-to-global strategy. instead algorithm learns directly global structure solution. pfmn designed improving eﬃciency gsimn algorithm. algorithm works performing statistical independence tests iteratively selecting greedily iteration statistical test eliminates major number inconsistent structures. decision taken ﬁrst modeling learning problem bayesian approach selecting solution structure maximizes posterior probability since direct computation probability intractable pfmn propose generative model independence tests approximation posterior probability. model possible compute eﬃciently probability given information independences. moreover authors claim possible demonstrate that assumption correctness tests distribution converges correct structure. approach useful domains independence tests expensive cases large data sets distributed domains. results obtained pfmn show improvements running times respect gsimn comparable qualities structures found gsimn gsmn. gsimn uses triangle theorem avoiding unnecessary tests. outline dgsimn similar gsmn gsimn using local-to-global strategy algorithm algorithm showed algorithm learning markov blanket variables interleaving diﬀerent inference step gsimn reducing number tests performed. dgsimn improves gsimn algorithm dynamically selecting locally optimal test increase state knowledge structure estimating number inferred independences obtained executing test selecting maximizes number inferences. helps decreasing number tests required evaluated data resulting overall decrease computational requirements algorithm. results experiments dgsimn algorithm shows improves ﬁxed ordering variables markov blanket learning subroutine improving running times gsimn obtaining comparable qualities gsmn. algorithms presented previous sections independence-based algorithms focus improving eﬃciency ignoring important problems quality learned structures arises statistical tests reliable data scarceness. independence-based approach dealing unreliable tests presented bromberg bromberg margaritis modeling problem reliability independence tests knowledge base independence assertions contain errors incorrect statistical tests performed pearl’s axioms advantage approach power correcting errors tests exploiting logically independence axioms pearl. exist independence assertions knowledge base conﬂict clear independence assertions incorrect approach proposes resolve conﬂicts argumentation framework defeasible logic proposed amgoud cayrol reason correct errors. approach presented robust conditional independence test called argumentative independence test learning bayesian markov networks. experimental evaluation shows signiﬁcant improvements accuracy argumentative independence test simple statistical tests improvements accuracy blanket discovery algorithms disadvantage approach that propositional formalism requires propositionalizing rules pearl ﬁrst-order. rules super-sets sub-sets variables propositionalization involves exponential number propositions then exact argumentative algorithm proposed exponential. work approximate solution presented polynomial running time still improving quality experimental evaluation making drastic rather simplistic approximation provide theoretical guarantees. section analyzes independence-based algorithms surveyed discussing relative advantages disadvantages describes series open problems future works produce advances area. independence-based algorithms learning markov networks able learn independence structure eﬃciently important advantage sound data sample markov network tests reliable underlying distribution strictly positive. algorithms perform succession statistical independence tests learn conditional independences present data assume independences satisﬁed underlying model. structure learned querying independences data greedily discarding structures inconsistent single structure left. important source errors algorithms cascade errors produced erroneous statistical tests produces cumulatively errors. complexity learn structure performing polynomial number tests number variables domain. fact together evidence statistical tests time proportional number rows input dataset result algorithms e.g. gsmn total execution time polynomial compared score-based algorithms learn structure without need interleaved estimation numerical parameters model main source intractability score-based algorithms markov networks. strength independence-based algorithms learn correctly structure assumptions. however equivalent theoretical guarantee correctness complete model resulting learning parameters structure. independence-based algorithms present literature learning structure markov network gsmn gsimn pfmn dgsimn. related structure learning argumentative independence test approach improving quality conditional independences discovery. table shows summary important features approaches. gsmn algorithm direct extension algorithm markov networks structure learning requires polynomial number tests number variables domain algorithm presented together gsimn algorithm improves eﬃciency gsmn exploiting pearl’s independence axioms infer unknown independences independences observed avoiding need performing redundant statistical tests. important datasets large datasets present distributed environments. results obtained gsimn show savings running times obtaining comparable qualities gsmn. pfmn algorithm designed improving eﬃciency gsimn. algorithm work local-to-global fashion. instead uses model eﬃciently computing approximate posterior probability structures results obtained pfmn show improvements running times respect gsimn equivalent quality learned structures. similarly dgsimn algorithm designed improving eﬃciency gsimn enhancing ﬁxed ordering variables markov blanket learning subroutine dynamic ordering mechanism. experiments published dgsimn show improvements running times gsimn still maintaining quality gsmn. important problem independence-based algorithms learning structure markov networks problem quality statistical independence tests reliable. problem tackled either gsmn gsimn dgsimn pfmn. approach presented improving quality uncertainty tests outcomes argumentative independence test. experimental results using approach show signiﬁcant improvements accuracy standard independence tests exact algorithms presented improving quality exponential cost approximate algorithm proposed make drastic sound assumptions local-to-global strategy triangle theorem reducing number tests performed useful using large datasets distributed domains savings running times respect gsmn comparable quality respect gsmn sound assumptions local-to-global strategy designed improving eﬃciency gsimn dynamic ordering reducing number tests performed useful using large datasets distributed domains savings running times respect gsimn comparable quality respect gsmn summary advantages independence-based algorithms learning markov networks overshadowed quality learned structures data scarce equivalently underlying network highly connected. independence-based algorithms currently implemented practice learning markov networks. however approach presents important advantages motivate work area. first independence-based algorithms sound eﬃcient. second data availability growing increasingly time. third several promising open problems whose following analysis last section section discusses series open problems remain area. listed problems focus quality eﬃciency independence-based approach learning markov networks. independence-based algorithms surveyed learn markov blanket variables using algorithm. learning structure using seen greedy search space structures outcomes tests used discarding structures inconsistent independence indicated test. therefore important source errors cascade errors produced erroneous statistical tests produces cumulatively errors. open problem independence-based quality measures. pfmn algorithm uses particle ﬁlter approach optimizing selection tests perform. utilizes generative model computes approximately posterior probability independence structures given data. interestingly posterior probability eﬃciently computed used quality measure candidate structures optimization method. measure structures quality advantage avoiding cascade errors assigning probabilities structures. unexplored area learning structure markov networks. open problem speeding independence-based algorithms. learning structure under independence-based approach requires cases execution massive amount statistical independence tests data. intermediate step computation independence tests construction contingency tables dataset record frequency distribution variables involved test resulting running times linear size dataset. open problem inconsistencies local-to-global algorithms. independence-based algorithms using local-to-global strategy decompose problem learning complete independence structure variables independent markov blanket learning problems. second step algorithms piece together learned markov blankets global structure using rule. insuﬃcient data result incorrect learning markov blankets conﬂicts decision edge inclusion when variables found blanket found blanket cases rule always decides edge making mistakes edge exist. experimental results published comparing quality complete models models learned score-based approach versus models learned independence-based approach open problem adapting recent bayesian network ideas markov networks. ﬁrst independence-based algorithm proposed gsmn adaptation markov networks algorithm. literature several recently proposed ideas improving eﬃciency quality sample complexity discussed authors iamb. mmpc/mb hiton-pc/mb fast-iamb pcmb ipc-mb algorithms however interesting ideas originally developed tested learning structure bayesian networks. open problem independence knowledge bases. argumentative independence test improves accuracy tests signiﬁcantly data scarce. however exact algorithm proposed approach runs exponential time pearl’s axioms ﬁrst-order logics knowledge bases argumentation propositional approximate solution presented polynomial running time still improving quality making drastic rather simplistic approximation provide theoretical guarantees. open problem relating independence assertions. statistical tests procedures independently other used black independence-based algorithms. test responds conditional independence query using input dataset. thus implicit assumption made independence-based algorithms independences queried algorithm mutually independent given dataset. assumption true data suﬃciently large test determine true underlying independence case information tests irrelevant. however data suﬃcient correctly determining independence tests become dependent given data i.e. information tests useful determining value test avoiding errors. example shown literature correcting errors data insuﬃcient argumentative independence test relates statistical tests pearl’s axioms additional information improving quality tests data suﬃcient. present work discussed relevant technical aspects problem learning markov network structure data stressing independence-based algorithms. summarizing analysis technology advantages independence-based algorithms learning markov networks overshadowed quality learned structures data scarce equivalently underlying network highly connected. however approach presents important advantages motivate work area. first independence-based algorithms sound assumptions eﬃcient. second data availability growing increasingly time. therefore expected solutions open problems posed work result improvements quality structures produced technology.", "year": 2011}