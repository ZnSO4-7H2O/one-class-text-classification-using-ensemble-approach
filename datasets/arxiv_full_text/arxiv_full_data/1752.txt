{"title": "Neural Variational Inference for Text Processing", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.", "text": "recent advances neural variational inference spawned renaissance deep latent variable models. paper introduce generic variational inference framework generative conditional models text. traditional variational methods derive analytic approximation intractable distributions latent variables construct inference network conditioned discrete text input provide variational distribution. validate framework different text modelling applications generative document modelling supervised question answering. neural variational document model combines continuous stochastic document representation bagof-words generative model achieves lowest reported perplexities standard test corpora. neural answer selection model employs stochastic representation layer within attention mechanism extract semantics between question answer pair. question answering benchmarks model exceeds previous published benchmarks. probabilistic generative models underpin many successful applications within ﬁeld natural language processing popularity stems ability unlabelled data effectively incorporate abundant linguistic features learn interpretable dependencies among data. however successes tempered fact structure generative models becomes deeper complex true bayesian inference becomes intractable high dimensional integrals required. markov chain monte carlo variational inference standard approaches approximating integrals. however computational cost former results impractical training large deep neural networks fashionable latter conventionally conﬁned underestimation posterior variance. lack effective efﬁcient inference methods hinders ability create highly expressive models text especially situation model non-conjugate. paper introduces neural variational framework generative models text inspired variational autoencoder principle idea build inference network implemented deep neural network conditioned text approximate intractable distributions latent variables. instead providing analytic approximation traditional variational bayes neural variational inference learns model posterior probability thus endowing model strong generalisation abilities. ﬂexibility deep neural networks inference network capable learning complicated non-linear distributions processing structured inputs word sequences. inference networks designed restricted multilayer perceptrons convolutional neural networks recurrent neural networks approaches rarely used conventional generative models. using reparameterisation method inference network trained back-propagating unbiased variance gradients w.r.t. latent variables. within framework propose neural variational document model document modelling neural answer selection model question answering task selects sentences correctly answer factoid question candidate sentences. nvdm unsupervised generative model text aims extract continuous semantic latent variable document. model interpreted variational auto-encoder encoder compresses bag-of-words document representation continuous latent distribution softmax decoder reconstructs document generating words independently. primary feature nvdm word generated directly dense continuous document representation instead common binary semantic vector experiments demonstrate neural document model achieves stateof-the-art perplexities newsgroups rcv-v. nasm supervised conditional model imbues lstms latent stochastic attention mechanism model semantics question-answer pairs predict relatedness. attention model designed focus phrases answer strongly connected question semantics modelled latent distribution. mechanism allows model deal ambiguity inherent task learns pair-speciﬁc representations effective predicting answer matches rather independent embeddings question answer sentences. bayesian inference provides natural safeguard overﬁtting especially training sets available task small. experiments show lstm latent stochastic attention mechanism learns effective attention model outperforms previously published results strong nonstochastic attention baselines. summary demonstrate effectiveness neural variational inference text processing diverse tasks. models simple expressive trained efﬁciently highly scalable stochastic gradient back-propagation. neural variational framework suitable unsupervised supervised learning tasks generalised incorporate type neural networks. inference models complex deep structure. section introduce generic neural variational inference framework apply unsupervised nvdm supervised nasm follow sections. deﬁne generative model latent variable considered stochastic units deep neural networks. designate observed parent child nodes respectively. hence joint distribution generative model pθpθp variational lower bound derived eqpθp pθpθpdh parameterises generative distributions order tight lower bound variational distribution approach true posterior here employ parameterised diagonal gaussian diag)) three steps construct inference network type deep neural networks suitable observed data; concatenates vector representations conditioning variables; linear transformation outputs parameters gaussian distribution. sampling variational distribution able carry stochastic back-propagation optimise lower bound training model parameters together inference network parameters updated stochastic back-propagation based samples drawn gradients w.r.t. form worth mentioning unsupervised learning special case neural variational framework parent node case directly drawn prior instead conditional distribution pθpθ discuss scenario latent variables continuous parameterised diagonal gaussian employed variational distribution. however framework also suitable discrete units modiﬁcation needed replace gaussian multinomial parameterised outputs softmax function. though reparameterisation trick continuous variables applicable case policy gradient approach help alleviate high variance problem stochastic estimation. proposed variational inference framework semi-supervised learning prior distribution hidden variable remains standard gaussian prior apply conditional parameterised gaussian distribution jointly learned variational distribution. neural variational document model simple instance unsupervised learning continuous hidden variable generates words document independently introduced represent semantic content. bag-of-words representation document one-hot representation word position unsupervised generative model could interpret nvdm variational autoencoder encoder compresses document representations continuous hidden vectors softmax decoder reconstructs documents independently generating words docgaussian prior here consider observed documents. conditional probability words modelled multinomial logistic regression shared across documents rk×|v learns semantic word embeddings represents bias term. supervision information latent semantics posterior approximation conditioned current document inference network diag)) modelled document neural network generates parameters parameterise latent distribution document semantics based samples lower bound optimised back-propagating stochastic gradients w.r.t. since standard gaussian prior gaussian kldivergence dklp] computed analytically lower variance gradients. moreover also acts regulariser updating parameters inference network answer sentence selection question answering paradigm model must identify correct sentences answering factual question candidate sentences. assume question associated answer sentences together judgements answer correct otherwise. classiﬁcation task treat training data point triple predicting unlabelled question-answer pair neural answer selection model supervised model learns question answer representations predicts relatedness. employs different lstms embed question inputs answer inputs state outputs lstms positions states. conventionally last state outputs independent question answer representations used relatedness prediction. nasm however learn pair-speciﬁc representations latent attention mechanism effective pair relatedness prediction. nasm applies attention model focus words answer sentence prominent predicting answer matched current question. instead using deterministic question vector nasm employs latent distribution model question semantics parameterised diagonal gaussian diag)). therefore attention model extracts context vector iteratively attending answer tokens based stochastic vector model able adapt ambiguity inherent questions obtain salient information attention. compared deterministic counterpart question semantics) stochastic units incorporated nasm allow multi-modal attention distributions. further marginalising latent variables nasm robust overﬁtting important small question answering training sets. model conditional distribution question neural network generates corresponding parameters parameterise latent distribution question semantics following bahdanau attention model deﬁned normalised attention score answer token context vector weighted state outputs adopt question answer representations predicting relatedness deterministic vector equal combination sequence output context vector prediction pair relatedness model conditional probability distribution sigmoid function maximise log-likelihood variational lower bound l=eqφ za)]−dkl||pθ) following neural variational inference framework construct deep neural network inference network also modelled lstms relatedness label modelled simple linear transformation vector according joint representation generate parameters parameterise variational distribution question semantics emphasise though modelled parameterised gaussian distributions approximation functions inference producing samples compute stochastic gradients generative distribution generates samples predicting question-answer relatedness based samples sgvb optimise lower bound model parameters inference network parameters updated jointly using stochastic gradients. case similar nvdm gaussian divergence dkl)pθ] analytically computed training process. experiment nvdm standard news corpora newsgroups reuters rcv-v. former collection newsgroup documents consisting training test articles. latter large collection reuters newswire stories training test cases. vocabulary size datasets make direct comparison prior work follow preprocessing procedure setup hinton salakhutdinov larochelle lauly srivastava mnih gregor train nvdm models dimensional document representations respectively. inference network layers dimension rectiﬁer linear units converts document representations embeddings. training carry stochastic estimation taking sample estimating stochastic gradients prediction samples predicting document perplexity. model trained table experimental results traditional topic model models documents mixtures topics undirected topic model implemented restricted boltzmann machines docnade neural topic model based autoregressive assumption. models based sigmoid belief networks deep autoregressive neural network structures implemented mnih gregor employs build monte carlo control variate estimator stochastic estimation. newsgroups dataset latent variables dimension. assume dimension latent space represents topic corresponds speciﬁc semantic meaning. table presents randomly selected topics words strongest positive connection topic. based words column deduce corresponding topics space religion encryption sport policy. although model impose independent interpretability latent representation dimensions still nvdm learns locally interpretable structure. experiment answer selection datasets qasent wikiqa datasets. qasent created trec track wikiqa constructed wikipedia less noisy less biased towards lexical overlap. table summarises statistics datasets. adam tuned hold-out validation perplexity. alternately optimise generative model inference network ﬁxing parameters updating parameters other. table presents test document perplexity. ﬁrst column lists models second column shows dimension latent variables used experiments. ﬁnal columns present perplexity achieved topic model newsgroups rcv-v datasets. document modelling perplexity computed exp) number documents represents length document baseline models listed table apply discrete latent variables nvdm employs continuous stochastic document representation. experimental results indicate nvdm achieves best performance datasets. experiments rcv-v dataset nvdm latent variable dimension performs even better fdarn dimension. demonstrates document model continuous latent variables higher expressiveness better generalisation ability. table compares nearest words selected according semantic vector learned nvdm docnade. order investigate effectiveness nasm model also implemented strong baseline models vanilla lstm model lstm model deterministic attention mechanism former directly applies matching function independent question answer representations last state outputs question answer lstm models. latter adds attention model learn pair-speciﬁc representation prediction basis vanilla lstm. moreover lstm+att deterministic counterpart nasm neural network architecture nasm. difference replaces stochastic units deterministic ones inference network required carry stochastic estimation. following previous work models also lexical overlap feature combining co-occurrence word count feature probability generated neural model. adopted evaluation metrics task. facilitate direct comparison previous work follow experimental setup severyn word embeddings obtained running wordvec tool english wikipedia dump aquaint corpus. lstms layers hidden units table results models comparison state models qasent wikiqa dataset. paragraph vector bigram-cnn simple convolutional model reported deep deep convolutional model model based word alignment lclr svm-based classiﬁer trained using features. model means result obtained combination lexical overlap feature output distributional model. apply dropout embedding layer. construction inference network layers tanh units dimension layers tanh units dimension modelling joint representation. training carry stochastic estimation taking sample computing gradients prediction samples calculate expectation lower bound. figure presents standard deviation nasm’s scores using different numbers samples. considering trade-off computational cost variance chose samples prediction experiments. models trained using adam hyperparameters selected optimising score development set. table compares results models current state-of-the-art models answer selection datasets. qasent dataset vanilla lstm model outperforms deep model approximately stated evaluation scripts used previous work noisy questions test treated answered incorrectly. makes scores lower true scores. since severyn wang ittycheriah cleaned-up evaluation scripts apply original noisy scripts re-evaluate outputs conditional distributions questions selected different groups start ‘how’ ‘what’ ‘who’ ‘when’ ‘where’. standard deviations initialised zero training. according figure questions starting ‘how’ white areas indicates higher variances uncertainties dimensions. contrast questions starting ‘what’ black squares almost every dimension. intuitively difﬁcult understand answer questions starting ‘how’ others ‘what’ questions commonly explicit words indicating possible answers. validate this compute stratiﬁed scores based different question type. ’how’ questions lowest among groups. hence empirically ’how’ questions harder ’understand answer’. shown experiments neural variational inference brings consistent improvements performance tasks. basic intuition latent distributions grant ability possibilities terms semantics. perspective optimisation important reasons bayesian learning guards overﬁtting. according nvdm since adopt standard gaussian prior divergence term dklp] analytically computed diag|). difﬁcult actually acts regulariser update similarly nasm also divergence term dkl)pθ]. different nvdm attempts minimise distance between conditional distributions. well learned training distributions mutually restrained updated. therefore nvdm simply penalises large encourages mrr. lstm+att performs slightly better vanilla lstm model nasm improves results further. since qasent dataset biased towards lexical overlapping features combining co-occurrence word count feature best model nasm outperforms previous models including neural network based models classiﬁers hand-crafted features similarly wikiqa dataset models outperform previous distributional models large margin. including word count feature models improve achieve state-of-the-art. notably datasets lstmbased models strong baselines nasm works even better demonstrates effectiveness introducing stochastic units model question semantics answer sentence selection task. figure compare effectiveness latent attention mechanism deterministic counterpart visualising attention scores answer sentences. negative answer sentences neither attention models attend reasonable words beneﬁcial predicting relatedness. correct answer sentences ones figure attention models able capture crucial information attending different parts sentence based question semantics. interestingly compared deterministic counterpart lstm+att nasm assigns higher attention scores prominent words relevant question forms peaked distribution turn helps model achieve better performance. order intuitive observation latent distributions present hinton diagrams standard deviation parameters hinton diagram size square proportional value’s magnitude colour indicates sign approach prior every document nasm acts like moving baseline distribution regularises update every different conditions. practice carry early stopping observing prediction performance development dataset question answer selection task. using learning rate neural network structure lstm+att reaches optimal performance starts overﬁt training dataset generally iteration nasm starts overﬁt around iteration. interestingly question answer selection experiments nasm learns peaked attention scores deterministic counterpart lstm+att. update process lstm+att exists relatively variance gradients w.r.t. question semantics nasm applies stochastic training dataset small contains many negative answer sentences brings beneﬁt noise learning attention model. contrast update process nasm observe stable gradients w.r.t. parameters latent distributions. optimisation lower bound hand maximises conditional log-likelihood hand minimises kl-divergence hence update lower bound actually keeps gradients w.r.t. swinging heavily. besides since values signiﬁcant case distribution attention scores mainly depends therefore learning attention model beneﬁts regularisation well explains fact nasm learns peaked attention scores turn helps achieve better prediction performance. since computations nvdm nasm parallelised sample required during training process efﬁcient carry neural variational inference. moreover nvdm nasm parameters updated backpropagation. thus increased computation time stochastic units comes added parameters inference network. control variates approximating posterior importance sampling instantiations ideas demonstrated strong performance tasks image processing. recent variants generative auto-encoder also competitive. tang salakhutdinov applies similar idea introducing stochastic units expression classiﬁcation inference carried monte carlo algorithm reliance importance sampling less efﬁcient lack scalability. another class neural generative models make autoregressive assumption applications models document modelling achieve signiﬁcant improvements generating documents compared conventional probabilistic topic models also rbms models binary semantic vectors nvdm employs dense continuous document representations expressive easy train. semantic word vector model also employs continuous semantic vector generate words model trained inference permit calculation posterior distribution. similar idea nvdm bowman employs generate sentences continuous space. apart work mentioned above interesting work question answering deep neural networks. popular streams mapping factoid questions answer triples knowledge base moreover weston sukhbaatar kumar further exploit memory networks long-term memories dynamic knowledge bases. another attention-based model applies attentive network help read comprehend long articles. training inference network approximate variational distribution ﬁrst proposed context helmholtz machines applications directed generative models come problem establishing variance gradient estimators. recent advances neural variational inference mitigate problem reparameterising continuous random variables using paper introduced deep neural variational inference framework generative models text. experimented diverse tasks document modelling question answer selection tasks demonstrate effectiveness framework cases models achieve state performance. apart promising results model also advantages simple expressive efﬁcient training sgvb algorithm; suitable unsupervised supervised learning tasks; capable generalising incorporate type neural network. bowman samuel vilnis luke vinyals oriol andrew j´ozefowicz rafal bengio samy. generating sentences continuous space. corr abs/. http//arxiv.org/ abs/.. hermann karl moritz kocisk´y tom´as grefenstette edward espeholt lasse will suleyman mustafa blunsom phil. teaching machines read comprehend. proceedings nips kumar ankit irsoy ozan jonathan bradbury james english robert pierce brian ondruska peter gulrajani ishaan socher richard. anything dynamic memory networks natural language processing. arxiv preprint arxiv. figure t-sne visualisation document representations achieved nvdm held-out test dataset newsgroups. documents collected different news groups correspond points different colour ﬁgure. computational complexity nvdm training document here represents cost inference network generate sample number layers inference network average dimension layers. besides cost reconstructing document sample average length documents represents volume words applied document model conventionally much lager computational complexity nasm training question-answer pair inference network needs takes produce joint representation question-answer pair label total number parameters lstm average length sentences. based joint representation spends generate sample number layers represents average dimension. generative model requires similarly costs construct generative latent distribution saved lstms shared inference network generative model. besides attention model takes relatedness prediction takes last since computations nvdm nasm parallelised sample required training process efﬁcient carry neural variational inference. nvdm instantiation variational auto-encoder computational complexity deterministic auto-encoder. addition computational complexity lstm+att deterministic counterpart nasm also time increase introducing inference network nasm compared lstm+att.", "year": 2015}