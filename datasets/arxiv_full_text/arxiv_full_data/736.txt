{"title": "A Growing Long-term Episodic & Semantic Memory", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.", "text": "long-term memory connectionist systems lies entirely weights system. since number weights typically ﬁxed bounds total amount knowledge learned stored. though normally problem neural network designed speciﬁc task bound undesirable system continually learns open range domains. address this describe lifelong learning system leverages fast though non-differentiable content-addressable memory exploited encode long history sequential episodic knowledge semantic knowledge many episodes unbounded number domains. opens door investigation transfer learning leveraging prior knowledge learned lifetime experiences domains. course several decades experience person typically learns variety disparate domains. person learn domain still retain long-held knowledge previously learned domains. however many neural models experience catastrophic interference trained domains learning overrides corrupts network’s earlier knowledge. machine similarly capable learning variety domains lifetime would potentially allow machine transfer knowledge among domains bring bear large tools facing problem. problem address machine store unbounded amount episodic semantic memory store knowledge previous tasks efﬁciently retrieve relevant information tasks? this must also address subproblems machine automatically segment experiences episodes encode episodes long-term memory relevant semantic knowledge analogous episodes efﬁciently recalled applied experiences. current work makes following contributions introduce system stores episodic semantic memory single memory system. system automatically separates domains without requiring explicit signal telling domain currently experiencing. describe classiﬁer used system retrieve relevant domain information explicitly considering fraction knowledge previous domains. contrast systems either require explicit domain indicators linearly consider known domains turn. describe system used automatically increase memory capacity overcome catastrophic interference. take liberal deﬁnition episodic memory includes memory speciﬁc sequential events. looser deﬁnition used tulving others require episodic memory autobiographical example. deﬁne semantic memory loosely abstractions summaries induced multiple sequences. reasonable measure human brains well several trillion parameters. artiﬁcial neural network going reach human-level intelligence seems likely also need similar order magnitude parameters. assuming typical bounds parameter machine represent huge amount knowledge world need large number parameters. achieve this call ﬁxed-brain design machine begin existence nearly parameters ever have. perfectly reasonable approach evidence total number neurons humans actually decreases even accounting neurogenesis however ﬁxed-brain design places upper bound machine’s total domain knowledge requires machine’s designers prior knowledge upper bound complexity machine’s lifetime experience instead investigating alternative ﬁxed-brain design call growing-brain design machine ability indeﬁnitely allocate parameters needed extendable memory. heart approach assume unlimited associative memory system read write real-valued vectors ﬁxed width assume operations takes time logarithmic number items memory. write operation takes value pair value vectors simply stores memory. read operation takes vector returns small vectors likely whose keys closest given key. absence information memory assumes nearby keys nearby values. unlike normal hash table retrieval need exactly match used originally store value. note vector serve results content addressable memory. read memory using noisy incomplete version vector retrieve completed denoised version. various models proposed type memory sparse distributed memory clean-up memory approximate nearest neighbor search methods methods differentiable sacriﬁce assumption differentiability vector memory gives ﬂexibility systems use. make following assumptions approach memory capacity single vector memory ﬁxed bound less amount information eventually want encode world. following system unsupervised goal compress experiences uninterrupted stream ﬁxed width vectors. includes episodic knowledge semantic knowledge nearly knowledge learned world stored associative memory. includes individual episodes semantic knowledge. allow ﬁxed number learned parameters meta-level controller outside vector memory. operations vector memory insertion deletion retrieval assumed differentiable. motivated early infant development assume unsupervised setup machine experiences continuous stream data external supervision reward signals actions affect environment. machine receives continuous stream ﬁxed-width vectors. experiments sequence -bit memory states atari games concatenated -bit one-hot encoding previous action. implementation available openai example data shown figure supplementary material. since system control it’s merely watching another player play games. although stream comes multiple runs different atari games machine given special signal marking beginning episode given explicit information game played time. though machine’s goal merely remember compress sequences healthy adult cortex estimated roughly billion neurons trillion synapses estimates number bits captured synaptic connections vary widely. recent estimate gives bits synapse yielding roughly trillion bits. bits ﬂoating point parameter gives roughly trillion ﬂoating point parameters cortex. contrast currently rare artiﬁcial neural networks billion ﬂoating point parameters typical networks much fewer. example recent resnet architecture cifar- million parameters hypothesize that machine develop model games useful later time given reward signal. setup want system capable learning games indeﬁnitely without forgetting earlier games. would also like system leverage knowledge earlier games learn faster games. assume vector memory works ﬂoating-point vectors ﬁxed size discuss memory used store virtually unlimited amount sequential trace data. inspired complementary learning systems assume large though ﬁxed memory buffer rotely store long sequence vectors. initial approach compressing data buffer train lstm sequence sequence auto-encoder encode subsequences longer sequence commit -element-wide thought vectors subsequences vector-memory. several problems approach. amount semantic knowledge bounded weights lstm auto-encoder. means cannot expect lstm keep learning dynamics atari games indeﬁnitely. around this number free parameters lstm models needs increased. possibility simply increasing number hidden states lstm expansion would require address train grown lstm without causing catastrophic interference. next approach train multiple lstm auto-encoders auto-encoder attempted compress input reported loss based difference original decoded sequences. sequence tied together losses minimum operation effect training model best encoded sequence. experiments caused models specialize trained three models data three different atari games model specialized encoding particular game. issue using multiple lstm auto-encoders model’s semantic knowledge stored outside vector memory since model parameters single vector vector memory address this reduced dimensionality auto-encoders manner reminiscent hypernetworks learnets trained -element embedding vectors lstm using feedforward stretcher network shown figure supplementary material stretches vector size size using layers nodes. ﬁnal weights stretcher network reshaped weights -hidden-unit lstm auto-encoder. layers stretcher network fully connected except last layer sparsely connected possible connections chosen randomly thus parameter speciﬁcation lstm auto-encoder differentiable function -element embedding weights stretcher network thus backpropagation adjusts embedding weights stretcher network instead directly changing auto-encoder’s parameters. ﬁnal embedding lstm auto-encoder program vector analogy embedding interpreted program called different thought-vectors arguments produce speciﬁc sequences. course knowledge stored weights stretcher network ﬁxed number parameters. hope stretcher network becomes somewhat generic encoding general knowledge training wide variety games. alternatively imagine using meta-stretcher network allows embed many different stretcher networks could another level generality. point ﬁxed controller. hypothesize practically useful lstm auto-encoders tiny fraction possible stretcher network learn generate sensible lstms -element vectors chosen gaussian distribution. trained stretcher network training sequences various atari games varying number program vectors system allowed use. strongest result number program vectors equal number atari games system tends program vector domain. extent automatically segments domains without given explicit information atari game traces coming from. many expandable architectures proposed recently several decades ago. nonparametric methods case-based reasoning k-means others survey) ability grow capacity linearly data. methods operate static vector data must adapted operate sequential data. furthermore semantic knowledge usually stored implicitly unlike proposed system stores instances embeddings semantic knowledge. methods proposed address catastrophic interference. example complementary learning systems learning without forgetting interleave training remembered earlier data data. draw inspiration systems work progressive networks freezes weights networks trained earlier domains. unlike others progressive networks allow network expand capacity. unlike system progressive networks attempt store semantic knowledge earlier systems content addressable memory problem network grows quadratically number domains. hypothesize storing semantic knowledge content addressable memory help address allowing fast lookup relevant program vectors potentially yielding linear storage logarithmic program lookup. several methods proposed expanding capacity neural networks. part work initially inspired cascade correlation algorithm early example incrementally learns features adds feed-forward network freezing weights previously learned features. models since built ideas growing neural netnet adanet work attempts build ideas providing means storing sequential instances addition semantic information. recent advances differentiable memory neural turing machines memory networks differentiable neural computers memory-based deep reinforcement learning provide system essentially working memory accessed course single episode. unlike system memory cleared episodes long-term memory systems retain network weights. episodic memory also component many cognitive architectures soar lida clarion work originally inﬂuenced speciﬁcally soar extends using recent developments sequence sequence models encode sequences static vectors. episodic memory shown useful reinforcement learning tasks system provides mechanism episodes stored retrieved. primary contribution work system encoding unbounded amount episodic semantic knowledge expandable content-addressable vector memory. work still infancy many unresolved issues answer question machine store lifetime knowledge usefully retrieved transferred situations. share current approaches addressing problems supplementary material.", "year": 2016}