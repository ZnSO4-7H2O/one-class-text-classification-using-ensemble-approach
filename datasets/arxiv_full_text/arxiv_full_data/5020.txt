{"title": "Rainbow: Combining Improvements in Deep Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.", "text": "figure median human-normalized performance across atari games. compare integrated agent published baselines. note match dqn’s best performance frames surpass baseline within frames reach substantially improved ﬁnal performance. curves smoothed moving average points. radically different issues since build shared framework could plausibly combined. cases done prioritized ddqn dueling ddqn double q-learning dueling ddqn also combined prioritized experience replay. paper propose study agent combines aforementioned ingredients. show different ideas integrated indeed largely complementary. fact combination results stateof-the-art results benchmark suite atari games arcade learning environment terms data efﬁciency ﬁnal performance. finally show results ablation studies help understand contributions different components. deep reinforcement learning community made several independent improvements algorithm. however unclear extensions complementary fruitfully combined. paper examines extensions algorithm empirically studies combination. experiments show combination provides state-of-the-art performance atari benchmark terms data efﬁciency ﬁnal performance. also provide results detailed ablation study shows contribution component overall performance. many recent successes scaling reinforcement learning complex sequential decision-making problems kick-started deep q-networks algorithm combination q-learning convolutional neural networks experience replay enabled learn pixels play many atari games human-level performance. since then many extensions proposed enhance speed stability. double addresses overestimation bias q-learning decoupling selection evaluation bootstrap action. prioritized experience replay improves data efﬁciency replaying often transitions learn. dueling network architecture helps generalize across actions separately representing state values action advantages. learning multi-step bootstrap targets used shifts bias-variance tradehelps propagate newly observed rewards faster earlier visited states. distributional q-learning learns categorical distribution discounted returns instead estimating mean. noisy uses stochastic network layers exploration. list course exhaustive. algorithms enables substantial performance improvements isolation. since addressing copyright association advancement artiﬁcial intelligence rights reserved. reinforcement learning addresses problem agent learning environment order maximize scalar reward signal. direct supervision provided agent instance never directly told best action. agents environments. discrete time step environment provides agent observation agent responds selecting action environment provides next reward discount state st+. interaction formalized markov decision process tuple ﬁnite states ﬁnite actions transition function reward function discount factor. experiments mdps episodic constant except episode termination algorithms expressed general form. agent side action selection given policy deﬁnes probability distribution actions state. state encountered time deﬁne rt+k+ discounted future rewards collected agent discount reward steps future given product discounts time γt+i. agent aims maximize expected discounted return ﬁnding good policy. policy learned directly constructed function learned quantities. value-based reinforcement learning agent learns estimate expected discounted return value following policy starting given state state-action pair common deriving policy state-action value function \u0001-greedily respect action values. corresponds taking action highest value probability otherwise uniformly random probability policies kind used introduce form exploration randomly selecting actions sub-optimal according current estimates agent discover correct estimates appropriate. main limitation difﬁcult discover alternative courses action extend future; motivated research directed forms exploration. deep reinforcement learning dqn. large state and/or action spaces make intractable learn value estimates state action pair independently. deep reinforcement learning represent various components agents policies values deep neural networks. parameters networks trained gradient descent minimize suitable loss function. given state step based current state agent selects action \u0001-greedily respect action values adds transition replay memory buffer holds last million transitions. parameters neural network optimized using stochastic gradient descent minimize loss time step randomly picked replay memory. gradient loss back-propagated parameters online network term represents parameters target network; periodic copy online network directly optimized. optimization performed using rmsprop variant stochastic gradient descent mini-batches sampled uniformly experience replay. means loss above time index random time index last million transitions rather current time. experience replay target networks enables relatively stable learning values superhuman performance several atari games. important milestone several limitations algorithm known many extensions proposed. propose selection extensions addressed limitation improved overall performance. keep size selection manageable picked extensions address distinct concerns double q-learning. conventional q-learning affected overestimation bias maximization step equation harm learning. double q-learning addresses overestimation decoupling maximization performed bootstrap target selection action evaluation. possible effectively combine using loss prioritized replay. samples uniformly replay buffer. ideally want sample frequently transitions much learn. proxy learning potential prioritized experience replay samples transitions probability relative last encountered absolute error buffer maximum priority providing bias towards recent transitions. note stochastic transitions might also favoured even little left learn them. dueling networks. dueling network neural network architecture designed value based features streams computation value advantage streams sharing convolutional encoder merged special aggregator corresponds following factorization action values multi-step learning. q-learning accumulates single reward uses greedy action next step bootstrap. alternatively forward-view multi-step targets used deﬁne truncated n-step return given state distributional learn approximate distribution returns instead expected return. recently bellemare dabney munos proposed model distributions probability masses placed discrete support vector natoms atoms deﬁned vmin vmax−vmin natoms− natoms}. approximating distribution time deﬁned support probability mass atom goal update distribution closely matches actual distribution returns. learn probability masses insight return distributions satisfy variant bellman’s equation. given state action distribution returns optimal policy match target distribution deﬁned taking distribution contracting next state action towards zero according discount shifting reward distributional variant q-learning derived ﬁrst constructing support target distribution minimizing kullbeck-leibler divergence distribution target distribution non-distributional case frozen copy parameters construct target distribution. parametrized distribution represented neural network natoms× nactions outputs. softmax applied independently action dimension output ensure distribution action appropriately normalized. noisy nets. limitations exploring using \u0001-greedy policies clear games montezuma’s revenge many actions must executed collect ﬁrst reward. noisy nets propose noisy linear layer combines deterministic noisy stream random variables denotes element-wise product. transformation used place standard linear time network learn ignore noisy stream different rates different parts state space allowing state-conditional exploration form self-annealing. paper integrate aforementioned components single integrated agent call rainbow. first replace -step distributional loss multi-step variant. construct target distribution contracting value distribution st+n according cumulative discount shifting truncated n-step discounted return. corresponds deﬁning target distribution t+n)). resulting loss combine multi-step distributional loss double q-learning using greedy action st+n selected according online network bootstrap action evaluating action using target network. standard proportional prioritized replay absolute error used prioritize transitions. computed distributional setting using mean action values. however experiments distributional rainbow variants prioritize transitions loss since algorithm minimizing shared representation value stream natoms outputs advantage stream natoms nactions outputs denote output corresponding atom action atom value advantage streams aggregated dueling passed softmax layer obtain normalised parametric distributions used estimate returns’ distributions replace linear layers noisy equivalent described equation within noisy linear layers factorised gaussian noise reduce number independent noise variables. evaluation methodology. evaluated agents atari games arcade learning environment follow training evaluation procedures mnih hasselt average scores agent evaluated training every steps environment suspending learning evaluating latest agent frames. episodes truncated frames hasselt agents’ scores normalized game corresponds random agent average score human expert. normalized scores aggregated across atari levels compare performance different agents. common track median human normalized performance across games. also consider number games agent’s performance fraction human performance disentangle improvements median come from. mean human normalized performance potentially less informative dominated games agents achieve scores orders magnitude higher humans besides tracking median performance function environment steps training re-evaluate best agent snapshot using different testing regimes. no-ops starts regime insert random number no-op actions beginning episode human starts regime episodes initialized points randomly sampled initial portion human expert trajectories difference regimes indicates extent agent over-ﬁt trajectories. space constraints focus aggregate results across games. however appendix provide full learning curves games agents well detailed comparison tables normalized scores no-op human starts testing regimes. hyper-parameter tuning. rainbow’s components number hyper-parameters. combinatorial space hyper-parameters large exhaustive search therefore performed limited tuning. component started values used paper introduced component tuned sensitive among hyper-parameters manual coordinate descent. variants perform learning updates during ﬁrst frames ensure sufﬁciently uncorrelated updates. found that prioritized replay possible start learning sooner frames. starts exploration corresponding acting uniformly random; anneals amount exploration ﬁrst frames ﬁnal value whenever using noisy nets acted fully greedily value hyper-parameter used initialize weights noisy stream. agents without noisy nets used \u0001-greedy decreased exploration rate faster previously used annealing ﬁrst frames. used adam optimizer found less sensitive choice learning rate rmsprop. uses learning rate rainbow’s variants used learning rate selected among value adam’s hyper-parameter. replay prioritization used recommended proportional variant priority exponent linearly increased importance sampling exponent course training. priority exponent tuned comparing values using loss distributional priority observed performance robust choice parameter history start learning adam learning rate exploration noisy nets target network period adam prioritization type prioritization exponent prioritization importance sampling multi-step returns distributional atoms distributional min/max values figure plot shows several agents number games achieved least given fraction human performance function time. left right consider thresholds. ﬁrst compare rainbow baselines. second compare rainbow ablations. section analyse main experimental results. first show rainbow compares favorably several published agents. perform ablation studies comparing several variants agent corresponding removing single component rainbow. comparison published baselines. figure compare rainbow’s performance corresponding curves ddqn prioritized ddqn dueling ddqn distributional noisy dqn. thank authors dueling prioritized agents providing learning curves these report re-runs ddqn distributional noisy dqn. performance rainbow signiﬁcantly better baselines data efﬁciency well ﬁnal performance. note match ﬁnal performance frames surpass best ﬁnal performance baselines frames reach substantially improved ﬁnal performance. ﬁnal evaluations agent training rainbow achieves median score no-ops regime; human starts regime measured median score table compare scores published median scores individual baselines. figure plot number games agent reached speciﬁed level human normalized performance. left right subplots show many games different agents achieved human normalized performance. allows identify overall improvements performance come from. note performance rainbow agents apparent levels performance rainbow agent improving scores games baseline agents already good well improving games baseline agents still human performance. learning speed. original setup agent single gpu. frames required match dqn’s ﬁnal performance correspond less hours wall-clock time. full frames corresponds approximately days varies less discussed variants. literatable median normalized scores best agent snapshots rainbow baselines. methods marked asterisk scores come corresponding publication. dqn’s scores comes dueling networks paper since dqn’s paper report scores games. others scores come implementations. distributional q-learning ranked immediately previous techniques relevance agent’s performance. notably early learning difference apparent shown figure ﬁrst million frames distributional-ablation performed well full agent. however without distributions performance agent started lagging behind. results separated relatively human performance figure distributional-ablation primarily seems lags games human level near terms median performance agent performed better noisy nets included; removed exploration delegated traditional \u0001greedy mechanism performance worse aggregate removal noisy nets produced large drop performance several games also provided small increases games aggregate observe signiﬁcant difference removing dueling network full rainbow. median score however hides fact impact dueling differed games shown figure figure shows dueling perhaps provided improvement games above-human performance levels degradation games sub-human performance also case double q-learning observed difference median performance limited component sometimes harming helping depending game investigate role double qlearning compared predictions trained agents actual discounted returns computed clipped rewards. comparing rainbow agent double qlearning ablated observed actual returns often higher therefore fall outside support distribution spanning leads underestimated returns rather overestimations. hypothesize clipping values constrained range counteracts overestimation bias q-learning. note however importance double q-learning increase support distributions expanded. demonstrated several improvements successfully integrated single learning algorithm achieves state-of-the-art performance. moreover shown within integrated algorithm components provided clear performance beneﬁts. many algorithmic components able include would promising candidates experiments integrated agents. among many possible candidates discuss several below. figure median human-normalized performance across atari games function time. compare integrated agent different ablations curves smoothed moving average points. ture contains many alternative training setups improve performance function wall-clock time exploiting parallelism e.g. nair salimans mnih properly relating performance across different hardware/compute resources non-trivial focused exclusively algorithmic variations allowing apples-to-apples comparisons. consider important complementary leave questions scalability parallelism future work. ablation studies. since rainbow integrates several different ideas single agent conducted additional experiments understand contribution various components context speciﬁc combination. gain better understanding contribution component rainbow agent performed ablation studies. ablation removed component full rainbow combination. figure shows comparison median normalized score full rainbow ablated variants. figure shows detailed breakdown ablations perform relative different thresholds human normalized performance figure shows gain loss ablation every game averaged full learning run. prioritized replay multi-step learning crucial components rainbow removing either component caused large drop median performance. unsurprisingly removal either hurt early performance. perhaps surprisingly removal multistep learning also hurt ﬁnal performance. zooming individual games components helped figure performance drops ablation agents atari games. performance area learning curve normalized relative rainbow agent dqn. games outperforms rainbow omitted. ablation leading strongest drop highlighted game. removal either prioritization multi-step learning reduces performance across games contribution component varies substantially game. number algorithms exploit sequence data achieve improved learning efﬁciency. optimality tightening uses multi-step returns construct additional inequality bounds instead using replace -step targets used q-learning. eligibility traces allow soft combination n-step returns however sequential methods leverage computation gradient multi-step targets used rainbow. furthermore introducing prioritized sequence replay raises questions store replay prioritise sequences. episodic control also focuses data efﬁciency shown effective domains. improves early learning using episodic memory complementary learning system capable immediately re-enacting successful action sequences. besides noisy nets numerous exploration methods could also useful algorithmic ingredients among bootstrapped intrinsic motivation count-based exploration integration alternative components fruitful subject research. dates without exploring alternative computational architectures. asynchronous learning parallel copies environment gorila evolution strategies effective speeding learning least terms wallclock time. note however less data efﬁcient. state representation could also made efﬁcient exploiting auxiliary tasks pixel control feature control supervised predictions successor features evaluate rainbow fairly baselines followed common domain modiﬁcations clipping rewards ﬁxed action-repetition frame-stacking might removed learning algorithm improvements. pop-art normalization allows reward clipping removed preserving similar level performance. fine-grained action repetition enabled learn repeat actions. recurrent state network learn temporal state representation replacing ﬁxed stack observation frames. general believe exposing real game agent promising direction future research.", "year": 2017}