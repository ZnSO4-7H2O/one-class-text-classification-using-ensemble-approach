{"title": "Specific-to-General Learning for Temporal Events with Application to  Learning Event Definitions from Video", "tag": ["cs.AI", "cs.LG"], "abstract": "We develop, analyze, and evaluate a novel, supervised, specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video sequences. First, we introduce a simple, propositional, temporal, event-description language called AMA that is sufficiently expressive to represent many events yet sufficiently restrictive to support learning. We then give algorithms, along with lower and upper complexity bounds, for the subsumption and generalization problems for AMA formulas. We present a positive-examples--only specific-to-general learning method based on these algorithms. We also present a polynomial-time--computable ``syntactic'' subsumption test that implies semantic subsumption without being equivalent to it. A generalization algorithm based on syntactic subsumption can be used in place of semantic generalization to improve the asymptotic complexity of the resulting learning algorithm. Finally, we apply this algorithm to the task of learning relational event definitions from video and show that it yields definitions that are competitive with hand-coded ones.", "text": "develop analyze evaluate novel supervised speciﬁc-to-general learner simple temporal logic resulting algorithm learn visual event deﬁnitions video sequences. first introduce simple propositional temporal event-description language called sufﬁciently expressive represent many events sufﬁciently restrictive support learning. give algorithms along lower upper complexity bounds subsumption generalization problems formulas. present positive-examples–only speciﬁc-to-general learning method based algorithms. also present polynomialtime–computable syntactic subsumption test implies semantic subsumption without equivalent generalization algorithm based syntactic subsumption used place semantic generalization improve asymptotic complexity resulting learning algorithm. finally apply algorithm task learning relational event deﬁnitions video show yields deﬁnitions competitive hand-coded ones. humans conceptualize world terms objects events. reﬂected fact talk world using nouns verbs. perceive events taking place objects interact world performing events objects reason effects actual hypothetical events performed others objects. also learn object event types novel experience. paper present evaluate novel implemented techniques allow computer learn event types examples. show results application techniques learning event types automatically constructed relational force-dynamic descriptions video sequences. wish acquired knowledge event types support multiple modalities. humans observe someone faxing letter ﬁrst time quickly able recognize future occurrences faxing perform faxing reason faxing. thus appears likely humans learn event representations sufﬁciently general support fast efﬁcient multiple modalities. long-term goal research allow similar cross-modal learning event representations. intend learned representations used vision planning robotics crucial requirement event representations capture invariants event type. humans classify picking table picking dumbbell ﬂoor picking suggests human event representations relational. abstract relational notion picking parameterized participant objects rather distinct propositional notions instantiated speciﬁc objects. humans also classify event picking matter whether hand moving slowly quickly horizontally vertically leftward rightward along straight path circuitous one. appears characteristics participant-object motion distinguish picking event types. rather fact object picked changes supported resting initial location supported grasped agent. suggests primitive relations used build event representations force dynamic another desirable property event representations perspicuous. humans introspect describe deﬁning characteristics event types. introspection allows create dictionaries. support introspection prefer representation language allows characteristics explicitly manifest event deﬁnitions emergent consequences distributed parameters neural networks hidden markov models. develop supervised learner event representation possessing desired characteristics follows. first present simple propositional temporal logic called sublanguage variety familiar temporal languages logic expressive enough describe variety interesting temporal events restrictive enough support effective learner demonstrate below. proceed develop speciﬁc-to-general learner logic giving algorithms complexity bounds subsumption generalization problems involving formulas. show semantic subsumption intractable provide weaker syntactic notion subsumption implies semantic subsumption checked polynomial time. implemented learner based upon syntactic subsumption. next show means adapt learner learn relational concepts. evaluate resulting relational learner complete system learning force-dynamic event deﬁnitions positive-only training examples given real video sequences. ﬁrst system perform visual-event recognition video. review prior work compare current work later paper. fact prior systems built authors. howard learns classify events video using temporal relational representations. representations force dynamic. leonard classiﬁes events video using temporal relational force-dynamic representations learn representations. uses library hand-code representations. work adds learning component leonard essentially duplicating performance hand-coded deﬁnitions automatically. demonstrated utility learner visual-event–learning domain note many domains interesting concepts take form structured temporal sequences events. machine planning macro-actions represent useful temporal patterns action. computer security typical application behavior represented perhaps temporal patterns system calls must differentiated compromised application behavior follows section introduces application domain recognizing visual events provides informal description system learning event deﬁnitions video. section introduces language syntax semantics several concepts needed analysis language. section develops analyzes algorithms subsumption generalization problems language introduces practical notion syntactic subsumption. section extends basic propositional learner handle relational data negation control exponential run-time growth. section presents results visual-event learning. sections compare related work conclude. section provides overview system learning recognize visual events video. provide intuitive picture system providing technical details. formal presentation event-description language algorithms theoretical empirical results appears sections ﬁrst introduce application domain visual-event recognition leonard system event recognizer upon learner built. second describe positive-only learner overall system. third informally introduce event-description language used learner. finally give informal presentation learning algorithm. leonard system recognizing visual events video camera input— example simple visual event hand picking block. research originally motivated problem adding learning component leonard—allowing leonard learn recognize event viewing example events type. below give high-level description leonard system. leonard three-stage pipeline depicted figure input consists video-frame image sequence depicting events. first segmentation-and-tracking component transforms input polygon movie sequence frames frame convex polygons placed around tracked objects video. figure shows partial video sequence pick event overlaid corresponding polygon movie. next model-reconstruction component transforms polygon movie force-dynamic model. model describes changing support contact attachment relations tracked objects time. constructing model somewhat involved process described siskind figure shows visual depiction force-dynamic model corresponding pick event. finally eventrecognition component armed library event deﬁnitions determines events occurred model accordingly video. figure shows text output input event-recognizer pick event. ﬁrst line corresponds output indicates interval pick occurred. remaining lines text encoding event-recognizer input indicating time intervals various force-dynamic relations true video. formula asserts event picking deﬁned sequence states supports contact ﬁrst state supports attachment second state. supports contacts attached primitive force-dynamic relations. formula speciﬁc example general class formulas learning. figure upper boxes represent three primary components leonard’s pipeline. lower depicts event-learning component described paper. input learning component consists training models target events along event labels prior work reported paper deﬁnitions leonard’s event-recognition library hand coded. here learning component leonard learn recognize events. figure shows event learner overall system. input event learner consists force-dynamic models model-reconstruction stage along event labels output consists event deﬁnitions used event recognizer. take supervised-learning approach force-dynamic model-reconstruction process applied training videos target event type. resulting force-dynamic models along labels indicating target event type given learner induces candidate deﬁnition event type. example input learner might consist models corresponding videos hand picking block green block label pickup \u0004hand; red; green\u0005 hand picking green block block label pickup \u0004hand; green; red\u0005—the output would candidate deﬁnition pickup applicable previously unseen pick events. note learning component positive-only sense learning target event type uses positive training examples negative examples positive-only setting interest appears humans able learn many event deﬁnitions given primarily positive examples. practical standpoint positive-only learner removes often difﬁcult task collecting negative examples representative event learned construction learner involves primary design choices. first must choose event representation language serve learner’s hypothesis space second must design algorithm selecting good event deﬁnition hypothesis space given training examples event type. figure leonard recognizes pick event. frames video input automatically generated polygon movie overlaid. frames visual depiction automatically generated force-dynamic properties. text input/output event classiﬁer corresponding depicted movie. line output remaining lines make input encodes changing force-dynamic properties. green represents block table represents block picked restrictive subset event logic called learner’s hypothesis space. subset excludes many practically useless formulas confuse learner still retaining substantial expressiveness thus allowing represent learn many useful event types. restriction formulas form syntactic learning bias. basic formulas called states express constant properties time intervals arbitrary duration. example supports contacts state tells must support contact general state conjunction number primitive propositions using also describe sequences states. example \u0004supports contacts \u0004supports attached sequence states ﬁrst state given second state indicating must support attached formula true whenever ﬁrst state true time interval followed immediately second state true time interval meeting ﬁrst time interval. sequences called timelines since meets ands. general timelines contain number states. finally conjoin timelines formulas example formula deﬁnes event timelines must true simultaneously time interval. using formulas represent events listing various property sequences must occur parallel event unfolds. important note however transitions states different timelines formula occur relation another. example formula transition states ﬁrst timeline occur before after exactly transition states second timeline. important assumption leveraged learner primitive propositions used construct states describe liquid properties purposes property liquid holds time-interval holds subintervals. force-dynamic properties produced leonard liquid—e.g. hand supports block interval clearly hand supports block subintervals. primitive propositions liquid properties described states also liquid. however properties described formulas general liquid. recall examples wish classify learn force-dynamic models thought movies depicting temporal events. also recall learner outputs deﬁnitions hypothesis space. given formula covers example model true model. particular target event type ultimate goal learner output formula covers example model model depicts instance target event type. understand learner useful deﬁne generality relationship formulas. covers every formula example formal analysis different notions generality section ignore distinctions. note however algorithm informally describe later section based syntactic notion generality. learning goal formula consistent positivetraining data result trivial solution returning formula covers examples. rather problem adding negative training examples instead change learning goal ﬁnding least-general formula covers positive examples. learning approach pursued variety different languages within machine-learning literature including clausal ﬁrst-order logic deﬁnite clauses description logic important choose appropriate hypothesis space bias learning approach hypothesis returned simply extremes either disjunction training examples universal hypothesis covers examples. experiments found that enough training data least-general formula often converges usefully. take standard speciﬁc-to-general machine-learning approach ﬁnding least-general formula covers positive examples. approach relies computation functions least-general covering formula example model least-general generalization formulas. lgcf example model least general formula covers example. intuitively lgcf formula captures information model. formulas least-general formula general formula set. intuitively formula formula captures largest amount common information among formulas. viewed differently formula covers examples covered formulas covers examples possible resulting speciﬁc-to-general learning approach proceeds follows. first lgcf function transform positive training model formula. second return resulting formulas. result represents least-general formula covers positive training examples. thus specify learner remains provide algorithms computing lgcf language. informally describe algorithms computing functions formally derived analyzed sections increase readability presentation follows dispense presenting examples primitive properties meaningfully named force-dynamic relations. rather examples utilize abstract propositions current application propositions correspond exclusively force-dynamic properties applications. demonstrate system computes lgcf example model. consider following example model represent time interval arbitrary duration nothing changes fact figure generalizing timelines exponentially many interdigitations timelines. computing interdigitation generalization corresponding interdigitation part states formed intersecting aligned states timelines. state represents state propositions. given interdigitation timelines easy construct timeline must true whenever either timelines true figure give construction interdigitation given figure horizontal lines ﬁgure correspond interdigitation divided every state either timeline identical states whenever transition occurs state timeline. resulting pair timelines simultaneous transitions viewed sequence state pairs timeline. bottom horizontal line labeled timeline state state pair state intersection proposition sets state pair. here represents empty propositions state true anywhere. holds along clear true whenever either time-interval model sequence consecutive subintervals sequence states along interval view states sets states subsets corresponding aligned state thus states true model alignment showing true model. general exponentially many input timelines possible interdigitation two. clearly since generalization input timelines conjunction igs. conjunction formula generalizes input timelines. fact show later paper formula serves timelines. show conjunction lgg. formula contains redundant timelines pruned. first clear different result timelines remove copy timeline lgg. second note timeline general timeline equivalent \b—thus prune away timelines generalizations others. later paper show efﬁciently test whether timeline general another. performing pruning steps left ﬁrst next last timelines formula—thus i.e. formula informally described lgcf operations needed carry speciﬁc-to-general learning approach described above. follows formally develop operations analyze theoretical properties corresponding problems discuss needed extensions bring operations practice. present formal account hypothesis space analytical development algorithms needed speciﬁc-to-general learning ama. readers primarily interested high-level view algorithms empirical evaluation wish skip sections instead proceed directly sections discuss several practical extensions basic learner present empirical evaluation. study subset interval-based logic called event logic utilized leonard event recognition video sequences. logic interval-based explicitly representing possible interval relationships given originally allen calculus interval relations event-logic formulas allow deﬁnition event types specify static properties intervals directly dynamic properties hierarchically relating sub-intervals using allen relations. paper formal syntax semantics full event logic needed proposition given appendix restrict attention much simpler subset event logic call deﬁned below. believe choice event logic rather ﬁrst-order logic well restriction fragment event logic provide useful learning bias ruling large number practically useless concepts maintaining substantial expressive power. practical utility bias demonstrated empirical results visual-event–recognition application. also seen restriction conjunction until similar motivations. present syntax semantics along technical properties used throughout paper. natural describe temporal events specifying sequence properties must hold consecutive time intervals. example hand picking block might become block supported hand block supported hand. represent sequences timelines sequences conjunctive state restrictions. intuitively timeline given sequence propositional conjunctions separated semicolons taken represent events temporally match sequence consecutive conjunctions. formula conjunction number timelines representing events simultaneously viewed satisfying conjoined timelines. formally syntax formulas given prop primitive proposition take grammar formally deﬁne terms timeline formula formula state. kformula formula states k-ama formula formula whose timelines k-ma timelines. often treat states proposition sets empty formulas ma-timeline sets. also treat formulas sets states—it important note however formulas contain duplicate states duplication signiﬁcant. reason treating timelines sets formally intend sets state-index pairs indicate explicitly avoid encumbering notation implicit index must remembered whenever handling duplicate states. semantics formulas deﬁned terms temporal models. temporal model prop propositions pair mapping natural numbers truth assignments prop closed natural-number interval note siskind gives continuous-time semantics event logic models deﬁned terms real-valued time intervals. temporal models deﬁned discrete natural-number time-indices. however results still apply continuous-time semantics. important note natural numbers domain representing time discretely prescribed unit continuous time represented natural number. instead number represents arbitrarily long period continuous time nothing changed. similarly states timelines represent arbitrarily long periods time conjunctive restriction given state holds. satisﬁability relation formulas given follows condition deﬁning satisfaction timelines appear unintuitive ﬁrst fact ways satisﬁed. reason becomes clear recalling using natural numbers represent continuous time intervals. intuitively continuous-time perspective timeline satisﬁed consecutive continuous-time intervals satisfying sequence consecutive states timeline. transition occur either within interval constant truth assignment exactly boundary time intervals constant truth value. deﬁnition cases correspond satisﬁed time intervals intuitively projection gives sequence propositional truth assignments beginning model. later show projection model viewed representing model precise sense. example equivalent timelines. general timelines property duplicating state results formula equivalent original formula. recall that given model view truth assignment given propositions timeline subsumed formulas intuitively clear semantics viewed continuous-time perspective. interval true broken arbitrary number subintervals hold. example illustrates inﬁnite descending chains formulas entire chain subsumes given formula general formula involving propositions subsume timelines natural capture stretchable sequences state constraints. consider conjunction sequences i.e. ama? several reasons language enrichment. first show least-general generalization unique—this true second informally argue parallel conjunctive constraints important learning efﬁciency. particular space formulas length grows size exponentially making difﬁcult induce long formulas. however ﬁnding several shorter timelines characterize part long sequence changes exponentially easier. conjunction timelines places shorter constraints simultaneously often captures great deal concept structure. reason analyze well empirical work consider k-ama. language propositional. intended applications relational ﬁrst-order including visual-event recognition. later paper show propositional learning algorithms develop effectively applied relational domains. approach ﬁrst-order learning distinctive automatically constructing object correspondence across examples similarly though allow negative state constraints section discuss extend results incorporate negation learning algorithms crucial visual-event recognition. note formulas translated various ways ﬁrst-order clauses. straightforward however existing clausal generalization techniques learning. particular capture semantics clauses appears necessary deﬁne subsumption generalization relative background theory restricts continuous-time ﬁrst-order– model space. example consider formulas propositions—from example know translation formulas giving span variables represent time intervals meets indicates time intervals meet other span function returns time interval equal union time-interval arguments. meaning intend capture satisfying assignments satisﬁed respectively. since easy clear that contrary want thus translation unintended ﬁrst-order models satisfy similar translations capture continuous-time nature semantics. order capture semantics clausal setting might deﬁne ﬁrst-order theory restricts continuous-time models—for example allowing derivation property holds interval property also holds sub-intervals. given theory desired. however well known least-general generalizations relative background theories need exist prior work clausal generalization simply subsume results language. note particular training possible compile continuous-time background theory ﬁnite adequate ground facts. relative ground theories clausal lggs known always exist thus could used application. however compiling approaches look promising require exploiting analysis similar given paper—i.e. understanding generalization subsumption problem separately clausal generalization exploiting understanding compiling background theory. pursued compilations further. even given compilation procedure problems using existing clausal generalization techniques learning formulas. clausal translations found resulting generalizations typically fall outside language language bias lost. preliminary empirical work video-event recognition domain using clausal inductive-logic-programming systems found learner appeared lack necessary language bias effective event deﬁnitions. believe would possible ways build language bias systems chose instead deﬁne learn within desired language bias directly deﬁning class formulas studying generalization operation class. following convention naming results propositions theorems results work theorems results technical difﬁculty lemmas technical results needed later proofs propositions theorems. number results sequence regardless type. proofs theorems propositions provided main text—omitted proofs lemmas provided appendix. give pseudo-code methods non-deterministic style. non-deterministic language functions return value non-deterministically either contain non-deterministic choice points call non-deterministic functions. since nondeterministic function return possible value depending choices made choice points encountered specifying function natural specify richly structured relation actually enumerate values simply standard backtracking search different possible computations corresponding different choices choice points. basic formulas deal states propositional setting computing subsumption generalization state level straightforward. state subsumes viewing states sets propositions. this derive intersection states least-general subsumer states union states likewise general subsumee. given timelines need consider different ways model could simultaneously satisfy timelines set. start model initial state timeline must satisﬁed. time point model timelines transition second state timelines must satisﬁed place initial state initial state timelines remains satisﬁed. sequence transitions subsets timelines ﬁnal state timeline holds. choosing transition sequence constitutes different interdigitation timelines. viewed differently model simultaneously satisfying timelines induces co-occurrence relation tuples timeline states timeline identifying tuples co-occur point model. represent concept formally tuples co-occurring states i.e. co-occurrence relation. sometimes think tuples ordered sequence transitions. intuitively tuples interdigitation represent maximal time intervals timeline transition tuples giving co-occurring states time interval. deﬁnition interdigitation occurrence relation simultaneously consistent state orderings sequence tuples meaning sequence lexicographically ordered note exponentially many interdigitations even timelines example page shows interdigitation timelines. pseudo-code non-deterministically generating arbitrary interdigitation timelines found figure given interdigitation timelines veriﬁable figure pseudo-code an-interdigitation non-deterministically computes interdigg timelines. function head returns ﬁrst itation state timeline rest returns ﬁrst state removed. extend-tuple extends tuple adding ﬁrst element form longer tuple. a-non-emptysubset-of non-deterministically returns arbitrary non-empty subset proof show backward direction induction number states timeline existence witnessing interdigitation subset single state suppose induction backward direction theorem holds whenever fewer states. given arbitrary model state witnesses write logic discriminate models contains formula satisﬁes other. turns formulas discriminate models exactly much richer internalpositiveeventlogic formulas internal formulas deﬁne event occurrence terms properties within deﬁning interval. satisfaction depends proposition truth values given inside interval positive formulas contain negation. appendix gives full syntax semantics ipel fact discriminate models well ipel indicates restriction formulas retains substantial expressive power leads following result serves least-general covering formula component speciﬁc-to-general learning procedure. formally lgcf model within formula language formula covers covering formula strictly less general. intuitively lgcf model unique most representative formula model. analysis uses concept model embedding. model embeds model proposition tells that ipel lgcf model exists unique timeline. given property formula covers timelines covered another formula thus remainder paper considering subsumption formulas abstract away temporal models deal instead timelines. proposition also tells compute lgcf model constructing projection model. based deﬁnition projection straightforward derive lgcf algorithm runs time polynomial size model. note projection contain repeated states. practice remove repeated states since change meaning resulting formula interdigitations useful analyzing conjunctions disjunctions timelines. conjoining timelines model conjunction induces interdigitation timelines co-occurring states simultaneously hold model point constructing interdigitation taking union tuple co-occurring states sequence states timeline forces conjunction timelines hold. call sequence interdigitationspecialization timelines. dually interdigitation generalization involving intersections states gives timeline holds whenever disjunction timelines holds. deﬁnition interdigitation generalization timelines timeline intersection components j’th tuple sequence interdigitation generalizations called ig\u0004\u0006\u0005 timeline ig\u0004\u0006\u0005 subsumes timeline \u0006—this easily veriﬁed using proposition complexity analyses note number states member ig\u0004\u0006\u0005 is\u0004\u0006\u0005 bounded number states timelines bounded total number states timelines number interdigitations thus members ig\u0004\u0006\u0005 is\u0004\u0006\u0005 exponential total number states. algorithms present later computing lggs require computation ig\u0004\u0006\u0005 is\u0004\u0006\u0005. give pseudo-code compute quantities. figure gives pseudo-code function an-ig-member non-deterministically computes arbitrary member ig\u0004\u0006\u0005 given timelines compute ig\u0004\u0006\u0005 executing possible deterministic computation paths function call an-ig-member i.e. computing results obtainable non-deterministic function possible decisions non-deterministic choice points. give useful lemma proposition concerning relationships conjunctions disjunctions concepts convenience here disjunction concepts producing formulas outside obvious interpretation. figure pseudo-code an-ig-member non-deterministically computes member ig\u0004t timelines. function intersect-tuple takes tuple sets argument returns intersection. higher-order function takes function tuple arguments returns tuple length obtained applying element making tuple results. section study subsumption generalization formulas. first give polynomial-time algorithm deciding subsumption formulas show deciding subsumption formulas conp-complete. second give algorithms complexity bounds construction least-general generalization formulas based analysis subsumption including existence uniqueness lower/upper bounds algorithm formulas. third introduce polynomial-time–computable syntactic notion subsumption algorithm computes corresponding syntactic exponentially faster semantic algorithm. fourth section give detailed example showing steps performed algorithms compute semantic syntactic lggs formulas. methods rely critically novel algorithm deciding subsumption question polynomial-time. note merely searching possible formulas witnessing interdigitation provides obvious decision procedure interdigitations subsumption question—however general exponentially many interdigitations. reduce subsumption problem ﬁnding path graph pairs states polynomial-time operation. pseudo-code resulting subsumption algo\b rithm shown figure main data structure used subsumption algorithm subsumption graph. desired path exists \u0007\u0004\u0001\u0002\u0005 time example method shown pseudo-code illustrates. following theorem asserts correctness algorithm assuming correct polynomial-time path-ﬁnding method used. proof algorithm clearly runs polynomial time. lemma tells line algorithm return true witnessing interdigitation. combining proposition shows algorithm returns true given polynomial-time algorithm subsumption proposition immediately suggests exponential-time algorithm deciding subsumption—by computing subsumption exponentially many timelines formula timelines formula. next theorem suggests cannot better worst case—we argue subsumption conp-complete reduction boolean satisﬁability. readers uninterested technical details argument skip directly section develop correspondence boolean satisﬁability problems include negation formulas lack negation imagine boolean variable propositions true false. particular given boolean satisﬁability problem variables take prop\u0002 containing propositions truek falsek represent truth assignment variables state proposition suggests checking subsumption critically involves exponentially many interdigitation specializations timelines formulas. proof design formula whose interdigitation specializations seen correspond truth assignments boolean variables shown following lemma. proof ﬁrst show deciding ama-subsumption conp providing polynomial-length certiﬁcate answer. certiﬁcate non-subsumption interdigitation timelines certiﬁcate checked polynomial time given interdigitation corresponding member test whether resulting timeline subsumed timeline using polynomial-time timeline subsumption algorithm. proposition guarantees subsumed every timeline answer subsumption query show conp-hardness reduce problem deciding satisﬁability -sat formula problem recognizing non-subsumption formulas. here negation idea reduction construct formula view exponentially many members representing truth assignments. construct timeline view representing show satisﬁable start showing satisﬁable assume satisﬁed truth assignment a—we know lemma prop\u0002. show prop\u0002 prop\u0002 subsumed conclude using proposition desired. must suppose contradiction prop\u0002 subsumed state since satisﬁes must generalize consider arbitrary generalization proposition implies must generalize formula lemma implies must subsume timeline longer size also subsumes timeline timelines must timeline choice every timeline subsumes subsumed proof speciﬁed conjunction. since timeline ig\u0004\u0006\u0005 subsumes timelines subsumes member show least-general formula consider formula must subsume members lemma implies timeline subsumes member ig\u0004\u0006\u0005 thus timeline proof know must subsume would fail subsume using and-to-or represent disjunction timelines given must least-general formula subsumes e—i.e. fis\u0004 theorem tells timelines given timelines theorem leads directly algorithm computing lgg—figure gives pseudo-code computation. lines pseudo-code correspond computation fis\u0004 timelines included subsumed timelines already pruning accomplished test line often drastically reduces size timeline perform subsequent computation—the ﬁnal result affected pruning since subsequent computation generalization step. remainder pseudo-code corresponds computation fis\u0004 include timelines ﬁnal result subsume timeline set. pruning step sound since timeline subsumes another conjunction timelines equivalent speciﬁc one. section traces computations algorithm example calculation. since sizes is\u0004\u0001\u0005 ig\u0004\u0001\u0005 exponential sizes inputs code figure doubly exponential input size. conjecture cannot better this proven doubly exponential lower bound case. input formulas timelines algorithm takes singly exponential time since is\u0004f\bg\u0005 prove exponential lower bound input formulas again readers uninterested technical details proof safely skip forward section together facts imply timeline ig\u0004f\b sequence propositions starting different ending square equal pair consecutive propositions following lemma implies square timeline omitted conjunction timelines ig\u0004\b using proposition polynomial-time subsumption algorithm. remains show check whether least subsumer. since theorem shows where example -subsumption often used place entailment generality relation. unlike semantic subsumption syntactic subsumption requires checking polynomially many subsumptions polynomial time syntactic subsumption trivially implies semantic subsumption—however converse hold general. consider formulas primitive propositions. however neither syntactically subsume syntactic subsumption fails recognize constraints derived interaction timelines within formula. syntactic least-general generalization. syntactic syntactically least-general formula syntactically subsumes input formulas. here least means formula properly syntactically subsumed syntactic syntactically subsume input formulas. based hardness syntactic semantic subsumption might conjecture similar exists syntactic semantic problems. proving exists requires closing lower upper bounds shown theorem favor upper bound suggested conjecture cannot show hardness semantic syntactic give syntactic algorithm exponentially efﬁcient best semantic algorithm found first show syntactic lggs exist unique mutual syntactic subsumption proof conjunction timelines syntactically generalize size larger proof theorem well deﬁned. show syntactic first note syntactically generalizes timeline generalizes timeline every member choice consider arbitrary deﬁnition syntactic subsumption timeline syntactic generalization member lemma implies subsumed syntactically subsumes general know semantic syntactic lggs different though clearly syntactic semantic generalization must subsume semantic lgg. example semantic discussed above; syntactic subsumes subsumed even formulas proof forward direction immediate since already know syntactic subsumption implies semantic subsumption. reverse direction note implies timeline subsumes \b—thus since single timeline timeline subsumes some timeline deﬁnition syntactic subsumption. proof consider syntactic proposition implies semantic generalization consider semantic conclude syntactically subsumes follows semantic proposition implies syntactically subsumed syntactic syntactically subsumes would least syntactic ﬁrst examination strengthening appears trivial given equivalence however semantically least necessarily stronger condition syntactically least—we ruled possibility semantically least generalization syntactically subsume another generalization semantically equivalent. proposition together theorem nice consequence learning approach syntactic formulas semantic formulas long original formulas syntactic lggs sets timelines. learning approach starts training examples converted timelines using lgcf operation syntactic lggs computed always syntactic lggs sets timelines hence also semantic lggs spite fact syntactic subsumption weaker semantic subsumption. note however resulting semantic lggs considerably larger smallest semantic proof suppose always syntactic formulas exponentially large. since proposition formula also semantic always semantic formulas exponentially large. contradicts theorem discouraging algorithm syntactic whose time complexity matches lower-bound unlike semantic case best algorithm doubly exponential worst case. theorem yields exponential time method computing semantic timelines \u0006—since timeline is\u0004\b\u0005 simply conjoin timelines ig\u0004\u0006\u0005. given formulas syntactic algorithm uses method compute polynomially-many semantic lggs sets timelines chosen input formula conjoins results. theorem yields algorithm computes syntactic exponential time— pseudo-code method given figure exponential time bound follows fact exponentially many ways choose line timelines)—the exponentially many semantic-lgg members line equivalent formula speciﬁed theorem possibly smaller pruning achieved statement lines timeline pruned subsumed timeline pruning timelines sound since timeline pruned output subsumes formula output—this fact allows easy argument pruned formula syntactically equivalent unpruned formula. section traces computations algorithm example calculation. note empirical evaluation discussed section cost terms accuracy using efﬁcient syntactic semantic lgg. know learned deﬁnitions made errors direction overly speciﬁc—thus since semantic-lgg least speciﬁc syntactic-lgg would advantage using semantic algorithm. method exponential amount work even result small still open question whether output-efﬁcient algorithm computing syntactic lgg—this problem conp conjecture conp-complete. route settling question determine output complexity semantic input formulas. believe problem also conp-complete proven this; problem output-efﬁcient method computing syntactic based theorem ﬁrst step calculating semantic according algorithm given figure compute interdigitation-specializations input formulas trivially is\u0004\b\u0005 calculate must consider possible interdigitations three computing algorithm returns conjunction timelines ig\u0004s redundant case ig\u0004s timelines removed trivially timeline thus algorithm correctly computes semantic syntactic algorithm shown figure computes series semantic lggs timeline sets returning conjunction results line algorithm cycles timeline tuples cross-product input formulas. case tuples ai—for tuple algorithm computes semantic tuple’s timelines. semantic computation tuple uses algorithm given figure argument always timelines rather formulas. reason lines superﬂuous timeline lines algorithm compute remains compute interdigitationgeneralizations returning conjunction timelines pruning interdigitations following steps since subsume another computed lines syntactic algorithm equal thus algorithm computes syntactic note that case syntactic general semantic lgg. implemented speciﬁc-to-general learning algorithm based lgcf syntactic algorithms presented earlier. implementation includes four practical extensions. ﬁrst extension aims controlling exponential complexity limiting length timelines consider. second describe often efﬁcient algorithm based modiﬁed algorithm computing pairwise lggs. third extension deals applying propositional algorithm relational data necessary application domain visual event recognition. fourth negation language show compute corresponding lgcfs lggs using algorithms adding negation turns crucial achieving good performance experiments. section review overall complexity implemented system. already indicated syntactic algorithm takes exponential time relative lengths timelines input formulas. motivates restricting language k-ama practice formulas contain timelines states. increased algorithm able output increasingly speciﬁc formulas cost exponential increase computational time. visual-event–recognition experiments shown later increased resulting formulas became overly speciﬁc computational bottleneck reached—i.e. application best values practically computable ability limit provided useful language bias. k-cover operator order limit syntactic algorithm k-ama. k-cover formula syntactically least general k-ama formula syntactically subsumes input—it easy show k-cover formula formed conjoining k-ma timelines syntactically subsume formula figure gives pseudo-code computing k-cover formula. shown algorithm correctly computes k-cover input formula. algorithm calculates least general k-ma timelines subsume timeline input—the resulting k-ma formulas conjoined redundant timelines pruned using subsumption test. note k-cover formula exponentially larger formula; however practice found k-covers exhibit undue size growth. given k-cover algorithm restrict learner k-ama follows compute k-cover input formula. compute syntactic resulting kama formulas. return k-cover resulting formula. primary bottleneck figure pseudo-code non-deterministically computing k-cover formula along non-deterministic helper function selecting block partition states timeline. original syntactic algorithm computing exponentially large interdigitationgeneralizations—the k-limited algorithm limits complexity computes interdigitationgeneralizations involving k-ma timelines. implemented learner computes syntactic k-ama formula sets—however directly algorithm describe above. rather compute formula sets single call algorithm typically efﬁcient break computation sequence pairwise calculations. describe approach potential efﬁciency gains. straightforward show syntactic semantic subsumption formulas. thus lgg\u0004 recursively applying transformation incrementally compute formulas sequence pairwise calculations. note since operator commutative associative ﬁnal result depend order process formulas. refer incremental pairwise strategy incremental approach strategy makes single call k-ama algorithm direct approach. simplify discussion consider computing formula \u0006—the argument extended easily formulas recall syntactic algorithm figure computes lgg\u0004\u0006\u0005 conjoining timelines ig\u0004\u0006\u0005 subsume others eliminating subsuming timelines form pruning. incremental approach applies pruning step pair input formulas processed—in contrast direct approach must compute interdigitation-generalization input formulas pruning happen. resulting savings substantial typically compensates extra effort spent checking pruning formal approach describing savings constructed ig\u0004f\bg conjoining results lggs timeline pairs furthermore know involving pruned shows need compute lggs involving rather need consider adding observation leads modiﬁed algorithm computing syntactic pair formulas. algorithm computes lggs modiﬁed algorithm proceeds non-subsuming timelines. given formulas follows compute subsumer computed running algorithm figure conjunction timelines cost performing polynomially many subsumption tests noticed signiﬁcant advantage using procedure experiments. particular advantage tends grow process training examples. fact incrementally process training examples resulting formulas become general—thus general formulas likely subsuming timelines. best case subsuming) step produces empty formula thus step performs work—in case return leonard produces relational models involve objects relations objects. thus event deﬁnitions include variables allow generalization objects. example deﬁnition pickup recognizes pickup \u0004hand; block; table\u0005 well pickup \u0004man; box; ﬂoor\u0005. despite fact k-ama learning algorithm propositional still able learn relational deﬁnitions. take straightforward object-correspondence approach relational learning. view models output leonard containing relations applied constants. since support supervised learning distinct training examples event type. implicit correspondence objects ﬁlling role across different training models given type. example models showing pickup \u0004hand; block; table\u0005 pickup \u0004man; box; ﬂoor\u0005 implicit correspondences given hhand; mani hblock; boxi htable; ﬂoori. outline relational learning methods differ much objectcorrespondence information require part training data. ﬁrst approach assumes complete object correspondence given input along training examples. given information propositionalize training models replacing corresponding objects unique constants. propositionalized models given propositional k-ama learning algorithm returns propositional k-ama formula. lift propositional formula replacing constant distinct variable. lavrac taken similar approach. approach assumes complete object-correspondence information. sometimes possible provide correspondences information always available. partial object correspondence available automatically complete correspondence apply technique. moment assume evaluation function takes relational models candidate object correspondence input yields evaluation correspondence quality. given training examples missing object correspondences perform greedy search best object-correspondence completions models. method works storing propositionalized training examples unpropositionalized training examples ﬁrst step empty evaluate pairs examples possible correspondences select pair yields highest score remove examples involved pair propositionalize according best correspondence subsequent step previously computed values pairs examples possible correspondences. select example correspondence yields highest average score relative models —this example removed propositionalized according winning correspondence added ﬁxed number objects effort expended polynomial size training set; however number objects appear training example allowed grow number correspondences must considered grows evaluation function based intuition object roles visual events often inferred considering changes initial ﬁnal moments event. speciﬁcally given models object correspondence ﬁrst propositionalize models according correspondence. next compute delete lists model. list propositions true ﬁnal moment initial moment. delete list propositions true initial moment ﬁnal moment. delete lists motivated strips action representations given addi deletei lists models evaluation function returns cardinalities delete. heuristic measures similarity delete lists models. intuition behind heuristic similar intuition behind strips actiondescription language—i.e. differences initial ﬁnal moments event occurrence related target event event effects described delete lists. found evaluation function works well visual-event domain. note full object correspondences given learner training examples interpreted specifying target event took place well objects ﬁlled various event roles rather object correspondences provided training examples interpreted specifying existence target event occurrence specify objects roles accordingly rules learned correspondences provided allow infer target event occurred objects ﬁlled event roles. example object correspondences manually provided learner might produce rule worth noting however upon producing second rule availability single training example correspondence information allows learner determine roles variables upon output ﬁrst rule. thus assumption learner reliably extract object correspondences need label training examples correspondence information order obtain deﬁnitions explicitly recognize object roles. language allow negated propositions. negation however sometimes necessary adequately deﬁne event type. section consider language ama\u0000 superset addition negated propositions. ﬁrst give syntax semantics ama\u0000 extend syntactic subsumption ama\u0000. next describe approach learning ama\u0000 formulas using above-presented algorithms ama. show approach correctly computes ama\u0000 lgcf syntactic ama\u0000 lgg. finally discuss alternative related approach adding negation designed reduce overﬁtting appears result full consideration negated propositions. subsumption. important difference ama\u0000 proposition establishing existence witnessing interdigitations subsumption longer true ma\u0000. need words timelines interdigitation witnesses note important notation rather event-logic formula satisﬁed model whenever false instant model. rather event-logic interprets indicating never true model notice ﬁrst form negation yield liquid property—i.e. true along interval necessarily subintervals. second form negation however yield liquid property provided liquid. important learning algorithms since assume states built liquid properties. light examples conjecture computationally hard compute ama\u0000 subsumption even timelines. reason extend deﬁnition syntactic subsumption ama\u0000 provides clearly tractable subsumption test analogous discussed ama. difference deﬁnition previous need test witnessing interdigitations timelines rather subsumption timelines. formulas note deﬁnition equivalent however ama\u0000 deﬁnition weaker result general formulas. might expect ama\u0000 syntactic subsumption implies semantic subsumption tested polynomial-time using subsumption graph described lemma test witnesses. learning. rather design lgcf algorithms directly handle ama\u0000 instead compute functions indirectly applying algorithms transformed problem. intuitively adding propositions models represent proposition negations. assume training-example models propositions propositions construct training models time false model time. forming training models compute least general formula covers models resulting formula propositions propositions turns syntactic subsumption least general ama\u0000 formula covers original training models. show correctness transformational approach computing ama\u0000 lgcf syntactic lgg. first introduce notation. models models true assigns unique true. notice inverse functional mapping approach handling negation using purely algorithms begins applying original training models. follows consider ama\u0000 formulas propositions formulas propositions since syntactic subsumption deﬁned using property straightforward show compute syntactic preserves syntactic subsumption assigns truth values described above yielding syntactically least-general ama\u0000 formula covers examples. found however using full negation often results learning overly speciﬁc formulas. help alleviate problem second method places bias negation. choice bias inspired idea that often much useful information characterizing event type prepost-conditions. second method called boundary negation differs full negation allows true initial ﬁnal moments model often produces overly general results full negation often produces overly speciﬁc much complicated results. review overall complexity visual event learning component discuss scalability issues. given training temporal models system following propositionalize training models translating negation descried section compute lgcf propositional model. compute k-ama lgcfs. return lifted version lgg. steps four require little computational overhead linear sizes input output respectively. steps three computational bottlenecks system—they encompass inherent exponential complexity arising relational temporal problem structure. step one. recall section system allows user annotate training examples object correspondence information. technique propositionalizing models shown exponential number unannotated objects training example. thus system requires number objects relatively small correspondence information given small number objects. often event class deﬁnitions interested involve large number objects. true controlled learning setting manage relational complexity generating training examples small number irrelevant objects. case domains studied empirically paper. develop efﬁcient domain-speciﬁc techniques ﬁltering objects ﬁnding correspondences. particular problem possible construct simple ﬁlter removes irrelevant objects consideration correspondences remaining objects. second provide learning algorithm hand-coded ﬁrst-order formulas deﬁning domain-speciﬁc features features used propositionalize training instances. third draw upon ideas relational learning design truly ﬁrst-order version k-ama learning algorithm. example could existing ﬁrst-order generalization algorithms generalize relational state descriptions. effectively approach pushes object correspondence problem k-ama learning algorithm rather treating preprocessing step. since well known computing ﬁrst-order lggs intractable practical generalization algorithms retain tractability constraining lggs various ways step three. system uses ideas section speedup k-ama computation training data. nevertheless computational complexity still exponential k—thus practice restricted using relatively small values restriction limit performance visual event experiments expect limit direct applicability system complex problems. particular many event types interest adequately represented k-ama small. event types however often contain signiﬁcant hierarchical structure—i.e. decomposed short sub-event types. interesting research direction consider using k-ama learner component hierarchical learning system—there could used learn k-ama sub-event types. note learner alone cannot applied hierarchically requires liquid primitive events learns non-liquid composite event types. work required construct hierarchical learner based perhaps non-liquid learning. finally recall compute examples system uses sequence pairwise calculations. ﬁxed pairwise calculation takes polynomial time. however since size pairwise grow least constant factor respect inputs worst-case time complexity computing sequence pairwise lggs exponential expect worst case primarily occur target event type compact k-ama representation—in case hierarchical approach described appropriate. compact representation empirical experience indicates growth occur—in particular pairwise tends yield signiﬁcant pruning. problems reasonable assumptions amount pruning imply time complexity computing sequence pairwise lggs polynomial data contains examples different event types pick down stack unstack move assemble disassemble. involve hand three blocks. detailed description sample video sequences event types siskind frames sample video sequences event types shown figure results segmentation tracking model reconstruction overlaid video frames. recorded movies event classes resulting total movies comprising frames. replaced assemble movie duplicate copy another segmentation tracking errors. event classes hierarchical occurrences events class contain occurrences events simpler classes. example movie depicting move event contains subintervals pickup putdown\u0004a; events occur. experiments learning deﬁnition event class movies event class used training. train movies event classes also depict occurrence event class learned subevent. however evaluating learned deﬁnitions wish detect events correspond entire movie well subevents correspond portions movie. example given movie depicting move event wish detect move\u0004a; event also pickup putdown\u0004a; subevents well. movie type data intended events subevents detected. deﬁnition detect intended event deem error false negative. deﬁnition detects unintended event deem error false positive. example movie depicts move\u0004a; event intended events move\u0004a; pickup putdown\u0004a; deﬁnition pick detects occurrence pickup pickup pickup charged false positives well false negative. evaluate deﬁnitions terms false positive negative rates describe below. event type evaluate k-ama learning algorithm using leave-one-movie-out crossvalidation technique training-set sampling. parameters learning algorithm degree negative information used. value either positivepropositions only boundarynegation fullnegation. parameters evaluation procedure include target event type training-set size given information evaluation proceeds follows movie movies apply kama learning algorithm randomly drawn training sample movies movies event type leonard detect occurrences learned event deﬁnition based event type record number false positives false negatives detected leonard. total number false positives false negatives observed held-out movies respectively. repeat entire process calculating times record averages since event types occur frequently data others simpler events occur subevents complex events vice versa report directly. instead normalize dividing total number times leonard detected target event correctly incorrectly within movies normalize dividing total record times experiments system fast enough give live demos boundary negation giving best results show less favorable parameter settings take hour number correct occurrences target event within movies normalized value estimates probability target event occur given predicted occur normalized value estimates probability event predicted occur given occur. evaluate k-ama learning approach leave-one-movie-out experiments described above varying example movies recorded color-coded objects provide complete object-correspondence information. compared learned event deﬁnitions performance sets hand-coded deﬁnitions. ﬁrst hand-coded deﬁnitions appeared siskind response subsequent deeper understanding behavior leonard’s model-reconstruction methods manually revised deﬁnitions yield another hand-coded deﬁnitions gives signiﬁcantly better performance cost performance. appendix gives event deﬁnitions along machine-generated deﬁnitions produced k-ama learning algorithm given training data evaluate algorithm ﬁnding object correspondences ignored correspondence information provided color coding applied algorithm training models event type. algorithm selected correct correspondence training models. thus data learning results correspondence information given identical correspondences manually provided except that ﬁrst case rules specify particular object roles discussed section since evaluation procedure uses role information rest experiments manual correspondence information provided color-coding rather computing correspondence technique perfect experiments suited event types. furthermore likely produce errors noise levels increase. since correspondence errors represent form noise learner makes special provisions handling noise results likely poor errors common. example worst case possible single extremely noisy example cause trivial cases forced improve noise tolerance learner. ﬁrst three rows table show values event types similar trends found general trend that increases decreases remains increases remains same. trend consequence k-cover approach. because increases k-ama language contains strictly formulas. thus -cover formula never general -cover. strongly suggests prove non-increasing non-decreasing rows four table show event types similar trends observed values general trend that degree negative information increases learned event deﬁnitions become speciﬁc. words decreases increases. makes sense since negative information added training models speciﬁc structure found data exploited k-ama formulas. that deﬁnitions pick overly general produce high alternatively learned deﬁnitions overly speciﬁc giving cost high experiments well others found yields best worlds event types lower achieved experiments shown demonstrated that without negation pick down increase arbitrarily attempt specialize learned deﬁnitions never significantly reduce indicates negative information plays particularly important role constructing deﬁnitions event types. bottom rows table show results attempted automatically select parameters learning rather focus comparing hand-coded deﬁnitions parameter judged best performing across event types. believe however parameters could selected reliably using cross-validation techniques applied larger data set. case parameters would selected perevent-type basis would likely result even favorable comparison hand-coded deﬁnitions. results show learned deﬁnitions signiﬁcantly outperform current data set. deﬁnitions found produce large number false negatives current data set. notice that although produces signiﬁcantly fewer false negatives event types produces false positives pick down. hand deﬁnitions utilize pick macros deﬁning events. performance learned deﬁnitions competitive performance main differences performance pick down learned deﬁnitions achieve nearly learned deﬁnitions achieve whereas signiﬁcant unstack disassemble learned deﬁnitions perform moderately worse respect learned deﬁnitions perform signiﬁcantly better assemble events. conjecture manual revision could improve perform well learned deﬁnitions every event class. nonetheless view experiment promising demonstrates learning technique able compete with sometimes outperform signiﬁcant hand-coding efforts authors. practical interest know training-set size affects algorithm’s performance. application important method work well fairly small data sets tedious collect event data. table shows learning algorithm event type reduced experiments used note event types hence shown. expect increase decreased since speciﬁc-to-general learning data yields more-general deﬁnitions. generally increases slowly increases abruptly also that several event types decreases slowly increased indicates larger data might yield improved results event types. motivation using logic-based event representation support perspicuity—in respect results mixed. note perspicuity fuzzy subjective concept. realizing this event deﬁnition perspicuous humans knowledge language would deﬁnition natural. here assume human detailed knowledge model-reconstruction process learner trying adding assumption would presumably make deﬁnitions qualify perspicuous many complex features learned deﬁnitions appear fact idiosyncrasies model-reconstruction process. sense evaluating perspicuity output entire system learned hand-coded deﬁnitions similar respect accuracy typically learned deﬁnitions much less perspicuous. simplest event types however learned deﬁnitions arguably perspicuous. look issue detail. appendix gives hand-coded deﬁnitions along machine-generated deﬁnitions. learned deﬁnitions correspond output k-ama learner training movies event type perspicuous deﬁnitions. pickup putdown\u0004x; deﬁnitions particular interest since short state sequences appear adequate representing event types— thus hope perspicuous -ama deﬁnitions. fact hand-coded deﬁnitions involve short sequences. consider hand-coded deﬁnitions pickup\u0004x; \u0005—the deﬁnitions roughly viewed timelines form begin;trans;end. state begin asserts facts indicate held asserts facts indicate held state trans intended model fact leonard’s model-reconstruction process always handle transition begin smoothly make similar observations putdown\u0004x; figure gives learned -ama deﬁnitions pickup putdown\u0004x; deﬁnitions contain timelines respectively. since deﬁnitions consists multiple parallel timelines ﬁrst seem perspicuous. however closer examination reveals that deﬁnition single timeline arguably perspicuous—we placed perspicuous timelines beginning deﬁnition. perspicuous timelines natural begin;trans;end interpretation. fact practically equivalent deﬁnitions pickup putdown\u0004x; mind notice deﬁnitions overly general indicated signiﬁcant false positive rates. learned deﬁnitions however yield false positives without signiﬁcant increase false negatives. learned deﬁnitions improve upon essentially specializing deﬁnitions conjoining non-perspicuous timelines. non-perspicuous timelines often intuitive capture patterns events help rule non-events. example learned deﬁnition pickup non-perspicuous timelines indicate attached true transition period event. attachment relationship make intuitive sense. rather represents systematic error made model reconstruction process pick events. summary learned deﬁnitions pickup putdown\u0004x; contain perspicuous timeline non-perspicuous timelines. perspicuous timelines give intuitive deﬁnition events whereas non-perspicuous timelines capture nonintuitive aspects events model reconstruction process important practice. note that experienced users primary difﬁculty hand-coding deﬁnitions leonard note event-logic deﬁnition pickup\u0004x; written compact form deﬁnition converted rather cannot translated exactly since uses disjunction—it disjunction timelines. primary difference deﬁnitions contain negated propositions. learner considers proposition negation proposition true point training movies. many negated propositions never appear positively thus included learned deﬁnitions. determining non-perspicuous properties must included. typically requires many iterations trial error. automated technique relieve user task. alternatively could view system providing guidance task. large deﬁnitions. stack\u0004w; unstack events nearly identical pick respectively. difference picking putting onto block tower thus might expect perspicuous -ama deﬁnitions. however learned deﬁnitions stack unstack figures involve many timelines pickup putdown\u0004w; accordingly deﬁnitions quite overwhelming much less perspicuous. despite large number timelines deﬁnitions general structure pick down. particular contain distinguished perspicuous timeline placed beginning deﬁnition conjoined many non-perspicuous timelines. clear that above perspicuous timelines natural begin;trans;end interpretation again similar deﬁnitions case however deﬁnitions overly general thus inclusion non-perspicuous timelines detrimental effect since unnecessarily specialize deﬁnition resulting false negatives. suspect primary reason large number non-perspicuous timelines relative deﬁnitions pick stems increased difﬁculty constructing force-dynamic models. inclusion block tower examples causes modelreconstruction process produce unintended results particularly transition periods stack unstack. result often many unintuitive physically incorrect patterns involving three blocks hand produced transition period. learner captures patterns roughly non-perspicuous timelines. likely generalizing deﬁnitions including training examples would ﬁlter timelines making overall deﬁnition perspicuous. alternatively interest consider pruning learned deﬁnitions. straightforward generate negative examples. these could remove timelines contribute toward rejecting negative examples. unclear prune deﬁnitions without negative examples. hierarchical events. move\u0004w; assemble disassemble inherently hierarchical composed four simpler event types. hand-coded deﬁnitions leverage structure utilizing simpler deﬁnitions macros. light clear that viewed non-hierarchically events involve relatively long state sequences. thus -ama adequate writing perspicuous deﬁnitions. spite representational shortcoming learned -ama deﬁnitions perform quite well. performance supports arguments using section namely given easier short rather long sequences practical approach ﬁnding deﬁnitions long events conjoin short sequences within events. examining timelines learned -ama deﬁnitions reveals might expect. timeline captures often understandable property long event sequence conjunction timelines cannot considered perspicuous deﬁnition. future direction utilize hierarchical learning techniques improve perspicuity deﬁnitions maintaining accuracy. note however that level learned deﬁnition move given figure perspicuous. particular ﬁrst timeline naturally interpreted giving prepost-conditions move action. initially supported hand empty ﬁnally supported hand empty. thus care prepost-conditions might consider timeline perspicuous. remaining timelines deﬁnition capture pieces internal event structure facts indicating moved hand. weaker case made assemble disassemble. ﬁrst timeline learned deﬁnitions figures interpreted giving prepost-conditions. however cases pre-conditions assemble quite incomplete. incompleteness inclusion examples model-reconstruction process properly handle initial moments. discuss bodies related work. first present previous work visual event recognition relates experiments here. second discuss previous approaches learning temporal patterns positive data. system unique combines positive-only learning temporal relational force-dynamic representation recognize events real video. prior work investigated various subsets features system—but date system combined pieces together. incorporating pieces system signiﬁcant endeavor. respect competing approaches directly compare system against. given this following representative list systems common features ours. meant comprehensive focuses pointing primary differences systems ours primary differences actually render systems loosely related ours. borchardt presents representation temporal relational force-dynamic event deﬁnitions deﬁnitions neither learned applied video. regier presents techniques learning temporal event deﬁnitions learned deﬁnitions neither relational force dynamic applied video. addition learning technique truly positive-only—rather extracts implicit negative examples event type positive examples event types. yamoto ohya ishii brand essa siskind morris brand oliver pentland bobick ivanov present techniques learning temporal event deﬁnitions video learned deﬁnitions neither relational force dynamic. pinhanez bobick brand present temporal relational event deﬁnitions recognize events video deﬁnitions neither learned force dynamic. brand mann jepson present techniques analyzing force dynamics video neither formulate event deﬁnitions apply techniques recognizing events learning event deﬁnitions. temporal data mining. sequence-mining literature contains many general-to-speciﬁc algorithms ﬁnding frequent sequences explore speciﬁc-togeneral approach. previous work researchers studied problem mining temporal patterns using languages interpreted placing constraints partially totally ordered sets time points e.g. sequential patterns episodes languages place constraints time points rather time intervals work here. recently work mining temporal patterns using interval-based pattern languages though languages learning frameworks vary among approaches share central features distinguish approach. first typically goal ﬁnding frequent patterns within temporal data set—our approach focused ﬁnding patterns frequency ﬁrst learning application visual-event recognition required patterns frequency less one. however number ways extend method direction becomes necessary second approaches standard general-to-speciﬁc level-wise search techniques whereas chose take speciﬁcto-general approach. direction future work develop general-to-speciﬁc level-wise algorithm ﬁnding frequent formulas compare speciﬁc-to-general approach. another direction design level-wise version speciﬁc-to-general algorithm—where example results obtained k-ama used efﬁciently calculate \u0005-ama lgg. whereas level-wise approach conceptually straightforward general-tospeciﬁc framework clear speciﬁc-to-general case. familiar temporal data-mining systems take speciﬁc-to-general approach. first-order learning section pointed difﬁculties using existing ﬁrst-order clausal generalization techniques learning formulas. spite difﬁculties still possible represent temporal events ﬁrst-order logic apply general-purpose relational learning techniques e.g. inductive logic programming systems require positive negative training examples hence suitable current positive-only framework. exceptions include golem progol claudien among others. performed full evaluation table complexity results summary. complexities relative input plus output size. size column reports worst-case smallest correct output size. indicates conjecture. systems early experiments visual-event recognition domain conﬁrmed belief horn clauses lacking special handling time give poor inductive bias. particular many learned clauses patterns simply make sense temporal perspective turn generalize poorly. believe reasonable alternative approach incorporate syntactic biases systems done example cohen dehaspe raedt klingspor morik rieger work however chose work directly temporal logic representation. finite-state machines finally note much theoretical empirical research learning ﬁnite-state machines view fsms describing properties strings case however interested describing sequences propositional models rather sequences symbols. suggests learning type factored arcs labeled sets propositions rather single symbols. factored fsms natural direction extend expressiveness current language example allowing repetition. aware work concerned learning factored fsms; however likely inspiration drawn symbol-based fsm-learning algorithms. presented simple logic representing temporal events called shown theoretical empirical results learning formulas. empirically we’ve given ﬁrst system learning temporal relational force-dynamic event deﬁnitions positive-only input applied system learn deﬁnitions real video input. resulting performance matches event deﬁnitions hand-coded substantial effort human domain experts. theoretical side table summarizes upper lower bounds shown subsumption generalization problems associated logic. case provided provably correct algorithm matching upper bound shown. table also shows worst-case size smallest could possibly take relative input size inputs. results table polynomial-time subsumption syntactic subsumption conp lower bound subsumption exponential size lggs worst case apparently lower complexity syntactic versus semantic lgg. described build learner based results applied visual-event learning domain. date however deﬁnitions learn neither crossmodal perspicuous. performance learned deﬁnitions matches handcoded ones wish surpass hand coding. future intend address cross-modality applying learning technique planning domain. also believe addressing perspicuity lead improved performance. authors wish thank anonymous reviewers helping improve paper. work supported part grants -iis -iis graduate fellowship fern center education research information assurance security purdue university. part work performed siskind research institute inc. give syntax semantics event logic called internal positive event logic logic used main text motivate choice small subset logic showing proposition deﬁne models ipel deﬁne. event type said internal whenever contains model also contains model agrees truth assignments represented ipel formula. formally syntax ipel formulas given ipel formulas prop primitive proposition subset thirteen allen interval relations fsfdbmo=siﬁdibiaioig allen relation given table difference ipel syntax full propositional event logic event logic allows negation operator that full event logic subset thirteen allen relations. operators used deﬁne formulas respectively subset merely abbreviations ipel operators ipel prop primitive proposition ipel formulas allen relations deﬁnition easy span\u0004\u0001 show induction number operators connectives formula ipel formulas deﬁne internal events. also verify deﬁnition satisﬁability given earlier formulas corresponds give here. number endpoints intervals. show preserves singleton sets commutes union. follows preserves allen interval relations. fact preserves sense argued along fact span depends follows deﬁnition interdigitation minimum maximum numbers construction inductive case assume claim holds ipel formulas fewer oper claim trivially must subset relations fsfd=g. notice formulas single allen relation thus since furthermore know combining facts satisﬁed similar arguments hold satisﬁes remaining three allen relations. finally consider case allen relations. again sufﬁces handle case single allen relation since span\u0004\u0001 facts properties easy verify satisﬁes proof prove result ig\u0004\u0006\u0005. proof is\u0004\u0006\u0005 follows similar lines. proposition witnessing interdigitation combine interdigitation show corresponding member ig\u0004\u0006\u0005 subsumed construct interdigitation ﬁrst notice that speciﬁes furthermore since states interdigitation easy show states corresponds consecutive subj;i timeline corresponding subsequence. interdigitation take union show interdigitation since state appearing know that states vertices corresponding tuples called co-occurrence vertices satisfy ﬁrst condition belonging edge follows deﬁnition co-occurrence vertices. consider co-occurrence interdigitation vertex equal show requirements co-occurrence vertex since piecewise total must co-occurrence vertex tuples corresponding vertices along path. must simultaneously consistent orderings. must piecewise total edge cross state transition either edge deﬁnition. interdigitation. finally deﬁnition edge ensures witnessing interdigitation tuple proof prove ﬁrst part lemma construct interdigitation corresponding member equivalent prop\u0002 prop\u0002. intuitively construct ensuring tuple consists states form truek falsek agree truth assignment—the union states tuple taken equal assign clear piecewise total simultaneously consistent state orderings interdigitation. union states equal prop\u0002 since prop\u0002 included state tuples. furthermore union states thus member corresponding equal prop\u0002 prop\u0002 desired. prove second part lemma member ﬁrst argue every state must contain either truek falsek since con; truek tains prop\u0002 prop\u0002. every state prop\u0002 falsek truek next claim either truek falsek—i.e. either states include truek states include falsek prove claim assume sake contradiction that truek falsek. combining assumption ﬁrst claim must states respectively. consider interdigitation a\u0000\u0007e corresponds member know equal union states tuples must include state timeline prop\u0002. prop\u0002 includes follows simultaneously consistent state orderings contradicting choice interdigitation. shows timeline construct interdigitation need smallest witnesses index know must exist range index guide construction interdigitation. interdigitation exactly following co-occurring states easy check piecewise total simultaneously consistent state orderings interdigitation. show witnesses showing states subsumed states co-occur co-occurring i—this implies states contained second item above. since square choose either cases state proof recall models propositions assume ama\u0000 uses primitive propositions also assume formulas propositions propositions proven above. assume lemma holds states fewer literals inductive assumption know covers proven above. assume lemma holds ama\u0000 formulas fewer timelines inductive assumption know covers event deﬁnitions written event logic denotes negation proposition attached supports supports contacts supported attached supports supports supports supports attached supports supports supported attached supports supports supports supports attached supports supports supported attached supports supports supports supports attached supports supports contacts supported attached supports supports supports supports attached\u0004w; supports\u0004w; supports\u0004y supports\u0004z contacts\u0004z attached\u0004z supported\u0004w\u0005 attached\u0004x; supports\u0004x; supports\u0004x; supports\u0004w; supports\u0004y attached\u0004w; supports\u0004w; supports\u0004y contacts\u0004y supports\u0004z contacts\u0004z attached\u0004z supported\u0004w\u0005 attached\u0004x; supports\u0004x; supports\u0004x; supports\u0004w; supports\u0004y attached\u0004w; supports\u0004w; supports\u0004y contacts\u0004y supports\u0004z contacts\u0004z attached\u0004z supported\u0004w\u0005 attached\u0004x; supports\u0004x; supports\u0004x; supports\u0004w; supports\u0004y attached\u0004w; supports\u0004w; supports\u0004y supports\u0004z contacts\u0004z attached\u0004z supported\u0004w\u0005 attached\u0004x; supports\u0004x; supports\u0004x; supports\u0004w; supports\u0004y [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 attached\u0004x; [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; contacts\u0004x; [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 supports\u0004x; attached\u0004w attached\u0004x; attached\u0004y; [supported\u0004y\u0005 supported\u0004x\u0005supports\u0004y; [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004x; supports\u0004y; attached\u0004w [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; [supported\u0004y\u0005 attached\u0004w supports\u0004z contacts\u0004y; [supported\u0004y\u0005 attached\u0004y; [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; contacts\u0004y; [supported\u0004y\u0005 attached\u0004w supports\u0004z contacts\u0004y; [supported\u0004y\u0005 attached\u0004w attached\u0004y; [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; supported\u0004y\u0005 attached\u0004w supports\u0004z contacts\u0004y; [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 attached\u0004w supports\u0004z contacts\u0004y; [supported\u0004y\u0005 supported\u0004x\u0005℄ [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 attached\u0004w supports\u0004z supported\u0004x\u0005℄ [supported\u0004y\u0005 supported\u0004x\u0005℄ [supported\u0004y\u0005 attached\u0004w supported\u0004y\u0005 contacts\u0004y; supports\u0004z supported\u0004x\u0005^ [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 contacts\u0004y; supported\u0004x\u0005℄ [supported\u0004y\u0005 supported\u0004x\u0005 supported\u0004y\u0005x℄ [supported\u0004y\u0005 attached\u0004w [supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; supported\u0004y\u0005 supported\u0004x\u0005 supports\u0004y; contacts\u0004x; contacts\u0004y; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; attached\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; contacts\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y contacts\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; contacts\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y contacts\u0004x; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y attached\u0004x; [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 contacts\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; supports\u0004z contacts\u0004y attached\u0004w; supports\u0004x; supports\u0004y contacts\u0004x; attached\u0004x; attached\u0004y supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y contacts\u0004x; contacts\u0004y supports\u0004w; supports\u0004x; attached\u0004w; attached\u0004x; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y contacts\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y attached\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005℄ [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004w; attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004w; attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 attached\u0004w; [supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y [supported supports [supported attached supported supported \u0004x\u0005; [supported attached attached supported [supported \u0004x\u0005℄ [supported attached [supported contacts supported \u0004x\u0005; [supported attached supports supported supported \u0004x\u0005; [supported attached attached supported [supported contacts [supported attached supported supported\u0004x\u0005 supported\u0004y\u0005 supports\u0004y supports\u0004z contacts\u0004x; contacts\u0004z supports\u0004w; supports\u0004w; supports\u0004x; attached\u0004x; attached\u0004w; attached\u0004x; attached\u0004z supported\u0004y\u0005 supported\u0004y\u0005; [supported\u0004y\u0005 attached\u0004w; attached\u0004z supported\u0004y\u0005 supported\u0004y\u0005; [supported\u0004y\u0005 supports\u0004w; attached\u0004w; supported\u0004y\u0005", "year": 2011}