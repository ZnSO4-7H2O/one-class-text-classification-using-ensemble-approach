{"title": "Chi-square Tests Driven Method for Learning the Structure of Factored  MDPs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "SDYNA is a general framework designed to address large stochastic reinforcement learning problems. Unlike previous model based methods in FMDPs, it incrementally learns the structure and the parameters of a RL problem using supervised learning techniques. Then, it integrates decision-theoric planning algorithms based on FMDPs to compute its policy. SPITI is an instanciation of SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures representing the transition function of the problem. These representations are used by an incremental version of the Structured Value Iteration algorithm. In order to learn the structure, SPITI uses Chi-Square tests to detect the independence between two probability distributions. Thus, we study the relation between the threshold used in the Chi-Square test, the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value. We show that, on stochastic problems, one can tune the threshold so as to generate both a compact model and an efficient policy. Then, we show that SPITI, while keeping its model compact, uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large RL problem with an unknown structure. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. We qualitatively show that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems.", "text": "sdyna general framework designed address large stochastic reinforcement learning problems. unlike previous model-based methods factored mdps incrementally learns structure problem using supervised learning techniques. spiti instantiation sdyna uses decision trees factored representations. first show that structured problems spiti learns structure fmdps using chi-square tests performs better classical tabular model-based methods. second study generalization property spiti using chi-square based measure accuracy model built spiti. markov decision process mathematical framework providing semantic model planning uncertainty. solution methods solve mdps based dynamic programming known work well small problems rely explicit state space enumeration. factored mdps alternative framework compactly represent large structured mdps framework dynamic bayesian networks used represent compactly transition reward functions exploiting dependencies variables composing state. solution methods adapted classical techniques linear programming proposed successfully tested large stochastic planning problems. moreover model-based learning methods proposed learn parameters model within transition reward functions unknown learning fmdps becomes problem learning structure dbns experience only. chickering friedman goldszmidt investigate techniques learning bayesian networks local structure. given observations techniques explicitly learn global structure network local structures quantifying network. paper describe structured dyna general framework merging supervised learning algorithms planning methods within fmdp framework. sdyna require explicitly build global structure dbns. precisely focus spiti instantiation sdyna uses decision trees factored representations. spiti directly learns local structures dbns based incremental decision tree induction algorithm namely representations simultaneously exploited modiﬁed incremental version structured value iteration algorithm computing policy executed agent. context incremental decision tree induction test independence probability distributions build model transition reward functions. threshold used split distributions directly impact size model learned critical ﬁnding good policy. first show that setting threshold high value makes spiti able build compact model without impacting quality policy. second show that keeping model compact spiti exploits generalization property model learning method perform better stochastic version dyna-q tabular model-based method. finally introduce measure accuracy transition model based remainder paper organized follows section introduce fmdps. section describe sdyna framework. section describe spiti explain exploit model learning evaluation. section describes empirical results spiti. discuss results section conclude describe future work within sdyna framework section background deﬁned tuple ﬁnite states ﬁnite actions immediate reward function markovian transition function stationary policy mapping deﬁning action taken state considering inﬁnite horizon evaluate policy state value function deﬁned using discounted reward criterion rt|s discount factor reward obtained time action-value function deﬁned γvπ) policy optimal policies value function optimal policy called optimal value function noted factored framework representation language exploiting structure problem represent compactly large mdps factored representations. fmdp states composed random variables taking value ﬁnite domain dom. state deﬁned vector values dom. state transition model action deﬁned transition graph represented two-layer directed acyclic graph whose varinodes able time variable time noted arentsa. graph quantiﬁed conditional probability associated distribution cpda transition model node fmdp deﬁned separate model hga{cpda similarly dyna architecture sdyna proposes integrate planning acting learning solve trial-and-error stochastic problem unknown transition reward functions. however sdyna uses fmdps representation language able address large problems. overview sdyna given figure neither fmdp framework sdyna specify factored representations used. factored representations noted fact sdyna exploit certain regularities represented function representations include rules decision trees algebraic decision diagrams. sdyna decomposed three phases acting learning planning next section details phases context instantiation sdyna named spiti. acting spiti similar acting algorithms. planning phase builds action-value functions tree) representing expected discounted reward taking action state following greedy policy. thus agent behave greedily executing best action states. spiti uses \u0001-greedy exploration policy executes best action time small probability selects uniformly random action independently planning implemented using incremental version algorithm adapted value iteration algorithm using decision trees factored representations instead tabular representations. spiti uses incremental version rather original reasons. first returns greedy policy adequate exploration policies \u0001-greedy. second computes evaluation value function convergence despite incomplete incrementally updated model transition reward functions. modiﬁed version used spiti described figure time step algorithm spiti updates producing action-value function tree) respect value function tree using regress operator deﬁned boutilier then action-value functions tree) merged using maximization combination function compute value function tree associated greedy policy using erge operator. operator used produce single tree containing partitions occurring trees merged whose leaves labeled using combination function labels corresponding leaves original trees. tree reused time update action-value functions tree). refer boutilier detailed description erge regress operators. trials agent compose stream examples must learned incrementally. spiti incremental decision tree induction algorithms noted learnt ree. stream examples attributes class example learnt reea builds decision tree tree representing factored representation probability shown figure reward learning algorithm straightforward. observation agent learnt learn factored representation tree reward function example transition model composed action quantiﬁed local structures conditional probability distributions assuming syncpda chronic shown figure thus explicit representation global structure dbns representing transition functions built. learnt algorithm implemented using informationtheoric metric described next section. detailed derefer utgoﬀ scription iti. quantiﬁed tree) training examples cpda present leaf tree built iti. model tree learned used planning compute action-value functions. avoid training data overﬁtting thus test binary attribute installed value computed probabilities threshold noted determining whether node must split diﬀerent leaves. neither planning acting spiti require build explicit representation global structure dbns however shown figure build representation assigning arentsa variables corresponding attributes used tests tree spiti initialized empty tree terms discounted reward size model built structured problems. reward transition functions optimal value function known problems. based knowledge additional criteria used namely relative error accuracy model improve evaluation spiti. criteria respectively measure good policy compared optimal policy accurate model transitions learned spiti. average relative value error compute operators using tree representations. given policy tree evaluate structured successive approximation algorithm based exact transitions reward functions compute associated value function tree. then {tree tree} ﬁrst compute tree using erge operator using combination function relative value error. then computed according l∈tree i∈|x| |dom| accuracy model introduce measure qualify accuracy model learned spiti. accuracy model complementary relative error evaluates model learned spiti independently reward function. deﬁned values ias)) computed using implementations proposed press degree freedom. probability computed leaf weighted size state space represented leaf penalizing errors model covers large part state space. average obtained dividing weighted number state/action pairs. present three diﬀerent analyses based empirical evaluations spiti. ﬁrst analysis studies relation value threshold size model built spiti hand between relative error value function induced greedy policy hand. second analysis compares spiti dyna-q terms discounted reward model size. last analysis studies generalization property model learning process spiti. ﬁrst empirical study done coﬀee robot process planning. problems deﬁned boutilier complete deﬁnition reward transition functions available spudd website. coﬀee robot stochastic problem composed actions boolean variables. represents robot must caf´e coﬀee deliver owner. robot reaches terminal state owner coﬀee. process planning stochastic problem composed actions binary variables factory must achieve manufactured components producing depending demand high quality components quality components figure shows size transition model built spiti coﬀee robot process planning problems diﬀerent values illustrates impact number nodes created trees consequently number dependencies variables fmdp. problems size model least divided high values compared values. whereas value signiﬁcant impact size model much limited impact generated policy shown figure despite decreasing model sizes relative error computed increases slightly analyses stochastic problems deﬁned boutilier initial states terminal states added problem deﬁnitions agent perform multiple trials during experiment. agent terminal state state randomly initialized initial states. initial states composed non-terminal states least policy reaching terminal state. experiments analysis. required compute oﬀ-line optimal policy using span semi-norm termination criterion supremum norm termination criterion. algorithms. order study inﬂuence quality policy following protocol ﬁrst random trajectory executed time steps. second value ﬁxed transition reward model built trajectory spiti described section third policy examine consequences threshold stochastic problem deﬁne problem named noisy. boutilier deﬁne problems namely linear expon illustrate respectively best worst case scenario spi. transition reward functions noisy deﬁned according deﬁnition linear problem constant level noise existing transitions. present additional results spiti linear expon noisy problems section impact threshold important previous problems coﬀee robot process planning contain deterministic transitions. spiti builds model nodes less nodes identical trajectory. figure shows noisy problem compact transition model generates better policy larger transition model even model learned trajectory. study compare spiti stochastic implementation dyna-q coﬀee robot process planning. \u0001-greedy exploration policy ﬁxed dynaq spiti. dyna-q used number planning steps twice size model initialized optimistically spiti results previous section show high value threshold appropriate. thus also reference agents noted random optimal executing respectively random action best action time step. discounted reward deﬁned rdisc reward received agent figure shows discounted reward rdisc obtained agents coﬀee robot problem. small problem dyna-q spiti quickly execute near optimal policy. however model learned make results illustrating rdisc figure discounted reward obtained process planning problem. spiti executes quickly near optimal policy large structured problem unlike dyna-q. figure shows discounted reward rdisc obtained agents process planning problem. spiti able execute near optimal policy approximately time steps whereas dyna-q starts improve policy time steps. comparing size transition model learned dyna-q builds representation approximately nodes would keep growing experiment continued whereas spiti builds structured representation stabilized less nodes. third study qualify loss accuracy model built learn algorithm size problem grows whereas experience agent not. following protocol ﬁrst random trajectory executed environment time steps. then compute transition model built trajectory spiti actual deﬁnition problem. finally restart using problem action binary variable. random trajectory experiment avoid dependency reward function learning process. figure accuracy model learned spiti after time steps problems variable size. accuracy decreases linearly whereas size problem grows exponentially sdyna architecture designed integrate planning learning acting fmdps. paper focused spiti instantiation sdyna uses decision trees factored representation. spiti simultaneously learns structured model reward transition functions uses incremental version compute policy. spiti learns structure problem using test independence probability distributions. ﬁrst shown threshold used determine whether dependencies added transition model signiﬁcant impact size model learned limited impact quality policy generated spiti moreover ﬁgure ﬁgure show setting value adapted stochastic problems. indeed large models contain unnecessary dependencies consequently require samples accurately quantiﬁed. setting high value spiti able build compact representation transition reward functions problem withdegrading policy. structure problems million state/action pairs performs better dyna-q. unlike tabular learning algorithms decision tree induction algorithms build factored representations endow agent generalization property. decision trees used represent transition reward functions spiti propose default class distribution examples presented. consequently agent able choose adequate actions states visited yet. shown section generalization property spiti accelerates resolution large problems. third used accuracy measure study generalization property transition model learning spiti. shown constant number time steps accuracy model built spiti decreases linearly number state/action pairs problem grows exponentially. result indicates spiti able scale well larger structured problems. spiti evaluated using three diﬀerent criteria relative error discounted reward rdisc accuracy measure criteria cannot applied real world problems relative error requires know optimal value function whereas accuracy measure requires know exact transition function problem. moreover discounted reward criterion takes account loss reward received exploration policy. thus complementary rdisc fully evaluates parameters spiti used kind problems. described paper general approach model-based context fmdps assuming structure problems unknown. presented instantiation approach called spiti. results show empirically spiti performs better tabular model-based algorithm learning compact representation problem derive good policy exploiting generalization property learning method particularly problem gets larger. however spiti currently limited exploration policy \u0001-greedy planning method adapted svi. currently working integrating model-based learning planning methods fmdps address larger problems addressed paper.", "year": 2012}