{"title": "Solving internal covariate shift in deep learning with linked neurons", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "This work proposes a novel solution to the problem of internal covariate shift and dying neurons using the concept of linked neurons. We define the neuron linkage in terms of two constraints: first, all neuron activations in the linkage must have the same operating point. That is to say, all of them share input weights. Secondly, a set of neurons is linked if and only if there is at least one member of the linkage that has a non-zero gradient in regard to the input of the activation function. This means that for any input in the activation function, there is at least one member of the linkage that operates in a non-flat and non-zero area. This simple change has profound implications in the network learning dynamics. In this article we explore the consequences of this proposal and show that by using this kind of units, internal covariate shift is implicitly solved. As a result of this, the use of linked neurons allows to train arbitrarily large networks without any architectural or algorithmic trick, effectively removing the need of using re-normalization schemes such as Batch Normalization, which leads to halving the required training time. It also solves the problem of the need for standarized input data. Results show that the units using the linkage not only do effectively solve the aforementioned problems, but are also a competitive alternative with respect to state-of-the-art with very promising results.", "text": "work proposes novel solution problem internal covariate shift dying neurons using concept linked neurons. deﬁne neuron linkage terms constraints ﬁrst neuron activations linkage must operating point. share input weights. secondly neurons linked least member linkage non-zero gradient regard input activation function. means input activation function least member linkage operates non-ﬂat non-zero area. simple change profound implications network learning dynamics. article explore consequences proposal show using kind units internal covariate shift implicitly solved. result this linked neurons allows train arbitrarily large networks without architectural algorithmic trick effectively removing need using re-normalization schemes batch normalization leads halving required training time. also solves problem need standarized input data. results show units using linkage effectively solve aforementioned problems also competitive alternative respect state-of-the-art promising results. training deep neural networks difﬁcult task. simply solving optimization problem like support vector machines gradient boosting. factors role depth architecture nature non-linear activation functions initialization weights critical good behavior learning process. years techniques tricks hints used train reﬁned nowadays large variety options aimed improving neural network learning process. combination depth non-linear activations favors change data distribution ﬂows network. effect called internal covariate shift lies core problems vanishing/exploding gradients dead neurons. several solutions appeared years. either change activation function allow gradient back learning process correct internal distribution change means re-normalization techniques article propose concept linked neurons. brieﬂy deﬁne neuron linkage terms constraints ﬁrst neuron activations linkage must operating point. words share input weights. secondly neurons linked least member linkage non-zero gradient respect input activation function. means input activation function least member linkage operates non-ﬂat non-zero area. practice constraint forces least neuron linkage operate complementary space rest set. simple concept dramatic change network learning dynamics guarantees even neurons dead regions rectiﬁed linear unit always receive gradient ﬂow-back learning process. concept linked neurons important impact learning dynamics allowing implicitly effectively alleviate problem internal covariate shift since network able learn disregarding shift data scaling layer. beneﬁcial implications deep learning practice avoids need normalizing input data. additionally internal covariate shift implicitly handled avoids need additional re-normalization techniques robust activation functions. consider dead regions part activation function outputs constant value. sense gradient areas zero contribute learning process inducing dead neuron effect effective reduction learning process. seeing proposal ensures always nonzero gradient ﬂowing network opens horizons deep learning practitioner enabling creation architectures guaranteed learning dynamics. exploring similar ideas ones proposed article crelu drelu. works focus concatenating complementary relu activations either terms mirroring symmetry respect origin respectively. differing works focused improving accuracy convolutional neural networks recurrent neural networks proposal studies generalization works impacts learning dynamics effectively solving problem internal covariate shift dead neurons regardless base activation function used. particular experiments batch normalization. show study crelu drelu seen instantiations linked neurons concept batch normalization redundant linked neuron member implicitly corrects internal covariate shift. easily speed learning processing time factor without hindering learning dynamics predictive performance. following sections review widely used activation functions methods solve problems described introduction. following that formally introduce concept linked neurons. experimental section carry in-depth analysis proposal. first intuition behind proposal dynamics described empirically shown simple problems. then study learning behavior different activations considering wide deep architectures. highlight strengths weaknesses techniques different scenarios ranging unnormalized data lack re-normalization deep architectures. proposed technique successfully handles scenarios. finally compare proposal terms performance metrics different problems using state-of-the-art architectures showing suitability proposed technique. problems internal covariate shift dying neurons core many problems learning process deep learning models. order solve effects different approaches used literature. aims alleviating effects changing activation function. example dying neurons appear when usually covariate shift distribution data input layer sets operation point neuron area activation function. effect clearly seen activation functions relu sigmoid hyperbolic tangent activations. order alleviate effect variations functions proposed. proposals usually involve adding slope activation function area. examples family methods include leaky relu parametric relu. second solving problems directly deal covariate shift effect. again modify activation function deal effect. scaled exponential rectiﬁed linear unit successful example family. alternatively directly re-normalize data output layer. well known method family batch normalization. section review previously named methods. also include newly proposed swish activation claimed superior former activations. rectiﬁed linear unit rectiﬁed linear unit ﬁrst instantiation family activation functions inspired simpliﬁcation biological neurons activation ﬁrstly used neural networks however apparition considered integral part success becomes popular. practical success made default activation function computer vision tasks. consists simply truncating negative part input zero originally formulated solution problem vanishing gradient commonly appeared using sigmoid hyperbolic tangent functions. relu effectively solves problem removing saturation operation area input activation large positive value. comes cost allowing appearance exploding gradients. positive side bounded output large value propagates back gradient creating feedback turn makes activations even greater. this ultimately ends exploding gradient effect learning process breakdown. order solve problem practitioners frequently gradient clipping trick. another equally important issue relu death neuron effect. occurs operation point activation function deep negative side. means activations turn zero independently input data. effectively stops possible recovery neuron learning process rendering neuron unusable. leaky rectiﬁed linear units rons mentioned former subsection since cause dying neurons truncation zero relu leaky relu replaces truncation scaled linear model scaling allows model retain non-linear behavior simultaneously avoiding zero output activations. however introduces hyperparameter tuned. difﬁcult affected size activations turn determined amount regularization learning rate distribution data thus making difﬁcult establish safe default. leaky relu improvement proposed name parametric relu takes leaky relu step further expanding leak parameter shared neurons layer full weights neuron. contrary leaky relu hyperparameter ﬁxed beforehand parametric relu weight learnable jointly trained network weights. parametric relu following form swish novel activation function claims superior terms performance compared previous ones. equipped sigmoid multiplying input negative side causes small hump close zero understanding small hump helps preventing dying neurons effect decreases input activation function large negative value. additionally worth noting contrary activation functions swish convex function quasiconvex one. effects change specifically studied original article. swish activation function follows batch normalization technique developed directly solving problem internal covariate shift. name suggests consists normalizing batch using couple normalization parameters corresponding shift scale unit. parameters namely jointly trained weights using backpropagation rest network. increases number parameters improves performance enable higher learning rates. selu activation function proposed activation attempts simultaneously solve problem dying neurons covariate shift. proposal introduces exponential function negative side activation exponential linear units improves adding scaling term activations guaranteed tend ﬁxed point mean variance practice seen pseudo-normalization data ﬂows network. proposal introduces pair parameters chosen guarantee attraction desired ﬁxed point. experimental section solid general purpose activation caveat requires normalize data beforehand ensure operating point part activation function. following equation gives details activation function proposal aims solve problem internal covariate shift thus avoid dying neurons effect. different solutions described section proposal considered framework valid former activation functions. framework formalized terms constraints link neurons. idea behind proposal linked neurons always least neuron operating point outside area. formally name neurons linked least member linkage non-zero gradient data point. although amount neurons linked arbitrary number sake simplicity rest paper focus linking neurons. allow already proposed activation units inside proposed framework. figure shows simple examples linked neurons. observe members coupling exactly input. neurons coupling share weights. however consider hyperbolic tangent sigmoid also quasi-convex functions. however differently swish quasi-convexity activations models saturation effect. linking condition holds i.e. least partial gradient activations different zero .this directly ensures dead neurons training process guarantees constant gradient ﬂowing backwards coming units next layer. many ways deﬁning activations fulﬁll linking conditions. restrict linked activations case neurons current activation functions horizontal mirroring sufﬁces fulﬁll coupling constraints structure section parts. ﬁrst part investigate effects using linked neurons. study effects using linked neurons wide deep architectures separately. second part experiments evaluate proposal terms performance accuracy using state-of-the-art architectures namely allcnn resnet. experiments following single activations relu prelu generalization leaky relu selu swish. activations using linked versions using link mirror linking described proposal section. linked activations doubles number outputs effectively having twice number parameters network. clear implication terms increment capacity classiﬁer potentially increment performance. sake fairness experiments also compare proposals enlarged versions original network appropriate. cases propose multiply number neurons layer enlargement makes experiments fair since resulting network exactly number parameters linked version. implemented experiments using keras tensorﬂow code available effect linked neurons wide deep arconstruction activations must observe constraint coupling i.e. operating point coupling operates nonﬂat area figure shows linked rectiﬁed linear units using min-max policy linking. figure shows linked selu activations. note second case opted using different linking. case horizontal mirroring fulﬁls constraints. expected independently input value coupled always least activations functions active. formally linked activation vectorial function many dimensions linked activations. sake discussion consider element activations functions linked. thus gd). observe input data vectorial function input weights data sample. consider gradient loss function respect weights particular unit i.e. straightforward proof gradient ﬂowing back loss following form order explore behavior different compared methods terms depth simple network. designed layer network convolutional units layer size except ﬁrst layer size network trained using adam decay learning rate random seed order guarantee reproducibility. results experiment found table table shows results width scenario. observe general using linked activation improves results. expected number parameters increased respect single activations. terms performance linked relu linked prelu achieve best performance. also expected number neurons increases difference single activation linked counterparts reduced probably enough discriminant capacity solve problem best architecture capabilities. table shows results deep scenario. compare basic activations linked counterparts. also include extended versions basic activation order consider extra number parameters introduced proposal. linked versions activations except linked swish effectively able learn despite covariate shift. particular linked selu achieves best results closely followed linked relu linked prelu. claimed original article selu able learn scenario. commented previous section selu robust internal covariate shift long operating correct range failure linked swish expected deserves study. batch normalization relu included sake completion cannot compete terms performance. rest activations fail learn. also count amount dead neurons experiment. dead neuron characterized activation equal zero data samples. expected linked variants selu neurons alive. exception swish linked swish display unexpected exploding gradient effect effectively breaking convergence despite lowering learning rate. rest activations fail learn show dead neurons layer onwards. conﬁrms covariate shift negative effect learning process deep architectures. since proposal twice number outputs neuron doubles size parameters needed output layer. hereby reasonable suppose additional number parameters increases representation capabilities network resulting increment predictive performance. experiment compare performance using basic activation function linked counterpart extended version network accounts extra parameters. extended version network increases number units times number units thus matching layer number parameters linked case. network simple convolutional network composed layers size plus dense layer units. dataset cifar-. train network epochs using rmsprop learning rate decay results shown table single stands normal activations linked proposal extended versions. notice proposal beats single section conduct experiments order maximize predictive performance using state-of-the-art architectures. goal assess viability linked neurons proposal strictly compare rest activations scenario experimental settings avoiding drifts stochastic nature optimization process. cifar- dataset compare different architetures allcnn resnet. architecture details experimental settings allcnn network architecture currently holds performance ﬁgures close best performant state-of-the-art methods cifar-. network rather wide featuring layers units each. followed recommended settings. however linked versions usually accommodate larger gradient magnitude values ﬂowing back changed learning rate policy following step approach keep initial learning rate epochs epoch epoch epoch ﬁnally epoch original values learning rates also report results using ﬁxed learning rate resnet family networks based residual connections residual connections allow significantly increase depth network also increasing performance. experiment variation resnet originally designed imagenet scaled order suit cifar-. architecture features convolutional layers units. network trained using adam learning rate decay epochs. former case apply mild data augmentation consisting random ﬂips vertically horizontal shifts corresponding size axis. linked relu achieves best accuracy closely followed prelu. however observe lack consistency results linked versions. believe different gradient dynamics intrinsic linked versions. speciﬁc study effect deserves attention. resnet linked selu outperforms rest activations. however case linked versions achieve better results basic counterparts. considering point proposal selu share property robust covariate shift propose another experiment resnet network removing batch normalization layers. consider lk-relu lk-selu selu. results presented table lk-relu lk-selu perform similarly without batch normalization. selu lk-selu without batch normalization sensitive learning rate. seems suggest might related intrinsic dynamics selu. additionally compare convergence speed. taking reference lk-relu without batch normalization table linked versions times faster using batch normalization. compare speed gain achieved dropping batch normalization lk-relu times faster selu times faster lk-selu times faster. article introduced concept linked neurons. linked neurons neurons joined terms constraints. ﬁrst constraint states elements linkage share input. practical terms accomplished sharing weights. second constraint forces least activation functions linkage operates locally non-constant region function. ensures non-zero gradient propagate backwards vanish. result network learning dynamics change ensuring dead neurons providing constant gradient learning process implicitly corrects internal covariate shift. help deep learning practitioner ensuring model learn even front properly normalized data. additionally learning dynamics adapts network achieve internal covariate shift invariance. opens possibility remove renormalization schemes batch normalization network considerably speeding learning process. believe thanks regularity gradient approach opens opportunity explore learning rate policies guarantee avoiding exploding gradients allowing effectively increase convergence rate algorithm. also think approach help deﬁnition architectures focused concept linkages among neurons. side note tips tricks good practices neural networks speciﬁcally tuned independent activations. must updated linked neurons scenario.", "year": 2017}