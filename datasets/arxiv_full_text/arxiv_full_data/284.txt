{"title": "Sort Story: Sorting Jumbled Images and Captions into Stories", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.", "text": "fader weston answering questions related event occurs events occurred prior particular event require temporal reasoning. good temporal model events everyday life i.e. temporal common sense could also improve quality communication systems humans. stories form narrative sequences inherent temporal common sense structure. propose visual stories depicting personal events learn temporal common sense. stories sequential image narrative dataset aligned image-caption pairs together form coherent story. given input story jumbled train machine learning models sort coherent story temporal common sense applications tasks multi-document summarization human-ai communication. propose task sequencing given jumbled aligned image-caption pairs belong story task sort output sequence forms coherent story. present multiple approaches unary pairwise predictions ensemble-based combinations achieving strong results task. text-based image-based features depict complementary improvements. using qualitative examples demonstrate models learnt interesting aspects temporal common sense. sequencing task children aimed improving understanding temporal occurrence sequence events. task given jumbled images belong single story sort correct order form coherent story. motivation work enable systems better understand predict temporal nature events world. train machine learning models perform task sequencing. temporal reasoning number applications multi-document summarization multiple sources news information relative order events useful accurately merge information temporally consistent manner. question answering tasks recently narrative chains contain information participants causal relationships events enable understanding stories. number works learn temporal relations properties news events dense expert-annotated timebank corpus work however multimodal story data temporal annotations. number works also reason temporal ordering using manually deﬁned linguistic cues approach uses neural networks avoid feature design learning temporal ordering. recent works learn distributed representations predicates sentence tasks event ordering cloze evaluation. unlike work approach makes multi-modal data free-form natural language text learn event embeddings. further models trained end-to-end pipelined approach involves parsing extracting verb frames sentence errors propagate module next chen generalized mallows model modeling sequences coherence within single documents. approach also applicable task. recently mostafazadeh presented rocstories dataset sentence stories stereotypical causal temporal relations events. work though make multi-modal story-dataset contains images associated story-like captions. works vision also temporally order images; typically ﬁnding correspondences multiple images scene using geometry-based approaches. similarly choi compose story multiple short video clips. deﬁne metrics based scene dynamics coherence dense optical patch-matching. contrast work deals stories containing potentially visually dissimilar semantically coherent images captions. recent works summarize hundreds individual streams information deal single concept event learn common theme storyline timeline summarization. task however predict correct sorting given story different summarization retrieval. ramanathan attempt learn temporal embeddings video frames complex events. motivation similar ours deal sampled frames video attempt learn temporal common sense multi-modal stories consisting sequence aligned image-caption pairs. section ﬁrst describe components approach unary scores context pairwise scores encode relative orderings elements. next describe combine scores voting scheme. thought+mlp) takes input pair skipthought embeddings trains hinge-loss) outputs score placing language+vision pairwise model concatenates skipthought embeddings trains similar above. language-alone neural position embedding model. instead using frozen skip-thought embeddings learn task-aware ordered distributed embedding sentences. speciﬁcally sentence story embedded lstm relu non-linearities. similar max-margin loss applied negative examples vendrov asymmetric penalty encourages sentences appearing early story placed closer origin sentences appearing later story. train time parameters lstm learned end-to-end minimize asymmetric ordered loss test time lij. thus move away origin embedding space traverse sentences story. three pairwise approaches assigns score ordered pair elements used construct pairwise scoring model summing scores possible ordered pairs permutation. pairwise score captures local contextual information stories. finding best permutation maxσ∈σn pairwise model np-hard approximations required. experiments study short sequences space permutations easily enumerable longer sequences utilize integer programming methods well-studied spectral relaxations problem. denotes probability element present position output n-way softmax layer deep neural network. experiment networks language-alone gated recurrent thought+mlp) unit proposed embed caption vector space. skipthought trained bookcorpus predict context given sentence. embeddings input multi-layer perceptron language+vision thought+cnn+mlp) embeds caption embeds image convolutional neural network activations penultimate layer -layer vggnet shown generalize well. embeddings concatenated input mlp. ordering permutation) maxσ∈σn found efﬁciently time hungarian algorithm since unary scores inﬂuenced elements story capture semantics linguistic structures associated speciﬁc positions stories e.g. beginning middle end. pairwise models similar learning rank approaches develop pairwise scoring models given pair elements learn assign score indicating whether element placed element permutation here indicates iverson bracket develop experiment following pairwise models language-alone pairwise model combine complementary information captured unary pairwise models voting-based ensemble. method ensemble three permutations. permutations vote particular element placed particular position. vote matrix stores number votes element oc== j]]). hungarian algorithm optimal permutation maximizes votes assigned i.e. vote maxσ∈σn experimented number model voting combinations found combination pairwise skip-thought+cnn+mlp neural position embeddings work best train evaluate model personal multimodal stories sind story sequence images corresponding story-like captions. narrative captions dataset e.g. friends good time capture sequential conversational language characteristic stories. stories training validation stories testing. evaluate performance model correctly ordering jumbled story elements using following metrics spearman’s rank correlation measures ranking story elements predicted ground truth orders monotonically related pairwise accuracy measures fraction pairs elements whose predicted relative ordering ground truth order average distance measures average change position elements predicted results pairwise models unary models shown table pairwise models based skip-thought features outperform unary models task. however pairwise order model performs worse unary skip-thought model suggesting skip-thought features encode context sentence also provide crucial signal temporal ordering story sentences. contribution image features augmenting text features image features results visible performance improvement model trained unary features model trained pairwise features. image features result poor performance task seem capture temporal information complementary text features. ensemble voting exploit fact unary pairwise models well text image features capture different aspects story combine using voting ensemble. based validation found combining pairwise order model pairwise model skip-thought image features performs best. voting based method achieves best performance three metrics. shows different approaches indeed capture complementary information regarding feasible orderings caption-image pairs form coherent story. duplicate pipelined approach modi titov this ﬁrst parse story sentences extract tuples however step succeeds test data. even consider perfect downstream algorithm always makes correct position prediction given tuples overperformance still spearman correlation i.e. upper bound performance pipelined approach lower performance text-only end-to-end model table visualizations position predictions model demonstrate learnt three structure stories setup middle climax. also present success failure examples sorting model’s predictions. supplementary details ﬁgures. visualize model’s temporal common sense fig. word clouds show discriminative words words model believes indicative sentence positions story. size word proportional ratio frequency occurring position positions. words like ‘party’ ‘wedding’ etc. probably model believes start story describes setup occasion event. people often tend describe meeting friends family members probably results discriminative words ‘people’ ‘friend’ ‘everyone’ second third sentences. morepropose task sequencing image-caption pairs motivation learning temporal common sense. implement multiple neural network models based individual pairwise element-based predictions utilize image text features achieve strong performance task. best system average predicts ordering sentences within distance error positions. also analyze predictions show qualitative examples demonstrate temporal common sense. thank ramakrishna vedantam anonymous reviewers helpful suggestions. work supported career awards awards ictas junior faculty awards google faculty research award grant wnf--- grant n--- n--- award paul allen family foundation allen distinguished investigator award alfred sloan fellowship education research grant nvidia donations faculty award bloomberg data science research grant visualize -way classiﬁcation confusion matrix best performing method i.e. voting ensemble pairwise skip-thought+image pairwise order fig. block-diagonal matrix structure shows model predicts ﬁrst last element story reasonably well often confused elements middle story. visualization suggests model learnt three structure stories i.e. setup middle climax. present qualitative examples story orders predicted best performing model fig. fig. shows example stories position elements predicted correctly. fig. shows stories none positions predicted correctly model. examples show model clearly fails inherent temporal order story either language images. word cloud fig. visualize words model ﬁnds discriminative correct predictions. words correctly predicted stories model believes indicative sentence positions story. size word proportional ratio frequency occurring position positions. model captures events ‘carnival’ ‘reunion’ sports topics like ‘baseball’ ‘soccer’ ‘skate’ ﬁrst position. could case ﬁrst sentence story usually introduces event story based fig. also observe model correctly learns cue-words ‘overall’ ‘lastly’. also learns words events frequently conclude stories ‘returned’ ‘tired’ ‘winning’ ‘winner’ ‘celebration’. figure confusion matrix predictions best performing model voting ensemble pairwise skip-thought+image pairwise order neural position embedding", "year": 2016}