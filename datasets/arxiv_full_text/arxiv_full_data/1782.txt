{"title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor  Processes", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.", "text": "dirichlet process extension pitman-yor process stochastic processes take probability distributions parameter. processes stacked form hierarchical nonparametric bayesian model. article present eﬃcient methods processes hierarchical context apply latent variable models text analytics. particular propose general framework designing bayesian models called topic models computer science community. propose speciﬁc nonparametric bayesian topic model modelling text social media. focus tweets article ease access. nonparametric model performs better existing parametric models goodness real world applications. live information age. internet information obtained easily almost instantly. changed dynamic information acquisition example attain knowledge visiting digital libraries aware world reading news online seek opinions social media engage political debates forums. technology advances information created point infeasible person digest available content. illustrate context healthcare database number entries seen growth rate approximately entries ten-year period motivates machines automatically organise ﬁlter summarise analyse available data users. researchers developed various methods broadly categorised computer vision speech recognition text analytics researchers seek accomplish various goals including sentiment analysis opinion mining information retrieval text summarisation topic modelling illustrate sentiment analysis used extract digestible summaries reviews products services valuable consumers. hand topic models attempt discover abstract topics present collection text documents. topic models inspired latent semantic indexing probabilistic variant probabilistic latent semantic indexing also known probabilistic latent semantic analysis pioneered blei latent dirichlet allocation fully bayesian extension plsi considered simplest bayesian topic model. extended many diﬀerent types topic models. designed speciﬁc applications model structure text incorporate extra information modelling hand well known correspondence gamma-poisson family distributions dirichlet-multinomial family gamma-poisson factor models nonparametric extensions poisson-based variants non-negative matrix factorisation form methodological continuum topic models. methods often applied text however consider methods here. article concentrate topic models take account additional information. information auxiliary data accompany text keywords dates authors sources; external resources like word lexicons. example twitter popular social media platform messages known tweets often associated several metadata like location time published user written tweet. information often utilised instance kinsella model tweets location data wang hashtags sentiment classiﬁcation tweets. hand many topic models designed perform bibliographic analysis using auxiliary information. notable author-topic model which name suggests incorporates authorship information. addition authorship citation author topic model author cite topic model make citations model research publications. also topic models employ external resources improve modelling. instance buntine incorporate sentiment lexicon prior information weakly supervised sentiment analysis. independent auxiliary data recent advances nonparametric bayesian methods produced topic models utilise nonparametric bayesian priors. simplest examples replace dirichlet distributions dirichlet process simplest hierarchical dirichlet process proposed replaces document topic matrix lda. extend topic models using pitman-yor process generalises replacing second dirichlet distribution generates topic word matrix lda. includes work sato nakagawa lindsey among others. like pyps stacked form hierarchical pitman-yor processes used complex models. another fully nonparametric extension topic modelling uses indian buﬀet process sparsify document topic matrix topic word matrix lda. advantages employing nonparametric bayesian methods topic models ability estimate topic word priors infer number clusters data. using also allows modelling power-law property exhibited natural languages touted advantages shown yield signiﬁcant improvements performance however note best known approach learning hierarchical dirichlet processes chinese restaurant franchise requires dynamic memory allocation implement hierarchy extensive research attempting eﬃciently implement hdp-lda extension mostly based around variational methods variational methods rarely applied complex topic models consider here unfortunately bayesian nonparametric methods gaining reputation diﬃcult use. newer collapsed blocked gibbs sampler shown generally outperform variational methods well original chinese restaurant franchise computational time space standard performance metrics moreover technique appear suitable complex topic models consider here. article extending algorithm chen shows develop fully nonparametric relatively eﬃcient bayesian topic models incorporate auxiliary information goal produce accurate models work well tackling several applications. by-product wish encourage state-of-the-art bayesian techniques also incorporate auxiliary information modelling. remainder article follows. ﬁrst provide brief background pitman-yor process section then section detail modelling framework illustrating simple topic model. continue inference procedure topic model section finally section present application modelling social network data utilising proposed framework. section concludes. provide brief informal review pitman-yor process section. assume readers familiar basic probability distributions dirichlet process addition refer readers hjort tutorial bayesian nonparametric modelling. pitman-yor process also known twoparameter poisson-dirichlet process. two-parameter generalisation extra parameter named discount parameter addition concentration parameter similar sample corresponds discrete distribution support base distribution underlying distribution poisson-dirichlet distribution introduced pitman construction equation named stick-breaking process. also constructed using analogue chinese restaurant process extensive review given buntine hutter speciﬁcation also named compound poisson-dirichlet process doubly hierarchical pitman-yor process wood special case equivalent also known mixed random measures note assumed constant values though course fully bayesian assign prior distribution them natural prior would dirichlet distribution. performing exact bayesian inference nonparametric models often intractable diﬃculty deriving closed-form posterior distributions. motivates markov chain monte carlo methods approximate inference. notable mcmc methods metropolis-hastings algorithms gibbs samplers algorithms serve building block advanced samplers algorithms delayed rejection generalisations mcmc method include reversible jump mcmc delayed rejection variant also employed bayesian inference however scope article. instead sampling parameter time develop algorithm updates parameters iteration so-called blocked gibbs sampler also practice usually interested certain subset parameters; cases sometimes derive eﬃcient collapsed gibbs samplers integrating nuisance parameters. remainder article employ combination blocked collapsed gibbs samplers bayesian inference. section discuss basic design nonparametric bayesian topic models using thierarchical pitman-yor processes particular introduce simple topic model extended later. discuss general inference algorithm topic model hyperparameter optimisation. development topic models fundamentally motivated applications. depending application speciﬁc topic model suitable task designed used. however despite ease designing model majority time spent implementing assessing redesigning calls better designing cycle/routine eﬃcient spending less time implementation time model design development. extension allowing probability vectors modelled pyps instead dirichlet distributions. area left graphical model usually referred topic side right hand side called vocabulary side. word node denoted observed. notations deﬁned table achieve higher level implementation algorithms topic modelling. made possible statistical domains bugs jags albeit standard probability distributions. theoretically bugs jags work lda; however practice running gibbs sampling bugs jags slow. gibbs samplers uncollapsed optimised. furthermore cannot used model stochastic processes like gaussian process below present framework allows implement hpyp topic models eﬃciently. framework allows test variants proposed topic models without signiﬁcant reimplementation. hpyp topic model simple network nodes since distributions probability vectors modelled pyp. simplicity assume topic model three layers although practice limit number layers. present graphical model generic topic model figure model variant presented buntine mishra presented starting model illustrating methods subsequent extensions. root level distributed pyps variable root node topics topic model root node words. allow arbitrary number topics learned base distribution continuous distribution discrete distribution inﬁnite samples. usually choose discrete uniform distribution based word vocabulary size text corpus. decision technical nature able assign tiny probability words observed training eases evaluation process. vocabulary side topic learned model topic– word distribution tells words associated topic. topic– word distribution distributed given parent node follows bayesian setting posterior inference requires analyse posterior distribution model variables given observed data. instance joint posterior distribution hpyp topic model here bold face capital letters represent relevant variables. instance captures words corpus. additionally denote hyperparameters constants model. note deriving posterior distribution analytically almost impossible complex nature. leaves approximate bayesian inference techniques mentioned section however even techniques performing posterior inference eﬃcient inference procedure pyps marginalise pyps model record various associated counts instead yields collapsed sampler. achieve this adopt chinese restaurant process metaphor represent variables topic model. metaphor data model customers; nodes restaurants customers visit. restaurant customer seated table though table number customers. table restaurant serves dish dish corresponds categorical label data point figure illustration chinese restaurant process representation. customers represented circles tables represented rectangles. dishes symbols middle rectangles denoted sunny symbol cloudy symbol. illustration know number customers corresponds table example green table occupied three customers. also since restaurant parent restaurant tables restaurant treated customers restaurant share dish. means data passed recursively root node. illustration present simple example figure showing seating arrangement customers restaurants. na¨ıvely recording seating arrangement customer brings computational ineﬃciency inference. instead adopt table multiplicity representation chen requires dynamic memory thus consuming factor memory loss inference eﬃciency. representation store customer counts table counts associated denotes number customers restaurant. customer count dish restaurant corresponding symbol without subscript denotes collection customer counts restaurant total number customers restaurant denoted capitalised symbol instead figure illustration chinese restaurant table counts representation. setting figure seating arrangement customers forgotten table customer counts recorded. thus know three sunny tables restaurant total nine customers. similarly. instance example figure corresponding illustration table multiplicity representation presented figure refer readers chen detailed derivation posterior likelihood restaurant. posterior likelihood hpyp topic model marginalise probability vector associated pyps represent customer counts table counts following chen present modularised version full posterior hpyp topic model allows posterior computed quickly. full posterior consists modularised likelihood associated model deﬁned function true otherwise. would like point even though probability vectors pyps integrated explicitly stored easily reconstructed. discussed section move bayesian inference next section. focus mcmc method bayesian inference hpyp topic model. mcmc method topic models follows simple procedures decrementing counts contributed word sample topic word update model accepting rejecting proposed sample. here describe collapsed blocked gibbs sampler hpyp topic model. note pyps marginalised deal counts. ﬁrst step gibbs sampler remove word corresponding latent topic decrement associated customer counts table counts. give example figure remove customer restaurant would decrement customer count customer customer table. turn decrements customer count however requires keep track customers’ seating arrangement leads increased memory requirements poorer performance inadequate mixing overcome issue follow concept table indicator indicates whether removing customer also removes table customer seated. note bernoulli indicator diﬀerent chen indicates restaurant customer contributes bernoulli indicator sampled needed decrementing procedure stored means simply forget seating arrangements re-sample later needed thus need store context hpyp topic model described section formally present decrement counts associated word latent topic document position first vocabulary side decrement φzdn φzdn also according equation customer count decrement decrement respective customer count bernoulli indicator second would need decrement counts associated latent topic procedure similar decrement sample bernoulli indicator note whenever decrement customer count sample corresponding bernoulli indicator. repeat procedure recursively bernoulli indicator procedure hits root node. decrementing variables associated word blocked gibbs sampler sample topic word corresponding customer counts table counts. conditional posterior used sampling computed quickly full posterior represented modularised form. illustrate conditional posterior associated customer counts table counts conditional posterior deﬁned proceed sampling process. ﬁrst step involves ﬁnding possible changes topic customer counts table counts associated adding removed word back topic model. since word added model customer counts table counts increase constraining possible states reasonably small number. furthermore customer counts parent node incremented table counts child node increases. note possible added customer generate dish model. requires customer increment table count dish root node next compute conditional posterior possible states. conditional posterior computed quickly breaking posterior calculating relevant parts. normalise sample states proposed next state. note proposed state always accepted artifact gibbs sampler. choosing right hyperparameters priors important topic models. wallach show optimised hyperparameter increases robustness topic models improves model ﬁtting. hyperparameters hpyp topic models discount parameters concentration parameters pyps. here propose procedure optimise concentration parameters leave discount parameters ﬁxed coupling stirling numbers cache. shape parameter rate parameter. gamma prior chosen conjugacy gives gamma posterior optimise ﬁrst sample auxiliary variables given current value recall topic modelling analyse posterior model parameters equation although marginalised pyps gibbs sampler pyps reconstructed associated customer counts table counts. recovering full posterior distribution pyps complicated task. instead analyse pyps expected value conditional marginal posterior distribution simply posterior mean posterior mean corresponds probability sampling customer pyp. illustrate consider posterior topic distribution ˜zdn unknown future latent topic addition known this write posterior mean initialise hpyp topic model assigning random topic latent topic associated word update relevant customer counts table counts using equation setting table counts half customer counts. note dimension topic distributions number observed topics. accounts generation topic associated customer though probability generating topic usually much smaller. practice instead ignore extra dimension evaluation topic model since provide useful interpretation. simply discard extra dimension probability vectors computing posterior mean. another approach would normalise posterior mean root node discarding extra dimension computing posterior mean others pyps. note considerably large corpus diﬀerence approaches would small notice. generally ways evaluate topic model. ﬁrst evaluate topic model based task performs instance ability make predictions. second approach statistical evaluation topic model modelling data also known goodness-of-ﬁt test. section present commonly used evaluation metrics applicable topic models ﬁrst discuss procedure estimating variables associated test set. test documents used evaluations aside learning documents. such document–topic distributions associated test documents unknown hence need estimated. estimate posterior mean given variables learned gibbs sampler obtainable applying equation note since latent topics corresponding test sampled customer counts table counts associated thus equal posterior mean however good estimate topic distribution test documents since identical test documents. overcome issue instead words test documents obtain better estimate method known document completion part text estimate rest evaluation. getting better estimate requires ﬁrst sample latent topics ˜zdn test documents. proper running algorithm akin collapsed gibbs sampler would excruciatingly slow need re-sample customer counts table counts parent pyps. instead assume variables learned gibbs sampler ﬁxed sample ˜zdn conditional posterior sequentially given previous latent topics whenever latent topic ˜zdn sampled increment customer count document. simplicity table count counts avoids expensive operation sampling table counts. additionally re-estimated using equation sampling next latent topic. note estimated variables unbiased. monte carlo estimate used computing evaluation metrics. note estimating ignored possibility generating topic latent topics constrained existing topics previously discussed section measures goodness-of-ﬁt usually involves computing discrepancy observed values predicted values model. however observed variables topic model words corpus quantiﬁable since discrete labels. thus evaluations topic models usually based model likelihoods instead. popular metric commonly used evaluate goodness-of-ﬁt topic model perplexity negatively related likelihood observed words given model deﬁned likelihood sampling word given document–topic distribution topic–word distributions computing requires marginalise joint distribution follows although perplexity computed whole corpus practice compute perplexity test documents. measure topic model generalises well unseen data. good topic model would able predict words test also evaluate clustering ability topic models. note topic models assign topic word document essentially performing soft clustering documents membership given document–topic distribution evaluate clustering documents convert soft clustering hard clustering choosing topic best represents documents hereafter called dominant topic. dominant topic document corresponds topic highest proportion topic distribution commonly used evaluation measures clustering purity normalised mutual information purity simple clustering measure interpreted proportion documents correctly clustered information theoretic measures used clustering comparison. denote ground section looks employ framework discussed application tweet modelling using auxiliary information available twitter. propose twitter-network topic model jointly model text social network fully bayesian nonparametric particular incorporating authors hashtags follower network text content modelling. tntm employs hpyp text modelling gaussian process random function model social network modelling. show tntm signiﬁcantly outperforms several existing nonparametric models ﬂexibility. emergence services blogs microblogs social networking websites allows people contribute information freely publicly. user-generated information generally personal informal often contains personal opinions. aggregate useful reputation analysis entities products natural disaster detection obtaining ﬁrst-hand news even demographic analysis focus twitter accessible source information allows users freely voice opinions thoughts short text known tweets. although popular model text modelling direct application tweets often yields poor result tweets short often noisy tweets unstructured often contain grammatical spelling errors well informal words user-deﬁned abbreviations characters limit. fails short tweets since heavily dependent word co-occurrence. also notable text tweets contain special tokens known hashtags; used keywords allow users link tweets tweets tagged hashtag. nevertheless hashtags informal since standards. hashtags used inline words categorical labels. used labels hashtags often noisy since users create hashtags easily existing hashtags like. hence instead hard labels hashtags best treated special words themes tweets. properties tweets make challenging topic models alternatives used instead. instance maynard advocate shallow method tweets mehrotra utilise tweet-pooling approach group short tweets larger document. text analysis applications tweets often ‘cleansed’ methods lexical normalisation however normalisation also criticised normalisation change meaning text. following propose novel method better modelling microblogs leveraging auxiliary information accompanies tweets. information complementing word co-occurrence also opens door applications user recommendation hashtag suggestion. major contributions include fully bayesian nonparametric model named twitter-network topic model models tweets well combination hpyp jointly model text hashtags authors followers network. despite seeming complexity tntm model implementation made relatively straightforward using ﬂexible framework developed section indeed number variants rapidly implemented framework well. tntm makes accompanying hashtags authors followers network model tweets better. tntm composed main components hpyp topic model text hashtags based random function network model followers network. authorship information serves connect together. example hashtag hijacking well deﬁned hashtag used inappropriate way. notable example would hashtag mcdstories though initially created promote happy stories mcdonald’s hashtag hijacked negative stories mcdonald’s. hpyp topic model based random function network model author–topic distributions serve link together. attuned whole tweet hashtags words order. topic assignments hashtags note tokens hashtags shared words hashtag happy shares token word happy thus treated word. treatment important since hashtags used words instead labels. additionally also allows words hashtags useful hashtag recommendation. topic distributions generate global topic distribution serves prior distribution. generate author–topic distribution next generate topic distributions observed hashtags observed words following technique used adaptive topic model explicitly model inﬂuence hashtags words generating words conditioned hashtags. intuition comes hashtags themes tweet drive content tweet. speciﬁcally sample mixing proportions control contribution base distribution given number observed words tweet note hyperparameters model. show importance modelling ablation studies section although hpyp topic model seem complex simple network nodes since distributions probability vectors modelled pyp. tntm related many existing models removing certain components model. hashtags network components removed tntm reduced nonparametric variant author topic model oppositely authorship information discarded tntm resembles correspondence although diﬀers allows hashtags words generated common vocabulary. contrast existing parametric models network model tntm provides possibly ﬂexible network modelling nonparametric bayesian prior following lloyd diﬀerent lloyd propose kernel function purpose better achieves signiﬁcant improvement original kernel. previous sections represent tntm using representation discussed section however since variables tntm multiple parents extend representation following distinction store topic assignments words hashtags; table counts customer counts; introduce hyperparameters. marginalising latent variables write model likelihood corresponding hpyp topic model terms counts tntm combining hpyp makes posterior inference non-trivial. hence employ approximate inference alternatively performing mcmc sampling hpyp topic model network model conditioned other. hpyp topic model employ ﬂexible framework discussed section perform collapsed blocked gibbs sampling. network model derive metropolis-hastings algorithm based elliptical slice sampler addition author– topic distributions connecting hpyp sampled scheme since posteriors follow standard form. note pyps section multiple parents extend framework section allow this. collapsed gibbs sampling hpyp topic model tntm similar procedure section although main diﬀerences. ﬁrst diﬀerence need sample topics words hashtags diﬀerent conditional posterior compared section second pyps tntm multiple parents thus alternative decrementing counts required. detailed discussion performing posterior inference hyperparameter sampling presented appendix. evaluation tntm construct tweet corpus twitter dataset corpus queried using hashtags sport music ﬁnance politics science tech chosen diversity. remove non-english tweets langid.py obtain data followers network kwak however note followers network data complete contain information authors. thus ﬁlter authors part followers network data tweet corpus. additionally also remove authors written less ﬁfty tweets corpus. name corpus since queried hashtags. consists tweets authors ﬁltering. besides corpus also tweet datasets described mehrotra datasets contains three corpora queried exactly query terms. ﬁrst corpus named generic dataset queried generic terms. second named speciﬁc dataset composed tweets speciﬁc named entities. lastly events dataset associated certain events. datasets mainly used comparing performance tntm tweet pooling techniques mehrotra present summary tweet corpora table consider several tasks evaluate tntm. ﬁrst task involves comparing tntm existing baselines performing topic modelling tweets. also compare tntm random function network model modelling followers network. next evaluate tntm ablation studies perform comparison hashtags tweet. dataset queried diﬀerent hashtags thus higher number hashtags tweet. note typo number tweets events dataset mehrotra correct number induce power-law behaviour initialise concentration parameters noting learned automatically inference hyperprior gamma vague prior. hyperparameters values signiﬁcant impact model performance. following evaluations full inference algorithm iterations models converge. note algorithm starts iterations. repeat experiment times reduce estimation error evaluations. compare tntm hdp-lda nonparametric author-topic model ﬁtting text data performances measured using perplexity test perplexity tntm accounting words hashtags table test perplexity network likelihood comparisons hdp-lda nonparametric random function network model tntm. lower perplexity indicates better model ﬁtting. tntm signiﬁcantly outperforms models term model ﬁtting. table ablation test tntm. test perplexity network likelihood evaluated tntm several ablated variants tntm. result shows component tntm important. comparison perplexity network likelihood table note network likelihood less negative better. result tntm achieves much lower perplexity compared hdp-lda nonparametric atm. also nonparametric signiﬁcantly better hdp-lda. clearly shows using auxiliary information gives better model ﬁtting. additionally also jointly modelling text network data leads better modelling followers network. next perform extensive ablation study tntm. components tested study authorship hashtags connection power-law behaviour pyps. compare full tntm variations component ablated. table presents test perplexity network likelihood models shows signiﬁcant improvements tntm ablated models. this greatest improvement table clustering evaluations tntm diﬀerent pooling schemes. note higher purity indicate better performance. results diﬀerent pooling methods obtained table mehrotra tntm achieves better performance purity datasets except speciﬁc dataset obtains purity score best pooling method. perplexity modelling hashtags suggests hashtag information important modelling tweets. second hashtags authorship information important well. even though modelling power-law behaviour important perplexity improvement network likelihood best achieved modelling power-law. ﬂexibility enables learn author–topic distributions better thus allowing tntm network data better. also suggests authors corpus tend focus speciﬁc topic rather wide interest. mehrotra shows running pooled tweets rather unpooled tweets gives signiﬁcant improvement clustering. particular grouping tweets based hashtags provides improvement. here show instead resorting method tntm achieve signiﬁcantly better result clustering. clustering evaluations measured purity normalised mutual information described since ground truth labels unknown respective query terms ground truth evaluations. note tweets satisfy multiple labels removed. given learned model assign tweet cluster based dominant topic. perform evaluations generic speciﬁc events datasets comparison purpose. note lack network information datasets thus employ hpyp part tntm. additionally since purity trivially improved increasing number clusters limit maximum number topics twenty fair comparison. present results table tntm outperforms pooling method aspects except speciﬁc dataset achieves purity best pooling scheme. table topical analysis dataset tntm displays three hashtags words topics. instead manually assigning topic label topics hashtags serve topic labels. traditionally researchers assign topic topic–word distribution manually inspection. recently attempts label topics automatically topic modelling. instance wikipedia extract labels topics mehdad entailment relations select relevant phrases topics. here show hashtags obtain good topic labels. table display words topic–word distribution topic instead manually assigning topic labels display three hashtags topic–hashtag distribution table hashtags appear suitable topic labels. fact empirically evaluating suitability hashtags representing topics consistently that hashtags good candidates topic labels. moreover inspecting topics show major hashtags coincide query terms used constructing dataset expected. veriﬁes tntm working properly. article proposed topic modelling framework utilising pyps realisation probability distribution another stochastic process type. particular purpose performing inference described representation pyps. allows propose single framework discussed section implement topic models modularise pyps presented general hpyp topic model seen generalisation hdp-lda hpyp topic model represented using chinese restaurant process metaphor discussed posterior likelihood hpyp topic model modularised. detailed learning algorithm topic model modularised form. applied hpyp topic model framework twitter data proposed twitternetwork topic model tntm models authors text hashtags authors-follower network integrated manner. addition hpyp tntm employs gaussian process network modelling. main suggested tntm content discovery social networks. experiments show jointly modelling text content network leads better model ﬁtting compared modelling separately. results qualitative analysis show learned topics authors’ topics sound. experiments suggest incorporating auxiliary information leads better ﬁtting models. future work tntm would interesting apply tntm types data blogs news feeds. could also tntm applications. hashtag recommendation content suggestion twitter users. moreover could extend tntm incorporate auxiliary information instance model location tweets embedded multimedia contents images videos. another interesting source information would path retweeted content. another interesting area research combination diﬀerent kinds topic models better analysis. allows transfer learned knowledge topic model another. work combining already looked schnober gurevych however combining kinds topic models nonparametric ones unexplored. authors like thank shamin kinathil editors anonymous reviewers valuable feedback comments. nicta funded australian government department communications australian research council centre excellence program. decrementing customer count decrement respective table count. however table count decremented would decrement customer count parent pyp. relatively straight continue process recursively. present decrementing process algorithm remove word inference would need decrement counts contributed topic side decrement counts associated node group using decrementing sample topic word hashtag. sampling process follows procedure discussed section diﬀerent conditional posteriors full conditional posterior probability collapsed blocked gibbs sampling derived easily. instance conditional posterior sampling topic word formulations topic distributions word distributions tntm reconstructed customer counts table counts. instance author–topic distribution author determined recursively ﬁrst estimating topic distribution word distributions topic similarly estimated. here discuss learn topic distributions random function network model. conﬁgure algorithm start running thousand iterations collapsed blocked gibbs sampler quickly initialise tntm hpyp topic model running full algorithm. addition allows demonstrate improvement tntm random function network model. facilitate algorithm represent topic distributions explicitly probability vectors store customer counts table counts starting algorithm. algorithm propose samples accept reject samples. details algorithm follow. proposed sampled given previous values note ﬁrst computed using technique discussed proposed samples subsequently used sample qnew. ﬁrst compute quantities κnew using proposed µnew νnew equation equation sample qnew given κnew using elliptical slice sampler resent probability vectors explicitly. note treat acceptance probability expression equation evaluates accept proposed samples probability sample accepted keep respective values. completes iteration scheme. summarise algorithm algorithm sample hyperparameters using auxiliary variable sampler leaving ﬁxed. note auxiliary variable sampler pyps multiple parents identical pyps single parent since sampler used total customer would like point hyperparameter sampling performed pyps tntm ﬁrst thousand iterations. that represented probability vectors explicitly sample hyperparameters pyps note sampling concentration parameters allows topic distributions author vary authors speciﬁc topics authors wider range topics. simplicity kernel hyperparameters additionally also make priors mixing proportions uninformative setting summarise full inference algorithm tntm algorithm", "year": 2016}