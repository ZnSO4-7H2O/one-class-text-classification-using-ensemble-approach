{"title": "Word Representations, Tree Models and Syntactic Functions", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Word representations induced from models with discrete latent variables (e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification. We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident.", "text": "word representations induced models discrete latent variables shown beneﬁcial many applications. work exploit labeled syntactic dependency trees formalize induction problem unsupervised learning tree-structured hidden markov models. syntactic functions used additional observed variables model inﬂuencing transition emission components. syntactic information potentially lead capturing ﬁne-grain functional distinctions words which turn desirable many applications. evaluate word representations tasks named entity recognition semantic frame identiﬁcation. observe improvements exploiting syntactic function information cases results rivaling state-of-the-art representation learning methods. additionally revisit relationship sequential unlabeled-tree models advantage latter self-evident. word representations proven indispensable source features many systems allow better generalization unseen lexical cases roughly speaking word representations allow capture semantically otherwise similar lexical items categorically vectorial although methods obtaining word representations diverse normally share well-known distributional hypothesis according similarity established based occurrence similar contexts. however word representation methods frequently differ operationalize deﬁnition context. recently shown representations using syntactic contexts superior learned linear sequences downstream tasks named entity recognition dependency parsing pp-attachment disambiguation also shown perform well datasets intrinsic evaluation capture different type semantic similarity sequence-based representations unlike recent research word representation learning focused heavily word embeddings neural network tradition work falls framework hidden markov models drawing work grave huang attractive property hmms ability provide context-sensitive representations word different sentential contexts given distinct representations. account various senses word. however ability requires inference expensive compared simple look-up explore experiments word representations originally obtained context-sensitive available look-up static representations. method includes types observed variables words syntactic functions. allows address drawback learning word representation unlabeled dependency trees context hmms motivation including syntactic functions comes intuition proxies semantic roles. current research practice either discard type information include preprocessing step i.e. attaching syntactic labels words levy goldberg evaluate word representations structured prediction tasks named entity recognition semantic frame identiﬁcation. extension builds upon sequential unlabeledtree hmms also revisit basic difference unable entirely corroborate alleged advantage syntactic context word representations task. word typically occur distinct syntactic functions. since account words different semantic roles incorporation syntactic function word parent could give precise representations. example carla bought computer subject object represent different semantic roles namely buyer goods respectively. along similar lines pad´o lapata ˇsuster noord grave argue inaccurate treat context words equal contributors word’s meaning. learning parameters obtained training unlabeled syntactic structure encode probabilistic relationship hidden states parent child hidden state word. tree structure thus deﬁnes word’s context oblivious relationship words. example grave acknowledge precisely limitation unlabeled-tree representations providing example hidden state verb cannot discriminate left right neighbors shared transition parameters. adversely affects accuracy super-sense tagger english. similarly ˇsuster noord show ﬁltering dependency instances based syntactic functions positively affect quality obtained brown word clusters measured wordnet similarity task. represent sentence tuple words |v|} integer representing word vocabulary goal infer tuple states integer representing semantic class number states needs prior training. another possibility wk’s representation probability distribution states. case denote wk’s representation usual markovian models generation sentence decomposed generation classes generation words process deﬁned tree node generated single parent representing root tree denote syntactic function total number syntactic function types produced syntactic parser. encode syntactic function position rwk→wπ i.e. dependency relation parent. would like variable modulate transition emission processes. achieve drawing input-output architecture bengio frasconi introduce sequential model additional sequence train model expectationmaximization algorithm sum-product message passing inference trees inference procedure unlabeled-tree model except performed conditionally parameters estimated maximum likelihood estimation. e-phase obtain pseudo-counts existing parameters shown m-step normalizes transition pseudo-counts explore idea introducing complexity gradually order alleviate problem ﬁnding poor solution particularly severe search space large splitting procedure starts small number states splits parameters state cloning slightly perturbing. model retrained split round takes place. allow splitting states various degrees petrov also merge back split states improve likelihood least. although merge step done approximately require cycles inference extra running time justify spoobservations called input becomes part model model used conditional predictor. authors describe application model speech processing goal obtain accurate predictor output phoneme layer input acoustic layer. focus contrast representation learning rather prediction also adapt sequential topology trees. model satisﬁes single-parent constraint applied proper trees only. principle possible extend base representation model using approximate inference techniques work graphs explore possibility here. opposed unlabeled-tree extension fact categorized inhomogeneous model since transition emission probability distributions change function input bengio another comparison concerns learning long-term dependencies since input-output architecture transition probabilities change function input deterministic transition probabilities hmm. transition parameters closer zero reduces ambiguity next state allows context easily. concrete graphical example given tained either post-token posttype. visualize representations apply multidimensional scaling. model clearly separates management positions parts body interestingly puts head closer management positions explained business economic nature bllip corpus. words chief executive located together isolated others possibly strong tendency precede nouns. arrow plot indicates shift meaning post-token representation obtained head within sentence. decoding hmm-based models model trained search probable states given observed data using max-product message passing efﬁcient decoding trees maxc also tried posterior decoding without consistent improvements. search best states avoided taking posterior state distribution hidden states call vectorial representation post-token. cases inference performed concrete sentence thus providing context-sensitive representation. experiments post-token consistently outperforms maxproduct ability carry information uncertainty. exploited downstream task predictor. figure representations obtained model syntactic functions. static post-type representations except head obtained post-token concrete sentence. despite advantage post-token account word senses observe post-type performs better almost experiments. likely explanation averaging increases generalizability representations. concrete tasks apply word representations increased robustness simply important context sensitivity. also post-type might less sensitive parsing errors test time. disadvantage obtaining context-sensitive representations relatively costly inference. inference decoding also sometimes applicable information retrieval entire sentence usually given trade-off full context sensitivity efﬁciency achieved considering static representation obtained context-insensitive averaging posterior state distributions occurrences word type large corpus parameters setup observe faster convergence times online updates parameters frequently. speciﬁcally mini-batch step-wise determine hyper-parameters held-out dataset sentences maximize log-likelihood. higher values step-wise reduction power mini-batch size lead better overall loglikelihood somewhat negative effect convergence speed. ﬁnally settle mini-batch size sentences. couple iterations entire dataset sufﬁcient obtain good parameters klein initialization. since algorithm setting ﬁnds local optimum loglikelihood initialization model parameters major impact ﬁnal outcome. initialize emission matrices brown clusters ﬁrst assigning random values matrix elements multiplying elements represent words cluster factor finally normalize matrices. technique incorporates strong bias towards word-class emissions exist brown clusters. transition parameters simply random numbers sampled uniform distribution ﬁnally normalized. approximate inference. following grave approximate belief vectors inference speeds learning works regularization. k-best projection method k-largest coefﬁcients texts remove sentences whose length dependency parser english build projective second order model trained sections penn treebank prior that patched bracketing rules converted dependencies parser achieves unlabeled/labeled accuracy section without retagging pos. postagging bllip corpus evaluation datasets citar tagger parsing replace words occurring less times special symbol model words. results vocabulary size words. dutch. ﬁrst produce random sample sentences sonar corpus follow preprocessing steps english. parse corpus alpino hpsg parser maxent disambiguation component. contrast english word forms keep root forms produced parser’s lexical analyzer. resulting vocabulary size words. analyses produced parser represent multiple parents facilitate treatment wh-clauses coordination passivization. since method expects proper trees convert parser output conll format. evaluation tasks named entity recognition. standard conll- shared task dataset dutch conll- dataset english. also include out-of-domain muc- testset preprocessed according turian refer reader ratinov roth detailed description classiﬁcation problem. information is-alphanumeric all-digits all-capitalized is-capitalized is-hyphenated; preﬁxes sufﬁxes word window wk±; capitalization pattern word window. construct real-valued features word vector dimensionality simple indicator feature categorical word representation. http//github.com/danieldk/citar http//lands.let.ru.nl/projects/sonar http//www.let.rug.nl/bplank/alpinoconll/ http//github.com/lxmls/lxmls-toolkit semantic frame identiﬁcation. frame-semantic parsing task identifying semantic frames predicates sentence frame arguments participating events compared classiﬁcation decisions apply relatively small words problem semantic frame identiﬁcation concerns making predictions broader words semafor parser consisting log-linear components trained gradient-based techniques. parser trained tested framenet full-text annotations. test consists documents hermann investigate effect word representation features frame identiﬁcation component. measure semafor’s performance gold-standard targets report accuracy exact matches well partial matches. latter give partial credit identiﬁed related frames. modify publicly available implementation http//github.com/sammthomson/semafor. baseline features target include preparing word representations brown clusters. brown clusters known effective robust compared example word embeddings method seen special case word emissions deterministic i.e. word belongs semantic class. recently extension proposed basis dependency language model publicly available implementations. following work english coarseﬁne-grained clusters features using preﬁxes length addition complete binary tree path. dutch coarsergrained clusters yield improvement. brown features included window around target word word features. adding cluster features frame-semantic parser transform cluster identiﬁers onehot vectors gives small improvement indicator features. hmm-based models. n-dimensional representations obtained hmms variants included distinct continuous features. task word representations included dutch english determined development set. investigate state space sizes ﬁnally choose reasonable tradetraining time quality. dimensionality word representation models paper. observe constraining synfunchmm frequent syntactic functions treat remaining ones single special syntactic function obtain better results evaluation tasks. model syntactic functions produced parser less learning evidence infrequent syntactic functions. explore effect keeping frequent syntactic functions ignoring functional ones table results english dutch test sets. best result column bold. score increase reported parentheses comparison tree-hmm. f-type f-score measured word type f-unlab f-score measured word type ignoring labels. experiments representations models obtained three different decoding methods since post-type performing best overall report results method evaluation. word embeddings. skip-gram model presented mikolov trained negative sampling training seeks maximize product word-context pairs encountered training corpus minimize product pairs context word randomly sampled. number negative examples size context window downsampling threshold number iterations while exploring constraint number syntactic functions post-token outperforms posttype sets syntactic functions ﬁnal best-performing selection. baseline sequential achieving highest f-score. synfunc-hmm performs skip-gram. outperforms unlabeled-tree model indicating added observations useful correctly incorporated. brown clusters exceed baseline score. testing signiﬁcance bootstrap method improves signiﬁcantly macro-f baseline skipgram synfunc-hmm show signiﬁcant improvements location entity type. general trend dutch somewhat different. notably word representations contribute much effectively overall classiﬁcation performance compared english. best-scoring model synfunc-hmm improves baseline signiﬁcantly much points. part reason synfunchmm works well case make informative syntactic function between parts multiword unit. similarly english unlabeled-tree performs slightly worse sequential hmm. cluster features valuable english also observe .-point advantage using dependency brown clusters standard bigram brown clusters. skip-gram model perform well english might indicate hyper-parameters would need ﬁne-tuning speciﬁc dutch. out-of-domain dataset tree-based representations appear perform poorly whereas highest score achieved skip-gram method. unfortunately difﬁcult generalize results alone. concretely dataset contains named entities skip-gram method makes correct predictions tree-hmm. however dataset covers narrow topic missilelaunch scenarios system gets badly penalized mistake made repeatedly certain named entity. example entity nasa occurs times wrongly classiﬁed tree-hmm system correctly skipgram. overall performance therefore hinge limited number frequently occurring entities. workaround evaluate entity type calculate f-score entity average entity types. results evaluation scenario reported f-type. skip-gram still performs best difference models smaller. finally also report f-unlab calculated f-type ignoring actual entity label. named-entity token recognized such count correct prediction ignoring entity label type similarly done ratinov roth since synfunc-hmm performs better here conclude effective identifying entities rather labeling them. fact observe different tendencies english dutch attributed interplay factors language differences differently-performing syntactic parsers differences speciﬁc evaluation datasets. brieﬂy discuss ﬁrst possibility. clear table syntax-based models generally beneﬁt dutch english. hypothesize since word order dutch generally less ﬁxed english sequence-based model dutch cannot capture selectional preferences successfully i.e. interchanging semantically diverse words small word window. makes difference performance between sequential tree models apparent dutch. results shown table best score obtained skip-gram embeddings however difference models outperforming baseline small. example skipgram correctly identiﬁes cases dep-brown correctly disam. discussion conclude experiments unlabeled syntactic trees general provide better structure deﬁning contexts compared plain sequences. exception case dependency brown clustering dutch. comparing results grave therefore cannot conﬁrm advantage using unlabeled-tree representations. semantic frame identiﬁcation however unlabeled-tree representations compare favorably sequential representations. extension syntactic functions outperforms baseline hmm-based representations practically experiments. also outperforms word representations dutch ner. advantage comes discriminating types contexts example modiﬁer subject impossible sequential unlabeled-tree architectures. results english comparable state-of-the-art representation methods. exact matches dep-brown brown significantly outperform baseline partial matches dep-brown brown skip-gram synfunchmm outperform baseline signiﬁcantly. synfunchmm performs signiﬁcantly better tree-hmm partial matches whereas difference skip-gram synfunc-hmm signiﬁcant. signiﬁcance tests using paired permutation. complex architecture proposed factorial trained using approximate variational inference applied tagging chunking. recently semantic compositionality hmm-based representations based framework distributional semantics investigated grave long tradition unsupervised training hmms tagging recent work incorporating bias favoring sparse posterior distributions within posterior regularization framework example auto-supervised reﬁnement hmms would interesting well techniques could applied word representation learning methods like ours. extension hmms dependency trees purpose word representation learning ﬁrst proposed grave although baseline methods tree-hmm conceptually follow models grave still several practical differences. source differences precise steps taken performing brown initialization state splitting also approximation belief vectors during inference. another source involves evaluation setting. classiﬁer uses single feature inclusion brown clusters make clustering hierarchy. respect experimental setting similar turian another practical difference grave concatenate words pos-tags construct input text whereas tokens word roots incorporation word representations semantic frame identiﬁcation explored hermann perform projection generic word embeddings context words low-dimensional representation also learns embedding frame label. method selects frame closest low-dimensional representation obtained mapping input embeddings. approach differs induce representations tied speciﬁc application whereas obtain linguistically enhanced word representations subsequently used variety tasks. case word representations thus included additional features loglinear model. inclusion accounts syntactic functions target context words. although hermann also syntactic functions used position general word embeddings within single input context embedding. unfortunately unable directly compare results parser implementation proprietary. accuracy baseline system test percent lower exact matching regime lower partial matching regime compared baseline implementation used. proposed extension tree syntactic functions. obtained word representations achieve better performance unlabeled-tree model. results also show simply preferring unlabeled-tree model sequential model always lead improvement. important direction future work investigate discriminating between context types lead accurate models frameworks. code obtaining hmm-based representations described paper freely available http//github.com/ rug-compling/hmm-reps. yonatan belinkov regina barzilay amir globerson. exploring compositional architectures word vector representations prepositional phrase attachment. transactions association computational linguistics emily bender. linguistic fundamentals natural language processing. synthesis lectures human language technologies. morgan claypool publishers. huang arun ahuja doug downey yang yuhong alexander yates. learning representations weakly supervised natural language processing tasks. computational linguistics j¨uri lember alexey koloydenko. bridging viterbi posterior decoding generalized risk approach hidden path inference based hidden markov models. journal machine learning research lizhen gabriela ferraro liyuan zhou weiwei nathan schneider timothy baldwin. data small data domain out-of domain known word unknown word impact word representation sequence labelling tasks. arxiv preprint arxiv.. arvind neelakantan jeevan shankar alexandre passos andrew mccallum. efﬁcient nonparametric estimation multiple embeddings word vector space. emnlp.", "year": 2015}