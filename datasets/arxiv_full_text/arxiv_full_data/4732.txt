{"title": "Self-Correcting Models for Model-Based Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "I.2.6; I.2.8"], "abstract": "When an agent cannot represent a perfectly accurate model of its environment's dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \"correct\" itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model's ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.", "text": "figure training models predict environment states environment states. bottom training models predict environment states states sampled model. racy inadequate proxy mbrl performance. instance sorg joseph pointed accurate model measure necessarily best planning. proposed optimizing model parameters control performance using policy gradient methods. though appealing directness approach arguably discards beneﬁts learning model ﬁrst place. talvitie pointed one-step prediction accuracy account model behaves composed introduced hallucinated replay meta-algorithm address this. illustrated figure approach rolls model environment parallel training model predict correct environment state even input incorrect sampled state effectively causes model self-correct rollouts. hallucinated replay shown enable meaningful planning ﬂawed models examples standard approach failed. however offers theoretical guarantees. venkatraman used similar approaches improve models’ long-range predictions though mbrl setting. paper presents novel error bounds reveal theoretical principles underlie empirical success hallucinated replay. presents negative results identify settings hallucinated training would ineffecagent cannot represent perfectly accurate model environment’s dynamics model-based reinforcement learning fail catastrophically. planning involves composing predictions model; ﬂawed predictions composed even minor errors compound render model useless planning. hallucinated replay trains model correct produces errors substantially improving mbrl ﬂawed models. paper theoretically analyzes approach illuminates settings likely effective ineffective presents novel error bound showing model’s ability self-correct tightly related mbrl performance one-step prediction error. results inspire mbrl algorithm deterministic mdps performance guarantees robust model class limitations. model-based reinforcement learning agent learns predictive model environment uses make decisions. overall mbrl approach intuitively appealing many anticipated beneﬁts learning model notably sample efﬁciency despite this exceptions model-free methods successful large-scale problems. even modellearning methods demonstrate increasing prediction accuracy high-dimensional domains rarely corresponds improvements control performance. reason disparity model-free methods generally robust representational limitations prevent convergence optimal behavior. contrast model representation insufﬁcient perfectly capture environment’s dynamics planner produces suboptimal plans mbrl methods fail catastrophically. beneﬁts mbrl gained large-scale problems vital understand mbrl effective even model planner fundamentally ﬂawed. recently growing awareness standard measure model quality one-step prediction accucopyright association advancement artiﬁcial intelligence rights reserved. state-action values model assigns policy given distribution. following result straightforwardly adapted provided talvitie lemma state-action value function returned applying one-ply monte carlo model rollout policy rollout depth greedy w.r.t. policy state-distribution term represents error limitations planning algorithm error sample average sub-optimality -step value function respect \u0001ξρt term represents error model parameters. factor model’s usefulness planning accuracy value assigns rollout policy state-actions visited goal next sections bound \u0001ξρt terms measures model accuracy ultimately deriving insight train models effective mbrl. proofs found appendix. one-step prediction error intuitively value policy accurate model accurate states policy would visit. adapt bound ross bagnell lemma policy state-action distribution combining lemmas yields overall bound control performance terms model’s prediction error. result matches common mbrl practice; recommends minimizing model’s one-step prediction error. acknowledges model imperfect allowing one-step error unimportant states. however limitations model class prevent model achieving error important states bound quite loose following example illustrates. consider shooter domain introduced talvitie pictured figure agent moves spaceship left right bottom screen. bullets upward cost bullet hits three targets agent receives reward. target bullseye bullet hits column bullseye agent receives additional reward. though control problem simple state/observation space high-dimensional many possible conﬁgurations objects screen. tive identiﬁes case yields tighter performance bound standard training result allows derivation novel mbrl algorithm theoretical performance guarantees robust model class limitations analysis also highlights previously underexplored practical concern approach examined empirically notation background focus markov decision processes environment’s initial state drawn distribution step environment state agent selects action causes environment transition state sampled transition distribution environment also emits reward simplicity assume reward function known bounded within policy speciﬁes behave mdp. probability chooses action state sequence actions probability reaching starting taking actions sequence. state action state-action distribution policy obtained steps starting state action thereafter following policy state action distribution saπ. state distribution saπ. discount factor inﬁnitehorizon discounted state-action distribution policy represents expected discounted rewards obtained taking action state executing additional steps -step state value inﬁnite horizons write agent’s goal learn policy maximizes es∼µ]. mbrl approach learn model approximating model produce policy planning algorithm. represent corresponding quantities using learned model. represent model class models learning algorithm could possibly produce. critically paper assumed contains perfectly accurate model. consider mbrl architecture uses simple oneply monte carlo planning algorithm roots rollout algorithm every state-action pair planner executes -step rollouts starting taking action following rollout policy average discounted return rollouts. large closely approximate agent select actions greedily respect talvitie bounds performance one-ply terms model quality. effectively assumes worst possible loss value model samples incorrect state. contrast lemma accounts model’s ability recover error penalizing individual incorrect transitions. unfortunately difﬁcult directly optimize multi-step prediction accuracy. nevertheless bound suggests algorithms account model’s multi-step error yield robust mbrl performance. hallucinated one-step prediction error seek formally analyze practice hallucinated training described section venkatraman provide analysis uncontrolled time series prediction setting. focus impact control performance mbrl. ﬁrst step derive bound based model’s ability predict next environment state given state sampled model’s predictions i.e. self-correct. policy state-action disξπ represent joint distribution envitribution ronment model state-action pairs executed simultaneously. speciﬁcally inspired hallucinated replay call quantity right hallucinated one-step error. hallucinated one-step error intended proxy multi-step error formalized cases poor proxy. note that regardless policy multi-step one-step error perfect model always hallucinated error. proposition hallucinated one-step error perfect model non-zero. proof. consider simple three states single action initial state fair coin ﬂipped transitioning equal probability stays forever. consider perfect model how| thus halluciever nated one-step error perfect model non-zero. environment samples heads model samples tails. given state model rightly predicts tails incurs error nevertheless since environment’s next state heads. model environment dynamics uncoupled cannot distinguish model error legitimately different stochastic outcomes. original shooter bullseyes remained still move back forth across targets. such problem second-order markov; bullseye center cannot predict next position without knowing previous position. agent however factored markov model predicting pixel conditioned current image. cannot accurately predict bullseyes’ movement though predict everything else perfectly. might imagine limitation would fairly minor; agent still obtain reward even cannot reliably bullseyes. however consider sample rollout pictured figure image sampled model’s one-step predictions given input next predictions. model lowest possible onestep prediction error. still anticipated correctly predict movement bullseyes second image. resulting errors sampled image unlike environment would generate therefore unlike model trained model’s uninformed predictions based unfamiliar image cause errors third image ultimately model assigns probability target persisting steps making essentially useless planning. note however models within model class useful planning. consider sample rollpictured figure model generated rollout makes one-step errors previous model given environment state. however encounters unreasonable sampled state still makes reasonable predictions effectively self-correcting. talvitie presents several similar examples involving various model deﬁciencies. examples illustrate inadequacy lemma model class limited. models similar one-step prediction error vary wildly usefulness planning. true distinguisher accuracy predictions future. deterministic setting lemma gives upper bound multi-step error lower bound one-step error theorem deterministic blind policy state-action distribution thus deterministic environment blind rollout policy hallucinated one-step error model tightly related mbrl performance standard onestep error. theoretical reason empirical success hallucinated replay trains model predict next environment state given samples input. exploit fact develop novel mbrl algorithm similarly uses hallucinated training mitigate impact model class limitations offers strong theoretical guarantees. data aggregator algorithm ﬁrst practically implementable mbrl algorithm performance guarantees agnostic model class. however require planner near optimal. dagger-mc relaxed assumption accounting limitations planner uses model section augments dagger-mc hallucinated training resulting hallucinated dagger-mc algorithm h-dagger-mc addition assuming particular form planner h-dagger-mc assumes model unrolled rather learning single model h-dagger-mc learns models model responsible predicting outcome step rollout given state sampled input. importance learning unrolled model discussed deeply section dagger handle toward hand? come clutch thee. thee thee still. thou fatal vision sensible feeling sight? thou dagger mind false creation proceeding heat-oppress’d brain? such hallucinated error misleading true dynamics stochastic. corroborates conjecture hallucinated replay problematic stochastic environments note observation applies hallucinated training method attempts improve multi-step predictions comparing sample rollouts model environment. seem limiting restrict attention deterministic environments still large rich class problems. instance learned models atari games fully deterministic human players often perceive stochastic complexity. similarly synthetic domains stochasticity often added simulate complex deterministic phenomena necessarily capture inherently stochastic effects world. examples shall assume environment deterministic complex limited agent learn imperfect stochastic model. proof. alter coin giving agent actions fully determine coin’s orientation. original dynamics recovered stochastic policy randomly selects leaves coin alone. tied action selection environment state prevents stochastic decoupling fail train model state-action pairs policy would reach model’s dynamics. tighter bound remainder paper assume environment deterministic. unique state results starting state taking action sequence agent’s model still stochastic. recall goal bound value error one-ply rollout policy. proposition shows hallucinated error gives loose bound arbitrary policies. focus blind policies blind policy depends action history i.e. class policies ranges stateless policies open-loop action sequences. includes uniform random policy common rollout policy. blind policy state-action distribution distribution environment state model state action single action sequence sampled executed model environment. much h-dagger-mc algorithm identical dagger-mc. main difference lies lines executed environment model generate hallucinated examples. trains model self-correct rollouts. like dagger dagger-mc h-dagger-mc requires ability reset initial state distribution also ability reset exploration distribution exploration distribution ideally ensures agent encounter states would visited good policy otherwise agent could promise good performance. performance bound h-dagger-mc depend part quality selected represent mismatch discounted state-action distribution exploration distribution consider sequence policies generated h-dagger-mc. uniform mixture policies sequence. ¯\u0001mc error induced choice planning algov rithm averaged iterations. lemma h-dagger-mc policies note result holds comparison policy thus ¯\u0001mc small learned models hallucinated one-step prediction error similar state-action distribution good policy compare favorably like original dagger dagger-mc results lemma limitations. uses loss always practical learning objective. also assumes expected loss iteration computed exactly also applies average policy rather last policy sequence. ross bagnell discuss extensions address practical loss functions ﬁnite sample bounds results next question course learned models accurate? following ross bagnell note interpreted average loss online learner problem deﬁned aggregated datasets iteration. case horizon error best model depth training distribution depth retrospect. specif| ically a)]. average regret model depth mdl. no-regret online learning algorgt gives following bound rithm h-dagger-mc’s performance terms model regret. theorem h-dagger-mc policies policy rgt) ¯\u0001mc theorem says contains low-error model rollout depth error models learned. then discussed above ¯\u0001mc small visits important states resulting policy yield good performance. notably even hallucinated training contains perfect model h-dagger-mc learn perfect model. important note result promise h-dagger-mc eventually achieve performance best performing models class. model rollout depth trained minimize prediction error given input distribution provided shallower models. note however changing parameters model depth alters training distribution deeper models. possible better overall error could achieved increasing prediction error depth exchange favorable state distribution deeper models. effect taken account h-dagger-mc. h-dagger-mc shooter example described section experimental setup matches talvitie comparison’s sake though qualitative comparison presented robust parameter settings. cases one-ply used uniformly random rollouts depth every step. exploration distribution generated following optimal policy probability termination step. model pixel learned using context tree switching similar fac-ctw algorithm used neighborhood around pixel previous timestep input. data shared across positions. discount factor iteration training rollouts generated resulting policy evaluated episode length discounted return obtained policy iteration reported averaged trials. results seen figure shaded regions represent conﬁdence intervals mean performance. benchmark lines labeled random perfect model represent average performance uniform random policy one-ply monte carlo using perfect model respectively. figure bullseyes move simulating typical practical reality contain perfect model. figure bullseyes ﬁxed positions contain perfect model. observed talvitie dagger performs poorly versions suboptimal planner. dagger-mc able perform well ﬁxed bullseyes moving bullseyes model suffers compounding errors useful planning holds single model unrolled model. experiments practically-minded alteration made h-dagger-mc algorithm. early training model highly inaccurate thus deep rollouts produce incoherent samples. training samples counter-productive experiments training rollouts iteration truncated depth planning rollouts early iterations models trained repeatedly apply deepest model order complete rollout. talvitie venkatraman similarly discarded noisy examples early training. transient modiﬁcation impact h-daggermc’s asymptotic guarantees. figure clear h-dagger-mc obtains good policy despite limitations model class. hallucinated training made mbrl possible ﬂawed model ﬂawed planner standard approach failed entirely. case contains perfect model h-dagger-mc outperformed dagger-mc. despite adjustment training deep models still receive noisy inputs. theoretically model become perfectly accurate limit though practice slowly. recall h-dagger-mc algorithm assumes model unrolled separate model responsible sampling step rollout. clear practical disadvantages important theoretically. model used across time-steps convergence perfect model cannot guaranteed even exists figure h-dagger-mc trained using single model shooter ﬁxed bullseyes. temporary truncation schedule described employed training rollouts permanently limited various depths. first consider learning curve marked depth training rollouts permitted reach maximum depth. rollouts temporarily truncated model well performance degrades longer rollouts permitted even though contains perfect model recall section changing model parameters impacts prediction error future training distribution. furthermore training examples generated deep rollouts contain highly ﬂawed samples inputs. sometimes attempting correct large error causes additional even worse errors next iteration instance consider hallucinated training example screen figure input screen figure target. model would effectively learn targets appear nowhere error would even harder correct future iterations. single model across timesteps feedback loop emerge model parameters change attempt correct large errors thereby causing larger errors feedback loop causes observed performance crash. unrolled model parameters sub-model cannot impact sub-model’s training distribution ensuring stability. note none talvitie venkatraman used unrolled model. such approaches subject concern. notably three limited depth training rollouts presumably prevent overly noisy samples. figure shows experiment shorter training rollouts better performance. results show possible practice avoid unrolling model truncating training rollouts though performance guarantee principled choice rollout depth. primary contribution work deeper theoretical understanding perform effective mbrl face model class limitations. speciﬁcally examined novel measure model quality that assumptions tightly related mbrl performance standard one-step prediction error. using insight also analyzed mbrl algorithm achieves good control performance despite ﬂaws model planner provides strong theoretical performance guarantees. figure comparing dagger dagger-mc h-dagger-mc shooter moving ﬁxed bullseyes respectively. h-dagger-mc shooter ﬁxed bullseyes using single model across time steps truncating rollouts various depths. open challenge relaxing assumptions deterministic dynamics blind policies developing alternative approaches improving multi-step error general settings. observed hallucinated training cause stability issues since model parameters affect prediction error training distribution itself. would valuable develop techniques account effects adapting model parameters. specializing one-ply planning algorithm seem restrictive again choice planning algorithm cannot make poor model. model class limited h-dagger-mc likely still good choice dagger even sophisticated planner. still would valuable investigate whether principles applied sophisticated planning algorithms. though work assumed reward function known results presented straightforwardly extended account reward error. however also raises interesting point sampling incorrect state little negative impact sampled state’s rewards transitions similar correct state. possible exploit obtain still tighter bounds effective guidance model learning mbrl architectures. work supported part grant iis-. many thanks marc bellemare whose feedback positively inﬂuenced work substance presentation. thanks drew bagnell arun venkatraman valuable insights. thanks also joel veness freely available fac-ctw implementations", "year": 2016}