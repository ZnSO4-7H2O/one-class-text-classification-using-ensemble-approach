{"title": "Dynamic Bayesian Multinets", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work, dynamic Bayesian multinets are introduced where a Markov chain state at time t determines conditional independence patterns between random variables lying within a local time window surrounding t. It is shown how information-theoretic criterion functions can be used to induce sparse, discriminative, and class-conditional network structures that yield an optimal approximation to the class posterior probability, and therefore are useful for the classification task. Using a new structure learning heuristic, the resulting models are tested on a medium-vocabulary isolated-word speech recognition task. It is demonstrated that these discriminatively structured dynamic Bayesian multinets, when trained in a maximum likelihood setting using EM, can outperform both HMMs and other dynamic Bayesian networks with a similar number of parameters.", "text": "conditional random variables time window surrounding information-theoretic used induce sparse conditional timal approximation ability fication heuristic medium-vocabulary nition task. demonstrated inatively trained outperform dynamic bayesian parameters. markov chains sometimes quences poor representations highly successful lows random functions yield hidden markov model well known simply type dynamic bayesian network generally hmms considered enormous independence example bitrary obvious computational model kept simple tractable many times. conditional particular given task. appropriate improvements accuracy ments computational achieved. model given application data general case model semantics nents graphical implementation parameters. riety different network) chain graphs general distributions bility tics could potentially anew. fixing semantics obtaining ture crucial focus fixing structure variety random variables neural networks gression good assignment course case bayesian problem domain speech recognition necessary sification discriminative ther improve sparsity memory demands) information scribed section provides bayesian networks idea structural class models considered inferential tion theoretic structure approximation introduces veloped induction section tationally medium-vocabulary speech corpus shows structure method trained form hmms dynamic bayesian similar determined method performance section work structure perhaps directed topic found general networks grouped four categories data fully observable assumed structure not. easiest model structure case data partially observable structure note general learn many aspects needs maximum likelihood additional complexity bic. alternativel used sen. certain particularly tion approach principle optimization ously cover four components mantics however best implementation comes inherently difficult ponent cannot accurately good settings becomes arduous begins consider multi-implementation tice therefore optimization begins. fully-connected ability many important fully connected work structures quirements; training fitting; knowledge previously drowned graphical random variables sary\" depends current structure underlying analogous advantage dependencies cant reduction generalize bayesian networks ther reduce computation. network edges appear depending values certain called asymmetric general regular represented values parameters gaussian discrete dependence random variables). probability works bayesian however memory computation plexity relative work consider namic bayesian multinets markov chains determine window observations. equiva­ bayesian extensions hidden markov models lently cross-observation additional added function markov chain. model also called buried markov model hidden markov chain dependencies. first notation refers markov state learning concentrate many papers structure networks tant data. goal classification necessarily ability label considered maximum likelihood procedure assuming procedure scores penalized cies features nitude) ability. sion theory classification extended class conditional bayesian outperformed fication example could xt-} {xt-xt-o} forr multinetoccursbe­ cause function dependencies. tional independence ments becomes till{ \\zti} model depicted markov chain. edges must adjacent moralization cliques ables variable parent hidden variables. observations triangulated therefore ar-hmm there­ fore fixed however additional complexity becomes number dependency dependency structure tional much less complexity course actual probability known approximation able parameters method maximum likelihood approximate theorem still holds. therefore parameters differences proof). quantity could called explaining asks edges class-conditionally dent marginally result summarizing could used choose edges elements previous equations works underlying network unique class. achieve high score presence right class class. importantly structure optimal works inherently achieving samples wrong class. therefore priate parameters costly risk-minimization algorithm candidate dexed total edges added separately might allow redundancy already dencies scalars decreasing q)-i. penalties structures. third adding random dependencies poor performance benefit. pendencies inative implications randomly search space unlikely found reasonable cial therefore found useful argued past", "year": 2013}