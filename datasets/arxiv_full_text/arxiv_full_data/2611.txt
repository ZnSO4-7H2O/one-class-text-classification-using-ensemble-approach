{"title": "Hierarchical compositional feature learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We introduce the hierarchical compositional network (HCN), a directed generative model able to discover and disentangle, without supervision, the building blocks of a set of binary images. The building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below, arranged in a particular manner. At a high level, HCN is similar to a sigmoid belief network with pooling. Inference and learning in HCN are very challenging and existing variational approximations do not work satisfactorily. A main contribution of this work is to show that both can be addressed using max-product message passing (MPMP) with a particular schedule (no EM required). Also, using MPMP as an inference engine for HCN makes new tasks simple: adding supervision information, classifying images, or performing inpainting all correspond to clamping some variables of the model to their known values and running MPMP on the rest. When used for classification, fast inference with HCN has exactly the same functional form as a convolutional neural network (CNN) with linear activations and binary weights. However, HCN's features are qualitatively very different.", "text": "introduce hierarchical compositional network directed generative model able discover disentangle without supervision building blocks binary images. building blocks binary features deﬁned hierarchically composition features layer immediately below arranged particular manner. high level similar sigmoid belief network pooling. inference learning challenging existing variational approximations work satisfactorily. main contribution work show addressed using max-product message passing particular schedule also using mpmp inference engine makes tasks simple adding supervision information classifying images performing inpainting correspond clamping variables model known values running mpmp rest. used classiﬁcation fast inference exactly functional form convolutional neural network linear activations binary weights. however hcn’s features qualitatively different. deep neural networks coupled availability vast amounts data proved successful last years visual discrimination basic desire deep architectures discover blocks features– compose image different levels abstraction. tasks require degree image understanding performed easily using representations based building blocks. would make intuitive sense train models images containing e.g. text learned features would individual letters since building blocks provided images. addition matching intuition argue model realizes building blocks text letters able extract representation based those understood nature observations prove able efﬁciently compress text images. however case existing incarnations models. fig. features recovered hierarchical compositional network single image supervision. appear reasonable building blocks easy human. aware model perform apparently simple recovery supervision. multilayer generative model features deﬁned layer. feature deﬁned composition features layer immediately increase ﬂexibility positions composing features perturbed slightly respect default values results latent variable model latent variables shared images others speciﬁc image. comparing generative models images note existing models tend least following limitations priors rich enough; typically sources variation distributed among layers network instead generative model expressed random variables generated image network i.e. entire network behaves sophisticated deterministic function inference method considers latent variables independent solve explaining away leads learned features directly interpretable reusable parts learned images. although directed models enjoy important advantages ability represent causal semantics easy sampling mechanics known explaining away phenomenon makes inference difﬁcult models reason representation learning efforts largely focused undirected models tried avoid problem explaining away using complementary priors important contribution work show approximate inference using max-product message passing learn features composable interpretable causally meaningful. also noteworthy unlike previous works consider weights latent variables parameters. thus separate expectation-maximization stages. instead perform feature learning pool state inference jointly part message passing loop. augmented supervision information used classiﬁcation inference learning still taken care largely unmodiﬁed mpmp procedure. training discrimination achieved fast forward pass turns functional form convolutional neural network rest paper organized follows describe model section section describes learning inference single layer multilayer hcns; section tests experimentally conclude brief discussion section model discrete latent variable model generates binary images composing parts different levels abstraction. parts shared across images. training model involves learning parts data well combine create concrete image. model expressed factor graph consisting three types factors pool. perform obvious binary operations deﬁned precisely later section. ﬂexibility model allows training supervised semisupervised unsupervised settings including missing image data. trained used classiﬁcation missing value completion sparsiﬁcation denoising etc. fig. factor graph complete model. additional details layer type given fig. discriminative models features good classiﬁcation generation existing generative models also fail recovering building blocks image either positive negative weights lack inference mechanisms able perform explaining away. figure factor graph model connected multiple images weights variables entangle multiple images. variables clamped bottom variables clamped additional details layer type given fig. multidimensional array latent variables. class layer selects category within template going used producing top-level sparsiﬁcation. sparsiﬁcation simply encoding representation. sparsiﬁcation encodes representation specifying features compose placed. features turn stored form weights. convolutional layers deterministically combine sparsiﬁcation weights layer create representation. pooling layers randomly perturb position active elements introducing small variations process. case since additional top-down structure binary image created placing features random locations image. wherever features overlap ored i.e. pixel binary image activated features simply kept active. call features sparsiﬁcation image image. variables multidimensional binary arrays. values involved arrays concrete example single-channel image given fig. corresponding diagram shown fig. practice image possibly multichannel size ﬁrst dimension number channels image height width. size ﬁrst dimension number features height width. refer entry setting entry corresponds placing feature position ﬁnal image features stored size fbelow i.e. feature small array containing building blocks image. placed positions speciﬁed block used many times different positions hence calling layer convolutional. fully specify probabilistic model binary images adding independent priors entries connecting binary convolution noisy channel. complete model depends four scalar parameters controlling density features image pixels feature noise channel respectively. indexes channels features rows columns respectively. used binary convolution operator bconv. binary convolution performs operation normal convolution operates binary inputs truncates outputs latent variables arranged threefour-dimensional arrays deﬁne convd) convd usual convolution operator binary arrays binary arrays. operator truncates values performing oring overlapping features previously mentioned. binary convolution expressed factor graph seen fig. factor written takes value bottom variable logical variables takes value case. factor takes value bottom variable logical variables takes value case. layer used standalone mode inside multilayer variables connected pooling layer immediately variables connected pooling layer immediately mutually exclusive binary variables representing categories present. general deﬁne pool exactly bottom variables takes value pool takes value case. within category might multiple templates. template corresponds different visual expression conceptual category. instance category furniture could template chair another template table. category binary variables representing templates category active exactly templates active. joint probability templates variables arranged array size called forms top-level sparsiﬁcation template. sample always exactly element rest superscript used identify layer variable belongs. since layers layer sparsiﬁcation. since convolutional feature layer deterministic variation generated image must come pooling layers pooling layer shifts position active units produce sparsiﬁcation layer below. shifting local constrained region size pooling window. active units shifted towards position result single activation number active units equal smaller number activations description enough know sample provide rigorous probabilistic description need introduce intermediate binary variables u∆r∆cfrc intermediate variables associated shift element associated element since element shifted single position realization active elements described pooling window allows spatial perturbations i.e. translational pooling. general pooling layer would also pool third dimension across features would introduce richer variation also impose meaningful order feature indices. though pursue option work note type pooling required rich hierarchical visual model. fact pooling templates special-cased description class layer would particular case third-dimension pooling. observed binary image corresponds bottommost sparsiﬁcation traversed element element noisy channel probabilities drawn shared generation images write joint probability multiple images latent variables weights collected category variables {ck} image remaining latent variables convenience. image uses copy latent variables weights shared across images coupling latent variables. expression shows addition factorizing observations factorization across layers. furthermore previous description layers implies entire model reduced small factors type pool involving local variables each. since interested point estimate features given images {xn}n subset labels {cn}n attempt recover maximum posteriori conﬁguration features sparsiﬁcations unknown labels. note classiﬁcation selecting maximizing joint probability different selecting maximizing discriminative loss type since case prior information structure images lost. results samples required achieve performance less invariance test data. learning complete thus decoupling model every image approximate inference classify test images complete include missing data even though consider single-class-per-image setting compositional property model means train single-class images then without retraining change class layer make generate combinations classes image. heights widths involved arrays one. decades-old problem proved np-complete applications machine learning communications combinatorial optimization. another related problem non-negative matrix factorization additive instead oring contributions multiple features desired here. best-known heuristics address asso unfortunately clear extend solve relies assumptions longer hold present case. variational bound addresses inference presence noisy-or gate successfully used obtain noisy-or component analysis algorithm. noca addresses similar problem differences weight values continuous convolutional weight sharing among features. noca modiﬁed include convolutional weight sharing entirely satisfactory solution feature learning problem show. observed obtained local maxima even signiﬁcant tweaking parameters learning schedule poor problems small moderate size. aware existing algorithms solve medium image sizes. model directly amenable mean-ﬁeld inference without requiring additional lower-bounding used noca experimented several optimization strategies obtained local maxima consistently worse noca. shown max-product message passing produces stateof-the-art results problem improving even performance asso heuristic. also address problem using mpmp. even though mpmp guaranteed converge found right schedule even slight damping good solutions found consistently. model expressed directed bayesian network factor graph using factors involving small number local binary variables. finding features sparsiﬁcations cast inference factor graph. mpmp local message passing technique perform inference factor graphs. mpmp exact factor graphs without loops loopy models approximation convergence guarantees although convergence often attained using damping appendix quick review mpmp appendix message update equations required factors used work. unlike ravanbakhsh uses parallel updates damping update and-or factor turn following random sequential schedule. results faster convergence less damping. despite loopiness also apply mpmp inference full multilayer model obtain good results. learning procedure iterates forward backward passes forward pass proceed updating bottom-up messages variables starting bottom hierarchy going class layer. backward pass update top-down messages visiting variables top-down order. messages weight variables updated forward pass. damping update bottom-up messages pooling layer forward pass. and-or factors binary convolutional layer form trees treat trees single factor since closed form message updates obtained. factors updated random order inside layer i.e. sequentially. pools class layer also tree also treat single factor. message updates pool factors follow trivially deﬁnition provided appendix note marginalize latent variables conﬁguration given images. sparse priors weights sparsiﬁcation regularizers prevent overﬁtting. mpmp works iterating ﬁxed point equations dual bethe free energy zero-temperature enough iterations weights max-marginal difference positive otherwise. hard assignment converts factors pass-through rest disconnections. thus weight assignments deﬁne connectivity graph without ands. learned model perform inference test images. typical inference tasks classiﬁcation missing value imputation. classiﬁcation single forward pass seems good enough forward backward passes needed. missing value imputation single forward backward pass enough. forward backward passes follow general pattern described algorithm except step backward pass. order achieve higher quality explaining-away single backward pass replace step multiple alternating executions steps create several synthetic images building blocks features– obvious human observer check ability recover them. task deceptively simple existing state task noca unable solve several examples. since number free parameters model small easily explored using grid search selected using maximum likelihood. sensitivity results parameters small. requires straightforward mpmp random order factors. noca initializing variational posterior latent sources choosing interleave updates posterior update additional variational parameters tricky. best results step repeated following times update variational parameters iterations update variational posterior update also required inner loop variational parameter updating. performance noca assessed visually fig. column shows input image remaining columns show features reconstructions obtained noca. input images added noise ﬂips pixels probability. binarize beliefs range thresholding perform binary convolution obtain reconstruction. noise included reconstruction cleaner image obtained resulting unsupervised denoising quantitative comparison refer tab. algorithm-independent measure performance feature learning problem measure compression. known transmit long sequence bits probability need transmit bits optimal encoding entropy. thus sparse sequences compress well. order transmit images without loss need transmit either sequence bits three sequences bits encoding features another encoding sparsiﬁcation last encoding errors reconstruction original image. ideally second method efﬁcient features sent sparsiﬁcation errors sequences much sparser original image. ratio shown together running time single cpu. unused features discarded prior computing compression. avoid symmetry problems instead making distribution pool perfectly uniform introduce slight random perturbations keeping highest probability value center pool. speeds learning favors centered backward pass reconstructions case ties. figure online learning. show sample input images; show features learned batch online using input images epochs; shows features learned online using input images epoch. experiments batch formulation i.e. consider simultaneously available training data {xn}n since amount memory required store messages mpmp scales linearly training data imposes practical limit number images processed. order overcome limit also consider particular message update schedule messages outgoing factors connected image sparsiﬁcation updated therefore image processed discarded since never reused. effectively allows online processing images without memory scaling issues. modiﬁcations needed practice work well ﬁrst instead processing image time better results obtained factors multiple images processed random order. second forgetting mechanism must introduced avoid accumulating unbounded amount evidence processed minibatches. detail beliefs variables initialized uniformly random interval initialized initial outgoing messages and-or factors since factor processed once allows implementing mpmp without ever store messages requiring store beliefs. processing ﬁrst minibatch using mpmp call resulting belief weights post instead processing second minibatch using prior prior i.e. forget part observed evidence substituting prior. introduces exponential weighing contribution minibatch. forgetting factor speciﬁes amount forgetting. reduces normal mpmp completely forget previous minibatch process scratch. fig. illustrates online learning. shown small images containing randomly chosen randomly placed characters ﬂipping noise examples). learned different manners. fig. single batch damping using epochs fig. minibatches images damping using epochs; fig. minibatches images damping using single epoch using images running time same. create dataset combining traits either square circle either forward backward diagonal line. results four patterns group categories fig. categories chosen cannot decide label image based traits. position traits jittered within window combining them position individual pixels also jittered amount. finally pixel ﬂipped probability sampling procedure corresponds -layer sampling parameterization. generate training samples test samples. train described section training data samples using label information. architecture network match architecture generating data. four hyperparameters model selection critical. choose match generation process. inference discover disentangle almost perfectly compositional parts ﬁrst second layers hierarchy figs. rows correspond templates columns correspond features ﬁrst layer. model understood data used generate samples performing inference model challenging. aware previous method learn features simple dataset samples. experiments veriﬁed that using local message passing opposed gradient descent critical successfully minimize objective function. results quality figs. obtained every algorithm. running time single cpu. clamp discovered weights layers fast forward pass classify training image belonging four discovered templates even classify test images belonging four templates. this images training assigned right template images test classiﬁed right cluster. means labeled images cluster could perform -class minimally-supervised classiﬁcation error. retrain model using label information. results weights found time templates properly grouped classes shown fig. classiﬁcation error test compare classiﬁcation performance functional form trained discriminatively standard relu activations densely connected layer softmax activation. minimize crossentropy loss function regularization weights. test errors respectively much larger hcn. consider versions training different levels pixel-ﬂipping noise. evolution test error shown fig. competing methods needed many random restarts obtain good results. regularization parameter chosen based test performance. turn problem real data mnist database contains training/testing images size want generalize samples ﬁrst digits category train. pre-process image ﬁxed oriented ﬁlters inputs -channel image. -layer templates class lower level features size layers pooling values priori optimized. test regular mnist training different corrupted versions follow preprocessing procedure using regular discriminative training explore different regularizations architectures activation types ﬁxing pooling sizes number layers match hcn. select parameterization minimizes error clean test set. uses level features. results test sets reported fig. seen generalizes better. weights ﬁrst layer training shown fig. notice able discover reusable parts digits. training time scales exactly cnn. linear architectural parameters number images number pixels image features layer size features etc. however forward backward passes complex optimized code readily available signiﬁcant constant factor separates running times both. training time mnist around hours single cpu. required store messages training images mnist goes around scale bigger training sets online extension needs used. described hierarchical feature model rich prior provided novel method solve challenging learning problem poses. model effectively learns convolutional features interpretable ﬂexible. learned weights binary advantageous storage computation purposes future work entails adding structure prior leveraging reﬁned inference techniques exploring update schedules exploiting generalization-without-retraining capabilities model. matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems amir globerson tommi jaakkola. fixing max-product convergent message passing algorithms lp-relaxations. advances neural information processing systems song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. pauli miettinen taneli mielik¨ainen aristides gionis gautam heikki mannila. discrete basis problem. european conference principles data mining knowledge discovery springer huayan wang koller daphne. subproblem-tree calibration uniﬁed approach max-product message passing. proceedings international conference machine learning model expressed directed bayesian network factor graph using pool factors involving small number local binary variables. learning ulterior classiﬁcation cast inference factor graph. tasks ﬁlling unknown image data also performed inference. inference performed exactly factor graphs without loops linear time np-hard problem arbitrary graphs factor graph describing model highly structured also loopy. large body works addressing problem inference loopy factor graphs. perhaps simplest methods max-product algorithm variant dynamic programming proposed conﬁguration trees. max-product algorithm deﬁnes messages ma→i going factor ma→i deﬁnes approximate max-marginal max-product algorithm proceeds updating outgoing messages factor turn make approximate max-marginals consistent factor. algorithm guaranteed converge loops graph does guaranteed conﬁguration. damping updates factors shown improve convergence loopy belief propagation justiﬁed local divergence minimization using damping factor max-product update rule original update rule recovered value arbitrary affect algorithm. select make messages stored single scalar. storing messages provides max-marginal difference enough purposes. computed exactly three type factors appearing graph message updating performed closed form. despite graph model loopy turns careful choice message initialization damping parallel sequential updates produces satisfactory results experiments. details max-product inference inference message passing discrete graphical models refer reader following provide message update equations different types factors used main paper. messages normalized form message single scalar corresponds difference unnormalized message value evaluated unnormalized message value evaluated update assume incoming messages variables factor available. incoming messages messages going variable except factor consideration. outgoing messages well-deﬁned even incoming messages taking corresponding limit expressions below. input hyperparameters network structure init initialize bottom-up messages messages zero. initialize top-down messages initialize messages prior uniformly random break symmetry. constant bottom-up messages convolution bottom-up messages weights like standard cnn. max-marginal categories select template largest bottom-up message. realized max-pooling feature dimension done closely approximated using fully connected layer softmax standard cnns. consider generate data sets model using weights different bit-ﬂip probabilities. probabilities known would different classiﬁers dataset? single forward pass changing produces different monotonic transformation bottom-up messages every layer hierarchy selected category depends variable largest value change. single-pass classiﬁer class estimation change noise level. important implication need trained noisy data classify noisy data. trained clean data used noisy data without retraining.", "year": 2016}