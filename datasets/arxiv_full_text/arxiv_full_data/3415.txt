{"title": "Fast and Accurate Inference with Adaptive Ensemble Prediction in Image  Classification with Deep Neural Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Ensembling multiple predictions is a widely used technique to improve the accuracy of various machine learning tasks. In image classification tasks, for example, averaging the predictions for multiple patches extracted from the input image significantly improves accuracy. Using multiple networks trained independently to make predictions improves accuracy further. One obvious drawback of the ensembling technique is its higher execution cost during inference. If we average 100 predictions, the execution cost will be 100 times as high as the cost without the ensemble. This higher cost limits the real-world use of ensembling, even though using it is almost the norm to win image classification competitions. In this paper, we describe a new technique called adaptive ensemble prediction, which achieves the benefits of ensembling with much smaller additional execution costs. Our observation behind this technique is that many easy-to-predict inputs do not require ensembling. Hence we calculate the confidence level of the prediction for each input on the basis of the probability of the predicted label, i.e. the outputs from the softmax, during the ensembling computation. If the prediction for an input reaches a high enough probability on the basis of the confidence level, we stop ensembling for this input to avoid wasting computation power. We evaluated the adaptive ensembling by using various datasets and showed that it reduces the computation time significantly while achieving similar accuracy to the naive ensembling.", "text": "small devices handheld device embedded controller much smaller computation power energy supply large systems used training network. hence method achieve high prediction accuracy limited computation resources needed enable applications deployed real world. reduce computation costs inference phase hinton created smaller network deployment distilling knowledge ensemble multiple models. also targeted deployment small devices large networks significantly showed compressed training pruning unimportant connections quantizing connection. ensembling multiple predictions widely used technique improve accuracy various machine learning tasks cost computation power. image classification tasks example accuracy significantly local predictions multiple patches extracted input image make final prediction. moreover accuracy improved using multiple networks trained independently make local predictions. krizhevsky averaged local predictions using patches extracted center corners without horizontal flipping alexnet paper. googlenet szegedy averaged local predictions using patches networks. paper reported averaging predictions reduced top- error imagenet classification task whereas averaging predictions model reduced error compared baseline prediction without ensembling. ensemble methods meta-learning training learn best multiple local predictions networks used alexnet googlenet papers however significant improvements obtained averaging predictions without meta-learning. paper meta-learning either. although benefits ensemble prediction quite significant obvious drawback higher execution cost inference. make final prediction ensembling multiple predictions widely used technique improve accuracy various machine image classification tasks example averaging predictions multiple patches extracted input image significantly improves accuracy. using multiple networks trained independently make predictions improves accuracy further. obvious drawback ensembling technique higher execution cost inference. average predictions execution cost times high cost without ensemble. higher cost limits real-world ensembling even though using almost norm image classification competitions. paper technique called adaptive describe ensemble prediction achieves benefits ensembling much smaller additional execution costs. observation behind technique many easy-topredict inputs require ensembling. hence calculate prediction input basis probability predicted label i.e. outputs ensembling computation. prediction input reaches high enough probability basis confidence level stop ensembling input avoid wasting computation power. evaluated adaptive ensembling using various datasets showed reduces computation time significantly achieving similar accuracy naive ensembling. introduction huge computation power today’s computing systems equipped gpus special asics fpgas multi-core cpus makes possible train deep networks using tremendous datasets. although high-performance systems used training actual inference real world executed ensembling predictions need make local predictions hence execution cost times high without ensembling. higher execution cost limits real-world ensembling especially small devices even using almost norm image classification competitions emphasize prediction accuracy. make ensemble prediction feasible wider range applications developed adaptive ensemble prediction achieves benefits ensembling much smaller additional costs. observation behind technique many easy-topredict inputs require ensembling. output produced softmax neural network predicted class label probability prediction. ensembling process calculate confidence level probability obtained local predictions input. input reaches high enough confidence level stop ensembling making local predictions input avoid wasting computation power. evaluated adaptive ensembling using four image classification datasets ilsvrc cifar- cifar- svhn. results showed adaptive ensemble prediction reduces computation time significantly achieving similar accuracy naive ensemble prediction. igure improvements ensemble probabilities predictions ilsvrc validation set. x-axis shows percentile probability first local predictions high ensemble reduces error rates samples probabilities affect samples high probabilities. predictions left side i.e. easy-to-predict images highly probable predictions even non-negligible number mispredicted samples. example -percentile range error rate improved averaging predictions different networks. range average probability prediction determine whether characteristic ensembling unique googlenet architecture conducted experiment using alexnet another network architecture show results figure although prediction error rate higher alexnet googlenet observe similar characteristics improvements ensembling observed right figure i.e. hard-to-predict images. characteristics improvements ensemble unique ilsvrc dataset; observed similar trends datasets. ensembling probability prediction section describes motivated develop proposed technique ensemble prediction accuracy predictions different probabilities. show relationship probability prediction effect ensembling evaluate prediction accuracy ilsvrc dataset without ensembling predictions made independently trained networks. figure shows results experiment googlenet; networks follow design googlenet exactly configurations experiment evaluated images validation ilsvrc dataset using first network sorted images probability prediction evaluated images second network assessed accuracy ensembling local predictions using arithmetic mean. x-axis figure shows percentile probability high i.e. going left input images become easier predict. gray dashed line shows average probability percentile class. average images ensemble improves accuracy well although averaged predictions. interestingly observe improvements come right figure. almost section details proposed adaptive ensemble prediction method. shown figure ensemble typically improve accuracy predictions highly probable. hence terminate ensembling without processing local predictions basis probabilities predictions. execute following steps start obtain i-th local prediction i.e. probability class label. denote probability label static-threshold-based termination condition simple condition basis predetermined threshold terminate ensembling. condition compare highest average confidence-level-based termination condition instead pre-defined threshold confidence intervals termination condition. first find highest average probability then calculate probabilities using local predictions. calculated predicted label overlap labels i.e. predicated label best prediction certain confidence level terminate ensembling output predicted label final prediction. calculate confidence interval probability label using local predictions results motivate make adaptive ensemble prediction reducing additional cost ensembling keeping benefit improved accuracy. obtain high enough prediction probability input local predictions ensembling waste computation power without improving accuracy. challenge identify condition terminate ensembling. described later identify termination condition basis confidence level probability. also show results mixing googlenet alexnet appendix. related work various prediction methods ensemble outputs many classifiers widely studied achieve higher accuracy machine learning tasks. boosting bagging famous examples ensemble methods. boosting bagging produce enough variances classifiers included ensemble changing training classifier. recent image classifications deep neural studies networks however initialization ordering input images) used training phase give sufficient variances networks even using training classifiers hence networks trained using training network architecture study. higher execution cost ensembling known problem first attack example hinton also tackled high execution cost ensembling. unlike trained smaller network distilling knowledge ensemble networks following buciluǎ predictions inference. typically probability prediction generated softmax used training network; cross entropy probabilities often used objective function optimization. however using probability purposes target optimization unique example hinton used probabilities softmax distilling knowledge ensemble. know first study focusing relationship probability prediction effect ensembling current deep neural networks. opitz maclin showed important observation related ours. showed large part gain ensembling came ensembling first local predictions. observation discussed previous section enhances opitz’s observation different perspective gain ensembling comes hard-to-predict samples. here means confidence level number samples pair-wise comparisons preferably want labels. however computing labels costly especially many labels. avoid excess costs computing compare probability predicted label total probabilities labels. since total probabilities labels definition total probabilities labels computation equation cannot cases. since cannot calculated local prediction obvious equation avoid zero divisions hybrid termination conditions. static-threshold-based condition first local prediction quite conservative threshold second local prediction confidence-level-based condition used. section investigate effects adaptive ensemble prediction prediction accuracy execution cost using various image classification tasks ilsvrc street view house numbers cifar- cifar- datasets. ilsvrc dataset googlenet network architecture train network using stochastic gradient descent momentum optimization method. datasets network convolutional layers batch normalization followed fully connected layers. used network architecture except number neurons output layer. train network using adam optimizer. task trained networks independently. training used data augmentations extracting patch random position input image using random horizontal flipping. since adaptive ensemble prediction inference-time technique affect network training. executed training inference tesla ilsvrc dataset tesla datasets. averaged local predictions using ensembling. created patches input image extracting center four corners without horizontal flipping following alexnet. patch made local predictions using networks. patch size ilsvrc dataset datasets. adaptive ensemble prediction made local predictions following order inference batch samples. repeated local predictions batch became smaller computation parts samples terminated. results tables show adaptive ensemble prediction affected accuracy predictions execution costs. here adaptive ensemble confidence-level-based termination condition confidence level combined static threshold first local prediction. different configurations network networks datasets ensemble improved accuracy tradeoff increased execution costs expected. using networks doubled number local predictions average increased benefit drawback. local predictions benefit cost become much significant. comparing adaptive ensemble naive ensemble adaptive ensemble similarly improved accuracy reducing execution time one-network twonetwork configurations respectively. performance boosts came local predictions used ensembles. reductions one-network twonetwork configurations. reductions execution time naive ensemble smaller reduction number averaged predictions additional overhead confidence interval calculation written python current implementation. also mini batches gradually became small ensembling parts samples terminated. smaller batch sizes reduced efficiency execution current gpus. since speedup adaptive technique naive ensemble became larger number predictions ensemble increased benefit adaptive technique become impressive larger ensemble configurations. study differences termination condition adaptive ensemble show relationship prediction accuracy computation cost ilsvrc cifar- datasets figure used networks experiment i.e. predictions ensembled. figure x-axis number ensembled predictions smaller means faster. y-axis improvements classification error rate baseline higher means better. static-threshold-based conditions changing threshold drew lines figure prediction accuracy computation cost naive ensemble adaptive ensemble using different termination conditions. static threshold used control tradeoff accuracy computation cost. naive ensemble predictions ensemble tradeoff extremes. confidence-level-based condition achieved better accuracy static-threshold-based conditions computation cost especially cifar-. tuning confidence level less sensitive static threshold. figure. similarly evaluated confidence-levelbased condition three confidence levels static threshold first local prediction. also evaluated naive ensemble changing number predictions average. figure threshold used knob control tradeoff accuracy computation cost well number predictions average naive ensemble. adaptive ensemble static threshold naive ensemble naive ensemble predictions tradeoff corresponds t=%. baseline execute ensemble always terminates first prediction regardless probability. comparing lines adaptive ensemble static threshold achieved accuracy better naive ensemble using number predictions average unless threshold small. means probability prediction effective criterion control number predictions ensemble. figure reasonable excessively small threshold e.g. less decreases accuracy since significantly miss opportunity gain ensembling. obviously decide best threshold important problem static threshold based condition. confidence-level-based condition resolves problem. differences number samples song huizi william dally deep compression compressing deep neural networks pruning trained quantization huffman coding proceedings international conference learning representations. alex krizhevsky ilya sutskever geoffrey hinton classification deep convolutional neural networks proceedings advances neural information processing systems christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan andrew rabinovich going deeper convolutions proceedings computer vision pattern recognition. improvements error rate choice confidence level much less significant differences static threshold staticthreshold-based condition. hence task-dependent fine tuning confidence level important tuning static threshold. obvious figure static threshold tuning highly dependent dataset task. easier tuning parameter important advantage confidence-level-based condition. addition benefit easier parameter tuning confidence-level-based condition reduced computation cost maintaining accuracy. gain confidence-level-based condition static-threshold-based significant especially cifar- whereas marginal ilsvrc datasets show largest smallest gain confidence-level-based condition static-threshold-based condition; datasets showed improvements datasets shown figure using confidence-level-based condition adaptive ensemble largely outperformed naïve ensemble data sets. conclusion paper described adaptive ensemble prediction reduce computation cost ensembling many predictions. motivated develop technique observation ensembling improve prediction accuracy samples easy predict. experiments using various image classification tasks showed adaptive ensemble makes possible avoid wasting computing power without significantly sacrificing prediction accuracy probabilities local predictions. benefit technique become larger predictions ensemble. hence expect technique make ensemble techniques valuable real-world systems reducing total computation power required maintaining good accuracies throughputs. references breiman bagging predictors machine cristian buciluǎ rich caruana alexandru niculescu-mizil model compression proceedings international conference knowledge discovery data <ining accuracy alexnet. rightmost region i.e. -percentile samples ensemble improves error rate googlenet alone. characteristics consistent cases using identical networks shown figure interesting results middle regions -percentile samples. region ensemble slightly worsens prediction accuracy googlenet alone. googlenet better prediction performance alexnet samples region easy predict googlenet alexnet. although study ensemble predictions network paper need take behavior account extending work predictions different networks. predictions alexnet first prediction ensemble improves accuracy much wider regions shown figure appendix chapter showed predictions networks network architecture improve prediction accuracy probabilities prediction. appendix show results predictions googlenet alexnet. figure shows result googlenet first prediction alexnet second. figure x-axis shows percentile probability prediction googlenet high i.e. going left input images become easier predict. leftmost region i.e. -percentile samples ensemble different networks predicts accurately googlenet higher prediction", "year": 2017}