{"title": "Sliced Wasserstein Distance for Learning Gaussian Mixture Models", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Gaussian mixture models (GMM) are powerful parametric tools with many applications in machine learning and computer vision. Expectation maximization (EM) is the most popular algorithm for estimating the GMM parameters. However, EM guarantees only convergence to a stationary point of the log-likelihood function, which could be arbitrarily worse than the optimal solution. Inspired by the relationship between the negative log-likelihood function and the Kullback-Leibler (KL) divergence, we propose an alternative formulation for estimating the GMM parameters using the sliced Wasserstein distance, which gives rise to a new algorithm. Specifically, we propose minimizing the sliced-Wasserstein distance between the mixture model and the data distribution with respect to the GMM parameters. In contrast to the KL-divergence, the energy landscape for the sliced-Wasserstein distance is more well-behaved and therefore more suitable for a stochastic gradient descent scheme to obtain the optimal GMM parameters. We show that our formulation results in parameter estimates that are more robust to random initializations and demonstrate that it can estimate high-dimensional data distributions more faithfully than the EM algorithm.", "text": "gaussian mixture models powerful parametric tools many applications machine learning computer vision. expectation maximization popular algorithm estimating parameters. however guarantees convergence stationary point log-likelihood function could arbitrarily worse optimal solution. inspired relationship negative log-likelihood function kullback-leibler divergence propose alternative formulation estimating parameters using sliced wasserstein distance gives rise algorithm. speciﬁcally propose minimizing sliced-wasserstein distance mixture model data distribution respect parameters. contrast kl-divergence energy landscape sliced-wasserstein distance well-behaved therefore suitable stochastic gradient descent scheme obtain optimal parameters. show formulation results parameter estimates robust random initializations demonstrate estimate high-dimensional data distributions faithfully algorithm. finite gaussian mixture models also called mixture gaussians powerful parametric probabilistic tools widely used ﬂexible models multivariate density estimation various applications concerning machine learning computer vision signal/image analysis. gmms utilized image representation generate feature signatures point registration adaptive contrast enhancement inverse problems including super-resolution deblurring time series classiﬁcation texture segmentation robotic visuomotor transformations among many others. special case general latent variable models ﬁnite parameters could serve concise embedding provide compressed representation data. moreover gmms could used approximate density deﬁned large enough number mixture components. ﬁnite observed data required answer following questions estimate number mixture components needed represent data estimate parameters mixture components. several techniques introduced provide answer ﬁrst question focus paper latter question. existing methods estimate parameters based minimizing negative log-likelihood data respect parameters expectation maximization algorithm prominent minimizing remains popular method estimating gmms guarantees convergence stationary point likelihood function. hand various studies shown likelihood function local maxima arbitrarily worse log-likelihood values compared global maxima importantly proved random initialization algorithm converge critical point high probability. issue turns algorithm sensitive choice initial parameters. limit minimizing function equivalent minimizing kullbackleibler divergence data distribution respect parameters. here propose alternatively minimize p-wasserstein distance speciﬁcally sliced p-wasserstein distance data distribution gmm. wasserstein distance variations attracted attention machine learning signal processing communities lately shown optimizing respect wasserstein loss various practical beneﬁts kl-divergence loss importantly unlike kl-divergence related dissimilarity measures wasserstein distance provide meaningful notion closeness distributions supported non-overlapping dimensional manifolds. motivates proposed transport exists transport plan transport related note engineering computer science applications compact subset euclidean distance. abuse notation interchangeably throughout manuscript. detailed explanation wasserstein distances optimal mass transport problem refer reader recent review article kolouri references one-dimensional distributions case onedimensional continuous probability measures speciﬁcally interesting p-wasserstein distance closed form solution. precisely one-dimensional probability measures exists unique monotonically increasing transport pushes measure another. ixdτ cumulative distribution function deﬁne transport uniquely deﬁned consequently p-wasserstein distance calculated second line used change variable closed form solution p-wasserstein attractive property gives rise sliced-wasserstein distances. next review radon transform enables deﬁnition sliced p-wasserstein distance. radon transform overcome computational burden wasserstein minimization high-dimensional distributions propose sliced wasserstein distance method slices high-dimensional data distribution random projections minimizes wasserstein distance projected one-dimensional distributions respect parameters. note idea characterizing high-dimensional distribution random projections studied various work work instance minimizes norm between slices data distribution respect parameters. method however suffers shortcomings kl-divergence based methods. p-wasserstein distances generally optimal mass transportation problem recently gained plenty attention computer vision machine learning communities note p-wasserstein distances also used regard gmms however distance metric compare various models proposed method hand alternative framework ﬁtting data sliced p-wasserstein distances. follows ﬁrst formulate p-wasserstein distance radon transform sliced p-wasserstein distance section section reiterate connection k-means problem wasserstein means problem extend gmms formulate sliced wasserstein means problem. numerical experiments presented section finally conclude paper section section review preliminary concepts formulations needed develop framework. borel probability measures ﬁnite p’th moment deﬁned given metric space probability measures deﬁned corresponding probability density functions ixdx iydy. p-wasserstein distance deﬁned optimal mass transportation problem cost function that idea behind sliced p-wasserstein distance ﬁrst obtain family marginal distributions higher-dimensional probability distribution linear projections calculate distance input distributions functional p-wasserstein distance marginal distributions. sense distance obtained solving several one-dimensional optimal transport problems closed-form solutions. precisely sliced wasserstein distance deﬁned sliced p-wasserstein distance especially useful access samples high-dimensional pdfs kernel density estimation required estimate dimensional kernel density estimation slices much simpler task compared direct estimation samples. catch however dimensionality grows requires larger number projections estimate short reasonably smooth two-dimensional distribution approximated projections would require number projections approximate similarly smooth d-dimensional distribution later sections show projections could randomized stochastic gradient descent fashion learning gaussian mixture models. ﬁrst reiterate connection kmeans clustering algorithm wasserstein means problem extend connection gmms state need sliced wasserstein distance. samples rd×n k-means clustering algorithm seeks best points rd×k represent formally unit sphere note sake completeness note radon transform invertible linear transform denote inverse also known ﬁltered back projection algorithm deﬁned one-dimensional ﬁlter corresponding fourier transform c|ω|d− details) denotes convolution. radon transform inverse extensively used computerized axial tomography scans ﬁeld medical imaging x-ray measurements integrate tissue-absorption levels along hyper-planes provide tomographic image internal organs. note practice acquiring inﬁnite number projections feasible therefore integration equation replaced ﬁnite summation projection angles. formal measure theoretic deﬁnition radon transform probability measures could radon transform empirical pdfs radon transform simply follows equation however machine learning applications access distribution samples kernel density estimation could used scenarios approximate samples radon transform multivariate gaussians d-dimensional multivariate gaussian distribution mean covariance rd×d. slice/projection radon transform onedimensional normal distribution given linearity radon transform indicates slice d-dimensional one-dimensional component means variance σiθ. figure corresponding energy landscapes negative log-likelihood wasserstein means problem scenario scenario energy landscapes scaled shifted visualization purposes. p-wasserstein distance shown certain beneﬁts commonly used kl-divergence related distances/divergences distance itakura-saito distance) general model continuous smooth parameters locally lipschitz; therefore continuous differentiable everywhere true kl-divergence. addition scenarios distributions supported dimensional manifolds kl-divergence bregman divergences difﬁcult cost functions optimize given limited capture range. limitation ‘eulerian’ nature sense distributions compared ﬁxed spatial coordinates opposed p-wasserstein distance ‘lagrangian’ morphs distribution match another ﬁnding correspondences domain distributions practical sense beneﬁts wasserstein means problem maximum log-likelihood estimation study simple scenarios. ﬁrst scenario generate one-dimensional samples normal disbasis function kernel dirac delta function limit). then k-means problem alternatively solved minimizing statistical distance/divergence common choice distance/divergence kullback-leibler divergence alternatively p-wasserstein distance could used estimate parameters discuss beneﬁts p-wasserstein distance kl-divergence next sub-section. minimization known wasserstein means problem closely related wasserstein barycenter problem main difference works goal measure sets given dimensional distributions strategy could also extended clustering problem though formulations still signiﬁcantly different given inputs wasserstein distance different. note also k-means equivalent variational approximation isotropic gaussians therefore natural extension wasserstein means problem could formulated general distribution parametric follows equation solved αks. next describe beneﬁts using wasserstein distance equation general observed data compared common log-likelihood maximization schemes. absolutely continuous optimal transport exist even absolutely continuous moreover since slices/projections one-dimensional transport uniquely deﬁned minimization closed form solution calculated equation radon transformations equation formulation avoids optimization calculating wasserstein distance enables efﬁcient implementation clustering high-dimensional data. figure demonstrates high-level idea behind slicing high-dimensional pdfs minimizing pwasserstein distance slices components. moreover given high-dimensional nature problem estimating density requires large number samples however projections dimensional therefore critical large number samples estimate one-dimensional densities. optimization scheme obtain numerical optimization scheme minimizes problem equation ﬁrst discretize directions/projections. corresponds ﬁnite minimization following energy function sampling required equation good approximation sampling however becomes prohibitively expensive high-dimensional data. alternatively following approach presented utilize random samples minimization step approximate equation leads stochastic gradient descent scheme instead random sampling input data random sample projection angles. finally parameters updated emlike approach ﬁxed parameters calculate optimal transport random slices followed updating ﬁxed transport maps describe steps tribution assume known visualize negative log-likelihood wasserstein means problem function figure shows ﬁrst scenario corresponding energy landscapes negative log-likelihood wasserstein means problem. seen global optimum problems wasserstein means landscape less sensitive initial point hence gradient descent approach would easily converge optimal point regardless starting point. second scenario generated samples mixture one-dimensional gaussian distributions. next assumed mixture coefﬁcients standard deviations known visualized corresponding energy landscapes function clearly seen although global optimum problems same energy landscape wasserstein means problem suffer local minima much smoother. wasserstein means problem however suffers fact computationally expensive calculate high-dimensional true even using efﬁcient solvers including ones introduced cuturi solomon levy sliced wasserstein means propose approximation p-wasserstein distance using distance. substituting wasserstein distance equation distance leads sliced p-wasserstein means problem figure results runs em-gmm sw-gmm ﬁtting model modes ring-line-square dataset showing four samples random initializations histograms across runs negative log-likelihood ﬁtted model sliced-wasserstein distance ﬁtted model data distribution heavily parallelized iteration. note that also experimented adam optimizer improvements rmsprop. detailed update equations included supplementary materials. follows show solver estimating parameters action. various experiments three datasets test proposed method learning parameters. ﬁrst dataset two-dimensional data-point distribution consisting ring square connecting line show applicability method higher-dimensional datasets also used mnist dataset celebfaces attributes dataset started running simple experiment demonstrate robustness proposed formulation different initializations. test used two-dimensional dataset consisting ring square line connecting them. ﬁxed number modes experiment randomly initialized gmm. next initialization optimized parameters using algorithm well proposed method. repeated experiment times. notice derivative respect k’th component mixture model equation independent components. addition transport projection equation calculated independent projections. therefore optimization figure qualitative performance comparison mnist dataset method sw-gmm em-gmm showing decoded samples mode modes samples shown red. applied -dimensional embedding space figure shows sample results ﬁtted models algorithms moreover calculated histograms negative log-likelihood ﬁtted sliced-wasserstein distance ﬁtted empirical data distribution seen proposed formulation provides consistent model regardless initialization. initializations method achieved optimal negative log-likelihood compared em-gmm. addition negative log-likelihood sliced-wasserstein distance method smaller algorithm indicating solution closer global optimum analyzed performance proposed method modeling high-dimensional data distributions here using mnist dataset celeba dataset capture nonlinearity image data boost applicability gmms trained adversarial deep convolutional autoencoder image data. next modeled distribution data embedded space gmm. used generate samples embedding space consequently decoded generate synthetic images. learning compared algorithm proposed method sw-gmm. note entire pipeline unsupervised learning setting. figure demonstrates steps experiment provides qualitative measure generated samples mnist dataset. seen sw-gmm model leads visually appealing samples compared em-gmm. addition trained classiﬁer mnist training data. generated samples component classiﬁed samples measure ﬁdelity/pureness component. ideally component assigned single digit. found em-gmm components average pure compared pureness sw-gmm components. similarly deep convolutional autoencoder learned celeba face dataset trained embedding space. figure shows samples generated components learned proposed method note that figures provide qualitative measures well ﬁtting dataset. next provide quantitative measures ﬁtness gmms methods. used adversarial training neural networks provide goodness ﬁtness data distribution. short success fooling adversary network evaluation metric goodness gmm. deep discriminator/classiﬁer trained distinguish whether data point sampled actual data distribution gmm. fooling rate discriminator good measure ﬁtness higher error rate translates better figure deep discriminator learned classify whether input sampled true distribution data gmm. fooling rate discriminator corresponds ﬁtness score gmm. paper proposed novel algorithm estimating parameters minimization sliced p-wasserstein distance. iteration method projects high-dimensional data distribution small one-dimensional distributions utilizing random projections/slices radon transform estimates parameters one-dimensional projections. provide theoretical guarantee method convex fewer local minima empirical results suggest method robust compared kl-divergence-based methods including algorithm maximizing log-likelihood function. consistent ﬁnding showed p-wasserstein metrics result well-behaved energy landscapes. demonstrated robustness method three datasets two-dimensional ring-square-line distribution high-dimensional mnist celeba face datasets. finally used deep convolutional encoders provide embeddings datasets learned gmms embeddings emphasize method could applied embeddings including original data space. figure qualitative performance comparison method sw-gmm em-gmm showing decoded samples several components. images contrast enhanced visualization purposes. distribution data. figure shows idea behind experiment reports fooling rates three datasets used experiments. note sw-gmm consistently provides higher fooling rate indicating better datasets. moreover point low-dimensional ring-square-line dataset methods provide reasonable models dataset sw-gmm signiﬁcantly outperforms em-gmm higher-dimensional", "year": 2017}