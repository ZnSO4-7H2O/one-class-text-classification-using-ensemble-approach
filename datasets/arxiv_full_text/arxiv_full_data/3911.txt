{"title": "Vector Symbolic Architectures answer Jackendoff's challenges for  cognitive neuroscience", "tag": ["cs.NE", "cs.AI", "I.5.1; I.2.0, I.2.6"], "abstract": "Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.", "text": "jackendoff posed four challenges linguistic combinatoriality rules language present theories brain function. essence problems question rapid construction transformation compositional structures typically taken domain symbolic processing. typical connectionist approaches fail meet challenges dialogue linguistic theory cognitive neuroscience relatively unproductive importance problems widely recognised challenges answered technical innovation connectionist modelling. paper claims little-known family connectionist models able meet jackendoffs challenges. jackendoff posed four linguistic challenges cognitive neuroscience. holds language mental phenomenon linguistic functionality must neurally instantiated. however although great deal known functional localization various aspects language brain nothing known neurons instantiate details rules grammar lack progress cognitive term neuroscientists choice connectionism become synonymous single kind network model uses learning algorithm known backpropagation formulating challenges jackendoff draws heavily marcus argues multilayer perceptrons incapable symbol manipulation. marcus stresses anti-connectionist suggest adequate models cognition likely different less explored part space possible models lack progress cognitive neuroscientists also attempting solve wrong problems holding naive views linguistic phenomena apparent naivete conscious research strategy start simple intent current solutions scale ultimate needs. however strategy must fail initially chosen problems actually core ultimate problems. jackendoff's challenges attempt counter risk focussing attention functionality sees central linguistic phenomena. need challenges shown lack connectionist models able deal successfully fact that despite lack success challenges have widely recognized cognitive neuroscience community feldman broadcast challenges neural network modelling community connectionists mailing list. responses received unable standard connectionist techniques would meet jackendoffs challenges. challenges exclusively linguistic arguably fundamental cognition. something simply disregarded ignoring language certainly come fore dealing linguistic phenomena linguists deal every vector symbolic architectures littleknown class connectionist models directly implement functions usually taken form kernel symbolic processing enhancement tensor product variable binding networks like tensor product networks vsas create manipulate recursivelystructured representations natural direct connectionist training. however unlike tensor product networks vsas afford practical basis implementations require fixed dimension vector representations. fact vsas relate directly without training simple practical vector implementations core symbolic processing functionality suggests would provide fruitful connectionist basis implementation cognitive functionality. approach taken paper constrained required brevity. jackendoff's analysis linguistic challenges inadequacy typical cognitive neuroscience models accepted face value. range properties vsas discussed briefly. presentation vsas comparison connectionist approaches minimal topics adequately documented elsewhere. deal jackendoff's challenges turn showing answered vsas. vector symbolic architectures name vector symbolic architectures invented cover family related approaches. approaches easily implemented connectionist systems share commitment algebraic operations distributed representations highdimensional vectors. approaches descended smolensky's tensor product variable binding networks. smolensky demonstrated representation manipulation complex nested structures using connectionist methods possible. tensor product approach relies algebraic operations simple connectionist implementations. results structural manipulation occurring single pass system thus avoiding need prolonged learning unfortunately tensor product implementations thoroughly impractical vector dimensionality increases exponentially depth structures represented. consequently tensor product binding subsequent research. vsas retain advantages tensor product binding avoiding problem increasing vector dimensionality. connectionist systems entities represented vectors. tensor product binding representation association entities created outer product vectors representing entities. thus entity vectors dimensionality outer product dimensionality vsas overcome problem increasing dimensionality applying function elements outer product yield resultant vector dimensionality thus structures whether atomic complex represented vectors dimensionality. application vsas cognitive problems arguably complex application conventional multilayer perceptrons backpropagation. three separate levels description required. levels vector representation representational architecture cognitive architecture. paper like papers vsas deals primarily vector representation level touches representational architecture minimum extent necessary. contention vsas examined levels manifestly better suited connectionist alternatives extension analyses representational cognitive architecture levels reasonably expected consequence ongoing research program. currently available vsas employ three types operation vectors multiplication-like operator addition-like operator permutation-like operator. precise choice operator varies vsa. gayler implementations vsas. multiplicationlike operation used associate bind vectors. addition-like operation used superpose vectors set. permutation-like operation used quote protect vectors operations. sake concreteness examples coding scheme described coding elementwise multiplication used implement binding vectors; elementwise addition used implement permutation elements used implement quotation vectors. operations used compose decompose complex structures without requiring training analogous descriptions vsas found kanerva plate rachkovskij kussul description vsas vector representation level purely terms content vectors. corresponding operation representational architecture primitive. example implement binding would layer connectionist units taking inputs computing architectural primitives combined fixed circuit yield complete representational architecture. cognitive architecture level deals cognitive problem represented terms vector representations prior knowledge order obtain desired behaviour given representational architecture. jackendoffs first challenge arises observation linguistic representations must composed component representations. need combining independent bits single coherent percept recognized theory vision name binding problem typical presentation binding problem vision given terms associating attributes object example representing object square. challenge problem becomes obvious object example square blue circle. clear mechanism correct attributes associated object. nonetheless typical presentation problem given terms jackendoff refers repeatedly example sentence little star's beside star gives representation structure encoded sentence phonological syntactic semantic/ conceptual spatial levels informal count structure shows approximately tokens relations tokens. number bindings involved composition structure corresponding simple sentence obviously orders magnitude larger typical visual example binding. aspects binding problem must mentioned novelty speed. although individual components lowest level familiar total composite frequently novel. furthermore novel composite must constructed time takes comprehend single sentence. thus approaches relying extensive training construct composite structure implausible. separate inverse function required. case multiplication-like operation construed implementing number functions depending relationship vectors multiplied representational architecture level simple primitive layer connectionist units computing elementwise product inputs. binding occurs single pass units consequence algebraic relationship inputs outputs. thus fast oblivious novelty familiarity items bound. another consequence vector representation level vsas clearly capability deal principle binding problem. representational architecture primitives required characteristics speed ability cope novelty. trivially easy construct representational architecture implements fixed compositional template. however currently available representational architecture cognitive architecture implements fully variable input driven composition. current research aimed objective. connectionist recurrent associative memories work creating attractors corresponding items remembered. research aims create enhanced attractors corresponding novel valid compositions familiar items. memory would able recognise novel components generating mappings them. similar approach based vsas already successfully demonstrated visual domain response vsas superposition used represent multiple items representational space representational architecture level used items superposition operate superposed items simultaneously vsas identical vector values kept distinct representing separate parts representational architecture. given representational space vector values exist. superposition vector merely rescaling vector representation distinct entities. star star star distinct star virtue difference size. difference captured representing star composite structure includes size attribute. composite representations distinct superposed without losing identities. construction representational architecture implement make-frame function trivial. architecture consists permutation operator binding operator. unfortunately naive encoding scheme completely solve problem representing multiple instances. consider happens want represent little star star using encoding scheme rolefiller pair bound every rolefiller pair frame thus capturing holistic nature frame entity. note frames identical contents would identical third challenge arises productivity language. language users able recognise generate infinite variety utterances using finite resources. productivity construed arising variables; placeholders contain arbitrary values rule grammar construed template variables. variables instantiated remainder template relates values other. thus rule seen mechanism constructing recognising composite structure variables rule markers components productively replaced. variables rules generally typed. values variable instantiated must constrained. type variable constraint possible values. traditionally rules seen quite distinct structures operate types annotations variables rules. however distinction rules structures erased. thoroughly lexicalised grammar information rules expressed structural fragments combined procedural rule unify clip structures together) grammar compatibility constraints captured structure fragments. response smolensky demonstrated tensor product connectionist systems implement variables. vsas inherit capability. instantiation variable implemented binding vector representing variable vector representing value. value retrieved variable probing binding identity variable cue. unbind) note nothing special vector representing identity variable. another vector could easily representation complex structure representation atomic token. allows complex structures interpreted overlapping networks variable/value bindings example imperative programming language interpretation variable location hold value. however alternative interpretation concept variable based declarative programming languages congenial constraint-based grammar formalisms interpretation treats variables targets substitution. interpretation also implemented vsas. given capability component structure variable constraint component multiple occurrences structure substitution applied identically occurrences. apply-substitution suggests style processing literal episodes stored rules arise statistical regularities potential substitutions across episodes similar data oriented processing scha jackendoff's fourth challenge concerns transparency boundary working memory longterm memory. argues linguistic tasks require structures instantiated working memory long-term memory instantiations functionally equivalent aspects typical connectionist implementations suggest lack equivalence working memory long-term memory representations. working implemented memory representations using activation temporal synchrony whereas long-term memory implemented disparity using synaptic connectivity. implementation media suggests would difficult achieve functional equivalence. aspect speed again. linguistic phenomena require single trial learning single trial learning seems response jackendoff cast problem arising difference physical implementation working memory long-term memory. recast problem arising differences vector representation typical connectionist systems working level. represented vectors memory items represented matrices crucial difference physical implementation logical form representations course synaptic matrices reinterpreted vectors different dimensionality activation vectors. working memory items long-term memory items exist different incommensurable representational spaces. hardly surprising synaptic connectivities construed relations working memory items. mechanism items working memory create items working memory. vsas working memory items long-term memory items form vector representation level. represented vectors dimensionality. vectors represent items relationships items working memory equivalence logical form makes boundary working memory long-term memory transparent. implementation reflected represented persistence representations ability interact other. working memory items able interact items long-term memory whereas long-term memory items able interact items working memory. issue discussed respect different challenges gayler wales issue speed learning components speed binding speed laying trace. discussed response first challenge binding vsas arises algebraic properties operations occurs single pass architectural primitive. thus associations formed single trial. separate issue whether associations made persistent single trial. items made persistent working memory consists superposing association current activity vector. done single pass architectural primitive implements addition-like operation. could multiple example direct physical increment levels short-term potentiation representational units. long-term persistence achieved functionally equivalent ways different implementations example long-term potentiation representational units increments synaptic connectivities. jackendoff posed four challenges generally relevant language. functional capabilities required language arguably provided typical connectionist models. vsas little-known family connectionist architectures meet challenges. ability meet challenges arises algebraic properties vector representations. properties fact straightforward physical implementations suggest vsas ideal candidates cognitive modelling. however asked vsas relatively little-known. multiple reasons could presented offer one. vsas easy work vector representation level level primitives representational architecture difficult work level complete representational architectures cognitive architectures. typical connectionist architectures rely training procedures achieve effectiveness. however vsas provide opportunity training substitute architectural effectiveness. good performance depends good design rather automated training harder research task.", "year": 2004}