{"title": "Anytime Belief Propagation Using Sparse Domains", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Belief Propagation has been widely used for marginal inference, however it is slow on problems with large-domain variables and high-order factors. Previous work provides useful approximations to facilitate inference on such models, but lacks important anytime properties such as: 1) providing accurate and consistent marginals when stopped early, 2) improving the approximation when run longer, and 3) converging to the fixed point of BP. To this end, we propose a message passing algorithm that works on sparse (partially instantiated) domains, and converges to consistent marginals using dynamic message scheduling. The algorithm grows the sparse domains incrementally, selecting the next value to add using prioritization schemes based on the gradients of the marginal inference objective. Our experiments demonstrate local anytime consistency and fast convergence, providing significant speedups over BP to obtain low-error marginals: up to 25 times on grid models, and up to 6 times on a real-world natural language processing task.", "text": "marginal inference graphical models belief propagation algorithm choice impressive empirical results many models. models often contain many variables factors however domain variable neighborhood factors usually small. faced models contain variables large domains higher-order factors often intractable. primary reason unsuitable large domains cost message computations representation order cross-product neighbors’ domains. existing extensions address concern parameters deﬁne desired level approximation return approximate marginals convergence. results poor anytime behavior. since algorithms directly achieve desired approximation marginals inference cannot characterized often inconsistent other. further relationship parameter controls approximation quality intermediate marginals often unclear. result approaches suitable applications require consistent anytime marginals willing trade-off error speed example applications involve real-time tracking user interactions. need anytime algorithm interrupted obtain consistent marginals corresponding ﬁxed points well-deﬁned objective improve quality marginals execution period eventually obtaining marginals. work propose novel class message passing algorithms compute accurate anytimeconsistent marginals. initialized sparse domain variable approach alternates phases augmenting values sparse variable domains converging ﬁxed point approximate marginal inference objective deﬁned sparse domains. tighten approximate marginal inference objective selecting value sparse domains estimating impact adding value variational objective; accurate prioritization scheme depends instantiated domains requires runtime computation. also provide alternate prioritization scheme based gradient primal objective computed priori provides constant time selection value add. converge ﬁxed point approximate marginal objective perform message passing sparse domains. since naive schedules update messages round robin random fashion wasteful residual-based dynamic prioritization inference interrupted obtain consistent marginals ﬁxed point deﬁned instantiated domains longer execution results accurate marginals eventually optimizing objective. performing approximate variational inference represent approximate marginals contain elements every assignment variables µi∀xi factors d|xf|. minimizing divergence desired approximate realizable mean marginals results vectors entropy distribution yields polytope entropy need approximated order efﬁciently solve maximization. belief propagation approximates using local polytope constraints correspond local polytope messages correspond dual variables i.e. messages converge yedidia show marginals correspond saddle point i.e. ∇µlbp words convergence marginals locally consistent locally optimal. guaranteed converge global optimum does however often converges produces accurate marginals practice graphical models often deﬁned variables large domains factors neighbor many variables. message passing algorithms perform poorly models since complexity inference interrupted resulting marginals locally consistent correspond ﬁxed point well-deﬁned objective. here describe algorithm meets following desiderata anytime property results consistent marginals iterations improve accuracy marginals convergence marginals instead directly performing inference complete model approach maintains partial domains variable. message passing sparse domains converges ﬁxed point well-deﬁned objective followed incrementally growing domains resuming message passing domains till convergence. point marginals close ﬁxed point sparse objective tighten objective time growing domains. algorithm interrupted entire domains instantiated marginals converge ﬁxed point complete objective. first study propagation messages domains variables partially instantiated d|si| values associated instantiated domain variable message passing marginals corresponding non-instantiated domain zero i.e. setting values dual objective obtain optimization deﬁned sparse domains expected sparse domains much faster whole domains however optimizing different approximate objective. approximation tightened growing instantiated domains sparsity constraints removed obtain accurate marginals message passing newly instantiated domain converges. identifying values crucial good anytime performance propose approaches based gradient variational primal objectives. dynamic value prioritization inference sparse domains converges obtain marginals locally consistent deﬁne saddle point would like value removing constraint impact ∂µi|µi= approximate objective lsbp. words select gradient ∂lsbp although largest. derive ∂lsbp ignore term appears rest undeﬁned estimate gradient priority performing single round message update sparse domains. compute priority identify value highest priority improve search sorting factor scores further update priorities variables participated message passing. precomputed priorities values although dynamic strategy selects value improves approximation most also spends time computations result corresponding beneﬁt. alternative propose prioritization precomputes order values add; even though take current beliefs account resulting savings speed compensate. intuitively want values domain highest marginals ﬁnal solution. although ﬁnal marginals cannot computed directly estimate performing greedy coordinate ascent primal objective gradient w.r.t. zero priority obtain selected value added respective domain perform message passing described section converge ﬁxed point objective. focus message updates areas affected modiﬁed domains dynamic prioritization amongst messages dynamic range change messages choice message norm form bound reduction distance factor’s messages ﬁxed point allowing ways ﬁrst pick highest priority message since indicates part graph least locally consistent. second maximum priority indication convergence consistency; max-residual implies bound distance convergence. primary baseline belief propagation using random scheduling. also evaluate residual uses dynamic message scheduling. ﬁrst baseline uses sparsity truncated domain initialized domains contain ﬁxed fraction values selected according precomputed priorities modiﬁed inference. evaluate three variations framework. random instantiation baseline value added random followed priority based message passing. approach estimates gradient dual objective dynamic approach precomputes priorities fixed. grids ﬁrst testbed evaluation consists grid models consisting synthetically generated unary pairwise factors. runtime error approaches compared marginals obtained convergence signiﬁcantly better times faster obtain error truncbp efﬁcient however converges inaccurate solution suggesting preﬁxed sparsity domains desirable. similarly random initially fast since adding value signiﬁcant impact however selections become crucial rate convergence slows considerably. although fixed dynamic provide desirable trajectories fixed much faster initially constant time growth domains. however messages marginals become accurate dynamic prioritization utilizes eventually overtakes fixed approach. examine anytime local consistency examine average residuals figure since residuals imply consistent marginals objective deﬁned instantiated domain. approaches demonstrate residuals throughout residuals existing techniques remain signiﬁcantly higher lowering near convergence. total domain size varied figure observe although proposed approaches slower problems small domains obtain signiﬁcantly higher speedups larger domains joint information extraction also evaluate real-world task joint entity type prediction relation extraction entities appear sentence. domain sizes entity types relations respectively resulting neighbor assignments joint factors figure shows convergence time averaged runs. smaller sentences sparsity help much since converges iterations. however longer sentences containing many entities observe signiﬁcant speedup paper describe novel family anytime message passing algorithms designed marginal inference problems large domains. approaches maintain sparse domains efﬁciently compute updates quickly reach ﬁxed point approximate objective using dynamic message scheduling. further growing domains based gradient objective improve approximation iteratively eventually obtaining marginals. references james coughlan huiying shen. dynamic quantization belief propagation sparse spaces. computer vision image understanding april issn james coughlan sabino ferreira. finding deformable shapes using loopy belief alexander ihler john fisher alan willsky maxwell chickering. loopy belief propagation convergence effects message errors. journal machine learning research frank kschischang brendan frey hans andrea loeliger. factor graphs sum-product algorithm. ieee transactions information theory kevin murphy yair weiss michael jordan. loopy belief propagation approximate inference empirical study. uncertainty artiﬁcial intelligence pages nima noorshams martin wainwright. stochastic belief propagation low-complexity message-passing guarantees. communication control computing libin shen giorgio satta aravind joshi. guided learning bidirectional sequence proposed approach outlined algorithm initialize sparse domains using single value variable highest priority. domain priority queue contains priorities rest values variables remain ﬁxed updated depending prioritization scheme choice message passing uses dynamic message prioritization maintained message queue message passing converged obtain locally-consistent marginals select another value domains using value priority schemes continue till domains fully-instantiated. algorithm interrupted point return either current marginals last converged locally-consistent marginals. heap-based priority queue messages domain values update deletion take often smaller number factors total possible values.", "year": 2013}