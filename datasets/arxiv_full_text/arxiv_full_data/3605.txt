{"title": "Learning Privacy Preserving Encodings through Adversarial Training", "tag": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "abstract": "We present a framework to learn privacy-preserving encodings of images (or other high-dimensional data) to inhibit inference of a chosen private attribute. Rather than encoding a fixed dataset or inhibiting a fixed estimator, we aim to to learn an encoding function such that even after this function is fixed, an estimator with knowledge of the encoding is unable to learn to accurately predict the private attribute, when generalizing beyond a training set. We formulate this as adversarial optimization of an encoding function against a classifier for the private attribute, with both modeled as deep neural networks. We describe an optimization approach which successfully yields an encoder that permanently limits inference of the private attribute, while preserving either a generic notion of information, or the estimation of a different, desired, attribute. We experimentally validate the efficacy of our approach on private tasks of real-world complexity, by learning to prevent detection of scene classes from the Places-365 dataset.", "text": "present framework learn privacypreserving encodings images inhibit inference chosen private attribute. rather encoding ﬁxed dataset inhibiting ﬁxed estimator learn encoding function even function ﬁxed estimator knowledge encoding unable learn accurately predict private attribute generalizing beyond training set. formulate adversarial optimization encoding function classiﬁer private attribute modeled deep neural networks. describe optimization approach successfully yields encoder permanently limits inference private attribute preserving either generic notion information estimation different desired attribute. experimentally validate efﬁcacy approach private tasks real-world complexity learning prevent detection scene classes places- dataset. images videos forms high-dimensional data rich information environments represent. information used infer various environment attributes location shapes labels objects identities individuals classes activities actions etc. often desirable share data—with individuals un-trusted applications unsecured network etc.—without revealing values certain attributes user wish kept private. cases seek encoding transformation data privacydepartment electrical computer engineering university florida florida department computer science engineering washington university louis missouri usa. correspondence <f.pittalugauﬂ.edu>. preserving encoded data prevents inhibits estimation speciﬁc sensitive attributes still retains information environment—information useful inference other desirable attributes. relationship data attributes explicitly modeled possible derive explicit form encoding includes case goal encode ﬁxed dataset known values private label work consider case relationship data private attributes explicit instead learned training estimation function. seek encoding prevents inhibits trained estimator classiﬁer succeeding. note want encoding simply confounds ﬁxed classiﬁer rather want even encoding ﬁxed classiﬁer knowledge encoding therefore trained encoded training data unable make accurate predictions generalizing beyond training set. especially challenging high-dimensional signals like images contain multiple redundant cues towards environment attribute. speciﬁc transformations cause failures given estimators expect them estimators invariably recover transformations included training address issue present formulation learning encoding function adversarial training classiﬁer simultaneously learning succeed inferring private attribute encoded data. encoder turn trains prevent inference also maintaining notion utility—a generic objective maintaining variance outputs optionally promoting success second classiﬁer training different attribute. deep neural networks model classiﬁer well encoder carry optimization using gradient-based updates. figure privacy-preserving encodings inhibiting scene class detection. show positive example images four different categories places- dataset arch army-base airport air-ﬁeld. far-right column show test accuracy four tasks cast binary scene classiﬁcation setting. shows original images second show output ﬁlter size encoder’s receptive ﬁeld applied original images. blurred images baseline privacy preservation. last rows show visualizations output encoder trained optimization goal inhibiting negative task. arch army-base airport terminal airﬁeld optimize using generic objective maintaining variance encoder output. army-base/airport terminal addition generic objective optimize promote desired task. text indicates negative task trying inhibit green text indicates positive task trying promote. note negative tasks beat blurring baseline. promote desired task also better addition inhibiting negative task covery private attributes also maintaining notion utility. demonstrate efﬁcacy approach validation tasks real-world complexity. illustrated fig. approach able successfully learn encoders inhibit detection different scene categories encoded scenes places- dataset exist elegant approaches privacy provide formal guarantees relationship between data elements sensitive attributes precisely characterized. true also special case privacy preserving task aimed ﬁxed dataset—in case relationship simply enumerated list samples attribute values approaches k-anonymity employed. focus paper however applications relationship precisely known data elements censored high-dimensional contain multiple redundant cues towards private label learned estimator could trained exploit. prior work achieving privacy achieving privacy data especially images videos relied domain knowledge hand-crafted approaches— pixelation blurring face/object replacement etc.— degrade sensitive information methods effective many practical settings assume speciﬁc estimator algorithm approach used infer private attribute seek degrade corresponding cues. contrast assume adversary seeking recover private attribute learn estimator speciﬁcally encoding seek limit success. makes goal learning encoding signiﬁcantly challenging since modern classiﬁers based deep neural networks able learn make accurate predictions even severely degraded data also worth noting recent works ﬁnding adversarial examples methods learn perturbations cause incorrect predictions also trained ﬁxed classiﬁers. motivates framework training encoding function adversarially classiﬁer simultaneously trained predict private attribute. approach motivated recent success adversarial training learning generative adversarial networks demonstrated feasibility using stochastic gradient descent optimize min-max objective involving deep neural networks competing goals. nevertheless optimization process known unstable moreover goal reach point encoding function maintains success even ﬁxed adversary continues train. stationary point exists theoretically given enough expressive power competing networks rarely achieved practice training gans. contribution work developing stable optimization approach lasting effect classiﬁcation ability private attributes. closest formulations techniques also adversarial optimization learn image transformations prevent classiﬁer solving sensitive task along objective transformed image close original possible methods provide interesting proof concept target relatively simple private tasks— namely preventing detection synthetically superimposed text codes show adversarial training learns detect blur relevant regions. contrast present optimization approach demonstrated work training classiﬁers complex real-world tasks— namely scene recognition—on natural images. particular show learned encoders succeed even comsection describe overall formulation introduce framework learning privacy-preserving encoding functions respect speciﬁc sensitive tasks. consider following setting training encoder training labeled values private attribute. encoder trained seek limit ability adversary knowledge encoding function train estimator private attribute. particular assume encoder ﬁxed adversary able train estimator encoded labeled training seek restrict performance estimator beyond training set. measure performance encoded validation test sets. note seek prevent estimator performing well training itself e.g. memorization. goal limit generalization accuracy. privacy adversarial objective formally seek encoding function maps high-dimensional input encoded value goal preventing estimation private attribute encoded image consider parameterized estimator learnable parameters produces estimate encoded image then loss function deﬁne desired encoding function clearly trivial solution optimization namely encoder outputs constant value independent input. avoid this impose constraint statistics encoded outputs. speciﬁcally require average element encoded output zero mean unit variance i.e. denotes element therefore encoder constrained produce outputs reasonable diversity even tries remove information regarding label constraint aimed maintaining information content encoded outputs outputs informative estimating attributes majority experiments generic constraint framework also allows optimizing utility respective speciﬁc desirable attributes. speciﬁcally attribute deﬁne corresponding similar enforce zero mean unit variance constraints encoder outputs case well. note objective involves adversarial optimization collaboration encoder desirable attribute classiﬁer parameters classiﬁer parameters private attribute. deep-neural networks model estimators particular consider binary attributes corresponds classiﬁer trained log-loss also deep neural network model encoder maximization weights given chosen architecture. work consider case inputs images want encoder also output image-shaped encodings here convolutional layers spatial pooling layers classiﬁer encoder architectures. encoder employs downsampling produce lower resolution encoded outputs ﬁnal layer followed tanh non-linearity produce outputs saturate gradient updates minimizing orbits. training encoder proceeds applying alternating gradient updates encoder classiﬁer case classiﬁers. gradients computed mini-batches training data classiﬁcation tasks. empirically helpful different batches update classiﬁers update encoder. note updating encoder requires back-propagating gradients classiﬁers. classiﬁers take gradient steps minimize losses respect true labels encoded data. optimizing desirable attribute classifer encoder’s update also based minimizing classiﬁer’s loss however computing gradients respect private attribute classiﬁer negative loss—as indicated even logloss respect incorrect label used training approaches encoder tries push private classiﬁer predict incorrect label large subset images high conﬁdence. however encoder achieved goal classiﬁer able quickly recover simply reversing labels leads encoder classiﬁer making orbits during encoder training classiﬁer able easily recover encoder ﬁxed. therefore consider different loss updating encoder private task update minimize loss classiﬁer respect opposite current prediction. note stationary point loss classiﬁer predicting equal probabilities true false images implying positive negative examples indistinguishable. empirically loss proves crucial learning encoders permanent effect private classiﬁcation ability. stability batch normalization. mentioned above trivial solution encoder output constant images. indeed without constraints training moves rapidly degenerate solution. enforce zero mean unit variance constraints encoder outputs using batch statistics proxy population statistics i.e. simply employ batchnormalization output ﬁnal layer without learnable weight bias parameters. note traditional batch-normalization convolutional outputs imposes constraints mean variances computed across different images batch also across spatial locations. however framework constraints deﬁned every element encoded output i.e. require values speciﬁc channel speciﬁc pixel location averaged across images zero mean unit variance. indeed constraint necessary because without encoder learns simply produce encoded output input images variation across spatial locations output across different inputs. therefore used modiﬁed form per-location batch normalization treated output ﬁnal convolutional layer single large vector computed statistics averaging batch figure encoder classiﬁer architectures. convolutional architectures encoder classiﬁer networks. note encoder location speciﬁc batch normalization without biases every layer prevent encoder saturating trivial solution producing constant image. spatial dimensions. however still remains strong gradient signal often causes intermediate layer activations saturate—since loss function continues direct encoder towards producing trivial solution constant output. even batch normalization ﬁnal layer often found hidden layer activations would driven saturation causing constant output ﬁnal layer moreover activations hidden layer saturated weights layer stopped receiving gradients causing encoder remain stuck solution. address this included per-location batch normalization every layer removed biases together encoder network. ensured outputs intermediate layers maintained diversity throughout training avoided saturating constant output. experimental results preliminaries. evaluate framework ability inhibit identiﬁcation speciﬁc scene categories using images places- dataset frame binary classiﬁcation task i.e. whether image belongs speciﬁc category not. training ﬁxing encoder inhibit speciﬁc category evaluate ability train classiﬁer category. also evaluate impact encoder ability train classiﬁers categories. identiﬁcation task train evaluate classiﬁers balanced dataset half images belong category—therefore prior task chance. sets constructed groups categories each places-. negative examples category identiﬁcation task uniformly sampled nine categories group. therefore overlap tasks identifying categories within group different groups. note construct non-overlapping training validation testing sets ofﬁcial places- training set. assumed input encoder would images ﬁxed resolution size constructed places- images—with random scaling crops data augmentation training ﬁxed scale center-crop evaluation. encoder produced three channel images outputs provided input classiﬁcation networks. architectures encoder classiﬁcation networks shown fig. described sec. encoder used locationspeciﬁc batch normalization without biases layer. encoder training. learn four encoders trained inhibit identiﬁcation different scene class—arch army base airport terminal airﬁeld—all group encoders trained based generic variance constraint. also train ﬁfth encoder inhibit inference army base class support inference airport terminal class based train encoders described sec. adam optimizer train total iterations. group arch army base airport terminal airﬁeld alley arena hockey amusement park apartment building aquarium arena rodeo. group ampitheater auto showroom airplane cabin archaelogical excavation studtio artists loft assembly line athletic ﬁeld atrium auto factory. figure evolution training validation accuracy encoder training. focus three encoders. shows evolution different encoder. rows ﬁrst column shows training validation accuracy classiﬁers training adversarially encoder aims inhibit army base ﬁrst airport terminal second. note encoder succeeds inhibiting networks solving respective tasks. next columns show even encoder ﬁxed classiﬁers trained access encoder training data still fail. additionally show classiﬁers trained baselines converge much faster better performance trained ﬁxed encoders. last depict encoder trained inhibit army base promoting airport terminal. last ﬁrst column show encoder training adversarially network aims solve private task collusion network aims solve desired task note encoder succeeds inhibiting private task allowing desired task succeed. next columns last encoder ﬁxed. classiﬁers trained access ﬁxed encoder still fail. addition next columns last show classiﬁers trained baselines learn earlier higher rate trained ﬁxed encoder. learning rate classiﬁer kept ﬁxed encoder begin rate drop factor every iterations. empirically dropping learning rate signiﬁcant effect subsequent classiﬁcation performance encoder ﬁxed since encoder trains inhibit classiﬁer able adapt increasingly faster relative rates. private classiﬁer accuracy value even learning rate continues drop—although occasional increases private accuracy around learning rate drops. learned encoding functions visualized fig. show examples typically encoded images encoder. better show variability images encoder output image mapping value location channel histogram equalized value—i.e. compute histogram values location channel across images value quantile histogram. table test scene identiﬁcation accuracies different encoders. columns depict binary classiﬁcation tasks create categories taken places- dataset. ﬁrst show test accuracy vgg-like network trained original images second show test accuracy classiﬁer network shown fig. trained blurred images. learned results show test accuracy classiﬁer network shown fig. classiﬁer trained outputs four ﬁxed encoders trained optimization arch army-base airport air-ﬁeld private tasks. ﬁrst four encoders optimize using generic objective maintaining variance encoder output. boxes show inhibited task’s test accuracy beats baselines. addition generic object preserves information tasks categories. last shows test accuracy outputs ﬁxed encoder trained generic objective optimized promote desired task. shows inhibited task beats baselines green shows test accuracy desired task beats naive blur comparable identity baseline. evaluation. trained encoders next evaluate ability classiﬁer learn solve label classiﬁcation tasks encoded images. evaluate this train classiﬁers scratch task encoder. addition learned encoders above also train classiﬁers blur baseline—which produces images applying averaging ﬁlter downsampling factor well classiﬁers trained original images train classiﬁers also using adam initial learning rate learned encoders blur baseline classiﬁers original images. also continue training dropping learning rate factor validation loss saturates. second third columns fig. demonstrate behavior classiﬁer trained detect army base airport terminal classes outputs three encoders—those trained army base airport terminal trained army base support airport terminal. show validation training losses classiﬁers compare losses identity classiﬁers case learned classiﬁers private tasks perform better within encoder training saturate lower validation accuracy values addition degrading achievable accuracy private tasks also learned encoders extract higher computational cost training— proceeds signiﬁcantly slowly comparison original images also interesting compare classiﬁer training airport terminal task encoders trained inhibit army base. ﬁrst encoder generic variance constraint also removes information relevant airport terminal affects inference. trained explicitly support desirable class however leads much better classiﬁcation accuracy faster convergence—albeit increase achievable accuracy private task. table summarizes ﬁnal test accuracy classiﬁers number tasks trained encoders blur baseline. include four tasks trained encoders tasks groups also report validation accuracy different points training table every case learned encoder leads considerable degradation accuracy corresponding private task blur baseline table validation accuracy training iterations. table compare training iterations baselines learned encoders trained using optimization table ﬁnal represents validation accuracies convergence. baselines achieve high validation accuracies within iterations much iterations. hand ﬁxed encoders delay convergence signiﬁcantly iterations networks training private tasks still training converge. networks eventually converge ﬁnal value shown lower baselines. last show validation accuracies encoder aims inhibit private task also prioritize maintaining information desired task. network training private task converges positive task validation accuracy comparable baselines. learned encoders different degrees success censoring different tasks. interestingly censoring easy private tasks leads overall poorer performance tasks well likely encoder forced remove information also useful tasks. meanwhile encoder trained task degrades quickly arch able automatically preserve information encoded outputs. results demonstrate method able automatically discover remove information related sensitive private attributes permanently inhibiting inference attributes—both terms achievable accuracy slower training. data related sensitive private tasks. framework achieved formulating adversarial optimization problem encoding function estimators private tasks. modeled encoder estimator neural networks described effective strategy optimization. experimental results tasks real-world complexity validated efﬁcacy approach. note constrain encoded outputs appear natural resemble original data. consequently framework requires classiﬁers desirable tasks also retrained. others brkic successfully incorporated requirements different approaches privacy censorship goals future work extend framework similar manner. acknowledgments. received support work u.s. department homeland security grant award number -dn--ari. views conclusions contained document authors interpreted necessarily representing ofﬁcial policies either expressed implied department homeland security. thanks nvidia corporation donation titan used research. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems raval nisarg machanavajjhala ashwin landon protecting visual secrets using adversarial nets. computer vision pattern recognition workshops ieee conference ieee zhou bolei lapedriza agata khosla aditya oliva aude torralba antonio. places million image database scene recognition. ieee transactions pattern analysis machine intelligence", "year": 2018}