{"title": "A Spectral Algorithm for Learning Hidden Markov Models", "tag": ["cs.LG", "cs.AI"], "abstract": "Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.", "text": "hidden markov models fundamental widely used statistical tools modeling discrete time series. general learning hmms data computationally hard practitioners typically resort search heuristics suﬀer usual local optima issues. prove natural separation condition eﬃcient provably correct algorithm learning hmms. sample complexity algorithm explicitly depend number distinct observations—it implicitly depends quantity spectral properties underlying hmm. makes algorithm particularly applicable settings large number observations natural language processing space observation sometimes words language. algorithm also simple employing singular value decomposition matrix multiplications. hidden markov models workhorse statistical model discrete time series widely diverse applications including automatic speech recognition natural language processing genomic sequence modeling. model discrete hidden state evolves according markovian dynamics observations particular time depend hidden state time. learning problem estimate model observation samples underlying distribution. thus predominant learning algorithms local search heuristics baum-welch algorithm surprising practical algorithms resorted heuristics general learning problem shown hard cryptographic assumptions fortunately hardness results hmms seem divorced likely encounter practical applications. situation many ways analogous learning mixture distributions samples underlying distribution. there general problem also believed hard. however much recent progress made certain separation assumptions made respect component mixture distributions roughly speaking separation assumptions imply high probability given point sampled distribution determine mixture component generated point. fact prevalent sentiment often interested clustering separation condition holds. much theoretical work focused small separation still permit eﬃcient algorithm recover model. present simple eﬃcient algorithm learning hmms certain natural separation condition. provide results learning. ﬁrst approximate joint distribution observation sequences length increases approximation quality degrades polynomially. second result approximating conditional distribution future observation conditioned history observations. show error asymptotically bounded—i.e. conditioned observations prior time error predicting t-th outcome controlled. algorithm thought ‘improperly’ learning explicitly recover transition observation models. however model maintain hidden state representation closely related hmm’s used interpreting hidden state. separation condition require spectral condition observation matrix transition matrix. roughly speaking require observation distributions arising distinct hidden states distinct requirement thought weaker separation condition clustering observation distributions overlap quite bit—given observation necessarily information determine hidden state generated also spectral condition correlation adjacent observations. believe conditions quite reasonable many practical applications. furthermore given analysis extensions algorithm relax assumptions possible. algorithm present polynomial sample computational complexity. computationally algorithm quite simple—at core singular value decomposition correlation matrix past future observations. viewed canonical correlation analysis past future observations. sample complexity results present explicitly depend number distinct observations; rather implicitly depend number spectral properties hmm. makes algorithm particularly applicable settings large number observations space observations sometimes words language. ideas closely related work. ﬁrst comes subspace identiﬁcation literature control theory second idea that rather explicitly modeling hidden states represent probabilities sequences observations products matrix observation operators idea dates back literature multiplicity automata subspace identiﬁcation methods used control theory spectral approaches discover relationship hidden states observations. literature relationship discovered linear dynamical systems kalman ﬁlters. basic idea relationship observations hidden states often discovered spectral/svd methods correlating past future observations however algorithms presented literature cannot directly used learn hmms assume additive noise models noise distributions independent underlying states models suitable hmms setting idea performing past future observations uncover information observation process state-independent additive noise condition avoided second idea. second idea represent probability sequences products matrix operators literature multiplicity automata discussion relationship). idea re-used observable operator model jaeger predictive state representations littman closely related model hmms. fact former work jaeger provides non-iterative algorithm learning hmms asymptotic analysis. however algorithm assumed knowing ‘characteristic events’ rather strong assumption eﬀectively reveals relationship hidden states observations. algorithm problem avoided ﬁrst idea. techniques work tracking belief states used here. discussed earlier provide result showing model’s conditional distributions observations asymptotically diverge. result proven approximate model already known. roughly speaking reason error diverge previous observations always revealing information next observation; appropriate contraction property would expect errors diverge. work borrows contraction analysis. among recent eﬀorts various communities previous eﬃcient algorithm shown pac-learn hmms setting similar mossel roch algorithm hmms specialization general method learning phylogenetic trees leaf observations. algorithm rely rank condition compute similar statistics diﬀer signiﬁcant regards. first concerned large observation spaces thus algorithm assumes state observation spaces dimension. addition take ambitious approach learning observation transition matrices explicitly unfortunately results less sample-eﬃcient algorithm injects noise artiﬁcially spread apart eigenspectrum probability matrix. algorithm avoids recovering observation transition matrix explicitly instead uses subspace identiﬁcation learn alternative representation. appendix discuss step also show technique conjunction algorithm recover observation transition matrices. algorithm rely extra step—we believe generally unstable—but taken desired. initial state distribution conditional independence properties satisﬁes conditioned previous hidden state current hidden state sampled independently events history; conditioned current hidden state current observation sampled independently events history. conditional independence properties imply fully characterize probability distribution sequence states observations. useful computing probability sequences terms ‘observation operators’ idea dates back literature multiplicity automata following lemma straightforward verify already used lemma vector all-ones vector denote sequence reverse sequence subscript mean product quantities indexed sequence elements. example probability maxt~π. denote probability vector calculation lemma written arrow distinguishing random hidden state variable additional notation used theorem statements proofs listed table rank condition rules problematic case state output distribution equal convex combination states’ output distributions. case could cause learner confuse state mixture states. mentioned before general task learning hmms hard cryptographic assumptions; rank condition natural exclude malicious instances created hardness reduction. rank condition relaxed simple modiﬁcation algorithm looks multiple observation symbols simultaneously form probability estimation tables. example hidden states identical observation probability diﬀerent transition probabilities diﬀerentiated using consecutive observations. although analysis applied case minimal modiﬁcations clarity state results algorithm estimates probability tables rows columns corresponding single observations. learning model similar paclearning discrete probability distributions. assume sample observation sequences hmm. particular assume sequence generated starting initial state distribution setting valid practical applications including speech recognition natural language processing sequence modeling multiple independent sequences available. simplicity paper analyzes algorithm uses initial observations sequence ignores rest. avoid using concentration bounds complicated mixing conditions markov chains sample complexity calculation conditions essential main ideas present. practice however full sequences form probability estimation tables required algorithm. scenarios single long sequence suﬃcient learning eﬀective sample size simply discounted mixing rate underlying markov chain. typical strategy learning hmms estimate observation transition probabilities hidden state however since hidden states directly observed learner often resorts heuristics alternate however condition hmms admit eﬃciently learnable parameterization depends observable quantities. quantities estimated data learning representation avoids guesswork hidden states thus allows algorithms strong guarantees success. addition joint probabilities compute conditional probabilities using observable representation. conditional ‘internal states’ depend history observations. emphasize states fact probability distributions hidden states lemma initial state representation previous section suggests algorithm detailed figure simply uses random samples estimate model parameters. note practice knowing essential method presented tolerates models exactly hmms parameter tuned using cross-validation. discussed earlier requirement independent samples convenience sample complexity analysis. conditional probabilities require extra work compute normalization factor. however analysis shows normalization factor always close safely omitted many applications. note algorithm explicitly ensure predicted probabilities range dreaded problem faced methods learning using general operator models jaeger number heuristic coping problem proposed applicable recent developments). brieﬂy mention case joint probability prediction clipping predictions interval increase accuracy accuracy guarantee explicitly requires predicted probabilities non-zero. present main results. ﬁrst result guarantee accuracy joint probability estimates observation sequences. second result concerns accuracy conditional probability estimates much delicate quantity bound conditioning unlikely events. also remark probability distribution approximately modeled results degrade gracefully based approximation quality. words minimum number observations account total probability mass. clearly often much smaller real applications. form frequency k-th frequently observed symbol. becomes independent number observations means problems analysis leads sample complexity bound cumulative distribution independent useful domains large natural language processing. main challenge proving theorem understanding estimation errors accumulate algorithm’s probability calculation. would less problematic estimates usual parameters fully observable representation forces deal cumbersome matrix vector products. intuition conﬁrmed even-dar showed approximate model certain conditions conditional prediction diverge. condition positivity ‘value observation’ deﬁned note σm/√n guaranteed positive condition however interpret quantity consider distributions hidden states ~hbh γk~h −bhk. regarding true hidden state distribution estimated hidden state distribution inequality gives lower bound error estimated observation distributions words observation process average reveal errors hidden state estimation. work uses contraction property show prediction errors diverge. setting diﬃcult explicitly estimate explicitly maintain distributions hidden states. justify choice error measure note problem bounding errors conditional probabilities complicated issue that long condition probability event. thus need control relative accuracy predictions. makes kl-divergence natural choice error measure. unfortunately conditions naturally interpreted terms spectral normed quantities switching back forth errors pinsker-style inequalities clear signiﬁcantly better guarantee could obtained pure error analysis analysis dealt problem dividing zero explicitly modifying approximate model never assigns probability event zero setting condition ensures true model never assigns probability event zero. relax condition somewhat though discuss here. following initial publication work siddiqi boots gordon proposed various extensions learnhmm algorithm analysis siddiqi first show model parameterization used algorithm fact captures class hmms rank transition matrices general class hmms hidden states. second propose extensions using longer sequences parameter estimation also handling real-valued observations. extensions prove useful synthetic experiments application tracking video data. recent work song boots siddiqi gordon smola provides kernelization model parameterization context hilbert space embeddings probability distributions extends various aspects learnhmm algorithm analysis setting song extension also shown advantageous number applications. rn×m matrix left singular vectors ﬁrst lemma implies suﬃciently close i.e. small enough diﬀerence projecting range range small. particular invertible nearly well-conditioned εσm. proof. assumptions imply since second claim immediate corollary rn×m matrix left singular vectors kxkq kxk√ corollary fact remaining argue estimated parameters bb∞bbxbb close following true parameters observable representation used need quantify estimation errors propagate probability calculation. joint probability length sequence computed multiplying together matrices danger magnifying estimation errors exponentially. fortunately case following lemma shows errors accumulate roughly additively. axt. since non-negative entries kaxt~vk |~v| denotes element-wise absolute value fact mt|~v| m|~v| k~vk inductive hypothesis imply double bounded combining bounds completes induction. lemma corollary used prove contraction property kldivergence true hidden states estimated hidden states. analysis shares ideas even-dar though added diﬃculty fact state maintained algorithm probability distribution. proof. prove bound proceed induction base case lemmas imply −bgk non-decreasing pc/c pc/c together imply pc/c. inductive step uses fact γ/)) max)). prove bound kl||cpr). first denote lemma probability vector k~a/ proof. first easy check ~⊤~a ~ai/ ~bi}. |~ai/ −~bi| ~ai/ −~bi ~ai/ −~bi −~bi |~ai −~bi| c~ai. similarly |~bi −~ai/| −~ai/ −~ai/ |~bi −~ai| c~ai. therefore k~a/ lemma probability vectors. exists log. authors would like thank john langford ruslan salakhutdinov earlier discussions using bottleneck methods learn nonlinear dynamic systems; linearization bottleneck idea basis paper. also thank yishay mansour pointing hardness results learning hmms. finally thank geoﬀ gordon byron boots sajid siddiqi alerting error previous version paper. work completed intern tti-c partially supported following grants afosr- dms- iis-.", "year": 2008}