{"title": "Reinforcement Learning via Recurrent Convolutional Neural Networks", "tag": ["cs.LG", "cs.AI", "68T05"], "abstract": "Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.", "text": "problems within framework deep learning architectures; particular recurrent convolutional neural networks explicitly connecting steps solving mdps architectural elements rcnns representation inherits properties networks; allowing backpropagation deﬁned rcnns elegant solution model-based problems. representation also exploits inherent structure reduce cost replanning incentivizing model-based approaches. contributions paper hence three-fold. deﬁne value iteration rcnn whose forward passes carry value iteration efﬁciently obtain optimal policy. second deﬁne belief propagation rcnn update beliefs state bayes filter. backpropagation network learns underlying transition model agent partially observable environments. finally deﬁne qmdp rcnn combining rcnn rcnn. qmdp rcnn computes optimal choices actions given belief state. backpropagation qmdp rcnn learns reward function expert agent learning demonstration inverse reinforcement learning setting learnt reward function transition model used re-plan rcnn qmdp rcnn make optimal action choices beliefs learnt q-values. note follows approach mathematically similar rcnn; however differs modelfree approach. model-based approach followed learns models transferable across environments agents. further qmdp rcnn approach proposed follows naturally fully connected layer used gradient updates qmdp rcnn adopt intuitive form contributes intuitive understanding action choices qmdp rcnn compared evaluate rcnns proposed simulation given task robot planning problem. demonstrate capacity rcnn reduce cost re-planning signiﬁcantly reducing execution time compared classical value iteration. evaluate abstract—deep reinforcement learning enabled learning policies complex tasks partially observable environments without explicitly learning underlying model tasks. model-free methods achieve considerable performance often ignore structure task. present natural representation reinforcement learning problems using recurrent convolutional neural networks better exploit inherent structure. deﬁne rcnns whose forward passes execute efﬁcient value iteration propagate beliefs state partially observable environments choose optimal actions respectively. backpropagating gradients rcnns allows system explicitly learn transition model reward function associated underlying serving elegant alternative classical model-based evaluate proposed algorithms simulation considering robot planning problem. demonstrate capability framework reduce cost re-planning learn accurate models ﬁnally re-plan learnt models achieve near-optimal policies. deep reinforcement learning algorithms exploit model-free reinforcement learning achieve high levels performance variety tasks often human experts domain methods deep networks either approximate actionvalue functions deep networks directly parametrizing policy policy gradient methods methods adopt model-free approaches order generalize performance across various tasks difﬁcult intuitively understand reasoning approaches making particular choice action since often ignore underlying structure tasks. contrast model-based methods exploit inherent structure make decisions based domain knowledge. estimates transition model reward function associated underlying markov decision process transferable across environments agents provide insight system’s choice actions. signiﬁcant deterrent model-based indirect nature learning estimated model; subsequent planning required obtain optimal policy jointly exploit inherent structure overcome indirect nature present novel fusion reinforcement deep learning representing classical solutions fig. schematic representation value iteration recurrent convolution network. notice stages presentconvolution fixed-bias max-pooling recurrence. rcnn facilitates natural representation bellman update single rcnn layer. rcnn based accuracy learnt transition models ground truth models show appreciably outperforms naive model-based methods partially observable environments. experiments ﬁnally demonstrate qmdp rcnn able generate policies re-planning learnt models accurately represent policies generated ground truth models. section formulate value iteration recurrent convolution hence present value iteration rcnn carry value iteration. consider standard markov decision process consisting -dimensional state space size state action space size actions transition model reward function discount factor value iteration typically used solving optimal policies invokes bellman update equation actions correspond transitions agent’s state dictated internal dynamics agent case physical robots restricted robot’s conﬁguration actuator capacities. immediately move current state away state. allows deﬁne neighborhood centred around state agent ﬁnite probability transitioning states within mathematically transition model location occurring transition. example spatial differential drive robot’s dynamics independent robot’s position. assume transition model stationary incorporating appropriate state boundary conditions. visualize neighborhood restricted transition model centred around deﬁning ﬂipped transition model )nt−mnt−n indexing value iteration rcnn deﬁne value iteration rcnn represent classical value iteration based correlation bellman update equation architectural elements rcnn. equation thought single layer recurrent convolutional neural network consisting following stages convolutional stage convolution )∗vk) recurrence stage incremented successive iterations back ‘input’ network introducing recurrence relation network. think single-channel nd×nd image. corresponds series transition ﬁlters size convolved image. values transition ﬁlters correspond directly transition probabilities states upon taking action note convolutional transition ﬁlters naturally capture spatial invariance transitions virtue lateral parameter sharing inherent convolutional networks. further representation transition ﬁlter corresponds directly particular action. unique oneto-one mapping actions ﬁlters proves useful learning models demonstrate section iii. finally rcnn completely differentiable trained applying backpropagation section present belief propagation rcnn represent bayes ﬁlter belief update within architecture rcnn hence learn transition models. agent make optimal choices actions partially observable environments accurate belief state required. upon taking action receiving observation bayes ﬁlter propagate belief propagation rcnn deﬁne belief propagation rcnn represent bayes filter belief update analogous representation value iteration rcnn. single layer rcnn represents equation consisting following stages convolution stage convolution rep) recurrence stage network output input next time step forming output recurrence network. forward passes rcnn propagate beliefs state given choice action received observation belief state given instant treated single channel image convolved single transition ﬁlter corresponding action executed difference rcnn rcnn single transition ﬁlter activated belief propagation since agent execute single action given time instant. rcnn also completely differentiable. training loss learning weights rcnn backpropagation learns transition model agent. objective thus learn transition model ated one-hot representations observed state. training rcnn randomly initialized transition model magniﬁes initial uncertainty belief propagated forward time leading instability learning transition model. teacher forcing adopted handle uncertainty; thus target belief rather network output propagated input next time step. since target belief independent initial uncertainty transition model network able learn meaningful ﬁlter values. teacher forcing target recurrence also decouples consecutive time steps reducing backpropagation time backpropagation data points generated sequentially. fig. schematic representation belief propagation recurrent convolution network. forward passes represented black arrows backpropagation symbolized blue arrows. belief terms observation model associated probability discrete bayes ﬁlter depicted note denominator equivalent implemented normalization factor bayes filter convolution element-wise product represent motion update traditional bayes ﬁlter convolution analogous representation )vk) step value iteration equation transitions bayes ﬁlter occur contrast ‘backward’ passes executed value iteration suggests transition model centred around rather represent write intermediate belief b)pq v=−w bp−uq−v expressed convolution bayes filter correction update consider simple observation model observation state indexed agent observes state given actual state robot probability analogous neighborhood transition model introduce neighborhood observation model centred around observed state within neighborhood thus element-wise product since observation also centred around express b)ij oi−xj−y b)ij corresponds normalized element-wise product upon combining motion update convolution correction update element-wise product represent bayes filter convolutional stage followed element-wise product. have fig. schematic representation qmdp rcnn combination value iteration rcnn belief propagation rcnn. notice following stages inner product sum-pooling softmax stage providing output actions. gradients propagated reward function alternately propagated q-value estimate. forward passes qmdp rcnn used planning partially observable domains backpropagation learns reward function mdp. rcnn component trained parallel qmdp rcnn. pomdp treating outputs rcnn rcnn action-value function belief state respectively every time step fuse outputs compute belief space q-values using qmdp approxis∈s alternately represented frobenius inner product optimal action choices corresponding highest belief space q-values retrieved softmax qmdp rcnn addition stages rcnn rcnn qmdp rcnn constructed following stages frobenius inner product stage qmdp approximation step represents frobenius inner product stage. implemented ‘valid’ convolutional layer zero stride. softmax stage network. resultant qmdp rcnn depicted figure combines planning learning single network. forward passes network provide action choices output backpropagating network learns reward function updates q-values optimal action choices planning rcnn component. training loss qmdp rcnn trained learning demonstration inverse reinforcement learning setting. expert agent demonstrates tasks recorded series trajectories ...} serve inputs network. actions executed expert trajectories represented one-hot encoding serving objective hence learn reward function action choices output network match target actions every time step. loss function chosen cross-entropy loss output actions target actions given time step qmdp rcnn learns reward function backpropagating gradients retrieved loss function network update reward function. updated reward estimate back rcnn component update q-values hence action choices network. experience replay used randomize expert trajectories transitions training qmdp rcnn. closed form gradient reward updates form αt←− term thus dictates extent actions positively negatively reinforced belief term acts manner similar attention mechanism directs reward function updates speciﬁc regions state space agent believed qmdp rcnn differs traditional approaches qmdp rcnn provided samples reward function interaction environment rather provided action choices made expert. lfd-irl approach employed qmdp rcnn on-policy incompared original policy. comparison provided models learnt using counting style algorithm analogous used well weighted-counting style algorithm updates model-estimates counting belief values. experiment performance algorithms fully partially observable settings whether teacher forcing used different methods adapting learning rate explored including rmsprop linear decay maintaining individual learning rates transition ﬁlter ﬁlter-wise decay presented table fig. plot time iteration value iteration versus size state space. time taken rcnn orders magnitude lesser regular value iteration transition space sizes. built planning causes reward updates permeate entire state space. dependence lfdirl approach qmdp rcnn expert-optimal actions training differs rcnn uses arbitrary choices actions. sented respect rcnns deﬁned. rcnn here contribution representation enables efﬁcient computation value iteration. thus present per-iteration run-time value iteration rcnn versus standard implementation value iteration. algorithms implemented python intel core machine -cores. inherent parallelization convolution facilitates speed-up several orders magnitude depicted figure best rcnn completes single iteration times faster worst times faster regular implementation. rcnn primary objective training rcnn determine accurate estimate transition model. evaluate performance using least square error transition model estimate ground truth model deﬁned uv). since ﬁnal objective learnt models generate policies replanning rcnn also present replanning accuracy model; deﬁned percentage actions chosen correctly network’s policy rcnn’s loss function deﬁned terms non-stationary beliefs given time instant. online training approach hence rather batch mode. observe ﬁlter-wise decay outperforms linear decay different actions chosen varying frequencies. rmsprop suffers oscillations loss function arise dynamic nature hence performs poorly. teacher forcing mitigates dynamic nature time increasing replanning accuracy replanning accuracy attained partial-observability approaches accuracy algorithm fully observable settings. qmdp rcnn qmdp rcnn objective learn reward function results similar policy level performance original reward. since learning reward demonstrated trajectories optimal unique solution quantifying accuracy reward estimate meaningless. rather present replanning accuracy learnt known transition models rewards. also policy evaluation generated policy using original rewards present increase expected reward learnt results models; deﬁned presented table case rcnn rmsprop counter magnitude reward updates dictated qmdp rcnn hence adversely affect performance. experience replay marginally increases replanning accuracy signiﬁcant effect increase expected reward. similarly using delayed feedback also boosts increase expected reward. learning rewards transition models qmdp rcnn achieves appreciable policy error minimal change expected reward. emphasize given access solely observations action choices agent without assuming feature engineering reward qmdp rcnn able achieve near-optimal levels expected reward. utilizing ground truth transition learnt reward qmdp rcnn performs marginally worse accuracy change expected reward. true efﬁcacy rcnn qmdp rcnn ability learn accurate transition models reward functions partially observable settings outperforms existing model-based approaches naive weighted counting signiﬁcant margins terms replanning accuracy transition errors expected reward. finally note deﬁned rcnn architectures demonstrated cases architectures extended number dimensions number actions suitable modiﬁcations convolution operation higher dimensions. paper deﬁned rcnn like architectures namely value iteration rcnn belief propagation rcnn qmdp rcnn facilitate natural representation solutions model-based reinforcement learning. together contributions speed planning process partially observable environment reducing cost replanning model-based approaches. given access agent observations action choices time rcnn learns transition model qmdp rcnn learns reward function subsequently replans learnt models make nearoptimal action choices. proposed architectures also found outperform existing model-based approaches speed model accuracy. natural symbiotic representation planning learning algorithms allows approaches extended complex tasks integrating sophisticated perception modules. sutton mcallester singh mansour policy gradient methods reinforcement learning function approximation advances neural information processing systems press atkeson santamar´ıa comparison direct model-based reinforcement ieee international conference robotics automation vol. available ftp//ftp.cc.gatech.edu/pub/people/cga/rl-compare.ps.gz learning robotics survey robotic res. vol. available http//dx.doi.org/./ bakker zhumatiy gruener schmidhuber quasi-online reinforcement learning robots proceedings ieee international conference robotics automation icra hester quinlan stone generalized model learning reinforcement learning humanoid robot. icra. ieee available http//dblp.uni-trier.de/db/conf/icra/ icra.htmlhesterqs choi k.-e. inverse reinforcement learning partially observable environments mach. learn. res. vol. jul. available http//dl.acm.org/citation.cfm?id= l.-j. self-improving reactive agents based reinforcement learning planning teaching machine learning vol. available http//www.cs.ualberta.ca/ ∼sutton/lin-.pdf learning polynomial time mach. learn. vol. nov. available http//dx.doi.org/./a stadie levine abbeel incentivizing exploration reinforcement learning deep predictive models corr vol. abs/. available http//arxiv.org/abs/.", "year": 2017}