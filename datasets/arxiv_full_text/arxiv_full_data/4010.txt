{"title": "Feature selection using nearest attributes", "tag": ["cs.CV", "cs.AI"], "abstract": "Feature selection is an important problem in high-dimensional data analysis and classification. Conventional feature selection approaches focus on detecting the features based on a redundancy criterion using learning and feature searching schemes. In contrast, we present an approach that identifies the need to select features based on their discriminatory ability among classes. Area of overlap between inter-class and intra-class distances resulting from feature to feature comparison of an attribute is used as a measure of discriminatory ability of the feature. A set of nearest attributes in a pattern having the lowest area of overlap within a degree of tolerance defined by a selection threshold is selected to represent the best available discriminable features. State of the art recognition results are reported for pattern classification problems by using the proposed feature selection scheme with the nearest neighbour classifier. These results are reported with benchmark databases having high dimensional feature vectors in the problems involving images and micro array data.", "text": "abstractâ€”feature selection important problem high-dimensional data analysis classification. conventional feature selection approaches focus detecting features based redundancy criterion using learning feature searching schemes. contrast present approach identifies need select features based discriminatory ability among classes. area overlap inter-class intra-class distances resulting feature feature comparison attribute used measure discriminatory ability feature. nearest attributes pattern lowest area overlap within degree tolerance defined selection threshold selected represent best available discriminable features. state recognition results reported pattern classification problems using proposed feature selection scheme nearest neighbour classifier. results reported benchmark databases high dimensional feature vectors problems involving images micro array data. resolution images internet financial monitoring micro-arrays sources databases used pattern recognition research many contemporary databases uses considerably large number data points represent object sample. high dimensional feature vectors result samples often contain intra-class natural variability reflected noise irrelevant information noise feature vectors occurs inaccurate feature measurements whereas irrelevancy feature depends natural variability redundancy within feature vector. further relevance feature application depended; example consider hypothetical image images faces objects. using image face recognition application relevant pixels image face regions rest regions considered irrelevant. addition face region irrelevant information presence intra-class variability occlusions facial expressions illumination changes pose changes. natural variability occurs high dimensional data significant impact lowering performance pattern recognition methods. improve performance pattern recognition methods past effort compensate remove intra-class natural variability data samples various feature processing methods. dimensionality reduction feature selection types feature processing techniques used automatically improve quality data removing irrelevant information. dimensionality reduction methods popular achieves purpose reducing number features noise feature vector mathematical convenience feature transformations projections. however assumption correlations features data core aspect dimensionality reduction methods result inaccurate feature descriptions. further irrelevant information original data always possible remove dimensionality reduction approach. problem improve quality resulting features using linear recently non-linear dimensionality reduction methods consistently field intense research debate recent past alternative dimensionality reduction approach improve feature quality removing irrelevant features high dimensional feature vector using feature selection methods. feature selection methods intense field study recent years gained importance parallel dimensionality reduction methods. feature selection provides undue advantage dimensionality reduction methods ability distinguish select best available features data would mean feature selection methods applied without losing generality original feature vectors also feature vectors result application dimensionality reduction methods. point view feature selection considered essential component required developing high performance pattern classification systems uses high dimensional data since higher dimensional feature vectors contain several irrelevant features based discriminating nature among different classes individually selecting features nearest attributes best represent class samples among classes. here nearest attributes determined assessing area overlap results inter-class intra-class distance distributions attribute within class. section addresses area overlap calculated nearest attributes determined example application binary pattern classification problem. explain proposed approach consider wisconsin breast cancer database classes benign malignant sample database attributes clump thickness uniformity cell size uniformity cell shape marginal adhesion single epithelial cell size bare nuclei bland chromatin normal nucleoli mitoses. feature values samples dataset used classification experiment partitioning data sets training test set. sets equal number samples selected randomly benign class contains train samples malignant class contain train samples. training samples used process detecting useful attributes approach feature selection classification. order find relative discriminating ability attributes class look overlap area distribution differences inter-class intra-class feature comparisons. illustrate process consider attribute clump thickness total feature values training samples benign class feature values training samples malignant class. denote feature values {...} correspond benign class {...} correspond malignant class denotes clump thickness attribute. intra-class interclass differences absolute valued differences feature values within given class classes respectively. formal terms clump thickness attribute differences {|fi differences {|fi intra-class differences benign malignant class would result values respectively. inter-class differences benign malignant class would result values values respectively. training test sets formed form database randomly selecting approximately equal numbers instances. training would contain instances benign class instances malignant class reduce performance pattern recognition methods feature selection used modern pattern recognition methods combat issues resulting curse high dimensionality main types feature selection methods uses learning techniques uses criteria/filter based feature searching features selected based rank obtained evaluating individual features selection criterion redundancy features dataset minimised. methods rely learning strategies including feature relevance calculations select features holistically learning based solutions results features tend responsive changes training data feature selection largely depends feature relevance within class however increase dimensionality makes learning algorithms computationally intensive also results need development various optimization techniques. criteria driven methods computationally less complex robust changes training data methods main idea optimize objective function using common approach forward backward search features. approaches ability accurately determine important features depend heavily objective function. variations nature data database another makes optimal selection objective function difficult high classification accuracy using selected features methods always guaranteed. feature selection universally improve recognition performance classifiers provided correct features identified. ability feature contribute towards identification pattern belonging class belonging another class plays important aspect performance boosting classifiers. paper present method select features high dimensional training data studying individual ability feature discriminate within classes taking account area overlaps resulting statistics training data. robustness accuracy pattern classification method dependent availability features patterns discriminating among classes. classical feature selection methods focuses detecting features based strength feature within database. strong correlation exists class discriminating ability classifier selected features ignored conventional process feature selection. presented method recognizes need selecting features fig. area normalized histograms intra-class inter-class differences class classification problem generated feature feature comparison using clump thickness attribute shown. using intersection intra-class inter-class difference reference decision boundary plots shows regions correct detection representing true decisions region confusion representing false decisions benign cancer class malignant cancer class respectively. minimum number false decisions occur decision boundary assigned intersection inter-class intra-class distance distributions. area region overlap intra-class inter-class differences gives measure minimum number false decisions turn shows maximum discriminatory ability attribute classifying sample class. area overlap malignant class benign class means clump thickness alone used classfication chance accurately predicting class unknown sample belonging benign class sample belongs malignant class. applying benign malignant classes clump thickness attribute shown results values benign malignant. repeating process calculation attributes arrive values shown table table relative area overlap calculated subtracting values minimum among attributes within class. illustrative example benign malignant classes minimum value uniformity cell size attribute. values shows attributes useful large values shows attributes least useful process classification. obviously attributes represented values need retained selected classification task. area normalized histograms inter-class intra-class distance values clump thickness attribute training shown fig. total area distance histograms normalized value dividing histogram counts total area histogram. area overlap inter-class intra-class distances shows natural discriminatory ability attribute belonging class. figure illustrate overlap areas resulting interclass intra-class distance distribution respect benign class malignant class respectively. decision boundary shown partitions histogram regions based intersection intraclass inter-class differences. three distinct regions shown region true decision belonging class region true decision belonging class combined region overlap representing false decisions belonging belonging class. area overlap region represents measure false decisions sample belonging class using clump thickness attribute alone task classification. area overlap value equal zero implies maximum feature discrimination value would mean feature discriminable. seen using clump thickness attribute table show selected attributes based amin values table attributes fall within desired selection threshold individually evaluated using leave cross-validation gallery data nearest neighbor classifier. attributes highest recognition accuracy ranked highest. combined accuracy shows recognition accuracy testing test using combination attributes. seen ranked attribute gives accuracy attributes give accuracy contrast accuracy attributes used classification without applying feature selection. default selection threshold less would sufficient variability areas overlap attributes class less. hand variability areas overlap attributes class large assumed higher value threshold would required. selection threshold automated leave cross validation test training set. selection threshold gives highest recognition accuracy selected optimal value. figure shows recognition accuracy results cross validation optimal selection threshold. leave cross validation performed training formed split wisconsin breast cancer database. seen best recognition performance obtained around threshold value unless otherwise mentioned value selection threshold demonstrate performance presented feature selection approach experimental verification sections paper. recognition performance cross-validation test using nearest neighbour classifier using presented approach selecting features wisconsin breast cancer database fig. attributes relative overlap area less selection threshold selected discriminating attributes. four possible nine attributes selected keeping selection threshold value amin ensures attributes least value among different classes less selection threshold selected. seen amin results attributes possible getting selected. effectiveness selection processes explained verified using classification experiment. simplest approach perform classification using nearest neighbor classifier. clump thickness uniformity cell size uniformity cell shape marginal adhesion single epithelial cell size bare nuclei bland chromatin normal nucleoli mitoses gli-/gse diffuse infiltrating gliomas common primary brain malignancy found adults glioblastoma multiformethe highest grade glioma associated median survival months. database made transcriptional profiling gliomas patients elucidate glioma biology prognosticate survival define tumor sub-classes. classification task glioma tumor sub-classes done database formed large scale gene expressions. classes used classification study distinction classical methods feature selection dimensionality reduction feature vectors improve recognition performance classification problems involving high dimensional data. section address aspect standard datasets used benchmarking feature selection methods. advancement measurement techniques computing methodologies resulted microarray data various studies application genetics medicine diagnosis. high dimensionality feature vectors mircoarray data often contains features useful process classification. assessing recognition performance presented feature selection method micro-array databases listed table randomly select equal number samples forming training test set. noted experiments results presented section random split done individual classes databases form train test sets. average recognition accuracies reported repeated random splits. number features area overlap within specified selection threshold vary database another. means quality feature vary database based levels natural variability within database. figure shows observation quality features represented selection threshold normalised number features. seen almost every database quality features varies assessed purely based area overlap. interestingly fig. shows apart smk-can database remaining databases contain less features relative total number features within selection threshold means intra-class variability smk-can- lower databases possibility lung cancer effects several gene expressions distinctively comparisons cancer toxicology databases. figure shows recognition performance presented feature selection method used nearest neighbor classifier. seen databases selection threshold lesser sufficient obtain high recognition accuracies. maximum values accuracies possibly limited nature classifier quality feature present. gla-bra-/gds dataset application analysis analysis gliomas different grades. database consists expression profile stem cell factor useful determine tumor angiogenesis. four classes database brainoligodendrogliomas glioblastomas astrocytomas non-tumor. attributes instances. database gene expressions high density oligonucleotide arrays containing genetically clinically distinct subgroups b-cell chronic lymphocytic leukemia dataset formed attributes instances. tox-/gds database example toxicology integrate diverse biological data clinical chemistry expression types data. database contains profiles resulting three toxicants alpha-naphthyl-isothiocyanate dimethylnitrosamine n-methylformamide administered rats. classification task identify whether samples toxic toxic control. sample toxic alpha-naphthylisothiocyanate dimethylnitrosamine n-methylformamide administered non-toxic caerulein dinitrophenol rosiglitazone administered control untreated. database consists gene expression data smokers lung cancer without lung cancer. diagnostic gene expression profile could used distinguish classes. database consists gene expression attributes instances. cases highest recognition accuracies obtained number features comparison total number available features. means gene expression databases gene expressions useful process classification irrespective type classifier employed. table shows performance comparison presented feature selection method conventional feature selection methods seen presented method uses fewer number features achieve higher recognition accuracy shows presented method results accurate selection features useful recognition conventional methods. face recognition challenging widely studied problems recent years contain large number pixels attributes. feature selection experiments logical consider pixel feature applying feature selection method best pixels representative face used classification. benchmark study feature selection face image recognition problem select subsets posted feature selection summary website database subset face database benchmark database feature selection studies. main characteristic face database contains images occlusions illumination facial expression changes. face images used experiments contain total pixels. application mask exclude background pixels results face region shown used experiments feature selection. database formed face images collected pilot european image processing archive. database contains face images changing expressions poses. images contain total pixels. shown application mask exclude background results pixels face region used feature selection experiments. database subset database contain significant variation illumination images total pixels. seen application mask remove background pixels total pixels remain face region. cropped face images database. database contains large variation poses images. mask applied remove background pixels pixels suitable used feature selection task face recognition shown fig. average recognition accuracies using selected attributes shown. nearest neighbor classifier used show classification performance gene expression databases. area overlap attributes nearly same applying threshold based selection results almost available features classification. complete features process automatic classification often feasible option issues curse dimensionality. situations ranking features selecting ranked number features serve purpose dimensionality reduction also serve purpose selecting best available features classification. simplest common approaches select ranks individual search evaluates feature separately. leave cross-validation performed using training individual features selected based specified value selection threshold. selected features ranked based least recognition error evaluating individually nearest neighbor classifier. figure shows average recognition accuracy selecting features. features fall selection threshold ranked based least recognition error using cross validation test. seen ranking features selecting features group features based discriminating ability. result increased recognition accuracies numbers features databases. table show comparison best accuracies obtained ranked features using four conventional classifiers nearest neighbor linear naive bayes. overall seen classifiers perform equally well. noted similar experiments using gene expression data random split done forming training testing sets databases average recognition accuracies obtained testing repeated random splits presented. recognition accuracy presented method selecting different thresholds shown figure like gene expression databases face image databases also threshold less sufficient ensure high recognition accuracy. green color pixels shown figure represents features pixels ranked according discriminating ability. pixels selected rank based leave cross validation individual features fall within selection threshold training using nearest neighbor classifier. seen fig. face database other importance pixels located face vary considerably. largely different type intra-class natural variability databases. figure shows dependence numbers features selection threshold four face image databases. even selection threshold seen face image databases features selected. comparison micro-array datasets seen fig. image datasets larger percentage good quality features useful classification. conventional literature pixels facial features nose mouth considered important information recognition face idea used researchers face classification identification problems. since selected features images belong facial features nose location selected features facial region would indicate importance facial features classification. color pixels figure show location features selected within threshold fig. definitive inference importance location pixel part face database another largely unclear. would indicate discriminatory pixels face image result part face image entirely depended face structure itself table shows performance comparison nearest neighbor linear naive bays classifiers respect highest recognition accuracies obtained using ranked features. seen image data comparison gene expression data needs number features achieving higher accuracies. could attributes face important recognition opposed gene expression genes describe occurrence cancer. table also show pixels selected based discriminatory ability good recognition accuracies achieved. obviously higher recognition accuracy obtained number pixels. comparison feature selection methods table shows presented method performs better respect recognition accuracy although number features methods require almost identical. dependence selection threshold number attributes different face image databases shown. normalized number attributes ratio total number selected attributes total number available attributes database. paper presented feature selection method based assessment discriminatory ability individual features within class. area overlap inter-class intra-class distance distribution individual attributes used main assessment criteria. ability presented method select discriminatory features improves performance recognition requires fewer number fig. illustration pixel face images selection threshold used selecting features shown using color pixels. ranked pixels among selected pixels shown green color. images correspond example training images databases. features comparison conventional methods. number features required achieving high recognition accuracy varies database another. common framework select important features provided applying selection threshold. improved performance using presented method demonstrated using gene expression data face image data. example applying presented method distinct high dimensional databases shows general applicability method specific pattern recognition problems. street wolberg mangasarian nuclear feature extraction breast tumor diagnosis ist/spie international symposium electronic imaging science technology vol. haslinger schweifer stilgenbauer dhner lichter kraut stratowa abseher microarray gene expression profiling b-cell chronic lymphocytic leukemia subgroups defined genomic aberrations mutation status. clin oncol vol. spira beane shah steiling schembri gilman y.-m. dumas calner sebastiani sridhar beamis lamb anderson gerry keane lenburg brody airway epithelial gene expression diagnostic evaluation smokers suspect lung cancer vol. peng long ding feature selection based mutual information criteria max-dependency max-relevance min-redundancy ieee transactions pattern analysis machine intelligence vol. zhang ogihara comparative study feature selection multiclass classification methods tissue classification based gene expressions bioinformatics vol. babbs delp comparison feature selection methods detection breast cancers mammograms adaptive sequential floating search genetic algorithm ieee med. biol. vol. empirical study supervised gene screening donoho formost large underdetermined systems linear equations minimal l-norm solution also sparest solution comm. pure appl. math. vol. samworth ultrahigh dimensional feature selection beyond linear model journal machine learning research vol. zhao multi-scource feature selection geometry dependent covariance analysis journal machine learning research workshop conference proceedings volume challenges feature selection data mining knowledge discovery vol. king discriminative semisupervised feature selection manifold regularization international joint conference artificial intelligence efron hastie johnstone tibshirani least angle", "year": 2012}