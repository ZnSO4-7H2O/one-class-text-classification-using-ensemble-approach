{"title": "Human Activity Learning and Segmentation using Partially Hidden  Discriminative Models", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Learning and understanding the typical patterns in the daily activities and routines of people from low-level sensory data is an important problem in many application domains such as building smart environments, or providing intelligent assistance. Traditional approaches to this problem typically rely on supervised learning and generative models such as the hidden Markov models and its extensions. While activity data can be readily acquired from pervasive sensors, e.g. in smart environments, providing manual labels to support supervised training is often extremely expensive. In this paper, we propose a new approach based on semi-supervised training of partially hidden discriminative models such as the conditional random field (CRF) and the maximum entropy Markov model (MEMM). We show that these models allow us to incorporate both labeled and unlabeled data for learning, and at the same time, provide us with the flexibility and accuracy of the discriminative framework. Our experimental results in the video surveillance domain illustrate that these models can perform better than their generative counterpart, the partially hidden Markov model, even when a substantial amount of labels are unavailable.", "text": "learning understanding typical patterns daily activities routines people low-level sensory data important problem many application domains building smart environments providing intelligent assistance. traditional approaches problem typically rely supervised learning generative models hidden markov models extensions. activity data readily acquired pervasive sensors e.g. smart environments providing manual labels support supervised training often extremely expensive. paper propose approach based semi-supervised training partially hidden discriminative models conditional random ﬁeld maximum entropy markov model show models allow incorporate labeled unlabeled data learning time provide ﬂexibility accuracy discriminative framework. experimental results video surveillance domain illustrate models perform better generative counterpart partially hidden markov model even substantial amount labels unavailable. important task human activity recognition low-level sensory data segmenting data streams labeling meaningful sub-activities. labels used facilitate data indexing organisation recognise higher levels semantics provide useful context intelligent assistive agents. segmentation modules often built low-level sensor components produce primitive often noisy streams events handle uncertainty inherent data current approaches activity recognition typically employ probabilistic models hidden markov models expressive models stochastic context-free grammars hierarchical hmms abstract hmms dynamic bayesian networks model relation activity sequence observable data stream joint distribution maximum likelihood learning models performed ﬁnding parameter optimises joint probability modeling approach drawbacks general. firstly often diﬃcult capture complex dependencies observation sequence typically simplifying assumptions need made conditional distribution tractable. often advantageous optimise conditional distribution learn activity sequence maxy natural model directly. evolve given already observe sequence observations words activity nodes rather parents become children observation nodes. appropriate contextual information discriminative models represent arbitrary dynamic long-range interdependencies highly desirable segmentation tasks. moreover whilst capturing unlabeled sensor data training cheap obtaining labels supervised setting often requires expert knowledge time consuming. many cases certain particular labels example surveillance data person enters room steps pressure mat. labels left unknown. therefore desirable employ semi-supervised approach. speciﬁcally consider recent discriminative models namely undirected conditional random fields directed maximum entropy markov models original models fully observed provide treatment incomplete data crfs memms. algorithm presented models although strictly required crfs. provide experimental results video surveillance domain compare performance proposed models equivalent generative hmms learning segmenting human indoor movement patterns. three data sets studied common behaviour outperformed discriminative counterparts even large portion labels missing. providing contextual features models increases performance signiﬁcantly. novelty paper lies ﬁrst work modeling human activity using partially hidden discriminative models. although semi-supervised learning investigated while much work concentrated unstructured data classiﬁcation. little work structured data segmentation much labeling eﬀort needed. remainder paper organised follows. section reviews related work human activity segmentation background crfs memms semi-supervision. section describes partially hidden discriminative models. paper describes implementation experiments presents results section ﬁnal section summarises major ﬁndings work. hidden markov models used model simple human activities human motion patterns recent approaches used sophisticated generative models capture hierarchical structure complex activities. abstract hidden markov model used model human transportation patterns outdoor sensors model human indoor motion patterns sensors placed mobile robots. using ahmm multiple levels semantics built hmms allowing ﬂexibility modeling evolution activities across multiple levels abstraction. learn parameters expectation maximisation algorithm used. however models generative suitable work arbitrary overlapping features data streams. discriminative models specify conditional probability without modeling data {yin} assume probability speciﬁed respect graph vertex represents random variable edges encode correlation variables. graph undirected conditional clique deﬁned structure potential function deﬁned normalisation consider chain structure crfs labeling tasks {yt}. potential function becomes typically parameterised using λkfk. functions {fk} parameters source state memms also considered conditionally trained hmms although crfs solve label bias problem associated local normalised memms believe memms useful learning understanding activity patterns directly partially hidden models received signiﬁcant attention recently. partially hidden markov model proposed addresses similar partial labeling problem model compare discriminative models. crfs hidden layer introduced labels never given layer thus concerned robust model respect amount missing data. idea constrained inference introduced address learning problem recent work extends work learning addresses interactive empirical figure partially hidden discriminative models partially hidden markov models. filled circles bars data observations empty circles hidden labels shaded circles visible labels partially hidden discriminative models label sequence consists visible component hidden part joint distribution visible variables therefore given memms. stated section directed models like memms important activity modeling naturally encode state transitions given observations. oﬀer slightly general view memms deﬁne single model source states rather separate models source state addition model discriminative model observation sequence thus free encode arbitrary information exacted whole sequence local distribution. implementation realised using sliding window size centred current time capture local context observation. local distribution reads probabilistic inference properties learning slightly diﬀerent memm incorporates structural constraint shared parameters memms learns local classiﬁers independently. contextual features reﬂects fact current activity generally correlated past future sensor data. zeroing gradient yield analytical solution typically iterative numerical methods conjugate gradient newton methods needed. gradient lower bound framework similar except pairwise marginals replaced marginals previous iteration pairwise marginals computed easily using forward pass backward pass segmentation segmentation assignment maxy infer probable label sequence given data sequence crfs memms viterbi algorithm naturally adapted. labels provided so-called constrained inference trivial adaptation viterbi decoding joint distribution phmms. data distribution generated concern discriminative models. phmms contrary observation point presumably generated parent label node care must taken ensure diﬀerence implication that discriminative models good encode output labels directly arbitrary information extracted whole observation sequence phmms better represent little information associated example totally missing still modeled phmms provides useful information. experiments moreover whilst employ log-linear models unconstrained parameters phmms constrained transition emission probabilities parameters. terms modeling label ‘visibility’ phmms general allow subset labels associated certain nodes full hidden nodes single label visible nodes. however quite straightforward extend partially hidden discriminative models incorporate representation. task infer activity patterns person video surveillance scene. observation data provided static cameras labels activities ‘go-from-a-to-b’ time interval recognised trained models. cameras installed capture video actor making meals. landmarks person visit meals door chair fridge stove cupboard dining chair. figure shows room special landmarks viewed cameras. short meal) snack making normal meal scenario comprises number primitive activities listed table figure shows association scenarios primitive activities. short meal data training testing video sequences; snack normal meal data sets consists training testing video sequences. video sequence captured background subtraction algorithm extract corresponding discrete sequence coordinates person based person’s bounding box. training sequences partially labeled indicated portion missing labels testing sequences provide ground-truth algorithms. sequence length ranges features crucial components model observation data semantic outputs features need discriminative enough useful time simple intuitive possible reduce manual labour. current data extracted video contains coordinates. coordinate sequences time slice extract vector elements observation sequence thus incorporate neighbouring observation points within sliding window width intended capture correlation current activity past future observations realisation temporal rough idea observation context inﬂuences performance models diﬀerent window sizes experiments show incorporating context observation sequences help improve performance signiﬁcantly exhaustive searches best context size implement feature selection mechanisms. number features scales linearly context size figure role context window size extract observation data. crfs memms. ﬁgures x-axis portion missing labels y-axis averaged f-score states repetitions. clearly feature selection algorithm needed want capture long range correlation. practical purposes paper choose crfs memms. thus experiments crfs memms share feature making comparison models consistent. evaluate performance discriminative models equivalent generative counterparts implement phmms features extracted sensor data phmms include discretised position velocity. features diﬀerent used discriminative models discriminative features continuous. train discriminative models implement non-linear conjugate gradient polak-ribi`ere limited memory quasi-newton l-bfgs. several pilot runs select l-bfgs optimise objective function directly. case memms regularised algorithm chosen together algorithms stop rate convergence less regularisation constants empirically selected case crfs case memms. phmms observed initial parameter initialisation critical learn correct model. random initialisations often result poor performance. unlike discriminative counterparts initial parameters trivially zeros table figure show performance metrics models considered paper averaged repetitions. three models equivalent graphical structures. crfs memms share feature diﬀerent phmms. generative phmms outperformed discriminative counterparts cases given suﬃcient labels. clearly matches theoretical diﬀerences models enough labels richer information extracted labels available unlabeled data important makes sense model optimise generative framework. data sets crfs outperform models. behaviours consistent results reported fully observed setting. memms known suﬀer label-bias problem thus performance match crfs although memms better hmms given enough training labels. work presented semi-supervised framework activity recognition low-level noisy data sensors using discriminative models. illustrated appropriateness discriminative models segmentation surveillance video sub-activities. ﬂexible information encoded using feature functions discriminative models perform signiﬁcantly better equivalent generative hmms even large portion labels missing. crfs appear promising model experiments show consistently outperform models three data sets. although less expressive crfs memms still important class models enjoy ﬂexibility discriminative framework enable online recognition directed graphical models. study shows primitive intuitive features work well area video surveillance. semantically-rich discriminative contextual features realised technique sliding window. wide context especially suitable current problem human activities clearly correlated time space. however obtain optimal context make information embedded whole observation sequence feature selection mechanism remains designed conjunction models training algorithms presented paper. although crfs memms represent arbitrarily high-level activities many situations appropriate structure activity semantics multiple layers hierarchy. future work include models dynamic conditional random fields conditionally trained dynamic bayesian networks hierarchical model structures. drawback log-linear models considered slow learning curve compared traditional algorithm bayesian networks. therefore important investigate eﬃcient training algorithms. authors would like thank reviewers suggestions improve paper’s presentation. matlab code l-bfgs algorithm conjugate gradient algorithm polak-ribi`ere adapted ulbrich rasmussen respectively. implementation phmms based hmms code roweis.", "year": 2014}