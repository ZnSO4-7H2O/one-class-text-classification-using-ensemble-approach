{"title": "Local Structure Discovery in Bayesian Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Learning a Bayesian network structure from data is an NP-hard problem and thus exact algorithms are feasible only for small data sets. Therefore, network structures for larger networks are usually learned with various heuristics. Another approach to scaling up the structure learning is local learning. In local learning, the modeler has one or more target variables that are of special interest; he wants to learn the structure near the target variables and is not interested in the rest of the variables. In this paper, we present a score-based local learning algorithm called SLL. We conjecture that our algorithm is theoretically sound in the sense that it is optimal in the limit of large sample size. Empirical results suggest that SLL is competitive when compared to the constraint-based HITON algorithm. We also study the prospects of constructing the network structure for the whole node set based on local results by presenting two algorithms and comparing them to several heuristics.", "text": "learning bayesian network structure data np-hard problem thus exact algorithms feasible small data sets. therefore network structures larger networks usually learned various heuristics. another approach scaling structure learning local learning. local learning modeler target variables special interest; wants learn structure near target variables interested rest variables. paper present score-based local learning algorithm called sll. conjecture algorithm theoretically sound sense optimal limit large sample size. empirical results suggest competitive compared constraint-based hiton algorithm. also study prospects constructing network structure whole node based local results presenting algorithms comparing several heuristics. bayesian network representation joint probability distribution. consists parts structure parameters. structure bayesian network represented directed acyclic graph expresses conditional independence relations among variables parameters determine local conditional distributions variable. structure learning bayesian networks ﬁnding data drawn lots interest recent years. rather distinct approaches structure learning. score-based approach gets score based well data goal maximizes score. hand constraint-based approach based testing conditional independencies among variables constructing represents relations. approaches compatible sense matter approach uses exact algorithms converge number samples tends inﬁnity. structure learning bayesian networks np-hard np-hardness problem unlikely exact algorithms polynomial time. indeed fastest exact algorithms exponential time. although several attempts scale exact algorithms methods quickly become infeasible larger datasets. exact algorithms work large data sets often resort heuristics. common strategy diﬀerent variants greedy search. alternative approach problem local learning. large data often case variables equally interesting. might handful target variables goal learn structure vicinity targets. example problem local learning feasible prediction. know markov blanket target variable parents children parents children remaining variables give information. thus known going predict values handful variables learn markov blankets ignore structure rest network. local learning approach. several studies especially constraint-based local learning studies shown local learning powerful tool practice. paper study prospects score-based local learning bayesian network structures. present score-based local learning algorithm variant generalized local learning framework assuming consistent scoring criterion used conjecture algorithm theoretically sound sense greedy equivalence search hiton guaranteed true markov blanket target node sample size tends inﬁnity. optimality guarantees limit lack thereof tell anything results ﬁnite sample size. thus conducted experiments compared algorithm heuristics. based experiments found algorithm promising. structure bayesian network represented directed acyclic graph formally pair node set. parent child either parent child said neighbors. further directed path descendant nodes said spouses common child denote parents node further denote sets neighbors spouses respectively. node clear context identify cardinality denoted node corresponds random variable expresses conditional independence assumptions variables. random variables said conditionally independent distribution given random variables contains represents joint distribution random variables joint distribution satisﬁes local markov condition every variable conditionally independent non-descendants given parents. distribution speciﬁed using local conditional probability distributions specify distribution random variable given parents cpds usually taken parameterized class probability distributions like discrete gaussian distributions. thus variable determined parameters type number parameters speciﬁed particular class probability distributions. parameters bayesian network denoted consists parameters cpd. finally bayesian network pair distribution said faithful conditional independencies implied perfect distribution contains faithful denote marginal distribution conditional independencies implied extracted using d-separation criterion; equivalent local markov condition. skeleton undirected graph obtained replacing directed arcs undirected edges path cycle-free sequence edges corresponding skeleton. node head-to-head node along path consecutive arcs path. nodes d-connected nodes along path every head-to-head node along path descendant none nodes along path nodes d-separated nodes d-connected along path perfect conditionally independent give d-separated conditionally independent given notation u|z. markov blanket node smallest node conditionally independent nodes given markov blanket node consists parents children spouses nodes form v-structure spouses common child. v-structure denoted dags said markov equivalent contain distributions equivalently imply conditional independence statements. shown dags markov equivalent skeleton v-structures score-based structure learning methods bayesian networks assign score based well data according statistical principle. function assigns score based data called scoring criterion. scoring criterion decomposable score local scores depend data consists independent identically distributed samples distribution scoring criterion said consistent limit grows following properties hold related case useful property local consistency. results adding scoring criterion said locally consistent limit grows following properties hold intuitively adding eliminates independence constraint hold datagenerating distribution increases score adding eliminate constraint decreases score. scoring criterion said score equivalent markov equivalent dags always score. score equivalent scoring criterion learn structure bayesian network equivalence class cannot distinguish dags inside class. example commonly used bdeu score locally consistent score equivalent problem structure discovery bayesian networks given data sense best representation data. call global learning. score-based approach goodness measured score constraint-based approach would like perfect data-generating distribution. local learning problem output neighbor and/or spouse sets target node dag. goodness result measured comparing learned neighbor spouse sets corresponding sets perfect data-generating distribution. note comparison made known. section present score-based local learning algorithm algorithm ﬁnds markov blanket given target node. works phases first learns neighbors target rest markov blanket. score-based variant constraint-based local learning algorithm aliferis main diﬀerence between algorithm aliferis independence tests identify neighbors spouses whereas recognize optimal bayesian networks. section also analyze behavior algorithm limit. also consider constructing bayesian network whole node using local structures found. start learning potential neighbors target node. idea algorithm follows. input algorithm consists data node target node execution algorithm update sets consists nodes analyzed consists nodes currently considered potential neighbors beginning empty contains nodes initialization nodes considered learns optimal target current potential neighbors node consideration. structure learning subroutine optimalnetwork returns highest scoring given node set. subroutine dynamic programming algorithm silander myllymäki exact algorithm. optimal found nodes neighbors particular form potential neighbors rest nodes discarded. nodes either discarded potential neighbors potential neighbors returned. pseudocode procedure shown algorithm next show data generated distribution algorithm guaranteed correct neighbors nodes neighbors included limit. guarantee holds following assumptions. assumption data consists i.i.d. samples distribution faithful assumption procedure optimalnetwork uses locally consistent score equivalent scoring criterion. lemma neighbors target assumptions hold algorithm return proof. nodes conditionally independent given bayesian network contains must thus networks otherwise same local consistency higher score thus highest scoring network must reasoning applies every algorithm stops. based previous lemma know node neighbor node however guarantees nodes non-adjacent included indeed aliferis point perfect described figure extra node might added this assume target true neighbors discarded. thus local consistency adding always increases score therefore always included optimal dag. neighbor relation symmetric child parent allows remove extra nodes using simple symmetry correction similar fashion aliferis neighbors potential neighbor potential neighbor algorithm uses symmetry correction neighbors target node. analyze optimality algorithm lemma shows neighbors dependent given every subset neighbors conditionally independent given subset neighbors formally consisting nodes lemma lemma however clear whether output algorithm subset nodes conditionally dependent given subset neighbors aware counterexamples conjecture follows. conjecture output algorithm eap. proof. first prove lemma thus prove conjecture observe eap. suppose nodes therefore must thus eap\\h lemma symmetry thus eap. therefore algorithm include neighbors other. proof. target neighbor neighbor spouse added v-structure suppose v-structure v-structure contain thus v|at. means that local consistency adding increases score. symmetrically contain local consistency adding increase score. since always possible least arcs without introducing cycle cannot optimal graph. learning spouses target quite similar learning neighbors addition data target node take input learned algorithm consisting neighbors consist potential spouses potential spouses neighbors neighbor need subset nodes. neighbors ﬁxed keep updating nodes common child neighbors kept rest discarded. again nodes remain nodes considered considered potential spouses procedure summarized algorithm lemma guarantee spouses found using algorithm indeed figure shows example algorithm leaves spouse out. target. then neighbors consider learning optimal notice conditionally independent given subset thus local consistency optimal network must contain therefore cannot part v-structure discarded. however original graph involved v-structure spouses algorithm known whether algorithm guaranteed spouses limit. finding counterexample seems diﬃcult thus conjecture follows. conjecture spouses target assumptions hold algorithm return sap. found neighbors spouses target node markov blanket simply following conjecture holds conjectures hold summarizes theoretical guarantees. conjecture assumptions hold local learning algorithm optimal limit sample size approaches inﬁnity algorithm always ﬁnds correct markov blanket every target. algorithm loop executed times. time space requirement inside loop dominated procedure optimalnetwork. node runs time space worst case thus algorithms worst case time requirement space requirement practice however networks often relatively sparse running times signiﬁcantly lower worst case; experiments. algorithms call algorithms times respectively. thus total time requirement algorithms presented computing markov blanket single target. computes markov blankets nodes store reuse potential neighbor spouse sets. thus worst case markov blanket nodes found time. learned markov blankets target nodes. next introduce methods construct whole node based local results. ﬁrst method uses constraint-based approach. mentioned previous section computing local neighbor sets need symmetry correction separately node. instead ﬁrst potential neighbors node actual neighbors build skeleton using and-rule; edge added skeleton potential neighbors other. similar also skip separate symmetry checks ﬁnding spouses node. result procedure sll+c described algorithm construct dag. lines algorithm skeleton built lines v-structures directed; speciﬁes markov equivalence class dag. direct rest arcs done line rules listed pearl note practice conﬂicts v-structures. ﬁnite sample size v-structure could example force oriented another v-structure another construct local results heuristic greedy search constraints imposed local neighbors. later experiments section often leads structure better score compared constraint-based approach. another advantage need spouses nodes often computationally intensive phase sll. algorithm describes ssl+g procedure uses or-rule build skeleton potential edges local neighbor sets. calls greedy search subroutine corresponding edge present skeleton. approach similar mmhc algorithm tsamardinos comes correctness guarantees limit. implemented algorithm well sll+c sll+g algorithms c++. optimalnetwork subroutine used dynamic programming algorithm silander myllymäki fallback greedy equivalence search number nodes input larger dynamic programming limited maximum in-degree node implementation available http//www.cs.helsinki. fi/u/tzniinim/uai/. algorithms compared several state-of-the-art algorithms namely constraint-based hiton local structure discovery greedy search greedy equivalence search maxmin hill-climbing global structure discovery. association test used hiton mmhc built assoc function implementation test according aliferis tsamardinos hiton maximum conditioning size bdeu prior equivalent sample size used algorithms applicable. experiments used discrete data generated real world bayesian networks. chose subset data sets used tsamardinos freely available online. listed table networks generated tiling copies original network using method tsamardinos network independently generated random datasets containing i.i.d. samples. accommodate limited space chose representative sample four networks ﬁgures alarm child insurance hailfinder. ﬁgures rest networks available online appendix. local learning experiments goal learn neighbor sets markov blankets nodes. compared constraint-based hiton causal explorer toolkit aliferis neighbor sets markov blankets measured local hamming distance learned nodes true nodes. hamming distance sets number elements contained sets |a\\b|+|b\\a|. data computed local hamming distances possible target nodes call slhd. figure shows average slhds function size data learning neighbor nodes. expected accuracy learned neighbor improves number data samples increases. cases beats hiton. average slhds learning markov blankets shown figure results similar neighbor nodes. global learning experiments goal learn entire structure underlying bayesian network. considered sll+c sll+g heuristics. greedy search algorithm used steepest ascent hill-climbing tabu list last structures stopping criterion steps without improvement maximum score. parameters chosen mmhc greedy search implementation tsamardinos compared algorithms greedy search max-min hill-climbing causal explorer toolkit well greedy equivalent search tetrad software. algorithms return others return pdag partially directed acyclic graph corresponds markov equivalence class. compared corresponding pdag edges oriented members class undirected. since dags belonging equivalence class cannot distinguished using score equivalent scores converted returned dags pdags compared pdags. algorithm failed ignored. aﬀected failed return acyclic graph several runs. used measures evaluate goodness pdags returned algorithms normalized bdeu score structural hamming distance true structure. normalized bdeu score obtained computing bdeu score equivalent sample size extension pdag dividing result score true structure; lower normalized score better structure data. average normalized scores shown figure basic greedy search seems surprisingly well even compared ges. networks sll+g works better mmhc around. sll+c loses methods cases. structural hamming distance pdags deﬁned number edges missing/extra wrong type measured distances resulting pdags figure shows average running times. local learning times include learning neighbors markov blankets nodes. sll+c time spent local learning part running algorithm nodes times combined. times hiton directly comparable since symmetry correction node separately. spite extra work hiton often slower sll. also global learning rival algorithms usually faster sll+g sll+c. paper studied prospects score-based local learning. conjectured scorebased local learning provides theoretical guarantees greedy equivalence search constraint-based local learning natural avenue future research prove guarantees lack thereof. experiments suggest method provides competitive alternative constraint-based local learning. algorithm seems often local neighborhoods accurately competitor. however downside algorithm usually consumes signiﬁcantly time. hope algorithm could bridge exact algorithms constraint-based local learning providing trade accuracy scalability. localto-global approach seem perform less well compared various heuristics. however data sets local-to-global algorithms outperformed benchmarks structural hamming distance especially lots data. suggests development score-based local-to-global heuristic could competitive certain types networks. brandon malone changhe yuan eric hansen susan bridges. improving scalability optimal bayesian network learning external-memory frontier breadth-ﬁrst branch bound search. proceedings conference uncertainty artiﬁcial intelligence pages auai press andreas nägele mathäus dejori martin stetter. bayesian substructure learning approximate learning large network structures. european conference machine learning pages pekka parviainen mikko koivisto. exact structure discovery bayesian networks less space. proceedings conference uncertainty artiﬁcial intelligence pages auai press tomi silander petri myllymäki. simple approach ﬁnding globally optimal bayesian network structure. proceedings twenty-second annual conference uncertainty artiﬁcial intelligence pages auai press ioannis tsamardinos alexander statnikov laura brown constantin aliferis. generating realistic large bayesian networks tiling. international flairs conference pages thomas verma judea pearl. equivalence proceedings synthesis causal models. annual conference uncertainty artiﬁcial intelligence pages elsevier references constantin aliferis alexander statnikov ioannis tsamardinos subramani mani xenofon koutsoukos. local causal markov blanket induction causal discovery feature selection classiﬁcation part algorithms empirical evaluation. journal machine learning research constantin aliferis alexander statnikov ioannis tsamardinos subramani mani xenofon koutsoukos. local causal markov blanket induction causal discovery feature selection classiﬁcation part analysis extensions. journal machine learning research constantin aliferis ioannis tsamardinos alexander statnikov. causal explorer probabilistic network learning toolkit discovery. software http//discover.mc.vanderbilt. edu/discover/public/causal_explorer/. david maxwell chickering. learning bayesian networks np-complete. doug fisher hans-j. lenz editors learning data artiﬁcial intelligence statistics pages springer-verlag david maxwell chickering. optimal structure identiﬁcation greedy search. journal machine learning reseach gregory cooper edward herskovits. bayesian method induction probabilistic networks data. machine learning james cussens. bayesian network learning cutting planes. proceedings conference uncertainty artiﬁcial intelligence pages auai press cassio campos zeng qiang structure learning bayesian networks using constraints. proceedings international conference machine learning pages omnipress tommi jaakkola david sontag amir globerson marina meila. learning bayesian network structure using relaxations. proceedings international conference artiﬁcial intelligence", "year": 2012}