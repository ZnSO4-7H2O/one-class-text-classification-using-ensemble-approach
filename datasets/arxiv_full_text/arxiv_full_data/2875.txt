{"title": "Structured Output Learning with Abstention: Application to Accurate  Opinion Prediction", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way. For that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to structured output prediction. To compare fully labeled training data with predictions potentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. Thus, SOLA extends recent ideas about Structured Output Prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predictor.", "text": "problem attracted growing attention structured output prediction community also raised unprecedented challenge human interpretation opinions expressed reviews subjective opinion aspects related polarities sometimes expressed ambiguous difﬁcult annotate context prediction error ﬂexible integrate subjectivity that example mistakes aspect interfere prediction polarity. order address issue propose novel framework called structured output learning abstention allows abstaining predicting parts structure avoid providing erroneous insights object predicted therefore increasing reliability. approach extends principles learning abstention recently introduced binary classiﬁcation generalizes surrogate least-square loss approaches structured output prediction recently studied main novelty comes introduction asymetric loss based embeddings desired outputs outputs predicted abstention space. interestingly similarly case output kernel regression appropriate inner product-based losses approach relies simple surrogate formulation namely least-squares formulation followed resolution pre-image problem. paper organized follows. section introduces problem solve novel framework sola. section provides statistical guarantees excess risk framework least squares surrogate loss section devoted pre-image developed hierarchical output structures. section presents numerical experiments section draws conclusion. motivated supervised opinion analysis propose novel framework devoted structured output learning abstention structure prediction model able abstain predicting labels structured output cost chosen user ﬂexible way. purpose decompose problem learning pair predictors devoted structured abstention other structured output prediction. compare fully labeled training data predictions potentially containing abstentions deﬁne wide class asymmetric abstention-aware losses. learning achieved surrogate regression appropriate feature space prediction abstention performed solving pre-image problem. thus sola extends recent ideas structured output prediction surrogate problems calibration theory enjoys statistical guarantees resulting excess risk. instantiated hierarchical abstention-aware loss sola shown relevant ﬁne-grained opinion mining gives state-of-the-art results task. moreover abstention-aware representations used competitively predict user-review ratings based sentence-level opinion predictor. recent years opinion analysis reviews commonly handled supervised polarity classiﬁcation problem. however understanding grounds opinion formed highest interest decision makers. aligned goal emerging ﬁeld aspect-based sentiment analysis evolved towards involved machine learning task opinions considered structured objects—typically hierarchical structures linking polarities aspects relying different units analysis assume target structure hierarchical binary tree. then directed edges reﬂecting parent relationship among nodes regarding labeling impose following property oriented pair meaning node cannot greater father node. h-loss measures length common path root leaves assignments deﬁned follows tionship vertices. legal labeling assignment d-dimensional binary vector also satisﬁes properties induced graph structure i.e. call subset contains possible legal labelings given goal structured output labeling learn function predicts legal labeling given input emphasize necessarily share structure outputs objects. instance supervised opinion analysis inputs reviews natural language described sequence feature vectors representing sentence. extending supervised classiﬁcation abstention structured output learning abstention aims learning pair functions composed predictor predicts label component structure abstention function determines components structure abstain predicting label. note legal labelings abstention denotes abstention label abstention-aware predictive model deﬁned follows assuming random variable taking values distributed according probability distribution learning predictive model raises issue designing appropriate abstention-aware loss function deﬁne learning problem risk minimization task. given relationship risk converted risk pair using abstentionaware loss paper propose family abstention-aware losses generalizes abstention-aware loss binary classiﬁcation case extends scope hierarchical losses previously proposed hierarchical output labeling tasks. abstention-aware loss required deal asymmetrically observed labels supposed complete predicted labels incomplete partial abstention. thus propose following general form function choose solve problem vectorvalued reproducing kernel hilbert space associated operator-valued kernel sake simplicity chosen decomposable operatorvalued kernel identity positive deﬁnite kernel identity matrix. penalty chosen choice leads ridge regression problem caci chosen constants function predictions. thus designed loss adapted hierarchies nodes known hard predict whereas children easy predict. case abstention choice used particular node price predicting child. prediction still mistake price caci additionally paid. acting provides control number abstentions risk taken predicting given node also children. sake space product representation loss detailed supplementary material. solving directly problem raises difﬁculties practice usual know expected value conditionally y|xψwa needs estimated training sample simple regression problem referred learning step solved next subsection. section give statistical guarantees learning predictors framework previously described. build recent results framework least squares loss surrogate extended abstention-aware prediction. theorem given deﬁnition denote pair predictor reject functions associated estimate obtained solving learning problem stated exyψwa maxyhyr∈y ψarp. full proof given supplements. close extended taking norm moreover problem solved kernel ridge regression shown universal consistency obtained generalization bound still holds case since rely result theorem only. consequence excess risk predictors built sola framework controlled risk suffered learning step shelf vector valued regressors convergence guarantees. deﬁnition graph graph consisting nodes directed edges undirected edges subgraph directed acyclic graph subgraph self loop. deﬁnition assignment labels graph legal pair nodes labeled pair deﬁnition state space graph legal assignments note graphs represent type binary labeled graph since empty sets. previous works used model coarse ontology hierarchy incorporating prior known labels exclusions encoded output data consider consists graph assignments predictions abstention belong another space restrict belong rather allow choices detailed next section. complexity preimage problem aspects space search solution hard explore; function lead high dimensional representations minimization problem harder. pre-image problem involves minimization constrained binary variables. large class abstention-aware predictors propose branch-and-bound formulation nearly optimal initialization point obtained polynomial time. following line given form abstention aware predictor deﬁned section consider losses involving binary interaction predict function reject function suppose exists rectangular matrix kronecker product vectors. class takes special cases examples presented section state following linearization theorem binary interaction hypothesis theorem abstention-aware loss deﬁned output mappings corresponding cost matrix since objects intend predict graphs vectors output space satisfy hierachical constraint index parent according hierarchy. predicting abstention relax condition since suppose descendant node take value parent active abstained predicting condition equivalent constraint second condition used practice restriction abstention consecutive nodes structured abstention layer must used order reveal subsequent prediction known easy. condition encoded inequality complexity problem linked properties acanonical operator. shown case minimization h-loss hierarchical constraints linear operator acanonical satisﬁes property total unimodularity sufﬁcient condition problem solutions continuous relaxation leading polynomial time algorithm. general case solving integer program np-hard optimal solution obtained using branch-and-bound algorithm. implementing type approach choice initialization point strongly inﬂuence convergence time. point chosen solution original prediction problem without abstention moreover since abstention mechanism modify small subset predictions expect solution close abstention aware one. study three subtasks opinion mining namely sentencebased aspect prediction sentence-based joint prediction aspects polarities full review-based star rating. show tasks linked using hierarchical graph similar probabilistic model exploit abstention mechanism build robust pipeline based opinion labels available sentence-level build two-stage predictor ﬁrst predicts aspects polarities sentence level deducing corresponding review-level values. experiments rely expression haloss presented linear programming formulation used branch-and-bound solver derived supplementary material involves decomposition similar described section h-loss. implementing ha-loss requires choosing weights caci. here assumed index root node. weighting scheme commonly used previous studies related minimization hamming loss vectorized representation graph assignment. choice empirically gave best scores hamming loss fscore abstention weights caci concerned making exhaustive analysis possible choices impossible number parameters involved. therefore experiments focus weighting schemes built following infersent representation encode inputs. dense sentence embedding corresponds inner representation deep neural network trained natural language inference task shown give competitive results natural language processing tasks. test model different subtasks. ﬁrst apply model task opinion aspect prediction compare baselines original results test method baselines problem joint aspect polarity prediction order assess ability hierarchical predictor take advantage output structure. task additionally illustrate behavior abstention varying constants kac. illustrate abstention mean build robust pipeline task star rating regression based sentence-level opinion predictor. effect choices illustrated opinion prediction task. also experiments hierarchical classiﬁcation task images imageclef dataset reusing setting show results obtained different weighting schemes. settings results placed supplementary material. test model problem aspect-based opinion mining subset tripadvisor dataset released consists hotel reviews total sentences predeﬁned train test sets. addition review-level star ratings authors gathered opinion annotations sentence-level predeﬁned aspects corresponding polarity. similarly them discard related aspect consider remaining aspects different polarities each. propose graphical representation opinion structure sentence level objects output space consist trees depth ﬁrst node root second layer made aspect labels third polarities corresponding aspect. corresponding assignments encoded binary matrix concatenation vectors indicating presence aspect ones indicating polarity. exp. aspect prediction. ﬁrst task predicting different aspects discussed sentence. problem cast multilabel classiﬁcation problem target ﬁrst column output objects devise baselines. ﬁrst relies logistic regression model trained separately aspect. second baseline infersent) inspired work built hierarchical model based handcrafted sparse feature including one-hot word encoding tags sentiment vocabulary. since optimization gibbs sampling model relies sparsity feature could directly dense representation. linear chain infersent takes advantage input features remaining computationally tractable. linear chain trained node output structures chain used encode dependency successive sentences. method regression infersent logistic regression infersent linear chain infersent linear chain sparse features marcheggiani hierarchical sparse features marcheggiani next experiments consider them. even though regression trained order predict whole structure obtains results similar logistic regression linear chain crf. exp. joint polarity aspect prediction abstention. take output objects assignments graph described build adapted abstention mechanism. intuition cases polarity might easier predict aspect linked. typically happen vocabulary linked current aspect unseen training implicit whereas polarity vocabulary correctly recognized. example sentence great views east river\" aspect \"location\" implicit \"views\" could mislead predictor result prediction aspect \"other\". case underline interannotator agreement low. reason want classiﬁer allows multiple candidates aspect prediction providing polarity corresponding them. illustrate behavior running sets experiments allow predictor abstain polarity. ﬁrst experiment want analyze inﬂuence parameterization ha-loss. following parameterization caci previously proposed generated predictions varying values displayed hamming loss true labels predictions function mean number aspects predictor abstained handle cases original hierarchical constraint forced three curves obtained generalized constraint hypothesis yprp. additionally model regression without abstention baselines logistic regression measured similar hamming loss concerning micro-averaged score regression retrieved score slightly logistic regression scored linear chain conclusions raised. firstly value choice hypothesis hstrict little inﬂuence scores computed cases previously described. secondly increasing number abstentions aspects helps reducing number errors counted aspects nodes predictor abstains less labels. point quality overall prediction decreases since error rate remaining aspects selected abstention less polarity labels subsequently examine hamming loss polarity predictions situated aspect node understand inﬂuence coefﬁcients relaxation hstrict hypothesis figure orange curve gives best score mean number abstentions sentence. difference hstrict hypothesis ability predict polarity aspect candidate abstention even predictor function select behavior made possible fact prediction respect constraints instead belong ﬂexible space finally show abstention used build robust pipeline star-rating regression. exp. star rating regression review level based sentence level predictions. last round experresentations assumed known test text-level opinion regressors back star ratings. hierarchical line corresponds best results reported tasks. regression model without abstention used predictor sentence-level representation pipeline shown finally regression abstention used sentence-level representation h−). since non-zero components correspond aspects abstained subtracting original prediction results reduction conﬁdence regressor aspects biasing corresponding polarity predictions towards regression strongly outperforms hierarchical tasks. report score regression abstention since dependent number abstentions show improves results regression model text-level prediction task. iments show abstention used build robust intermediate representation task opinion rating regression consists predicting overall average star rating given reviewer subset predeﬁned aspects. ﬁgure illustrates different elements involved problem. procedure split steps. firstly learn sentence-level opinion predictor takes advantage available annotations. step corresponds studied previous experiment. vector-valued regressor built. takes input component-wise average sentence level opinion representations intends predict star ratings review level. overall aspects separate ridge regressor trained based true labels available. learned regressors take input prediction ﬁrst step pipelined similarly rescale star ratings scale report macro-averaged mean average error test-set table column text level. additionally include error measured polarity predictions sentence level counted underlying aspect predicted true positive. novel framework structured learning abstention extends families approaches learning abstention least-squares surrogate structured prediction. important notice beyond ridge regression vectorvalued regression model writes eligible. typically case output kernel tree-based methods also sola applied opinion analysis could prove suitable complex structure-labeling problems. concerning opinion analysis shown abstention used build robust representation star rating pipeline framework. extension work would consist learning abstain jointly predicting aspects polarity sentence text level. references bentaieb aïcha hamarneh ghassan. topology aware fully convolutional networks histology gland segmentation springer international pubmarcheggiani diego täckström oscar esuli andrea sebastiani fabrizio. hierarchical multi-label conditional random ﬁelds aspect-oriented opinion mining. ecir springer osokin anton bach francis lacoste-julien simon. structured prediction theory calibrated convex advances neural information surrogate losses. processing systems pontiki maria galanis dimitris papageorgiou haris androutsopoulos manandhar suresh mohammad alsmadi al-ayyoub mahmoud zhao yanyan bing clercq orphée semeval- task aspect based sentiment analysis. proceedings international workshop semantic evaluation rousu juho saunders craig szedmak sandor shawe-taylor john. kernel-based learning hierarchical multilabel classiﬁcation models. journal machine learning research wang hongning zhai chengxiang. latent aspect rating analysis without aspect keyword supervision. proceedings sigkdd international conference knowledge discovery data mining brouard céline szafranski marie d’alché florence. input output kernel regression supervised semi-supervised structured output prediction operator-valued kernels. journal machine learning research cesa-bianchi nicolò gentile claudio zaniboni luca. hierarchical classiﬁcation combining bayes svm. proceedings international conference machine learning ciliberto carlo rosasco lorenzo rudi alessandro. consistent regularization approach structured prediction. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. conneau alexis kiela douwe schwenk holger barrault loïc bordes antoine. supervised learning universal sentence representations natural language inference data. corr abs/. http//arxiv.org/abs/.. cortes corinna desalvo giulia mohri mehryar. boosting abstention. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. deng ding yangqing frome andrea murphy kevin bengio samy yuan neven hartmut adam hartwig. large-scale object classiﬁcation using label relation graphs. european conference computer vision springer dimitrovski ivica kocev dragi loskovska suzana džeroski sašo. hierchical annotation medical images. proceedings international multiconference information society ljubljana geurts pierre wehenkel louis d’alché-buc florence. kernelizing output tree-based methods. machine learning proceedings twenty-third international", "year": 2018}