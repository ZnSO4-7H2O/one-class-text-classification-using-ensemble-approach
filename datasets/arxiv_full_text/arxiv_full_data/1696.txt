{"title": "A Case Study in Text Mining: Interpreting Twitter Data From World Cup  Tweets", "tag": ["stat.ML", "cs.CL", "cs.IR", "cs.LG", "I.5.4; I.2.7; H.2.8; H.3.3"], "abstract": "Cluster analysis is a field of data analysis that extracts underlying patterns in data. One application of cluster analysis is in text-mining, the analysis of large collections of text to find similarities between documents. We used a collection of about 30,000 tweets extracted from Twitter just before the World Cup started. A common problem with real world text data is the presence of linguistic noise. In our case it would be extraneous tweets that are unrelated to dominant themes. To combat this problem, we created an algorithm that combined the DBSCAN algorithm and a consensus matrix. This way we are left with the tweets that are related to those dominant themes. We then used cluster analysis to find those topics that the tweets describe. We clustered the tweets using k-means, a commonly used clustering algorithm, and Non-Negative Matrix Factorization (NMF) and compared the results. The two algorithms gave similar results, but NMF proved to be faster and provided more easily interpreted results. We explored our results using two visualization tools, Gephi and Wordle.", "text": "cluster analysis ﬁeld data analysis extracts underlying patterns data. application cluster analysis text-mining analysis large collections text similarities documents. used collection tweets extracted twitter world started. common problem real world text data presence linguistic noise. case would extraneous tweets unrelated dominant themes. combat problem created algorithm combined dbscan algorithm consensus matrix. left tweets related dominant themes. used cluster analysis topics tweets describe. clustered tweets using k-means commonly used clustering algorithm non-negative matrix factorization compared results. algorithms gave similar results proved faster provided easily interpreted results. explored results using visualization tools gephi wordle. cluster analysis process grouping data points together based relative similarities. text mining subﬁeld cluster analysis analysis large collections text patterns documents. extracted tweets twitter containing words ‘world cup’; world games started. beginning tweets. tweets consisted english spanish words. working data kept tweets still contained important information. twitter useful tool gathering information users’ demographics opinions certain subjects. example political scientist could younger audience feels certain news stories advertiser could twitter users saying products. security important able discern threats threats. search engines also discern various topics apply word. example ‘jordan’ could apply michael jordan country jordan jordan river. many diﬀerent clustering algorithms advantages disadvantages. clustering algorithm perfect provides slightly diﬀerent clustering. certain clustering algorithms department mathematics university north carolina charlotte department mathematics brigham young university idaho department mathematics woﬀord college spartanburg department mathematics state university raleigh department mathematics state university raleigh better certain types datasets text data numerical data data wide range cluster sizes. purpose research compare advantages disadvantages clustering algorithms non-negative matrix factorization widely used k-means algorithm used twitter text data. cluster data points algorithm called k-means widely used algorithm ﬁeld. purpose algorithm divide data points clusters distance data point cluster’s center minimized. initially k-means chooses random points data space necessarily points data assigns centroids. then data point assigned closest centroid create clusters. ﬁrst step centroids reassigned minimize distance points cluster. data point reassigned closest centroid. process continues convergence reached. distance data points centroids measured using several diﬀerent metrics including widely used cosine euclidean distances cosine distance measure distance data points euclidean distance magnitude distance data points. example data points represented sentences containing three words common cosine would give distance independent many times three words appear sentences. euclidean distance however takes account magnitude similarity sentences. thus sentence containing words world times another containing words times considered dissimilar euclidean distance cosine distance. purpose research used cosine distance faster better equipped dealing sparse matrices provides distances tweets independent tweets’ lengths. thus long tweet several words might still considered similar shorter tweet fewer words. cosine distance measures cosine angle vectors disadvantages k-means highly dependent initializations centroids. since initializations random multiple runs k-means produce diﬀerent results another disadvantage value must known order algorithm. real-world data sometimes diﬃcult know many clusters needed performing algorithm. consensus clustering combines advantages many algorithms better clustering. diﬀerent algorithms dataset consensus matrix created time data points clustered together added consensus matrix positions noted consensus matrix created running algorithm k-means multiple times varying parameters number clusters. case text mining consensus matrix used place term document matrix clustering again. figure shows results three diﬀerent clustering algorithms. note data points cluster together three times thus figure position non-negative matrix factorization decomposes term-document matrix matrices term-topic matrix topic-document matrix topics. term document matrix decomposed biggest advantage multiplicative update rule that theory converges local minimum. however initialization greatly inﬂuence minimum problem multiplicative update rule time costly depending initialized. further matrices sparsity elements matrices locked meaning element becomes longer change. text mining results words removed added topic vectors. thus algorithm starts path local minimum cannot easily change diﬀerent even path leads poor topic vector elements matrices locked; thus algorithm ﬂexible creating topic vectors multiplicative update rule. biggest disadvantage algorithm lack sparsity matrices alternating constrained least square algorithm advantages alternating least square algorithm added beneﬁt matrices sparse. algorithm fastest three since data fairly large used acls algorithm every time performed twitter data density-based spatial clustering applications noise common clustering algorithm. dbscan uses similarity metric usually form distance group data points together. dbscan also marks points noise used noise removal applications. dbscan requires inputs minimum number points dense cluster distance. dbscan visits every data point dataset draws radius around point. least number points radius call point dense point. minimum number points radius dense point call point border point. finally neither number points dense point radius call point noise point. dbscan used remove noise. dbscan weaknesses. first highly dependent parameters. changing drastically change results algorithm. also good ﬁnding clusters varying densities allow variation data analyzed tweets twitter containing words ‘world cup’. many tweets same; twitter calls ‘retweet’. since retweet take much thought original tweet decided remove retweets prevent bias data. several columns term document matrix identical removed columns. process removed tweets data. preliminary exploration tweets found topic harry potter quiddich world cup. looked tweets contained cluster found tweet retweeted approximately times. retweets removed cluster longer existed reduced tweet quiddich. thus found removing retweets eliminated clusters contained small number original tweets. tweets written posted without much revision. tweets contain noise. noise vocabulary tweets removed stop list stemming. look collection tweets want tweets closely related speciﬁc topic. tweets edges clusters still related topic closely. therefore remove noise without damaging meaning cluster. figure shows simple two-dimensional example noise removed still keeping clusters. created four algorithms noise removal. frequently. considered clusters points removed noise. creating drop tolerance consensus matrix. tweets cluster together time term consensus matrix dropped looked sums consensus matrix employed another drop tolerance. entries consensus matrix averaged. tweets whose less average marked noise points. problem algorithm clusters must similar density. variation density clusters less dense cluster removed noise. second algorithm used multiple runs dbscan helped decide tweet true noise point not. distance matrix used based cosine distance tweets. used cosine distance standard looking distance text data. since ﬁrst algorithm removed less dense clusters wanted make sure still included removed noise. dbscan dependent used range order include clusters. experimentation found larger data sets required runs dbscan. created matrix number tweets number runs dbscan entry matrix classiﬁcation dense border noise point tweet run. tweet marked border point noise point runs considered true noise point. also looked varying however created problems algorithm marked tweets noise points. therefore decided keep value constant. algorithm kept clusters varying density diﬃcult tell clusters apart. since consensus matrix similarity matrix decided dbscan matrix instead distance matrix. idea similar second algorithm; still varied kept constant. value algorithm number times tweet clustered together. performed dbscan multiple times consensus matrix created matrix classiﬁcation described before. decided tweet marked border point noise point runs considered true noise point. algorithm unique removes noise points clusters. found points clusters vary frequently cluster belong. wanted strengths previous algorithms combined them. algorithm looks classiﬁcation previous algorithms noise point represented least three algorithms marked tweet noise point would removed data. allows remove points edge clusters still keep clusters varying density. process removed tweets data. ﬁnal dataset used order major topics tweets. diagonal matrix entries corresponding rows consensus matrix looked smallest eigenvalues laplacian matrix identify number topics look for. eigenvalues signiﬁes number topics. large gaps ﬁrst eigenvalues thought small number topics would make topics broad. since wanted larger number topics chose upper eigenvalues shown figure future work would like normalized laplacian matrix create better eigenvalue plot. clustered remaining tweets order major themes text data. since k-means widely used clustering algorithm since results highly dependent value k-means twitter data k-means ﬁnal time consensus matrix input algorithm gave cluster tweet belonged placed tweet text tweets cluster. created word cloud cluster order visualize overall themes throughout tweets. problem using k-means algorithm output k-means cluster number tweet. knowing cluster tweet belonged help know cluster about. although possible look tweet cluster determine overall theme usually requires visualization tools word cloud order discover word words form cluster. thus used non-negative matrix factorization algorithm speciﬁcally alternating constrained least square algorithm order easily detect major themes text data. algorithm returns term-topic matrix topic-document matrix. acls sorted rows descending order ﬁrst element column corresponded important word topic thus possible important words topics. found important words topic curious words together. created algorithm picked representative tweet topic representative tweet many words topic possible. called tweets topic sentences. visualizations graphing software gephi able close topics another. graph consensus matrix tweets connected clustered together times. tweets clustered together frequently closer together graph form topic represented color graph. unconnected nodes tweets clustered tweet times. k-means works topics split figure obvious split ‘falcao/spanish/stadium’ topic. graph colored nodes represent topic tweet closely related edges emanating node represent topics tweet slightly related created graph distance heavier weights shorter. pulls topics similar towards other. example ‘fifa’ ‘venue’ topics right next seen figure means tweets ‘fifa’ topic highly related ‘venue’ topic. examined topics found shared words ‘stadium’ ‘brazil’ frequently. wanted compare results k-means created visualizations frequent words software called wordle. example selected spanish tweets topic. looked topic results k-means found created cluster contained spanish topic topic player falcao topic stadiums. thought producing well deﬁned clusters. used cluster analysis topics collection tweets. proved faster provided easily interpreted results. selected single tweet represented entire topic whereas k-means provide tweets topic. visualization techniques necessary interpreting meanings clusters provided k-means. still explore understanding text data manner. looked k-means analyze tweets. algorithms could prove valuable. since looked deeply text data research could prove algorithms better diﬀerent types data. explored results using visualization tools gephi wordle. still much done aspect. retrospect would perform singular value decomposition consensus matrix running k-means. noise would removed clustering would reliable. interested exploration along lines case study natural extension would perform analysis real-time observe speciﬁc topics evolve time. would like thank national science foundation national security agency funding project. express gratitude state research experience undergraduates mathematics modeling industrial applied mathematics program providing opportunity.", "year": 2014}