{"title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abstract": "Continuous/Lifelong learning of high-dimensional data streams is a challenging research problem. In fact, fully retraining models each time new data become available is infeasible, due to computational and storage issues, while na\\\"ive incremental strategies have been shown to suffer from catastrophic forgetting. In the context of real-world object recognition applications (e.g., robotic vision), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques. In this work we propose a new dataset and benchmark CORe50, specifically designed for continuous object recognition, and introduce baseline approaches for different continuous learning scenarios.", "text": "continuous/lifelong learning high-dimensional data streams challenging research problem. fact fully retraining models time data become available infeasible computational storage issues naïve incremental strategies shown suffer catastrophic forgetting. context real-world object recognition applications continuous learning crucial datasets benchmarks available evaluate compare emerging techniques. work propose dataset benchmark core speciﬁcally designed continuous object recognition introduce baseline approaches different continuous learning scenarios. datasets imagenet pascal provide good playground classiﬁcation detection approaches. however designed static evaluation protocols mind; entire dataset split parts training used learning separate test used accuracy evaluation. splitting training number batches essential train test continuous learning approaches research topic currently receiving much attention. unfortunately existing datasets well suited purpose lack fundamental ingredient presence multiple views objects taken different sessions focusing object recognition consider three continuous learning scenarios instances training patterns classes become available subsequent batches poses conditions good model expected incrementally consolidate knowledge known classes without compromising learned before. classes training patterns belonging different classes become available subsequent batches. case model able deal classes without losing accuracy previous ones. instances classes training patterns belonging known classes become available subsequent training batches. good model expected consolidate knowledge known classes learn ones. appears clear dealing complex scenarios datasets/benchmarks speciﬁcally designed continuous learning needed order evaluate compare emerging approaches. particular ideal dataset composed high number classes added subsequent batches important large number views class acquired different sessions. presence temporal coherent sessions another feature since temporal smoothness used simplify object detection improve classiﬁcation accuracy address unsupervised scenarios section review existing datasets better clarify motivations induced propose benchmark. addressing real-world continuous learning assuming whole past data used training step biological learning also unlikely application engineering. fact cumulative approach would require updating already trained model data much feasible term computation memory constraints. recent advances transfer learning/tuning deep neural networks shown using previously learned knowledge similar tasks useful solving ones little done context continuous learning model required solve tasks maintaining good performances previous ones. indeed preserving previously learned knowledge without re-accessing patterns remains particularly challenging phenomenon known literature catastrophic forgetting learned seriously compromised. contributions paper summarized follows section review related literature compare datasets used continuous object recognition. section describes core. dataset intended continuous object recognition section provide overview accuracy achieved training recent cnns whole training data section introduce continuous learning scenario simple approaches used baseline references testing developments core. finally section conclusions drawn. last years witnessed renewed interest continuous learning supervised classiﬁcation reinforcement learning. several interesting approaches proposed learning without forgetting progressive neural networks active long term memory networks adaptive convolutional neural network pathnet incremental regularized least squares elastic weight consolidation encoder-based lifelong learning etc. elastic weight consolidation appears particularly intriguing since provides formal criterion identify subset weights changed model update reduce forgetting. learning without forgetting evolutions also interesting since prove enforcing output stability helps control forgetting time provides enough degrees freedom learn task. unfortunately experimentations performed setups appropriate learn short sequences tasks capabilities assessed continuous learning scenarios characterized frequent updates. example classiﬁcation tests carried permuted mnist task tasks learn obtained scrambling pixel positions mnist dataset. although original ingenious experiment truly linked real-world applications. experiments performed consider pairs large datasets proves model trained ﬁrst dataset trained second without forgetting ﬁrst task; even deal multiple tasks simple experiments reported multiple tasks. furthermore scenario addressed. table compare datasets/benchmarks which opinion could used continuous object recognition. datasets temporal coherent sequences available excluded. principle datasets could used continuous learning well think temporally coherent sequences allow larger number realworld applications addressed youtube-m provides huge number videos acquired difﬁcult natural settings. however classes quite heterogeneous acquisition conditions completely uncontrolled terms object distance pose lighting occlusions etc. words believe challenging current continuous learning approaches ﬁrst group datasets table objects positioned turntables acquisition systematically controlled term pose/lighting. neither complex backgrounds occlusions present datasets. norb coil- deﬁned number exploration sequences turn native static benchmarks continuous learning tasks; also reports supervised semi-supervised accuracy scenario. exploration sequences generated datasets group well randomly walking adjacent static frames multivariate parameter space; however obtained sequences would remain quite unnatural. bigbrother dataset interesting incremental learning setup face recognition domain unfortunately owner copyright allow public diffusion dataset. finally icubworld datasets acquired robotic vision context closest ones core. fact objects hand hold nearly constant distance camera randomly moved. respect datasets core consists higher number longer sessions complex backgrounds also provide depth information opinion important feature core presence distinct acquisition sessions object; allows deﬁne incremental strategies long enough appreciate learning trends. preparing paper noted icubworld-transf expanded think crossevaluating continuous object recognition approaches core icubworld-transf could interesting. core speciﬁcally designed ontinuous bject cognition collection domestic objects belonging categories plug adapters mobile phones scissors light bulbs cans glasses balls markers cups remote controls classiﬁcation performed object level category level ﬁrst task much challenging objects category difﬁcult distinguished certain poses. dataset collected distinct sessions characterized different backgrounds lighting. session object seconds video recorded kinect sensor delivering rgb-d frames. objects hand hold operator camera point-of-view operator eyes. operator required extend smoothly move/rotate object front camera. subjective point-of-view objects grab-distance well-suited number robotic applications. grabbing hand changes throughout sessions relevant object occlusions often produced hand itself. data consists depth frames. depth information mapped coordinates upon calibration. acquisition interface identiﬁes central region object kept allows performs ﬁrst cropping thus reducing frame size figure acquisition interface identiﬁes central region operator required keep objects moving rotating them. since domestic objects typically extend less pixels small fraction frame contains object interest. therefore exploited temporal information crop frame around object. purpose implemented simple effective motion-based tracker working data similar approach could used even depth information available yolo however since main interest continuous object recognition stage preferred focus object classiﬁcation instead complex time demanding detection task. example). cases objects fully contained crop window sometimes extend beyond borders manual correction applied believe tracking imperfections unavoidable properly dealt later processing stages. figure example second recording object session note smooth movement pose change partial occlusion. frames shown automatically cropped images based fully automated tracker. ﬁnal dataset consists rgb-d images sessions objects frames session. figure shows frame object throughout eleven sessions. three eleven sessions selected test remaining sessions used training. tried balance much possible difﬁculty training test sessions respect indoor/outdoor holding hand complexity background. full dataset along information downloaded vlomonaco.github.io/ core. repository make available code reproducibility benchmarks described following sections. designed continuous learning core dataset still used medium size benchmark object recognition static evaluation protocol. high object pose variability complex acquisition setting make problem sufﬁciently hard solve even learning performed whole training data. table show accuracy well-known models adapted medium size trained three different modalities using data already shown many authors ﬁne-tuning pre-trained model dataset often effective strategy especially dataset large enough avoid overﬁtting large learn representative features scratch. table accuracy caffenet models core different learning strategies. test consists sessions training remaining sessions. term mid-cnn used highlight using original caffenet models adaption mid-size pixels. many researchers available pre-trained models simply stretch images model input size even image size much smaller input. stretching input pattern would require much computation inference time decided adapt pre-trained models work input images. however case pre-trained models step neutral obvious could expect details provided appendix improve classiﬁcation accuracy instead classifying single frames temporally adjacent frames fused. purpose implemented simple sum-rule fusion conﬁdence level. graph figure shows result mid-vgg model. classiﬁcation experiment tested cases concatenate frames test sequences without considering end-of-sequence events assume reset signal available. former window size increases risk fusing frames different classes increases well. general fusing frames seems good compromise even sequences cannot reliably segmented. figure mid-vgg classiﬁcation accuracy classiﬁcation conﬁdence adjacent frames fused. horizontal axis number frames fused end-of-sequence reset available using long temporal windows lead dangerous drifts continuous learning scenarios test composed sessions remaining sessions split batches provided sequentially training. since batch order affect ﬁnal result compute average runs batches randomly shufﬂed. moreover scenario provide accuracy cumulative strategy target term upper bound principle smart sequential training approach could outperform baseline cumulative training. following sections report results object level classiﬁcation task since experiments lead analogous conclusions. furthermore study models trained data scenario training batches coincides sessions available training set. fact since session includes sequence objects training model ﬁrst session tuning times line scenario classes known since ﬁrst batch successive batches provide instances classes reﬁne consolidate knowledge. baseline naïve approach scenario simply continuing training batches become available. figure compare baseline cumulative approaches. figure accuracy results naïve cumulative strategies midcaffenet mid-vgg. colored areas represent standard deviation curve. tabular data available http//vlomonaco.github.io/core. accuracy naïve cumulative approach quite modest. fact careful tuning learning rate number iterations forgetting tamed scenario model memory regularly refreshed poses similar ﬁndings reported norb coil bigbrother scenario scenario sequential batch objects recognize presented. batch contains whole training sequences small group classes therefore memory refresh possible across batches. ﬁrst batch include classes remaining batches contain classes each. slightly simplify task runs randomly chose classes biased policy privileges maximal categorical representation required classify patterns never seen classes. full test rejection option test ﬁxed includes patterns classes however model possibility reject pattern believes pattern belong known classes. since training include negative examples cannot extra neuron unknown class rejection mechanism compare class probability given threshold. option drawback increase number classes test task becomes complex difﬁcult appreciate learning trend beneﬁts subsequent batches. option realistic real applications evaluation comparison different techniques difﬁcult step instead single point curve considering comparative evaluation among continual object recognition approaches believe option good trade-off simplicity usefulness task. option also maintains test coherent across scenarios decided adopt figure mid-caffe mid-vgg accuracy scenario cumulative naïve approach full depth models lacks colored areas represent standard deviation curve. tabular data available http//vlomonaco.github.io/ core. naive approach scenario working all. graphs figure show models completely forget tasks learning classes initial accuracy drop larger size ﬁrst batch w.r.t. following ones. investigated approaches that spite simplicity performs fairly well used baseline studies skip layers directly connect pool ﬁnal layer maintaining weights pool ﬁxed. allows isolating subsets weights class uses. training sets weights maintained model pool connections consolidated weights used inference temporary weights used training; initialized ﬁrst batch randomly re-initialized training batch. batch training weights corresponding classes current batch copied trivial scenario class segregation different batches. words seen sort hippocampus consolidated concepts maintained short term working memory cortex used learn concepts without interfering stable ones simple approaches implemented tested used freeze class weights already encountered classes; without re-init step; however shown figure results obtained signiﬁcantly worse cwr. third last scenario classes instances presented training batch. scenario closest many real-world applications agent continuously learns objects reﬁnes knowledge previously discovered ones. scenario ﬁrst batch includes classes subsequent batches classes each. however training sequence class included batch thus resulting double partitioning scheme total number batches maximized categorical representation ﬁrst batch left composition order subsequent batches completely random. test scenarios. figure mid-caffe mid-vgg accuracy scenario colored areas represent standard deviation curve. tabular data available http//vlomonaco. github.io/core. update number times class encountered far. figure reports accuracy naïve cumulative approaches. graph clearly shows scenario difﬁcult room improvements. biological learning requires neither store perceptual data streams process cumulative way; however effectively tackles incremental learning tasks object representations continuously learned consolidated useless forgot. artiﬁcial learning systems still lack capabilities research needed gap. paper introduced dataset associated benchmarks support continuous learning studies context object recognition. argued many researchers results also prove naïve approaches incremental tuning cannot avoid catastrophic forgetting complex real-world scenarios like nic. testing approaches core important indicator absolute accuracy speciﬁc scenarios relative accuracy w.r.t. corresponding cumulative approach. fact using state-of-the-art full size easily exceed absolute accuracy reported mid-size cnns really effective continual learning techniques signiﬁcantly reduce w.r.t. corresponding cumulative approaches. proposed approach performs better naïve solution accuracy drops w.r.t. cumulative approach large leaves much room improvements. directly connecting pool freezing model parameters conv allowed easily disentangle sets weights relevant different classes sophisticate techniques needed presence multiple shared layers. interesting approaches direction intend test core near future. future steps plan extending dataset terms classes sessions; combining motion depth provide better object segmentation; classifying core objects models exploiting depth information. references sami abu-el-haija nisarg kothari joonseok paul natsev george toderici balakrishnan varadarajan sudheendra vijayanarasimhan. youtube-m large-scale video classiﬁcation benchmark. arxiv preprint arxiv. borji saeed izadi laurent itti. ilab-m large-scale controlled object dataset investigate deep learning. international conference computer vision pattern recogniton pages raffaello camoriano giulia pasquale carlo ciliberto lorenzo natale lorenzo rosasco giorgio metta. incremental robot learning objects fixed update time. arxiv preprint arxiv. chatﬁeld karen simonyan andrea vedaldi andrew zisserman. return devil details delving deep convolutional nets. proceedings british machine vision conference pages chrisantha fernando dylan banarse charles blundell yori zwols david rusu alexander pritzel daan wierstra. pathnet evolution channels gradient descent super neural networks. arxiv preprint arxiv. annalisa franco dario maio davide maltoni. brother database evaluating face recognition smart home environments. advances biometrics third international conference pages goodfellow mehdi mirza xiao aaron courville yoshua bengio. empirical investigation catastrophic forgeting gradient-based neural networks. arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. spatial pyramid pooling deep convolutional networks visual recognition. ieee transactions pattern analysis machine intelligence james kirkpatrick razvan pascanu neil rabinowitz joel veness guillaume desjardins andrei rusu kieran milan john quan tiago ramalho agnieszka grabska-barwinska demis hassabis claudia clopath dharshan kumaran raia hadsell. overcoming catastrophic forgetting neural networks. proceedings national academy sciences volume pages yann. lecun huang leon. bottou. learning methods generic object recognition invariance pose lighting. proceedings ieee computer society conference computer vision pattern recognition volume pages zhizhong derek hoiem. learning without forgetting. computer vision eccv european conference amsterdam netherlands october proceedings part volume lncs pages vincenzo lomonaco davide maltoni. comparing incremental learning strategies convolutional neural networks. artiﬁcial neural networks pattern recognition iapr workshop pages giulia pasquale carlo ciliberto francesca odone lorenzo rosasco lorenzo natale. teaching icub recognize objects using deep convolutional neural networks. proceedings workshop machine learning interactive systems pages giulia pasquale carlo ciliberto lorenzo rosasco lorenzo natale. object identiﬁcation examples improving invariance deep convolutional neural network. ieee/rsj international conference intelligent robots systems pages shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection region proposal networks. ieee transactions pattern analysis machine intelligence andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv preprint arxiv. schwarz hannes schulz sven behnke. rgb-d object recognition pose estimation based pre-trained convolutional neural network features. ieee international conference robotics automation arjun singh james karthik narayan tudor achim pieter abbeel. bigbird large-scale database object instances. ieee international conference robotics automation pages rupesh kumar srivastava jonathan masci sohrob kazerounian faustino gomez jürgen schmidhuber. compete compute. advances neural information processing systems pages jeremy steward derek lichti jacky chow reed ferber sean osis canada key. performance assessment calibration kinect time-of-flight range camera motion capture applications. working week pages recent years pervasiveness deep neural networks complexity training architectures datasets remarkable size proliferation pre-trained models represent good starting point many customized solutions. however approach requires adapting problem-speciﬁc data ﬁxed size architecture designed optimized solve another task. context computer vision object recognition example common stretch images arbitrary sizes pixels typical input wellknown models pre-trained imagenet often leads highly distort original patterns signiﬁcantly increases inference time. elegant approach adapting pre-trained model work input patterns different size. straightforward convolution pooling layers thanks local connections much problematic fully connected layers whose number weights depends input image size. case main strategies used applying ﬁxed size pooling last convolution/pooling layer proposed however ﬁnetuning upper levels might necessary input scale changes dramatically original model designed ﬁxed-size pooling layer all. reusing pre-trained network last convolution layer retraining fully connected layers scratch task input size. typical approach also train external classiﬁer pooled features last convolutional layer. independently network adaption different input size problem classes change ﬁnal softmax layer needs replaced re-trained scratch. since experiments used classic caffenet models aimed fast processing opted second strategy. hence reshaped input volume halved number units fully connected layers re-trained scratch. results relevant table accuracy differences reduced size cnns corresponding full-size models classes task. models pre-trained ilsvrc-. training ﬁne-tuning performed core. speedup inference time resulting mid-size models suitable tuned core native frames. table summaries ﬁndings. full-size models extracting features pool nearly equivalent terms accuracy lack fully pre-trained mid-size models critical. however experiments mid-size networks loose accuracy respect original version. similar observed networks ﬁnetuned reason accuracy drop totally clear hand consider ﬁnetuning experiments pre-trained higher number patterns full-size networks therefore reasonable expect higher accuracy; hand consider pool experiments network exploits pre-training stretching input patterns information. similar experiments datasets obtained close results seems zoomed image even blurred allow detailed feature extraction performed network. spatial scale ﬁlters learned ilsvrc- richer hierarchical representation believe investigations necessary fully understand reasons make available pre-trained mid-size networks competitive full-size ones.", "year": 2017}