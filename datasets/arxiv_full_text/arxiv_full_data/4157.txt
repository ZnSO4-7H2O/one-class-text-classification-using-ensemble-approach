{"title": "Places: An Image Database for Deep Scene Understanding", "tag": ["cs.CV", "cs.AI"], "abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.", "text": "abstract—the rise multi-million-item dataset initiatives enabled data-hungry machine learning algorithms reach nearhuman semantic classiﬁcation tasks object scene recognition. describe places database repository million scene photographs labeled scene semantic categories attributes comprising quasi-exhaustive list types environments encountered world. using state convolutional neural networks provide impressive baseline performances scene classiﬁcation. high-coverage high-diversity exemplars places database offers ecosystem guide future progress currently intractable visual recognition problems. take reach human-level performance machine-learning algorithm? case supervised learning problem two-fold. first algorithm must suitable task pattern classiﬁcation case object recognition pattern localization object detection necessity temporal connections different memory units natural language processing second must access training dataset appropriate coverage density optimal space datasets often taskdependent rise multi-million-item sets enabled unprecedented performance many domains artiﬁcial intelligence. successes deep blue chess watson jeopardy alphago expert human opponents thus seen advances algorithms increasing availability large datasets million million items respectively convolutional neural networks likewise achieved near human-level visual recognition trained million object million scene images expansive coverage space classes samples allows getting closer right ecosystem data natural system like human would experience. describe places database quasi-exhaustive repository million scene photographs labeled scene semantic categories attributes comprising types visual environments encountered world. image samples shown fig. context places places database coverage categorical space primary asset high-quality dataset expansive coverage categorical space want learn. strategy places provide exhaustive list categories environments encountered world bounded spaces human body would dataset provided initial list semantic categories. dataset built around quasi-exhaustive list scene categories different functionalities namely categories unique identities discourse. wordnet database team selected words concrete terms described scenes places environments used complete phrase place let’s the/a place. words referred basic entry-level names resulting corpus different scene categories bundling together synonyms separating classes described word referring different environments details building initial corpus found places database inherited list scene categories dataset. construction database step downloading images using scene category adjectives online image search engines candidate images downloaded using query word list scene classes provided database order increase diversity visual appearances places dataset scene class query combined common english adjectives million images unique urls identiﬁed. importantly places datasets complementary pcabased duplicate removal conducted within scene category databases contain images. step labeling images ground truth category image ground truth label veriﬁcation done crowdsourcing task amazon mechanical turk fig. illustrates experimental paradigm used workers given instructions relating particular image category time deﬁnition samples true false images. workers performed go/no-go categorical task experimental interface displayed central image ﬂanked smaller version images worker responded left respond next right. information gleaned construction dataset suggests ﬁrst iteration labeling show downloaded images true exemplars category. illustrated fig. default answer worker easily press space move majority images forward. whenever true category exemplar appears center worker press speciﬁc mark positive exemplar reaction time moment image centrally placed space press recorded. interface also allows moving backwards revise previous annotations. fig. annotation interface amazon mechanical turk selecting correct exemplars scene downloaded images. left plot shows instruction given workers deﬁne positive negative examples. right plot shows binary selection interface. consisted images manual annotation. control positive samples negative samples ground-truth category labels database intermixed well. worker hits accuracy higher control images kept. positive images resulting ﬁrst cleaning iteration sent second iteration cleaning. used task interface default answer yes. second iteration images relabeled tested third iteration exemplars pursue percentage images relabeled signiﬁcant. iterations annotation collected scene label images pertaining scene categories. expected number images scene category vary greatly scene categories ended least exemplars scene categories exemplars. step scaling dataset using classiﬁer result previous round image annotation million remaining downloaded images assigned scene categories therefore third annotation task designed re-classify re-annotate images using semiautomatic bootstrapping approach. deep learning-based scene classiﬁer alexnet trained classify remaining million images ﬁrst randomly selected images scene category training images validation alexnet achieved scene classiﬁcation accuracy validation training used classify fig. boundaries place categories blurry images made mixture different components. images shown ﬁgure show soft transition ﬁeld forest. although extreme images easily classiﬁed ﬁeld forest scenes middle images ambiguous. million images. used predicted class score alexnet rank images within scene category follow given category exemplars ranked images predicted class conﬁdence higher sent third round manual annotation using interface shown fig.. default answer completing third round annotation distribution number images category ﬂattened scene categories images category scene categories images. totally million images added dataset. step classes despite initial effort bundle synonyms wordnet scene list database still contained categories close synonyms identiﬁed synonym pairs like merged images single category. fig. annotation interface amazon mechanical turk differentiating images similar categories. left plot shows instruction give several typical examples category. right plot shows binary selection interface worker needs select shown image either class none. additionally scene categories easily confused blurry categorical boundaries illustrated fig. means answering question does image belong class might difﬁcult. easier answer question does image belong class case decision boundary becomes clearer human observer also gets closer ﬁnal task computer system trained solve. indeed previous three steps annotation became apparent workers confused pairs scene categories instance putting images ‘canyon’ ‘butte’ ‘mountain’ putting ‘jacuzzi’ ‘swimming pool indoor’ mixing images ‘pond’ ’lake’ ‘volcano’ ‘mountain’ ‘runway’ ‘landing deck’ ‘highway road’ ‘operating room’ ‘hospital room’ etc. whole categories identiﬁed ambiguous pairs. differentiate images categories shared content designed interface fourth step annotation. combined exemplar images categories shared content asked workers classify images either categories neither them. scene-centric datasets correspond images labeled scene place name opposed object name. fig. illustrates differences among number images found places imagenet scene categories common three datasets. places database largest scene-centric image dataset far. deﬁning benchmarks places describe four subsets places benchmarks. places places benchmarks added categories selected categories images create places-standard places-challenge. places-standard training images image number class varying validation images class test images class. note experiments paper reported places-standard. places-challenge contains categories places-standard training signiﬁcantly larger total million training images. validation testing placesstandard. subset released places challenge held conjunction european conference computer vision part ilsvrc challenge. places. places described million images scene categories. image number class varies training total images images category validation images category test set. places. places contains common scene categories among imagenet places databases. note places contains images obtained round annotations ﬁrst version places used call corresponding subsets places imagenet sun. subsets used compare performances across different scene-centric databases three datasets contain different exemplars category. note ﬁnding correspondences classes deﬁned imagenet places brings challenges. imagenet follows wordnet deﬁnitions wordnet deﬁnitions always appropriate describing places. instance class ’elevator’ imagenet refers object. places ’elevator’ takes different meanings depending location observer elevator door elevator interior elevator lobby. many categories imagenet differentiate indoor outdoor places indoor outdoor versions separated necessarily afford function. dataset diversity given types images found internet categories biased others terms viewpoints types objects even image style however bias compensated high diversity images next section describe measure dataset diversity compare diverse images three scenecentric datasets are. comparing datasets open problem. even datasets covering visual classes notable differences providing different generalization performances used train classiﬁer beyond number images categories aspects important difﬁcult quantify like variability camera poses decoration styles type objects appear scene. although quality database often task dependent reasonable assume good database dense diverse imagine instance dataset composed images taken within bedroom. dataset would high density diversity images look similar. ideal dataset expected generalize well high diversity well. achieve high density collecting large number images diversity obvious quantity estimate image sets assumes notion similarity images. estimate similarity question images similar? however similarity wild subjective loose concept images viewed similar contain similar objects and/or similar spatial conﬁgurations and/or similar decoration styles circumvent problem deﬁne relative measures similarity comparing datasets. several measures diversity proposed particularly biology characterizing richness ecosystem review). here propose measure inspired simpson index diversity simpson index measures probability random individuals ecosystem belong species. measure well distributed individuals across different species ecosystem related entropy distribution. extending measure evaluating diversity images within category non-trivial annotations subcategories. reason propose measure relative diversity image datasets based following idea diverse random images likely visually similar random samples then diversity respect deﬁned divb randomly selected. deﬁnition relative diversity diverse divb diva. arbitrary number datasets randomly selected. measured relative diversities imagenet places using amt. workers presented different pairs images select pair contained similar images. pairs randomly sampled database. trial composed pairs database giving total pairs choose from. used pairs results show large variation terms diversity among three datasets showing places diverse three datasets. average relative diversity dataset places imagenet sun. illustrate categories largest variation diversity across three datasets playground veranda waiting room. convolutional neural networks scene classification given impressive performance deep convolutional neural networks particularly imagenet benchmark choose three popular architectures alexnet googlenet convolutional-layer train places places-standard respectively create baseline models. trained cnns named placessubset-cnn i.e. places-alexnet placesvgg. places-cnns presented trained using caffe package nvidia gpus tesla titan additionally given recent breakthrough performances residual network imagenet classiﬁcation ﬁne-tuned resnet places-standard compared trained-from-scratch placescnns scene classiﬁcation. results places places training various places-cnns used ﬁnal output layer network classify test images places classiﬁcation results top- accuracy top- accuracy listed table baseline comparison show results linear trained imagenet-cnn features images category places images category respectively. places-cnns perform much better imagenet feature+svm baseline while expected placesgooglenet places-vgg outperformed placesalexnet large margin deeper structures. date ranked results test places leaderboard top- accuracy top- accuracy. note test didn’t ﬁne-tune places-cnns training directly evaluated test sun. fig. examples pairs diversity experiment playground bedroom. pair shows similar images? bottom pairs chosen examples. histogram relative diversity category dataset. places contains diverse images imagenet lowest diversity database images prototypical class. database increase chances ﬁnding similar pair avoiding users skip trials. workers select similar pair trial. trials category observers trial categories common imagenet places databases. fig. .a-b shows examples pairs diversity experiments scene categories playground bedroom ﬁgure pair database shown. observed different annotators consistent deciding whether pair images similar another pair images. fig. shows histograms relative diversity scene categories common three databases. three datasets identical terms diversity average diversity three datasets. note measure diversity relative measure three datasets. experiment users selected pairs database closest time pairs places database judged similar trials. imagenet pairs selected time. places categories places accuracy places-cnns test drops fig. shows responses examples correctly predicted places-vgg. top- responses relevant scene description. failure ambiguous cases shown fig. broadly identify kinds misclassiﬁcation given current label attribution places less-typical activities happening scene taking group photo construction site camping junkyard; images composed multiple scene parts make ground-truth scene label sufﬁcient describe whole environment. illustrate need multi-ground truth labels describing environments. important emphasize many scene categories top- accuracy might ill-deﬁned measure environments inherently multi-labels terms semantic description. different observers different terms refer place different parts environment labels might well description scene. obvious examples fig.. future development places database places challenge explore assign multiple ground truth labels free-form sentences images better capture richness visual descriptions inherent environments. web-demo scene recognition based places-cnn trained created webdemo scene recognition accessible computer browser mobile phone. people upload photos web-demo predict type environment likely semantic categories relevant scene attributes. screenshots prediction result mobile phone shown fig.. note people submit feedback result. top- recognition accuracy recognition web-demo wild impressive given people uploaded kinds photos real-life fig. examples predictions rated incorrect validation places-vgg. states ground truth label. note responses often wrong predicting semantic categories near category. text details. generic visual features imagenet-cnns places-cnns used activation trained placescnns generic features visual recognition tasks using different image classiﬁcation benchmarks. activations higher-level layers also termed deep features proven effective generic features state-ofthe-art performance various image datasets fig. predictions given places-vgg images validation set. ground-truth label predictions shown. number beside label indicates prediction conﬁdence. evaluated classiﬁcation performances deep features object-centric cnns scenecentric cnns systematic way. deep features several places-cnns imagenet-cnns following scene object benchmarks tested indoor scene attribute caltech caltech stanford action uiuc event experiments follow standards papers. experiment training size images category. experiments splits training test given dataset. indoor experiment training size images category. experiment split training test given dataset. scene experiment training size images category. experiments random splits training test set. attribute experiment training size images attribute. reported result average precision. splits training test given paper. caltech caltech experiment training size images category. experiments random splits training test set. stanford action experiment training size images category. experiments fig. screenshots scene recognition demo based places-cnn. web-demo predicts type environment semantic categories associated scene attributes uploaded photos. random splits training test set. reported result classiﬁcation accuracy. uiuc event experiment training size images category test size images category. experiments random splits training test set. places-cnns imagenet-cnns network architectures alexnet googlenet trained scene-centric data object-centric data respectively. alexnet used -dimensional feature vector activation fully connected layer cnn. googlenet used -dimensional feature vector response global average pooling layer softmax producing class predictions. classiﬁer experiments linear default parameter features. table summarizes classiﬁcation accuracy various datasets deep features places-cnns deep features imagenet-cnns. fig. plots classiﬁcation accuracy different visual features database different numbers training samples category. classiﬁer linear default parameters deep feature layers places-cnn features show impressive performance scene-related datasets outperforming imagenet-cnn features. hand imagenet-cnn features show better performance object-related image datasets. importantly comparison shows places-cnn imagenet-cnn complementary strengths scenecentric tasks object-centric tasks expected type datasets used train networks. hand deep features placesvgg achieve best performance challenging scene classiﬁcation dataset deep features places-vgg performs best indoor dataset. know state-of-the-art scores achieved single feature linear datasets. furthermore merge classes imagenet classes places-standard train deep feature hybrid-vgg achieves best score averaged eight image datasets. visualization internal units cnns visualization units responses various levels network layers better understanding learned inside cnns differences object-centric trained imagenet scene-centric trained places given share architecture following methodology estimated receptive ﬁelds units places-cnn imagenet-cnn. segmented images high unit activation using estimated receptive ﬁelds. image segmentation results receptive ﬁelds units fig. classiﬁcation accuracy dataset. compare deep features placesvgg places-alexnet imagenet-alexnet hand-designed features. deep features places-vgg outperforms deep features hand-designed features large margins. results hand-designed features/kernels fetched different layers shown fig.. pool pool units detect visual concepts low-level edge/texture high-level object/scene parts. furthermore object-centric imagenet-cnn units detecting object parts people’s heads pool layer scene centric placescnn units detecting scene parts chair buildings pool layer. synthesized preferred input images places-cnn using image synthesis technique proposed method uses learned prior deep generator network generate images maximize ﬁnal class activation intermediate unit activation places-cnn. synthetic images scene categories shown fig.. abstract image contents reveal knowledge speciﬁc scene learned memorized places-cnn examples include buses within road environment station tents surrounded forest-types features campsite. used places-alexnet used synthesis technique generate images preferred units pool layer places-alexnet. shown fig. synthesized images similar segmented image regions estimated receptive ﬁeld units. deep feature places-alexnet places-alexnet imagenet-alexnet places-googlenet places-googlenet imagenet-googlenet places-vgg places-vgg imagenet-vgg hybrid-vgg fig. visualization units’ receptive ﬁelds different layers imagenet-cnn places-cnn. subsets units layer shown. show activated images. images segmented based estimated receptive ﬁelds units different layers imagenet-cnn placescnn. take imagenet-alexnet places-alexnet comparison examples. detailed visualization methodology fig. synthesized images preferred pool units places-alexnet corresponds segmented images receptive ﬁelds units. synthetic images similar segmented image regions units. segmented images correspond unit. patterson hays attribute database discovering annotating recognizing scene attributes proc. cvpr fei-fei fergus perona learning generative visual models training examples incremental bayesian approach tested object categories computer vision image understanding nguyen dosovitskiy yosinski jason band brox clune synthesizing preferred inputs neural networks deep generator networks arxiv preprint arxiv. torralba fergus freeman million tiny images large data nonparametric object scene recognition ieee trans. pattern analysis machine intelligence cordts omran ramos rehfeld enzweiler benenson franke roth schiele cityscapes dataset semantic urban scene understanding arxiv preprint arxiv. conclusion tiny image dataset imagenet places rise multi-million-item dataset initiatives densely labeled datasets enabled data-hungry machine learning algorithms reach nearhuman semantic classiﬁcation visual patterns like objects scenes. high-coverage high-diversity exemplars places offers ecosystem visual context guide progress currently intractable visual recognition problems. problems could include determining actions happening given environment spotting inconsistent objects human behaviors particular place predicting future events cause events given scene. acknowledgments authors would like thank santani teng zoya bylinskii mathew monfort caitlin mullin comments paper. years places project supported national science foundation grants a.t; a.o; well data initiative csail toyota google xerox amazon awards hardware donation nvidia corporation a.t. supported facebook fellowship. references krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems zhou lapedriza xiao torralba oliva learning deep features scene recognition using places database advances neural information processing systems girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation hochreiter schmidhuber long short-term memory neural computation silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search nature russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge international journal computer vision", "year": 2016}