{"title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "This tutorial introduces a new and powerful set of techniques variously called \"neural machine translation\" or \"neural sequence-to-sequence models\". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.", "text": "tutorial introduces powerful techniques variously called neural machine translation neural sequence-to-sequence models. techniques used number tasks regarding handling human language powerful tool toolbox anyone wants model sequential data sort. tutorial assumes reader knows basics math programming assume particular experience neural networks natural language processing. attempts explain intuition behind various methods covered delves enough mathematical detail understand concretely culiminates suggestion implementation exercise readers test understood content practice. getting details might worth describing terms appear title neural machine translation sequence-to-sequence models. machine translation technology used translate human language. think universal translation device showing sci-ﬁ movies allow communicate eﬀortlessly speak diﬀerent language plethora online translation sites assimilate content native language. ability remove language barriers needless potential useful thus machine translation technology researched shortly advent digital computing. call language input machine translation system source language call output language target language. thus machine translation described task converting sequence words source converting sequence words target. goal machine translation practitioner come eﬀective model allows perform conversion accurately broad variety languages content. second part title sequence-to-sequence models refers broader class models include models sequence another. this course includes machine translation also covers broad spectrum methods used handle tasks shown figure fact think computer program something takes sequence input bits outputs sequence output bits could every single program sequence-to-sequence model expressing behavior machine translation widely-recognized useful instance sequence-to-sequence models allows many intuitive examples demonstrating diﬃculties encountered trying tackle problems. tutorial ﬁrst starts general mathematical deﬁnition statistical techniques machine translation section rest tutorial sequentially describe techniques increasing complexity leading attentional models represent current state-of-the-art ﬁeld. first sections focus language models calculate probability target sequence interest. models capable performing translation sequence transduction provide useful preliminaries understand sequence-to-sequence models. probability next word based features context. describes learn parameters models stochastic gradient descent calculating derivatives gradually updating parameters increase likelihood observed data. together multiple pieces information easily log-linear models resulting increased modeling accuracy. gives example feed-forward neural language models calculate probability next word based previous words using neural networks. mechanisms allow remember information multiple time steps. lead recurrent neural network language models allow handling long-term dependencies useful modeling language sequential data. encode target sequence vector numbers another network decode vector numbers output sentence. also describes search algorithms generate output sequences based model. input sentence generating translations. allows eﬃcient intuitive method representing sentences often eﬀective simpler encoder-decoder counterpart. parameters model specifying probability distribution. parameters learned data consisting aligned sentences source target languages called parallel corpora technical terminology.within framework three major problems need handle appropriately order create good translation system assess naturalness given sentence tell look like actual natural sentence target language? learn model tell this assess ﬂuency sentences generated automated system improve results. could also used evaluate sentences generated human purposes grammar checking error correction. language model interesting model thinks natural-looking sentences practically useful context neural translation models described following chapters. joint probability length sentence identity ﬁrst word sentence identity second word sentence last word sentence unfortunately directly creating model probability distribution straightforward length sequence determined advance large number possible combinations words. make things easier common re-write probability full sentence product single-word probabilities. takes advantage fact joint probability example calculated multiplying together conditional probabilities elements. example means sentence. stepping equation order means ﬁrst calculate probability coming beginning sentence probability went coming next sentence starting probability home coming sentence preﬁx formulation equation problem language modeling much manageable calculating probability whole sentence ﬁxed items looking calculate probabilities for. next couple sections show ways ﬁrst calculate probabilities simple prepare training data count word strings count number times seen particular string words divide number times seen context. simple method beginning sentence training data. approach called maximum likelihood estimation simple guaranteed create model assigns high probability sentences training data. however let’s want model assign probability sentence we’ve never seen before. example want calculate probability sentence utah based training data example. sentence extremely similar sentences we’ve seen before unfortunately string utah observed training data cpreﬁx becomes zero thus probability whole sentence calculated equation also becomes zero. fact language model assign probability zero every sentence hasn’t seen training corpus useful model loses ability tell whether sentence system generates natural generate outputs. solve problem take measures. first instead calculating probabilities beginning sentence ﬁxed window previous words upon limit base probability calculations approximating true probability. utah utah somewhere training corpus thus patch together probabilities calculate non-zero probability whole sentence. however still problem encounter two-word string never appeared training corpus? case we’ll still zero probability particular two-word string resulting full sentence probability also becoming zero. ngram models problem smoothing probabilities combining maximum likelihood estimates various values simple case smoothing unigram bigram probabilities think model combines together probabilities follows variable specifying much probability mass hold unigram distribution. long regardless context words vocabulary assigned probability. method called interpolation standard ways make probabilistic models robust low-frequency phenomena. interpolation coeﬃcient context αet− weight higher order n-grams suﬃcient number training examples parameters estimated accurately fall back lower-order n-grams fewer training examples. context-dependent smoothing coeﬃcients chosen using heuristics learned data back-oﬀ equation interpolated together probability distributions full vocabulary alternative formulation back-oﬀ lower-order distribution used calculate probabilities words given probability zero higher-order distribution. back-oﬀ expressive also complicated interpolation reported give similar results modiﬁed distributions also possible diﬀerent distribution pml. done subtracting constant value counts calculating probabilities method called discounting. also possible modify counts currently modiﬁed kneser-ney smoothing generally considered standard eﬀective methods smoothing n-gram language models. uses context-dependent smoothing coeﬃcients discounting modiﬁcation lower-order distributions ensure accurate probability estimates. development data used make choices alternate models tune hyperparameters model. hyper-parameters model could include maximum length n-gram model type smoothing method. language models basically want know whether model accurate model language number ways deﬁne this. straight-forward deﬁning accuracy likelihood model respect development test data. likelihood parameters respect data equal likelihood used couple reasons. ﬁrst probability particular sentence according language model small number product small numbers become small number cause numerical precision problems standard computing hardware. second sometimes convenient mathematically deal space. example taking derivative gradient-based methods optimize parameters convenient deal equation product equation intuitive explanation perplexity confused model decision? accurately expresses value randomly picked words probability distribution calculated language model time step average many words would pick correct one? reason common perplexities research papers numbers calculated perplexity bigger making diﬀerences models easily perceptible human eye. assume closed vocabulary sometimes assume words test set. example calculating language model ascii characters reasonable assume characters observed training set. similarly speech recognition systems common simply assign probability zero words don’t appear training data means words able recognized. interpolate unknown words distribution mentioned equation interpolate distributions higher lower order. case unknown words think distribution order deﬁne -gram probability interpolation unigram distribution unknown word distribution here punk needs distribution assigns probability words vall ones vocabulary derived training corpus. could done example training language model characters spells unknown words case don’t exist vocabulary. alternatively simpler approximation nonetheless fairer ignoring unknown words guess total number representing unknown words. common remove singletons words appear training corpus. this explicitly predict contexts seeing unknown word instead implicitly predicting read detail n-gram language models gives nice introduction comprehensive summary number methods overcome various shortcomings vanilla n-grams like ones mentioned above. large-scale language modeling language models integral part many commercial applications applications common build language models using massive amounts data harvested sources. handle data research eﬃcient data structures distributed parameter servers lossy compression algorithms language model adaptation many situations want build language model speciﬁc speaker domain. adaptation techniques make possible create large general-purpose models adapt models closely match target case reach much farther back sentence even span across whole documents. recurrent neural network language models introduce section handle problem also non-neural approaches cache language models topic models skip-gram models syntax-based language models also models take account syntax target sentence. example possible condition probabilities words occur directly next sentence close syntactically handle unknown words uniform distribution method described section assuming words english vocabulary. sanity check better report number unknown words portions per-word log-likelihood incurred main model portion incurred unknown word probability punk. order ﬁrst need data make easier start pre-processed data german-english translation task iwslt evaluation campaign here http//phontron.com/data/iwslt-en-de-preprocessed.tar.gz. potential improvements model include reading implementing better smoothing method implementing better method handling unknown words implementing advanced methods section calculating features log-linear language models revolve around concept features. short features basically something context useful predicting next word. formally deﬁne feature function takes context input outputs real-valued feature vector describe context using diﬀerent features. example bi-gram models previous chapter know identity previous word something useful predicting next word. want express identity previous word real-valued vector assume http//iwslt.org noted cited papers call maximum entropy language models. models chapter motivated ways log-linear models calculate un-normalized log-probability scores function normalize probabilities maximum-entropy models spread probability mass evenly possible given constraint must model training data. maximum-entropy interpretation quite interesting theoretically interested readers reference learn more explanation log-linear models simpler conceptually thus description chapter. course limited considering previous word. could also calculate one-hot vectors concatenate together would allow create model considers values previous words. fact many types feature functions think ability ﬂexibly deﬁne features advantages log-linear language models standard n-gram models. thing note special case one-hot vectors sparse vectors elements zero. also think equation diﬀerent numerically equivalent make computation eﬃcient. speciﬁcally instead multiplying large feature vector large weight matrix together columns weight matrix active features follows column allows think calculating scores look vector features active instance together instead writing matrix math. example calculation paradigm feature calculating probabilities noted scores arbitrary real numbers probabilities negative greater restriction one. this scores function performs following transformation next optimize parameters reduce loss. many methods recent years go-to methods stochastic gradient descent iterative process randomly pick single word take step improve likelihood respect order ﬁrst calculate derivative loss respect features full feature vanilla variety quite simple still competitive method optimization large-scale systems. however also things consider ensure training remains stable adjusting learning rate requires also requires carefully choose training become unstable diverge small training become incredibly slow fall local optima. handle problem learning rate decay starting higher learning rate gradually reducing learning rate near training. sophisticated methods listed below. early stopping common held-out development measure log-likelihood save model achieved best log-likelihood heldset. useful case model starts over-ﬁt training losing generalization capability re-wind saved model. another method prevent over-ﬁtting smooth convergence training common measure likelihood held-out development likelihood stops improving starts getting worse reduce learning rate. shuﬄing training order features processes training data time. nice simple eﬃcient also causes problems bias order data. example data corpus news text news articles come ﬁrst sports entertainment chance near training model hundreds thousands entertainment examples resulting parameters moving space favors recently seen training examples. prevent problem common randomly shuﬄe order training data presented learning algorithm every pass data. momentum instead taking single step direction current gradient momentum keeps exponentially decaying average past gradients. reduces propensity simple jitter around making optimization move smoothly across parameter space. adagrad adagrad focuses fact parameters updated much frequently others. example model above columns weight matrix corresponding infrequent context words updated times every pass corpus bias updated every training example. based this adagrad dynamically adjusts training rate parameter individually frequently updated parameters getting smaller updates infrequently updated parameters getting larger updates. adam adam another method computes learning rates parameter. keeping track exponentially decaying averages mean variance past gradients incorporating ideas similar momentum adagrad. adam popular methods optimization greatly speeds convergence wide variety datasets facilitating fast experimental cycles. however also known prone over-ﬁtting thus high performance paramount used caution compared standard methods. context class context words grouped classes similar words instead looking one-hot vector separate entry every word could look one-hot vector entry class thus words class could share statistical strength allowing models generalize better. context suﬃx features maybe want feature ﬁres every time previous word ends ...ing common suﬃxes. would allow learn generalized patterns words tend follow progressive verbs etc. bag-of-words features instead using past words could previous words sentence. would amount calculating one-hot vectors every word previous sentence instead concatenating simply summing together. would lose information word position could capture information words tend co-occur within sentence document. also possible combine together multiple features create expressive feature also downside greatly increasing size feature space. discuss features detail section whole-sentence language models models instead predicting words one-by-one predict probability whole sentence normalize conducive introducing certain features probability distribution lengths sentences features whether sentence contains verb. discriminative language models case want language model determine whether output system good sometimes useful train directly system output re-rank outputs achieve higher accuracy even don’t real negative examples possible hallucinate negative examples still useful training exercise chapter construct log-linear language model evaluate performance. highly suggest numpy library hold perform calculations feature vectors make things much easier. never used numpy before take look tutorial started https//docs.scipy.org/ doc/numpy-dev/user/quickstart.html. writing code calculate loss function. writing code calculate gradients perform stochastic gradient descent updates. writing code evaluate language models. similarly n-gram language models measure per-word likelihood perplexity text corpus compare n-gram language models. handling unknown words similarly require uniform distribution words english vocabulary. potential improvements model include designing better feature functions adjusting learning rate measuring results researching implementing types optimizers adagrad adam. chapter describe language models based neural networks learn sophisticated functions improve accuracy probability estimates less feature engineering. moving technical detail neural networks ﬁrst let’s take look motivating example figure example farmers compatible also compatible using log-linear model features dependent problem creating another features learn vector pair words et−. case vector context farmers could assign score resolving problem. however adding combination features major disadvantage greatly expands parameters instead parameters pair need parameters triplet numbers greatly increase amount memory used model enough training examples parameters learned properly. importance diﬃculty learning using combination features number methods proposed handle features kernelized support vector machines neural networks speciﬁcally section cover neural networks ﬂexible relatively easy train large data desiderata sequence-to-sequence models. computation split stages calculation hidden layer takes input outputs vector hidden variables calculation output layer takes calculates ﬁnal result layers consist aﬃne transform using weights biases followed step function calculates following function example class neural networks called multi-layer perceptrons general mlps consist hidden layers consist aﬃne transform followed non-linear function culminating output layer calculates variety output. figure demonstrates type network better representing non-linear function figure short ﬁrst hidden layer transforms input hidden vector diﬀerent space conducive modeling ﬁnal function. speciﬁcally case space deﬁne linear function correctly calculates desired output mentioned above mlps speciﬁc variety neural network. generally neural networks thought chain functions takes input calculates desired output. power neural networks lies fact chaining together variety simpler functions makes possible represent complicated functions easily trainable parameter-eﬃcient way. fact simple single-layer described universal function approximator means approximate function arbitrary accuracy hidden vector large enough. model equation would like train parameters remembering gradient-based training methods last chapter need deﬁne loss function calculate derivative loss respect this common non-linear functions hyperbolic tangent function. tanh function shown figure looks much like softened version step function continuous gradient everywhere making conducive training gradient-based methods. number alternatives well popular rectiﬁed linear unit shown left figure short relus solve problem tanh function gets saturated small gradients absolute value input large empirical results often shown eﬀective alternative tanh including language modeling task described chapter let’s swap tanh non-linearity instead step function network proceed calculate derivatives like section first perform full calculation loss function could derivations hand precisely calculate gradients parameters model. interested readers free even simple model like above quite work error prone. complicated models like ones introduced following chapters even case. fortunately actually implement neural networks computer useful tool saves large portion pain automatic diﬀerentiation understand automatic diﬀerentiation useful think computation equation data structure called computation graph examples shown figure graphs node represents either input network result computational operation multiplication addition tanh squared error. ﬁrst graph ﬁgure calculates function interest would used want make predictions using model second graph calculates loss function would used training. thus implement general purpose training algorithm neural networks necessary implement dynamic programs well atomic forward function backward derivative computations type node would need use. trivial itself plethora toolkits either perform general-purpose auto-diﬀerentiation auto-diﬀerentiation speciﬁcally tailored machine learning neural networks implement data structures nodes backpropogation parameter optimization algorithms needed train neural networks eﬃcient reliable allowing practitioners started designing models. following sections take approach taking look create models interest toolkit called dynet programming interface makes relatively easy implement sequence-to-sequence models covered here. figure shows example implementing neural network dynet we’ll step line-by-line. lines import necessary libraries. lines specify parameters models size hidden vector number epochs we’ll perform training. line initializes dynet model store parameters attempting learn. lines initialize parameters appropriate size dimensions equations equation match. line initializes trainer update parameters model according update strategy line creates training data function figure lines deﬁne function takes input creates computation graph calculate equation first line creates computation graph hold computation particular training example. lines take parameters adds computation graph dynet variables particular training example. line takes python list representing current input puts computation graph dynet variable. line calculates hidden vector line calculates value line returns import dynet import random parameters model training hidden_size num_epochs define model optimizer model dy.model w_xh_p model.add_parameters) b_h_p model.add_parameters w_hy_p model.add_parameters) b_y_p model.add_parameters trainer dy.simplesgdtrainer define training data consisting tuples data define function would like calculate calc_function lines perform training epochs passes data line creates variable keep track loss epoch later reporting. line shuﬄes data recommended section lines perform stochastic gradient descent looping training examples. line creates computation function itself line adds computation loss function. line runs forward calculation calculate loss adds loss epoch. line runs back propagation line updates model parameters. epoch print loss epoch line make sure loss going model converging. basics down time apply neural networks language modeling feed-forward neural network language model much like log-linear language model mentioned previous section simply addition non-linear layers output. first let’s recall tri-gram log-linear language model. case assume sets features expressing identity equation log-linear model looks like this define lookup parameters model definition time vocab_size number words vocabulary embeddings_size length word embedding vector model.add_lookup_parameters) load parameters computation graph dy.lookup look vector word word vocabulary. vector called word embedding word representation vector real numbers corresponding particular words vocabulary. interesting thing expressing words vectors real numbers element vector could reﬂect diﬀerent aspect word. example element vector determining whether particular word consideration could noun another element vector expressing whether word animal another element expresses whether word countable not. figure shows example deﬁne parameters allow look vector dynet. hidden layer obtain vector model learn combination features reﬂect information regarding multiple words context. allows model expressive enough represent diﬃcult cases figure example given context cows elements vector m·cows identify word large farm animal elements m·eat corresponds relatives could potentially learn unit hidden layer active context represents things farm animals eat. next calculate score vector word done performing aﬃne transform hidden vector weight matrix |×|h| adding bias vector finally probability estimate running calculated scores purposes model chapter vectors basically viewed tunable parameters neural language model also large amount interest learning vectors tasks. methods outlined section better generalization contexts n-gram language models treat word discrete entity. using input embeddings possible group together similar words behave similarly prediction next word. order thing n-gram models would explicitly learn word classes using classes eﬀectively trivial problem quadratic number words class thus learning parameters diﬃcult face limited training data. neural networks handle problem learning nodes hidden layer represent quadratic combination feature-eﬃcient way. ability skip previous words n-gram models generally fall back sequentially longer contexts shorter contexts doesn’t allow skip word reference example word words et−. log-linear models neural networks handle skipping naturally. smaller subset. examples include methods simply true word higher score others subsampled probabilistically motivated methods importance sampling noise-contrastive estimation interestingly objective functions linear regression special variety softmax called spherical softmax possible calculate objective function ways scale linearly vocabulary size class-based softmax assigns word class divides computation steps predicting probability class given context predicting probability word given class current t−n+). advantage method context instead hierarchical softmax computational complexity models learn word representations mentioned section learn word embeddings by-product training language models. nice feature word representations language models trained purely text resulting representations capture semantic syntactic features words thus used eﬀectively improve down-stream tasks don’t annotated data part-of-speech tagging parsing usefulness extremely large number approaches proposed learn diﬀerent varieties word embeddings early work based distributional similarity dimensionality reduction recent models based predictive models similar language models general current thinking predictive models eﬀective ﬂexible .the wellknown methods continuous-bag-of-words skip-gram models implemented software wordvec deﬁne simple objectives predicting words using immediately surrounding context vice-versa. wordvec uses sampling-based approach parallelization easily scale large datasets perhaps primary reason popularity. thing note methods language models themselves calculate probability sentence many parameter estimation techniques shared. potential improvements model include tuning various parameters model. additional hidden layers? optimizer learning rate use? happens implement eﬃcient versions softmax explained section neural-network models presented previous chapter essentially powerful generalizable versions n-gram models. section talk language models based recurrent neural networks additional ability capture long-distance dependencies language. example strong constraint starting ﬁnal himself herself must match gender. similarly based subject sentence conjugation verb change. sorts dependencies exist regardless i−n+ like mentioned previous chapter never able appropriately capture this. dependencies frequent english even prevalent languages russian large number forms word must match case gender words sentence. another example long-term dependencies exist selectional preferences nutshell selectional preferences basically common sense knowledge what what. example salad fork perfectly sensible fork tool salad friend also makes sense friend companion. hand salad backpack doesn’t make much sense backpack neither tool eating companion. selectional preference violations lead nonsensical sentences also span across arbitrary length fact subjects verbs objects separated great distance. figure examples computation graphs neural networks. shows single time step. unrolled network. simpliﬁed version unrolled network gray boxes indicate function parameterized finally also dependencies regarding topic register sentence document. example would strange document discussing technical subject suddenly started going sports violation topic consistency. would also unnatural scientiﬁc paper suddenly informal profane language lack consistency register. recurrent neural networks variety neural network makes possible model long-distance dependencies. idea simply connection references previous hidden state calculating hidden state written equations time steps diﬀerence hidden layer standard neural network addition connection whhht− hidden state time step connecting time step recursive equation uses performing visual display rnns also common unroll neural network time shown figure makes possible explicitly information multiple time steps. unrolling network still dealing standard computation graph form feed-forward networks still forward computation backward propagation making possible learn parameters. also makes clear recurrent network start somewhere initial hidden state initial state often vector full zeros treated parameter hinit learned initialized according information finally simplicity common abbreviate whole recurrent neural network step single block shown figure example boxes corresponding function applications gray show internally parameterized convention future represent parameterized functions. rnns make possible model long distance dependencies ability pass information timesteps. example nodes encode information subject sentence male possible pass information turn pass sentence. ability pass information across arbitrary number consecutive time steps strength recurrent neural networks allows handle long-distance dependencies described section thing noted compared feed-forward language model feeding previous word instead previous words. reason expect information previous words already included making unnecessary feed information directly. function following simpliﬁed view drawing rnns figure however rnns previous section conceptually simple also problems vanishing gradient problem closely related cousin exploding gradient problem. conceptual example vanishing gradient problem shown figure example recurrent neural network makes prediction several times steps model could used classify documents perform kind prediction sequence text. makes prediction gets loss expected backpropagate time steps neural network. however time step back propagation algorithm gradient gets smaller smaller time back beginning sentence gradient small eﬀectively ability signiﬁcant eﬀect parameters need updated. reason eﬀect happens unless dht− exactly tend either diminish amplify gradient exponential eﬀect gradient loss. method solve problem case diminishing gradients neural network architecture speciﬁcally designed ensure derivative recurrent function exactly one. neural network architecture designed purpose enjoyed quite success popularity wide variety sequential processing tasks long short-term memory neural network architecture. fundamental idea behind lstm addition standard hidden state used neural networks also memory cell gradient exactly one. gradient exactly information stored memory cell suﬀer vanishing gradients thus lstms capture long-distance dependencies eﬀectively standard recurrent neural networks. equation equation input gate output gate lstm respectively. function gates indicated name either allow information pass block passing. gates perform aﬃne transform this particularly detrimental case receive loss sentence like example above. real-life example scenario document classiﬁcation this rnns less successful task methods convolutional neural networks suﬀer vanishing gradient problem shown pre-training language model attempting perform classiﬁcation help alleviate problem extent output another function. results gating eﬀect result sigmoid close particular vector position little eﬀect input result sigmoid close zero block input setting resulting value zero equation important equation lstm equation implements intuition must equal allows conquer vanishing gradient problem. equation sets equal update modulated input gate plus cell value previous time step ct−. directly adding consider part equation easily conﬁrm gradient indeed one. accurate sigmoid function actually mathematical function s-shaped curve tanh function also type sigmoid function. logistic function also slightly broader +exp) however machine learning literature sigmoid usually class functions used refer particular variety equation importance recurrent neural networks number applications many variants networks exist. modiﬁcation standard lstm used widely addition forget gate equations lstm forget gate shown below compared standard lstm changes. first equation additional gate forget gate. second equation gate modulate passing previous cell current cell forget gate useful allows cell easily clear memory justiﬁed example let’s model remembered seen particular word strongly correlated another word himself herself example above. case would probably like model remember used predict himself forget information longer relevant. forget gates advantage allowing sort ﬁne-grained information control also come risk zero time model forget everything lose ability handle long-distance dependencies. thus beginning neural network training common initialize bias forget gate somewhat large value make neural start training without using forget gate gradually start forgetting content trained extent. lstm provides eﬀective solution vanishing gradient problem also rather complicated simpler variant nonetheless proven eﬀective gated recurrent unit expressed following equations modulated update gate update gate close candidate hidden value update close zero previous value. candidate hidden state calculated equation similar standard update includes additional modulation hidden state input reset gate calculated equation compared lstm slightly fewer parameters also separate concept cell. thus grus used conserve memory computation time. important modiﬁcation rnns lstms grus really neural network layer simple powerful stack multiple layers example -layer stacked calculation time step would look follows hidden state layer time step abbreviation equation equation similarly could substitute function lstm recurrence step. reason stacking multiple layers useful reason non-linearities proved useful standard neural networks introduced section able progressively extract abstract features current words sentences. example evidence two-layer stacked lstm ﬁrst layer tends learn granular features words part speech tags second layer learns abstract features sentence voice tense. stacking rnns potential beneﬁts also disadvantage suﬀers vanishing gradient problem vertical direction standard horizontal direction. gradient back-propagated layer close output layer close input gradient vanish process causing earlier layers network under-trained. simple solution problem analogous lstm vanishing gradients time residual networks idea behind networks simply output previous layer directly result next layer follows observant reader noticed previous sections gradually introduced complicated models; started simple linear model added hidden layer added recurrence added lstm added layers lstms. expressive models ability model higher accuracy also come cost largely expanded parameter space complicated operations section describes eﬀective technique improve stability computational eﬃciency training complicated networks minibatching. point used stochastic gradient descent learning algorithm introduced section performs updates according following iterative process. type learning performs updates single example time called online learning. minibatching happy medium strategies. basically minibatched training similar online training instead processing single training example time calculate gradient training examples time. extreme case equivalent standard online training extreme equals size corpus equivalent fully batched training. case training language models common choose minibatches sentences process single time. increase number training examples parameter update becomes informative stable amount time perform update increases common choose allows good balance two. major advantage minibatching using tricks actually possible make simultaneous processing training examples signiﬁcantly faster processing diﬀerent examples separately. speciﬁcally taking multiple training examples grouping similar operations together processed simultaneously realize large gains computational eﬃciency fact modern hardware eﬃcient vector processing instructions exploited appropriately structured inputs. shown figure common examples neural networks include grouping together matrix-vector multiplies multiple examples single matrix-matrix multiply performing element-wise operation multiple vectors time opposed processing single vectors individually. luckily dynet library using relatively easy much machinery elementary operation handled automatically. we’ll give example changes need make implementing language model below. basic idea batched language model instead processing single sentence process multiple sentences time. instead looking single word embedding look multiple word embeddings batched word embeddings softmax normal resulting separate probability distributions words ﬁrst second sentences. calculate loss word together losses loss entire sentence. sticking point however need create batches sentences diﬀerent sizes also shown ﬁgure. case common perform sentence padding masking make sure sentences diﬀerent lengths treated properly. padding works simply adding end-of-sentence symbol shorter sentences length longest sentence batch. masking works multiplying loss functions calculated padded symbols zero ensuring losses sentence symbols don’t counted twice shorter sentences. lengths we’ll wasting computation padded symbols. problem also common sort sentences corpus length creating mini-batches ensure sentences mini-batch approximately size. recurrent neural networks learn? rnns surprisingly powerful tools language thus many people interested exactly going inside them. demonstrate ways visualize internal states lstm networks nodes charge keeping track length sentences whether parenthesis opened salietn features sentences. show ways analyze visualize parts input contributing particular decisions made rnn-based model back-propagating information network. architectures also quite recurrent network architectures. perform interesting study ablate various parts lstm attempt best architecture particular tasks. take step further explicitly training model best neural network architecture. vious time step updates according appropriate equations. reference dynet componentwise multiply sigmoid functions dy.cmult dy.logistic respectively. potential improvements model include measuring speed/stability improvements achieved mini-batching. comparing diﬀerences recurrent architectures lstm. ﬁrst model cover called encoder-decoder model basic idea model relatively simple language model starting calculation probabilities ﬁrst calculate initial state language model using another source sentence name encoder-decoder comes idea ﬁrst neural network running encodes information vector real-valued numbers second neural network used predict decodes information target sentence. decoder phase predict probability word time step. first similarly look time previous word must condition probability previous word itself. then decoder calculate ﬁnal state encoder h|f| allowing condition finally calculate probability basic pattern able perform translation similar accuracy heavily engineered systems specialized machine translation task diﬀerent encoder ensembling first random sampling useful cases want variety outputs particular input. example situation useful would sequenceto-sequence model dialog system would prefer system always give response particular user input prevent monotony. luckily models like using method called ancestral sampling. ancestral sampling works sampling variable values time gradually conditioning context time step encoder-decoder model means simply calculate according previously sampled inputs leading simple generation algorithm algorithm thing note sometimes also want know probability sentence sampled. example given sentence generated model might want know certain model prediction. sampling process calculate probabilities sampled word. however remember discussion probability probability section using probabilities as-is result small numbers cause numerical precision problems computers. thus calculating full-sentence probability common instead together probabilities word avoids problem. next let’s consider problem generating -best result. variety generation useful machine translation applications simply want output translation model thought best. simplest greedy search simply calculate every time step select word gives highest probability next word sequence. words algorithm exactly algorithm exception line instead sampling randomly according solve problem beam search. beam search similar greedy search instead considering best hypothesis consider best hypotheses time step width beam. example beam search shown figure ﬁrst time step expand hypotheses corresponding three words vocabulary keep delete remaining /s). second time step expand hypotheses corresponding continuation ﬁrst hypotheses words vocabulary temporarily creating b∗|v active hypotheses. thing careful generating sentences using models neural tend prefer shorter sentences. every time another word multiply another probability reducing probability whole sentence. increase beam size search algorithm gets better ﬁnding short sentences result beam search larger beam size often signiﬁcant length bias towards shorter sentences. several attempts length bias problem. example possible prior probability length sentence given length heuristic still widely used approach normalizes probability length target sentence eﬀectively searching sentence highest average probability word section described model works encoding sequences linearly word time left right. however natural eﬀective turn sentence vector section we’ll discuss number diﬀerent ways perform encoding reported eﬀective literature. motivation behind method pairs languages similar ordering words beginning generally correspond words beginning assuming extreme case words identical indices correspond each-other distance corresponding words linear encoding decoding prediction diﬃcult feat. beginning training even variants lstms trouble essentially guess part information encoded hidden state used without prior bias. reversing encoder helps solve problem reducing length dependencies subset words sentence speciﬁcally ones beginning sentences. shown figure length dependency subsequent short-distance dependencies bootstrap model training becomes possible gradually learn longer dependencies words sentence. proved critical learn eﬀective models encoder-decoder framework. however approach reversing encoder relies strong assumption order words input output sequences similar least words beginning sentences same. true languages like english french share subject-verb-object word ordering true typologically distinct languages. type encoder slightly robust method diﬀerent diﬀerences bi-directional encoder combined initial vector decoder rnn. combination done simply concatenating ﬁnal vectors however also requires size vectors decoder exactly equal combined size encoder rnns. ﬂexible alternative additional parameterized hidden layer encoder decoder states allows convert bidirectional encoder states appropriately-sized state decoder addition also methods decoding move beyond simple linear view input sentence. example convolutional neural networks figure variety neural combines together information spatially temporally local segments. widely applied image processing also used speech processing well processing textual sequences. many varieties cnn-based models text show example model ﬁlters width passed incrementally general cnns found quite eﬀective text classiﬁcation important pick indicative features text less emphasis getting overall view content also positive results reported using speciﬁc varieties cnns sequence-to-sequence modeling finally popular form encoder widely used number tasks treestructured networks figure basic idea behind networks combine information particular word guided sort structure usually syntactic structure sentence example shown figure reason intuitively useful syntactic phrase usually also corresponds coherent semantic unit. thus performing calculation manipulation vectors coherent units appropriate compared using random substrings words like used cnns. example let’s phrase chased little bird shown ﬁgure. case following syntactic tree would ensure calculate vectors coherent units correspond grammatical phrase chased little bird combine phrases together obtain meaning larger coherent phrase chased little bird. take advantage fact language compositional meaning complex phrase resulting regular combinations transformation smaller constituent phrases taking linguistically motivated intuitive view sentence hope help neural networks learn generalizable functions limited training data. perhaps simple type tree-structured network recursive neural network proposed network strong parallels standard rnns instead calculating hidden state time previous hidden state follows thus representation node tree calculated bottom-up fashion. like standard rnns recursive networks suﬀer vanishing gradient problem. problem adaptation lstms tree-structured networks ﬁttingly called tree lstms ﬁxes vanishing gradient problem. also wide variety kinds tree-structured composition functions interested readers explore also interest study examines various tasks tree structures necessary unnecessary nlp. method widely used encoder-decoders models translation ensembling combination prediction multiple independently trained models improve overall prediction results. intuition behind ensembling diﬀerent models make diﬀerent mistakes average common models agree answer correct mistaken. thus combine multiple models together becomes possible smooth mistakes ﬁnding correct answer often. training write code calculate loss function perform training. development generate translations using greedy search. evaluate generated translations comparing reference translations look good not. translations also evaluated automatic means bleu score reference implementation bleu evaluation script found here https//github.com/moses-smt/mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl. potential improvements model include implementing beam search comparing results greedy search. implementing alternative encoder. implementing ensembling. past chapter described simple model neural machine translation uses encoder encode sentences ﬁxed-length vector. however ways view overly simpliﬁed introduction powerful mechanism called attention overcome diﬃculties. section describes problems encoder-decoder architecture attention problems. theoretically suﬃciently large well-trained encoder-decoder model able perform machine translation perfectly. mentioned section neural networks universal function approximators meaning express function wish model including function accurately predicts predictive probability next however practice necessary learn functions limited data important proper inductive bias appropriate model structure allows network learn model accurately reasonable amount data. things worrying standard encoder-decoder architecture. ﬁrst described previous section long-distance dependencies words need translated other. previous section alleviated extent reversing direction encoder bootstrap training still large number long-distance dependencies remain hard guarantee learn handle properly. second perhaps more worrying aspect encoder-decoder attempts store information sentences arbitrary length hidden vector ﬁxed size. words even machine translation system expected translate sentences lengths word words still intermediate representation store information input sentence. network small able encode information longer sentences expected translate. hand even make network large enough handle largest sentences inputs processing shorter sentences overkill using needlessly large amounts memory computation time. addition networks large numbers parameters diﬃcult learn face limited data without encountering problems overﬁtting. basic idea attention instead attempting learn single vector representation sentence instead keep around vectors every word input sentence reference vectors decoding step. number vectors available reference equivalent number words input sentence long sentences many vectors short sentences vectors. result express input sentences much eﬃcient avoiding problems ineﬃcient representations encoder-decoders mentioned previous section. give matrix every column corresponds word input sentence. however faced diﬃculty. matrix variable number columns depending length source sentence would like compute example probabilities output vocabulary know case vector input. insight attention calculate vector used combine together columns vector figure example attention english source french target higher attention weight generating particular target word indicated lighter color matrix. basic idea behind attention vector telling much focusing particular source word particular time step. larger value impact word predicting next word output sentence. example attention plays actual translation example shown figure values alignment vectors generally align intuition. next question becomes answer lies decoder track state generating output. before decoder’s hidden state ﬁxed-length continuous vector representing previous target words h|f|+. used calculate context vector used summarize source attentional context used choosing target word initialized worth noting means encoding source word considered much directly calculation output probabilities. contrast encoderdecoder encoder-decoder able access information ﬁrst model advantage adds additional parameters model. however also intuitive disadvantage forces input output encodings space must close space order product high). also noted product calculated eﬃciently every word source sentence instead deﬁning attention score concatenated matrix follows combining many attention operations useful eﬃcient impementation especially gpus. following attention functions also calculated like similarly. bilinear functions slight modiﬁcation product expressive bilinear function. function helps relax restriction source target embeddings must space performing linear transform parameterized taking product advantage square matrix possible vectors diﬀerent sizes possible encoder decoder diﬀerent dimensions. however introduce quite parameters model multi-layer perceptrons finally also possible calculate attention score using multi-layer perceptron method employed original implementation attention weight matrix vector ﬁrst second layers respectively. ﬂexible product method usually fewer parameters bilinear method generally provides good results. addition methods above essentially defacto-standard sophisticated methods calculating attention well. example possible recurrent neural networks tree-structured networks based document structure convolutional neural networks structured models calculate attention. pleasant side-eﬀect attention increases translation accuracy also makes easier tell words translated words output. obvious consequence draw intuitive graphs shown figure error analysis. also common alignment models described obtain translation dictionary unknown word replacement even further. specifically instead copying word as-is output chosen source word importance attention modern systems also number proposals improve accuracy estimating attention introduction intuitively motivated prior probabilities. propose several methods incorporate biases training model ensure attention weights match belief alignments languages look like. position bias languages similar word order likely alignments fall along diagonal. demonstrated strongly figure possible encourage behavior adding prior probability attention makes easier things near diagonal aligned. markov condition languages assume time words target contiguous aligned words source also contiguous. example figure true contiguous pairs english words except european area was. take advantage property possible impose prior discourages large jumps encourages local steps attention. model similar motivation diﬀerent implementation local attention model selects part source sentence focus using neural network itself. fertility assume words translated certain number words langauge. example english word cats translated words chats french. priors fertility takes advantage fact giving model penalty particular words attended much attended much. fact major problems poorly trained neural systems repeat word over drop words violation fertility constraint. this several methods proposed incorporate coverage model constraint decoding process bilingual symmetry finally expect words aligned performing translation also aligned performing translation enforced training models parallel enforcing constraints alignment matrices look similar directions. hard attention shown equation standard attention uses soft combination various contents. also methods hard attention make hard binary decision whether focus particular context motivations ranging learning explainable models processing text incrementally supervised training attention addition sometimes hand-annotated data showing true alignments particular language pair. possible train attentional models using data deﬁning loss function penalizes model predict alignments correctly ways memorizing input finally ways accessing relevant information attention. propose method using memory networks separate memory written read processing continues. extend encoder-decoder code attention. training write code calculate loss function perform training. development generate translations using greedy search. evaluate translations either manually automatically. tutorial covered basics neural machine translation sequence-to-sequence models. gradually stepped models increasing sophistication starting ngram language models culminating attention represents state-of-the-art many sequence-to-sequence modeling tasks. handling large vocabularies diﬃculty neural models perform badly using large vocabularies; hard learn properly translate rare words limited data computation becomes burden. method handle break words smaller units characters subwords also possible incorporate translation dictionaries broad coverage handle low-frequency phenomena reality actually care accuracy generated sentences. number works proposed resolve disconnect directly considering accuracy generated results training models. include methods sample translation results current model move towards parameters result good translations methods optimize parameters towards partially mistaken hypotheses improve robustness mistakes generation methods prevent mistakes occur search process multi-lingual learning assumed training model languages however reality many languages world work shown beneﬁt using data languages learn models together also possible perform transfer across languages training model ﬁrst language pair ﬁne-tuning others applications similar sequence-to-sequence models used wide variety tasks dialog systems text summarization speech recognition speech synthesis image captioning image generation more. extremely grateful qinlan shen dongyeop kang careful reading materials useful comments unclear parts. also thank students machine translation sequence-to-sequence models class pointing various bugs materials preliminary version used class.", "year": 2017}