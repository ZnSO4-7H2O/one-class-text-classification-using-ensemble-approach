{"title": "Gated-Attention Architectures for Task-Oriented Language Grounding", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "abstract": "To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.", "text": "tackle problem propose architecture comprises state processing module creates joint representation instruction images observed agent policy learner predict optimal action agent take timestep. state processing module consists novel gated-attention multimodal fusion mechanism based multiplicative interactions modalities contributions paper summarized follows propose end-to-end trainable architecture handles pixel-based input task-oriented language grounding environment assumes prior linguistic perceptual knowledge. show proposed model generalizes well unseen instructions well unseen maps. develop novel gated-attention mechanism multimodal fusion representations verbal visual modalities. show gated-attention mechanism outperforms baseline method concatenating representations using various policy learning methods. visualization attention weights gatedattention unit shows model learns associate attributes object mentioned instruction visual representations learned model. introperform tasks speciﬁed natural language instructions autonomous agents need extract semantically meaningful representations language visual elements actions environment. problem called task-oriented language grounding. propose end-toend trainable neural architecture task-oriented language grounding environments assumes prior linguistic perceptual knowledge requires pixels environment natural language instruction input. proposed model combines image text representations using gated-attention mechanism learns policy execute natural language instruction using standard reinforcement imitation learning methods. show effectiveness proposed model unseen instructions well unseen maps quantitatively qualitatively. also introduce novel environment based game engine simulate challenges task-oriented language grounding rich instructions environment states. artiﬁcial intelligence systems expected perceive environment take actions perform certain task task-oriented language grounding refers process extracting semantically meaningful representations language mapping visual elements actions environment order perform task speciﬁed instruction. consider scenario shown figure agent takes natural language instruction pixel-level visual information input carry task real world. accomplish goal agent draw semantic correspondences visual verbal modalities learn policy perform task. problem poses several challenges agent learn recognize objects pixel input explore environment objects might occluded outside ﬁeld-of-view agent ground concept instruction visual elements actions environment reason pragmatics language based objects current environment task-oriented language grounding rich actions objects attributes. environment provides ﬁrst-person view world state allows simulating complex scenarios tasks navigation. grounding language robotics. context grounding language objects attributes guadarrama present method ground open vocabulary objects environment. several works look grounding concepts human-robot interaction works grounding include attempts ground natural language instructions haptic signals teaching robot ground natural language using active learning work aims ground navigational instructions include focus ground verbs like follow etc. spatial relations verbs mapping instructions action sequences. chen mooney artzi zettlemoyer present methods based semantic parsing navigational instructions sequence actions. bansal walter look neural mapping instructions sequence actions along input bag-of-word features extracted visual image. works focus grounding navigational instructions actions environment ground visual attributes objects shape size color. deep reinforcement learning using visual data. prior work explored using deep reinforcement learning approaches playing games challenge learn optimal policy variety tasks including navigation using visual pixel information. chaplot look transfer learning different tasks doom environment. methods policy task learned separately using deep q-learning contrast train single network multiple tasks/instructions. look targetdriven visual navigation given image target object. natural language instruction visual image object. zhang look learning navigate maze-like environment execute commands seen zero-shot setting combination words seen before. misra langford artzi also look mapping visual observations text input actions blocks environment. works also looks executing variety instructions tackle environments. look zero-shot task generalization environment. method tackles long instructions several subtasks wide variety action verbs. however position agent discretized like mazes method encodes prior linguistic knowledge analogy making objective. compared prior work paper aims address visual language grounding challenging setting involving raw-pixel input continuous agent positions partially observable envrionment poses additional challenges perception exploration reasoning. unlike many previous methods model assumes prior linguistic perceptual knowledge trainable end-to-end. tackle problem task-oriented language grounding context target-driven visual navigation conditioned natural language instruction agent navigate object described instruction. consider agent interacting episodic environment beginning episode agent receives natural language instruction indicates description target visual object environment. time step agent receives pixel-level image ﬁrst person view environment performs action episode terminates whenever agent reaches object number time steps exceeds maximum episode length. denote state time step. objective agent learn optimal policy maps observed states actions eventually leading successful completion task. case task reach correct object episode terminates. consider different learning approaches imitation learning agent access oracle speciﬁes optimal action given state environment; reinforcement learning agent receives positive reward reaches target object negative reward reaches object. propose novel architecture task-oriented visual language grounding assumes prior linguistic perceptual knowledge trained end-to-end. proposed model divided modules state processing policy learning shown figure state processing module state processing module takes current state input creates joint representation image instruction. joint representation used policy learner predict optimal action take timestep. consists convolutional network process image gated recurrent unit network process instruction multimodal fusion unit combines representations instruction image. rd×h×w representation image θconv denote parameters convolutional network denotes number feature maps convolutional network output denote height width feature map. representation instruction θgru denotes parameters network. multimodal fusion unit combines image instruction representations. many prior methods combine multimodal representations concatenation develop multimodal fusion unit gated-attention based multiplicative interactions instruction image representation. gated-attention gated-attention unit instruction embedding passed fully-connected linear layer sigmoid activation. output dimension linear layer equal number feature maps output convolutional network output linear layer called attention vector denotes fullyconnected layer sigmoid activation. element expanded matrix. results dimensional matrix rd×h×w whose element given matrix multiplied element-wise output convolutional network proposed gated-attention unit inspired gated-attention reader architecture text comprehension integrate multi-hop architecture gated-attention mechanism based multiplicative interactions query embedding intermediate states recurrent neural network document reader. contrast propose gatedattention multimodal fusion unit based multiplicative interactions instruction representation convolutional feature maps image representation. architecture extended application multimodal fusion verbal visual modalities. intuition behind gated-attention unit trained convolutional feature maps detect different attributes objects frame color shape. agent needs attend speciﬁc attributes objects based instruction. example depending whether instruction green object pillar green pillar agent needs attend objects ‘green’ objects look like ‘pillar’ both. gated-attention unit designed gate speciﬁc feature maps based attention vector instruction policy learning module output multimodal fusion unit policy learning module. architecture policy learning module speciﬁc learning paradigm imitation learning reinforcement learning. rectly. customizable nature environment enables create scenarios varying levels difﬁculty believe leads designing sophisticated learning algorithms address challenge multi-task zero-shot reinforcement learning. instruction combination object) triple. instruction attribute limit number actions objects each. environment allows variety objects spawned different locations map. objects various visual attributes color shape size. provide manually generated instructions. instructions environment allows automatic creation multiple episodes randomly created correct object incorrect objects. although number instructions limited combinations correct incorrect objects instruction allows create multiple settings instruction. time instruction selected environment generates random combination incorrect objects correct object randomized locations. signiﬁcant challenges posed learning algorithm understand instruction refer different objects different episodes. example object refer keycard episode torch another episode. similarly keycard refer keycards various colors different episodes. objects could also occlude other might even present agent’s ﬁeld view could complicated making difﬁcult agent make decision based solely current input stressing need efﬁcient exploration. environment also provides different modes respect spawning objects varying difﬁculty levels easy agent spawned ﬁxed location. candidate objects spawned ﬁxed locations along single horizontal line along ﬁeld view agent. medium candidate objects spawned random locations environment ensures ﬁeld view agent. agent still spawned ﬁxed location. hard candidate objects agent spawned random locations objects agents ﬁeld view initial conﬁguration. agent needs explore view objects. perform experiments three environment difﬁculty modes restrict number objects episode training objects spawned training instructions instructions pertaining unseen attributeobject combinations held test zero-shot evaluation. training start episode train instructions selected randomly. correct target object selected incorrect objects selected random. objects placed random locations dependoracle implemented extracting agent target object locations orientations doom game engine. given state oracle determines optimal action follows agent ﬁrst reorients towards target object. moves forward reorienting towards target object deviation agent’s orientation greater minimum turn angle supported environment. reinforcement learning asynchronous advantage actor-critic algorithm uses deep neural network learn policy value functions runs multiple parallel threads update network parameters. also entropy regularization improved exploration described addition generalized advantage estimator reduce variance policy gradient updates. policy learning module imitation learning contains fully connected layer estimate policy function. policy learning module reinforcement learning using consists lstm layer followed fully connected layers estimate policy function well value function. lstm layer introduced agent memory previous states. important reinforcement learning agent might explore states objects visible need remember objects seen previously. create environment task-oriented language grounding agent execute natural language instruction obtain positive reward successful completion task. environment built vizdoom based doom classic ﬁrst person shooting game. provides visual information ﬁrst-person perspective every timestep. scenario environment comprises agent list objects correct rest incorrect customized map. agent interact environment performing navigational actions turn left turn right move forward. given instruction green torch task considered successful agent able reach green torch corfigure comparison performance proposed gated-attention unit baseline concatenation unit using reinforcement learning algorithm easy medium hard environments. difﬁculty level environment. episode terminates agent reaches object time step exceeds maximum episode length evaluation metric accuracy agent success rate reaching correct object episode terminates. consider scenarios evaluation multitask generalization agent evaluated unseen maps instructions train set. unseen maps comprise unseen combination objects placed randomized locations. scenario tests agent doesn’t overﬁt memorize training maps execute multiple instructions tasks unseen maps. zero-shot task generalization agent evaluated unseen test instructions. scenario tests whether agent generalize combinations attribute-object pairs seen training. maps scenario also unseen. baseline approaches reinforcement learning adapt reinforcement learning baseline proposed environment. misra langford artzi looks jointly reasoning linguistic visual inputs moving blocks grid environment execute instruction. work uses features grid processed instruction processed lstm. text visual representations combined using concatenation. agent trained using reinforcement learning enhanced using distance based reward shaping. reward shaping would like method generalize environments distance target available. imitation learning adapt imitation learning baseline proposed environment. bansal walter sequence instructions actions treated sequence-to-sequence learning problem visual state input received decoder decode timestep. bag-ofvisual words representation visual state adapt baseline directly process pixels environment using cnns. hyper-parameters input neural network instruction image size ﬁrst layer convolves image ﬁlters kernel size stride followed ﬁlters kernel size stride another ﬁlters kernel size stride architecture convolutional layers adapted previous work playing deathmatches doom input instruction encoded gated recurrent unit size imitation learning approach experiments behavioral cloning dagger algorithms online fashion data generation policy update function outer iteration. policy learner imitation learning comprises linear layer size fully-connected neurons predict policy function data generation step sample state trajectories based oracle’s policy based mixture oracle’s policy currently learned policy dagger. mixing policies governed exploration coefﬁcient linear decay state collect optimal action given policy oracle. policy updated epochs state-action pairs collected using rmsprop optimizer methods huber loss estimated policy optimal policy given policy oracle. reinforcement learning experiments algorithm. policy learning module linear layer size followed lstm layer size encodes history state observations. lstm layer’s output fully-connected single neuron predict value function well three neurons predict policy function. network parameters shared predicting value function policy function except ﬁnal fully connected layer. convolutional layers fully-connected linear layers relu activations model trained using stochastic gradient descent learning rate used discount factor calculattable accuracy models concatenation gated-attention units. concat concat adapted versions misra langford artzi bansal walter respectively proposed environment. accuracy values averaged episodes. expected rewards parallel threads experiment. mean-squared loss estimated value function discounted rewards training respect value function policy gradient loss using training respect policy function. models described section performance multitask zero-shot generalization shown table performance models multitask generalization training plotted figure performance models observe models gated-attention unit outperform models concatenation unit multitask zero-shot generalization. figure observe models units learn faster concat models converge higher levels accuracy. hard mode achieves accuracy multitask generalization zero-shot generalization whereas concat achieves respectively fails show considerable performance. imitation learning observe models perform better concat environment modes harder imitation learning perform well need exploration medium hard settings. contrast inherent extensive exploration reinforcement learning algorithm makes model robust agent’s location covers state trajectories. policy execution figure shows policy execution model hard mode instruction short green torch. ﬁgure demonstrate agent’s ability explore environment handle occlusion. example none objects ﬁeld-of-view agent initial frame. agent explores environment eventually navigates towards target object. also learned distinguish short green torch tall green torch avoid tall torch reaching short torch. analysis attention maps figure shows heatmap values attention vector different instructions grouped object type target object seen ﬁgure dimension corresponds ‘armor’ dimensions corresponds ‘skullkey’ dimension corresponds ‘pillar’. also note dimension high instructions ﬁrst group. indicates model also recognizes word ‘object’ correspond particular object type rather refers object color observations indicate model learning recognize attributes objects color type speciﬁc feature maps gated based attributes. furthermore attention vector weights test instructions also indicate gated-attention unit also able recognize attributes object unseen instructions. also visualize tsne plots attention vectors based attributes color object type shown figure attention vectors objects blue green yellow present clusters whereas instructions mention object’s color spread across belong clusters corresponding object type. similarly objects particular type present clusters. clusters indicate model able recognize object attributes learns similar attention vectors objects similar. paper proposed end-to-end architecture task-oriented language grounding pixels environment reinforcement learning imitation learning. architecture uses novel multimodal fusion mechanism gated-attention learns joint state representation based multiplicative interactions between instruction image representation. observe models gated-attention unit outperform models concatenation units multitask zero-shot task generalization across three modes difﬁculty. visualization attention weights gated-attention unit indicates agent learns recognize objects color attributes size attributes. would like thank prof. louis-philippe morency tadas baltruaitis valuable comments guidance throughout development work. work partially supported grants adelaide fa-c-- contain hr--c-. figure heatmap values -dimensional attention vector different instructions grouped object type sub-grouped object color. test instructions marked boxes indicate certain dimensions attention vector activated particular attributes target object referred instruction. figure ﬁgure shows example policy execution different points instruction short green torch’. left navigation agent right frames point. initial frame none objects visible. agent turned objects ﬁeld view. agent successfully avoids tall green torch. agent moves towards short green torch. agent reaches target.", "year": 2017}