{"title": "Foolbox: A Python toolbox to benchmark the robustness of machine  learning models", "tag": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "abstract": "Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox . The most up-to-date documentation can be found at http://foolbox.readthedocs.io .", "text": "even todays advanced machine learning models easily fooled almost imperceptible perturbations inputs. foolbox python package generate adversarial perturbations quantify compare robustness machine learning models. build around idea comparable robustness measure minimum perturbation needed craft adversarial example. foolbox provides reference implementations published adversarial attack methods alongside ones perform internal hyperparameter tuning minimum adversarial perturbation. additionally foolbox interfaces popular deep learning frameworks pytorch keras tensorflow theano mxnet allows different adversarial criteria targeted misclassiﬁcation top-k misclassiﬁcation well different distance measures. code licensed license openly available https//github.com/bethgelab/foolbox. up-to-date documentation found http//foolbox.readthedocs.io. szegedy demonstrated minimal perturbations often almost imperceptible humans devastating effects machine predictions. so-called adversarial perturbations thus demonstrate striking difference human machine perception. result adversarial perturbations subject many *equal contribution centre integrative neuroscience university tübingen germany bernstein center computational neuroscience tübingen germany international planck research school intelligent systems tübingen germany planck institute biological cybernetics tübingen germany institute theoretical physics university tübingen germany. correspondence jonas rauber <jonas.rauberbethgelab.org>. unfortunately ﬁnding global minimum adversarial perturbation close impossible practical setting thus employ heuristic attacks suitable approximation. heuristics however fail case could easily mislead believe model robust best strategy thus employ many attacks possible minimal perturbation found across attacks approximation true global minimum. moment however strategy severely obstructed problems ﬁrst code known attack methods either available available particular deep learning framework. second implementations attack often differ many details thus directly comparable. foolbox improves upon existing python package cleverhans papernot three important aspects gradient another. makes possible attack non-differentiable models using gradient-based attacks allows transfer attacks type described papernot crafting adversarial examples requires elements ﬁrst model takes input makes prediction second criterion deﬁnes adversarial third distance measure measures size perturbation finally attack algorithm takes input label well model adversarial criterion distance measure generate adversarial perturbation. interface initialized framework speciﬁc representation model interface provides adversarial attack standardized methods compute predictions gradients given inputs. straight-forward implement interfaces frameworks providing methods calculate predictions gradients speciﬁc framework. distance measures foolbox.distances distance measures used quantify size adversarial perturbations. foolbox implements commonly employed distance measures extended custom ones attacks foolbox.attacks foolbox implements large number adversarial attacks section overview. attack takes model adversarials found criterion deﬁnes adversarial default criterion misclassiﬁcation. applied reference input adversarial close corresponding label. attacks perform internal hyperparameter tuning minimum perturbation. example implementation fast gradient sign method searches minimum step-size turns input adversarial. result need specify hyperparameters attacks like fgsm. computational efﬁciency complex attacks several hyperparameters tune them. adversarial foolbox.adversarial instance adversarial class encapsulates information adversarial including model criterion distance measure used original unperturbed input label size smallest adversarial perturbation found attack. adversarial object automatically created whenever attack applied -pair. default actual adversarial input returned. calling attack unpack false returns full object instead. adversarial object passed adversarial attack instead -pair enabling advanced cases pausing resuming longrunning attacks. release foolbox tagged version number type major.minor.patch follows principles semantic versioning additional precautions comparable benchmarking. increment thus compare robustness models important major.minor version foolbox. accordingly version number foolbox always reported alongside benchmark results section iterative gradient attack foolbox.attacks.iterativegradientattack iterative gradient ascent seeks adversarial perturbations maximizing loss along small steps gradient direction i.e. algorithm iteratively updates step-size tuned internally minimum perturbation. iterative gradient sign attack foolbox.attacks.iterativegradientsignattack similar iterative gradient ascent attack seeks adversarial perturbations maximizing loss along ascent direction sign) small sign i.e. algorithm iteratively updates sign). step-size tuned internally minimum perturbation. deepfool attack foolbox.attacks.deepfoollattack iteration deepfool computes class minimum distance takes reach class boundary approximating model classiﬁer linear classiﬁer. makes corresponding step direction class smallest distance. slsqp attack foolbox.attacks.slsqpattack compared l-bfgs-b slsqp allows additionally specify non-linear constraints. enables skip line-search directly optimise jacobian-based saliency attack foolbox.attacks.saliencymapattack targeted attack uses gradient compute saliency score input feature saliency score reﬂects strongly feature push model classiﬁcation reference target class. process iterated iteration feature maximum saliency score perturbed. single pixel attack foolbox.attacks.singlepixelattack attack probes robustness model changes single pixels setting single pixel white black. repeats process every pixel image. local search attack foolbox.attacks.localsearchattack attack measures model’s sensitivity individual pixels applying extreme perturbations observing effect probability correct class. perturbs pixels model sensitive. repeats process image adversarial searching additional critical pixels neighborhood previously found ones. approximate l-bfgs attack foolbox.attacks.approximatelbfgsattack l-bfgs except gradients computed numerically. note attack suitable input dimensionality small. boundary attack foolbox.attacks.boundaryattack foolbox provides reference implementation boundary attack boundary attack effective decision-based adversarial attack minimize l-norm adversarial perturbations. ﬁnds adversarial perturbations small best gradientbased attacks without relying gradients probabilities. pointwise attack foolbox.attacks.pointwiseattack foolbox provides reference implementation pointwise attack. pointwise attack effective decision-based adversarial attack minimize l-norm adversarial perturbations. additive uniform noise attack foolbox.attacks.additiveuniformnoiseattack attack probes robustness model i.i.d. uniform noise. line-search performed internally minimal adversarial perturbations. additive gaussian noise attack foolbox.attacks.additivegaussiannoiseattack attack probes robustness model i.i.d. normal noise. line-search performed internally minimal adversarial perturbations. salt pepper noise attack foolbox.attacks.saltandpeppernoiseattack attack probes robustness model i.i.d. saltand-pepper noise. line-search performed internally minimal adversarial perturbations. contrast reduction attack foolbox.attacks.contrastreductionattack attack probes robustness model contrast reduction. line-search performed internally minimal adversarial perturbations. gaussian blur attack foolbox.attacks.gaussianblurattack attack probes robustness model gaussian blur. line-search performed internally minimal blur needed turn image adversarial. images corresponding adversarial candidates. applied image tests models robustness precomputed adversarial candidate corresponding given image. useful test models robustness image perturbations created using external method. work supported carl zeiss foundation bosch forschungsstiftung international planck research school intelligent systems german research foundation intelligence advanced research projects activity department interior/interior business center contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/ibc u.s. government.", "year": 2017}