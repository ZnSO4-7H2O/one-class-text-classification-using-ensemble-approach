{"title": "Fast Stochastic Variance Reduced Gradient Method with Momentum  Acceleration for Machine Learning", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.", "text": "composite problem naturally arises many applications machine learning data mining regularized empirical risk minimization eigenvector computation summarized mainly four interesting categories problem follows methods expensive hence stochastic gradient descent also known incremental gradient descent widely used many large-scale problems approximates gradient example mini-batch thus enjoys per-iteration computational complexity. moreover extremely simple highly scalable making particularly suitable large-scale machine learning e.g. deep learning however variance stochastic gradient estimator large practice leads slow convergence poor performance. even case standard achieve sub-linear convergence rate recently convergence speed standard dramatically improved various variance reduced methods sdca svrg saga proximal variants indeed many stochastic methods past full gradients progressively reduce variance stochastic gradient estimators leads revolution area ﬁrst-order methods. thus also called semistochastic gradient descent method hybrid gradient descent method particular recent methods converge linearly case overall complexity made exciting progress however best-known methods requires least auxiliary variables momentum parameters. paper propose fast stochastic variance reduction gradient method design novel update rule nesterov’s momentum incorporate technique growing epoch size. fsvrg auxiliary variable momentum weight thus much simpler much lower per-iteration complexity. prove fsvrg achieves linear convergence gence rate non-strongly convex problems number outer-iterations. also extend fsvrg directly solve problems non-smooth component functions svm. finally empirically study performance fsvrg solving various machine learning problems logistic regression ridge regression lasso svm. results show fsvrg outperforms state-of-the-art stochastic methods including katyusha. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. copyright x-xxxxx-xx-x/xx/xx ..... shows stochastic methods always converge faster accelerated deterministic methods moreover proved svrg minor modiﬁcations converge asymptotically stationary point non-convex case. however still overall complexity theoretical bound provided case converge much slower accelrecently accelerated stochastic methods proposed. among them successful techniques mainly include nesterov’s acceleration technique choice growing epoch length momentum acceleration trick presents accelerating contributions address aforementioned weaknesses existing methods propose fast stochastic variance reduced gradient method design novel update rule nesterov’s momentum update rule auxiliary variable momentum weight. thus fsvrg much simpler eﬃcient fsvrg direct accelerated method without using dummy regularizer also works non-smooth proximal settings. unlike variance reduced methods svrg convergence guarantee case fsvrg convergence guarantees cases particular fsvrg uses ﬂexible growing epoch size strategy speed convergence. impressively fsvrg converges much faster state-of-the-art stochastic methods. summarize main contributions follows. tiable sub-gradient lipschitz continuous. mostly focus case problem l-smooth. non-smooth component functions proximal operator oracle nesterov’s smoothing homotopy smoothing representative methods stochastic variance reduced optimization svrg proximal variant proxsvrg particularly attractive storage requirement compared need store gradients component functions proposed accelerated svrg method svrg++ doubling-epoch techniques. moreover katyusha direct accelerated stochastic variance reduction method main update rules formulated follows paper propose fast stochastic variance reduction gradient method momentum acceleration cases cases acceleration techniques classical nesterov’s momentum katyusha momentum incorporated explicitly well-known svrg method moreover fsvrg also uses growing epoch size strategy speed convergence. similar existing stochastic variance reduced methods svrg prox-svrg design simple fast stochastic variance reduction algorithm momentum acceleration solving smooth objective functions outlined algorithm clear algorithm divided epochs epoch consists average past stochastic iterates rather last iterate reported work better practice although convergence guarantee case depends initialization choices also work well practice especially case regularization parameter relatively small suggested momentum acceleration regularizer smooth e.g. -norm need replace update rule algorithm case non-smooth regularizers. inspired momentum acceleration trick accelerating ﬁrst-order optimization methods design following update rule accelerated svrg method svrg++ words svrg++ viewed special case fsvrg method. shown above fsvrg additional variable existing accelerated stochastic variance reduction methods e.g. katyusha require additional variables shown addition fsvrg momentum weight compared weights katyusha therefore fsvrg much simpler existing accelerated methods requires additional storage general. addition fsvrg much lower per-iteration complexity accelerated methods katyusha least variable analyzed above. stated section classes problems transformed smooth ones eﬃciently solved algorithm however smoothing techniques degrade performance involved algorithms similar case reduction problems problems thus extend algorithm non-smooth setting propose fast stochastic variance reduced sub-gradient algorithm solve problems directly well case algorithm directly solve problems case mini-batching able reduce variance upper bound lemma based variance upper bound corollary analyze convergence properties algorithms mini-batch setting. obviously number stochastic iterations epoch reduced batch variant fsvrg almost identical convergence properties theorem contrast need update procedure case objective functions. theorem also extended mini-batch setting follows. section evaluate performance fsvrg method solving various machine learning problems logistic regression ridge regression lasso svm. codes fsvrg related methods downloaded ﬁrst author’s website. fair comparison fsvrg related stochastic variance reduced methods including svrg prox-svrg svrg++ katyusha implemented experiments performed intel ram. suggested epoch size svrg prox-svrg katyusha. fsvrg svrg++ similar strategy growing epoch size e.g. fsvrg svrg++. methods parameter tune i.e. learning rate. note compare performance terms number eﬀective passes running time moreover compare stochastic algorithms saga catalyst shown comparable inferior katyusha shown mini-batching eﬀectively decrease variance stochastic gradient estimates. extend fsvrg convergence results mini-batch setting. here denote mini-batch selected random index size outer-iteration inner-iteration ms}. becomes figure comparison svrg svrg++ katyusha fsvrg method -norm regularized logistic regression problems. y-axis represents objective value minus minimum part conducted experiments norm elastic regularized logistic regression problems four popular data sets ijcnn covtype susy protein obtained libsvm data website website. example date sets normalized unit length leads upper bound figures show performance diﬀerent methods solving classes logistic regression problems respectively. seen svrg++ fsvrg consistently converge much faster methods terms running time number effective passes. accelerated stochastic variance reduction method katyusha much better performance standard svrg method terms number eﬀective passes sometimes performs worse terms running time. fsvrg achieves consistent speedups data sets outperforms methods settings. main reason fsvrg takes advantage momentum acceleration trick also much larger step sizes e.g. fsvrg svrg++ svrg. also conﬁrms report experimental results diﬀerent methods figure regularization parameter varied results observe fsvrg converges much faster methods also outperforms katyusha terms number eﬀective passes matches optimal convergence rate problem. svrg++ achieves comparable sometimes even better performance svrg katyusha. veriﬁes eﬃciency growing epoch size strategy svrg++ fsvrg. part implemented algorithms mentioned high-dimensional sparse data compared performance solving ridge regression problems sparse data sets sido shown figure data sets downloaded libsvm data website causality workbench website. results observe although katyusha outperforms svrg terms number eﬀective passes training examples test examples. regularization parameter figure shows performance stochastic sub-gradient descent method svrg fsvrg solving problem. note also extend svrg non-smooth settings scheme variance reduced methods svrg fsvrg yield signiﬁcantly better performance ssgd. fsvrg consistently outperforms ssgd svrg terms convergence speed testing accuracy. intuitively momentum acceleration trick lead faster convergence. leave theoretical analysis fsvrg case future research. variance reduction gradient optimization. svrg++ uses doubling-epoch technique reduce number gradient calculations early iterations lead faster convergence general. contrast fsvrg method uses general growing epoch size strategy i.e. factor implies much ﬂexible choosing unlike svrg++ fsvrg also enjoys momentum acceleration trick convergence guarantee problems case usually similar convergence speed. svrg++ relatively inferior performance methods. seen objective value fsvrg much lower methods suggesting faster convergence. figure compares performance fsvrg method diﬀerent mini-batch sizes data sets ijcnn protein. seen increasing minibatch size fsvrg comparable even better performance case feature-label pair. binary classiﬁcation data ijcnn randomly divided training test set. used standard onevs-rest scheme multi-class data mnist data k−)+∇f xs−)−f xs−)−f k−x+θsg) +gxs−) k−−xs−)+β θsx+xs−−xs xs−) xs−)+∇f xs−) θsx+xs−−xs k−x+θsg) +gxs−) k−−xs−) +xs−−xs xs−)−f xs−) θsggxs−); third inequality holds lemma +∇is θs∇is =∇fis +−∇fis e−∇fis ≤fθsx shown katyusha best known overcomplexities problems. analyzed above fsvrg much simpler eﬃcient katyusha also veriﬁed experiments. therefore fsvrg suitable large-scale machine learning data mining problems. paper proposed simple fast stochastic variance reduction gradient method integrates momentum acceleration trick also uses ﬂexible growing epoch size strategy. moreover provided convergence guarantees method show fsvrg attains linear convergence problems problems. empirical study showed performance fsvrg much better state-of-the-art stochastic methods. besides problems section apply fsvrg machine learning problems e.g. deep learning eigenvector computation fast incremental gradient method support non-strongly convex composite objectives. proc. adv. neural inf. process. syst. pages garber hazan kakade musco bounds optimizing composite objectives. proc. adv. neural inf. process. syst. pages xiao zhang. proximal stochastic gradient method progressive variance reduction. siam optim. ﬁrst present detailed descriptions seven data sets shown table particular report additional experimental results -norm elastic regularized logistic regression ridge regression lasso problems respectively. similar algorithm also extend original svrg setting non-smooth component functions present stochastic variance reduced sub-gradient descent algorithm outlined algorithm figure comparison svrg svrg++ katyusha fsvrg solving -norm regularized logistic regression problems diﬀerent regularization parameters. y-axis represents objective value minus minimum x-axis corresponds number eﬀective passes. figure comparison svrg svrg++ katyusha fsvrg solving -norm regularized logistic regression problems diﬀerent regularization parameters. y-axis represents objective value minus minimum x-axis corresponds running time figure comparison svrg svrg++ katyusha fsvrg solving ridge regression problems diﬀerent regularization parameters. y-axis represents objective value minus minimum x-axis corresponds number eﬀective passes. figure comparison svrg svrg++ katyusha fsvrg solving ridge regression problems diﬀerent regularization parameters. y-axis represents objective value minus minimum x-axis corresponds running time figure comparison prox-svrg svrg++ katyusha fsvrg solving lasso problems four data sets ijcnn protein covtype susy. note y-axis represents objective value minus minimum x-axis corresponds number eﬀective passes. figure comparison prox-svrg svrg++ katyusha fsvrg solving lasso problems four data sets ijcnn protein covtype susy. note y-axis represents objective value minus minimum x-axis corresponds running time", "year": 2017}