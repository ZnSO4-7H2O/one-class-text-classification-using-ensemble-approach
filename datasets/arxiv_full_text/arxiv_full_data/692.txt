{"title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating  Data Science", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.", "text": "ﬁeld data science continues grow ever-increasing demand tools make machine learning accessible non-experts. paper introduce concept tree-based pipeline optimization automating tedious parts machine learning— pipeline design. implement open source tree-based pipeline optimization tool python demonstrate eﬀectiveness series simulated real-world benchmark data sets. particular show tpot design machine learning pipelines provide signiﬁcant improvement basic machine learning analysis requiring little input prior knowledge user. also address tendency tpot design overly complex pipelines integrating pareto optimization produces compact pipelines without sacriﬁcing classiﬁcation accuracy. such work represents important step toward fully automating machine learning pipeline design. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. gecco denver acm. isbn xxxxxx. xx.xx xxxxxx number self-reported data scientists doubled time machine learning returned forefront academia business government data scientists discover applications algorithms automatically learn create actionable insights data. owing growth commensurate demand oﬀ-the-shelf tools make machine learning accessible scalable ﬂexible applied across wide variety domains non-experts. unfortunately eﬀective application many machine learning tools typically requires expert knowledge tool problem domain knowledge assumptions involved analysis and/or exhaustive brute force techniques. thus harnessing many machine learning tools often costly endeavor—both terms time computation. example typical data scientist approach machine learning problem demonstrated figure step presents dozens possible choices make data preprocessed model used data ideal model parameters learning experienced data scientists typically good sense promising starting points given problem domain inexperienced data scientists easily spend time exploring myriad pipeline conﬁgurations settling best one. theoretically manual design machine learning pipelines longer necessary. recent years witnessed development intelligent systems ﬁeld evolutionary computation consistently surprise capabilities. space antenna design software development debugging study ﬁnite algebras evolutionary algorithms outperformed humans variety domains previously considered exclusive humans. intelligent systems proven capable many domains must ourselves evolutionary algorithms automate design machine learning pipelines? figure depiction typical supervised machine learning process. ﬁtting model data practitioner must prepare data modeling performing initial exploratory analysis either correct remove oﬀending records next practitioner transform data make suitable modeling e.g. normalizing features removing features useful modeling and/or creating features existing data afterward practitioner must select machine learning model data choose model parameters allow model make accurate classiﬁcation data lastly practitioner must validate model ensure model’s predictions generalize data sets ﬁtted example testing model’s performance holdout data excluded earlier phases pipeline. light grey area indicates steps pipeline automated tree-based pipeline optimization tool paper report recent development evolutionary algorithm called tree-based pipeline optimization tool automatically designs optimizes machine learning pipelines tpot uses version genetic programming automatically design optimize series data transformations machine learning models maximize classiﬁcation accuracy given supervised learning data set. following sections demonstrate tpot’s capabilities across series benchmarks including simulated genetic analysis data sets nine benchmark data sets well-known irvine machine learning repository particular show tpot capable discovering pipelines achieve competitive performance combining pre-existing algorithms novel ways. also compare standard version tpot tpot-pareto version tpot pareto optimization show simultaneously optimizing pipeline accuracy pipeline complexity leads eﬀective compact pipelines. historically machine learning automation research primarily focused optimizing subsets pipeline example grid search commonly-used form hyperparameter optimization applies brute force search explore broad range model parameters order discover parameter allows best model recent research shown randomly evaluating parameter sets within grid search often discovers ideal parameter eﬃciently exhaustive search shows promise intelligent search hyperparameter space. bayesian optimization model hyperparameters particular eﬀective realm even outperformed manual hyperparameter tuning expert practitioners another focus machine learning automation research feature construction. recent example automated feature construction data science machine automatically constructs features relational databases deep feature synthesis work kanter demonstrated crucial role automated feature construction machine learning pipelines entering data science machine three machine learning competitions achieving expert-level performance them. recently fuerer developed machine learning pipeline automation system called auto-sklearn uses bayesian optimization discover ideal combination feature preprocessors models model hyperparameters maximize classiﬁcation accuracy. however autosklearn explores ﬁxed pipelines include data preprocessor feature preprocessor model. thus auto-sklearn incapable producing arbitrarily large pipelines important machine learning analyses. ﬁndings point take-away message intelligent systems capable automatically designing portions machine learning pipelines make machine learning accessible save practitioners considerable figure example tree-based machine learning pipeline. data ﬂows pipeline operators remove modify features successive manner. combination operators allow separate copies data combined provided classiﬁer make ﬁnal classiﬁcation. amounts time automating laborious parts machine learning. such work presented paper establishes blueprint future research automation machine learning pipeline design. section describe tree-based pipeline optimization detail including tools concepts underlie tree-based pipeline optimization tool begin section listing basic pipeline operators currently implemented tpot. next describe operators combined together tree-based pipeline illustrate tree-based pipelines evolved genetic programming. finally section providing overview data sets evaluate tpot. list four main types pipeline operators currently implemented tpot. pipeline operators make existing implementations scikit-learn reading operators refer scikit-learn online documentation preprocessors. implemented standard scaling operator uses sample mean variance scale features robust scaling operator uses sample median inter-quartile range scale features operator generates interacting features polynomial combinations numerical features feature selection. implemented recursive feature elimination strategy strategy selects features strategy selects percentile features strategy removes features meet minimum variance threshold models. paper focus supervised learning models. implemented individual ensemble treebased models non-probabilistic probabilistic linear models k-nearest neighbors combine operators ﬂexible pipeline structure implemented pipelines trees shown figure diﬀerent operators nodes tree. every tree-based pipeline begins copies input data leaves tree four classes pipeline operators preprocessing decomposition feature selection modeling. data passed tree modiﬁed node’s operator. multiple copies data processed possible combine single data data combination operator. time data passed modeling operator resulting classiﬁcations stored recent classiﬁer process data overrides previous predictions earlier classiﬁer’s predictions stored feature. data fully processed pipeline ﬁnal predictions used evaluate overall classiﬁcation performance pipeline. cases divide data stratiﬁed training testing sets pipeline make predictions therefore evaluated testing set. tree-based pipeline structure allows arbitrary pipeline representations; example pipeline could apply operations serial single copy data whereas another pipeline could easily work several copies data combine making ﬁnal classiﬁcation. automatically generate optimize tree-based pipelines well-known evolutionary computation technique called genetic programming implemented python package deap traditionally builds trees mathematical functions optimize toward given criteria. tpot evolve sequence pipeline operators well operator’s parameters maximize classiﬁcation accuracy pipeline. follow standard procedure settings described table changes pipeline modify remove insert sequences pipeline operators tree-based pipeline. paper tpot pipelines evaluated based classiﬁcation accuracy testing set. also introduce extension tpot tpot-pareto uses pareto optimization optimize separate objectives maximizing ﬁnal classiﬁcation accuracy pipeline well minimizing pipeline’s overall complexity since globally optimal solution maximally optimizes criteria maintain pareto front tpot-pareto select pipelines reproduction according nsga-ii selection strategy consecutive generations evolution tpot’s algorithm tinker pipelines—adding pipeline operators improve ﬁtness removing redundant detrimental operators—in intelligent guided search high-performing pipelines. every tpot single best-performing pipeline ever discovered tpot representative pipeline. evaluate tpot adopt diverse complex simulation study design. generate total genetic models associated data sets using gametes open source software package designed generate diverse spectrum pure strict epistatic genetic models. gametes generates random biallelic n-locus single nucleotide polymorphism models pure epistasis loci fewer predictive disease status. precisely generate genetic models speciﬁc heritabilities minor allele frequencies population prevalences. paper data sets included attributes snps predictive binary case/control endpoint snps randomly generated using allele frequency predictive snps simulated four separate purely epistatic models additively combined using newly-added hierarchical data simulation feature gametes. separate interaction model additively contributes determination endpoint overall data include main eﬀects i.e. direct associations single variables endpoint. simulate two-locus epistatic genetic models heritabilities attribute minor allele frequencies gametes select model median diﬃculty generated models generate data sets sample size either within four underlying two-locus epistatic models carry equal additive weight. model generate replicate data sets yielding total data sets together simulation study design allows evaluate tpot across broad range data sets varying diﬃculties sample sizes explore limits tpot’s modeling capabilities. demonstrate tpot’s capabilities evaluate hand-picked benchmark supervised learning data sets well-known uc-irvine machine learning repository purpose benchmarks demonstrate tpot’s performance across broad range application domains data types tpot intended general-purpose supervised machine learning tool. information data sets refer documentation benchmark data sets figure tree-based pipeline optimization tool performance comparison across range data sizes diﬃculties. subplot grid shows distribution balanced cross validation accuracies holdout notched plot represents sample data sets. experiments compared include random forest decision trees version tpot random generation pipelines tpot guided search version tpot uses pareto optimization subplots correspond varying gametes conﬁgurations x-axis modiﬁes number records data y-axis modiﬁes heritability model grid ranges easy conﬁgurations right diﬃcult conﬁgurations bottom left figure tree-based pipeline optimization tool performance comparison across series benchmark data sets. subplot grid shows distribution balanced cross validation accuracies holdout notched plot represents sample diﬀerent cross validation divisions data set. experiments compared include random forest decision trees version tpot random generation pipelines tpot guided search version tpot uses pareto optimization subplots correspond diﬀerent benchmark data sets. note plots tpot missing none replicates ﬁnished within hours. section compare tpot’s classiﬁcation performance controls. ﬁrst control random forest decision trees meant represent basic machine learning analysis state-of-theart model. second control tpot-random version tpot number pipelines randomly generated meant explore whether guided search useful pipeline optimization. addition compare tpot tpot-pareto version tpot uses pareto optimization discover high-performing pipelines smallest pipelines possible. cases divide data sets stratiﬁed training holdout sets performance reported balanced accuracy holdout sets. figure compare four experiments across range gametes data sets. general experiments achieve higher classiﬁcation accuracy larger data sets and/or higher heritability genetic model expected. interestingly even easiest case sample size heritability incapable discovering epistatic interactions features data achieves accuracy average. contrast versions tpot capable achieving accuracy easiest gametes data sets indicates able discover epistatic interactions data using combination feature preprocessing modeling. ﬁnding demonstrates tpot adds value simple machine learning analysis feature preprocessing. furthermore figure shows that cases versions tpot perform more-or-less average gametes data sets. result suggests guided search vital automated design pipelines random search generally performs well guided search. however tpot-pareto tends much consistent discovering eﬀective classiﬁcation pipelines indicated lower variance tpot-pareto’s accuracy distributions—especially gametes data sets larger sample sizes higher heritability. figure compares four experiments series benchmark data sets. again tpot achieves classiﬁcation accuracy across data sets achieves signiﬁcantly higher classiﬁcation accuracy hill-valley car-evaluation data sets. hillvalley-without-noise data particular tpot-pareto achieves accuracy replicates outperforms even standard version tpot. ﬁnding demonstrates value automated pipeline design intelligently explores many diﬀerent ways preprocessing data prior modeling similar gametes data comparisons figure also shows tpot-random typically performs well versions tpot guided search. however major drawbacks randomly generating tpot pipelines presented here. randomly generating tpot pipelines tends much slower optimizing pipelines guided search random pipelines needlessly complex take several hours figure comparison ﬁnal pipeline sizes across tpot experiments. experiments compared include version tpot random generation pipelines tpot guided search version tpot uses pareto optimization evaluate. example massive randomly generated pipelines none tpot-random replicates running hill-valley spambase data sets ﬁnished within hours terminated prematurely. hand tpot tpot-pareto replicates ﬁnished number evaluations less hours. furthermore even though tpot-random pipelines perform nearly well regular tpot pipelines tpot-random pipelines tend needlessly complex contain pipeline operators average contrast tpot tpot-pareto achieve performance operators average respectively. thus even though versions tpot usually achieve accuracy tpot-pareto particular discovers pipelines signiﬁcantly compact. paper shown that many cases automated machine learning pipeline design optimization provide signiﬁcant improvement basic machine learning analysis requiring little input prior knowledge user. however important note goal automated pipeline design replace data scientists machine learning practitioners. rather tree-based pipeline optimization tool data science assistant explores data discovers novel features data recommends pipelines user. there user free export pipelines integrate domain knowledge goal released tpot open source python package provides ﬂexible implementation concepts introduced paper. encourage interested practitioners involve project challenge raised paper tpot randomly generated pipelines consistently achieves accuracy versions tpot guided search practice accuracy criteria pipeline evaluated. randomly generated pipelines tend slower pipelines generated guided search shown inability tpotrandom evaluate pipelines within -hour period several data sets consequence randomly generating pipelines quickly become computationally infeasible data sets grow beyond records. furthermore compactness tpot tpot-pareto pipelines pipelines much easier user interpret apply production setting. thus guided evolutionary search plays vital role automated design machine learning pipelines cannot captured random search. course tpot still early stages development still many improvements made. particular tpot still fairly slow large data sets often requires several hours properly analyze large data set. near future plan explore auto-sklearn heuristics seed tpot population promising pipelines kick start tpot population. similarly plan integrate learning system genetic programming mutation crossover operators bias operators toward changes tend improve ﬁtness goal spending less time exploring detrimental changes. hope enable tpot deliver eﬀective pipelines speedier manner. tree-based pipeline optimization technique shows signiﬁcant promise making machine learning tools accessible non-experts saving practitioners considerable amounts time automating tedious parts machine learning. paper demonstrated tpot achieves similar level performance basic machine learning analysis across wide variety data sets without input prior knowledge user. furthermore several cases tpot able automatically discover combinations preprocessing modeling operators signiﬁcantly outperformed basic machine learning analysis. finally integrating pareto optimization tpot demonstrated tpot design compact easy-to-interpret pipelines without sacriﬁcing classiﬁcation accuracy. such work represents important step toward fully automating machine learning pipeline design. fredericks cheng. exploring automated software composition genetic programming. proceedings annual conference companion genetic evolutionary computation gecco companion pages york acm. tree-based pipeline optimization. proceedings european conference applications evolutionary bio-inspired computation lecture notes computer science berlin germany springer-verlag.", "year": 2016}