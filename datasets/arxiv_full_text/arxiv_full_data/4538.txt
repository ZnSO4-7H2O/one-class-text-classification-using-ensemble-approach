{"title": "Feature-Weighted Linear Stacking", "tag": ["cs.LG", "cs.AI"], "abstract": "Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.", "text": "ensemble methods stacking designed boost predictive accuracy blending predictions multiple machine learning models. recent work shown meta-features additional inputs describing example dataset boost performance ensemble methods greatest reported gains come nonlinear procedures requiring signiﬁcant tuning training time. here present linear technique feature-weighted linear stacking incorporates meta-features improved accuracy retaining well-known virtues linear regression regarding speed stability interpretability. fwls combines model predictions linearly using coeﬃcients linear functions meta-features. technique facet solution second place team recently concluded netﬂix prize competition. signiﬁcant increases accuracy standard linear stacking demonstrated netﬂix prize collaborative ﬁltering dataset. stacking technique predictions collection models given inputs second-level learning algorithm. second-level algorithm trained combine model predictions optimally form ﬁnal predictions. many machine learning practitioners success using stacking related techniques boost prediction accuracy beyond level obtained individual models. contexts stacking also referred blending terms interchangeably here. since introduction modellers employed stacking successfuly wide variety problems including chemometrics spam ﬁltering large collections datasets drawn machine learning repository prominent recent example power model blending netﬂix prize collaborative ﬁltering competition. team bellkor’s pragmatic chaos million prize using blend hundreds diﬀerent models indeed winning solution blend multiple levels i.e. blend blends. intuition suggests reliability model vary function conditions used. instance collaborative ﬁltering context wish predict preferences customers various products amount data collected vary signiﬁcantly depending customer product consideration. model reliable model users rated many products model outperform model users rated products. attempt capitalize intuition many researchers developed approaches attempt improve accuracy stacked regression adapting blending basis side information. additional source information like number products rated user number days since product released often referred meta-feature terminology here. unsurprisingly linear regression common learning algorithm used stacked regression. many virtues linear models well known modellers. computational cost involved ﬁtting models usually modest always predictable. typically require minimum tuning. transparency functional form lends naturally interpretation. minimum linear models often obvious initial attempt performance complex models benchmarked. unfortunately linear models appear well suited capitalize meta-features. simply merge list meta-features list models form overall list independent variables linearly combined blending algorithm resulting functional form appear capture intuition relative emphasis given predictions various models depend meta-features since coeﬃcient associated model constant unaﬀected values meta-features. previous work indeed suggested nonlinear iteratively trained models needed make good meta-features blending. winning netﬂix prize submission bellkor’s pragmatic chaos complex blend many sub-blends many sub-blends blending techniques incorporate meta-features. number user movie ratings number items user rated particular date predicted various internal parameters extracted recommendation models used within overall blend. almost cases algorithms used sub-blends incorporating meta-features nonlinear iterative i.e. either neural network gradient system called stream blends recommendation models presented. eight meta-features tested results showed beneﬁt came using number user ratings number item ratings also commonly used meta-features bellkor’s pragmatic chaos. linear regression model trees bagged model trees used blending algorithms bagged model trees yielding best results. linear regression least successful approaches. collaborative ﬁltering application area meta-features dynamic approaches model blending attempted. classiﬁcation problem context dzeroski zenko attempt augment linear regression stacking algorithm meta-features entropy predicted class probabilities although found yielded limited beneﬁt suite tasks irvine machine learning repository. approach meta-features employ adaptive approach blending described puuronen terziyan tsymbal present blending algorithm based weighted nearest neighbors changes weightings assigned models depending estimates accuracies models within particular subareas input space. thus survey pre-existing literature suggests nonparametric iterative nonlinear approaches usually required order make good meta-features blending. method presented paper however capitalize meta-features linear regression techniques. method simply meta-features additional inputs regressed against. parametrizes coeﬃcients associated models linear functions meta-features. thus technique familiar speed stability interpretability advantages associated linear regression still yielding signiﬁcant accuracy boost. blending approach important part solution submitted ensemble team ﬁnished second place netﬂix prize competition. represent space inputs denote learned prediction functions machine learning models addition represent collection meta-feature functions used blending. meta-feature function maps datapoint corresponding meta-feature standard linear regression stacking seeks blended prediction function thereby obtain expression linear free parameters single linear regression estimate parameters. independent variables regression products fjgi meta-feature function model predictor evaluated figure shows graphical interpretation fwls. outputs individual models represented k-nn ﬁgure. acronyms represent common collaborative ﬁltering algorithms described next section. helpful conceptually think linear combination models coeﬃcients combination vary function meta-features ﬁgure portrays alternative equivalent interpretation corresponding equation corresponding concrete software implementation i.e. regression possible two-way products models meta-features. alternate interpretation fwls view kind bipartite quadratic regression independent variables consisting models meta-features. obtain fwls form starting full quadratic regression dropping terms resulting interacting models other well terms resulting interacting meta-features other. important dataset collected stacked regression consists out-of-sample model predictions. words obtain prediction model particular data point model parameters ﬁtted training include data point. normally achieved k-fold cross-validation. training data split subsets versions model trained version data diﬀerent subset removed. model predictions subset generated version model whose training include subset. occasionally however data distribution models tested distribution training data drawn case blending procedure diﬀer reasonable assume constant component well component varies experiments shown next section paper indeed allow constant component weights. represent within notation deﬁning special meta-feature always takes value similarly might expect modest amount value included inputs regression notation understood cover case including special constant model always takes value number estimated parameters substantial blending large collection models using long-list meta-features. ridge regression used combat overﬁtting cases experimental results presented later paper. noted prior work fwls functional form employed small scale important diﬀerences. netﬂix progress prize paper bell koren volinsky make meta-features within linear model construction similar formulation present here although approach also includes coeﬃcients speciﬁc movie. perhaps importantly approach employs stochastic gradient descent order blending parameters rather classic linear-system-solution approach regression advocate here. results speciﬁc blending eﬀort appear play minor role overall blend. situation small subset training data drawn distribution test data subset removed training data used instead stacking linear regression model since blend optimized respect test distribution. order maximize accuracy fullest models retrained full training including blending function obtained stacking linear regression used blend predictions retrained models. although procedure diﬃcult justify full rigor work well practice. approach commonly taken netﬂix prize competition described detail section number data points used stacking regression matrix elements fjgi input vector data point performing linear regression tikhonov regularization amounts solving system time complexity fwls ﬁrst term corresponds cost computing second term corresponds solution linear system. practice normally much larger almost computational cost comes computing many realistic scenarios computation completed quickly. instance parameters entire regression ﬁnishes minutes seconds single core .ghz intel processor. large problems hundreds models blended using dozens meta-features however computational cost signiﬁcant. fortunately computation naturally lends parallelization multiple cores easily capitalized large-problem scenarios naive implementation entire matrix represented simultaneously memory could memory constraint diﬃculties. straightforward however implement approach calculates entries directly without ever forming entirety memory time. approach requires memory executed iterating training data. dynamic setting models meta-features gradually added blend time signiﬁcant computation saved serializing previously computed matrices disk. model meta-feature arrives previous results reloaded entries need computed. reduces computational cost adding model cost adding meta-feature assuming linear system solved scratch. faster approach sherman-morrison formula updating inverse matrix case second terms preceding expressions improved respectively. similarly adding single data point existing saved blend linear system solved beginning every time sherman-morrison formula inverse employed. netﬂix prize dataset collection ratings submitted customers rental company netﬂix. rating indicates much customer liked particular movie seen past. users movies movie/user pairs rating supplied. date rating made also included. qualifying movie/user pairs constructed rating user made supplied competitors. competitors asked submit rating predictions qualifying set. qualifying derived larger data points formed collecting recent ratings user. larger randomly split subsets equal size probe quiz test set. probe included training data. quiz test together formed qualifying although competitors told million data points other. quiz virtually bearing oﬃcial outcome competition accuracy teams’ predictions quiz reported publicly viewable leaderboard competition. prediction accuracy teams achieved test determined prize. order qualify prize team improve upon accuracy netﬂix’s pre-existing algorithm cinematch least test terms root mean squared error since cinematch’s test rmse improvement terms rmse closely corresponded basis point percentage improvement. test scores unknown anyone netﬂix competition ensure test served truly out-of-sample evaluation submitted solutions. prior awarding million grand prize also progress prizes awarded fall teams best scores point competition. overview techniques used prizes presented papers written prize winners brieﬂy summarize main techniques order provide background meta-features selected. perhaps important class algorithms proved matrix factorization techniques sometimes referred techniques. overview techniques. simplest version approach represents user movie vector real numbers. predicted rating given product user vector movie vector. user movie vector parameters minimized training data although regularization typically employed well. called matrix factorization approach rating matrix possible pairs approximated low-rank matrix product matrix user parameters transpose matrix movie parameters. sophisticated versions various additional parameters means user movie parameters model time eﬀects including model single-day eﬀects. nsvd important variation ﬁrst proposed paterek model represents user vectors corresponding movies user seen vectors distinct vectors comprising aforementioned movie matrix. perhaps second prominent class algorithms used prizewinning solution nearest neighbor models longer widespread academic commerical collaborative ﬁltering history. nearest neighbor algorithms measure similarity movie predicted movies user already rated order generate prediction. common similarity measure involves computing correlation ratings movies received users although similarity measures also employed. standard approaches take weighted average user’s ratings similar movies weighting function similarity level. many variations approach also implemented also user-based version nearest neighbors ratings correlated users gave movie predicted used generate prediction proved much less useful netﬂix prize dataset. restricted boltzmann machines kind stochastic recurrent neural network third major class algorithms. many algorithms involved kind preprocessing known amongst netﬂix prize community removal global eﬀects largely pioneered bell koren volinsky global eﬀects predictions made without simultaneous knowledge speciﬁc identities user movie. simplest examples global eﬀects average rating user average rating movie although many others also found. global eﬀects estimated succession average rating movie instance might estimated residual rating subtracting average rating user. ultimately grand prize. however form half larger coalition known ensemble tied bellkor’s pragmatic chaos terms test rmse ﬁnished second best submission made minutes best submission bellkor’s pragmatic chaos. since probe statistically representative test standard practice among netﬂix competitors probe data sake ﬁtting blend followed procedure here. versions models trained ﬁrst version ﬁtted training probe removed second version ﬁtted training including probe set. ﬁrst version models used generate probe predictions fwls regressions performed using probe set.the ﬁnal blending parameters used generate qualifying predictions obtained ﬁtting entire probe using parameters blend second version models ﬁtted training probe included. note parameters chosen minimize squared error probe reductions probe rmse entirely reﬂective reductions test rmse. evaluating methods addressed issue computing out-of-sample probe rmse based -fold cross validation. blends version probe diﬀerent removed out-of-sample predictions generated blend portion data based probe rmse found list meta-features proved helpful. description meta-features shown table blend uses meta-feature equivalent standard linear regression stacking creation useful meta-features guided detailed understanding characteristics models blended intuition conditions certain models might merit greater emphasis. sake brevity discuss reasoning behind handful meta-features. meta-features commonly used meta-features previously existing literature. intuition behind usage fairly clear i.e. relative accuracy models depend much information regarding particular users movies. reasoning behind meta-features similar. information particular movie depends number users rated also whether users rated many movies. analogous statement made switch users movies previous statement. constant voting feature binary variable indicating whether user rated movies particular date number times movie rated number distinct dates user rated movies bayesian estimate mean rating movie subtracted user’s bayesian-estimated mean number user ratings mean rating user shrunk standard bayesian towards mean users simple averages users norm factor vector user -factor trained residuals global eﬀects norm factor vector movie -factor trained residuals global eﬀects standard deviation prediction -factor ordinal average number user ratings users rated movie standard deviation dates movie rated. multiple ratings date represented multiple times calculation first movie-movie matrix created entries containing probability pair movies rated conditional user rated movies. then movie correlation probability vector movies vector ratings correlations movies computed mentioned previous section many nearest-neighbor-based techniques rely estimated correlations pairs movies. metafeatures attempt characterize correlation information available regarding particular pair predicted. metafeature large user rated many movies signiﬁcant correlation movie predicted bode well neighborhood-based techniques. meta-feature measures whether total represented meta-feature concentrated highly correlated movies whether distributed across larger movies. many models attempt capture eﬀects vary time several meta-features designed reﬂect this. models associate mean rating distinct date user rated movies date-speciﬁc means certain users vary great deal might guess models capture eﬀects given emphasis cases. meta-feature motivated observation. meta-feature also designed suggest importance time eﬀects measuring diﬀerent ways dispersed time ratings are. possible implement version algorithm produces probability distribution possible ratings hence standard deviation indicating uncertainty around predicted rating. speciﬁc approach used derive meta-feature described although alternate technique builds separate models probability rating less equal described high standard deviation reasonably interpreted conﬁdence model’s prediction surprising meta-feature useful blending algorithms algorithms. table shows cross-validated probe rmse results also rmse test corresponds results using meta-features diﬀerence rmses rows represents incremental contribution adding meta-feature meta-features. previously mentioned test rmse unknown competitors competition meta-features developed without knowing results. since contribution meta-features accuracy probe modest unsurprising learn meta-features fact mildly harmful test rmse general cross-validated probe rmse reasonably reliable indicator test rmse. meta-features contribute basis points accuracy probe basis points accuracy test set. course debatable whether meta-features would used commercial context given computational cost ﬁtting blend grows quadratically number meta-features. context netﬂix prize competition however every basis point argued introduction simple linear regression approach merely includes meta-features additional independent variables regressed ill-suited approach stacking metafeatures. experiments netﬂix prize data conﬁrm suspicion. including meta-features additional inputs regression yields cross-validated probe rmse i.e. basis point better rmse obtained without using meta-features all. important note meta-features added collection demonstrating ability decrease rmse obtained previous meta-features. reason metafeatures arrived path dependent sense depended order potential meta-features occurred authors tested. many meta-features evaluated listed table highly valuable used isolation sense rmse meta-feature alone signiﬁcantly improved upon rmse obtained standard linear regression meta-features. however meta-features improve rmse blend using entire meta-features already employed. possible removing meta-features already adding meta-features consideration superior meta-features could obtained line experimentation generally pursued. several potential extensions work many plan pursue present longer paper. classic papers stacking breiman strongly advocates non-negative weights using linear blending model. results presented constrain work underway evaluate value using nonnegativity constraints fwls. another line research pursue involve pruning expanded space model/meta-feature pairs order reduce number parameters estimated thereby perhaps improve upon out-of-sample accuracy speed blend variety linear model feature selection pruning algorithms applicable here. also likely case meta-features discovered course using fwls would useful inputs nonlinear blenders neural networks trees. indeed initial experiments involving neural networks conﬁrm suspicion suggest second-level blending neural network blend fwls blend using meta-features yields accuracy superior either individual blend. plan present detailed results topic future. speed fwls used blending moderately-sized model collection allows quick discovery useful meta-features passed neural networks trees nonlinear techniques. thus even another blending approach strongly preferred fwls value mechanism discovering meta-features. interpretability linear models forgotten additional merits approach. fwls aﬀords opportunity examine eﬀective coeﬃcients associated model various conditions i.e. various values meta-features. scrutiny coeﬃcients lead insights regarding conditions various models successful. finally authors wish emphasize fwls principle applicable wide variety situations stacking employed applications domains collaborative ﬁltering explored future.", "year": 2009}