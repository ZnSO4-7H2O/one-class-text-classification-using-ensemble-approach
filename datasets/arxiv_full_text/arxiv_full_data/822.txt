{"title": "Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency  for Sequence Modeling", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of information inside the units: a first flow along time dimension and a second flow along depth dimension. It also offers a symmetry in how information can flow horizontally and vertically. We analyze the effects of decoupling three different components of our LRU model: Reset Gate, Update Gate and Projected State. We evaluate this family on new LRU models on computational convergence rates and statistical efficiency. Our experiments are performed on four publicly-available datasets, comparing with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has better empirical computational convergence rates and statistical efficiency values, along with learning more accurate language models.", "text": "figure depiction proposed lattice recurrent unit create distinct ﬂows information time depth dimensions. enables better convergence accuracy especially limited resource settings. exemplify character prediction task. ﬁgure shows predicting word \"lattice\" using character starting point. depth neural network makes modeling data exponentially efﬁcient challenge optimize parameters multi-layer models especially resource settings. gridlstm recurrent highway networks introduced improve training deep lstm models impractical train deep stacked lstm models vanishing exploding gradient problem along depth. paper propose family models called lattice recurrent units address challenge deep recurrent models. variants adaptation grus lattice multi-dimensional architecture. structural differences amongst variants lies coupling weights. observe recurrent neural networks shown remarkable success modeling sequences. however resource situations still adversely affect generalizability models. introduce family models called lattice recurrent units address challenge learning deep multi-layer recurrent models limited resources. models achieve goal creating distinct information inside units ﬁrst along time dimension second along depth dimension. also offers symmetry information horizontally vertically. analyze effects decoupling three different components model reset gate update gate projected state. evaluate family models computational convergence rates statistical efﬁciency. experiments performed four publicly-available datasets comparing grid-lstm recurrent highway networks. results show better empirical computational convergence rates statistical efﬁciency values along learning accurate language models. recurrent neural networks shown turing complete hence approximate given function. even though theoretically represent form sequential data networks hard optimize gradient methods gradients start diminishing backpropagated large number time-steps overcome gating mechanisms long short-term memory recently gated recurrent units gates ensures constant backpropagated error along temporal dimension hence making neural sequence models trainable grus lstms variants gated recurrent networks seem capture temporal dependencies successful tasks language modeling machine translation handwriting recognition generation image figure comparison lattice recurrent unit model compared simpler gated recurrent unit model recent gridlstm model. units create distinct information along time depth dimensions unlike unit. also propose variants model reset gate couples update gates projected state couples reset gates decoupling gates model allows adaptation different information propagated along depth time gives boost accuracy convergence rates statistical efﬁciency. effects decoupling weights perform experiments four language modeling datasets compare performancess grus proposed variants lrus. increasing interest training speeds rnns parallelizing computations reducing complexity models also compare models computational convergence rates statistical efﬁciency metrics. also comparison state-of-the recurrent units perform experiments lstms gridlstms rhns. section introduces technical background related lattice recurrent unit models. first describe gated recurrent units multi-layer extensions called stacked gru. models builds upon family recurrent units enable deeper representations. second describe long short-term memory models multi-layer extensions speciﬁcally grid lstm model recurrent highway networks given relevance research goals. models also used experiments baselines. notation sequence modeling consider ordered input-output sequence y)}n length input corresponding output. ordered sequence mean comes time. language modeling classic example inputs could characters sequence sentence. order characters important retain meaning sentence. hence model dependencies sample would useful original recurrent neural networks designed model sequential data hard optimize parameters gated variations proposed counter effects vanishing gradients. purpose paper consider described consistency notation later part paper deﬁne hidden state horizontal direction similarly hidden state vertical direction deﬁned case model represents input hidden state represents output hidden state. seen vertical input means it’s important note dedicated output vertical dimension. multi-layer grus simply replicate horizontal output vertical dimension formulation unit follows where transform matrices input hidden states. tanh usual logistic sigmoid hyperbolic tangent functions respectively element-wise product. gating unit often called reset gate controls information coming input hidden state transformation decides features previous hidden state alongside input state projected onto common space give refer common space gating unit often referred update gate decides fraction passed next time step. increase capacity networks recurrent layers stacked other. since output states output hidden state passed next vertical layer. words next layer equal forces learn transformations useful along depth well time. model potentially trained learn transformations large training set. training size limited natural would output states time dimension second depth dimension. long short-term memory long short-term memory models introduced address issue vanishing gradient recurrent neural networks grus lstms compared extensively numerous tasks. grus performances generally lstms. major difference formulations grus lstms presence dedicated memory state lstm cell unlike grus memory state encoded hidden state itself. grid-lstm grid-lstm produce distinct memory states outputs help understand gird-lstm model let’s consider unit dimensional conﬁguration borrowing notation write input hidden memory states while output hidden memory states. parameters dimension subscripts denote direction information. lstm function typical lstm recurrent unit described grid-lstm unit spits distinct hidden memory states different dimensions. time depth forget gates clever trick allows reduction effect vanishing gradient optimization. recurrent highway network recurrent highway networks increase number non-linear transformations recurrent unit increase capacity model. combined gated previous current hidden states ensures trainability resulting network even deep. even though rhns capacity lack ability transferring intermediate hidden states along depth subsequent time step. section introduce family models lattice recurrent units designed distinct information time depth dimensions. models seen expansion model. shown equations three main components projected state reset gate update gate paper created three members family study important research question components model decoupled enable channels information? ﬁrst model named projected state decouple projected states dimension. second model named reset gate step also decoupling reset gates. finally update gate also call decouple three components including update gate. decouple gate time formulate different members family. following subsections describe model detail. projected state ﬁrst model ps-lru decouple projected state create projected states used compute separate output state formally ps-lru deﬁned using following update functions objectives study effects coupling gates control information hence split experiments parts. ﬁrst comparative study family second compares baseline models including lstm grid-lstm. models baseline models trained tested methodology. words copying result numbers papers instead testing every model sure fair comparisons. focus fair comparisons motivates choice include dropout variational layers models. computational convergence rate number epochs taken converge best possible model based validation scores. lesser number epochs complete optimization often desirable trait. especially useful cases data continuously added model needs trained multiple times statistical efﬁciency generalizing capacity increasing size dataset e.g. grab particular dataset train models independently. would expect models perform better increase size dataset. model performs consistently better loosely considered efﬁcient. task character level language modeling well-established task evaluating sequence models character level modeling entails predicting next character token given previous character tokens. datasets penn treebank dataset pre-processing peace dataset standard benchmarks character-level language modeling. contains collected stories designed allow extraction simple predicate argument structure. basically book \"war peace\" tolstoy. novel brings challenges domain language modeling huge punctuation marks data. example quotation marks come pairs forcing model learn long range dependencies. datasets relatively small around million characters. selected datasets represent scenarios relatively resources. among bigger datasets enwik text hutter prize dataset datasets contain million characters pages wikipedia. enwik markups special characters latin/non-latin characters adding unicode symbols text much cleaner dataset unicode symbols. shear size datasets enough figure visual representation computational convergence rates lstm grid-lstm datasets enwik text. solid lines denote actual training curves dotted lines extension visually compare test score. example figure training ends around epochs lstm’s training continues till around epochs. converges faster lower loss unseen data models. models parameters. y-axis loss categorical crossentropy error test sets x-axis number epochs. figure visual representation computational convergence rates ps-lru rg-lru datasets enwik text. extreme ends coupled-weights spectrum. training curves show systematically decoupling weights gives signiﬁcant boosts convergence rate accuracy model. models parameters. training details make comparison fair ﬁxed number parameters based insights example baseline models number parameters close possible million experiments. done experiments. models either layers deep except rhns trained transition depth following protocol batch size ﬁxed models trained backpropagating error till time steps. optimizer adam exponentially decaying learning rate weights initialized using glorot initialization evaluation. lower better. purpose analysis store loss values held test validation every epoch. checkpoint smallest validation loss considered best model report test loss obtained best model. family mentioned earlier tested recurrent units training environment parameter budgets million parameters. first compare ps-lru rg-lru evaluation metrics. accuracy table observe ps-lru rg-lru loss validations metric. consistently best performing model worst. interesting note number gates also order ps-lru rg-lru lru. seems indicate correlation number gates performance. table table compares modeling capacity lstm grid-lstm ps-lru rg-lru task character level language modeling. dataset report losses test-split categorical crossentropy error time number epochs. lower better. nature test curve similar across family prevents worse model catch computational convergence rate also look time taken different networks converge best model expressed number epochs. choice library recurrent unit implementations make huge difference speed applying gradient descent updates hence choose number epochs unit comparison. compact overview consider experiments conducted make plot interesting note variants average take around epochs converge. statistical efﬁciency grab text construct smaller mini-datasets. running experiments family datasets calculate losses held-out test choose best models mini-dataset. models parameter budget performed best amongst family surprise consistently best loss mini-datasets graphs indication best generalizing capability empirical statistical efﬁciency consistently better recurrent units especially smaller datasets suggests better modeling power scenarios small training datasets. figure test losses plot number epochs. interesting behaviour seen rhns curves test loss rises ﬁrst starts decreasing models converge almost monotonically minimum value. almost always best model epoch continues maintain lead time. computational convergence rate evident figure grid-lstms rhns grus average take around epochs converge almost double family. lstms average require even epoch around complete dataset converge. also lstms high standard deviation around epochs. lstms could potentially take large number epochs converge lrus standard deviation around epochs stable convergence rate different datasets. statistical efﬁciency expected test losses model decrease monotonically increase amount data exception transition text. possible reason could last text much harder model rest. lrus perform good grid-lstms lstms increase amount data available training better. difference losses different models increase decrease data best worst lstm. models perform equally well text signiﬁcant difference across models mini-datasets. evidence hand lrus seem better empirical statistical efﬁciency especially cases lesser data. figure visual representation statistical efﬁciency lstm grid-lstm ps-lru rg-lru text. even though decreasing amount data negatively impacts accuracy language model performs better compared models. network trained data better models data indicates superiority low-resource scenarios. y-axis test loss best parameters validation sets x-axis percent data used training. models parameters. figure plot number epochs required convergence across models conducted experiments. average family models converge faster standard deviation convergence epochs across experiments. lstm grid-lstm networks much higher mean standard deviation convergence epochs along many outliers take much times network train. hence models demonstrate faster stable convergence rate. figure gradient norms across layers number epochs heatmap. represents distribution gradients across layers number epochs model. darker values bigger. note color ﬁgures mean values gradients apart other. grus extremely gradients high gradients across layers. language model recurrent units. evident figure average gradient norms concentrated close last layers model spread across evenly model. indicates gradients backpropagated deep enough gradient based optimization work. introduced family models called lattice recurrent units address challenge learning deep multi-layer recurrent models limited resources. experiments performed four publicly-available datasets comparing grid-lstm recurrent highway networks lstms grus. results indicated best accuracy convergence rate statistical efﬁciency amongst baseline models training language models especially dealing small datasets. also analyzed effects decoupling three different components model reset gate update gate projected state. amongst ps-lru rg-lru best metrics evaluation mentioned above. fact trend observed decoupling gates leads better performance. hence models achieved goal learning multi-layer networks limited resources creating distinct information along time depth. project partially supported oculus research grant. authors would like thank volkan cirik rama kumar pasumarthi bhuwan dhingra valuable inputs.", "year": 2017}