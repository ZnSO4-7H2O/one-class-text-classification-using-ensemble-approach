{"title": "Variational Deep Q Network", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a framework that directly tackles the probability distribution of the value function parameters in Deep Q Network (DQN), with powerful variational inference subroutines to approximate the posterior of the parameters. We will establish the equivalence between our proposed surrogate objective and variational inference loss. Our new algorithm achieves efficient exploration and performs well on large scale chain Markov Decision Process (MDP).", "text": "propose framework directly tackles probability distribution value function parameters deep network powerful variational inference subroutines approximate posterior parameters. establish equivalence proposed surrogate objective variational inference loss. algorithm achieves efﬁcient exploration performs well large scale chain markov decision process deep reinforcement learning enjoyed numerous recent successes video games board games robotics control deep algorithms typically apply naive exploration schemes \u0001−greedy directly injecting noise actions action level entropy regularization however local perturbations actions likely lead systematic exploration hard environments recent work deep exploration applies bootstrap approximate posterior probability value functions injects noise value function/policy parameter space propose framework directly approximates distribution value function parameters deep network. present surrogate objective combines bellman error entropy term encourages efﬁcient exploration. equivalence proposed objective variational inference loss allows optimization parameters using powerful variational inference subroutines. algorithm interpreted performing approximate thompson sampling partially justify algorithm’s efﬁciency. demonstrate algorithm achieves efﬁcient exploration good performance large scale chain markov decision processes surpasses \u0001−greedy exploration noisynet markov decision process markov decision process tuple state space action space transition kernel reward function initial distribution states. policy mapping state action time state agent takes action transitions receives reward unless otherwise stated expectation state reward respect transition kernel reward function objective policy maximize discounted cumulative reward deep networks proposes approximate action value function neural network parameters greedy policy respect choose bellman error equation minimized. practice expectation estimated sample trajectories collected environment assumed period approximate action value function computed state-action pair sample trajectory. approximate bellman error called target value. minimize essentially term maxa minimize discrepancy target value prediction stabilize training proposes compute target value target network parameter target network architecture original network parameters slowly updated allowing target distribution stationary. ﬁnal approximate bellman error variational inference given generative model parameter samples generated distribution deﬁne prior parameters given generated data {xj}n posterior computed bayes rule cases challenging evaluate directly. consider using variational family distribution parameter approximate posterior. approach minimize divergence complex generative models bayesian neural networks approximately solve minimization problem using gradient descent. gradient derived expectation estimated sample averages practical implementation. kl||p) approximately minimized could directly infer ﬁrst successful frameworks deep reinforcement learning. built upon original work numerous attempts improve learning stability sample efﬁciency prioritized replay double duel among others. duality control inference encourages application variational inference reinforcement learning problems. propose specialized inference techniques applicable small mdps could scaled large problems. vime proposes encourage exploration informational bonus. algorithm learns dynamics model environment computes informational bonus based changes posterior distribution dynamics model. informational bonus computed variational family distribution approximates posterior. offers novel approach exploration exploration strategy still intuitively local. bootstrapped proposes approximate formidable posterior value function parameters bootstrap. different heads bootstrapped trained different sets bootstrapped experience data. multiple heads bootstrapped entail diverse strategies encourage exploration. though bootstrapping performed efﬁciently parallel computing method general computationally costly. recent work noisynet proposes noise value function parameters policy parameters directly. true parameters model parameters govern distribution value function/policy parameters. re-parametrization trick distribution parameters updated conventional backpropagation. noisynet applies randomization parameter space corresponds randomization policy space entails consistent exploration. network closest spirit work. network also randomizes policy space achieves good performance dialogue tasks combined vime entire pipeline natural language processing system. compared work formulation starts surrogate objective explicitly encourages exploration establishes connection variational inference. variational interpretation allows leverage efﬁcient black variational subroutines update network parameters. formulation optimal action value function approximated neural network parameter consider following parameterized distribution parameter minimize expected bellman error adopted sample estimate bellman error distribution speciﬁes distribution equivalently speciﬁes distribution policy entail efﬁcient exploration need dispersed. entropy distribution. regularization constant used balance expected bellman error entropy bonus. achieves expected bellman error encompassing many different policies possible. stabilize training target parameter distribution slowly updated parameters target maxa computed target network sampled target distribution qφ−. ﬁnal surrogate objective bridge variational inference consider bayesian neural network parameter improper uniform prior network generates data gaussian distribution given standard error given data {dj}n denotes posterior distribution parameter objective reduces divergence posterior hence update parameter based proposed objective equivalent variational family distribution approximation posterior fact know posterior distribution minimizer distribution established equivalence surrogate objective variational inference loss general need assume gaussian generative model variational inference algorithm perform approximate minimization bellman error. interpretation allows apply powerful black-box variational inference packages edward update value function leverage different black-box algorithms recover original special case variational dqn. appendix. variational inference interpretation proposed objective allows leverage powerful variational inference machinery update policy distribution parameter principal distribution parameter target distribution parameter time step sample select action greedy respect experience tuple st+} added buffer update. updating parameters sample mini-batch tuples using target network parameter evaluate divergence using generated data improper uniform prior parameter updated taking gradient descent step divergence. target parameter updated original dqn. pseudocode summarized below. algorithm interpreted performing approximate thompson sampling appendix. input improper uniform prior target parameter update period learning rate genera initialize parameters replay buffer step counter counter ...e counter counter sample state choose maxa transition reward save experience tuple st+} buffer sample tuples sample parameters compute target maxa take gradient divergence counter four classic control tasks openai environments require agent learn good policy properly control mechanical systems. among them mountaincar acrobot considered challenging since solve environment requires efﬁcient exploration. example mountaincar exploration strategy stuck valley agent never learn optimal policy. chain serves benchmark environment test algorithm entails deep exploration. environment consists states episode lasts time steps. agent actions {left right} state state absorbing. transition deterministic. state agent receives reward state agent receives reward reward anywhere else. initial state always making hard agent escape local optimality agent explores randomly expected number time steps required reach large almost possible randomly exploring agent reach single episode optimal strategy reach keeping choosing right never learned. feature state used input neural network compute approximate action value function. suggested consider feature mapping φtherm i{·} indicator function. compare variational control tasks. variational solve four control tasks within given number iterations display quite different characteristics training curves. simple tasks like cartpole variational makes progress faster converges slower rate. potentially variational optimizes bellman error exploration bonus exploration bonus term effect hinders fast convergence optimal strategy simple tasks. figure variational training curves algorithms four control tasks. training curves averaged multiple initializations algorithms. iteration episodes. chain compare variational noisynet chain tasks. small algorithms converge optimal policy within reasonable number iterations even training curves variational noisy network similar. increases barely makes progress cannot converge optimal policy noisynet converges slowly optimal policy oscillates much. noisynet barely make progress tranining. performance variational fairly stable across large range variational converges optimal policy within episodes average. however keeps increasing variational takes longer time optimal policy makes steady improvement time. discrepancy performance three algorithms chain tasks potentially different exploration schemes. stated previously random exploration expected number steps takes reach approximately since applies \u0001−greedy exploration large never even reach within limited number episodes letting alone learning optimal policy. noisynet maintains distribution value functions allows agent consistently execute sequence actions different policies leading efﬁcient exploration. however since noisynet explicitly encourage dispersed distribution policies algorithm still converge prematurely variance parameter converges quickly zero. hand variational encourages high entropy policy distribution prevent premature convergence. figure variational noisynet chain training curves three algorithms chain tasks different training curves averaged multiple initializations algorithms. iteration episodes. investigate variational systematic efﬁcient exploration environment plot state visit counts variational figure visit count state episode agent ever visits otherwise. running average consecutive episodes approximate visit probability state current policy. figure show visit probability meant show agent ever explores half chain probability episode. early stage training variational starts maintains relatively high probability visiting three states. enables agent visit sufﬁcient number trials converges optimal policy keeping going right reaching \u0001−greedy random hand occasionally nontrivial probability visiting exploration. since enough momentum consistently beyond visit visits ﬁnally suppressed agent converges locally optimal policy appendix comparison visit counts sets noisynet. figure variational chain state visit counts. count state state ever visited episode. running averages multiple consecutive episodes produces approximate state visit probability current policy. probability curve average multiple initializations. iteration episodes. proposed framework directly tackle distribution value function parameters. assigning systematic randomness value function parameters entails efﬁcient randomization policy space allow agent efﬁcient exploration. addition encouraging high entropy parameter distribution prevents premature convergence. also established equivalence proposed surrogate objective variational inference loss allows leverage black variational inference machinery update value function parameters. potential extension current work apply similar ideas q-learning continuous control tasks policy gradient methods. leave future work. lillicrap hunt pritzel heess erez tassa silver wierstra continuous control deep reinforcement learning. international conference learning representations. silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game using deep neural networks tree search. nature ./nature. posterior general possible evaluate. hence propose given samples {dj}n variational family distribution parameter approximate posterior. variational inference literature provided numerous techniques compute ﬂexible model black variational inference scalable. consider minimizing divergence therefore variational inference procedure updates approximate posterior particular variational inference using converge approximate minimizer bellman error. kl||p] result additional entropy bonus term surrogate objective effect encourages dispersed policy distribution efﬁcient exploration. since established equivalence surrogate objective variational inference loss could leverage highly optimized implementation probabilistic programming packages perform parameter update. experiment used edward minimize divergence variational distribution posterior classic control tasks train variational noisynet agents episodes task. learning rate discount factor chain mdps train agents episodes ﬁxed learning rate discount factor experiments batch size mini-batch sampled replay buffer target network updated every time steps. experiment replicated multiple times using different random seeds start entire training pipeline. uses exploration constant throughout training. variational noisynet component-wise gaussian distribution parameterize distribution value function parameters initialized according recipes variational updated using klqp inference algorithm regularization constant variational family distribution point distribution hence dimension effect itself. apply maximum posterior inference update gaussian generative model improper uniform prior equivalent minimizing bellman error only. present state visit counts three algorithms small variational noisynet identify optimal path faster display larger variance performance makes progress steady manner. medium sized variational still manages converge optimal policy though initial exploration stage exhibits larger variance. occasionally pass middle point cannot reach noisynet explores efﬁciently since sometimes converge optimal policy less stable variational dqn. figure variational noisynet chain state visit counts. count state state ever visited episode. running averages multiple consecutive episodes produces approximate state visit probability current policy. probability curve average multiple initializations. iteration episodes. thompson sampling efﬁcient exploration scheme multi-arm bandits mdps. step variational maintains distribution action-value function algorithm proceeds hence quality exact thompson sampling depends quality approximation second fact goal. approximation perfect approximate sampling scheme still beneﬁcial exploration.", "year": 2017}