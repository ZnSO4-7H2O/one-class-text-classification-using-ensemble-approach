{"title": "Natural Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.", "text": "introduce natural neural networks novel family algorithms speed convergence adapting internal representation training improve conditioning fisher matrix. particular show speciﬁc example employs simple efﬁcient reparametrization neural network weights implicitly whitening representation obtained layer preserving feed-forward computation network. networks trained efﬁciently proposed projected natural gradient descent algorithm amortizes cost reparametrizations many parameter updates closely related mirror descent online learning algorithm. highlight beneﬁts method unsupervised supervised learning tasks showcase scalability training large-scale imagenet challenge dataset. deep networks proven extremely successful across broad range applications. deep complex structure affords rich modeling capacity also creates complex dependencies parameters make learning difﬁcult ﬁrst order stochastic gradient descent long remains workhorse deep learning ability extract highlevel representations data hindered difﬁcult optimization evidenced boost performance offered batch normalization inception architecture though adoption remains limited natural gradient appears ideally suited difﬁcult optimization issues. following direction steepest descent probabilistic manifold natural gradient make constant progress course optimization measured kullback-leibler divergence consecutive iterates. utilizing proper distance measure ensures natural gradient invariant parametrization model. unfortunately application limited high computational cost. natural gradient descent typically requires estimate fisher information matrix square number parameters worse requires computing inverse. truncated newton methods avoid explicitly forming memory require expensive iterative procedure compute inverse. computations wasteful take account smooth change fisher optimization highly structured nature deep models. inspired recent work model reparametrizations approach starts simple question devise neural network architecture whose fisher constrained identity? important question would equivalent resulting model. main contribution paper providing simple theoretically justiﬁed network reparametrization approximates ﬁrst-order gradient descent block-diagonal natural gradient update layers. method computationally efﬁcient local nature reparametrization based whitening amortized nature algorithm. second contribution unifying many heuristics commonly used training neural networks roof natural gradient highlighting important connection model reparametrizations mirror descent finally showcase efﬁciency scalability method across broad-range experiments scaling method standard deep auto-encoders large convolutional models imagenet trained across multiple gpus. knowledge ﬁrst-time natural gradient algorithm scaled problems magnitude. section provides necessary background derives particular form whose structure efﬁcient approximation. tailor development method classiﬁcation setting approach generalizes regression density estimation. consider problem ﬁtting parameters model empirical distribution log-loss. denote observation vector associated label. concretely stochastic optimization problem aims solve deﬁning per-example loss stochastic gradient descent performs minimization iteratively following direction steepest descent given column vector namely iterate solution auxiliary optimization problem controls distance consecutive iterates using distance. contrast natural gradient relies kl-divergence iterates appropriate distance measure probability distributions. metric determined fisher information matrix start deriving precise form fisher canonical multi-layer perceptron composed layers. consider following deep network binary classiﬁcation though approach generalizes arbitrary number output classes. deﬁne backpropagated gradient i-th non-linearity. ignore block-diagonal components fisher matrix focus block corresponding interactions parameters layer block takes form element column matrix i-th element vector entry fisher capturing interactions parameters hypothesis veriﬁed experimentally sec. greatly improve conditioning fisher enforcing ignoring possible correlations block diagonal terms fisher. section introduces whitened neural networks perform approximate whitening internal hidden representations. begin presenting novel whitened neural layer assumption network statistics ﬁxed. show layers adapted efﬁciently track population statistics course training. resulting learning algorithm referred projected natural gradient descent highlight interesting connection prong mirror descent section rate equivalently size trust region. parameters rni×ni− analogous canonical parameters neural network introduced though operate space whitened unit activations layer stacked form deep neural network layers model parameters d··· whitening coefﬁcients c··· cl−} depicted fig. though layer might appear over-parametrized ﬁrst glance crucially learn whitening coefﬁcients loss minimization instead estimate directly model statistics. coefﬁcients thus constants point view optimizer simply serve improve conditioning fisher respect parameters denoted indeed using derivation block-diagonal terms algorithm projected natural gradient descent input training initial parameters hyper-parameters reparam. frequency number samples regularization term repeat convergence taking care interfere convergence properties gradient descent. achieved coupling updates corresponding updates overall function implemented remains unchanged e.g. preserving product viui− update whitening coefﬁcients unfortunately estimating mean diag could performed online minibatch samples recent batch normalization scheme estimating full covariance matrix undoubtedly require larger number samples. statistics could accumulated online exponential moving average rmsprop k-fac cost eigendecomposition required computing whitening matrix remains cubic layer size. simplest instantiation method exploit smoothness gradient descent simply amortizing cost operations consecutive updates. updates whitened model closely aligned immediately following reparametrization. quality approximation degrade time subsequent reparametrization. resulting algorithm shown pseudo-code algorithm improve upon basic amortization scheme including diagonal scaling based standard deviation layer activations gradient update thus mimicking effect diagonal natural gradient method. update valid enhanced version method denoted prong+ scales rows accordingly preserve feed-forward computation network. implemented combining prong batch normalization. inherent duality parameters whitened neural layer parameters canonical model. indeed exist linear projections canonical parameters whitened parameters vice-versa. corresponds line algorithm corresponds line duality reveals close connection prong mirror descent mirror descent online learning algorithm generalizes proximal form gradient descent class bregman divergences strictly convex differentiable function. replacing distance mirror descent solves proximal problem applying ﬁrst-order updates dual space projectψ complex back onto primal space. deﬁning conjugate mirror descent updates given well known natural gradient special case distance generating function chosen mirror updates somewhat unintuitive however. gradient applied dual space computed space parameters prong relates θt√f instead previously deﬁned trivial show using function enables directly update dual parameters using gradient computed directly dual space. indeed resulting updates shown implement natural gradient thus equivalent updates appropriate choice used prong canonical neural parameters whitened layers illustrated fig. advantage whitened form amortize cost projections several updates gradients computed directly dual parameter space. work extends recent contributions formalizing many commonly used heuristics training mlps importance zero-mean activations gradients well importance normalized variances forward backward passes recently vatanen extended previous work introducing multiplicative constant centered non-linearity. contrast introduce full whitening matrix focus whitening feedforward network activations instead normalizing geometric mean units gradient variances. recently introduced batch normalization scheme quite closely resembles diagonal version prong main difference normalizes variance activations non-linearity opposed normalizing latent activations looking full covariance. furthermore implements normalization modifying feed-forward computations thus requiring method backpropagate normalization operator. diagonal version prong also bares interesting resemblance rmsprop normalization terms involve square root fim. important distinction however prong applies update whitened parameter space thus preserving natural gradient interpretation. k-fac also closely related prong developed concurrently method. implementations targets block diagonal prong also exploiting figure optimizing deep auto-encoder mnist. impact eigenvalue regularization term impact amortization period showing initialization whitening reparametrization important achieving faster learning better error rate. training error number updates. training error cpu-time. plots show prong achieves better error rate number updates wall clock time. rank structure blocks efﬁciency reminiscent tonga. method however operates online low-rank updates block similar preconditioning used kaldi speech recognition toolkit contrast approach based amortization. also consider covariance backpropagated gradients prong looks covariance activations k-fac proposes tri-diagonal variant decorrelates gradients across neighboring layers though resulting complex algorithm. similar algorithm prong later found appeared simply thought experiment amortization recourse efﬁciently computing begin diagnostic experiments highlight effectiveness method improving conditioning. also illustrate impact hyper-parameters controlling frequency reparametrization size trust region. section evaluates prong unsupervised learning problems models deep fully connected. section moves onto large convolutional models image classiﬁcation. conditioning. provide better understanding approximation made prong train small -layer tanh non-linearities downsampled version mnist model size chosen order full fisher tractable. fig. shows middle hidden layers whitening model activations fig. depicts evolution condition number training measured percentage initial value present curves rmsprop prong. results clearly show reparametrization performed prong improves conditioning observations conﬁrm initial assumption namely improve conditioning block diagonal fisher whitening activations alone. sensitivity hyper-parameters. figures highlight effect eigenvalue regularization term reparametrization interval experiments performed best performing auto-encoder section mnist dataset. figures plot reconstruction error training various values determines maximum multiplier learning rate learning becomes extremely sensitive learning rate high. smaller step sizes however lowering yield signiﬁcant speedups often converging faster simply using larger learning rate. conﬁrms importance manifold curvature optimization compares impact models proper whitened initialization models initialized standard fan-in initialization results quite surprising showing effectiveness whitening reparametrization simple initialization scheme. said performance degrade conditioning becomes excessively large following martens compare prong task minimizing reconstruction error -layer auto-encoder mnist dataset. encoder composed densely connected sigmoidal layers number hidden units layer symmetric decoder. hyper-parameters selected grid search based training error following grid speciﬁcations training batch size learning rates momentum term rmsprop tuned moving average coefﬁcient regularization term controlling maximum scaling factor prong ﬁxed natural reparametrization using samples reconstruction error respect updates wallclock time shown fig. prong signiﬁcantly outperforms baseline methods order magnitude number updates. respect wallclock method signiﬁcantly outperforms baselines terms time taken reach certain error threshold despite fact runtime epoch prong compared batch normalization rmsprop note timing numbers reﬂect performance optimal choice hyper-parameters case batch normalization yielded batch size compared methods. breaking performance runtime prong spent performing whitening reparametrization compared estimating layer means covariances. conﬁrms amortization paramount success method. next experiments addresses problem training deep supervised convolutional networks object recognition. following perform whitening across feature maps only treat pixels given feature independent samples. allows implement whitened neural layer sequence convolutions ﬁrst whitening ﬁlter. prong compared rmsprop batch normalization algorithm accelerated momentum. results presented cifar- imagenet challenge datasets cases learning rates decreased using waterfall annealing schedule divided learning rate validation error failed improve number evaluations. cifar- model used cifar experiments consists convolutional layers receptive ﬁelds. spatial max-pooling applied stacks convolutional layers exception last convolutional layer computes class scores followed global max-pooling soft-max non-linearity. particular choice architecture inspired model held ﬁxed across experiments. number ﬁlters layer follows unstable combinations learning rates omitted clarity. note implementation whitening operations optimized take advantage acceleration opposed neural network computations. therefore runtime method expected improve move eigen-decompositions gpu. cifar- validation error estimated every updates learning rate decreased factor validation error failed improve consecutive evaluations. imagenet employed aggressive schedule required validation error improves epoch. figure classiﬁcation error cifar- imagenet cifar- prong achieves better test error converges faster. imagenet prong+ achieves comparable validation error maintaining faster covergence rate. model trained random crops random horizontal reﬂections. model selection performed held-out validation examples. results shown fig. respect training error prong batch normalization seem offer similar speedups compared momentum. hypothesis beneﬁts prong pronounced densely connected networks number units layer typically larger number maps used convolutional networks. interestingly prong generalized better achieving test error batch normalization. could reﬂect ﬁndings showed leverage unlabeled data better generalization unlabeled data comes extra perturbations training estimating whitening matrices. ﬁnal experiments aims show scalability method thus apply natural gradient algorithm large-scale ilsvrc dataset using inception architecture order scale problems size parallelized training loop split processing single minibatch across multiple gpus. note prong scale well setting estimation mean covariance parameters layer also embarassingly parallel. eight gpus used computing gradients estimating model statistics though eigen decomposition required whitening parallelized current implementation. optimization algorithms considered initial learning rates used value momentum coefﬁcient. prong tested reparametrization periods typically using eigenvalues regularized adding small constant scaling eigenvectors given difﬁculty task employed enhanced prong+ version algorithm simple periodic whitening model proved unstable. figure shows batch normalisation prong+ converge approximately top- validation error similar cpu-time. comparison achieved validation error prong+ however exhibits much faster convergence initially updates obtains around error compared alone. stress imagenet results somewhat preliminary. top- error higher reported used much less extensive data augmentation pipeline. beginning explore natural gradient methods achieve large scale optimization problems encouraged initial ﬁndings. grid searched exhaustively cost would prohibitive. main focus optimization regularization consisted simple weight decay parameter dropout this instability compounded momentum initially reset model began paper asking whether convergence speed could improved simple model reparametrizations driven structure fisher matrix. theoretical experimental perspective shown whitened neural networks achieve simple scalable efﬁcient whitening reparametrization. however several possible instantiations concept natural neural networks. previous incarnation idea exploited similar reparametrization include whitening backpropagated gradients. favor simpler approach presented paper generally found alternative less stable deep networks. ensuring zero-mean gradients also required skip-connections tedious book-keeping offset reparametrization centered non-linearities maintaining whitened activations also offer additional beneﬁts point view model compression generalization. virtue whitening projection uihi forms ordered representation least signiﬁcant bits. sharp roll-off eigenspectrum explain deep networks ammenable compression similarly could envision spectral versions dropout dropout probability function eigenvalues. alternative ways orthogonalizing representation layer also explored alternate decompositions perhaps exploiting connection linear auto-encoders pca. also plan pursuing connection mirror descent bridging deep learning methods online convex optimization. extremely grateful shakir mohamed invaluable discussions feedback preparation manuscript. also thank philip thomas volodymyr mnih raia hadsell sergey ioffe shane legg feedback paper. references shun-ichi amari. natural gradient works efﬁciently learning. neural computation jimmy rich caruana. deep nets really need deep? nips. amir beck marc teboulle. mirror descent nonlinear projected subgradient methods convex raskutti mukherjee. information geometry mirror descent. arxiv october nicolas roux pierre antoine manzagol yoshua bengio. topmoumoute online natural gradient olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru tommi vatanen tapani raiko harri valpola yann lecun. pushing stochastic gradient towards second-order methods backpropagation learning transformations nonlinearities. iconip", "year": 2015}