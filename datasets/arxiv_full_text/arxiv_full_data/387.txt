{"title": "Bounding and Counting Linear Regions of Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE", "math.OC", "stat.ML"], "abstract": "In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly build upon the work of Montufar et al. (2014), Montufar (2017) and Raghu et al. (2017) by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. In addition to achieving tighter bounds, we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed-integer linear formulation that maps the input space to output. We use this new capability to visualize how the number of linear regions change while training DNNs.", "text": "paper study representational power deep neural networks belong family piecewise-linear functions based activation units rectiﬁer maxout. investigate complexity networks studying number linear regions function. typically function seen large family linear functions acting millions regions. directly build upon work mont´ufar mont´ufar raghu reﬁning upper lower bounds number linear regions rectiﬁed maxout networks. addition achieving tighter bounds also develop novel method perform exact enumeration counting number linear regions mixed-integer linear formulation maps input space output. capability visualize number linear regions change training dnns. witnessed unprecedented success deep learning algorithms computer vision speech domains popular deep learning architectures alexnet googlenet residual networks shown record beating performance various image recognition tasks empirical results still govern design network architecture terms depth activation functions. important practical considerations part successful architectures greater depth activation functions rectiﬁed linear units large theory practice many researchers looking theoretical modeling representational power dnns continuous function approximated arbitrary accuracy using single hidden layer sigmoid activation functions imply shallow networks sufﬁcient model problems practice. typically shallow networks require exponentially number neurons model functions modeled using much fewer activation functions deeper ones wide variety activation functions threshold logistic hyperbolic tangent tanh) rectiﬁed linear units max{ maxouts max{z zk}). activation functions offer different modeling capabilities. example sigmoid networks shown expressive similar-sized threshold networks recently shown relus expressive similar-sized threshold networks deriving transformations network another complexity neural networks belonging family functions analyzed looking network partition input space exponential number linear response regions basic idea function simple divide input space several regions individual linear functions regions. functions partitioning input space larger number linear regions considered complex ones words possess better representational power. case relus shown deep networks separate input space exponentially linear response regions shallow counterparts despite using number activation functions results later extended improved particular mont´ufar shows upper lower bounds maximal number linear regions relu single layer maxout network lower bound maxout dnn. furthermore raghu mont´ufar improve upper bound relu dnn. upper bound asymptotically matches lower bound mont´ufar number layers input dimension constant layers width. finally arora improves lower bound providing family relu dnns exponential number regions given ﬁxed size depth. consider feedforward neural networks paper. assume network input variables given output variables given ym}. hidden layer hidden neurons whose activations given matrix corresponds weights neuron layer bias vector used obtain activation functions neurons layer based relu max{ activation function activations hidden neurons outputs given below considered pascanu output layer linear layer computes linear combination activations previous layer without relus. treat piecewise linear function maps input paper primarily deals investigating bounds linear regions function. subtly different deﬁnitions linear regions literature formally deﬁne them. deﬁnition given function linear region deﬁned maximal connected subset input space linear activation pattern consider input vector xn}. every layer deﬁne activation relu active aggregate activation sets call activation pattern. note consider activation patterns layer activation patterns previously deﬁned terms strings input corresponds activation pattern feeding results activations deﬁnition given function represented linear region input vectors corresponds activation pattern dnn. prefer look linear regions activation patterns interchangeably refer activation pattern region. deﬁnitions essentially same except degenerate cases. could scenarios different activation patterns correspond adjacent regions linear function. case deﬁnition produce linear region whereas deﬁnition yield linear regions. effect bounds derive paper. figure simple inputs three hidden layers activation units each. visualization hyperplanes ﬁrst second third hidden layers respectively partitioning input space several linear regions. arrows indicate directions corresponding neurons activated. visualization hyperplanes ﬁrst second third hidden layers space given outputs respective previous layers. activation units hidden layers thought hyperplanes divide space two. side hyperplane unit outputs positive value. points side hyperplane including itself unit outputs bound attained general position. term general position basically means small perturbation hyperplanes change number regions. corresponds exact maximal number regions single layer relus input dimension figs. provide visualization relus partition input space. figs. show hyperplanes corresponding relus layers respectively. figs. consider hyperplanes input space fig. hyperplanes second third layers respectively affected transformations applied earlier hidden layers. regions partitioned consider additional layers. fig. also highlights activation boundaries behave like hyperplanes inside region bend whenever intersect boundary previous layer. also pointed raghu particular cannot appear twice region deﬁned single hyperplane region. moreover boundaries need connected illustrated fig. main contributions summarize main contributions paper below figure network input three activation units show hyperplanes corresponding activation units ﬁrst hidden layer. words activation units given max{ max{−x activation unit third layer given max{ activation boundary neuron disconnected. present exact maximal number regions input dimension one. additionally provide ﬁrst upper bound number linear regions multi-layer maxout networks show relus exact maximal number linear regions shallow networks larger deep networks input dimension exceeds number neurons. result particularly interesting since cannot inferred bounds derived prior work. mixed-integer linear formulation show exact counting linear regions indeed possible. ﬁrst time show exact counting number linear regions several small-sized dnns training process. capability used evaluate tightness bounds potentially analyze correlation validation accuracy number linear regions. also provides insights linear regions vary training process mont´ufar derive upper bound hidden units obtained mapping linear regions activation patterns. raghu improves result deriving asymptotic upper bound maximal number regions assuming note stronger upper bound appeared mont´ufar derived bound relaxing terms factoring expression. layers width expression best known asymptotic bound ﬁrst presented raghu bottleneck effect. bound sensitive positioning layers small relative others property call bottleneck effect. subtract neuron layers width choosing closer input layer lead larger decrease bound. occurs index essentially limited widths current previous layers words smaller widths ﬁrst layers network imply bottleneck bound. particular -layer network show appendix input dimension sufﬁciently large create bottleneck moving neuron ﬁrst layer second layer strictly decreases bound tightens bottleneck. figure illustrates behavior. solid line keep total size network shift small-to-large network large-to-small network terms width. bound monotonically increases reduce bottleneck. layer constant width represented dashed line bound decreases layers last become small create bottleneck last layer. property upper bound rather exact maximal number regions observe section empirical results number regions trained network exhibit behavior resembles bound width layers vary. deep shallow large input dimensions. several applications imaging input dimension large. mont´ufar show input dimension constant number regions deep networks asymptotically larger shallow networks. complement picture establishing input dimension large shallow networks attain regions deep networks. precisely compare deep network layers equal width shallow network layer width appendix show using theorem input dimension exceeds size network ratio exact maximal number regions deep shallow network goes zero approaches inﬁnity. also show appendix -layer network input dimension larger widths turning shallow network layer relus increases exact maximal number regions. figure illustrates behavior. increase number layers keeping total size network constant bound plateaus value lower exact maximal number regions shallow networks. moreover number layers yields highest bound decreases increase input dimension important note property cannot inferred previous upper bounds derived prior work since least max{n total number neurons. remark asymptotically deep shallow networks attain exponentially many regions input dimension least build towards proof theorem given activation matrix rows operation zeroes rows inactive according represents effect relus. region layer deﬁne σsl− region layer partitioned hyperplanes deﬁned neurons layer viewed input space hyperplanes rows verify this note that recursively substitute hidden variables original hyperplane lhl− following resulting weight matrix applied finally deﬁne dimension region rank··· σs). interpreted dimension space corresponding effectively partitions. proof theorem focuses dimension region observation falls certain value regions contained cannot recover higher dimension. zaslavsky showed maximal number regions induced arrangement proof given appendix idea sufﬁces count regions within space next lemma brings lemma context. lemma number regions induced neurons layer within certain region proof. hyperplanes region input space given rows rank upper bounded min{rank rank··· σs)} min{rank dim}. rank min{nl dim}. applying lemma yields result. next lemma show dimension region bounded recursively terms dimension region containing number activated neurons deﬁning lemma region layer region layer contains min{|sl| dim)}. proof. rank··· min{rank) rank··· min{|sl| dim)}. last inequality comes fact zeroed rows count towards rank matrix. remainder proof theorem combine lemmas construct recurrence bounds number regions within given region dimension simplifying recurrence yields expression theorem formalize idea complete proof theorem appendix rank proof. term rank follows proof theorem index theorem becomes min{n rank} insight lemmas dimensions regions non-increasing move layers partitioning words layer dimension region becomes small region able partitioned large number regions. instance dimension region falls zero region never partitioned. suggests want many regions need keep dimensions high. idea next section construct many regions. input dimension equal layers upper bound presented previous section reduces hand lower bound given mont´ufar becomes nl−. natural either bounds tight? answer upper bound tight case assuming sufﬁciently many neurons. theorem consider deep rectiﬁer network layers rectiﬁed linear units layer input dimension maximal number regions neural network expression simpliﬁed form upper bound theorem case proof theorem appendix construction regions replicate themselves layers instead mont´ufar motivated insight previous section order obtain regions want dimension every region large possible. want regions dimension one. intuition leads construction additional region replicated strategies. lower bound mont´ufar arora slightly improved since approaches based extending -dimensional construction similar section since directly comparable former bound terms number neurons layer latter terms total size network. theorem maximal number linear regions induced rectiﬁer network input units hidden layers lower bounded proof theorem appendix comparison differences lower bound theorem mont´ufar theorem replacement condition restrictive nl/n nl/n theorem values exists rectiﬁer network input units proof theorem appendix differences theorem arora theorem replacement construct m-width layer many regions one-dimensional construction remaining layers. terms bounding number regions major difference next result maxout units previous relus reductions dimensionality inactive neurons zeroed output become particular case now. nevertheless using techniques similar ones section following theorem shown theorem consider deep neural network layers rank-k maxout units layer input dimension maximal number regions neural network exact counting linear regions input space bounded minimum maximum values along dimension else corresponds polytope generally deﬁne mixed-integer linear formulation mapping polyhedral regions output space assumption bounded polyhedral natural applications value known lower upper bounds among things formulation count number linear regions. formulation follows continuous variables represent input also denote output neuron layer output hl+. simplify representation lift formulation space also contains output complementary neurons active corresponding neuron not. namely neuron layer also variable binary variables form denote neuron layer active else complement neuron finally assume sufﬁciently large constant. proof statement given appendix details procedure exact counting appendix addition show theory unrestricted inputs mixedinteger formulation maxout networks appendices respectively. results important consequences. first allow literature mixedinteger representability disjunctive programming understand modeled rectiﬁer networks ﬁnite number neurons layers. best knowledge discussed before. second imply mixedinteger optimization solvers analyze mapping trained neural network. example cheng another mixed-integer formulation generate adversarial examples dnn. technically feasible linear proportion size neural network mixed-integer formulation. compared cheng show appendix formulation implemented reﬁnements value constants. perform different experiments region counting using small-sized networks relu activation units mnist benchmark dataset ﬁrst experiment generate rectiﬁer networks hidden layers neurons each ﬁnal test error training carried epochs training steps count number linear regions training step. networks count number linear regions within single neuron active output layer hence partitioning regions terms digits classify. fig. show number regions classifying digit progresses training. digits zero linear regions beginning explains begin later plot. total number regions training step presented fig. error measures found appendix overall observe number linear regions jumps orders magnitude varies widely added layer. furthermore initial jump number linear regions classifying digit seems proportional number layers. figure total number linear regions classifying single digit mnist training progresses plot corresponding different number hidden layers. comparison upper bounds mont´ufar mont´ufar theorem total number linear regions network hidden layers totaling neurons. second experiment train rectiﬁer networks hidden layers summing neurons. train network width conﬁguration conditions above test error half ranging case count linear regions within hence restricting activation output layer before. number linear regions networks plotted fig. along upper bound theorem upper bounds mont´ufar mont´ufar error measures experiments found appendix runtimes counting linear regions appendix representational power studied observing number linear regions function represents. work improve upper lower bounds linear regions rectiﬁed networks derived prior work introduce ﬁrst upper bound multi-layer maxout networks. obtain several valuable insights extensions. relu upper bound indicates small widths early layers cause bottleneck effect number regions. reduce width early layer dimensions linear regions become irrecoverably smaller throughout network regions able partitioned much. moreover dimensions linear regions driven width also number activated relus corresponding region. intuition allowed create -dimensional construction maximal number regions eliminating zero-dimensional bottleneck. unexpected useful consequence result shallow networks attain linear regions input dimensions exceed number neurons dnn. addition achieving tighter bounds mixed-integer linear formulation maps input space output show exact counting number linear regions several small-sized dnns training process. ﬁrst experiment observed number linear regions correctly classifying digit mnist benchmark increases vary proportion depth network ﬁrst training epochs. second experiment count total number linear regions vary width layers ﬁxed number neurons experimentally validate bottleneck effect observing results follow similar pattern upper bound show. current results suggest avenues future research. first believe study linear regions eventually lead insights design better dnns practice example validating bottleneck effect found study. properties bounds turn actionable insights conﬁrmed bounds sufﬁciently close actual number regions. example plots appendix show particular network depths maximize relu upper bound given input dimension number neurons. sense number neurons proxy computational resources available. also believe analyzing shape linear regions promising idea future work could provide insight design dnns. another important line research understand exact relation number linear regions accuracy also involve potential overﬁtting. conjecture network training likely generalize well many regions point singled different region particular regions similar labels unlikely compositionally related. second applying exact counting larger networks would depend efﬁcient algorithms using approximations instead. case exact counting smaller scale assess quality current bounds possibly derive insights tighter bounds future work hence leading insights could scaled bianchini scarselli. complexity neural network classiﬁers comparison shallow deep architectures. ieee transactions neural networks learning systems hinton deng g.e. dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury. deep neural networks acoustic modeling speech recognition. ieee signal processing magazine maass schnitger e.d. sontag. comparison computational power sigmoid boolean threshold circuits. theoretical advances neural computation learning proofs theorems lemmas associated upper lower bounds linear regions provided below. theory mixed-integer formulation exact counting case maxouts unrestricted inputs also provided below. results implications terms exact maximal number regions. denote following notation above. moreover following lemma useful throughout section. lemma ﬁrst examine properties related -layer networks. proposition characterizes bound large input dimensions. proposition consider -layer network widths input dimension recall expression right-hand side proposition equal maximal number regions single-layer network relus input dimension discussed section hence proposition implies large input dimensions two-layer network regions single-layer network number neurons formalized below. corollary consider -layer network widths input dimension moreover inequality strict next corollary illustrates bottleneck effect layers. states large input dimensions moving neuron second layer ﬁrst strictly increases bound. corollary consider -layer network widths input dimension assumption must large required proposition; otherwise input create bottleneck respect second layer decrease size. note bottleneck affects subsequent layers layer immediately remainder section consider deep networks equal widths next proposition viewed extension proposition multiple layers. states network widths input dimension least layers halve number layers redistribute neurons widths become bound increases. words assume bound close maximal number regions suggests making deep network shallower allows regions input dimension equal width. proposition consider l-layer network equal widths input dimension proof. since deriving upper bound assume bound nondecreasing ﬁrst assume even. relax constraints index theorem apply vandermonde’s identity pair particular corollary implies exceeds total size network liml→∞ words ratio maximal number regions deep network shallow network goes zero goes inﬁnity. obtained last tightening step proof theorem note replacing expression value smaller input dimension still yields valid lower bound. holds increasing input dimension network cannot decrease maximal number regions. choose theorem lower bound expressed n/n/ )ln/ ln/. implies maximal number regions subspace directly follows nrd. show converse apply orthogonal decomposition theorem linear algebra point expressed uniquely here thus means side hyperplane thus belong region. words given region lies therefore hence result follows. proof. illustrated figure partitioning viewed sequential process layer partition regions obtained previous layer. viewed input space region obtained layer potentially partitioned hyperplanes given rows bias hyperplanes fall outside interior partition region. process mind recursively bound number subregions within region. precisely construct recurrence upper bound maximal number regions obtained partitioning region dimension layers base case write recurrence grouping together regions activation size |sl| follows nnldjr here nnldj represents maximum number regions |sl| obtained partitioning space dimension hyperplanes. bound value next. proof theorem theorem consider deep rectiﬁer network layers rectiﬁed linear units layer input dimension maximal number regions neural network proof. section provides helpful insight construct example large number regions. tells want regions large dimension general. particular regions dimension zero cannot partitioned. suggests one-dimensional construction mont´ufar improved contains regions dimension region dimension zero. relus point direction depicted fig. leaving region empty activation pattern. construction essentially increases dimension region zero one. done shifting neurons forward ﬂipping direction third neuron illustrated fig. assume review intuition behind construction strategy mont´ufar construct linear function zigzag pattern composed relus. precisely relus. linear function absorbed preactivation function next layer. zigzag pattern allows replicate slope scaled copy function domain fig. shows example effect. essentially compose itself linear piece maps entire function interval piece backward manner. figure construction mont´ufar units point right leaving region dimension zero origin. construction described section. within interval regions instead four construction want relus create regions instead words want construct zigzag pattern slopes. order that take steps give freedom. first observe need linear piece zero zero; construction works independently length piece. therefore turn breakpoints parameters second sign bias parameters function parameters set. here max{ ˜wix ˜bi} since relu. deﬁne weights biases seek interval form zigzag pattern. parameters needed signs cannot arbitrary must match directions relus point towards. particular need positive slope want point right negative slope want point left. hence without loss generality need consider si’s since directly deﬁned signs wi’s directions. precisely otherwise otherwise. summarize parameters weights biases relu global bias breakpoints goal values parameters piece function domain linear zero zero. precisely domain want linear piece either deﬁne linear functions zero zero respectively. since want zigzag pattern former happen interval latter happen even. parameters relu corresponds hyperplane point dimension one. fact points breakpoints directions deﬁne inputs neuron activated. instance neuron points right neuron outputs zero linear function previously discussed construction neurons point right except third neuron points left. ensure region activated neuron instead zero would happen neurons pointed left. however although ensuring every region dimension necessary reach bound every directions yields valid weights. directions chosen admit valid weights. directions neurons tells neurons activated region. left right start activated activate move forward deactivate ﬁnally activate sequence. yields following system equations deﬁned simplicity left show exists solution system linear equations first note biases written terms note subtract express terms variables. remaining equations become triangular therefore given values ti’s back-substitute remaining bias variables. subtraction yields terms ti’s. however deﬁne terms variables must same values ti’s satisfying equation weights obtained back-substitution since eliminating yields triangular equations. particular following values valid remaining weights biases obtained described above completes desired construction. example construction four units depicted fig. breakpoints max{ max{ max{ finally proof. follow proof theorem except different -dimensional construction. main idea proof organize network independent networks input dimension apply -dimensional construction individual network. particular layer assign nl/n relus network ignoring l=nl/n since networks independent other number activation patterns compound network product number activation patterns networks. hence holds number regions. therefore number regions network proof. denote rank-k maxout unit layer similarly j-th biases layer case maxout activation pattern vector maps layer-l neurons activation neuron attains maximum among functions; case ties assume function lowest index considered activation. similarly relu case denote rnl×nl−×k rnl×nl− operator selects rows matrix i-th i-th neuron activation essentially applies maxout effect weight matrices given activation pattern. region linear function. boundaries regions composed pieces contained hyperplane. piece part boundary least regions conversely pair regions corresponds piece. extending pieces hyperplanes cannot decrease number regions. therefore consider maxout units single layer number regions arrangement hyperplanes. matrix rows given every distinct pair within neuron every neuron given region write weight matrix corresponding hyperplanes described above φsl− words hyperplanes extend boundary pieces within region given rows bias main difference maxout case relu case maxout operator guarantee reductions rank unlike relu operator show analogous lemma maxout case. however fully relax rank. lemma number regions induced neurons layer within certain region proof theorem possible value formulation theorem provided constraints neuron rectiﬁer network feasible solution ﬁxed value yields output neural network. sufﬁces prove constraints neuron input output neural network would. according since variables non-negative whereas non-positive similarly follows either respectively. case value systematic method count solutions one-tree approach resumes search optimal solution found using branch-and-bound tree. method also applied near-optimal solutions revisiting nodes pruned solving optimal solution. note constraints variables either activation boundary whereas want consider neuron active output strictly positive. discrepancy cause double-counting activation boundaries overlap. address deﬁning objective function maximizes minimum output active neuron positive non-degenerate cases. formulation follows practice value constant chosen small possible also implies choosing different values different places make formulation tighter stable numerically constraints sufﬁces choose large either given bounds input. hence respectively replace constraints involving variables. given lower upper bounds deﬁne subsequent bounds follows generally represent linear regions disjunctive program consist union polyhedra. disjunctive programs used integer programming literature generate cutting planes lift-and-project follows assume neuron either active inactive output lies activation hyperplane. proof. count regions need point linear region. since number linear regions ﬁnite sufﬁces large enough correctly single point region. conversely infeasible linear region either corresponds empty sets else polyhedron neither case would yield solution z-projection formulation generalizes relus small modiﬁcations. first computing output term constraint output neuron lower bounded term constraint finally binary variable term neuron denotes neuron active. constraint enforces variable neuron whereas constraint equates output neuron active term. constant chosen terms vary freely hence effectively disabling constraint corresponding binary variable zero. figure shows error training different conﬁgurations ﬁrst experiment. figure shows errors training different conﬁgurations second experiment. both observe relation accuracy order magnitude linear regions suggest linear regions represent reasonable proxy representational power dnns. figure contrast cross-entropy along training number regions identifying single digit ﬁrst experiment shows training error green; shows validation error purple. figure shows upper bound theorem maximized layers added number neurons increase. contrast figure shows smallest depth preserving growth better secondary although still exponential effect starts shrinks bound number layers large total number neurons. figure bounds theorem semilog scale total number neurons increase evenly distributing neurons layers actual values showing overall impact depth; ratio layers showing local impact particular depths.", "year": 2017}