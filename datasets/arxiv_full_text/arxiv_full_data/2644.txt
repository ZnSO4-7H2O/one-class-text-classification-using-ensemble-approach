{"title": "Sample and Computationally Efficient Learning Algorithms under S-Concave  Distributions", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We provide new results for noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown about the geometry of this class of distributions and their applications in the context of learning. The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of $s$-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and the disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passive learning of intersections of halfspaces. Our analysis of geometric properties of $s$-concave distributions might be of independent interest to optimization more broadly.", "text": "provide results noise-tolerant sample-efﬁcient learning algorithms s-concave distributions. class s-concave distributions broad natural generalization logconcavity includes many important additional distributions e.g. pareto distribution tdistribution. class studied context efﬁcient sampling integration optimization much remains unknown geometry class distributions applications context learning. challenge unlike commonly used distributions learning broader class closed marginalization operator many distributions fat-tailed. work introduce convex geometry tools study properties s-concave distributions properties provide bounds quantities interest learning including probability disagreement halfspaces disagreement outside band disagreement coefﬁcient. results signiﬁcantly generalize prior results margin-based active learning disagreement-based active learning passive learning intersections halfspaces. analysis geometric properties s-concave distributions might independent interest optimization broadly. developing provable learning algorithms central challenges learning theory. study algorithms signiﬁcant advances theory practice passive active learning. passive learning model learning algorithm access labeled examples sampled i.i.d. unknown distribution instance space labeled according underlying target function. active learning model however algorithm access unlabeled examples request labels choice goal learn target function signiﬁcantly fewer labels. work study learning models case underlying distribution belongs class s-concave distributions. prior work noise-tolerant sample-efﬁcient algorithms mostly relies assumption distribution instance space log-concave distribution log-concave logarithm density concave function. assumption log-concavity made purposes computational efﬁciency reasons sample efﬁciency reasons. computational efﬁciency reasons made obtain noise-tolerant algorithm even seemingly simple decision surfaces like linear separators. simple algorithms exist noiseless scenarios e.g. linear programming notoriously hard noise progress noise-tolerant algorithms focused uniform log-concave distributions concept spaces like intersections halfspaces even computationally efﬁcient algorithm noise-free settings works general distributions nice progress uniform log-concave distributions sample efﬁciency reasons context active learning need distributional assumptions order obtain label complexity improvements concrete general class prior work obtains improvements marginal distribution instance space satisﬁes log-concavity work provide broad generalization results showing extend s-concave distributions distribution density s-concave concave function. identify properties distributions allow simultaneously extend results. general important class s-concave distributions? class s-concave distributions broad contains many well-known distributions special cases. example s-concave distributions reduce log-concave distributions. furthermore s-concave class contains inﬁnitely many fat-tailed distributions belong class log-concave distributions e.g. cauchy pareto distributions widely applied context theoretical physics economics much remains unknown provable learning algorithms active learning halfspaces perform realistic distributions. also compare sconcave distributions nearly-log-concave distributions slightly broader class distributions log-concavity. distribution density nearly-log-concave e−.f class s-concave distributions includes many important extra distributions belong nearly-log-concave distributions nearly-logconcave distribution must sub-exponential tails tail probability s-concave distribution might decay much slower also note efﬁcient sampling integration optimization algorithms s-concave distributions well understood analysis s-concave distributions bridges algorithms strong guarantees noise-tolerant sample-efﬁcient learning algorithms. structural results. study various geometric properties s-concave distributions. properties serve structural results many provable learning algorithms e.g. margin-based active learning disagreement-based active learning learning intersections halfspaces etc. results exactly reduce log-concave distributions below state structural results informally theorem isotropic s-concave distribution exist closed-form functions marginal arguments might independent interest optimization broadly. margin based active learning apply structural results margin-based active learning halfspace isotropic s-concave distribution realizable adversarial noise models. realizable case instance drawn isotropic s-concave distribution label sign. adversarial noise model adversary corrupt fraction labels. cases show exists computationally efﬁcient algorithm outputs linear separator prx∼d sign] label complexity w.r.t. improves exponentially passive learning scenario s-concave distributions though underlying distribution might fat-tailed. best knowledge ﬁrst result concerning computationally-efﬁcient noise-tolerant margin-based active learning broader class s-concave distributions. work solves open problem proposed awasthi exploring wider classes distributions provable active learning algorithms. disagreement based active learning apply results agnostic disagreement-based active learning s-concave distributions. analysis estimating disagreement coefﬁcient distribution-dependent measure complexity used analyze certain types active learning algorithms e.g. algorithm work disagreement coefﬁcient isotropic s-concave distribution composing existing work active learning obtain bound label complexity class s-concave distributions. aware ﬁrst result concerning disagreement-based active learning goes beyond log-concave distributions. bounds disagreement coefﬁcient match best known results much less general case log-concave distributions furthermore apply s-concave case allow arbitrary number discontinuities case captured learning intersections halfspaces baum’s algorithm famous algorithms learning intersections halfspaces. algorithm ﬁrst proposed baum symmetric distribution later extended log-concave distribution klivans distributions almost symmetric. paper show approximate symmetry also holds case s-concave distributions. this work label complexity baum’s algorithm broader class s-concave distributions advance state-of-the-art results provide lower bounds partially show tightness analysis. results potentially applied provable learning algorithms well might independent interest. discuss techniques related papers supplementary material. marginalization results inspired isoperimetric inequality sconcave distributions work chandrasekaran roughly isoperimetry states sets well-separated area large measure relative measure sets results kind particularly useful margin-based active learning halfspace algorithm proceeds rounds aiming error half round band. since measure band large even dominates error whole space decreases almost half round resulting exponentially fast convergence rate. however order make analysis algorithms work s-concave distribution typically require reﬁned geometric properties isoperimetry isoperimetric inequality states nothing absolute measure band s-concave distributions. insight behind isoperimetry collection properties concerning geometry probability density. geometric properties classic paradigms log-concave distributions well-studied typically hard generalize results s-concave distribution broader range fact class s-concave functions closed marginalization marginal s-concave function s-concave more. directly restricts possibility applying prior proof techniques log-concave distribution sconcave one. furthermore previous proofs heavily depend assumption density light-tailed applicable probably fat-tailed s-concave distribution. mitigate concerns begin powerful tool convex geometry brascamp lieb result viewed extension celebrated pr´ekopa-leindler inequality integral inequality closely related number classical inequalities analysis serves building block isoperimetry log-concave distributions this show marginal s-concave function γ-concave closed-form related parameter dimension marginalization. analysis tight exists s-concave function γ-concave marginal. reduction baseline function general hard study high-dimensional s-concave distribution. instead build marginalization technique described reduce n-dimensional sconcave function one-dimensional case. thus sufﬁces investigate geometry one-dimensional γ-concave functions. still inﬁnitely many functions class. proofs take novel analysis reducing one-dimensional γ-concave density certain baseline function. baseline function meet goals represents worst case class γconcave functions namely functions achieve bounds geometric properties interest; function easy analyze e.g. closed-form moments integrations. note choosing baseline function boundary γ-concavity non-γ-concavity classes readily achieves goal achieve goal template function easy particular choice parameters functions many good properties exploit. first moments represented closed-form beta function. enables ﬁgure relations among moments various orders explicitly obtain recursive inequality critical deducing bounds one-dimensional geometric properties. second boundary γ-concave class concave function therefore enables analyze whole class s-concavity focusing below summarize high-level proof ideas brieﬂy. labeled examples error dimension hypothesis class algorithm works data distribution runs polynomial time consistent hypothesis found efﬁciently e.g. linear programming realizable case. algorithms perception winnow support vector machine provide better guarantees target vector norm. results form basis passive learning. explore possibility improving label complexity several algorithms later proposed active learning literature uniform distributions among disagreement-based active learning margin-based active learning typical approaches. disagreement-based active learning algorithm proceeds rounds requesting labels instances disagreement region among current candidate hypothesises. cohn provided ﬁrst disagreement-based active learning algorithm realizable case. balcan later extended algorithm agnostic setting estimating conﬁdence interval disagreement region. analysis technique generalized thanks hanneke introducing concept disagreement coefﬁcient measure complexity active learning problems serves important element bounding label complexity. however seminal work focused disagreement coefﬁcient uniform distribution. margin-based active learning another line research active learning literature. algorithm proceeds rounds requesting labels examples aggressively margin area around current guess hypothesis. balcan ﬁrst proposed algorithm margin-based active learning uniform distribution realizable case. also provided guarantees tsybakov noise model algorithm inefﬁcient. mitigate issue awasthi considered subclass tsybakov noise massart noise algorithm runs polynomial time sequence hinge loss minimizations labeled instances. however clear whether analysis works distributions instead uniform one. geometry log-concave distribution log-concave distribution class probability distributions logarithm density function concave common generalization uniform distribution convex bertsimas vempala kalai vempala noticed efﬁcient sampling integration optimization algorithms distribution class rely heavily good isoperimetry density functions. informally function good isoperimetry cannot remove small-measure domain partition domain disjoint large-measure sets. isoperimetry commonly believed characteristic good geometric properties. this lov´asz vempala proved isoperimetric inequality log-concave distribution provided bunch reﬁned geometric properties distribution class. going slightly beyond log-concave distribution caramanis mannor showed good isoperimetry nearly log-concave distributions reﬁned geometry provided there. active learning halfspace log-concave distribution natural connection geometry distribution connection ﬁrst introduced sufﬁcient success disagreement-based margin-based active learning log-concave distribution resolve computational issue awasthi studied probability disagreement outside margin log-concave distribution proposed efﬁcient algorithm challenging adversarial noise. recently awasthi provided stronger guarantees efﬁcient learning halfspace massart noise model log-concave distribution. s-concave distribution problem extending log-concave distribution broader provable learning algorithms received signiﬁcant attention recent years. although efforts devoted generalizing probability distribution e.g. nearly log-concave distribution analysis intrinsically built upon geometry log-concave distribution. moreover best knowledge efﬁcient noise-tolerant active learning algorithm goes beyond log-concave distribution. candidate extension class s-concave distributions many appealing properties exploit distribution class much broader log-concave distributions implies log-concavity; s-concave function mapping good isoperimetry efﬁcient sampling integration optimization algorithms available distribution class. properties inspire work. proceeding deﬁne notations clarify problem setup section. notations capital lower-case letters represent random variables represent sconcave distribution represent conditional distribution deﬁne sign function sign otherwise. denote tα−e−tdt gamma function. consider single norm vectors namely -norm denoted frequently represent measure probability distribution density function notation ball represents convenience symbol slightly differs ordinary addition gs}/s otherwise same. deﬁne angle deﬁnition function distribution measure). function s-concave s)/s probability distribution s-concave density function s-concave. probability measure s-concave µs]/s sets special classes s-concave functions include concavity harmonic-concavity quasi-concavity etc. conditions deﬁnition progressively weaker becomes smaller s-concave densities s-concave thus verify concave log-concave s-concave quasi-concave section develop geometric properties s-concave distribution. challenge unlike commonly used distributions learning broader class closed marginalization operator many distributions fat-tailed. address issue introduce several techniques. ﬁrst introduce extension prekopa-leindler inequality reduce high-dimensional problem one-dimensional case. reduce resulting one-dimensional s-concave function well-deﬁned baseline function explore geometric properties baseline function. begin analysis marginal distribution forms basis geometric properties s-concave distributions unlike log-concave distribution marginal remains log-concave class s-concave distributions closed marginalization operator. study marginal primary tool theory convex geometry. speciﬁcally extension pr´ekopa-leindler inequality developed brascamp lieb allows characterization integral s-concave functions. similar marginal s-concave distribution might remain class. sharp contrast log-concave distributions. following theorem studies s-concave distribution. theorem serve bridge connects high-dimensional s-concave distributions onedimensional γ-concave distributions. them able reduce high-dimensional problem one-dimensional one. tail probability distinct characteristics s-concave distributions compared logconcave distributions. shown log-concave distribution exponentially small tail tail s-concave distribution proved following theorem. theorem come isotropic distribution s-concave density. every investigate geometry s-concave distributions. ﬁrst consider one-dimensional s-concave distributions provide bounds density centroid-centered halfspaces range density function building upon these develop geometric properties high-dimensional s-concave distributions reducing distributions one-dimensional case based marginalization begin analysis one-dimensional halfspaces. bound probability normal technique bound centroid region tail region separately. however challenge s-concave distribution fat-tailed probability one-dimensional halfspace bounded absolute constant log-concave distributions probability s-concave distributions decays becomes smaller. following lemma captures intuition. high-dimensional case move high-dimensional case following assume though working range vanishes becomes larger almost broadest range hopefully achieve chandrasekaran showed lower bound require s-concave distribution good geometric properties. addition theorem marginal s-concave distribution might even exist; case happen certain s-concave distributions e.g. cauchy distribution. range almost tight factor. following theorem extension lemma high-dimensional spaces. proofs basically reduce n-dimensional density ﬁrst marginal theorem apply lemma bound image. theorem isotropic s-concave density. analyze problem learning linear separators interested studying disagreement hypothesis output hypothesis target. following theorem captures characteristic s-concave distributions. theorem assume isotropic s-concave distribution unit vectors prx∼d sign] absolute constant section show many algorithms work log-concave distributions behave well s-concave distributions applying above-mentioned geometric properties. simplicity frequently notations theorem ﬁrst investigate margin-based active learning isotropic s-concave distributions realizable adversarial noise models. algorithm follows localization technique proceeds rounds aiming error half round margin analysis requires reﬁned geometric properties below. theorem basically claims error mostly concentrates band theorem guarantees variance direction cannot large. defer detailed proofs supplementary material. theorem vectors assume isotropic s-concave distribution. absolute constant function exists function prx∼d sign −cfαβ beta function s/s) given lemma theorem assume isotropic s-concave. given theorem absolute ex∼dut] given lemma theorem show margin-based active learning works s-concave distributions realizable case. theorem realizable case isotropic s-concave distribution absolute constants algorithm runs labels k-th round outputs linear separator error probability least theorem algorithm margin-based active learning s-concave distributions works almost well log-concave distributions resizable case improving exponentially w.r.t. variable passive learning algorithms. marginal s-concave fraction labels ﬂipped adversarially. analysis builds upon induction technique round hinge loss minimization band loss half. algorithm previously analyzed special class log-concave distributions. paper analyze much general class s-concave distributions. theorem isotropic s-concave distribution label obey adversarial noise model. rate adversarial noise satisﬁes absolute constant absolute constant algorithm runs iterations outputs linear separator prx∼d sign] probability least label complexity k-th round min{θ. recall deﬁnition disagreement coefﬁcient w.r.t. classiﬁer precision distribution follows. deﬁne balld prx∼d deﬁne disagreement region s.t. alexander capacity capw∗d prd)) disagreement coefﬁcient deﬁned θw∗d supr≥\u0001]. below state results disagreement coefﬁcient isotropic s-concave distributions. theorem isotropic s-concave distribution disagreement coefﬁcient θw∗d particular θw∗d furthermore apply s-concave case allow arbitrary number discontinuities case captured result immediately implies concrete bounds label complexity disagreement-based active learning algorithms e.g. instance composing result obtain bound baum provided polynomial-time algorithm learning intersections halfspaces w.r.t. symmetric distributions. later klivans extended result showing algorithm works distribution long section show possible learn intersections halfspaces broader class s-concave distributions. theorem realizable case algorithm outputs hypothesis error probability least isotropic s-concave distributions. label complexity max{m/\u0001 log} theorem ﬁxed value have s-concave distribution whose covariance matrix full rank sample complexity learning origin-centered linear separators passive learning scenario /\u0001); label complexity active learning linear separators s-concave distributions /\u0001)). covariance matrix full rank intrinsic dimension less lower bounds essentially apply s-concave distributions. according theorem possible exponential improvement label complexity w.r.t. passive learning active sampling even though underlying distribution fat-tailed s-concave distribution. observation captured theorems paper study geometric properties s-concave distributions. work advances stateof-the-art results margin-based active learning disagreement-based active learning learning intersections halfspaces w.r.t. distributions instance space. results reduce best-known results log-concave distributions. geometric properties s-concave distributions potentially applied learning algorithms might independent interest broadly. acknowledgements. work supported part grants nsf-ccf ccf- ccf- sloan fellowship microsoft research fellowship.", "year": 2017}