{"title": "Large Margin Boltzmann Machines and Large Margin Sigmoid Belief Networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Current statistical models for structured prediction make simplifying assumptions about the underlying output graph structure, such as assuming a low-order Markov chain, because exact inference becomes intractable as the tree-width of the underlying graph increases. Approximate inference algorithms, on the other hand, force one to trade off representational power with computational efficiency. In this paper, we propose two new types of probabilistic graphical models, large margin Boltzmann machines (LMBMs) and large margin sigmoid belief networks (LMSBNs), for structured prediction. LMSBNs in particular allow a very fast inference algorithm for arbitrary graph structures that runs in polynomial time with a high probability. This probability is data-distribution dependent and is maximized in learning. The new approach overcomes the representation-efficiency trade-off in previous models and allows fast structured prediction with complicated graph structures. We present results from applying a fully connected model to multi-label scene classification and demonstrate that the proposed approach can yield significant performance gains over current state-of-the-art methods.", "text": "structured prediction important machine learning problem occurs many different ﬁelds e.g. natural language processing protein structure prediction semantic image annotation. goal learn function maps input vector output vector representing labels whose components take value traditional approach multi-label classiﬁcation problems train binary classiﬁers independently. structured prediction hand also considers relationships among output variables example image annotation problem entire image parts image annotated labels representing object scene event involving multiple objects labels usually dependent other e.g. buildings beaches occur truck type automotive sunsets likely co-occur beaches trees relations capture semantics among labels play important role human cognition. major advantage structured prediction structured representation output much compact unstructured classiﬁer resulting smaller sample complexity greater generalization extending traditional classiﬁcation techniques structured prediction difﬁcult potentially complicated inter-dependencies exist among output variables. problem modeled probabilistic graphical model well-known exact inference general graph np-hard. therefore practical approaches make simplifying assumptions dependencies among output variables order simplify graph structure maintain tractability. examples include maximum entropy markov models large margin sigmoid belief networks structured prediction. lmsbns particular allow fast inference algorithm arbitrary graph structures runs polynomial time high probability. probability data-distribution dependent maximized learning. approach overcomes representationefﬁciency trade-off previous models allows fast structured prediction complicated graph structures. present results applying fully connected model multi-label scene classiﬁcation demonstrate proposed approach yield signiﬁcant performance gains current state-of-the-art methods. conditional random ﬁelds max-margin markov networks structured support vector machines approaches typically restrict tree-width graph viterbi algorithm junction tree algorithm still efﬁcient. hand much research fast approximate inference complicated graphs based e.g. markov chain monte carlo variational inference combinations methods. general mcmc slow particularly graphs strongly coupled variables. good heuristics developed speed mcmc highly dependent graph structure associated parameters variational inference another popular approach complicated distribution approximated simpler distribution trade accuracy speed. example variables assumed independent obtains mean ﬁeld algorithm. bethe energy formulation yields loopy belief propagation algorithm combination trees considered obtains treereweighted sum-product algorithm also relax higher-order marginal constraints obtain linear programming algorithm lesser dependency constraints less accurate inference algorithms become faster speed. however sacriﬁced accuracy inference could detrimental learning. example mean ﬁeld produce highly biased estimates loopy belief propagation might even cause learning algorithm diverge necessary accurately precisely represent semantic knowledge. unfortunately approaches discussed operate assumption cannot avoid traderepresentational power computational efﬁciency. paper propose large margin sigmoid belief networks large margin boltzmann machines models structured prediction. provide theoretical analysis tool derive generalization bounds them. importantly lmsbns allow fast inference arbitrarily complicated graph structures. inference based branch-and-bound technique depend dependency structure referred clique. sbns features deﬁned product variables clique. example {vvv} order clique vvv. edges order cliques e.g. {vv} ﬁrst order cliques variable themselves e.g. variables take values feature function also known parity function function. therefore softly encodes boolean function and-of-xor expansion provides ﬂexible encode human expert knowledge model. without ambiguity simplify representation summation taken cliques include variable sbns require variables clique must parents requirement insures underlying graph acyclic used output variable i.e. conditional likelihoods sbns become same i.e. features well known logistic regression loss function log. fact considered product according topological order graph. overall loss function log. needs normalization loss function usually factorized locally puts challenge learning. facilitate derivation fast inference algorithm lmsbns fast learning algorithm lmbms hinge loss approximate log-loss log. call resulting large margin sigmoid belief network resulting large margin boltzmann machine approximations presented remark approximation lmbm similar pseudo likelihood approximation markov random ﬁeld. difference extra regularization. latter section show regularizer crucial lmbms generalize well. note lmsbns feature appears lmbms feature appears section applies lmsbns lmbms semantic image annotation problem using fully-connected graph structure. empirically study performance inference algorithm illustrate efﬁciency effectiveness. present results experiments benchmark dataset demonstrate lmsbns outperform current state-of-the-art methods image annotation based kernels threshold-tuning. deﬁnition sigmoid belief network directed acyclic graph random variables size directed edges. represents edge node parents {vj| joint likelihood edges undirected feature appears sbns edges directed feature appears either both. generalize function utilize high order features variables. probabilistic graphical models major concern structured prediction well classiﬁcation problems generalization performance. generalization performance structured prediction well studied binary multi-class classiﬁcation taskar tsochantaridis employed maximum-margin approach builds binary support vector machines generalization performance addressed upper bound prediction errors. however derivation bound speciﬁcally restricted loss function hard apply loss functions. daum´e consider sequential decision approach solves structured prediction problem making decisions time. sequential decisions made multiple times output obtained averaging results. generalization bound analyzed terms binary classiﬁcation losses. major drawback approach averaged losses averaged classiﬁers need large number iterations converge. even converges bound still loose compared bound presented. discuss section section provide general analysis tool single variable classiﬁcation structured prediction allows arbitrary loss functions holds tight. ﬁrst need following threshold theorem proof figure easy verify leads ﬁrst upper bound sbn. features involves multiple variables appear corresponding makes upper bounding much harder. prove second upper bound follows since hinge loss also contains partition function marginalizes relax summation term proportional norm weights whose corresponding cliques include relaxation represented constant determined whole partition function relaxed upper bound contains regularizer weights whose corresponding cliques include least output cliques output values predicted minimizing loss function shown equation below. norm regularization weights training problem lmsbns deﬁned equation below. note that lmbms extra regularization weights among proof pick ﬁrst topological order equal optimal value i.e. since takes values easy verify −ˆzi. otherwise assume data drawn unknown distribution since unknown minimize empirical risk rather expected risk. fast convergence rate empirical objective expected proved single output variable case. extend general structured output case providing structured rademacher complexity bound shown lemma threshold theorem allows discuss prediction error bounds number outputs loss function. example logistic regression whenever mistake made threshold adaboost whenever makes mistake ssvms −z)]+ makes according theorem goal classiﬁcation tasks hypothesis predict expected loss possible. hand lmsbns fast inference algorithm whose performance directly dependents quantity. smaller expected loss faster inference. reasons log-loss exponential-loss unfavorable usually larger zero even model data well. therefore choose hinge loss loss function lmsbns lmbms. threshold lmbms given remark threshold lmsbns given remark tight bound threshold large enough lmbms need constrain weights among output variables. words coupling outputs stronger coupling output input possibility overﬁtting increases. also explains approximate loss lmbms contains regularizations coupling weights among output variables. however lmsbns threshold always generally speaking lmsbns expected generalize better lmbms. section propose simple efﬁcient inference algorithm solve prediction problem equation lmsbns. according topological order graph branch compute parents ﬁrst value makes i.e. left branch algorithm right branch opposite value search keep upper bound initialized parameter whenever current objective higher upper bound backtrack previous variable. search terminates before states visited. following theorem shows high probability algorithm computes optimal values polynomial time basic idea structured rademacher complexity bound whole functional space combination rademacher complexity subspaces. lmbms shared subspaces overlap other overall rademacher complexity counts features multiple times counts once. therefore generalization bound loosened maximum clique size. complicated graph larger lmsbns feature appears subspace always hence bound lmsbns tighter lmbms. furthermore bound given better pac-bayes bound ssvms affected inference algorithm. ssvms cheap exact inference algorithm available pac-bayes bound becomes worse extra degrees freedom introduced relaxations leading potentially poorer generalization performance. lmsbns learning problem deﬁned equation decomposed independent optimization problems. solved efﬁciently modern fast solvers dual coordinate descent algorithm primal stochastic gradient descent algorithm exponentiated gradient descent algorithm lmbms weights shared multiple optimize whole objective simultaneously. similar give dual coordinate descent based optimization algorithms lmbms. proof search branch right hinge loss greater given true objective optimal objective well optimal path contains right branches. since algorithm always searches left branch ﬁrst optimal path reached searched. according markov inequality algorithm adjusts search tree according model weights. training optimal paths condensed energy side i.e. left side search tree high probability. probability directly related expected loss respect given data distribution. therefore label algorithm data-dependent inference algorithm. popular inference algorithms exact approximate inference depend graph complexity complicated graph slower inference. trade-off diminishes applicability algorithms presents researchers difﬁcult problem selecting graph structure balances accuracy efﬁciency. algorithm lmsbns circumvents trade-off allows arbitrary complicated graphs without sacriﬁcing computational efﬁciency. fact particular complicated graph yields smaller expected loss algorithm turn runs even faster. algorithm speciﬁcally designed lmsbns directed graphical model. undirected models algorithm guarantee polynomial time complexity high probability. indeed observe exponential time complexity applied lmbms. undirected models including ssvms lmbms implement convex relaxation-based linear programming note although lmbms don’t fast inference algorithm unlike ssvms learning affected inference algorithm. experiments section show lmbms outperforms ssvms. algorithm differs search-based decoding algorithms e.g. beam search best ﬁrst search several aspects. first search algorithms typically prune supports maximum cliques grow exponentially. hand pruning lead misclassiﬁcation quickly backtracking implemented. hand number remaining states might still large inference still slow. furthermore even backtracking procedure implemented unlike algorithm lmsbns still guaranteed heuristics prune states efﬁciently correctly. demonstrate efﬁciency data dependency property algorithms test data rcvv trained model random untrained model. running times collected varying number output variables. time measured .ghz pentium desktop computer. upper graph figure demonstrates algorithm performs several orders magnitude faster experiment large value solution guaranteed optimal solution. running time respect number output variables vary trained model random untrained model running time changes signiﬁcantly. random untrained model algorithm demonstrates exponential time complexity respect number output variables. however after training running time algorithm scales much slowly. observation underscores data distribution dependent property i.e. better model data faster performs. illustrate property further second experiment. experiment probability algorithm reaching optimal values well-known np-hard problems many instances solved efﬁciently. area speedup learning focuses learning good heuristics speedup problem solvers. approach presented regarded novel method speedup learning maximum margin markov networks maximum margin bayesian networks conditional graphical models adopt min-max formulation shown below embedded maximization operation potentially induces exponential number constraints. exponential number constraints makes optimization intractable. mbns local normalization constraints makes problem even harder. ssvms utilize cutting plane algorithm select small constraints. directly treat dual variables decomposable pseudo-marginals. undirected graph tree-width ssvms computationally efﬁcient generalize well. however high tree-width approximate inference used computational complexity sample complexity increase signiﬁcantly cgms decompose single hinge loss summation several hinge losses corresponding feature function exponential number combinations greatly reduced. decomposition hinge loss multiple hinge loss similar lmbms lmsbns. however cgms decompose feature function. real problems every feature function could compatible data leads large trivial upper bound. therefore performance guaranteed. large margin estimation threshold theorem generalizes maximum margin estimation approach. long loss function satisﬁes threshold theorem margin function implicitly deﬁned minimizing expected loss maximizes margins. traditional log-loss based models e.g. crfs memms discussed large margin estimation framework thresholds possibly small upper bounds become trivial. suggests large margin estimated models could generalize better maximum likelihood estimated models. problems like semantic annotation low-treewidth graph usually insufﬁcient represent knowledge relationships among labels. example figure illustrates motivation high-treewidth graph. models discussed lack fast accurate inferfig. upper graph. running time comparisons algorithms test dataset rcv-v. dashed lines standard deviation mean. time axis log-scaled. lower graph. accuracy data distribution dependency. training data accurate faster prediction. corresponding estimated theoretical lower bounds plotted blue. plotted varying cutoff threshold according theorem reﬂects running time overhead algorithm. compare curve several models namely random model models trained training instances respectively. lower graph figure shows signiﬁcant improvement trained models random untrained model. moreover training instances test instances predicted exactly quickly. ﬁgure also plot corresponding theoretical lower bounds estimated testing dataset lower graph figure veriﬁes theorem empirically. fast accurate inference algorithm lmsbns start complicated graph structure i.e. fully connected model. linear form generalized high order features. moreover kernel trick applied augment modeling power. thing needs concern minimize expected loss much possible small expected loss guarantees high prediction accuracy also fast inference. speed inference high-treewidth graphical models mixture models represent probabilities. example mop-memms extend memms address long-range dependencies represent conditional probability mixture model. wainwright uses mixture trees approximate markov random ﬁelds. demonstrate performance gains still improve inference speed restricting number mixtures. uses arithmetic circuits represent bayesian networks. inference linear circuit size. long circuit size inference fast. learning optimal np-hard problem. similarly improve inference speed penalizing circuit size takes different approach probabilistic graphical models handle high tree-width graphs. solves structured prediction making decisions sequentially. later classiﬁer take earlier decisions inputs similar lmsbns. fact inference considered initial decision algorithm. expected errors caused naive inference could high. searn implements averaging approach reduce expected errors. trains sequential classiﬁers iteration outputs prediction averaging decisions made iterations. earlier decisions later classiﬁers later classiﬁers possibly make fewer mistakes. averaging iterations expected loss reduced thereafter. roughly speaking prediction errors bounded averaged expected loss multiplied compared bounds lmbms lmsbns prediction errors bounded minimum expected loss divided threshold generalization bound searn rather loose. furthermore according needs large number iterations reach bound slows inference. therefore still limit number iterations faster inference might sacriﬁce prediction accuracy. unlike approaches lmsbns possess interesting property constraints modeling power. smaller expected loss faster inference. usually obtains smaller expected loss using complicated graph. property leads performance lmsbns tested scene annotation problem based scene dataset dataset contains training instances test instances. image represented dimensional color proﬁle feature vector output combination possible scene classes methods binary classiﬁers ssvms threshold selected binary classiﬁers train classiﬁer label predict independently. ssvms follow implement fully connected undirected model binary features. implement convex relaxation-based linear programming algorithm inference since convex relaxation-based approximate inference algorithm shown outperform approximate inference algorithms loopy belief propagation graph cuts tsbcs iteratively tune optimal decision threshold classiﬁer increase overall performance respect certain measure e.g. exact match ratio f-scores. many labels multilabel datasets highly unbalanced leading classiﬁers biased. tsbcs effectively adjust classiﬁer’s precision recall achieve state-of-the-art performance. comparisons borrow best results directly. implemented linear kernelized three lmsbns lmsbnlo trained default order i.e. ascending along label indices; lmsbnlf trained order selected according f-scores sort variables according f-scores higher f-score smaller index order; lmsbnkf kernelized model order lmsbnlf. also implemented ssvms ssvmhmm trained using ﬁrst-order markov chain. different ssvmhmm package consider inputs inference algorithm ssvmhmm viterbi algorithm; ssvmfull trained using fully connected graph. consider three categories performance measures. ﬁrst consists instance-based measures includes exact match ratio instance-based f-score second consists labelbased measures includes hamming loss macro-f score curate semantic modeling labeling images. experimental results demonstrate approach outperforms current state-of-the-art approaches. future research focus applying framework annotating parts images spatial relationships enhancing representational power model introducing hidden variables.", "year": 2010}