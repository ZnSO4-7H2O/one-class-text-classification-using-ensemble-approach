{"title": "Exploiting Sentence and Context Representations in Deep Neural Models  for Spoken Language Understanding", "tag": ["cs.AI", "cs.CL", "cs.NE"], "abstract": "This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).", "text": "paper presents deep learning architecture semantic decoder component statistical spoken dialogue system. slot-ﬁlling dialogue semantic decoder predicts dialogue slot-value pairs n-best hypotheses returned automatic speech recognition. current models spoken language understanding assume word-aligned semantic annotations sequence taggers delexicalisation mapping input words domain-speciﬁc concepts using heuristics capture morphological variation scale domains language variation work semantic decoder trained using unaligned semantic annotations uses distributed semantic representation learning overcome limitations explicit delexicalisation. proposed architecture uses convolutional neural network sentence representation long-short term memory network context representation. results presented publicly available dstc corpus in-car corpus similar dstc signiﬁcantly higher word error rate existing work spoken language understanding semantic decoding usually seen sequence tagging problem models trained tested datasets word-level annotations spoken language understanding unaligned data utterances annotated abstract semantics faces additional challenge knowing speciﬁc words relevant extracting semantics. problem tackled using conditional random ﬁelds driven ﬁnely-tuned hand-crafted features. discriminative approaches deal unaligned data form delexicalisation mapping input known ontological concepts main disadvantage delexicalisation difﬁculty scaling larger complex dialogue domains also handle many forms language variation. propose paper semantic decoder learns unaligned data exploits rich semantic distributed word representations instead delexicalisation. semantic decoder predicts dialogue slot-value pairs n-best hypotheses returned automatic speech recognition prediction made steps. first deep learning architecture used joint prediction dialogue acts presence absence slots. second architecture reused predicting values slots detected ﬁrst jointclassiﬁer. deep architecture combines sentence context representations. convolutional neural network used generate sentence representation longshort term memory network used generate context representation. non-linear function combines layers neural networks distinct softmax layers used predict dialogue slots ﬁrst joint model. second model single softmax predicts possible values slot. models evaluated datasets dstc in-car using accuracy f-measure item cross entropy score show models outperform previous proposed models without using manually designed features without preprocessing input exploiting distributed word representations claim allows semantic decoders built easily scale larger complex dialogue domains. remainder paper structured follows. ﬁrst present related work section describe architecture section describe experimental setup evaluation results introduced section finally present conclusions future work section sequence tagging discriminative models crfs sequence neural networks widely explored spoken language understanding. instance recurrent neural networks proposed generative deep neural networks consisting composition restricted boltzmann machines studied combination neural networks triangular crfs presented convolutional neural network used extracting input features triangular order perform joint intent detection slot ﬁlling. models word-level semantic annotations. however providing word-level semantic annotations costly since requires specialised annotators. proposed learning crfs unaligned data however manually tuned lexical syntactic features. work avoid need word-level annotation exploiting distributed word embeddings using deep learning feature representation. convolutional neural networks used previously sentiment analysis work explore similar presented generating sentence representation. however unlike input single well formed sentence ill-formed hypotheses. additionally softmax layer used binary classiﬁcation replaced softmax layer multiclass dialogue prediction softmax layer added distinct slot domain. proposed generating intent embeddings uses tri-letter input vectors. instead paper models initialised glove word embeddings glove embeddings trained unsupervised fashion large amount data model contextual similarity correlation words. chen he’s model aims learn embeddings utterances intents utterances similar intents close continuous space. although share spirit sentence embeddings intent approaches adaptive proposed however focused domain adaptation existing component. moreover classical discriminative models crfs svms require manually designed features. contrast focus paper exploit deep learning models learn feature representations automatically. recently researchers focused mapping word level hypotheses directly beliefs without using explicit semantic decoder step systems track user’s goal course dialogue maintaining distribution slot-value pairs. systems interesting clear scaled large domains constraint delexicalisation. furthermore still require explicit semantic decoding layer domain identiﬁcation general topic management. split task semantic decoding steps training joint model predicting dialogue presence absence slots predicting values probable slots detected shown figure deep learning architecture steps combining sentence context representations generate ﬁnal hidden unit feeds many softmax layers. ﬁrst step shown figure distinct softmax layers joint optimisation dialogue possible slot. second step single softmax layer predicts value speciﬁc slot. following explain architecture detail. sentence representation used generating hypothesis representation representations weighted conﬁdence scores summed obtain sentence representation variant inputs word vectors hypothesis. k−dimensional word embedding i-th word hypothesis. hypothesis length represented operation applied window words produce feature. rn−l+. pooling operation applied give maximum value max{c} representative feature ﬁlter. multiple ﬁlters applied varying window size obtain several adjacent features given hypothesis. features hypothesis summed hypotheses generate multiplied conﬁdence score representation sentence shown figure figure sentence representation applying convolution operations n-best list hypotheses resulting hidden layers weighted conﬁdence scores summed. context representation lstm used tracking context implied previous dialogue system actions. layer lstm network provides context representation decoding current input utterance. lstm sequence model utilises memory cell capable preserving states long periods time. cell recurrently connected three multiplication units input gate forget gate output gate. gating vectors cell makes selective decisions information preserved allow access units gates open close. lstm transition equations follows shown figure system actions encoded form system dialogue plus slot-value pairs. track history system actions slots values treated words input formed corresponding word vectors. length context vary. consider system actions previous current user utterance window previous system actions. instance currently processing last user input figure total number system actions consider previous system actions last system actions index output neuron representing class. dialogue classiﬁcation possible values inform request offer etc. slot prediction either absent present. slot-value prediction correspond possible values slot. instance slot price-range possible values cheap moderate expensive dontcare. result prediction probable class corpora experimental evaluation used similar datasets dstc in-car corpora collected using spoken dialogue system provides restaurant information system city cambridge. users specify restaurant suggestions area pricerange food type query system additional restaurant speciﬁc information phone number post code address. ﬁrst dialogue corpus released dialogue state tracking challenge semantic annotations also provided trainset dialogues turns total testset dialogues turns total. second corpus contains dialogues collected various noisy in-car conditions. stationary conditioning moving simulator trainset dialogues turns total testset dialogues turns total. noise average word error rate signiﬁcantly higher dstc hyperparameters training dropout used penultimate layers lstm networks prevent coadaptation hidden units randomly dropping proportion hidden units forward propagation models implemented theano used ﬁlter windows feature maps cnn. dropout rate batch size employed trainset used validation early stopping adopted. training done stochastic gradient descent shufﬂed mini-batches adadelta update rule initialise models glove word vectors used dimension system-action word-embeddings tuned training instead hypothesis word-embeddings heavy computations. experiments step joint classiﬁcation dialogue-acts slots evaluated different model conﬁgurations joint classiﬁcation dialogue-acts presence absence slots. cnn+lstm study inﬂuence context considering previous system actions study different context length using context window previous system actions namely cnn+lstm cnn+lstm cnn+lstm respectively. lstm finally study impact long distance dependencies using mainly lstm model previous system actions input inject sentence representation last lstm input. step classiﬁcation slot value pairs select best model step predicting presence slots slot present predict value using best architecture previous step. addition used score hypotheses reference semantics measure overall quality distribution returned models. number utterances number available semantic items. given ...w step joint classiﬁcation dialogue-acts slots step classiﬁers must predict jointly dialogue acts slots dstc dataset well dialogue acts slots in-car dataset. evaluate using fold cross-validation trainsets corpora’ testsets. table shows fold cross-validation results corpora. results suggest dtsc context representation signiﬁcantly impacting prediction. although model window cnn+lstm improves slightly accuracy f-score. in-car dataset however including context help disambiguate semantic predictions illformed hypotheses. expected since data much higher error rate hence higher levels confusion output. although signiﬁcant difference f-score using immediate previous system longer context cnn+lstm gives better accuracy lower score dataset. table shows results test sets. consequently evaluating dstc test window performs slightly better window sizes better simple model. in-car testset context window outperforms settings cnn+lstm. however test using sentence representation last input lstm context neural network improves f-score reduces error. step prediction slot value pairs evaluating step selected best model obtained -fold cross-validation experiments terms score. corpora cnn+lstm conﬁguration. dstc th-fold crossvalidation in-car th-fold crossvalidation used models classify whether given slot appears given hypothesis not. slot train another cnn+lstm classiﬁer predicting values. in-car corpus slot type possible value restaurant. similarly slot task value ﬁnd. slots value report values using model step since enough detect slot utterance. given domain speciﬁc delexicalisation models achieve good level performance overall note slot food possible values dstc in-car. hence slot much higher cardinality slots. overall performance baseline assessing overall performance provided model presented vector representation obtained summing frequency n-grams extracted -best hypotheses weighted conﬁdence scores. compare performance henderson’s model without context features namely wngrams+ctxt wngrams repectively. henderson reported results in-car dataset. similar model namely evaluated dstc implementations consist many binary classiﬁers dialogue slot-value pairs. terms score model cnn+lstm outperforms baselines terms score model signiﬁcantly outperforms wngrams baselines. however slightly worse wngrams+ctxt enhanced context features in-car. remember however model uses word-embeddings automatically generating sentence context representations without manually designed features using explicit application speciﬁc semantic dictionaries. paper presented deep learning architecture semantic decoding spoken dialogue systems exploits semantically rich distributed word vectors. compared different models combining sentence context representations. found context representations signiﬁcantly impact slot f-measure hypotheses generated noisy conditions. combination sentence context representations context window words outperforms baselines terms score. terms scores model outperforms baseline dstc corpus baseline without manually designed features in-car corpus. although f-score model outperforms baseline enriched context features in-car corpus proposed model remains competitive especially considering model requires manually designed features application speciﬁc semantic dictionaries. semantic distributed vector representations used detecting similarity domains. future work want study adoption sentence contex representations generated step within topic management multidomain dialogue systems. topic manager charge detecting domain intention behind users’ utterances. furthermore would interesting study embeddings domain adaptation potentially open-domains. research partly funded ep-src grant ep/m/ open domain statistical spoken dialogue systems. data used paper produced industry funded project available public use. references fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. nips workshop deep learning unsupervised feature learning. yun-nung chen xiaodong learning bidirectional intent embeddings convolutional deep structured semantic models spoken language understanding. extended abstract annual conference neural information processing systems–machine learning spoken language understanding interactions workshop ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research emmanuel ferreira bassam jabaian fabrice lef`evre. online adaptative zero-shot learning spoken language understanding using word-embedding. ieee international conference acoustics speech signal processing pages ieee. matthew henderson milica gaˇsi´c blaise thomson pirros tsiakoulis steve young. discriminative spoken language understanding using word confusion networks. spoken language technology workshop ieee. gr´egoire mesnil yann dauphin kaisheng yoshua bengio deng dilek hakkani-tur xiaodong larry heck gokhan dong using recurrent neural networks slot ﬁlling spoken language understanding. ieee/acm transactions audio speech language processing sarikaya hinton ramabhadran. deep belief nets natural language call-routing. ieee international conference acoustics speech signal processing pages may. blaise thomson milica gasic simon keizer francois mairesse jost schatzmann steve young. evaluating semantic-level conﬁdence scores multiple hypotheses. interspeech pages pirros tsiakoulis milica gaˇsic matthew henderson joaquin planells-lerma jorge prombonas blaise thomson steve young tzirkel. statistical methods building robust spoken dialogue systems automobile. proceedings applied human factors ergonomics.", "year": 2016}