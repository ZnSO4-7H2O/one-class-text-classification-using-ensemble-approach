{"title": "When Ignorance is Bliss", "tag": ["cs.AI", "cs.LG", "I.2.4"], "abstract": "It is commonly-accepted wisdom that more information is better, and that information should never be ignored. Here we argue, using both a Bayesian and a non-Bayesian analysis, that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures. These include situations in which the information is relevant for the prediction task at hand. In the non-Bayesian analysis, we show how ignoring information avoids dilation, the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty. In the Bayesian analysis, we show that for small sample sizes and certain prediction tasks, the Bayesian posterior based on a noninformative prior yields worse predictions than simply ignoring the given information.", "text": "commonly-accepted wisdom information better information never ignored. argue using bayesian non-bayesian analysis situations better ignoring information uncertainty represented probability measures. include situations information relevant prediction task hand. non-bayesian analysis show ignoring information avoids dilation phenomenon additional pieces information sometimes lead increase uncertainty. bayesian analysis show small sample sizes certain prediction tasks bayesian posterior based noninformative prior yields worse predictions simply ignoring given information. commonly-accepted wisdom information better information never ignored. indeed formalized number ways bayesian framework uncertainty represented probability measure paper argue occasionally better ignoring information uncertainty represented probability measures. related observations made seidenfeld compare work section deﬁniteness focus relatively simple setting. random variable taking values random variable taking values goal agent choose action whose utility depends value observed value assume that making observation agent prior value agent actually prior joint distribution obvious thing would condition observation best estimate value interested situations agent single prior joint distributions family priors following example shows situation arises often. example consider doctor trying decide patient tuberculosis. doctor learns patient’s address. doctor knows patient’s address correlated disease know correlation all. case random variable disease patient tuberculosis} neighborhood agent lives. doctor trying choose treatment. effect treatment depends value circumstances many doctors would simply take patient’s address account thereby ignoring relevant information. paper show commonly-adopted strategy often quite sensible. relatively obvious sense ignoring information right thing joint distributions whose marginal represents distributions compatible agent’s knowledge. roughly speaking best action given prior gives payoff joint distribution show every action worse joint distribution therefore ignoring information leads adopt minimax optimal decision. idea formalized proposition section also show ignoring information compares favorably obvious updating measures proposition makes three important assumptions remainder paper investigate effect dropping assumptions. section consider happens assume probability distribution probabilities. obvious question use. distinguish purely subjective bayesian approaches so-called noninformative pragmatic objective bayesian approaches based adopting so-called non-informative priors. show large class priors including uniform distribution jeffreys’ prior using bayesian posterior lead worse decisions using prior better ignoring information rather conditioning noninformative prior; examples examples posterior based relatively small sample. course sample grows larger using reasonable prior result posterior converges true distribution. follows directly standard bayesian consistency theorems section investigate effect dropping second third assumptions. show partial information relationship right thing becomes sensitive kind bookie adversary agent viewed playing consider related work particularly seidenfeld section focus paper optimality minimax sense. clear appropriate notion optimality. indeed seidenfeld explicitly argues analysis section suggests situations ignoring information reasonable thing even though minimax approach. discuss alternative notions optimality section conclude discussion section section formalize problem non-bayesian setting. show that setting pragmatic assumptions ignoring information sensible strategy. also show ignoring information compares favorably standard approach working sets measures said interested agent must choose action loss action depends value random variable takes values assume action value associated loss agent. loss function. ease exposition assume paper ﬁnite. suppose agent observes value variable takes values assume that although agent knows marginal distribution know depends agent’s uncertainty characterized consisting distributions marginal distribution agent must choose decision rule determines function observations. allow decision rules randomized. thus consists probability distributions decision rule function chooses distribution actions based observations. decision rules. special case deterministic decision rule assigns probability particular action. deterministic sometimes abuse notation write action assigned probability distribution given decision rule loss function random variable pa∈a stands probability performing action according distribution actions adopted observed. note special case deterministic decision rule moreover decision rule always chooses following result whose proof leave full paper shows decision rule always chooses optimal action independent observation optimal minimax sense. note worst-case expected loss decision-rule suppr∈p epr. thus best worst-case loss decision rules suppr∈p epr. standard decision rule uncertainty represented probability measures maxmin expected utility rule compute expected utility action respect probability measures choose action whose worst-case expected utility best proposition says consists probability measures marginal loss depends value action least worst-case loss optimal action respect example consider perhaps simplest case suppose agent knows epry ﬁxed before distributions marginal suppose actions loss function right value predicted otherwise; so-called classiﬁcation loss. easy optimal choose loss optimal min. perhaps standard approach dealing uncertainty case work whole distributions. assume contain distribution words distributions observing causes information lost. remarkably holds matter value observed. thus even though agent knew observing observing agent idea probability special case phenomenon called dilation statistical imprecise probability literature possible lower probabilities strictly decrease upper probabilities strictly increase matter value observed. dilation severe consequences decisionmaking. minimax-optimal decision rule respect randomize choosing probability note that matter actually obtains viewed example decision theorists called time inconsistency. suppose deﬁniteness then priori optimal strategy decide matter what. hand either observed optimal action randomize. uncertainty described single probability distribution time inconsistency cannot occur. suppose that instead probability measures agent probability measure probability measure take? broadly speaking possibilities here. consider either purely subjective bayesian agents pragmatic bayesian agents. purely subjective bayesian agent come prior expresses subjective beliefs situation. makes sense assess consequences ignoring information terms expected loss expectation taken respect agent’s subjective prior. good’s total evidence theorem classical result bayesian decision theory states that taking expectation respect agent’s prior optimal decision always based conditioning available information—information never ignored. contrast consider agent adopts bayesian updating pragmatic reasons rather fundamental reasons. case because computation time limited and/or prior knowledge hard obtain formulate prior adopted typically easily computable noninformative prior uniform natural parameterization suspect many statisticians pragmatic bayesians sense. analyzing pragmatic approach longer makes much sense compare ignoring information bayesian updating information looking expected loss respect adopted prior. reason prior longer expected correctly reﬂect agent’s degrees belief. seems meaningful pick single probability measure analyze behavior bayesian under assumption true state nature. varying sense behavthus observing data bayesian ignores value thus makes minimax-optimal decisions. potentially suboptimal behavior bayesian occur bayesian observed data. analyze case need assume sequence observations trying predict value given value xn+. distribution prαβ extended distribution assuming observations independent. course hope observations help learn allowing make better decisions take simplest case suppose observed want calculate value note since must follows using symmetry argument then matter value observed value ignored. similar calculations show that then matter value observed value ignored. example claimed bayesian dict predictive distribution deﬁned example. standard bayesian approach also directly consider expected conditional probability approaches give different answers since expectation commute division. prefer standard approach note that independence prαβ prαβ; similarly repeated observations. alternative approach would learning data. thus remainder paper predictivedistribution approach comment. example consider general situation arbitrary before. consider straightforward extension previous distributions element -dimensional unit simplex; deﬁned focus large class priors includes standard recommendations noninformative priors. essentially show prior class sample size small ignoring information better using bayesian posterior. pragmatic agent choice ﬁrst adopting pragmatic prior perhaps correctly reﬂecting beliefs reasoning like bayesian simply ignoring available information then sample size small might prefer option hand information becomes available bayesian posterior behaves almost well ignoring information worst case substantially better ignoring cases. example example deﬁniteness suppose known prior throughout section assume probability measure completely determined moreover every choice conditional probabilities probability prαβ fact calculate bayesian predictions given must ﬁrst determine bayesian marginal probability measure prαβdαdβ calculate expected loss predicting calculate so-called predictive distribution calculate directly without performing integration follows. symmetry must must case implies thus random variable used denote outcome observations given sequence observations denote number observations sequence similarly denotes number observations sequence next prior {pr~α~β restrict attention priors written product dirichlet distributions dirichlet distribution -dimensional unit simplex parameterized -dimensional vector ~adirichlet distribution density satisﬁes show odds-ratio behaves like times correction factor. ideally correction factor would close small samples smoothly change right direction bayesian’s predictions never much worse minimax predictions data comes monotonically better better. consider examples show extent happens. probability difference bayesian’s expected loss expected loss someone ignores information clearly depends moreover ﬁxed limn→∞ this course says eventually bayesian learn correctly. however relatively small hard construct situations nontrivial. example cause true computing probability.) thus bayesian’s expected loss worse agent ignores information. conclusion assumed bayesian chose particular noninformative prior depend strongly choice. well known unique deﬁning uniform prior distributions since uniform depends chosen parameterization. reason people developed types noninformative priors. wellknown so-called jeffreys’ prior speciﬁcally designed prior expressing ignorance. prior invariant under continuous -to- reparameterizations turns jeffreys’ prior also dirichlet form satisﬁes pragmatic priors often used practice so-called equivalent sample size priors case also take dirichlet form. thus analysis example substantially change jeffreys’ prior prior. remains case that certain sample sizes ignoring information preferable using bayesian posterior. example shows noninformative priors small sample sizes ignoring information better bayesian updating. essentially reason standard noninformative priors assign probability zero distributions according independent. measures exactly ones lead minimax-optimal decisions. course reason bayesian must noninformative prior. settings preferable adopt hierarchical pragmatic prior puts uniform probability assigns probability prior makes easier bayesian learn independent. context universal coding logarithmic loss function.) prior bayesian would better example. notion optimality used minimax loss optimality. prediction better prediction worst-case loss predicting better worst-case loss predicting certainly quite reasonable criteria could used comparing predictions. particular could consider minimax regret. could consider prediction minimizes worst-case difference best prediction actual prediction. second half example calculated true probability makes independent difference between loss incurred agent ignores prior bayesian roughly know probabilities bayesian agent much worse agent ignores prior respect pr′. hand completely correlated true probability bayesian predict correctly half time agent ignores information not. difference loss incurred bayesian agent agent ignores information thus sense expected regret bayesian approach bound least good ignoring information example. currently investigating whether true generally. sections showed ignoring information sensible long contains distributions given marginal loss function ﬁxed; particular depend realized value section consider assumption contains distributions given marginal amounts assumption agent knows agent information probability distribution ignoring information general reasonable thing take simple example suppose contains distribution clearly minimax optimal strategy decision rule based conditional distribution means available information taken account. using clearly right thing hand neither singleton distributions given marginal ignoring minimax optimal depending details even cases ignoring minimax optimal still reasonable update rule because matter ignoring reliable update rule means following suppose loss function known agent. optimal action resulting ignoring information adopting marginal distribution independently observed. must case meaning loss agent expects using adopted action guaranteed identical true expected loss agent’s action thus quality agent’s predictions exactly good agent thinks agent cannot overly optimistic performance. data behave agent’s adopted distribution correct even though not. desirable property reliability lost loss function depend observation understand impact possibility consider situation example except assume loss function depend observation. example example assume agent knows epry ﬁxed distributions marginal loss function takes three arguments loss predicted true value observed. suppose observation then before loss difference predicted value actual value; hand observation loss twice difference. note that loss function technically longer makes sense talk ignoring information since cannot even talk optimal rule respect however shall optimal action still predict likely value according hard show randomization help case minimax optimal decision rule predict thus minimax-optimal decision rule still chooses likely prediction according independent observation. hand either observed arguments show minimax-optimal action respect conditional probability randomize predicting probability again time inconsistency sense discussed section ignoring information right thing consider happens loss function thus actual value observation same loss function difference actual value prediction; however actual value observation different loss twice difference. four decision rules above augustin’s seidenfeld’s work seidenfeld provides analysis minimax decision rules closely related ours markedly different conclusions. suppose agent predict value random variable observing another random variable seidenfeld observes section minimax paradigm applied situation different ways seidenfeld notes section local minimax strategy equivalent global minimax strategy. moreover exhibits rather counterintuitive property local minimax. suppose that observing agent offered following proposition. additional small cost told value predict agent uses local minimax strategy would accept proposition observing leads smaller minimax prediction loss observing therefore local minimax agent would willing information. phenomenon observed example seidenfeld interprets observations evidence local minimax strategy ﬂawed least extent. views discrepancy local global minimax problematic aspect minimax paradigm. closely related context augustin also observes discrepancy global local minimax strategy writes there sound arguments both. paper express third point view regard strategy ignoring information global minimax loss strategy reasonable decision rules preferable example local minimax loss strategy. however certainly claim global minimax loss reasonable strategy. example explained section situations minimax regret appropriate. also explained section complex structure considered sections ignoring information longer coincide global minimax strategy. remains investigated whether cases clear preference either ignoring information global minimax. cost-free information never ignored observed section purely subjective bayesian pragmatic sense always condition available information information never ignored. result reconciled ﬁndings noting depends agent representing uncertainty single distribution. bayesian case agent starts distributions transformed single distribution adopting subjective prior expected value information calculated using expectation based agent’s prior contrast bayesian analysis section reasons explained beginning section computed expectation relative probabilities meant represent agent’s uncertainty. consequently results differ subjective bayesian analysis. shown that minimax sense sometimes better ignore information least while rather updating. strategy essentially different popular probability updating mechanisms nonbayesian mechanism described section bayesian mechanism section method aware leads similar results following form maximum-entropy formalism agent ﬁrst chooses unique distribution maximizes shannon entropy predicts based conditional distribution application maximum entropy principle ignore value contains distributions given marginal however indicated section updating ignoring still useful contains subset distributions given cases well known maximum entropy introduce counterintuitive dependencies exempliﬁed judy benjamin problems thereby making method different merely ignoring all. minimax-optimality results depend assumption possible prior distributions contains information possible correlations between variable interest observed variable. addition depend assumption payoff depends actual value predicted value variable interest. understanding issues involved terms knowledge advocated halpern tuttle speciﬁcally knowledge agent knowledge adversary choosing loss function. knowledge agent encoded possible prior distributions. knowledge adversary encoded assumptions loss function. adversary know observation time loss function determined loss function candepend observation; adversary knows observation can. generally especially negative losses allowed adversary know true distribution adversary choose whether allow agent play depending observation. future work plan consider impact allowing adversary extra degree freedom. thank teddy seidenfeld fraassen helpful discussions topic paper. joseph halpern supported part grants ctc- itr- grants n--- n--- multidisciplinary university research initiative program administered grant n--- afosr grant f---.", "year": 2005}