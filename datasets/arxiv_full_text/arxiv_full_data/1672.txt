{"title": "Improving Variational Encoder-Decoders in Dialogue Generation", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Variational encoder-decoders (VEDs) have shown promising results in dialogue generation. However, the latent variable distributions are usually approximated by a much simpler model than the powerful RNN structure used for encoding and decoding, yielding the KL-vanishing problem and inconsistent training objective. In this paper, we separate the training step into two phases: The first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding. In this case, latent variables are sampled by transforming Gaussian noise through multi-layer perceptrons and are trained with a separate VED model, which has the potential of realizing a much more flexible distribution. We compare our model with current popular models and the experiment demonstrates substantial improvement in both metric-based and human evaluations.", "text": "real data intermediate latent variables expected uncover disentangle causal factors important explain data. exact log-likelihood normally requires integral high-dimensional space cannot analytically expressed. current approaches solve intractability problem imposing recognition network approximate real posterior probability. variational autoencoders bring scalability stability training procedure introduces reparameterization trick reduce variance estimating backpropagated gradients. proposed vhred structure applied conditional encoder-decoders dialogue generation hope cvae’s advantage learning global representations good complement rnn’s power modeling local dependencies. however simple combination runs kl-vanishing problem part ends explaining structures without making latent representation. reason universal approximator much ﬂexibility simple gaussian distributed latent variables model lacks enough motivation utilize them. current approaches normally address problem weakening decoder match simpler latent variable distribution essentially sacriﬁces generating capacity better representation learning inappropriate main goal learn generative model. paper contrary take advantage universality rnns help realize ﬂexible latent variable distribution. means motivation utilizing latent variables also strengthen expressiveness generating model. speciﬁcally split whole structure cvae module autoencoder module. cvae module learns generate latent variables module builds connection real dialogue utterances. outputs cvae serve input latent variables module potentially much ﬂexible restricting latent variables follow ﬁxed distribution. encoder-decoders module universal approximators adjusted extract continuous vectors dialogue data easily modelled cvae module. combined variational encoder-decoders shown promising results dialogue generation. however latent variable distributions usually approximated much simpler model powerful structure used encoding decoding yielding kl-vanishing problem inconsistent training objective. paper separate training step phases ﬁrst phase learns autoencode discrete texts continuous embeddings second phase learns generalize latent representations reconstructing encoded embedding. case latent variables sampled transforming gaussian noise multi-layer perceptrons trained separate model potential realizing much ﬂexible distribution. compare model current popular models experiment demonstrates substantial improvement metric-based human evaluations. recurrent neural networks widely used natural language processing tasks. however given history context rnns estimate probability word time work holistic sentence representation applied dialogue generation corresponding result would generate either short boring responses long inconsistent sentences. length generated sentences grows would easily deviate original intention tokenlevel estimation considers immediate short rewards neglects global structure consistency. hence vanilla rnns prefer generating generic safe short responses avoid risk making errors improving deﬁcient generating process introduce sentencelevel representation conditioned ensure sentence-level consistency. deep latent variable models popular learn representations generative setting. latent representations generators jointly trained unsupervised way. learning probability synthesiz∗authors contributed equally. correspondence shen copyright association advancement artiﬁcial intelligence rights reserved. scheduled sampling trick structure signiﬁcantly improve generating performance. show structure compared adversarial encoder-decoder substitutes step alternative. though theoretically less accurate framework preferred training process much reliable seqseq tasks universality ensures inaccuracy controlled within acceptable range. vhred variational autoencoder popular generative model. generating process follows data generated generative distribution sampled prior distribution contrast calculating exact log-likelihood efﬁciently trained optimizing valid lower bound objective takes following form real posterior distribution given prior distribution likelihood optimizing objective namely maximizing likelihood time minimizing mismatch approximated posterior parametrized neural networks real posterior kl||pθ) large objective becomes inconsistent generating process cannot recover real data distribution even global optimum. whole process conditioned additional context leads conditional output generated distribution latent variable drawn prior distribution variational lower bound cvae written follows −eqφ] klpθ) specially extent context output sequential data cvae also treated seqseq model variational hierarchical recurrent encoder-decoder cvae hierarchical encoders ﬁrst-layer encodes tokenlevel variations second-layer captures sentencelevel topic shifts. case equation. stands dialogue history response decoded latent variable reﬂecting high-level representation distribution usually simple gaussian distributions diagonal covariance matrix. optimization challenges vhred straightforwardly optimizing equation. suffers kl-vanishing problem decoder universal function approximator tends represent distribution without referring latent variable. beginning training process approximate posterior carries little useful information natural model blindly closer gaussian prior extra cost divergence avoided better analyze optimizing comes from item pθpθdz family complex enough includes real distribution optimal value item reliance necessary. however reliance provides model chance taking advantage distribution reduces complexity requirement distribution family example suppose modeling accurately without reliance requires include gaussian distribution means linear mapping describe real distribution linear complexity. gaussian distribution covered family model exploit relation model real distribution. likewise dialogue generation although decoder theory approximate arbitrary function perfectly ﬁtting real dialogue distribution still difﬁcult optimizing challenge training corpus size approximating errors. therefore achieve global optimum believe ﬁrst item always prefer utilizing latent variables long decoder perfect. weaker decoder family biased utilizing latent variables. ﬂexible prior distribution also increase chance provides possibilities utilisation. second item divergence whose minimum value according bayes theorem express ignoring latent variable cancels setting easily arrive global optimum otherwise parametrised mean-ﬁeld gaussian distribution vhred real posterior impossible fall distribution family. firstly independence relation cannot satisﬁed. make dimensions independent other likelihood must exactly disentangle effect every dimension unrealistic categorical distribution modelled softmax. secondly real posterior distribution hardly still follow gaussian distribution likelihood based discrete sequential data. normally training process adjust make real posterior easier modelled however represents sentences variable length value vanishes greatly length grows makes adjusting task much difﬁcult. implies second item always prefer ignoring latent variables long approximated posterior powerful enough perfectly match real posterior. weaker approximating posterior distribution family biased ignoring latent variables. objective function variational encoderdecoders dialogue generation essentially competition items biased utilizing ignoring latent variables respectively. reason divergence vanishing global optimum second term gain ignoring latent variables ﬁrst term utilizing them. chen normalizing ﬂow. latent variables ﬁrst sampled simple distribution passed several invertible transformations better ﬂexibility. normalizing computationally costly applied text generation yet. also change original elbo objective easier optimization. kl-annealing free bits popular strategies. klannealing small weight added divergence term equation. starts zero gradually increases prevents model zeroing divergence earlier training stage. divergence vanishes difﬁcult recovered short sight nature gradient descent. free bits reserve space divergence every dimension latent variables. divergence optimized exceeding predeﬁned quota. similar ideas found reserved space total divergence instead every dimension. current approaches elbo objective explained directions prevent kl-vanishing problem improving advantage utilising latent variables pθpθdz weakening advantage abandoning latent variables kl||pθ). former direction need smaller distribution family model decoder decoder weaker ignoring latent variables becomes farther real distribution global optimum thus encouraging latent variables exploited. word drop-out common method weaken decoder. time step input word certain chance becoming anword decoder therefore cannot store continuous history context. word dropalso explained special kind smoothing. similarly decoders limiting power also encode information latent variables bag-of-word loss proposed also fall category. imposes extra loss forces latent variable predict whole sentence without word inputs essentially increasing weight reconstruction loss droprate latter direction need ﬂexible prior posterior distribution latent variables. approximated posterior distribution powerful enough divergence close zero without losing dependence latent variables. applies piecewise distribution replace gaussian prior distribution. though represent multi-modal conditions still limited ﬁxed distribution pre-deﬁned number modes. samples latent variables markov chains imposed extra approximation objective becomes less accurate. proposed adversarial autoencoder samples posterior latent variables transforming gaussian noise multilayer-perceptrons. ﬂexibility neural networks ensures arbitrary distribution. however probability density intractable adversarial learning must implemented replace original divergence term. training alternates autoencoder phase optimize −eqφpθ phase match aggregated posterior prior implicitly deﬁned passing context-dependent gaussian random variables multi-layer perceptrons. graphical model depicted figure. shown objective differs original elbo adding extra punishment entropy using jensen-shannon divergence lieu divergence. non-parametric limit generating model recover exact data distribution. replacing idea sounds appealing notoriously difﬁcult train especially prior posterior need updated towards posterior objective global optimum js||pθ). therefore instead alternate phase cvae phase achieve effect aed. constraining encoder accuracy cvae objective relies matching degree therefore phase apart encoding representative information reduce normal reconstruction loss encoder also encode utterances manner real posterior easily modelled distribution deﬁned cvae phase. this divergence constraint encoder phase. encoder keep kl||pθ) within speciﬁc range. also possible constrain value whole cvae objective equation. constraining divergence enough alternating step large. note encoder phase model adjust encoder-decoders control divergence generating parameters latent variables ﬁxed. scheduled sampling trick phase also useful initially ground-truth encoding gradually change noisier cvae output apply scheduled sampling strategy proposed decoding coin ﬂipped decide whether feed real hidden vector noisy beginning make easy mostly pick real training proceeds gradually improve difﬁculty increasing chance selecting noisy ﬁnally inputs other model becomes extremely sensitive hyperparameters training unstable. consequence replacing phase cvae alternative. encoder ﬁrst applied extract corresponding latent variable target dialogue turn based cvae trained reconstruct context-dependent gaussian noise. connection seen figure speciﬁcally replace js||pθ) equation following cvae objective different topics. dataset crawled various websites english learner practice english daily life. statics show speaker turns roughly average tokens utterance appropriate training dialog models. switchboard two-sided telephone conversations speciﬁed topics manually transcribed speech alignment. compared dailydialog turn every dialogue much longer subject disperse. datasets randomly separated training/validation/test sets ratio models training procedures comparison also implemented hred model basis vhred. latent variable models trained standard kl-annealing different weights additional loss word drop-out free bits collaborative scheduled sampling trick framework encoder transformation function tuned parameters validation measure performance test set. experiments letters transformed lower-case vocabulary size words mapped special token <unk>. word embeddings size initialized wordvec embeddings trained google news corpus. ﬁrst second-layer encoder decoder following experiments single-layer hidden neurons. dimension latent variables batch size learning rate models. framework trained epochwise alternatively training cvae part. probability estimators -layer feedforward neural networks. test time output likely responses using beam search beam size <unk> tokens prevented generated. implemented models open-sourced python library tensorﬂow optimized using adam optimizer dialogs slices slice containing words memory. metric-based evaluation compare model basic hred several current approaches including kl-annealing word drop-out free-bits bag-of-words loss details summarized table initialize weight gradually increase ﬁrst training steps dailydialog switchboard respectively. word drop-out rate ﬁxed words dropped training step. reserved space every dimension free bits also reserving bits whole dimension space value collaborative model scheduled sampling weight dailydialog switchboard. also experiment jointly training cvae part step number constant controlling decaying speed. decaying functions also applicable like exponential decay inverse sigmoid decay. training process model contains cvae phase phase. phases trained iteratively equilibrium achieved. cvae phase sample obtained transforming dialogue texts continuous embedding used target maximum likelihood training cvae. assume generative model loss function phase observation sampled training data transform function continuous vector representation corresponding latent variable sampled posterior distribution provided cvae part. sampled latent variable together forms target training objective function ﬁrst item used control divergence reasonable range transformed easily modelled cvae phase. used adjust leverage reconstruction loss divergence lower value lead lower divergence end. keeping rate deﬁned equation. detailed architecture depicted figure refer framework collaborative cvae phase collaborate achieve better generating performance. model summary summary replace phase cvae alternative. output cvae part latent variables represent much broader distribution family mean-ﬁeld gaussian. cvae theory less accurate needs approximate real posterior leverage powerful encoder-decoders. phase autoencode utterenaces make real posterior easily representable cvae part. seen model co+ss achieves lowest datasets. schedule sampling strategy signiﬁcantly helps brings nll. word drop-out though weakening decoder improved performance combined veriﬁed assumption function smoothing technique neural network language models needs early stop otherwise divergence vanish weight increases avoids kl-vanishing problem overall performance signiﬁcantly decrease adding additional loss theory leads biased result latent variables. information encoded latent variable prevents decoder stably learning word order pattern training step thus sacriﬁces performance. fb-all performs much better suggests important information concentrated dimensions. equally reserving space every dimension suitable. finally also testiﬁed necessity iteratively training model. jointly training model brings recession perplexity divergence datasets. figure t-sne visualization sampled latent variables. left vhred right co+ss. dots correspond samples prior distribution blue dots correspond samples posterior distribution vhred framework. randomly pick dialogue context like invite dinner tonight time apply information retrieval based method gather responses similar context corpus. responses veriﬁed humans appropriate ones span different possibilities like thank invitation. don’t silly let’s dutch asking date response samples drawn posterior latent variable distribution forms posterior latent variable samples total. likewise samples drawn prior latent variable distribution given dialogue context. visualization clearly indicates superiority framework modelling ﬂexible prior posterior latent variable distributions. vhred model prior posterior distributions limited uni-modal gaussians little overlap. framework distributions diverse samples prior posterior distribution share overlap other. table reports results embedding-based topic similarity embedding average embedding extrema embedding greedy unlike measures token-level match embedding-based metrics responses vector space compute cosine similarly golden answers large extent measure sentence-level semantic similarity. model still achieved highest topic similarity according three metrics. suggests model bring improvement token-level coherence sentence-level topic match. though good metric performed remarkably well metric implies beneﬁcial decoder generate correct high-level meaning fails transform meaning ﬂuent sentence. contrast relative lower on-topic similarity score compared performance token-level likelihood. human evaluation accurate evaluation dialogue systems open problem. validate previous metric-based results conduct human evaluation several models. randomly sampled context test corpus apply differresponse right i’ll take kla+bow well much price price price co-ss that’s good want gold really potato sorry sure don’t want look moment kla+bow sorry it’s pleasure room again-b co+ss questions company gold sure like cold medicine doctor’s honest familiar friends kla+bow like would like mind co+ss really gold right enjoy models generate best response beam search. evaluation conducted dailydialog corpus since closer daily conversation easier humans make judgement. generated responses together dialogue context randomly shufﬂed judged crowdsourcing website crowdflower. people asked judge plausibility generated response giving binary score three aspects grammaticality coherence dialogue context diversity people ﬁnally involved evaluating total responses judged different people score agreed people adopted. person judge responses ﬁlter manually-set test questions. results shows model generates highly ﬂuent sentences compared approaches. kla+bow expected receives lowest score ﬂuency. model also achieves relative good scores coherence diversity implying novel responses related conversation topic generated model. however notice human evaluation rather subjective reliable enough. sentence inﬂuent humans tend reject though topic might coherent content might diverse. difﬁcult give objective score separately three aspects. models lower scores ﬂuency normally also receive lower scores ﬁelds like kla+bow fb-all. therefore consider evaluation complement metric-based results indicating humans agree generations models others. table shows exampled generated responses. improved collaborative model scheduled sampling accurately identify topic generate coherent responses. standard kl-annealing tends generate smooth sentences irrelevant context. imposing additional loss increase probability correctly capturing main topic generated responses sometimes grammatically wrong also shown metric-based results. ﬁrst example context taxi drivers’ request reducing gasoline table human judgements models trained dailydialog corpus refers ﬂuent refers coherence refers diversity. model kla+do kla+bow fb-all co+do co+ss price response ﬂuent natural sentence closely related context. model kla+bow starts reasonable beginning ends inﬂuent continuations. though inﬂuent kla+bow model capture main topic price indicating successfully predict order-insensitive words fail establish natural sentence. contrast model ﬂuent sentence also close topic. importantly brings information want helpful interactive conversation. similar conditions seen examples. variational encoder-decoders recurrent neural networks powerful representation learning natural language processing respectively. though recently quite work started apply dialogue generation training process still unstable performance hard guaranteed. work thoroughly analyze reason training difﬁculty compare different current approaches propose framework allows effectively combining structures dialogue generation. split whole structure parts ﬂexible prior posterior latent variable distributions. training process simple efﬁcient scales well large datasets. demonstrate superiority model popular methods dialogue corpus. experiments show model samples latent variables ﬂexible distributions without sacriﬁcing recurrent neural network’s capability synthesizing coherent sentences. without losing generality model able apply seseq tasks leave future work. acknowledgements xiaoyu shen supported imprscs fellowship. work partially funded collaborative research center national natural science china grant", "year": 2018}