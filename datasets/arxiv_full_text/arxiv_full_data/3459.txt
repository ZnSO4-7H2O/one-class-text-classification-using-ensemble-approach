{"title": "Poseidon: An Efficient Communication Architecture for Distributed Deep  Learning on GPU Clusters", "tag": ["cs.LG", "cs.CV", "cs.DC", "stat.ML"], "abstract": "Deep learning models can take weeks to train on a single GPU-equipped machine, necessitating scaling out DL training to a GPU-cluster. However, current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network, because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs, leading to more frequent network synchronization. We present Poseidon, an efficient communication architecture for distributed DL on GPUs. Poseidon exploits the layered model structures in DL programs to overlap communication and computation, reducing bursty network communication. Moreover, Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer, according to layer properties and the number of machines. We show that Poseidon is applicable to different DL frameworks by plugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables Caffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even with limited bandwidth (10GbE) and the challenging VGG19-22K network for image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU machines on Inception-V3, a 50% improvement over the open-source TensorFlow (20x speed-up).", "text": "deep learning models take weeks train single gpu-equipped machine necessitating scaling training gpu-cluster. however current distributed implementations scale poorly substantial parameter synchronization network high throughput gpus allows data batches processed unit time cpus leading frequent network synchronization. present poseidon efﬁcient communication architecture distributed gpus. poseidon exploits layered model structures programs overlap communication computation reducing bursty network communication. moreover poseidon uses hybrid communication scheme optimizes number bytes required synchronize layer according layer properties number machines. show poseidon applicable different frameworks plugging poseidon caffe tensorflow. show poseidon enables caffe tensorflow achieve speed-up single-gpu machines even limited bandwidth challenging vgg-k network image classiﬁcation. moreover poseidon-enabled tensorflow achieves speed-up single-gpu machines inception-v improvement open-source tensorflow deep learning class machine learning approaches achieved notable success across wide spectrum tasks including speech recognition visual recognition language understanding models exhibit high degree model complexity many parameters deeply layered structures usually take days weeks train gpu-equipped machine. high computational cost programs large-scale data necessitates training distributed cluster order software tensorflow caffe allow practitioners easily experiment models single machine. however distributed implementations scale poorly larger models. example vgg-k network open-source tensorflow machines slower single machine observation underlines challenge scaling clusters high computational throughput gpus allows data batches processed minute leading frequent network synchronization grows number machines. existing communication strategies parameter servers overwhelmed high volume communication moreover despite increasing availability faster network interfaces inﬁniband ethernet gpus continued grow rapidly computational power continued produce parameter updates faster naively synchronized network. instance machine cluster ethernet titan machine updates vgg-k model bottleneck network speedup single machine achieved scalability limitations distributed stem least causes gradient updates communicated large matrices quickly saturate network bandwidth; iterative nature algorithms causes updates transmitted bursts signiﬁcant periods network usage between. propose solution problems exploit structure algorithms levels hand identify ways matrix updates separated other schedule avoids bursty network trafﬁc. hand solution also exploit structure matrix updates themselves wherever possible reduce size thus overall load network. solution relevant practitioners would prefer exploit speciﬁc traits tensorflow’s caffe’s design strive relevant many existing frameworks possible. motivation design poseidon efﬁcient communication architecture data-parallel distributed gpus. poseidon exploits sequential layer-by-layer structure programs ﬁnding independent computation operations network communication operations training algorithm scheduled together reduce bursty network communication. moreover poseidon implements hybrid communication scheme accounts program layer’s mathematical properties well cluster conﬁguration order compute network cost different communication methods select cheapest currently poseidon implements supports parameter server scheme well-suited small matrices sufﬁcient factor broadcasting scheme performs well large matrices. focus synchronous parallel training shown yield faster convergence compared asynchronous training distributed gpus unless otherwise speciﬁed discussion paper assumes synchronous replication model parameters training iteration although note poseidon’s design easily applied asynchronous bounded-asynchronous consistency models demonstrate poseidon’s applicability multiple frameworks implement different frameworks caffe tensorflow show poseidon allows scale almost-linearly algorithm throughput additional machines incurring little additional overhead even single machine setting. distributed execution network bandwidth available poseidon consistently delivers near-linear increases throughput across various models engines speedup training inception-v network using tensorflow engine nodes improves upon original tensorflow training parameter network poseidon still achieves near-linear speedup using caffe tensorflow engines distributed tensorflow sometimes experiences negative scaling additional machines. experiments also conﬁrm poseidon successfully alleviates network communication bottlenecks reducing required bandwidth parallelizing large models. example training vggk limited bandwidth contrast ps-based parallelization achieves speedup machines poseidon effectively reduces communication overheads automatically specializing best communication method layer able keep linearly scaling throughput. compared communication reduction methods poseidemonstrates either systems advantages statistical advantages poseidon suffer much imbalanced communication loads found case using sufﬁcient factor strategy used project adam poseidon also guarantees number algorithm steps reach termination remains unchanged unlike -bit quantization strategy used cntk approximate hurt statistical performance applications. rest paper organized follows. section motivates poseidon introduction largescale parameter servers sufﬁcient factor broadcasting. section section elaborates poseidon’s design implementation respectively. section evaluates poseidon training different models multiple datasets including comparisons state-of-the-art gpubased distributed systems. section discusses related works section concludes. large-scale deep learning section formulate training iterative-convergent algorithm describe parameter server sufﬁcient factor broadcasting parallelizing computation clusters. distributed deep learning programs distinguished programs mainly neural networks family hierarchical models containing many layers many figure illustrates neural network layers. ﬁrst layer input layer reads data application-speciﬁc formats e.g. pixels trained classify images. input layer connected sequence intermediate layers consists neurons neuron applies function transformation input produces output. vector output obtained concatenating output neurons layer. stacking multiple intermediate layers transform input data layer time ﬁrst series intermediate representations ﬁnally desired output prediction programmers usually need specify computation layer deﬁning properties neurons. ﬁrst transformation function input neuron optional trainable parameter. connectivity determines neuron connected adjacent layer. allelism parallelization strategy partitions data distributes cluster computational worker machines illustrated figure iteration every worker fetches batch data partition computes gradients gradients workers aggregated applied update following data-parallelism allows data locally partitioned worker advantageous large datasets. however requires every worker read write access shared model parameters causes communication among workers; shared access provided parameter server architecture peer-to-peer broadcasting architecture designed general-purpose data-parallel programs cpus. parameter server. parameter server distributed shared memory system provides systematic abstraction iterative-convergent algorithms data-parallel distributed typically enables worker access global model parameters network communications following client-server scheme. trivially parallelized distributed workers using following steps worker computes gradients data partition send remote servers; servers receive updates apply globally shared parameters; consistency scheme coordinates synchronization among servers workers sufﬁcient factor broadcasting. many models represent parameters matrices. example fullyconnected trained using gradient training sample rank- matrix cast outer product vectors called sufﬁcient factors sufﬁcient factor broadcasting designed parallelize models broadcasting among workers reconstructing gradient matrices using locally. presents three differences uses communication strategy transmits instead full matrices. unlike gradients additive training samples i.e. number needed transmitted grows linearly number data samples overall communication overheads increase quadratically number workers. parallel distributed gpus modern models mostly trained using nvidia gpus primary computational steps match simd operation could efﬁciently performed gpus. figure illustration parameter server sufﬁcient factor broadcasting distributed instance convolutional neural network types neuron convolutional neuron locally connected subset neurons previous layer fully-connected neurons need trained data give accurate predictions. stochastic gradient descent backpropagation commonly employed train iteratively iteration performs feed forward pass followed backpropagation pass. pass network takes training sample input forwards input layer output layer produce prediction. loss function deﬁned evaluate prediction error backpropagated network reverse network parameters updated gradients towards error would decrease. repeating sufﬁcient number passes network usually converge state loss close minima training terminated. mathematical form given data loss function ﬁtting parameters formulated iterative-convergent algorithm repeatedly executing update equation ∇ld) reaches stopping criteria denotes iteration. update function calculates gradients current data gradients scaled learning rate applied updates. gradients additive data samples i.e. ∇ldi) efﬁciency usually feed batch training samples training iteration eq.. practice practitioners often single-node software frameworks caffe torch mathematically derive correct training algorithm execute calling gpu-based acceleration libraries cublas cudnn. thus straightforward parallelize programs across distributed gpus using either moving computation performing memory copy operations communication whenever needed. however argue show empirically section usually lead suboptimal performance. inefﬁciency mainly caused parameter synchronization network. compared cpus gpus order magnitude efﬁcient matrix computations; production gradients gpus much faster naively synchronized network. result training computations usually bottlenecked communications. example training alexnet titan standard batch size million gradients generated second parallelize training nodes using every node also holding parameters shard; then every node needs transfer ﬂoat parameters second make sure next iteration computation blocked. apparently demanded throughput exceeds bandwidth commodity ethernet provides; gpus distributed across clusters cannot fully utilized. practically usually difﬁcult partition parameters completely equally result severe bandwidth demands bursty communication trafﬁc several server nodes prevents trivial realization efﬁcient distributed gpus next describe strategies system design overcome aforementioned obstacles. poseidon design section ﬁrst analyze program single-node distributed environment decomposing program sequence operations. based introduce strategies address issues. structure programs. core program algorithm performs forwardbackward pass network repeatedly. deﬁne forward backward pass layer network computation step iteration notated illustrated fig. frequent memory copy operations dram memory also cause extra overheads minor compared network communication according empirical results. however strategies paper also alleviate overhead. figure backpropagation distributed environment. executing distributed gpus inter-machine communications required step guarantee synchronized replication model parameters. similarly deﬁne synchronization step process worker sends locally generated updates receives updated parameters remote workers iteration therefore naive parallelization training distributed gpus using either expressed alternating deﬁned above. note training highly sequential; communication computation perform sequentially waiting ﬁnish fortunately also note every layer contains independent parameters decoupled ﬁrst sending local updates layer reads updated parameters remotely rewrite training iteration sequential nature algorithm presents opportunity overlap computations communications. ﬁrst strategy wait-free backpropagation overlaps partially rescheduling independent. second strategy hybrid communication utilizes independency among tries reduce communication overheads specializing different communication methods different wait-free backpropagation wait-free backpropagation designed overlap communication overheads computation based independencies program send-out operation independent backward operations could executed concurrently without blocking other; read-in operation could update layer parameters long ﬁnished without blocking subsequent backward operations therefore enforce layer start communication gradients generated could overlapped wfbp beneﬁcial training models parameters concentrating upper layers computation concentrating lower layers e.g. adamnet overlaps communication layers computation bottom layers besides chain-like wfbp generally applicable non-chain like structures parameter optimization deep neural networks depends adjacent layers always opportunity parameter optimization communication different layers performed concurrently. frameworks tensorflow represent data dependencies programs using graphs therefore implicitly enable auto-parallelization. however fail exploring potential opportunities parallelization iterations. example tensorflow needs fetch updated parameters remote storage beginning iteration possible overlap communication procedure computation procedure previous iteration. comparison wfbp enforces overlapping explicitly pipelining compute send receive procedures. describe implementation wfbp section empirically show effectiveness section hybrid communication wfbp overlaps communication computation reduce communication overhead. situations network bandwidth limited communication would still unacceptably slow. address issue introduce hybrid communication strategy combines best aware mathematical property models structure computing clusters. idea comes observations ﬁrst presented section synchronization operations independent other meaning different communication methods different according methods described figure second structure usually predeﬁned ﬁxed throughout training measuring number parameters needed transferred able estimate communication overhead always choose optimal method even communication happens. overheads could estimated follows assume batch size number worktable estimated communication cost adam synchrnizing parameters layer cluster workers servers batchsize server nodes respectively. hand layer synchronizing parameters transfer million parameters worker node pmn/p million server node mn/p million node server worker compared million single node using sfb. hand conv layer updates indecomposable sparse directly resort therefore synchronization overheads depend model also size clusters. optimal solution usually changes mnkpp. hybcomm takes account factors allows dynamically adjust communication method different parts model always chooses best method available ones whenever results fewer communication overheads. microsoft adam employs different communication strategy figure instead broadcasting across workers ﬁrst send parameter server shard pull back whole updated parameter matrices. seems reduce total number parameters needed communicated usually leads load imbalance; server node holds corresponding parameter shard overloads broadcast parameter matrices workers messages need broadcasted) easily causes communication bottleneck noticeable reconstructing gradients cause extra computation cost however often negligible compared communication. describe implementation hybcomm next section assess effectiveness section section ﬁrst elaborates poseidon’s system architecture apis describes modify framework using poseidon enable distributed execution. system implementation apis figure illustrates architecture poseidon communication library manages parameter communication programs running distributed gpus. three main components coordinator maindescription best communication scheme layer query information coordinators’ information book send parameter updates corresponding layer receive parameter updates either parameter server peer workers move contents transformations application updates needed send updated parameters receive gradient updates workers tains model cluster conﬁguration; store shared memory key-value store provides support parameter server based communication; client library plugged programs handle parameter communication. apis listed table coordinator. setup distributed training client program ﬁrst instantiates poseidon creating coordinator within process. coordinators ﬁrst collect necessary information including cluster information model architecture information coordinator initialize stores client library steps allocate proper communication ports shard peer worker; determine parameters transmitted store hash parameters equally store necessary save mapping information book which throughout whole training maintained synchronized across nodes could accessed elsewhere coordinator’s query api. besides coordinator provides another bestscheme takes layer returns optimal communication scheme according strategy described section store. store implemented based bulk synchronous parameter server instantiated coordinators list user-speciﬁed server machines. instance store holds shard globally shared model parameters form pairs pair stored chunk dram. poseidon sets size pair ﬁxed small size partition distribute model parameters server nodes equally possible reducing risk ethernet bottleneck. store instance manages parameter buffer provides ps-like apis receive send receiving applying updates client libraries sending parameters. regularly checkpoint current parameter states fault tolerance. client library. poseidon coordinates programs client library. particularly users plug client library training program client library create syncer layer network assembling accounting parameter synchronization. sycner initialized example setting connections corresponding shards peer syncers according coordinator’s information book allocating small memory buffer receiving remote parameter matrices etc. client library manages thread pool stream pool worker machine allocated syncer apis syncer created. syncer three main apis send receive move used client programs. move takes care memory movement memory performs necessary computation e.g. transformation gradients application updates. multi-threaded using cuda asynchronous apis trigger allocation client library’s thread/stream pools syncer starts send receive communication apis synchronize layer parameters across different model replicas. send nonblocking; sends parameter updates backpropagation generated following protocol returned coordinator’s bestscheme api. receive called send ﬁnished. requests either fresh parameter matrices stores peer syncers block current thread receives requested. received messages syncer’s memory buffer move fetch. managing consistency. poseidon implements bulk synchronous consistency model follows. client library maintains binary vector length number syncers values reset zeros start iteration. syncer corresponding entry ﬁnishes client starts next iteration entries while store maintains zero-initialized count value pair start iteration. every time update applied pair count value increased pair broadcasted send count equals number workers. poseidon handles stragglers simply dropping them. although asynchronous models alleviate straggler problem distributed poseidon focuses synchronous parallel training synchronous execution yields fastest per-iteration improvement accuracy distributed gpus algorithm parallelize library using poseidon poseidon could plugged existing frameworks enable efﬁcient distributed execution. algorithm provides example. speciﬁcally needs ﬁrst include poseidon’s client library framework ﬁgure backpropagation proceeds insert poseidon’s syner apis gradient generation application demonstrate section slight modiﬁcations poseidon-enable caffe tensorflow deliver linear scalings machines. poseidon respects programming interfaces native library stores necessary arguments distributed execution environment variables allow zero changes application programs. evaluation section evaluate poseidon’s performance scaling distributed gpus. focus image classiﬁcation task successfully applied. evaluation reveals following results poseidon little overhead plugged existing frameworks; achieves near-linear speedups across different frameworks titan xequipped machines. poseidon’s system design effectively improves bandwidth utilization. poseidon’s communication strategy hybcomm effectively alleviates communication bottleneck thus achieves better speedups limited bandwidth; moreover poseidon compares favorably communicationreduction methods strategy adam -bit quantization cntk cluster conﬁguration. conduct experiments cluster node equipped nvidia geforce titan card intel -core interconnected -gigabit ethernet switch. cluster nodes shared access read data ethernet interface. system ubuntu nvidia driver version cuda cudnn computation engines. deploy poseidon frameworks caffe tensorflow caffe ofﬁcial version single node baseline modify using poseidon’s client library distributed execution. tensorflow open source version parallelize single-node version poseidon’s client library compare original distributed version. dataset models. experiments three wellknown image classiﬁcation datasets. cifar- contains colored images classes images training testing; ilsvrc subset imagenetk million training images validation images categories; imagenetk largest public dataset image classiﬁcation including note distributed engine tensorflow highly optimized poseidon avoids leveraging build-in optimization distributed tensorflow parallelizing single-node version instead. table neural networks evaluation. single-node batchsize reported. batchsize chosen based standards reported literature labeled images categories. test poseidon’s scalability across different neural networks cifar- quick caffe converges accuracy classifying images cifar- dataset; googlenet -layer parameters. inception-v imagenet winner improved version googlenet tensorflow; popular feature extraction network computer vision community conv layers layers total parameters; vgg-k modify network replacing -way classiﬁer classiﬁer classify images imagenetk dataset. modiﬁed network parameters. resnet- imagenet winner network layers. list statistics conﬁgurations table metrics. paper mainly focus metrics measure system performance speedups throughput experiments focus medium-scale distributed cluster machines distributed empirically beneﬁts from. larger clusters require larger batch sizes hurt convergence rate iteration completeness also report statistical performance resnet. poseidon uses synchronized replication enables many models converge fewer steps scalability demonstrate poseidon’s scalability train cnns using poseidon different computational engines compare different systems terms speedups throughput. caffe engine train googlenet vgg-k networks; tensorflow engine train inception-v vgg- vgg-k. caffe engine. figure shows throughput number workers training three networks using caffe engine given ethernet bandwidth avail caffe able. compare following systems unmodiﬁed caffe executes single gpu; caffe+ps parallelize caffe using vanilla i.e. parameter synchronization happens sequentially backpropagation iteration; caffe+wfbp parallelized caffe using poseidon communication poseidon shows little overheads combined caffe; running single node communication involved poseidon-caffe process images second training googlenet vgg-k respectively compared original caffe process images caffe+ps process images second overheads caused memory copy operations overlapped poseidon computation. distributed environment rescheduling computation communication signiﬁcantly improves throughput training googlenet incorporating wfbp achieves almost linear scalings machines larger vgg-k network caffe+wfbp achieves speedup machines. conclude rescheduling multi-threading communication computation performance distributed gpus even bandwidth resource abundant. poseidon provides effective implementation overlap operations frameworks guarantee better utilization. available bandwidth sufﬁcient poseidon’s hybcomm strategy shows small improvement training googlenet vgg. however training vgg-k three layers occupy model parameters improves caffe-wfbp nodes. tensorflow engine. also modify tensorflow using poseidon compare following systems terms speedup throughput tensorflow original distributed executions; tf+wfbp modify tensorflow using poseidon’s client library. specifically change assign operator tensorflow instead applied parameter updates synchronized poseidon’s interface wfbp; poseidon full version poseidon-parallelized tensorflow hybcomm enabled. train inception-v vgg-k models report results figure running single node poseidon processes images second training inception-v vgg-k original tensorflow processes images second three models respectively little overhead introduced modiﬁcation. distributed execution poseidon achieves allinear speedup machines. distributed tensorflow however demonstrates speedup training inception-v even fails scale training networks experiments. investigate problem tensorflow explain poseifigure breakdown computation stall time training three networks nodes using different systems. improves upon illustrates figure ratio busy stall time training three networks using different systems nodes. observe poseidon keeps gpus busy time tensorflow wastes much time waiting parameter synchronization. inefﬁciency distributed tensorflow stems sources. first tensorflow partitions model parameters coarse-grained granularity tensor model assigned shard. tensor highly likely create communication bottleneck located server node. poseidon ﬁxes problem partitioning parameters among server nodes ﬁner-grained granularity using pairs every node evenly distributed communication load; evidence tf-wfbp demonstrates higher computation-to-stall ratio figure second tensorflow cannot reduce communication overheads poseidon’s hybcomm effectively reduces size messages. result poseidon improves upon tf-wfbp nodes. multi-gpu settings. poseidon’s strategies directly extended support distributed multi-gpu environment minor modiﬁcations. speciﬁcally worker node poseidon ﬁrst collect gradient updates following wfbp locally multiple gpus leader using cudamemcpy api. updates determined communicated full matrices poseidon aggregate locally sending out. using caffe engine single node poseidon achieves linear scalings titan gpus training three networks outperforming caffe’s multi-gpu version shows speedups training goolenet vgg. running p.xlarge instances poseidon reports speedups training googlenet nodes conﬁrming statement overheads caused memory movement gpus usually negligible compared network communication. statistical performance. completeness report figure statistical performance training resnet- using poseidon. poseidon achieves nearlinear speedups system throughput statistical convergence poseidon delivers speedup terms throughput reaches reported error less epochs nodes thus linear scales terms time accuracy compared nodes batchsize standard nodes accelerated observation conﬁrms argument limited bandwidth would result communication bottleneck training models distributed gpus. fortunately poseidon signiﬁcantly alleviates issue. limited bandwidth constantly improves throughput directly reducing size messages needed communicated especially batch size small; training vgg-k poseidon achieves near-linear speedup machines using bandwidth optimized would otherwise need even higher achieve. note poseidon never underperform traditional scheme reduce parameter server whenever results less communication overheads; instance observe poseidon reduces training googlenet nodes googlenet thin layer trained large batch size comparisons methods section compare poseidon communication methods including adam cntk quantization show poseidon’s advantages. adam. save bandwidth adam synchronizes parameters layer ﬁrst pushing generated workers node pulling back full parameter matrices thereafter. direct comparisons adam inaccessible implement strategy poseidon compare tfwfbp poseidon monitoring network trafﬁc machine training nodes using tensorflow engine. shown figure communication workload highly imbalanced using adam’s strategy. unlike traditional parameters equally distributed multiple shards adam cannot partition parameters layers because usage sfs. although push operation uses reduce message size pull requires server nodes broadcast matrices worker node creates bursty trafﬁc results communication bottleneck them. contrast poseidon either partitions parameters equally multiple shards transmits among peer workers ting echoing recent results synchronous training distributed gpus yields better performance asynchronous training terms time quality table. poseidelivers quality accuracies reported papers gpus. bandwidth experiments assess poseidon’s hybcomm strategy simulate environment network bandwidth limited. linux trafﬁc control tool lower available bandwidth node compare training throughput without hybcomm. focus caffe engine section lighter less optimized tensorflow. figure plots speedup throughput number workers training googlenet vgg-k different maximum bandwidth. clearly limited bandwidth prevents standard psbased system linearly scaling number nodes; example given bandwidth training using communication load-balanced avoid bursty communication situations. quantitatively adam delivers speedup nodes training vgg. cntk. compare poseidon -bit quantization technique proposed cntk create baseline poseidon-bit uses -bit strategy quantize gradients layers residual updates next iteration. train cifar- quick network plot training loss test error iterations systems figure -bit quantization yields worse convergence terms accuracy gpus achieves error iterations poseidon quickly converges error iteration conjecture caused quantization residual equivalent delayed updates hurt convergence performance training images conﬁrmed also directly train using cntk-bit system report speedups nodes respectively thus less scale-ups poseidon also compromised statistical performance approximated updates. figure training loss test error iteration training cifar- quick network using poseidon poseidonbit gpus caffe engine. related work ps-based distributed systems. based parameter server architecture number cpubased distributed systems developed adam purely psbased systems cpu-only clusters whereas address challenging case clusters. scaling distributed gpus active ﬁeld research. coates build gpu-based multimachine system using model parallelism rather data parallelism implementation rather specialized ﬁxed model structure demanding specialized hardware inﬁband networking. tensorflow google’s distributed platform uses dataﬂow graph represent models synchronizes model parameters therefore candynamically adjust communication method depending layer cluster information poseidoes. mxnet another system uses distributed execution supports tensorflowlike graph representations models. autoparallelizing independent subgraphs frameworks implicitly overlap communication computation. contrast poseidon explicit overlap client library. hence poseidon also used parallelize non-graph-based frameworks. moreover mxnet tensorflow address bottleneck caused limited network bandwidth undermines scalability training large models dense layers besides propose geeps manages limited memory report speedups distributed gpus. while geeps address issue limited network bandwidth. therefore poseidon’s technique could combined enable better training speedups. also note several efforts port caffe onto distributed platforms sparknet yahoocaffe firecaffe former reports times speedup machines distributed systems. cntk framework supports distributed executions addresses problem communication bottleneck -bit quantization technique. cntk demonstrates little negative impact convergence speech domains however domains performance usually compromised noisy gradients contrast poseidon’s hybcomm reduces communication always guaranteeing synchronous training. also growing interest parallelizing applications using peer-topeer communication malt poseidon draws inspiration works goes step adaptive best-of-bothworlds protocol select client-server communication whenever would result fewer overheads. conclusion present poseidon scalable efﬁcient communication architecture large-scale distributed gpus. poseidon’s design orthogonal tensorflow caffe frameworks techniques present poseidon could used produce better distributed version them. empirically show poseidon constantly delivers linear speedups using nodes limited bandwidth variety neural network datasets computation engines compares favorably adam microsoft cntk. acknowledgments thank shepherd reviewers helpful feedback. thank parallel data laboratory machine resources henggang insightful discussion. research supported data parallel ccf.", "year": 2017}