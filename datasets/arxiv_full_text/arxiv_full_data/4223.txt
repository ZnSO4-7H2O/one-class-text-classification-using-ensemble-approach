{"title": "Classification Driven Dynamic Image Enhancement", "tag": ["cs.CV", "cs.AI"], "abstract": "Convolutional neural networks rely on image texture and structure to serve as discriminative features to classify the image content. Image enhancement techniques can be used as preprocessing steps to help improve the overall image quality and in turn improve the overall effectiveness of a CNN. Existing image enhancement methods, however, are designed to improve the perceptual quality of an image for a human observer. In this paper, we are interested in learning CNNs that can emulate image enhancement and restoration, but with the overall goal to improve image classification and not necessarily human perception. To this end, we present a unified CNN architecture that uses a range of enhancement filters that can enhance image-specific details via end-to-end dynamic filter learning. We demonstrate the effectiveness of this strategy on four challenging benchmark datasets for fine-grained, object, scene and texture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD. Experiments using our proposed enhancement shows promising results on all the datasets. In addition, our approach is capable of improving the performance of all generic CNN architectures.", "text": "figure overview proposed uniﬁed architecture using enhancement ﬁlters improve classiﬁcation tasks. given input image instead directly applying image ﬁrst enhance image details convolving input image ﬁlter resulting improved classiﬁcation high-conﬁdence fully emulate wide range image enhancement training input target output image pairs. cnns often signiﬁcant advantage terms run-time performance. current strategy however train cnn-based image ﬁlters approximate output non-cnn counterparts. paper propose extend training cnnbased image enhancement incorporate high-level goal image classiﬁcation. contribution method jointly optimizes enhancement-approximation image classiﬁcation. achieve adaptively enhancing features image basis dynamic convolutions enables enhancement selectively enhance features leads improved image classiﬁcation. since understand critical role selective feature enhancement propose dynamic convolutional layer dynamically enhance image-speciﬁc features classiﬁcation objective. work inspired however apply dynamic convolutional module transform angle ﬁlter using input-output image pairs used terminology dynamic ﬁlters convolutional neural networks rely image texture structure serve discriminative features classify image content. image enhancement techniques used preprocessing steps help improve overall image quality turn improve overall effectiveness cnn. existing image enhancement methods however designed improve perceptual quality image human observer. paper interested learning cnns emulate image enhancement restoration overall goal improve image classiﬁcation necessarily human perception. present uniﬁed architecture uses range enhancement ﬁlters enhance image-speciﬁc details end-to-end dynamic ﬁlter learning. demonstrate effectiveness strategy four challenging benchmark datasets ﬁne-grained object scene texture classiﬁcation cub-- pascal-voc mit-indoor dtd. experiments using proposed enhancement shows promising results datasets. addition approach capable improving performance generic architectures. image enhancement methods commonly used preprocessing steps applied improve visual quality image higher level-vision tasks classiﬁcation object recognition. examples include enhancement remove effects blur noise poor contrast compression artifacts boost image details. examples enhancement methods include gaussian smoothing anisotropic diffusion weighted least squares bilateral ﬁltering. enhancement methods simple ﬁlter operations often involve complex optimization. practice time methods expensive take seconds even minutes high-resolution images. function input therefore vary sample another train/test time means presented image image enhancement done image-speciﬁc enhance texture patterns sharpen edges discrimination. speciﬁcally network learns amount various enhancement ﬁlters applied input image enhanced representation provides better performance terms classiﬁcation accuracy. proposed approach evaluated four challenging benchmark datasets bird ﬁne-grained object scene texture classiﬁcation respectively cub-- pascalvoc mit-indoor experimentally show cnns combined proposed dynamic enhancement technique consistently improve classiﬁcation performance vanilla architectures datasets. addition experiments demonstrate full capability proposed method show promising results comparison state-of-the-art. remainder paper organized follows. section overviews related work. section describes proposed enhancement architecture. experimental results analysis presented sections finally paper concluded section considerable progress seen development removing effects blur noise compression artifacts using architectures. reversing effect degradations order obtain sharp images currently active area research investigated frameworks typically build simple strategies train networks minimizing global objective function using input-output image pairs. frameworks encourage output similar structure target image. training similar approach transfer details images proposed frameworks ﬁlter specialized speciﬁc enhancement method. example learn architecture approximate existing edge-aware ﬁlters input-output image pairs. chen learn approximates end-to-end several image processing operations using parameterization deeper context-aware. learn approximating image transformations image adjustment. learn architecture remove rain streaks image. training authors rainy clean image detail layer pairs rather regular images. propose learning-based joint ﬁlter using three architectures. al.’s work sub-networks take target guidance images respectively third-network selectively transfers main content structure reconstructs desired output. remez propose fully convolutional architecture image denoising using image prior i.e. class-aware information. closest work chakrabarty chakrabarty propose architecture predict complex fourier coefﬁcients deconvolution ﬁlter applied individual image patches restoration. cnn+rnns learn enhancement ﬁlters cnns learning ﬁlters. methods produce representative ﬁlter method produce -way directional propagation ﬁlters method. like others work meant low-level vision tasks similar goal enhancement classiﬁcation. contrast prior works work differs substantially scope technical approach. goal approximate different image enhancement ﬁlters classiﬁcation objective order selectively extract informative features enhancement techniques improve classiﬁcation necessarily approximating enhancement methods. similar goal works authors also seek ameliorate degradation effects accurate classiﬁcation. dodge karam analyzed blur noise contrast compression hamper performance convnet architectures image classiﬁcation. ﬁndings showed that convnets sensitive blur blur removes textures images; noise affects performance negatively though deeper architectures performance falls slower; deep networks resilient compression distortions contrast changes. study karahan reports similar results face-recognition task. ullman showed minor changes image barely perceptible humans drastic effects computational recognition accuracy. szegedy showed applying imperceptible non-random perturbation cause convnets produce erroneous prediction. help mitigate problems costa designed separate models specialized noisy version augmented training set. improved classiﬁcation results noisy data extent. peng explored potential jointly training low-resolution high-resolution images order boost performance low-resolution inputs. similarly vasijevic al.’s work authors augment training degradations ﬁne-tune network diverse different types degraded high-quality images regain much lost accuracy degraded images. fact approach authors able learn generate degradation invariant representation hidden layers. figure dynamic enhancement ﬁlters. input network input-output image pairs well image class labels training. architecture learn single enhancement ﬁlter enhancement method individually. model operates luminance component color space. enhancement network generates dynamic ﬁlter parameters sample-speciﬁc conditioned input enhancement network overall goal improve image classiﬁcation. ﬁgure upper-right corner shows whole pipeline workﬂow. previously mentioned learn dynamic image enhancement network overall goal improve classiﬁcation necessarily approximating enhancement methods speciﬁcally. propose three architectures described section. ﬁrst architecture proposed learn single enhancement ﬁlter enhancement method endto-end fashion end-to-end mean image enhanced recognized unique deep network dynamic ﬁlters. second architecture uses pre-learned enhancement ﬁlters ﬁrst architecture combine weighted-way cnn. adaptation weights ﬁlters third architecture similar ﬁrst architecture differently show end-to-end joint learning multiple enhancement ﬁlters also combine weighted-way cnn. setups jointly optimized classiﬁcation objective selectively enhance image feature quality improved classiﬁcation. network training image-level class labels used testing input image multiple labels. section describe model learn representative enhancement ﬁlters different enhancement methods input target output enhanced image pairs end-to-end learning approach goal improve classiﬁcation performance. given input image ﬁrst transform luminance-chrominance cbcr color space. enhancement methods operates luminance component image. allows ﬁlter modify overall tonal properties sharpness image without affecting color. luminance image rh×w convolved image enhancement method resulting enhanced target output luminance image rh×w denote height width input respectively. generate target images range enhancement methods preprocessing step ﬁlter generation explicitly dataset enhancement method time learning transformation. scheme illustrated figure first stage enhancement network inspired composed convolutional fully-connected layers. enhancenet maps input ﬁlter. enhancement network takes channel luminance image outputs ﬁlters rs×s×n parameters transformation generated dynamically enhancement network ﬁlter size number ﬁlters equal single generated ﬁlter meant channel luminance image. generated ﬁlter applied input image every spatial position output rh×w. predicted image ﬁlters image-speciﬁc conditioned generating enhancement ﬁlter parameters network trained using mean squared error target image network’s predicted output image note that parameters ﬁlter obtained output enhancenet maps input ﬁlter therefore vary sample another. compare reconstruction image ideal loss measure image quality although note complex loss functions could used chrominance component recombined image transformed back found ﬁlters learned expected transformation applied correct enhancement image. figure shows qualitative results dynamically enhanced image textures. second stage enhanced images image enhancement methods original image input classiﬁcation network oneby-one sequentially class labels weights indicating importance enhancement method input image. similar last approach network parameters fully-connected layer c-way classiﬁcation layer ﬁne-tuned using pre-trained network end-to-end learning approach. end-to-end training loss network training weighted individual softmax losses term. weighted loss given weight indicating importance enhancement method original image contributing total loss whole pipeline. multiple dynamic filters classiﬁcation here recycle architectures previous section .-.. figure shows schematic layout whole architecture. architecture uses similar architecture proposed sec. differently generate dynamically ﬁlters using enhancement networks enhancement method. proposed architecture loss associated stage between predicted output images target output images computing weights enhancement method enhanced images transformed weights comparing relative strengths followed max) scaling constraint weights methods equal enhanced images smallest errors obtain highest weight vice-versa. addition also compare giving equal weights enhancement methods. weighting strategies mse-based weighting yielded best results therefore selected default. note that also include original image simply convolving identity ﬁlter similar approach weight image i.e. training weights estimated cross-validation train/validation testing phase pre-computed weights. further observed training network without regularization weights prevented model converge throughout learning overﬁtting signiﬁcant drop performance. alexnet fully-connected layers last convolutional layer classiﬁcation layer parameters fully-connected layer c-way classiﬁcation layer learned ﬁne-tuning pre-trained network. end-to-end learning stage cascade loss functions softmax-loss enables joint optimization end-to-end propagation gradients classnet enhancenet using optimizer. total loss function whole pipeline given ﬁne-tune whole pipeline convergence thus leading learned enhancement ﬁlters dynamic enhancement layer. joint optimization allows loss gradients classnet also backpropagate enhancenet making ﬁlter parameters also optimized classiﬁcation. here show integrate pre-learned enhancement ﬁlters obtained ﬁrst approach. image train obtain dynamic ﬁlter using ﬁrst approach. static ﬁlter computed taking mean dynamic ﬁlters. extracted static ﬁlters convolved input luminance component image chrominance component added image transformed back classiﬁcation network. figure shows schematic layout whole architecture. first stage begin extracting pre-trained ﬁlters image enhancement methods learned ﬁrst approach. given input luminance image ﬁlters convolved input image generate enhanced images also include identity ﬁlter generate original image learned enhancements perform worse original image itself. investigate different strategies weight enhancement methods giving equal weights value equal giving weights basis discussed sec. figure architecture pre-learned ﬁlters sec. image enhancement original image. individual softmax scores combined weighted-way cnn. adaptation weights ﬁlters. figure architecture similar sec. differently show end-to-end joint learning multiple ﬁlters. individual softmax scores combined weighted-way cnn. adaptation weights ﬁlters. evaluate proposed method four visual recognition tasks grained classiﬁcation using cub-- object classiﬁcation using pascal-voc scene recognition using mit-indoorscene texture classiﬁcation using describable textures dataset table shows details datasets. datasets standard training/validation/testing protocols provided original evaluation scheme report classiﬁcation accuracy. generate target output images enhancement methods weighted least squares ﬁlter bilateral ﬁlter image sharpening ﬁlter guided ﬁlter histogram equalization given input image end-to-end training finally extend loss approach adding term joint optimization enhancement networks classiﬁcation objective. learn parameters network jointly end-toend fashion. weighted loss sample-speciﬁc given believe training network manner offers natural encourage ﬁlters apply transformation enhances image structures accurate classiﬁcation classiﬁcation network regularized enhancement networks. moreover joint optimization helps minimize overall cost function whole architecture hence leading better results. section demonstrate enhancement ﬁltering technique four different image classiﬁcation tasks. first introduce dataset target output data generation implementation details exploring design choices proposed methods. finally test compare proposed method baseline methods current convnet architectures. note that purpose paper improve baseline performance generic architectures using add-on enhancement ﬁrst transform color space luminancechrominance color space apply enhancement methods luminance image obtain enhanced luminance image. enhanced luminance image used target image training. used default parameters imsharp histeq parameters adapted image thus requires parameter setting. comprehensive discussion refer readers source code fast publicly available others available matlab framework. matcovnet torch frameworks convnets trained titanx gpu. discuss implementation details convnet training dynamic enhancement ﬁlter networks static enhancement ﬁlters without enhancement ﬁlters classic convnet training scenario. googlenet vgg-vd vgg- bn-inception case models pre-trained imagenet ﬁne-tuned target datasets. ﬁne-tune network replace classiﬁcation layer c-way softmax layer classes target dataset. ﬁne-tuning different architectures depending dataset iterations used scheduled learning rate decrease starting small learning rate convnet architectures trained identical optimization schemes using optimizer ﬁxed weight decay scheduled learning rate decrease. follow steps ﬁne-tune whole network. first ﬁne-tune convnet architecture using images embed stat/dyn-cnn ﬁne-tuning whole network enhancement ﬁlters setting small learning rate layers except last layers high learning rate. speciﬁcally example bn-inception network requires ﬁxed input size images mean-subtracted network training. apply data augmentation cropping corners center x-axis ﬂips along color jittering network training. below provide details convnet training using bn-inception. −dynamic enhancement filters enhancement network consists learnable model parameters last fully-connected layer containing neurons i.e. ﬁlter-size initialize enhancement networks model parameters randomly except last fully-connected layer initialized regress identity transform suggested initialize learning rate decrease factor every iterations. maximum number iterations terms computation speed training enhancement network along bn-inception takes approx. training time network convergence comparison bn-inception approach enhancement networks generating enhancement ﬁlters approach also include original image too. −without enhancement filters similar classical convnets ﬁne-tuning scenario replace last classiﬁcation layer pre-trained model c-way classiﬁcation layer ﬁne-tuning. fully connected layers classiﬁcation layer ﬁne-tuned. initialize learning rate decrease factor every iterations. maximum number iterations −static enhancement filters similar fc-cnn here enhanced images static ﬁlters original image input input convnets network training. practice static ﬁlters image enhancement low-complex operations. optimization scheme used fc-cnn. static learned ﬁlters approach testing previously mentioned input image transformed luminance-chrominance color space luminance image convolved enhancement ﬁlter leading enhanced luminance image. chrominance recombined enhanced luminance image image transformed back rgb. convnet testing input frame either image enhanced image using static dynamic ﬁlters network. total enhanced images original image network sequentially. ﬁnal image label prediction predictions images combined weighted pre-computed weights obtained dyn-cnn. section cub-- dataset test explore design choices proposed method ﬁnally compare method baseline methods current methods. dataset ﬁne-grained bird species classiﬁcation dataset. dataset contains bird species images. dataset measure accuracy predicting class image. ablation study explore four aspects proposed method impact different ﬁlter size; impact enhancement method separately; impact weighting strategies; impact dif−filter size experiment explore three different ﬁlter sizes. speciﬁcally implement enhancement network convolutional fully-connected layers last-layer containing neurons neurons neurons literature exploited insights good ﬁlter size. ﬁlter-size determines receptive ﬁeld application dependent. found ﬁlter-size produce smoother images thus dropped classiﬁcation performance approx. comparison ﬁlter-size similar case ﬁlter-size correct enhancement transferred leading drop performance approx. found ﬁlter-size learned expected transformation applied correct enhancement input image sharper preserved edges. −enhancement method here compare performance individual enhancement method three aspects employ alexnet pre-trained imagenet ﬁnetuned groundtruth enhancement method separately using pre-trained alexnet model ﬁnetune whole model gt-ems setting small learning rate layers except last layers high learning rate. slightly improves performance pre-trained model small margin. similar ﬁnetune whole model using approach dynamic enhancement approach improves performance margin ∼-.% comparison generic network ﬁnetuned images only. table summarize results. fig. example show qualitative results difference textures enhancement method extracts gt-ems primarily responsible improving classiﬁcation performance. −weighting strategies combining enhancement methods late-fusion averaging scores gives improvements shown table observation realized effective weighting strategy applied importance could given better methods combining. evaluation explore weighting strategies giving equal weights value equal i.e. weight computed basis estimated cross-validation training shown table table clearly shows weighting adds positive regularization effect. found training network regularization mse-loss prevents classiﬁcation objective divergence throughout learning. table shows dyn-cnn weight enhancement ﬁlter relates well individual performance shown table observe mse-based weighting performs best. therefore choose default weighting method. −convnet architectures here compare different convnet architectures. speciﬁcally compare alexnet googlenet bn-inception among architectures shown table bn-inception exhibits best performance terms classiﬁcation accuracy comparison others. therefore choose bninception default architecture experiment. results table explore static dynamic cnns current methods. consider bn-inception using step ﬁne-tuning scheme stat-cnn dyn-cnn. notice that dyn-cnn improves generic bn-inception performance using image enhancement. enhancenet takes figure qualitative results cub. comparison target image enhanced luminance image compliment difference image obtained using approach enhancement methods. constant time generate enhanced images altogether comparison generating groundtruth target images time-consuming takes seconds image/method testing time whole model enhancenet plus classnet further extend baseline ×st-cnn include static dynamic ﬁlters immediately following input weighted loss. reference st-cnn work evaluate methods keeping training evaluation setup fair comparison. results indicate dyn-cnn improves performance furthermore stat-cnn static ﬁlters competitive performs better ×st-cnn means static ﬁlters dropped network perform explicit enhancement features thus gains accuracy expected convnet architecture. object classiﬁcation dataset pascal-voc dataset contains object classes images contain total annotated objects. dataset report mean average precision averaged classes. results table show results. dyn-cnn ./.% better stat-cnn/fc-cnn using alexnet ./.% using vgg-. observe smaller network alexnet shows improvement performance comparison deeper vgg- network. also stat-cnn ./.% better fc-cnn using alexnet/vgg-. furthermore bilen performs lower dyn-cnn using vgg-. additional experiment used subset enhancement methods dyncnn using alexnet. setup underperforms wrt. using methods together. indoor scene recognition dataset mit-indoor scene dataset contains total indoor scene categories images. dataset measure accuracy predicting class image. results table show results. expected previously observed dyn-cnn ./.% better stat-cnn/fc-cnn using alexnet ./.% ustable performance comparison table compares fc-cnn stat-cnn dyn-cnn alexnet networks trained imagenet ﬁne-tuned target datasets using standard training testing sets. texture classiﬁcation dataset describable texture datasets contains describable attributes images. dataset report averaged classes. results table show results. story similar previous observation dyn-cnn outperforms stat-cnn fc-cnn signiﬁcant margin. surprisingly interesting dyn-cnn shows signiﬁcant improvement ./.% comparison fccnn using alexnet/vgg-vd. paper propose uniﬁed architecture emulate range enhancement ﬁlters overall goal improve image classiﬁcation end-toend learning approach. demonstrate framework four benchmark datasets pascal-voc cub- mit-indoor scene describable textures dataset. addition improving baseline performance vanilla architectures datasets method shows promising results comparison state-of-theart using static/dynamic enhancement ﬁlters. also enhancement ﬁlters used existing networks perform explicit enhancement image texture structure features giving cnns higher quality features learn from turn lead accurate classiﬁcation. believe work opens many possibilities further exploration. future work plan investigate enhancement methods well complex loss functions appropriate image enhancement tasks.", "year": 2017}