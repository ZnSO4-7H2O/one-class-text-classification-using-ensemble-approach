{"title": "Adversarial Images for Variational Autoencoders", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target's. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.", "text": "investigate adversarial attacks autoencoders. propose procedure distorts input image mislead autoencoder reconstructing completely different target image. attack internal latent representations attempting make adversarial input produce internal representation similar possible target’s. autoencoders much robust attack classiﬁers examples tolerably small input distortion reasonable similarity target image quasi-linear trade-off aims. report results mnist svhn datasets also test regular deterministic autoencoders reaching similar conclusions cases. finally show usual adversarial attack classiﬁers much easier also presents direct proportion distortion input misdirection output. proportionality however hidden normalization output maps linear layer non-linear probabilities. adversarial attacks expressly optimize input fool models e.g. image classiﬁcation adversarial input visually tantamount ordinary original image leads mislabelling high conﬁdence. here explore adversarial images autoencoders models optimized reconstruct inputs compact internal representations. autoencoder attack targets single label whole reconstruction. contributions include adversarial attack variational comparison deterministic autoencoders. attack aims disturbing reconstruction fooling autoencoder reconstructing completely different target image; comparison attacks autoencoders classiﬁers showing former much harder cases amount distortion input proportional amount misdirection output. classiﬁers however proportionality hidden normalization output maps linear layer non-linear probabilities. evaluating generative models hard clear-cut success criteria autoencoder reconstruction therefore neither attack. attempt bypass difﬁculty analyzing inputs outputs differ across varying regularization constants. seminal article szegedy introduced adversarial images showing force deep network misclassify image applying nearly imperceptible distortions. goodfellow exploited linear nature deep convolutional networks attempt explaining adversarial samples arise propose much faster technique create them. tabacof valle explored geometry adversarial regions showing appear relatively dense regions input space shallow simple classiﬁers tend robust them. existence adversarial images lead interesting questions signiﬁcance even usefulness. training models resist adversarial attacks advanced form regularization used autoencoders pre-process input reinforce network adversarial attacks ﬁnding although cases resistance improved attacks small distortions remained possible. recent trend training adversarial models attempts generate artiﬁcial samples attempts recognize samples makhzani employ scheme train autoencoder. although autoencoders appear literature adversarial images attempt obtain robustness attacks literature adversarial training models trained technique unaware attempts create attacks targeted them. closest related literature sara sabour show adversarial attacks lead mislabelling also manipulate internal representations network. paper show analogous manipulation allows attack autoencoders remain much resistant classiﬁers attacks. autoencoders models input compact latent representation then representation build back input therefore autoencoders trained minimize distortion input output plus regularization terms. model comprises parts encoder maps input latent representation; decoder maps representation output close input possible. regular autoencoders training loss function simple -distance input output. famous variants include sparse autoencoders -regularization denoising autoencoders implicit regularization feeding noise input keeping original input reconstruction loss term important offshoot models similar encoder–decoder structure seek reconstruct input produce output related modern variant growing popularity variational autoencoders interpret latent representation bayesian lens thus offering theoretical foundation reconstruction regularization objectives. variational autoencoders probabilistic generative models probability distribution data marginalizing latent variables likelihood probabilistic explanation observed data practice often simply output decoder network noise consideration subscript comprises decoder parameters latent representation marginalize. representation prior often standard normal might instead discrete distribution even distribution geometric interpretation since integration often intractable maximize variational lower bound... ...which kullback–leibler divergence approximate exact posterior. thus maximizing variational lower bound also interpreted ﬁnding best posterior approximation. context variational autoencoders approximate posterior usually uncorrelated multivariate normal determined encoder network approximate likelihood expectation eqφ] monte carlo. prior approximated posterior normal distributions divergence analytic form reparameterization trick reduce variance gradient estimator encoder decoder neural network multilayer perceptron convolutional network even lstms. latter recent development recurrent variational autoencoders soft attention encode decode patches input image simulating chain samples latent variables likelihood allows denoise images impute missing data latent variables variational autoencoder also allow visual analogy interpolation adversarial procedures minimize adversarial loss mislead model distorting input little possible. attack successful humans hardly able distinguish adversarial regular inputs even strict allow distortion input quantization noise build adversarial images classiﬁcation maximize misdirection towards certain wrong label away correct distortion minimized constrained small finally often requires images stay within valid space autoencoders single class output misclassify instead whole image output scramble. attack attempts mislead reconstruction slightly altered image enters autoencoder reconstruction wrecked attack worked. dramatic attack attempt paper would change slightly input image make autoencoder reconstruct completely different valid image attack consists selecting original image target image feeding network original image added small distortion optimized output close target image possible attempts attack output directly failed minimizing distance target succeeded blurring reconstruction. autoencoders reconstruct latent representation attack instead. latent layer information bottleneck autoencoder thus particularly convenient attack. used following adversarial optimization figure adversarial attacks autoencoders distortions input aiming making autoencoder reconstruct different target. attack latent representation attempting match target image’s. bounds input space; regularizing constant balances reaching target limiting distortion. must choose function compare representations. regular autoencoders simple distance sufﬁced; however variational autoencoders kl-divergence distributions induced latent variables worked better also offered sounder justiﬁcation. variational autoencoders uncorrelated multivariate normal distributions parameters given encoder representation mean vector covariance matrix output last layer encoder network; autoencoder parameters learned previously training ordinary task reconstruction. entire adversarial procedure remains ﬁxed. worked binarized mnist svhn datasets former allows fast experiments controlled conditions; latter still allowing manage large number experiments provides much noise variability. following literature modeled pixel likelihoods independent bernoullis independent normals used parmesan lasagne implementation. loss function train variational autoencoder expectation likelihood approximated posterior plus divergence approximated posterior prior. approximate expectation likelihood sample posterior. extract gradients lower bound using automatic differentiation maximize using stochastic gradient ascent adam algorithm used latent variables mnist svhn respectively. parameterized encoder decoder fully-connected networks mnist case convolutional deconvolutional networks svhn case. training done autoencoder reconstruct image samples latent variables learned representation images. example pair input image/reconstructed output appears fig. classiﬁcation tasks regularization term chosen bisection smallest constant still leads success autoencoders complicate choice longer binary criterion success. goodfellow sabour optimize differently choosing ∞-norm constrained make distortion imperceptible maximizing misdirection. found solution restrictive leading reconstructions visually distinct target images. solution instead forgo single choice analyze behavior system throughout series values. experiments pick random pairs original/target images pair span different values regularization constant logarithmic scale measuring -distance adversarial input original image -distance reconstructed output target image distortion axis normalized -distance original target images pair adversarial−target normalized -distance reconstruction target target -distance reconstruction original target geometry normalization illustrated colored lines graphs fig. variational autoencoders reconstruction stochastic therefore data point sampled times average reported. comparison purposes protocol generate range adversarial images usual classiﬁcation tasks datasets. contrast behavior adversarial attacks across tasks experiments pick pairs original image adversarial class varying measure distortion above probability attributed adversarial original classes axes longer normalized center distortion axis transition point attack failure success point blue lines cross. found generating adversarial images autoencoders much harder task classiﬁers. apply little distortion reconstructions stay essentially untouched. reconstructions close target’s apply heavy distortions input. however hand-tuning regularization parameter possible trade-offs reconstruction approaches target’s adversarial image still resemble input plots full original/target image pairs appear fig. series saturate latent representation adversarial image essentially equals target’s. saturation appears well upper distortion limit provides measure resistant model attack variational autoencoders appear slightly resistant deterministic autoencoders mnist much resistant svhn. latter surprising since large complex models seem general susceptible adversarial attacks. hinge attack saturates quasi-linear trade-off input distortion output similarity target combinations dataset autoencoder choice. initially hoping non-linear behavior sudden drop point scale data suggests give-and-take attacking autoencoders gain attack requires proportional increase distortion. comparison attacks classiﬁers showed beginning much different behavior contrasted probability attributed adversarial class distortion imposed input observed non-linear sudden change expecting question remained however whether non-linearity intrinsic whether highly non-linear nature probability scale. answer appears right column fig. where logit transformation probabilities linear behavior appears again. seems attack classiﬁers show internally linear give-and-take present autoencoders normalization outputs last layer valid probabilities aids attack changes input lead proportional changes logit much larger changes probability. makes feasible attack classiﬁers much better figure mnist. bottom svhn. ﬁgures left show trade-off between quality adversarial attack adversarial distortion magnitude changing regularization parameter ﬁgures right correspond points shown graphs illustrating adversarial images reconstructions using fully-connected convolutional variational autoencoders sweet spots attack autoencoders goodfellow suggested linearity deep models make susceptible adversarial attacks. results seems reinforce linearity plays indeed critical role internal success attack proportional distortion inputs. classiﬁcation networks however essentially piecewise linear last layer non-linearity latter seems compound problem. proposed adversarial method attack autoencoders evaluated robustness attacks. showed linear trade-off much adversarial input similar original input much adversarial reconstruction similar target reconstruction frustrating hope small change input could lead drastic changes reconstruction. surprisingly linear trade-off also appears adversarial attacks classiﬁcation networks undo non-linearity last layer. future intend extend empirical results datasets larger inputs complex networks well different autoencoder architectures. example draw variational autoencoder uses feedback reconstruction error improve reconstruction thus could robust attacks. also interested advancing theoretical explanations illuminate results. thank brazilian agencies capes cnpq fapesp ﬁnancial support. gratefully acknowledge support nvidia corporation donation tesla used research. eduardo valle partially supported google awards latam grant cnpq grant figure plots whole experiments mnist svhn. variational autoencoders bottom deterministic autoencoders line graph corresponds experiment adversarial images single pair original/target images varying regularization parameter distortion adversarial−target axes show tradecost success. hinge lines saturate show point reconstruction essentially equal target’s distortion hinge measures resistance attack. figure examples classiﬁcation attacks. mnist. bottom svhn. left probabilities. middle logit transform probabilities. right images illustrating intersection point curves. adversarial class mnist svhn. curve shows probability/logit adversarial class blue curve shows original class point curves cross transition point failure success attack. figure plot whose experiments classiﬁers. mnist. bottom svhn. left probabilities. right logit transform probabilities. experiment corresponds graphs shown fig. centered make crossing point blue lines stay distortion axis.", "year": 2016}