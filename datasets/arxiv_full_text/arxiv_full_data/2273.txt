{"title": "Multi-focus Attention Network for Efficient Deep Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Deep reinforcement learning (DRL) has shown incredible performance in learning various tasks to the human level. However, unlike human perception, current DRL models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input. Because of this difference, DRL needs vast amount of experience samples to learn. In this paper, we propose a Multi-focus Attention Network (MANet) which mimics human ability to spatially abstract the low-level sensory input into multiple entities and attend to them simultaneously. The proposed method first divides the low-level input into several segments which we refer to as partial states. After this segmentation, parallel attention layers attend to the partial states relevant to solving the task. Our model estimates state-action values using these attended partial states. In our experiments, MANet attains highest scores with significantly less experience samples. Additionally, the model shows higher performance compared to the Deep Q-network and the single attention model as benchmarks. Furthermore, we extend our model to attentive communication model for performing multi-agent cooperative tasks. In multi-agent cooperative task experiments, our model shows 20% faster learning than existing state-of-the-art model.", "text": "example assume grid world navigation task using agent receives image representing whole grid input cell grid rendered small image. agent visits waypoints orderly sequence. images positions different relative positions same. therefore model attend learn task faster applying policy. however perceives images figure completely different states. therefore learn policy every possible placement entities independently. number possible placement entities grows exponentially number entities. thus requires enormous experience samples learn training often intractable real world environment agent cannot expect experience exactly scene repetitively. recently overcome limitation works like applied attention concepts models order learn atari minecraft maze task respectively. however attention mechanisms used softmax layer. hence models show significant difficulty attending multiple entities equal importance whereas humans deep reinforcement learning shown incredible performance learning various tasks human level. however unlike human perception current models connect entire low-level sensory input state-action values rather exploiting relationship among entities constitute sensory input. difference needs vast amount experience samples learn. paper propose multi-focus attention network mimics human ability spatially abstract low-level sensory input multiple entities attend simultaneously. proposed method first divides low-level input several segments refer partial states. segmentation parallel attention layers attend partial states relevant solving task. model estimates state-action values using attended partial states. experiments manet attains highest scores significantly less experience samples. additionally model shows higher performance compared deep q-network single attention model benchmarks. furthermore extend model attentive communication model performing multi-agent cooperative tasks. multi-agent cooperative task experiments model shows faster learning existing state-of-the-art model. deep reinforcement learning shown incredible performance learning various tasks human level. deep q-network successfully learned play atari games achieved human-level scores many games zhang used robotic control attaining high performance results. however connects entire low-level sensory input state-action values rather exploiting relationship inter intra entities constitute sensory input. ability attend multiple entities simultaneously paper propose multi-focus attention network enhances agent’s ability attend important entities using multiple parallel attentions. model segments low-level sensory input multiple segments refer partial states. parallel attention layers attend partial states relevant solving task. attended partial states estimate stateaction values. experiments model shows remarkably faster learning speed higher performance single attention model. furthermore viewing partial states partially observed states perceived multiple agents extend model multi-agent reinforcement learning model. multi-agent cooperation task model learns roughly faster state-of-the-art model deep reinforcement learning deep neural networks successful supervised learning tasks like image classification recently proposed deep q-network used deep convolutional neural network state-action value function approximation. learned many atari games humanlevel end-to-end fashion. zhang applied robotic control trained robot using simulation images. however training requires long time enormous experiences making direct application real-world environment intractable. many works followed accelerate learning speed dqn. mnih proposed asynchronous learning methods nair proposed massively parallel methods. methods boosted learning speed greatly. however methods focused exploiting massive computing powers rather improving model’s efficiency. cognitive science developmental psychology literatures evidence human cognitive system found core knowledge entities agents numbers spaces attention considered important perceiving coherent entities agents. furthermore evidence humans ability attend multiple entities simultaneously work inspired insights focusing simultaneous attention fast efficient learning. attention deep reinforcement learning recently visual attention successful many supervised learning tasks like caption generation image generation language translation following achievements supervised learning domain sorokin proposed deep attention recurrent q-network integrated attention mechanism dqn. darqn surpassed performance atari games. however failed surpass many atari games. proposed memory q-network used attention context dependent memory retrieval. outperformed large margin maze tasks require ability memorize important information long time-steps. contrast works attention extract important partial states input time-step apply multiple attention layers attend multiple partial states fast efficient learning multi-agent communication many multi-agent reinforcement learning models communication agents features used determine model attend value features used encode information state-action value estimation. stream feature extraction inspired partial state first define common feature follows number partial states common feature i-th partial state extraction function i-th partial state. experiments used deep convolutional neural network deep neural network additionally concatenated index partial state common feature. using extracted define value feature follows feature i-th partial state value feature i-th partial state non-linear activation function weight matrixes. leaky relu parallel attentions using features extracted feature extraction module parallel attention layers determine partial states important using following equations i-th element n-th soft attention weight vector n-th selector vector trainable like weights network. randomly initialized beginning training. since multiple initialized similarly inefficient multiple attend partial states explore regularization methods entropy distance encourage attends different partial states. entropy regularization encourages attention layer attend partial state. distance regularization encourages attention layer attend different partial states attention layers. regularization terms added loss function usually pre-determined recently sukhbaatar szlam fergus proposed communication neural commnn learns policy individual agents also communication protocol agents end-to-end fashion. commnn outperformed noncommunicative models baselines various multiagent cooperative tasks. contrast commnn model encodes different feature. estimates stateaction values used communication between agents. furthermore agents attend agents important information. attributes enable clear rich communication agents leading faster learning. background deep learning deep q-learning framework introduced mnih deep q-learning uses neural network approximate state-action value function. lowlevel sensory input time step discrete action reward loss function deep qlearning framework defined follows discount coefficient possible actions chosen neural network target network. target network used stabilize learning. structure network weights network copied target network periodically. also replay memory mechanism stores transitions uses randomly sampled transitions train network. single agent setting model consists modules input segmentation feature extraction parallel attentions state-action value estimation. input segmentation input segmentation module segments low-level sensory input multiple segments refer partial states. done various methods. experiments used simple grid method. partitioned input image uniform grid used cells grid partial states. believe apply sophisticated methods like super-pixel segmentation spatial transformer networks explore feasibility future works. feature extraction feature extraction module extracts value features partial states. formulation agent agent higher agent information important agent state-action value estimation multi-agent setting agents action need state-action values agent. agent communication feature concatenated feature state-action value estimation defined respectively entropy distance regularization multi agent setting like single agent setting different agents unlikely similar values. instead regularization term designed game environments single agent task multi-agent cooperative task. trained model well state-of-the-art model baselines environments. following subsections cover details environments results experiments. navigation experiments single agent task environment single agent task designed navigation task grid world waypoints grid waypoints. agent visit waypoints orderly sequence. weighted respectively. investigate effects regularizations detail experiment section. state-action value estimation using attention weights parallel attention layers weighted value feature defined function approximation. experiments used fully-connected layer followed linear layer output units action possible actions calculated single feed-forward pass. extension multi-agent reinforcement learning multi-agent reinforcement learning tasks agents perceive partially observed states part whole environment state. agents communicate share information cooperate solve task. partially observed states perceived multiple agents partial states model manet extended model control multiple agents. using parallel attentions manet learn policy individual agents also communication protocol agents end-to-end fashion. purpose change modules model follows input segmentation mentioned above partially observed states multiple agents partial states model. thus extra segmentation process required. feature extraction multi-agent setting agent able attend different agents according situation. thus agent requires different selector vectors. purpose extract features selector vectors inputs agent. equation extended follows partial state significant difference performances. hypothesize least domain experiments random initialization enough make attention layers attend different partial states. leave exploration regularization methods investigation effects regularizations domains future work. combat experiments multi-agent task environment compare model’s performance baselines state-of-the-art model implemented modified version combat environment used combat experiments teams battle grid. team five agents. team fixed starting point agents spawned randomly surrounding area episode begins. agent health points perceive surrounding area. agents take actions moving down left right attacking enemy specifying index nothing. attack effective target attacker’s surrounding area successful attack reduces target’s health point agent dies health point becomes agents must time-step cool time attack. episode ends team wins time-steps passed model controls team team controlled hard-coded bots. like hard-coded bots move randomly bots spots enemy. bots spots enemy bots move nearest spotted enemy attack model gets reward time-step agent move down left right. agent gets reward reaches proper waypoint visited waypoints movement. episode ends agent reaches last waypoint time-steps passed. cell grid rendered patch size input image cell contains agent colored green cells contain waypoints colored red. waypoints visited earlier darker color. training details trained model single attention model dqn. model used attention layers. details network structures hyper parameters given supplementary material. quantitative results shown learning curves presented figure model showed great improvement training speed performances. model reached near-optimal score roughly times faster single attention model result shows parallel attentions clearly benefits learning complex task. reached near optimal score three training cases. furthermore comparing successful case model reached near-optimal score nearly times faster qualitative results effect regularization examples game screenshots corresponding attention weights proposed model depicted figure illustrated figure every case trainings attention layer mainly attended next waypoint mainly attended agent. also examined attention weights learning curves model regularizations. seen supplementary material although attention weights regularized model concentrated wins game gets reward loses draws encourage agents attack accelerate learning reward given every successful attack reward given model loses draws. rewards shared agents. agent’s input -dimensional vector concatenation twenty-five -dimensional vectors encode cells agent’s surrounding area. agent cell values index team health cool-time training details trained model commnn no-communication model dense communication model details network structures hyper parameters given supplementary material. quantitative results shown learning curves figure model showed roughly faster learning speed commnn though could compare performances accurately model commnn reached near-optimal scores model showed slightly better performances. figure game screens visualization attention matrixes combat task. cell attention matrixes attention weight agent agent brighter cell means agent mainly attends agent qualitative results examples game screenshots corresponding attention weights model presented figure shown figure attention weights concentrated agents engaging enemies helped agents successfully defeat enemies focusing fire power enemy time propose multi-focus attention network effectively attend multiple partial states constituting sensory input learn faster. model shows improved performance faster learning compared single attention model single agent task. moreover extended model multi-agent shows faster learning state-of-the-art model. future work intend apply sophisticated segmentation methods experiment model real world environment. secondly explore communication agents different state-action spaces. lastly apply parallel attention layers spatial temporal manner. final goal achieve modular model robot training real world. sukhbaatar szlam fergus learning multiagent communication backpropagation. arxiv preprint arxiv.. kiros courville salakhutdinov zemel bengio show attend tell neural image caption generation visual attention. arxiv preprint arxiv. zhang leitner milford upcroft corke towards vision-based deep reinforcement learning robotic motion control. paper presented australasian conference robotics automation", "year": 2017}