{"title": "Classifying Relations by Ranking with Convolutional Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.", "text": "relation classiﬁcation important semantic processing task state-ofthe-art systems still rely costly handcrafted features. work tackle relation classiﬁcation task using convolutional neural network performs classiﬁcation ranking propose pairwise ranking loss function makes easy reduce impact artiﬁcial classes. perform experiments using semeval- task dataset designed task classifying relationship nominals marked sentence. using crcnn outperform state-of-the-art dataset achieve without using costly handcrafted features. additionally experimental results show that approach effective followed softmax classiﬁer; omitting representation artiﬁcial class improves precision recall; using word embeddings input features enough achieve state-of-the-art results consider text target nominals. relation classiﬁcation important natural language processing task normally used intermediate step many complex applications question-answering automatic knowledge base construction. since last decade increasing interest applying machine learning approaches task reason availability benchmark datasets semeval- task dataset encodes task classifying relationship between nominals marked sentence. following sentence contains example component-whole relation nominals introduction book. recent work relation classiﬁcation focused deep neural networks reducing number handcrafted features however order achieve state-ofthe-art results approaches still features derived lexical resources wordnet tools dependency parsers named entity recognizers work propose convolutional neural network name classiﬁcation ranking tackle relation classiﬁcation task. proposed network learns distributed vector representation relation class. given input text segment network uses convolutional layer produce distributed vector representation text compares class representations order produce score class. propose pairwise ranking loss function makes easy reduce impact artiﬁcial classes. perform extensive number experiments using semeval- task dataset. using crcnn without need costly handcrafted feature outperform state-of-the-art dataset. experimental results evidence that cr-cnn effective followed softmax classiﬁer; omitting representation artiﬁcial class improves precision recall; using word embeddings input features enough achieve state-of-the-art results consider text target nominals. word embeddings ﬁrst layer network transforms words representations capture syntactic semantic information words. given sentence consisting words every word converted real-valued vector rwn. therefore input next layer sequence real-valued vectors embx rwn} word representations encoded column vectors embedding matrix rdw×|v ﬁxed-sized vocabulary. column corresponds word embedding i-th word vocabulary. transform word word embedding using matrix-vector product target nouns normally comes words close target nouns. zeng propose word position embeddings help keeping track close words target nouns. features similar position features proposed collobert semantic role labeling task. work also experiment word position embeddings proposed zeng derived relative distances current word target noun noun. instance sentence shown figure relative distances left plant respectively. relative distance mapped vector dimension dwpe initialized random numbers. dwpe hyperparameter network. given vectors word respect targets noun noun position embedding given remainder paper structured follows. section details proposed neural network. section present details setup experimental evaluation describe results section section discuss previous work deep neural networks relation classiﬁcation tasks. section presents conclusions. given sentence target nouns cr-cnn computes score relation class class network learns distributed vector representation encoded column class embedding matrix classes. detailed figure input network tokenized text string sentence. ﬁrst step cr-cnn transforms words realvalued feature vectors. next convolutional layer used construct distributed vector representations sentence finally cr-cnn computes score relation class performing product sentence. ﬁxed-sized distributed vector representation sentence obtained using word windows. matrix vector parameters learned. number convolutional units size word context window hyperparameters chosen user. important note corresponds size sentence representation. classes embedding matrix whose columns encode distributed vector representations different class labels column vector contains embedding class note number dimensions class embedding must equal size sentence representation deﬁned embedding matrix classes parameter learned network. initialized randomly sampling value uniform distribution training procedure network trained minimizing pairwise ranking loss function training input training round sentence different class labels correct class label not. sθy+ sθc− respectively scores class labels generated network parameter propose logistic loss function scores order train cr-cnn margins scaling factor magniﬁes difference score margin helps penalize prediction errors. ﬁrst term right side equation decreases score sθy+ increases. second term right sentence representation next step consists creating distributed vector representation input sentence main challenges step sentence size variability fact important information appear position sentence. recent work convolutional approaches used tackle issues creating representations text segments different sizes characterlevel representations words different sizes here convolutional layer compute distributed vector representations sentence. convolutional layer ﬁrst produces local features around word sentence. then combines local features using operation create ﬁxed-sized vector input sentence. given sentence convolutional layer applies matrix-vector operation window size successive windows embx rwn}. deﬁne vector rdwk concatenation sequence word embeddings centralized n-th word side decreases score sθc− decreases. training cr-cnn minimizing loss function equation effect training give scores greater correct class scores smaller incorrect classes. experiments regularization adding term equation experiments stochastic gradient descent minimize loss function respect like ranking approaches update classes/examples every training round efﬁciently train network tasks large number classes. advantage softmax classiﬁers. hand sampling informative negative classes/examples signiﬁcant impact effectiveness learned model. case loss function informative negative classes ones score larger −m−. number classes relation classiﬁcation dataset experiments small. therefore experiments given sentence class label incorrect class choose perform step highest score among incorrect classes c=y+ tasks number classes large number negative classes considered example select largest score perform gradient step. approach similar used weston select negative examples. special treatment artiﬁcial classes work consider class artiﬁcial used group items belong actual classes. example artiﬁcial class class semeval relation classiﬁcation task. task artiﬁcial class used indicate relation nominals belong nine relation classes interest. therefore class noisy since groups many different types important characteristic cr-cnn makes easy reduce effect artiﬁcial classes omitting embeddings. embedding class label omitted means embedding matrix classes contain column vector main beneﬁts strategy learning process focuses natural classes only. since embedding artiﬁcial class omitted inﬂuence prediction step i.e. cr-cnn produce score artiﬁcial class. experiments semeval- relation classiﬁcation task training sentence whose class label other ﬁrst term right side equation zero. prediction time relation classiﬁed actual classes negative scores. otherwise classiﬁed class largest score. dataset evaluation metric semeval- task dataset perform experiments. dataset contains examples annotated different relation types artiﬁcial relation other used indicate relation example belong nine main relation types. nine relations cause-effect component-whole content-container entitydestination entity-origin instrument-agency member-collection message-topic productproducer. example contains sentence marked nominals task consists predicting relation nominals taking consideration directionality. means relation causeeffect different relation causeeffect shown examples below. information dataset found semeval- task dataset already partitioned training instances test instances. score systems using semeval- task ofﬁcial scorer computes macro-averaged f-scores nine experiments discussed section assess impact using word position embeddings also propose simpler alternative approach almost effective wpes. main idea behind wpes relation classiﬁcation task give hint convolutional layer close word target nouns based assumption closer words impact distant words. hypothesize information needed classify relation appear target nouns. based hypothesis perform experiment input convolutional layer consists word embeddings word sequence correspond positions ﬁrst second target nouns respectively. table compare results different cr-cnn conﬁgurations. ﬁrst column indicates whether full sentence used whether text span target nouns used second column informs wpes used not. clear wpes essential full sentence used since jumps effect wpes reported hand using text span between target nouns impact much smaller. strategy achieve using word embeddings input result good previous state-of-the-art reported semeval task dataset. experimental result also suggests that task works better short texts. word embeddings used experiments initialized means unsupervised pre-training. perform pre-training using skip-gram architecture available wordvec tool. december snapshot english wikipedia corpus train word embeddings wordvec. preprocess wikipedia text using steps described removal paragraphs english; substitution non-western characters special character; tokenization text using tokenizer available stanford tagger removal sentences less characters long less tokens. lowercase words substitute numerical digit resulting clean corpus contains billion tokens. -fold cross-validation tune neural network hyperparameters. learning rates range give relatively similar results. best results achieved using between training epochs depending cr-cnn conﬁguration. table show selected hyperparameter values. additionally learning rate schedule decreases learning rate according training epoch learning rate epoch computed using equation experiment assess impact omitting embedding class other. mentioned above class noisy since groups many different infrequent relation types. embedding difﬁcult deﬁne therefore brings noise classiﬁcation process natural classes. table present results comparing omission embedding class other. ﬁrst lines results present ofﬁcial take account results class other. omitting embedding class precision recall classes improve results increase results suggest strategy cr-cnn avoid noise artiﬁcial classes effective. softmax classiﬁer. tune parameters cnn+softmax using -fold cross-validation training set. compared hyperparameter values cr-cnn presented table difference cnn+softmax number convolutional units table compare results crcnn cnn+softmax. cr-cnn outperforms cnn+softmax precision recall improves third line table shows result reported zeng word embeddings wpes used input network believe word embeddings employed main reason result much worse cnn+softmax. word embeddings size word embeddings size trained using much less unlabeled data did. last lines table present results class other. note recall cases classiﬁed remains precision signiﬁcantly decreases embedding class used. means cases natural classes classiﬁed other. however precision recall natural classes increase cases classiﬁed must cases also wrongly classiﬁed embedding class used. cr-cnn versus cnn+softmax section report experimental results comparing cr-cnn cnn+softmax. order fair comparison we’ve implemented cnn+softmax trained data word embeddings wpes used cr-cnn. concretely cnn+softmax consists getting output convolutional layer vector figure giving input comparison state-of-the-art table compare cr-cnn results results recently published semeval- task dataset. rink harabagiu present support vector machine classiﬁer rich feature set. obtains best result semeval- task socher present results recursive neural network employs matrix-vector representation every node parse tree order compose distributed vector representation complete sentence. method named matrix-vector recursive neural network achieves wordnet features used. authors present results cnn+softmax classiﬁer employs lexical sentencelevel features. classiﬁer achieves adding handcrafted feature based wordnet. present factorgram entity-origin away reverse direction relation entityorigin origin-entity source informative trigram. results step towards extraction meaningful knowledge models produced cnns. years various approaches proposed relation classiﬁcation treat multiclass classiﬁcation problem apply variety machine learning techniques task order achieve high accuracy. recently deep learning become attractive area multiple applications including computer vision speech recognition natural language processing. among different deep learning strategies convolutional neural networks successfully applied different task part-of-speech tagging sentiment analysis question classiﬁcation semantic role labeling hashtag prediction sentence completion response matching recent work deep learning relation classiﬁcation include socher zeng authors tackle relation classiﬁcation using recursive neural network assigns matrix-vector representation every node parse tree. representation complete sentence computed bottom-up recursively combining words according syntactic structure parse tree method named matrix-vector recursive neural network zeng propose approach relation classiﬁcation sentence-level features learned word embedding position features input. parallel lexical features extracted according given nouns. sentence-level lexical features concatenated single vector softmax classiﬁer prediction. approach achieves state-of-the-art performance semeval- task dataset. based compositional embedding model achieves deriving sentencelevel substructure embeddings word embeddings utilizing dependency trees named entities. last line table crcnn using full sentence word embeddings wpes outperforms previous reported results reaches state-of-the-art remarkable result since complicated features depend external lexical resources wordnet tools named entity recognizers dependency parsers. table cr-cnn also achieves best result among systems word embeddings input features. closest result produced system points behind cr-cnn result table relation type present trigrams test contributed scoring correctly classiﬁed examples. remember cr-cnn given sentence score class computed order compute repr resentative trigram sentence trace back position trigram responsible trigram compute particular contribution score summing terms score positions trace back representative trigram largest contribution improvement score. order create results presented table rank trigrams selected representative sentence decreasing order contribution value. trigram appears largest contributor sentence contribuition value becomes contribution sentence. table classes trigrams contributed increase score indeed informative regarding relation type. expected different trigrams play important role depending direction relation. instance informative trifeature preﬁxes morphological wordnet dependency parse levin classes probank framenet nomlex-plus google n-gram paraphrases textrunner word embeddings word embeddings wordnet word embeddings word embeddings wordnet word embeddings word embeddings word position embeddings word pair words around word pair wordnet word embeddings word embeddings dependency parse word embeddings word embeddings word position embeddings positional embedding model deriving sentence-level substructure embeddings word embeddings utilizing dependency trees named entities. achieves slightly higher accuracy dataset syntactic information used. main differences approach proposed paper ones proposed cr-cnn uses pair-wise ranking method approaches apply multiclass classiﬁcation using softmax function cnn/rnn; cr-cnn employs effective method deal artiﬁcial classes omitting embeddings approaches treat classes equally. work tackle relation classiﬁcation task using performs classiﬁcation ranking. main contributions work deﬁnition state-of-the-art semeval- task dataset without using costly handcrafted features; proposal classiﬁcation uses class embeddings rank loss function; effective method deal artiﬁcial classes omitting embeddings cr-cnn; demonstration using text target nominals almost effective using wpes; method extract cr-cnn model representative contexts relation type. although apply cr-cnn relation classiﬁcation method used classiﬁcation task. james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david wardefarley yoshua bengio. theano math expression compiler. proceedings python scientiﬁc computing conference. c´ıcero nogueira santos ma´ıra gatti. deep convolutional neural networks sentiment analysis short texts. proceedings international conference computational linguistics dublin ireland. c´ıcero nogueira santos bianca zadrozny. learning character-level representations part-of-speech proceedings international tagging. conference machine learning jmlr w&cp volume beijing china. jianfeng patrick pantel michael gamon xiaodong deng. modeling interestingness deep neural networks. proceedings conference empirical methods natural language processing iris hendrickx zornitsa kozareva preslav nakov diarmuid s´eaghdha sebastian pad´o marco pennacchiotti lorenza romano stan szpakowicz. semeval- task multi-way classiﬁcation semantic relations pairs nominals. proceedings international workshop semantic evaluation pages baotian zhengdong hang qingcai chen. convolutional neural network architectures matching natural language sentences. proceedings conference neural information processing systems pages yoon kim. convolutional neural networks sentence classiﬁcation. proceedings conference empirical methods natural language processing pages doha qatar. longhua qian guodong zhou fang kong qiaoming zhu. semi-supervised learning semantic relation classiﬁcation using proceedings stratiﬁed sampling strategy. conference empirical methods natural language processing pages bryan rink sanda harabagiu. classifying semantic relations combining lexical semantic resources. proceedings international workshop semantic evaluation pages richard socher brody huval christopher manning andrew semantic compositionality recursive proceedings joint matrix-vector spaces. conference empirical methods natural language processing computational natural language learning pages kristina toutanova klein christopher manning yoram singer. feature-rich part-of-speech tagging cyclic deproceedings conferpendency network. ence north american chapter association computational linguistics human language technology pages jason weston samy bengio nicolas usunier. wsabie scaling large vocabulary image annotation. proceedings twenty-second international joint conference artiﬁcial intelligence pages jason weston sumit chopra keith adams. tagspace semantic embedproceedings condings hashtags. ference empirical methods natural language processing pages daojian zeng kang siwei guangyou zhou zhao. relation classiﬁcation convolutional deep neural network. proceedings international conference computational linguistics pages dublin ireland. zhang. weakly-supervised relation classiﬁcation information extraction. proceedings international conference information knowledge management pages york usa.", "year": 2015}