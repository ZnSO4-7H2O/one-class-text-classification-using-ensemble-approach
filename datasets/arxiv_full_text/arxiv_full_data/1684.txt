{"title": "Multilingual Topic Models for Unaligned Text", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "We develop the multilingual topic model for unaligned text (MuTo), a probabilistic model of text that is designed to analyze corpora composed of documents in two languages. From these documents, MuTo uses stochastic EM to simultaneously discover both a matching between the languages and multilingual latent topics. We demonstrate that MuTo is able to find shared topics on real-world multilingual corpora, successfully pairing related documents across languages. MuTo provides a new framework for creating multilingual topic models without needing carefully curated parallel corpora and allows applications built using the topic model formalism to be applied to a much wider class of corpora.", "text": "develop multilingual topic model unaligned text probabilistic model text designed analyze corpora composed documents languages. documents muto uses stochastic simultaneously discover matching languages multilingual latent topics. demonstrate muto able shared topics real-world multilingual corpora successfully pairing related documents across languages. muto provides framework creating multilingual topic models without needing carefully curated parallel corpora allows applications built using topic model formalism applied much wider class corpora. topic models powerful formalism unsupervised analysis corpora important tool information retrieval sentiment analysis collaborative ﬁltering interpreted mixed membership model similar assumptions successfully applied vision population survey analysis genetics work build latent dirichlet allocation generative probabilistic topic model text. assumes documents distribution topics topics distributions vocabulary. posterior inference discovers topics best explain corpus; uncovered topics tend reﬂect thematically consistent patterns words goal paper topics express thematic coherence across multiple languages. capture coherence single language semantically similar words tend used similar contexts. case multilingual corpora. example even though hund hound orthographically similar nearly identical meanings german english likely appear similar contexts almost documents written single language. consequently topic model bilingual corpus reveals coherent topics bifurcates topic space languages order build coherent topics across languages must connection languages together. previous multilingual topic models connect languages assuming parallelism either sentence level document level many parallel corpora available represent small fraction corpora. also tend relatively well annotated understood making less suited unsupervised methods like lda. topic model unaligned text multiple languages would allow exciting applications developed monolingual topics models applied broader class corpora would help monolingual users explore understand multilingual corpora. propose multilingual topic model unaligned text muto assume given explicit parallelism instead discovers parallelism vocabulary level. parallelism model assumes similar themes ideas appear languages. example word hund appears german side corpus hound appear somewhere english side. assumption similar terms appear similar contexts also used build lexicons nonparallel comparable corpora. makes contexts similar evaluated measures cooccurrence tf-idf although emphasis work building consistent topic spaces task building dictionaries good translations required consistent topics. however build successful techniques building lexicons across languages. paper organized follows. detail model assumptions section develop stochastic expectation maximization inference procedure section discuss corpora linguistic resources necessary assume that given bilingual corpus similar themes expressed languages. bark hound leash associated pet-related topic english pet-related words german without translated terms. guess told hund corresponds words discover words like leinen halsband bellen also appear hund german making reasonable guess words part topic expressed german. steps—learning words comprise topics within language learning word translations across languages—are part model. section describe muto’s generative model ﬁrst describing matching connects vocabulary terms across languages describing process using matchings create multilingual topic model. posit following generative process produce bilingual corpus source language target language first select matching terms languages. matching consists pairs linking term vocabulary ﬁrst language term vocabulary second language matching viewed bipartite graph words language side other. word either unpaired linked single node opposite language. arbitrary text. mcca uses matching together words similar meanings slightly looser assumption; require words similar document level contexts matched. another distinction instead assuming uniform prior matchings mcca consider matching regularization term edge. prefer larger values matching. parameterization allows incorporate prior knowledge derived morphological features existing dictionaries dictionaries induced non-parallel text. also knowledge gleaned parallel corpora understand non-parallel corpus interest. sources matching prior discussed section muto documents generated conditioned matching. documents endowed distribution topics. instead distributions terms topics muto distributions pairs going back intuition pair might might high probability pet-related topic. another difference unmatched terms don’t come topic instead come unigram distribution speciﬁc language. full generative process matching corpora follows choose maximum posteriori matching given topic assignments using hungarian algorithm ﬁrst consider adding single edge impacts likelihood. adding edge means occurrences term language term language come topic distributions instead different background distributions. must likelihood contribution topic-speciﬁc occurrences likelihood subtract global language-multinomial contributions likelihood. using posterior posterior estimates topics markov chain number times word appears language combined topic count putative pair resulting weight term term intuitively matching encourages words paired together appear similar topics explained background language model compatible preferences expressed matching prior πij. words appear specialized contexts better modeled topics rather background distribution. muto requires initial matching subsequently improved. experiments initial matching contained words length greater characters appear languages. languages share similar orthography produces high precision initial matching note adding term matching also potentially changes support thus counts associated terms appear estimate handled gibbs sampler across m-step updates because topic assignments alone represent state. figure graphical model muto. matching vocabulary terms determines whether observed word drawn topic-speciﬁc distribution matched pairs language-speciﬁc background distribution terms language. background distribution documents. choose topic-speciﬁc distributions unmatched words reasons. ﬁrst reason prevent topics divergent themes different languages. example even topic matched pair distinct language topic multinomials words could istanbul atat¨urk nato german stufﬁng gravy cranberry english. second reason encourage frequent nouns well explained language-speciﬁc distribution remain unmatched. given corpora goal infer matching topics per-document topic distributions topic assignments solve posterior inference problem stochastic algorithm components inference procedure ﬁnding maximum posteriori matching sampling topic assignments given matching. ﬁrst discuss estimating latent topic space given matching. collapsed gibbs sampler sample topic assignment word document conditioned topic assignments matching integrating topic distributions document topic distribution number words assigned topic document number times either terms pair assigned topic example hund assigned unrelated. correct overﬁtting stopping inference three steps gradually increasing size allowed matching iteration correcting overﬁtting principled explicitly controlling number matchings employing expressive prior matchings left future work. distance preimages feature vectors latent space proportional weight used mcca algorithm construct matchings. used method selecting initial matching mcca muto. thus identical pairs used initial seed matching rather randomly selected pairs dictionary. used mcca prior mcca dataset ﬁrst step compute prior weights. studied muto corpora four sources matching prior. matching prior term order incorporate prior information matches model prefer. source used depends much information available language pair interest. pointwise mutual information parallel text even dataset interest parallel exploit information available parallel corpora order formulate construction computed pointwise mutual information terms appearing translation aligned sentences small germanenglish news corpus dictionary machine readable dictionary available existence link dictionary matching prior. used ding dictionary terms translations given weight possible translations given dictionary gives extra weight unambiguous translations. edit distance reliable resources language pair assume signiﬁcant borrowing morphological similarity languages string similarity formulate used although deeper morphological knowledge could encoded using specially derived substitution penalty substitutions deletions penalized equally experiments. mcca bilingual corpus matching canonical correlation analysis model ﬁnds mapping latent points observed feature vector term language term second language. mcca algorithm bilingual corpus learn mapping although muto designed non-parallel corpora mind parallel corpora experiments purposes evaluation. emphasize model parallel structure corpus. using parallel corpora also guarantees similar themes discussed assumptions. first analyzed german english proceedings european parliament chapter considered distinct document. document english side corpus direct translation german side; used sample documents. another corpus variation languages wikipedia. bilingual corpus explicit mappings between documents assembled taking wikipedia articles cross-language links german english versions. documents corpus similar themes vary considerably. documents often address different aspects topic thus generally direct translations case europarl corpus. used sample titles marked german-english equivalents wikipedia metadata. used part speech tagger remove nonnoun words. nouns likely constituents topics parts speech ensures terms relevant topics still included. also prevents uninformative frequent terms highly inﬂected verbs included matching. frequent terms used vocabulary. larger vocabulary sizes make computing matching difﬁcult full weight matrix scales although could addressed ﬁltering unlikely weights. provides intuition workings model. second assess accuracy learned matchings ensures topics discover built unreasonable linguistic assumptions. last investigate extent muto recover parallel structure corpus emulates document retrieval task given query document source language well muto corresponding document target language? order distinguish effect learned matching information already available matching prior model also considered prior only version matching weights held ﬁxed matching uses prior weights better illustrate latent structure used muto build insight workings model table shows topics learned german english articles wikipedia. topic distribution pairs terms languages topics seem demonstrate thematic coherence. example topic computers topic concerns science etc. using edit distance matching prior allowed identical terms similar topic proﬁles languages computer lovelace software. also allowed terms like objekt astronom programm werk similar terms orthography topic usage. mistakes matching different consequences. instance earth matched stickstoff topic although meanings words different appear sufﬁciently similar scienceoriented contexts doesn’t harm coherence topic. contrast poor matches dilute topics. example topic table seems split math roman history. encourages matches terms like rome english r¨omer german. r¨omer refer inhabitants rome also refer historically important danish mathematician astronomer name. combination different topics reinforced subsequent iterations roman mathematical pairings. spurious matches accumulate time especially version muto prior. table shows poor matches lead lack correspondence topics across languages. instead developing independent internally coherent topics languages arbitrary matches pull topics many directions creating incoherent toptable topics twenty topic muto model trained wikipedia prior matching. topic distribution pairs; pairs topic shown. without appropriate guidance matching prior poor translations accumulate topics show thematic coherence. results demonstrate need inﬂuence choice matching pairs. figure shows accuracy multiple choices computing matching prior. matching prior used essentially correct matches chosen. models trained wikipedia lower vocabulary accuracies models trained europarl. reﬂects broader vocabulary less parallel structure limited coverage dictionary. corpora prior weights accuracy matchings found muto nearly indistinguishable matchings induced using prior weights alone. adding topic structure neither hurts helps translation accuracy. translation accuracy measures quality matching learned algorithm well recover parallel document structure corpora measures quality latent topic space muto uncovers. corpora explicit matches documents across languages effective multilingual topic model associate topics document pair regardless language. compare muto models bilingual corpora matching across languages applied multilingual corpus using union intersection vocabulary. union vocabulary words languages retained language documents ignored. posterior inference setup effectively partitable five topics twenty topic muto model trained wikipedia using edit distance matching prior topic distribution pairs; pairs topic shown. topics display semantic coherence consistent languages. correctly matched word pairs bold. muto ﬁnds consistent latent topic space distribution topics matched document pairs similar. document computed hellinger distance documents’ ranked them. proportion documents less similar designated match measures consistent topics across languages. results presented figure truly parallel corpus like europarl baseline using intersection vocabulary well less parallel wikipedia corpus intersection baseline worse muto methods. corpora union baseline little better random guessing. although morphological cues effective ﬁnding high-accuracy matchings information doesn’t necessarily match documents well. edit weight prior wikipedia worked well vocabulary pages varies substantially depending subject methods morphological features effective homogenous europarl corpus performing little better chance. even themselves matching priors good connecting words across languages’ vocabularies. wikipedia corpus better baselines muto without prior. suggests end-user interested obtaining multilingual topic model could obtain acceptable results simply constructing matching using schemes outlined section running muto using static matching. however muto perform better matchings allowed adjust reﬂect data. many conditions muto matchings updated using weights equation performs better document matching task work presented muto model simultaneously ﬁnds topic spaces matchings multiple languages. evaluations real-world data muto recovers matched documents better prior alone. suggests muto used foundation multilingual applications using topic modeling formalism corpus exploration. corpus exploration especially important multilingual corpora users often comfortable language corpus other. using widely used language english french provide readable signposts multilingual topic models could help uncertain readers relevant documents language interest. muto makes linguistic assumptions input data precludes ﬁnding relationships semantic equivalences symbols discrete vocabularies. data often presented multiple forms; models explicitly learn relationships different modalities could help better explain annotate pairings words images words sound genes different organisms metadata text. conversely adding linguistic assumptions incorporating local syntax form feature vectors effective translations without using parallel corpora. using local information within muto rather prior matching would allow quality translations improve would another alternative techniques attempt combine local context topic models models like muto remove assumption monolingual corpora topic models. exploring latent topic space also offers opportunities researchers interested multilingual corpora machine translation linguistic phylogeny semantics. authors would like thanks aria haghighi percy liang providing code advice. conversations richard socher christiane fellbaum invaluable developing model. david blei supported career grants google microsoft.", "year": 2012}