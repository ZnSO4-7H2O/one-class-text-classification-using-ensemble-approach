{"title": "Riemannian Multi-Manifold Modeling", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "This paper advocates a novel framework for segmenting a dataset in a Riemannian manifold $M$ into clusters lying around low-dimensional submanifolds of $M$. Important examples of $M$, for which the proposed clustering algorithm is computationally efficient, are the sphere, the set of positive definite matrices, and the Grassmannian. The clustering problem with these examples of $M$ is already useful for numerous application domains such as action identification in video sequences, dynamic texture clustering, brain fiber segmentation in medical imaging, and clustering of deformed images. The proposed clustering algorithm constructs a data-affinity matrix by thoroughly exploiting the intrinsic geometry and then applies spectral clustering. The intrinsic local geometry is encoded by local sparse coding and more importantly by directional information of local tangent spaces and geodesics. Theoretical guarantees are established for a simplified variant of the algorithm even when the clusters intersect. To avoid complication, these guarantees assume that the underlying submanifolds are geodesic. Extensive validation on synthetic and real data demonstrates the resiliency of the proposed method against deviations from the theoretical model as well as its superior performance over state-of-the-art techniques.", "text": "paper advocates novel framework segmenting dataset riemannian manifold clusters lying around low-dimensional submanifolds important examples proposed clustering algorithm computationally eﬃcient sphere positive deﬁnite matrices grassmannian. clustering problem examples already useful numerous application domains action identiﬁcation video sequences dynamic texture clustering brain ﬁber segmentation medical imaging clustering deformed images. proposed clustering algorithm constructs data-aﬃnity matrix thoroughly exploiting intrinsic geometry applies spectral clustering. intrinsic local geometry encoded local sparse coding importantly directional information local tangent spaces geodesics. theoretical guarantees established simpliﬁed variant algorithm even clusters intersect. avoid complication guarantees assume underlying submanifolds geodesic. extensive validation synthetic real data demonstrates resiliency proposed method deviations theoretical model well superior performance state-of-the-art techniques. many modern data sets moderate high dimension manifest intrinsically low-dimensional structures. natural quantitative framework studying common data sets multi-manifold modeling special case hybrid-linear modeling framework given dataset modeled union submanifolds proposing valid algorithm assumes underlying dataset modeled mixture submanifolds tries prove conditions proposed algorithm cluster dataset according submanifolds. framework extensively studied applied datasets embedded euclidean space sphere nevertheless overwhelming number application domains information extracted datasets riemannian manifolds grassmannian sphere orthogonal group manifold symmetric positive deﬁnite matrices. example auto-regressive moving average models utilized extract low-rank linear subspaces identifying spatio-temporal dynamics video sequences similarly convolving patches images gabor ﬁlters yields covariance matrices capture eﬀectively texture patterns images nevertheless current strategies suﬃciently accurate handling data general riemannian spaces. related work. recent advances parsimonious data representations important implications dimensionality reduction techniques eﬀected development non-standard spectral-clustering schemes result state-of-the-art results modern applications schemes rely assumption data exhibit lowdimensional structures unions low-dimensional linear subspaces submanifolds embedded euclidean spaces. several algorithms clustering manifolds generalizations well-known schemes developed originally euclidean spaces. example gruber theis extended classical k-means algorithm euclidean spaces grassmannians illustrated application nonnegative matrix factorization. tuzel capitalized riemannian distance design eﬃcient mean-shift algorithm multiple rigid motion estimation. subbarao meer well cetingul vidal extended algorithm general analytic manifolds including grassmannians stiefel manifolds matrix groups. o’hara showed promising results using geodesic distance product manifolds clustering human expressions gestures actions videos. rathi solved image segmentation problem recasting matrix clustering problem probability distributions symmetric matrices. vidal extended spectral clustering nonlinear dimensionality reduction techniques riemannian manifolds. previous works quite successful convex hulls individual clusters well-separated often fail clusters intersect closely located. accommodate low-dimensional data structures unions subspaces submanifolds respectively restricted manifolds embedded either euclidean space sphere. many strategies suggested solving problem known also subspace clustering. strategies include methods inspired energy minimization algebraic methods statistical methods spectral-type methods various types aﬃnities representing subspace-related information recent tutorial papers vidal aldroubi theoretical guarantees particular algorithms appear fewer strategies problem also known manifold clustering. include higher-order spectral clustering spectral methods based local sparse-coding-based spectral clustering euclidean space modiﬁcation sphere cetingul energy minimization strategies methods based manifold learning algorithms methods based clustering dimension local density notwithstanding higher-order spectral clustering spectral local theoretically guaranteed diﬀerent context rahman suggested multiscale strategies signals taking values riemannian manifolds particular sphere orthogonal group grassmannian manifold. even though rahman addresses completely diﬀerent problem basic principle similar spirit described follows. local analysis performed tangent spaces exponential logarithm maps used transform data local manifold neighborhoods local tangent space neighborhoods. information local neighborhoods integrated infer global properties. contributions. despite popularity manifold learning associated literature lacks generic schemes clustering low-dimensional data embedded non-euclidean spaces. furthermore even euclidean setting algorithms theoretically guaranteed. paper aims ﬁlling provides approach non-euclidean setting theoretical guarantees even clusters intersect. order avoid nontrivial theoretical obstacles theory assumes underlying submanifolds geodesic refer multi-geodesic modeling clearly modeling paradigm direct generalization euclidean spaces riemannian manifolds. practical robust variant theoretical algorithm also developed superior performance state-of-the-art clustering techniques exhibited extensive validation synthetic real datasets. remark practice require logarithm computed eﬃciently show assumption restrict wide applicability work. believe possible extend theoretical foundations work deal general submanifolds using local geodesic submanifolds however signiﬁcantly increase complexity proof already simple. nevertheless proposed method directly applies general setting since geodesics used local neighborhoods globally. furthermore numerical experiments show proposed method works well real practical scenarios deviate theoretical model. technical level paper distinguished previous works multi-manifold modeling careful incorporation directional information e.g. local tangent spaces geodesics. done purposes distinguish submanifolds intersections; ﬁlter neighboring points belong clusters diﬀerent cluster query point. proposed algorithm allows neighborhoods include points diﬀerent clusters previous multi-manifold algorithms need careful choice neighborhood radii avoid points belonging clusters. assumes point given dataset {xi}n lies tubular neighborhood unknown geodesic submanifold riemannian manifold goal cluster dataset groups points associated submanifold problem serves theoretical justiﬁcation. numerical experiments show proposed algorithm works well general setting. setting include general submanifolds non-uniform sampling diﬀerent kinds levels noise. manifold metric tensor geodesic curve whose length locally minimized among curves connecting distg riemannian distance denotes tangent space stands tangent subspace d-dimensional geodesic submanifold shown figure linear subspace exponential expx maps tangent vector point expx provides local coordinates around deﬁnition geodesic submanifold image expx functional inverse expx logarithm logx maps origin denote image data point txim logarithm suggest solutions problem theoretical guarantees supporting solutions restricting problem mgm. section deﬁnes quantities quantifying directional information estimated local tangent subspaces geodesic angles. section presents solutions discusses properties. subspaces. tangent space exponential manifold point note tangent subspace pre-image exponential map. logarithm logxi w.r.t. images logxi data points local neighborhood particular image note diﬀerence txis image logxi subspace estimated images data points local neighborhood. }j∈j txim tubular neighborhood d-dimensional subspace estimates intrinsic dimension local tangent subspace also dimension formed bottom eigenvalues adopt strategy dimension estimation deﬁne estimated local tangent subspace span txim eigenvectors theory number eigenvectors number eigenvalues exceed ηkcxjk ﬁxed practice number eigenvectors number eigenvalues largest occurs. empirical geodesic angles. shortest geodesic connecting txim tangent vector words shows direction shortest path given dataset {xj}n empirical geodesic angle elevation angle lerman whitehouse vector subspace section propose theoretical solution data sampled according uniform mgm. start basic motivation describe proposed algorithm last formulate theoretical guarantees. section propose practical algorithm. last section discusses numerical complexity algorithms. proposed solution mgm-clustering task applies spectral clustering carefully chosen weights. speciﬁcally similarity graph constructed whose vertices data points whose edges represent similarity data points. challenge construct graph points locally connected come cluster. spectral clustering recover exactly underlying clusters. sake illustration assume underlying geodesic submanifolds also assume data sampled according uniform mgm. given point wishes connect points submanifold within local neighborhood clearly realistic assume points submanifold ﬁrst assume intersection demonstrated figure order able identify points submanifold local tangent information belongs geodesic large angle tangent space hand belongs geodesic angle close zero. therefore thresholding empirical geodesic angles become beneﬁcial eliminating neighboring points belonging diﬀerent submanifold figure figures blue points submanifold points submanifold figure local neighborhood disk radius around blue point observed goal exclude points done thresholding angles geodesics tangent subspace txs. indeed angles w.r.t. blue points close zero angles w.r.t. points suﬃciently large. figure point arbitrary point suﬃciently goal assure connected done comparing local estimated dimensions. estimated dimension dim+dim estimated dimension dim. dimension diﬀerence intersection disconnected submanifolds. near intersection hard estimate correctly tangent spaces submanifold geodesic angles reliable. instead compare dimensions estimated local tangent subspaces. estimated dimensions local neighborhoods data points close intersections larger estimated dimensions local neighborhoods data points away intersections algorithm thus connects neighboring points dimension diﬀerence criterion together angle ﬁltering procedure guarantee false connection diﬀerent clusters simple ideas common spectral-clustering procedure form theoretical geodesic clustering tangent information algorithm following theorem asserts tgct achieves correct clustering high probability. proof clariﬁed proof depend underlying geometry generative model. simplicity theorem assumes geodesic submanifolds dimension. however extended geodesic submanifolds diﬀerent dimensions. theorem consider smooth compact d-dimensional geodesic submanifolds riemannian manifold dataset generated according uniform w.r.t. noise level positive parameters tgct algorithm satisfy inequalities practical version tgct algorithm refer geodesic clustering tangent information described algorithm algorithm implemented experiments section choice parameters clariﬁed section diﬀers tgct three diﬀerent ways. first hard thresholds tgct replaced soft ones ﬂexible. second dimension indicator function dropped aﬃnity matrix indeed numerical experiments indicate algorithm works properly without dimension indicator function whenever small portion points near intersection. numerical observation makes sense since dimension indicator used theory avoid connecting intersection points points intersection. last pairwise distances replaced weights resulting sparsity-cognizant optimization tasks. sparse coding takes advantage low-dimensional structure submanifolds produces larger weights points coming submanifold algorithm solves sparse coding task penalty used non-standard since codes |sij| multiplied k/σd latter terms weights chosen increase eﬀect nearby points particular avoids sparse representations far-away points unrelated local manifold structure similarly cetingul clustering weights exponentiate sparse-coding weights. figure illustration need weighted sparse optimization non-weighted sparse k/σd timization fail detect local structure manifold setting. term used avoid assigning large weights far-away blue points. computed complexity depends riemannian manifold space symmetric matrices cl=o. grassmannian chosen order dimension subspaces cl=d. applications riemannian multi-manifold modeling aware known examples. general unknown estimation logarithm discussed m´emoli sapiro possible reduce total computational cost assumptions. particular theory possible implement tgct sphere grassmannian computational complexity order assess performance synthetic real datasets algorithm compared following algorithms sparse manifold clustering adapted clustering within riemannian manifold still referred spectral clustering riemannian metric vidal embedded k-means three methods choices parameters four methods reviewed appendix ground truth labeling given experiment. measure accuracy method assigned labels ﬁrst permuted maximal match ground truth labels. clustering rate computed permuted labels follows datasets generated according postulated models above experiment repeated times. table shows average clustering rate method. based spectral clustering scheme. however dataset low-dimensional structures gct’s unique procedure ﬁltering neighboring points ensures yields superior performance methods. sensitive local scale require neighborhood contain points diﬀerent groups. becomes clear results datasets non-intersecting submanifolds. works well dataset neighborhoods contain points cluster neighborhoods datasets often contain points diﬀerent ones. embedded k-means generally requires intrinsic means diﬀerent clusters located other. performance good diﬀerent groups low-dimensional structures. section illustrated gct’s superior performance competing variety manifolds. section investigates gct’s robustness noise computational cost pertaining running time. summary shown robust presence noise price small increase running time. proposed tangent ﬁltering scheme enables successfully eliminate neighboring points originate diﬀerent groups. such exhibits robustness presence noise and/or whenever diﬀerent groups close even intersecting. hand appears sensitive noise figure performance clustering methods grassmannian various noise levels. datasets generated according model dataset increasing standard deviation noise. datasets figure generated grassmannian according model dataset section diﬀerent noise levels appear volatile diﬀerent datasets best performance never exceeding clustering rate. worth noticing shows poor clustering accuracy. contrary exhibits remarkable robustness noise achieving clustering rates even standard deviation noise approaches gct’s robustness noise also demonstrated figure datasets generated unit sphere according model dataset diﬀerent noise levels. appears volatile also setting; collapses standard deviation noise exceeds since aﬃnity matrix precludes spectral clustering identifying eigenvalues suﬃcient accuracy section demonstrates outperforms price small increase computational complexity. similarly manifold clustering algorithm computations performed local neighborhood local linear structures leveraged increase clustering accuracy. overall complexity scales quadratically w.r.t. number data-points last step algorithm amounts spectral clustering aﬃnity matrix optimization task computation principal eigenvectors covariance matrix algorithm contribute much complexity since operations performed small number points neighborhood computational complexity detailed appendix also noteworthy figure performance clustering methods sphere various noise levels. datasets generated according model dataset increasing standard deviation noise. ratios running times three types manifolds illustrated table readily veriﬁed extra step identifying tangent spaces increases running time less smc. ratios running times also investigated increasing ambient dimensions sphere. precisely dataset section lies embedded random orthonormal matrix unit sphere ranged figure shows ratios running time function observe extra cost computing eigendecomposition mostly less never exceeds even ambient dimension large cetingul cast problem segmenting diﬀusion magnetic resonance imaging data diﬀerent ﬁber tracts clustering problem crux methodology lies transforfigure relative running times w.r.t. ambient dimension increases. dimensions ranging dataset section embedded random orthonormal matrix unit sphere dataset section matrix random mation diﬀusion images associated diﬀerent views object orientation distribution functions nothing probability density functions discretized probability mass function fd+]t describes water diﬀusion pattern corresponding location object’s image according viewing directions {si}d+ ﬁxed location square-root dodf vector pfd+]t lies sphere since pmf. pixels diﬀusion images object given location mapped element cetingul assume ﬁber tract mapped submanifold thus identify diﬀerent ﬁber tracts multi-manifold modeling synthetic ﬁber tracts domain. generate ﬁbers points randomly chosen colored region figure cubic splines passing respectively center ﬁbers fibers given pair ﬁbers next step pixel point software code provided canales-rodriguez used generate srdodfs diﬀusion images {sn}g gradient directions srdodfs formed clustering carried riemannian manifold turn provides segmentation pixels according diﬀerent ﬁber tracts. total number pairs synthetic brain ﬁbers randomly generated clustering performed pair. table reports mean three clustering methods. case figure plots sample distributions accuracy rates shows demonstrates highest probability achieving almost accurate clustering among competing schemes. figure histogram clustering rates noise level total number experiments. shows number experiments whose rates fall within intervals length partition. example since tallest within range blue likely method achieve almost accurate clustering. contrary brown tallest range meaning clustering rate within likely achieved experiments. cluster local covariance matrices obtained various transformations images brodatz database goal able distinguish diﬀerent images independently transformation. beach sand grass) captured uniform lighting frontview position. apply three simple deformations images mimic real settings diﬀerent lighting conditions stretching diﬀerent viewpoints figure shows sample images brodatz database deformations. three clustering tests type deformation carried out. test transformed patches generated equally diﬀerent textures region covariance computed patch. clustering algorithms applied dataset region covariances belonging texture patterns. generate transformed patches described below. figure sample images brodatz database deformations. ﬁrst shows original images; second image contains unique texture diﬀerent regions diﬀerent lighting; third shows horizontal-shifted images image; fourth shows aﬃne-transformed images image. procedure generating data repeated times type transformation. well three clustering methods applied datasets average clustering rates reported table exhibits best performance datasets types transforms. figure projection covariance matrices local patches transformed images onto principal directions. sample images dataset covariance matrices computed spatio-temporal data dynamic textures videos human actions often approximated linear dynamical models particular leveraging autoregressive moving average model experiment spatio-temporal databases dyntex++ ballet. following turaga employ arma model associate local spatio-temporal patches linear subspaces dimension. apply manifold clustering grassmannian order distinguish diﬀerent textures actions dyntex++ ballet database respectively. arma model. premise arma modeling based assumption spatio-temporal dataset study governed small number latent variables whose temporal variations obey vector latent variables rp×d observation matrix rd×d transition matrix i.i.d. sampled vector-values r.vs. obeying gaussian distributions respectively. given data turaga moreover arbitrarily choosing veriﬁed experiments column space d-dimensional linear subspace rpm. words arma gives rise point grassmannian diﬀerent choices s.t. several local regions within video sequence patches size randomly chosen. frame patch vectorized resulting patches size reduce size random projection operator applied patch. result patch reduced expect points cluster near submanifold repeated pattern textures space time visualize submanifold structure isometrically embedded euclidean space subspaces mapped euclidean points. projected latter points principal components. figure demonstrates projection well submanifold structure within cluster. spatio-temporal patches generated selecting consecutive frames size following overlapping time intervals three videos spatio-temporal patches size generated. associate patch subspace consequently subspaces grassmannian generated. figure visualizes representation subspaces created three random videos. intersection represents still motion. idea proof follows. excluding points sampled near possibly nonempty intersection submanifolds form graph whose vertices points remaining whose edges determined proof establishes resulting graph connected components correspond diﬀerent submanifolds spectral clustering exactly cluster graph appropriate choice tuning parameter speciﬁed self-tuning mechanism claim follows unpublished supplemental material. basic strategy proof organization described follows. section presents additional notation used proof. section reminds reader underlying model proof section eliminates undesirable events negligible probability show graphs show graphs disconnected respectively connected. other proof concluded. subsequent auxiliary sets instrumental proof. constant depends verify fact consequence second part proof. part shows graph disconnected graph well graph therefore cannot connected points last show also cannot connected within show third part proof graphs disconnected other. three parts imply graphs form connected components within deﬁnition identiﬁed respectively. conclude proof estimate measure excluded. precisely consider measure xs∩s deﬁne follows various ideas proof follow arias-castro considered multi-manifold modeling euclidean spaces. arguments proof arias-castro even apply general metric spaces particular riemannian manifolds. thus tried maintain notation arias-castro however algorithm construction main theoretical analysis arias-castro valid dataset lies euclidean space nontrivial extend riemannian manifold. indeed basic idea arias-castro compare local covariance matrices comparison infer relation corresponding data points matrices generated. however comparing local covariance matrices case ambient space riemannian manifold straightforward euclidean spaces. fact local covariance matrices computed diﬀerent tangent spaces diﬀerent coordinate systems. instead show suﬃcient compare local directional information local dimension. quantities derived local covariance matrices. however nonlinear mapping tangent spaces distorts uniform assumption within ambient space care must taken using inverse nonlinear i.e. logarithm map. denote r-neighborhoods respectively. related exponential follows φx). refer coordinates obtained tangent space exponential normal coordinates. using normal coordinates endowed riemannian metric distg measure hand tangent space also identiﬁed choosing orthonormal basis. provides euclidean metric diste measure particular simple relation shows tangent space figure highlights diﬀerence diste distg. north pole straight blue line connecting shortest path w.r.t. diste. hand shortest path w.r.t. distg clearly equator black tns; diﬀerent blue line. fact lines connecting origin points correspond geodesics general metric. consequence measures induced distg diste also diﬀerent. given submanifold metric tensor inherited induces measure called uniform measure simplicity assume throughout proof thus mainly discuss measure section generalize proof noisy case thus discuss push-forward measure measure denoted µgs. deﬁnition support push-forward measure diste similarly induces another measure supported compact submanifolds model denote deﬁne following measures w.r.t. µes. covariance matrices w.r.t. denoted eµgs eµes respectively. simplicity denote eµgs eµes eµes compact geodesic submanifolds dimension riemannian manifold denote -tubular neighborhoods respectively. recall example distg distg miny∈s distg. dataset size i.i.d. sampled normalized version recall notation fixing point i.i.d. sampled normalized verify concentration local covariance matrices existence suﬃciently large samples local neighborhoods submanifold. follow arias-castro deﬁne probability space following events speciﬁed arias-castro }j∈j txim note datasets samples dataset satisﬁes following condition point enough samples also belong datasets samples suﬃcient concentration local covariance matrices. following theorem arias-castro ensures ensuring connectedness following proposition establishes wlog connectedness graph uses constant clariﬁed proof depends geometric properties angle intersection. part construction aﬃnity matrix lemmata connectivity points solely determined indicator function distg<σd. obvious graph nodes weights distg<σd connected follows arias-castro graph nodes also connected component argument arias-castro requires careful adaptation riemannian case. related determination constant constant euclidean case guaranteed lemma arias-castro adaptation lemma riemannian case stated following lemma disconnectedness show points connected points section showed estimated dimensions local neighborhoods points equal section show estimated dimensions local neighborhoods points larger multiplicative term conclude disconnected since deﬁned recall denote normal coordinate charts around respectively suﬃcient restrict respectively. using chart correspond subspaces denote respectively. hand using chart corresponds manifold whereas still corresponds subspace. follows invertibility composition embeds bzr) shown figure recall denotes sample covariance data denotes sample covariance data using notation orthogonal matrices claim disconnectedness show graphs nodes disconnected. idea show function distg<σdθij +θji<σa zero points appropriate choice constants. proposition imply graphs associated disconnected. ﬁrst establish lower bound empirical geodesic angle lemma conclude direct connection sets corollary proof satisfy respectively lemma implies words direct connection distg<σdθij +θji<σa hand lemma proposition imply cannot connected points thus conclude disconnected. theorem replace obtain statement probability least rd+/c proposition corollary imply connected components. require parameters tgct satisfy additional requirement speciﬁed also note requirement also appears auxiliary lemmata follows fact requirements i.e. suﬃcient equivalent next explain choose parameters satisfy requirements section. problem make sure last inequality satisﬁed. given suﬃciently small satisfying ﬁxed tends approach zero. note lower bound therefore suﬃciently small π/)/ lower π/)/ chosen interval π/)/]. ﬁrst equality follows fact dataset i.i.d. sampled µgs. second inequality follows lemma last follows theorem gray constant depending geometry underlying generative model replaced cd−dim. requires though suﬃciently small noise level precise bound trivial. furthermore analysis employed optimal. thus claim theory robustness small levels noise whereas robustness higher levels noise studied experiments. aiming eﬃciently organizing data embedded non-euclidean space according low-dimensional structures present paper studied multi-manifold modeling spaces. paper solves clustering problem proposing novel algorithm. thoroughly exploits geometry data build similarity matrix eﬀectively cluster data even underlying submanifolds intersect diﬀerent dimensions. particular introduces novel idea non-euclidean multi-manifold modeling using directional information local tangent spaces avoid neighboring points clusters diﬀerent query point. theoretical guarantees successful clustering established variant namely tgct setting non-euclidean generalization widely-used framework hybrid-linear modeling. unlike tgct combined directional information local tangent spaces sparse coding aims improve clustering result succinct representations underlying low-dimensional structures increasing robustness corruption. geodesic information used locally thus practice algorithm well practice mgm. validated state-of-the-art existing methods non-euclidean setting exhibited notable performance clustering accuracy. speciﬁcally paper tested synthetic real data deformed images clustering action identiﬁcation video sequences brain ﬁber segmentation medical imaging dynamic texture clustering. work supported digital technology initiative seed grant program digital technology center university minnesota awards dms-- dms-- eager-- university minnesota doctoral dissertation fellowship program feinberg foundation visiting faculty program fellowship weizmann institute science. ﬁrst competing algorithm sparse manifold clustering algorithm ﬁrst suggested elhamifar vidal clustering submanifolds embedded euclidean spaces later modiﬁed cetingul clustering submanifolds sphere. adapt current setting clustering submanifolds riemannian manifold still refer smc. basic idea follows data-point local neighborhood mapped tangent space logarithm sparse coding task solved provide weights spectral-clustering similarity matrix. second competing algorithm spectral clustering riemannian metric vidal third competing scheme embedded k-means. embeds given dataset lies riemannian manifold euclidean spaces applies classical k-means embedded dataset. experiments grassmannian manifolds embedded well-known chosen average distance nearest point distance angle thresholds experiments dimension local tangent space determined largest eigenvalues local covariance matrix since online available codes wrote implementations post supplemental webpage paper accepted publication. spectral clustering code well k-means code taken implementations nikvand make faithful comparison input parameter also implicitly sets parameters smc. remark elhamifar vidal formed weight matrix follows |sij|+|sji| |sij| |sji| sparse coeﬃcients. however weight unstable experiments certain ii-vi section exponential weight used rest experiments synthetic dataset real stylized applications. also used dataset figure noise levels mostly higher noise level used section collapse phenomenon evident figure noise levels discuss complexity computing logarithm maps grassmannians symmetric matrices spheres. remark though possible compute logarithm maps data sampled general riemannian manifolds without knowledge manifold signiﬁcantly slower rate also show logarithm computed cases computation geodesic distances lower equal order. orthonormal matrix subspace subspace spanned columns comprising ﬁrst columns given pairs subspaces needs compute logl. computation clariﬁed gallivan includes singular value decomposition total complexity equivalently symmetric matrices computes logarithm logm matrices ﬁrst ﬁnding cholesky decomposition computing logm logg latter matrix logarithm. complexities major operations complexity computing riemannian distance logarithm w.r.t. denoted respectively major part occurs r-neighborhood deﬁned average distance nearest point associated data-point. facilitate analysis computational complexity instead required form loss small scale convex optimization task solved eﬃciently oﬀ-the-shelf solver popular alternating direction method multipliers douglas-rachford algorithm third step algorithm eigenvectors sample covariance matrix deﬁned neighbors shown section complexity step finally compute geodesic angles operations necessary. considering data-points total complexity main loop o+kn log+n main loop spectral clustering invoked aﬃnity matrix main computational burden identify eigenvectors matrix entails complexity order summary complexity computed neighboring points reducing thus complexity step moreover consider data matrix speciﬁc neighborhood points. need identify principal eigenvectors covariance matrix avoid costly direct computation leveraging following elementary facts linear algebra eigenvalue-eigenvector pair eigenvalue-eigenvector pair rank rank. facts suggest spectra coincide thus suﬃcient compute eigendecomposition much smaller matrix complexity renders overall cost eigendecomposition equal including picking subsequence necessary assume wlog {xn}∞n= since compact always convergent subsequence. therefore assume {xn}∞n= also convergent. show converges point since compact distg bounded. equation implies distg approaches inﬁnity. suppose {xn}∞n= converges point distg distg since contradiction. {xn}∞n= converges assume {xn}∞n= normal coordinate chart ﬁxed denote since geodesic submanifolds subspaces sequence {yn}∞n= performing gram-schmidt orthogonalization rows matrix written product upper triangular matrix orthogonal matrix since preserves length vectors condition becomes similarly proof proposition argmin uniquely deﬁned arbitrarily chosen among minimizers. composition transition note maps subspace another subspace tzs. denote image rotation matrix using terminology main term expressed follows", "year": 2014}