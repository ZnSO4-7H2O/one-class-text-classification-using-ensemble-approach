{"title": "Deep Reinforcement Learning with Model Learning and Monte Carlo Tree  Search in Minecraft", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep reinforcement learning has been successfully applied to several visual-input tasks using model-free methods. In this paper, we propose a model-based approach that combines learning a DNN-based transition model with Monte Carlo tree search to solve a block-placing task in Minecraft. Our learned transition model predicts the next frame and the rewards one step ahead given the last four frames of the agent's first-person-view image and the current action. Then a Monte Carlo tree search algorithm uses this model to plan the best sequence of actions for the agent to perform. On the proposed task in Minecraft, our model-based approach reaches the performance comparable to the Deep Q-Network's, but learns faster and, thus, is more training sample efficient.", "text": "deep reinforcement learning successfully applied several visual-input tasks using model-free methods. paper propose model-based approach combines learning dnn-based transition model monte carlo tree search solve block-placing task minecraft. learned transition model predicts next frame rewards step ahead given last four frames agent’s ﬁrst-person-view image current action. monte carlo tree search algorithm uses model plan best sequence actions agent perform. proposed task minecraft model-based approach reaches performance comparable deep q-network’s learns faster thus training sample efﬁcient. would like express sincere gratitude supervisor stefan uhlich continuous support patience immense knowledge helped study. thanks appreciation also colleague anna konobelkina insightful comments paper well sony europe limited providing resources project. deep reinforcement learning visual-input tasks oftentimes used evaluate algorithms minecraft various atari games appear quite challenging agents solve applied tasks model-free reinforcement learning shows noticeably good results e.g. deep q-network agent approaches human-level game-playing performance asynchronous advantage actor-critic algorithm outperforms known methods half training time achievements however cancel fact generally model-free methods considered statistically less efﬁcient comparison model-based ones model-free approaches employ information environment directly whereas model-based solutions working known environment model beneﬁts changes environment state foreseen therefore planning future becomes less complicated. time developing algorithms known environment model available start demanding promising less training data required model-free approaches agents utilize planning algorithms. research progressing direction e.g. modelbased agent surpassed dqn’s results using atari games’ true state modelling constructing transition models video-frames prediction proposed ideas paved following question feasible apply planning algorithms learned model environment partially observable minecraft building task? investigate question developed method predicts future frames visual task also calculates possible rewards agent’s actions. model-based approach merges model learning deep-neural-network training monte carlo tree search demonstrates results competitive dqn’s tested block-placing task minecraft. evaluate performance suggested approach well compare model-free methods namely block-placing task designed makes malmo framework built inside minecraft game world beginning game agent positioned wall playing room. playing ﬁeld center room. ﬁeld white tile probability become colored start. colored tiles indicate location agent place block. goal game cover colored tiles blocks actions maximum. five position-changing actions allowed moving forward tile turning left right moving sideways left right tile. agent focuses tile place block action. action agent receives feedback every correctly placed block brings reward erroneously block causes punishment action costs agent penalty evaluate environment agent provided pre-processed ﬁrst-person-view picture current state. task deterministic discrete action state space. challenge lies partial observability environment already placed blocks obscuring agent’s view. equally important place blocks systematically obstruct agent’s pathway. example task depicted fig. short demonstration available https//youtu.be/aqlbaqdpa. action input predicts following framest+. additionally predicts rewards transitions following predicted frame action at+. predicting rewards step ahead makes application search-based algorithms efﬁcient additional simulation required explore rewards transitions neighboring states. method however fails predict reward following ﬁrst state. address issue noop action predicting reward current state introduced. network takes four ×-sized input frames uses four convolutional layers followed rectiﬁer linear unit encode input information vector size vector concatenated onehot encoded action input noop action represented vector zeros linearly transformed fully connected layer size followed relu. resulting embedded vector used both reward prediction frame prediction last part network. frame prediction four deconvolutional layers used relus sigmoid end. dimensions layers equivalent convolutional layers ﬁrst part reversed order. reward prediction done applying fully connected linear layers sizes respectively. relu used layers ﬁnal output predict rewards. architecture neural network illustrated fig. training network done help experience replay re-use de-correlate training data. mini-batches size sampled training rmsprop used update weights. both frame prediction reward prediction trained mean squared error loss. mini-batch update gradients frame prediction loss backpropagated completely network gradients reward prediction loss backpropagated layers embedded vector shared frame prediction encountered. procedure ensures network uses full capacity improve prediction next state reward prediction independently trained embedded feature vector last shared intermediate layer. network’s structure shared layer contain necessary information construct prediction next frame past information. block-placing task frame sufﬁces predict reward. different tasks using previous layer action input worth considering finding actions maximum future reward done help uct-based strategy monte carlo tree search learned model described sec. input next step based frame prediction model. procedure repeated several times roll model future states. treesearch trajectory rolled maximum depth reached. rollout rewards along trajectory predicted well rewards neighboring states network’s structure. outputs neural network deterministic state evaluated still visited multiple times since many paths maximum discounted future reward state number visits state number visits parent. hyperparameter controls trade-off exploration exploitation higher translates exploration. state visited preferred already visited ones. case several states consideration visited highest immediate reward chosen. reward given neural network model predicts rewards step ahead. whenever path higher maximum value encountered search trajectory path’s value propagated tree’s root node updating every node maximum reward value. evaluation task minecraft agent performs mcts action decision make. agent given ﬁxed number trajectories roll decides action maximum future discounted reward take next action. subsequently agent receives ground-truth input frame environment last step updates root search tree state information. next step chosen applying mcts beginning state. instead starting scratch value calculated maximum future reward value previous turn. tree-search results carried following steps trajectories maximum future reward updated ﬁrst input. mcts agent uses maximum depth rollouts explores trajectories deciding next action. course solving single task mcts agent explores trajectories since task limited actions. trajectories however repeated input frames arriving every action decision. hyperparameter eight. compared domains value rather large. reason behind decision follows exploitation measure limited range number visits remain since node evaluated once. original architecture train agent block-placing task. restricting input four frames method grounds number frames used dqn’s structure provide approaches equal input information. fig. results task-solving success rate average reward presented. agents mcts performance measures evaluated course training steps step corresponds mini-batch update. test block-placing tasks employed. training agents appear level roughly scores. agent overestimates q-values ﬁrst million training steps. although reducing learning rate helped weakening effect slowed training process. known problem q-learning. model-based approach quickly learn meaningful model achieve good results mcts. suggests learning transition function easier task learning q-values block-placing problem. main pitfall model-based approach lies accumulating errors rollouts reach many steps ahead. particular block-placing task mcts trajectories steps deep minecraft frames rather structured hence accumulating error problem prominent. slight increase variance reward predictions future steps alleviated using discount rate mcts agent compared discount rate used dqn. value found empirically provide best results. table demonstrates smoothed scores agents million training steps. million steps model-based approach already solve considerate amount block-placing tasks whereas agent learned reasonable policy yet. agent catch mcts agent needs million additional training steps underlines model-based approach’s data efﬁciency. beats mcts agent small margin additionally table includes mcts agent’s scores one-stepahead prediction reward employed. tree search agent chooses random action unexplored future states instead greedily choosing action maximum immediate reward. block-placing task using one-step-ahead predicted rewards doubles score across training steps i.e. scores comparable agent become achievable trajectories. paper explored idea creating model-based reinforcement learning agent could perform competitively model-free methods particular. implement agent synthesis learning transition model deep neural network mcts developed. tests block-placing task minecraft show learning meaningful transition model requires considerably less training data learning q-values comparable scores dqn. mcts agent uses tree search ﬁnding best action takes longer perform action comparison dqn. therefore approach interesting cases obtaining training samples environment costly. nature block-placing task justiﬁes greedy choice immediate rewards hence application one-step-ahead prediction signiﬁcantly improves score performance. transition model suffers limited information last four input frames past information becomes unavailable model makes incorrect predictions environment leading suboptimal actions. research using recurrent neural network could help eliminating issue.", "year": 2018}