{"title": "Resnet in Resnet: Generalizing Residual Architectures", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.", "text": "residual networks recently achieved state-of-the-art challenging computer vision tasks. introduce resnet resnet deep dualstream architecture generalizes resnets standard cnns easily implemented computational overhead. consistently improves performance resnets outperforms architectures similar amounts augmentation cifar- establishes state-of-the-art cifar-. recently proposed residual networks state-of-the-art performance ilsvrc classiﬁcation task allow training extremely deep networks layers similar highway networks residual networks make identity shortcut connections enable information across layers without attenuation would caused multiple stacked non-linear transformations resulting improved optimization residual networks shortcut connections gated untransformed input always transmitted. empirical performance resnets impressive current residual network architectures several potential limitations identity connections implemented current resnet leads accumulation levels feature representations layer even though deep network features learned earlier layers longer provide useful information later layers hypothesis resnet architecture learning identity weights difﬁcult argument difﬁcult learn additive inverse identity weights needed remove information representation given layer. ﬁxed size layer structure residual block modules also enforces residuals must learned shallow subnetworks despite evidence deeper networks expressive introduce generalized residual architecture combines residual networks standard convolutional networks parallel residual non-residual streams. show architectures using generalized residual blocks retain optimization beneﬁts identity shortcut connections improving expressivity ease removing unneeded information. present novel architecture resnet resnet incorporates generalized residual blocks within framework resnet demonstrate state-of-the-art performance architecture cifar-. modular unit generalized residual network architecture generalized residual block consisting parallel states residual stream contains identity shortcut connections similar structure residual block original resnet single convolutional layer transient stream standard convolutional layer additional sets convolutional ﬁlters block also transfer same-stream cross-stream activations summed applying batch normalization relu nonlinearities output states block function residual stream resembles original structure resnet shortcut connections unit processing transient stream adds ability process information either stream nonlinear manner without shortcut connections allowing information earlier states discarded. form shortcut connection identity function appropriate padding projection implement generalized residual block single convolutional layer modiﬁed initialization call resnet init generalized residual block either standard layer single-layer resnet block repeating generalized residual block several times generalized residual architecture expressivity learn anything between including standard -layer resnet block architecture allows network learn residuals variable effective number processing steps addition back residual stream investigate visualizations. generalized residual block speciﬁc cnns applied standard fully connected layers feedforward layers. replacing convolutional layers within residual block original resnet generalized residual block leads architecture call resnet resnet figure summarize relationship standard resnet init resnet architectures. evaluate architectures cifar- cifar- datasets report best results found grid search hyperparameters including learning rate penalty initialization among xavier orthogonal optimizer among momentum nesterov momentum adam rmsprop type shortcut connections residual blocks. optimize hyperparameters original resnet architecture momentum minibatch size penalty train epochs. learning rate scaled epochs initialization used weight tensors. test time batch normalization statistics approximated using exponential moving average training batch normalization statistics. projection convolution used residual blocks increase dimensionality shortcut connections identity. equal numbers ﬁlters residual transient streams generalized residual network optimizing hyperparameter could lead potential improvements. experiments resnet init architecture shows consistent improvement standard architectures architecture outperforms original resnet architecture performs well across range numbers blocks layers block resnet init applied existing architectures all-cnn-c yields improvement standard initialization stream uses half total ﬁlters investigate effect architectures wider -layer network architecture remarkably effective obtaining competitive results cifar- standard augmentation random crops horizontal ﬂips state-of-the-art results cifar-. visualize effect zeroing learned connections stream trained resnet init model single layer time shows streams contribute accuracy relative residual transient streams changes different stages processing figure show performance architecture robust increasing depth residual blocks architecture allows training deeper residuals compared original resnet. interacting transformation streams stream includes shortcut connections also used blocks lstm grid-lstm networks however contrast highway networks control shortcut connections input-dependent carry transform gates memory hidden states lstm grid-lstm blocks information residual transient states generalized residual block gates thus implemented additional parameters standard feedforward network another difference previous architectures generalized residual block present memory hidden states lstm grid-lstm block calculated sequentially tanh) transformation residual memory streams generalized residual block occurs parallel depends learned convolutional ﬁlters layer without constraints relation. scrn architecture mikolov also uses hidden context units together within single layer learn longer term information behave similarly transient residual streams scrn allows unidirectional context hidden units connections context units ﬁxed contrast bidirectional streams learned connections transient residual streams generalized residual architecture. present generalized residual architecture simply implemented modiﬁed initialization scheme resnet init apply original resnet create novel architecture achieves state-of-the-art results. future work includes additional study related residual models determine cause beneﬁcial effects. references monica bianchini franco scarselli. complexity neural network classiﬁers comparison shallow deep architectures. neural networks learning systems ieee transactions deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision jost tobias springenberg alexey dosovitskiy thomas brox martin riedmiller. striving simplicity convolutional net. corr abs/. http//arxiv. org/abs/.. ilya sutskever james martens george dahl geoffrey hinton. importance initialization momentum deep learning. proceedings international conference machine learning implement generalized residual block modiﬁed initialization standard convolutional fully connected layer combines identity shortcut desired linear transformation call resnet init concatenating tensors residual transient streams form single tensor identity shortcut results same-stream cross-stream transformations summed give output stream linear operations composed single linear operation implement resnet init single layer concatenate weight matrices initialized existing scheme partial identity matrix concatenated weight matrix. implement resnet init single convolutional layer ﬁrst concatenate convolutional kernels along input dimension ﬁlter dimension similarly half identity kernel. output generalized residual block implemented standard layer modiﬁed initialization implemented separate linear operations generalized residual block implemented using modiﬁed initialization perform differently implemented separate linear operations weight regularization pulls weights towards zero present regularization. maintain equivalence implementation subtract partial identity weights prior application weight decay. table description network architectures used experiments comparing performance standard resnet resnet init models. standard resnet init models identity connections residual blocks listed cifar- models output layer units cifar- models output layer units. otherwise speciﬁed convolutions performed stride conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu global mean pool layer units softmax total number parameters conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu residual block conv. ﬁlters relu conv. ﬁlters global mean pool softmax total number parameters", "year": 2016}