{"title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in  Caffe", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. We have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder's encoder and decoder parts. Our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks, and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron. The best results were provided by a model where the encoder part contains convolutional and pooling layers, followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. We believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future.", "text": "abstract paper presents development several models deep convolutional auto-encoder caffe deep learning framework experimental evaluation example mnist dataset. created five models convolutional auto-encoder differ architecturally presence absence pooling unpooling layers auto-encoderâ€™s encoder decoder parts. results show developed models provide good results dimensionality reduction unsupervised clustering tasks small classification errors used learned internal code input supervised linear classifier multi-layer perceptron. best results provided model encoder part contains convolutional pooling layers followed analogous decoder part deconvolution unpooling layers without switch variables decoder part. paper also discusses practical details creation deep convolutional auto-encoder popular caffe deep learning framework. believe approach results presented paper could help researchers build efficient deep neural network architectures future. auto-encoder model based encoder-decoder paradigm encoder first transforms input typically lower-dimensional representation decoder tuned reconstruct initial input representation minimization cost function trained unsupervised fashion allows extracting generally useful features unlabeled data. unsupervised learning methods widely used many scientific industrial applications mainly solving tasks like network pre-training feature extraction dimensionality reduction clustering. classic shallow hidden layer lowerdimensional representation input. last decade revolutionary success deep neural network architectures shown deep many hidden layers encoder decoder parts state-of-the-art models unsupervised learning. comparison shallow number trainable parameters same deep reproduce input lower reconstruction error deep extract hierarchical features hidden layers therefore substantially improve quality solving specific task. variations deep deep convolutional auto-encoder which instead fully-connected layers contains convolutional layers encoder part deconvolution layers decoder part. deep caes better suited image processing tasks fully utilize properties convolutional neural networks proven provide better results noisy shifted corrupted image data modern deep learning frameworks i.e. convnet theano lightweight extensions lasagne keras torch caffe tensorflow others become popular tools deep learning research since provide fast deployment state-of-the-art deep learning models along state-of-the-art training algorithms allowing rapid research progress emerging commercial applications. moreover frameworks implement many state-of-the-art approaches network initialization parametrization regularization well state-of-the-art example models. besides many outstanding features chosen caffe deep learning framework mainly reasons description deep pretty straightforward text file describing layers caffe matlab wrapper convenient allows getting caffe results directly matlab workspace processing goal paper present practical implementation several models caffe deep learning framework well experimental results solving unsupervised clustering task using mnist dataset. study extended version paper published arxiv developed caffe .prototxt files reproduce models along matlab-based visualization scripts included supplementary materials. paper organized follows section describes relevant related work section presents developed models along practical rules thumb used build models section presents experimental results section discusses observations received experiments section contains conclusions. studies presenting advanced regularization parametrization techniques non-convolutional including deep denoising transforming contractive k-sparse variational importance-weighted adversarial aes. work ranzato first studies uses convolutional layers unsupervised learning sparse hierarchical features. model consists levels cae; convolutional maxpooling layers encoder part upsampling full convolutional layers decoder part. model trained independently using greedy layer-wise approach output first level serves input second level. authors suggested learn identity function trivial produce uninteresting features hidden layer describes code overcomplete. making code sparse overcome disadvantage. norouzi researched unsupervised learning hierarchical features using stack convolutional restricted boltzmann machines greedy layer-wise training approach. fully-connected operations substituted convolutional operations probabilistic max-pooling introduced deterministic max-pooling used norouzi argue convolutional rbms increased overcompleteness learned features suggest adding sparsity hidden features. masci investigated shallow deep caes hierarchical feature extraction trained greedy layer-wise approach. valid convolutional layers without max-pooling used encoder part full convolutional layers used decoder part. authors stated max-pooling layers elegant provide architecture enough sparsity additional regularization parameters needed. closest work results recent paper zhao entitled stacked what-where auto-encoders architecture integrates discriminative generative pathways provides unified approach supervised semi-supervised unsupervised learning. within unsupervised part swwae consists several convolutional max-pooling layers followed fully-connected layer encoder part inversely fully-connected unpooling deconvolution layers decoder part. swwae symmetric encoder decoder parts. terms what where correspond pooling appropriate unpooling operations proposed output max-pooling layer what variable next layer encoder part. complementary where variables max-pooling switch positions. what variables inform next layer content incomplete information position where variables inform corresponding feed-back unpooling layer decoder part max-pooled features reconstructed unpooled feature map. addition standard reconstruction cost function input level authors proposed middle reconstruction cost function corresponding hidden layers encoder decoder parts provide better model training. similarly solutions authors used dropout layer added fully-connected layers sparsity penalty hidden layers regularization technique. high quality extracted latent hierarchical features existing solutions analyzed confirmed state-of-the-art classification results obtained different classifiers trained supervised way. extracted features used inputs classifiers. however paper provide visualization extracted features two-dimensional space. believe visuals might considered inherent addition results presented studies above. pooling layer still controversial question theory practice cnns state-of-the-art approach building supervised convolutional models max-pooling layer computes maximum activation units small region previous convolutional layer. encoding result convolution operation max-pooling allows higher-layer representations invariant small translations input reduces computational cost scherer shown max-pooling operation considerably better capturing invariances image data compared subsampling operation. unsupervised convolutional models masci shown without max-pooling layers learns trivial solutions interesting biology plausible filters emerge trained max-pooling layer. zhao proven swwae max-pooling unpooling layers provides much better quality image reconstruction maxpooling unsampling layers. controversially recent findings springenberg proven maxpooling operation simply replaced convolutional operation increased stride without decreasing accuracy several image recognition benchmarks. previous paper presented without pooling unpooling layers provided acceptable quality dimensionality reduction unsupervised clustering tasks therefore included pooling unpooling layers study aiming find model without pooling unpooling layers better. taking account deep unsupervised model complex model training perspective question training approach important. chosen caffe research implements stateof-the-art training approach called top-down terms jointly trained multiple layers jointly trained models top-down approach implies efficient training hidden layers model respect input greedy layer-wise training approach specifies layer receives input latent representation layer trains independently. zeiler argued major drawback greedy layer-wise approach image pixels discarded first layer thus higher layers model increasingly diluted connection input. makes learning fragile impractical models beyond layers moreover zeiler proven conclusions comparison top-down greedy layerwise approaches showing significantly better performance former caltech- database. advantage top-down training approach greedy layer-wise proven swwae study also several practical solutions/attempts develop model different platforms shallow convolutional matlab deep theano/lasagne theano/keras torch neon general caffe models end-to-end machine learning systems. typical network begins data layer loads data disk ends several loss layers specify goal learning models include convolutional pooling fully-connected deconvolution unpooling loss layers. brevity include description fully connected layer pretty well-known. latent representation feature current layer activation function feature group feature maps previous layer -th-channel input image total channels case first convolutional layer denotes convolution operation convolutional layer performs â€˜valid convolutionâ€™ image feature size decreasing. deconvolution layer performs â€˜full size output feature increasing convolutionâ€™ size output feature convolutional layers provide encoding input image decreasing output feature maps layer layer encoder part inversely deconvolution layers provide reconstruction input image increasing output feature maps layer layer decoder part. max-pooling layer pools features taking maximum activity within input feature maps produces output feature reduced size according size pooling kernel supplemental switch variables describe position max-pooled features unpooling layer restores max-pooled feature either correct place specified switches specific place within unpooled output feature map. fig. illustrates convolution deconvolution pooling unpooling operations. used caffe implementation unpooling layer provided successfully applied semantic segmentation task used following practical rules thumb creating models caffe main issue deep asymmetry thus model symmetric terms total size feature maps number neurons hidden layers encoder decoder parts. sizes numbers decrease layer layer encoder part increase decoder part providing encoder-decoder paradigm i.e. contractive sizes numbers less minimal values allowing handling size input problem informational point view; cost function better loss layers <sigmoid_cross_entropy_loss> <euclidean_loss> preliminary experiments shown loss layers separately provide good training convergence visualization values trainable filters feature maps hidden units layer layer plays important diagnostic role. allows inspect function intermediate layers therefore better understand data converted/processed inside deep model main purpose activation function layer non-linear data processing since nature convolution/deconvolution operations multiplication visualization showed result convolution/deconvolution operations increasing sharply layer layer preventing model converging training. thus sigmoid hyperbolic tangent activation functions constrain resulting values feature maps interval respectively sets appropriate limits values feature maps decoder part provides good convergence whole model; well-known fact good generalization properties depend ratio trainable parameters i.e. weights biases size input data therefore experimentation required find architecture best generalization properties. then similar size terms total size feature maps number neurons hidden layers could considered appropriate starting point designing good generalization properties. direct comparison models inappropriate size given fully-connected network would fewer trainable parameters created model stable. training converge different local optima different runs depending initial weights/biases. stable model mean convergence results obtained several consecutive runs model notation contains convolutional layers followed fully-connected layers encoder part inversely fully-connected layer followed deconvolution layers decoder part. model investigated arxiv paper model notation contains pairs convolutional pooling layers followed fully-connected layers encoder part inversely fully-connected layer followed deconvolution layers decoder part. table model symmetric. however included seen similar idea neon deep learning framework wanted study model notation deconv unpool) contains pairs convolutional pooling layers followed fully-connected layers encoder part inversely fullyconnected layer followed pairs deconvolution unpooling layers switch variables decoder part. unpooling layer works follows max-pooled feature restored correct place within unpooled output feature specified switch variable. elements zeros. model similar networks presented model notation contains pairs convolutional pooling layers followed fully-connected layers encoder part inversely fully-connected layer followed deconvolution unpooling layers without switch variables decoder part. unpooling layer works follows max-pooled feature restored predetermined place within unpooled output feature elements zeros. approach could called centralized unpooling max-pooled feature placed center pool implementation used specific position left corner pool. research results unpooling mode presented model notation deconv unpool model model except using hyperbolic tangent activation function layers. previous model model standard sigmoid activation function. size models used mnist dataset experimental research. experiments used standard examples training examples testing set. size mnist training dataset examples elements |data| elements. using number calculate ratio trainable parameters models size input data |data/| experiments below. created version mnist dataset applied modification input images except normalization range used mnist labels visualization clustering results. caffe examples models solve dimensionality reduction task semi-supervised siamese network proposed hadsell deep proposed hinton included models results comparison models siamese network consists coupled lenet architectures followed contrastive loss function. trains semi-supervised fashion since form training labelling pair input images images belong class otherwise. placed siamese model first table semi-supervised model first list followed unsupervised models. siamese network available examples changed number planes convolutional layers number neurons fullyconnected layers encoder part models left relu layer fully connected layers caffe examples according rule section above researched deep previous paper shown exact deep architecture presented provides best generalization properties different models. model specified second table rule says model total size feature maps number neurons hidden layers could considered appropriate starting point designing model good generalization properties. however research results models without pooling unpooling layers actually shown model twice gives better results. model model specified third table model training parameters |data/| ratio therefore created models trying keep |data/| ratio model model respectively. note models half size swwae trainable parameters table contains architecture parameters researched models. table contains detailed calculation number trainable parameters siamese network. table contains detailed calculation number trainable parameters deep models only models parametrically similar model trained model three fashions neurons last hidden layer encoder part <ipencode> corresponds -dimensional space encoding respectively. tables calculations provided case. graphically layer <ipencode> looks different since linear inner product operation only green rectangle activation function; find blue rectangle <ipencode> somewhere center model. followed blob title yellow octagon <ipencode> keeps result operation. actual place i.e. neurons data stored. calling internal code models. also serves input decoder part. therefore graphically models fully-connected layers <ipencode> <ipencode> encoder part fullyconnected layer <ipdecode> decoder part. therefore rhetoric models fullyconnected layers encoder part fully-connected layer decoder part. however provides symmetric number feature maps number neurons hidden layers provide better understanding notations table well figs. explain conv/deconv layers model model encoder part model planes kernel conv layer planes kernel conv layer. decoder part planes kernel deconv layer planes kernel deconv layer. model planes kernel conv layer followed max-pooling layer pool kernel planes kernel conv layer followed max-pooling layer pool kernel encoder part. decoder part model planes kernel deconv layer followed unpooling layer unpool kernel planes kernel deconv layer followed unpooling layer unpool kernel decoder part purpose deconvolution layer deconvneur corresponds term model term model third column table reshape feature maps last decoder layer deconv unpool reconstructed image size pixels original mnist image. explanation caffe user group since deconvolution operation nature convolution used approach calculate number trainable parameters encoder decoder parts accuracy calculations easily checked calling caffe matlab python using command <caffe;>. tables models practically symmetric terms convpoolconvpool- ------ ------ convconv--deconvdeconv ------ convpoolconvpool--deconvdeconv ------- convpoolconvpool--deconvunpooldeconvunpool --------- model encode->+ encode->+ encode->+ encode->= conv->*i*o)+ conv->*i*o)+ ipencode->+ ipencode-> conv->*i*o)+ conv->*i*o)+ ipencode->+ ipencode-> conv->*i*o)+ conv->*i*o)+ ipencode->+ ipencode-> decode->+ decode-> decode->+ decode->= ipdecode->+ deconv->*i*o)+ deconv->*i*o)+ deconvneur->*i*o)+ ipdecode->+ deconv->*i*o)+ deconv->*i*o)+ deconvneur->*i*o)+ ipdecode->+ deconv->*i*o)+ deconv->*i*o)+ deconvneur->*i*o)+ dimensionality reduction results experimental results siamese network deep along withmodels presented table three runs model. unsupervised models values loss layers <sigmoid_cross_entropy_loss> <euclidean_loss> separated semicolon. presented value <contrastive_loss> layer siamese network. values loss layers presented training black test set. learning parameters solver models same training iterations used stochastic gradient descent algorithm used hundred patterns batch base learning rate learning policy \"fixed\" weight decay equal standard sigmoid activation function used deep models standard hyperbolic tangent activation function used model many experiments changing architecture learning parameters many cases stable architectures. example tried different initializations weights biases presented results obtained following initialization <bias_filler {type \"constant\"}> layers <weight_filler {type \"xavier\"}> convolutional/deconvolution layers <weight_filler {type \"gaussian\" sparse fullyconnected layers. order estimate quality feature extraction general quality encoded internal code particular used internal code input classifier literature goal experiment apply classifiers structure learning parameters estimate model provides best internal code determined classification results. used simple classification models linear model multi-layer perceptron implemented matlab statistics machine learning toolbox neural network toolbox respectively. created three separate models input code keeping ratio |data/| thus used three models three cases. standard matlab functions used <patternnet> create <train> train <net> provide classification results. used sigmoid softmax activation functions hidden output layers respectively cases. following learning parameters used training accuracy minimum training gradient training epochs maximum training validations checks scaled conjugate gradient backpropagation algorithm â€˜trainscgâ€™ used training. used -fold cross-validation calculated average per-class classification error classification results specified seventh eighth columns table last column table contains sparsity estimation internal code researched models obviously shown better classification results linear model. semi-supervised siamese network showed best classification results comparison unsupervised models. model model provided slightly better results deep model pooling-unpooling layers switches obtained worst classification results model pooling-unpooling layers without switches shown best classification results results model show hyperbolic tangent activation function provides worse results sigmoid activation function. comparison also classification experiment using number trainable parameters learning parameters -fold cross validation original -dimensional mnist images. obtained worse average per-class classification error. therefore conclude developed models provide good quality feature extraction dimensionality reduction mnist dataset obtained classification errors less case linear model less case mlp. also collected training times unsupervised models modes three computational systems located canadian centre behavioural neuroscience university lethbridge canada workstation operated ubuntu equipped -core inter xeon processor nvidia geforce gpu. fermi architecture cuda cores compute capability workstation polaris operated ubuntu equipped -core inter xeon processors nvidia geforce titan gpu. maxwell architecture cuda cores compute capability cluster hodgkin operated enterprise linux. cluster consists blade servers. server equipped -core intel xeon processors ram. cluster total cores ram. used hodgkin mode only. since cores many models parallel therefore received results computational platform. interestingly table relation training times straightforward computational mode bigger models obviously train longer. training times mode smaller bigger models larger feature maps conv/deconv layers trainable parameters comparison models gpu-implementation conv/deconv layers caffe much better optimized appropriate implementation thanks nvidia cuda technology cudnn library clustering visualization results used t-sne technique visualize internal code produced models. t-sne technique advanced method dimensionality reduction visualization. based stochastic neighbor embedding converts high-dimensional euclidean distances datapoints conditional probabilities represent similarities. minimizes kullback-leibler divergences datapoints using gradient descent method. t-sne differs ways uses symmetrized version cost function simpler gradients uses student-t distribution rather gaussian compute similarity points low-dimensional space. allows t-sne cost function easier optimize produces significantly better visualizations advanced technology used visualization many state-of-the-art problems example visualize last hidden-layer representations deep q-network playing atari games clustering task encode instances mnist test space. siamese network semi-supervised fashion deep models unsupervised fashion. visualization results previous paper shown difference visualized internal codes. therefore presenting visualizations only. visualizations siamese network deep models solve clustering task depicted figs. respectively. table shows deep models provide reconstruction several example mnist images. clustering results figs. reconstruction results table confirm numerical results table model provided smallest classification errors shown best clustering results among unsupervised models. clusters produced model dense look qualitatively similar result semi-supervised siamese model comparing models. model also shown second-best reconstruction quality mnist images among unsupervised models model model shown contradictory results. provided worst classification errors table worst clustering results however shown smallest values cost function practically ideal reconstruction quality mnist images discuss results section mentioned rule visualization deep network plays important diagnostic role allows easily inspect function intermediate layers. similarly zeiler fergus stated visualization allowed find model architectures outperform krizhevsky imagenet classification benchmark visualization allowed basically create models. implementation visualization simple straightforward thanks matlab wrapper implemented caffe. order visualize model necessary create number <.prototxt> files corresponding number layers. training appropriate <.caffemodel> file saved weights biases necessary call caffe matlab using <.prototxt> files argument. received values produced layer visualized. visualization example model encodes decodes digit depicted fig. visualizations models look similar. fig. read together graphical representation model fig. processing results intermediate layers stored blobs. blobs matrixes depicted yellow octagons fig. visualized content fig. panel fig. corresponds appropriate blob fig. title panel fig. starts name corresponding blob fig. seen fig. blobs inputs. example blob conv inputs convolutional layer conv layer sigen uses blob conv input performs input transformation sigmoid activation function stores result back blob conv. therefore titles appropriate panels fig. title conv|conv means panel visualizes content blob conv stored convolutional layer conv title conv|sigen means panel visualizes content blob conv stored sigmoid layer sigen. approach used panels fig. next element title panel size appropriate blob example panel conv|conv means blob conv contains feature maps size elements; panel ipencode|sigen means blob ipencode contains neurons size elements; titles panels square brackets indicated minimum maximum values appropriate blob. panels visualize blobs ipencode ipencode ipdecode belonging fully-connected layers contain numerical representation ordinate axis. thus title every single panel fig. clearly describes processing results provided every intermediate layer deep network. visualizations especially numerical representation blobs allowed understand failed experiments outputs deconv deconv layers negatively positively saturated therefore pixels final reconstructed image layer deconvneursig value values loss layers training. developed caffe .prototxt files along matlab-based visualization scripts included supplementary materials made also available caffe user group web-page note developed models work caffe version used/contributed models latest version seems newer versions caffe developers changed syntax layer descriptions layers layer layersâ€™ types convolution convolution etc. internal representation fully-connected layers array previous version. deal issues necessary change syntax .prototxt files accordingly change dimensionality blob last fully-connected layer ipdecode first deconvolution layer decode decoder part using reshape layer follows <layer {name \"reshape\" type \"reshape\" bottom \"ipdecode\" \"ipdecodesh\" reshape_param shape }}}> variable \"ipdecodesh\" input following layer decode. model switches provide worst dimensionality reduction unsupervised clustering? model featured pooling-unpooling layers switches. reached much smaller value cost function provided practically ideal reconstruction input images however fulfilled dimensionality reduction unsupervised clustering tasks worst way. section discuss interesting situation. explanation mentioned zhao paper mnist dataset simple problem model model model similar swwae presented perfect reconstruction mnist images figure zhao stated reason swwae working well fact reconstructing mnist digits overly easy swwae. since mnist roughly binary dataset thus within unpooling stage decoding necessarily demand information â€˜whatâ€™ reconstruction; i.e. could perfect reconstruction pinning positions indicated â€˜whereâ€™. therefore believe reconstructing mnist dataset renders insufficient regularization encoding pathway addition statement results show model pooling-unpooling layers without switches decoder part necessarily demand where information successful reconstruction what information. model restores max-pooled features predefined positions unpooled feature maps therefore prevents much self-adapting. centralized unpooling plays role peculiar penalty penalizes sensitivity model input encourages robustness learned representation model reconstruct input image perfectly; however showed best quality dimensionality reduction unsupervised clustering among considered unsupervised models. present alternate explanation model poor dimensionality reduction unsupervised clustering performance. principal difference model considered unsupervised models switch variables considerably increases dimensionality internal representation model model low-dimensional internal code switch variables used reconstruction unsupervised models low-dimensional internal code only. thus model contractive anymore. dimension internal representation much bigger dimension input mnist image model simply learns identity function trivial produces useless uninteresting features; internal code overcomplete corresponds description overcompletness features norouzi following what happens iterations parameter update sampled images become close original ones learning signal disappears. confirmation features model overcomplete fig. last hidden internal code model overcomplete even state-of-the-art t-sne method cannot find appropriate similarity datapoints; barely distinguish clusters digit digit stated sparsity overcomplete code low. specifically zhao stated quotation regularization model insufficient means sparsity low. similarly zeiler calculated relative sparsity feature maps calculated sparsity hidden internal code models used sparsity measure dimensionality number describes sparsity vector analyzed vector smaller threshold means bigger value specifies bigger level sparsity. three experiments three values threshold maximum value analyzed internal code models presented table received similar results thresholds specified average value sparsity threshold three runs table rather specify sparsity number specified percentage elements internal code smaller threshold appropriate internal code. example sparsity model means total elements code within test patterns less maximum value found total elements. allows compare models. note means elements internal relative measure code elements internal code elements internal code. analysis table shown sparsity internal code model less models case interval minimum siamese network maximum model case interval minimum model maximum model case interval minimum model maximum model results show clear relation better sparsity internal code better average per-class classification error state obviously sparsity internal code model less successfully-working models. well-known regularization techniques example dropout weight decay improve sparsity deep models prevent trainable parameters growing large. therefore estimated sparsity trainable parameters researched models using expression approach described above. again threshold successfully-working models shown bigger sparsity model shown lower value overcompleteness model internal representation makes reconstruction overly easy explaining good reconstruction poor clustering performance. complementary explanation arises realize essential function compression finding low-dimensional code represent high-dimensional input low-dimensional information-rich code represents useful clustering data facilitate effective classification. propose switch variables undermine autoencoderâ€™s principal role compressor. illustrate this consider compression ratio achieved model simple approximate compression ratio assume switch variables values uniformly distributed ranges. simple assumption mnist image pixels range requires bytes encode single-precision codes require bytes respectively. switch variables stored model first pooling layer integer address range thus requires least bits encode. switch variables second pooling layer take values range require bits encode. thus model switch variables internal code together total bytes information significantly information stored input image itself giving negative compression ratio switch variables causes model inflate rather compress input data. contrast unsupervised models produce internal codes achieve compression ratios respectively. thorough analysis compression ratio could measure self-information entropy suppose simple calculation sufficiently illustrative. model inflation rather compression input data short-circuits training process normally guides achieve minimal reconstruction error effective compression. extra information cached switch variables allow model achieve good reconstruction training without learning effective compression scheme explains resulting internal codes clustering classification value. experiments relu activation function relu activation function proven better supervised deep machine learning tasks research studies provides better sparsification deep model models sparser coding provide better classification results. useful property relu function serve unsupervised models too. summary different activation functions existing solutions following used sigmoid used sparsifying logistic used scaled hyperbolic tangent used hyperbolic tangent used relu. used relu function models they surprisingly train. experiments using initialization parameters weights biases described section above i.e. <bias_filler {type \"constant\"}> layers <weight_filler {type \"xavier\"}> convl/deconv layers <weight_filler {type \"gaussian\" sparse fully-connected layers models contained value feature maps values cost function training too. initialized weights biases within smaller interval i.e. used <weight_filler {type \"gaussian\" <bias_filler {type \"constant\" value layers models calculated values cost function first training iteration only. first iteration values changed slightly; staying moreless training stopped. opinion necessary find correct initialization model relu activation function ideas described pylearn user group then hopefully better sparsification properties relu activation function could able provide better results models sigmoid hyperbolic tangent activation functions. future lines research. development several deep convolutional auto-encoder models caffe deep learning framework experimental evaluation mnist dataset presented paper. contrast deep fully-connected auto-encoder proposed hinton convolutional auto-encoders allow using desirable properties convolutional neural networks image processing tasks working within unsupervised learning paradigm. created researched five convolutional auto-encoder models caffe model contains convolutional layers followed fully-connected layers encoder part inversely fully-connected layer followed deconvolution layers decoder part model contains pairs convolutional pooling layers followed fully-connected layers encoder part inversely fully-connected layer followed deconvolution layers decoder part model contains pairs convolutional pooling layers followed fully-connected layers encoder part inversely fully-connected layer followed pairs deconvolution unpooling layers switch variables decoder part model model without switch variables decoder part model model except using hyperbolic tangent activation function layers hidden low-dimensional internal code learned models unsupervised used input linear classifier standard one-hidden-layer perceptron classification errors model estimated. results show developed models provide good results dimensionality reduction unsupervised clustering tasks small classification errors. specifically model model without pooling unpooling layers provide slightly better results deep fullyconnected auto-encoder model pooling-unpooling layers without switches shows best result model shows hyperbolic tangent activation function instead sigmoid provides worse results model pooling-unpooling layers switches shows practically ideal reconstruction input images worst fulfilment dimensionality reduction unsupervised clustering tasks therefore large classification errors think model switch variables considerably inflates dimensionality internal representation. model likely learns trivial identity mapping. creation models convolutional auto-encoder followed several rules thumb mentioned section above used many machine learning researchers every day. paper also discusses practical details creation deep convolutional auto-encoder popular caffe deep learning framework. developed caffe .prototxt files along matlab-based visualization scripts included supplementary materials made also available caffe user group web-page believe approach results presented paper could help researchers build efficient deep neural network architectures future. would like thank caffe developers creating powerful framework deep machine learning research. thank karim help caffe installation hodgkin polaris hyeonwoo using caffe implementation unpooling layer discussions results presented paper robert sutherland help financial support. d.e. rumelhart g.e. hinton r.j. williams learning representations back-propagating errors nature. lecun modeles connexionistes lâ€™apprentissage ph.d. thesis universite paris bourland kamp auto-association multilayer perceptrons singular value decomposition biological cybernetics. g.e. hinton r.r. salakhutdinov reducing dimensionality data neural networks science. lecun bottou bengio haffner gradient-based learning applied document recognition proc. ieee. keras deep learning library theano tensorflow. https//keras.io/ collobert kavukcuoglu farabet torch matlab-like environment machine learning shawe-taylor r.s. zemel p.l. bartlett pereira k.q. weinberger advances neural information processing systems nips foundation inc. granada turchenko. luczak creation deep convolutional auto-encoder caffe arxiv. ranzato f.j. huang y.-l. boureau lecun unsupervised learning invariant feature hierarchies applications object recognition ieee conference computer vision pattern recognition ieee minneapolis vincent larochelle bengio p.a. manzagol extracting composing robust features denoising autoencoders international conference machine learning international machine learning society helsinki g.e. hinton krizhevsky s.d. wang transforming auto-encoders lecture notes computer sci. rifai vincent muller glorot bengio contractive auto-encoders explicit invariance feature extraction burda grosse salakhutdinov importance weighted autoencoders arxiv. makhzani shlens jaitly goodfellow frey adversarial autoencoders international conference learning grosse ranganath a.y. convolutional deep belief networks scalable unsupervised learning hierarchical representations annual international conference machine learning york dosovitskiy fischer springenberg riedmiller brox discriminative unsupervised feature learning exemplar convolutional neural networks ieee transactions pattern analysis machine intelligence. cuda zone. https//developer.nvidia.com/cuda-zone nvidia cudnn accelerated deep learning. https//developer.nvidia.com/cudnn l.j.p. maaten hinton visualizing data using t-sne machine learning res. g.e. hinton s.t. roweis stochastic neighbor embedding advances neural information processing systems luczak lethbridge brain dynamics. http//lethbridgebraindynamics.com/artur-luczak/ hurley rickard comparing measures sparsity ieee transactions inf. theory. geman bienenstock doursat neural networks bias/variance dilemma neural computation. rectified linear units autoencoder. https//groups.google.com/forum/topic/pylearn-dev/iwqctwnkag brest polytechnic university belarus ph.d. computer engineering lviv polytechnic national university ukraine. worked postdoctoral fellow funded marie curie grant university calabria italy fulbright scholar university tennessee researching parallelization schemes artificial neural networks. currently postdoctoral fellow canadian centre behavioural neuroscience university lethbridge. research interests include theory application artificial neural networks deep machine learning. chalmers received b.sc. electrical engineering ph.d. electrical computer engineering university alberta canada. currently postdoctoral fellow canadian centre behavioural neuroscience university lethbridge. research interests include brain-inspired machine learning intelligent systems. rtur luczak received m.sc. biomedical engineering wroclaw university technology ph.d. jagiellonian university poland. postdoctoral fellow worked yale rutgers university studying neural information processing. currently faculty canadian centre behavioural neuroscience university lethbridge research involves high density neuron recordings study brain dynamics.", "year": 2017}