{"title": "Learning to update Auto-associative Memory in Recurrent Neural Networks  for Improving Sequence Memorization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Learning to remember long sequences remains a challenging task for recurrent neural networks. Register memory and attention mechanisms were both proposed to resolve the issue with either high computational cost to retain memory differentiability, or by discounting the RNN representation learning towards encoding shorter local contexts than encouraging long sequence encoding. Associative memory, which studies the compression of multiple patterns in a fixed size memory, were rarely considered in recent years. Although some recent work tries to introduce associative memory in RNN and mimic the energy decay process in Hopfield nets, it inherits the shortcoming of rule-based memory updates, and the memory capacity is limited. This paper proposes a method to learn the memory update rule jointly with task objective to improve memory capacity for remembering long sequences. Also, we propose an architecture that uses multiple such associative memory for more complex input encoding. We observed some interesting facts when compared to other RNN architectures on some well-studied sequence learning tasks.", "text": "simulate human answer question read passage question sequentially word-by-word single lstm chain. composes passage question words single hidden state infer answer state. accuracy relatively low. effective methods reading comprehension usually alleviate stress lstm compare question representation directly passage word representation attention mechanism. mainly ideas enhance rnns deal long inputs attention mechanism external memory. attention mechanism speciﬁcally applications neural machine translation machine comprehension external memory built rnns. example machine reading comprehension passage related question encoded separately rnns word corresponding hidden state. then passage word hidden state question hidden state composed generate attention score denoting similarity passage word state question. attention mechanism facto industry standard helps achieve state accuracy datasets stanford question answering dataset although effective network attention mechanism scales linearly input computationally inefﬁcient network complex. register memory models neural turing machines introduce mechanism enhance recurrent neural networks distributing inputs differentiable external memory block alleviate state compression. copy memory state preserved time step memory addressing mechanism controls reading writing memory block always lead welldistributed content thus reducing effect memory block. problems impair families popularity resolving real-world problems involve long input sequences. ways enhance could naively increasing networks layers inserting intermediate steps between adjacent time steps adding bi-directional encodings learning remember long sequences remains challenging task recurrent neural networks. register memory attention mechanisms proposed resolve issue either high computational cost retain memory differentiability discounting representation learning towards encoding shorter local contexts encouraging long sequence encoding. associative memory studies compression multiple patterns ﬁxed size memory rarely considered recent years. although recent work tries introduce associative memory mimic energy decay process hopﬁeld nets inhershortcoming rule-based memory updates memory capacity limited. paper proposes method learn memory update rule jointly task objective improve memory capacity remembering long sequences. also propose architecture uses multiple associative memory complex input encoding. observed interesting facts compared architectures well-studied sequence learning tasks. recent years recurrent neural networks widely used deep-learning solutions many real-world tasks. however rnns usually challenged encode long sequences popular rnns choices able handle easily. example machine reading comprehension text entailment neural machine translation always require long sentences passages input; text generation tasks image captioning text summarization always require rnns generate many output words time; reinforcement learning tasks dialogue systems planning control frequently uses rnns policy networks action sequence generation. delayed reward signal mandates rnns retain intelligently compose historical information memory time. however accumulates input information ﬁxedsize vector. despite understanding rnns lstm turingcomplete could theoretically simulate function reality hard encode long sequences complexity problem. example deep lstm reader built machine reading comprehension task uses lstm would investigate attention memory could remember long sequences efﬁciently number inputs scales need increase memory much approaches mentioned fact answer decades. associative memory efﬁcient working memory first proposed later populated associative memory models hopﬁeld nets studies problem storing multiple input patterns single network fully interconnected artiﬁcial neurons. main research question increase network capacity i.e. maximum number input patterns stored almost perfectly recovered controlled following aspects neuron connection weight matrix deﬁnes connection strength pair neurons. memory update rule controls modify states neurons collectively total energy network minimized using weight matrix. hopﬁeld nets optimized unsupervised maximum likelihood estimation towards minimum system energy storing collection input patterns. settling process conducted update network memory states neuron connection weight matrix predeﬁned stay unchanged throughout course. continuous version associative memory deep boltzmann machines restricted boltzmann machines recurrent temporal share concept settling process unsupervised feature learning. objective process data independent objective goal network label conditioned learning claimed help tasks image classiﬁcation though widely observed fact. also boltzmann machine perform sampling estimate partition function gradient calculation inevitably introduces high computational cost. making hopﬁeld nets boltzmann machines recurrent investigated before. example rtrbm dybm merges energy minimization objective back propagation time algorithm seen supervised task-speciﬁc ﬁnetuning integrated learning procedure. example image classiﬁcation could ﬁrst pre-train weights towards minimizing associative memory system energy learn representation image later learned weights start point ﬁne-tune networks towards task objective e.g. image classiﬁcation. pre-training although maximizing utility training data discovering shape invariant representation usually classiﬁcation target takes much longer time sampling gradient estimation. work naturally addresses issue retargeting objective settling process supervised task objective. favorable learn associative memory updates bptt. energy annealing phase needed suggested under-ﬁtting memory higher energy tune system ﬁne-tuning towards supervised task goal beneﬁcial. admittedly thinking could diminish effect unsupervised feature learning. guaranteed elaborated feature learning could success different problems. paper create learn-able memory accumulation rule different resembles energy minimization rule tuned towards task objective bptt. argue method align idea associative memory learning task ﬁne-tuning preserving strength associative memory architecture. question memory stored rnn. would create external memory additional transformations memory addressing necessary rnns memory component. associative memory perform explicit addressing though easier integrate rnns. cells provide room requirement. think weights hidden states lstm example. gates contains ri×h rh×h used balance contribution input hidden state sufﬁcient purpose introduces additional transformation another vector shape. convenient associative memory storage without adding term call weight matrix various ways create memory notably introduces concept fast weights form composing fast slow weights deblur images providing partial image input effect similar hopﬁeld net. recently creates using fast weights accumulates outer-product hidden states. learned end-to-end back-propagation supervised task objective method understood attention history. although tries mimic hopﬁeld nets energy minimization multistep memory processing intermediate steps still involved. thus multi-step process shows little effect. however accumulation input hidden state outer-product interesting since provides recipe storing sequence data different recurrent boltzmann machines hopﬁeld nets. setting memory connection weight matrix deﬁned interaction input hidden states changing time memory update rule critical memory learning still rule-based. could also help increase memory capacity i.e. connection weights composed input bits instead hopﬁeld nets update rule hopﬁeld nets could applied. intuition higher order weights could distribute input patterns larger space order weights collision patterns could alleviated. however expansion higher order connection matrix elegant solution since size associative memory weight grow exponentially increase order weights. instead increasing order neuron connection weights neuron update rule also lead change memory capacity. abundant research manually deﬁned update rules hopﬁeld nets. boltzmann machines. learning memory updating end-toend rnn. thus associative memory update process task-oriented ﬁne-tuning aligned. compared latest fast weights work make fundamental intuitive change effectively increased memory capacity. meanwhile difference network design embarrassingly small. increase memory capacity non-linearly also several associative memory blocks addressing mechanism control block inputs routed dynamically. linearly increasing cells could lead interesting non-linear storage patterns inputs different types. routing mechanism generates attention associative memory blocks. combining contributions mentioned previous paragraphs created architecture uses associative memory working memory reuses excess weights uses learned memory update rule increase memory capacity; scales array associative memory blocks enhance input representation. next session going discuss detail contributions section diving architecture describe learned associative memory update critical success model. closely related work fast weights work introduced auto-associative memory accumulating outer-product hidden state form memory. recurrent update tanh accumulation outer-product λat− +ηht⊗ht denotes outer-product scalars tanh hyperbolic tangent function rh×h ri×h hidden unit size input dimension. although driven idea associative memory rationale fast weight explained attention mechanism atht attentive history. effect fast weights explained correspondence hopﬁeld neurobiological intuition. krotov hopﬁeld explain capacity hopﬁeld bounded speed energy decay related second-order neuron connection weight. could explain inferior hidden state ﬁrst-order means cross connection deﬁned intuition higher order connection easier inputs stored. example input vectors orthogonal higher order memory easier learn distribute inputs different locations memory resolve input conﬂict ﬁrst-order vector memory capacity memory increase. thus uses second-order auto-associative memory could better chance storing recalling input contents. however also experimented higher order memory connection weights showed nonsigniﬁcant improvement second-order one. observation indicates connection matrix could easily storage limit hopﬁeld-net-like memory update rule better update mechanism critical enable better memory storage. uses ﬁxed update rule scaler hyperparameters control fast accumulation decays well much input discounted accumulated although efﬁcient remembering short inputs scalar hyperparameters limit speed energy decay terms krotov hopﬁeld speciﬁcally different bits decay rate makes storing disentangled input representation harder; decay rate ﬁxed advance instead learned automatically limits efﬁcient exploration memory update speed different bits generate disentangled memory storage. hadamard product i.e. bit-wise multiplication. again form although simple fundamentally changed associative memory accumulates. hypothesize weight could change direction stored input towards orthogonal other intelligently distributing inputs increase memory capacity. attention mechanism perspective could regard parameterized attention using angle hard explain signiﬁcance compared fast weights. different using static scalars adjusted iteration iteration learning identical learnable parameters recurrent cell. obviously addition introduces overhead training correspondingly. network testing difference negligible. idea behind learn interpolate attention attention used balance contribution terms. scalar vector length auto-associative memory array update provides simpler version update make update efﬁcient introduce cross-talk term rh×h. third term directly models hadamard product outer product. design driven effectiveness types attention mechanism machine-translation-style attention encoderstate decoder-state weighted addition readingcomprehension-style attention score state-state product. choosing addition product depend whether hidden states compared lies space. third term provides chance discover relationships ﬁrst terms provide capacity associative memory enhanced. also notice term weight performs bitwise multiplication instead matrix production make learning faster adequate serve purpose input accumulation. function work simpliﬁed identity function increase network training efﬁciency enabled faster convergence using hyperbolic tangent. also tried removing third term simplify form observed slightly worse accuracy across tasks conﬁrmed effectiveness formation. writing read memory active memory simply stores hidden states denoted however directly content generate retrieved content probability distribution array soft-selected generate single sharp distribution. product attention demonstrates retrieval process attention memory. attended enables non-linear memory composition found single memory similar register memory’s memory slot addressing mechanism. mechanism effective even addressing cannot learned. also regarded attention attention mechanism ﬁrst enforced router second attention series weinet consists following major components shown figure input encoding controller converts input signal internal representation router takes soft-select associative memory process array auto-associative memory array auto-associative memory {a}k. self-loop denotes self-update process matrix work. active memory column active memory trivially stores outputs previous layer. memory reader memory reader generates output whole network read auto-associative memory active memory. figure shows auto-associative memory impacts time space dimension temporally cell updated state; spatially goes newly updated transformation. whole networks although looks intimidating simple components described below input encoding controller parameterized recurrent function takes input previous hidden state memory reader output generate hidden unit size with introducing could make aware output previous reader module. however also understood highway connection shortcuts generated full transformation pipeline cell. found using lstm cells updates leads signiﬁcant learning slow-down difﬁculty learning gates lstm. mitigate gradient vanishing problem applied layer normalization right generating regularize output passing next time step. routing auto-associative memory array router recurrent function rh×h. denotes column-wise row-wise mean weighted average using adding statistics useful hidden state ignored non-signiﬁcance task objective. also introduce highway connection provide shortcut skip memory components. hopﬁeld nets weinet radically different handling inputs. take image classiﬁcation task example hopﬁeld stores image shot encode relation among pixels. multiple images remembered learning rule-based neuron connections adjusting memory whole dataset. however weinet handle task time step handle partial image compose together representation full image. whole dataset remembered adjusting network weights training. recurrence provides chance handle complex input patterns sequential data hopﬁeld nets difﬁcult handling. boltzmann machines also recurrent variants temporal restricted boltzmann machines dynamic boltzmann machines trbm models interaction visible units hidden units mechanism similar feed-forward neural nets neurons fully connected. weinet well models interactions different various time steps also interactions bits within outer-product like hopﬁeld nets dynamic boltzmann machines uses full neuron connection similar networks sense. however connection weights stdp-driven modeled follow parameterized exponential decay process. weinet’s update rule simply matrices scalar weights. uses ﬁxed scalers even simpler less ﬂexible weinet’s counterpart. associative recall task associative recall task requires recurrent networks remember character sequence retrieve speciﬁc character spatially related query character. example input sequence looks like ckjf??k sequence remember ckjf search previous sequence. special character separating sequence query neural network asked encode whole concatenated sequence left right character time last hidden state encoding predict retrieved answer successfully handle task recurrent neural networks capable remembering input sequence also interactions characters. would like test models different length sequences provide insights model’s memory capacity. three figure comparing weinet fast weight layer normalization recurrent highway networks layer normalization lstm layer normalization associative recall task using different input lengths. compare weinet lstm recurrent highway networks fast weights layer normalization settings. model hidden units recurrent layer. adam optimizer learning rate mini-batch size fw-ln follows setting coupled gates; lstm general architecture without peephole connections weinet uses fast weight matrix without router networks comparable models. setting corresponds using single instead weighted uses column mean single memory simpliﬁed version degrades weinet structure similar difference auto-associative memory updates. table figure shows test accuracy length- length- length- sequences. weinet achieved accuracy three tasks. fast weights layer normalization fails length- sequence. tried using unit fast weight model fails converge length either. modiﬁed fast weights model directly learned updates respectively model converged achieved nearly test accuracy. removed parameterized update weinet rulebased update observed difﬁculty learning conﬁrms advantage learned autoassociative memory update. fig. also shows critical settings weinet without router router necessary task. using router apply associative memory blocks weinet. curve shows adding memory linearly help complicate learning task. fig. shows evaluation different choices weights updating compare approaches using rh×h denoted full matrix; using vectors generate outer product denoted using recurrent highway connections updating introducing coupled gates sigmoid ht). then curves length- recall task using approach signiﬁcantly reduce parameter size also achieves faster convergence thanks efﬁcient training. observation complies intuition degrees freedom parameter degree-of-freedom sufﬁce. accuracy lstm shows difﬁculty learning sigmoid gates slows signiﬁcantly memory storage conﬁrms memorization weinet combined effect input representation learning memory update learning. without sigmoid gates highway connection weinet uses matrix instead gates ﬁght vanishing gradient applying gradient clipping layer normalization right generating proved effective task. weinet discussed before weinet store training input shot hopﬁeld does patterns encoded different. example would learn associative recall task hopﬁeld would hopﬁeld encode character neuron association neurons could directly encode relationships characters. however weinet could also learn relationships following manner let’s unroll simpliﬁed weinet sequence time assume it’s hard assumption every scalar memory i-th column j-th could encode temporally adjacent bits orthogonal adjacent pairs close time steps basically says using associative recall task example could learn store pair adjacent characters actually t-th order polynomial function variables. variables mapped every combination space deﬁned basis scalar thus bits easily entangled made hard stored discretely auto-associative memory weinet different various thus basis different especially network randomly initialized. bit-wise scaler associative memory element function values corresponding positions namely a=i=j relevant making convert hadamard product ﬁrst second term product using weight matrices learn. enables memory receive information memory bits. observed that handling length- associative recall task change leads extremely fast convergence weinet. however length length able convergence anymore might over-ﬁtting introducing cross-bit contribution time step. mnist image classiﬁcation visual glimpses recurrent visual attention designed image classiﬁcation task includes glimpses network extracts smaller regions image recurrent neural network compose glimpses generate compressed figure demonstrating input sequence weinet. controller router reader weight memory. ﬁxated attention small glimpse input image pixel overlap. dynamic attention glimpse generated glimpse networks pixel overlap allowed. table comparison test error rate mnist classiﬁcation task. result setting models hidden size recurrent layer fw-ln uses single inner recurrent layer. weinet uses fast weight memory cell representation classiﬁcation. visual attention model obtain meaningful glimpses image i.e. glimpses on-target also composes glimpses intelligently i.e. compose meaningful glimpses makes sense classiﬁcation. model uses previous glimpses representation generate hidden state proposing next glimpse classifying whole image. experiment compares weinet three rnns lstm recurrent highway networks fast weights recurrent visual attention model switching component therein. weinet auto-associative memory without router component. tested rnns number hidden units recurrent layer time step. evaluated rnns within visual attention framework following glimpse settings dynamic attention follow using glimpse network learn extract glimpse input image glimpse composition. fixated attention decompose image four ×seven ﬁxed glimpse regions withpixel overlap compose glimpses table comparison test error rate models mnist classiﬁcation task. setting glimpses network inner loop fw-ln weinet. dynamic attention generated glimpse networks; ﬁxated attention using sub-images implementation details follow reinforcement learning training preserving glimpse chain sampling. reasons change hypothesis adding reward overlaps training criterion lead improved accuracy model could stress-test glimpse networks capability generating hidden states aggregating historical glimpses withchance trial error. without rewards network hard avoid generating wrong chain glimpses generate good chain shot even harder learn setting. kept monte carlo sampling acquire samples glimpses i.e. steps image. hidden state size four tested rnns. results using dynamic attention compare rnns table comparing ﬁxed dynamic attention table results reported test took glimpses dynamic attention setting four glimpses ﬁxed attention setting. steps dynamic attention setting ﬁxed attention setting would increase possibility informative glimpses included. ﬁxed attention guaranteed include pixels hard image-cutting decision lead non-informative glimpses none segments give hint digit image shown fig. obvious ﬁxed attention setting much harder dynamic attention setting conﬁrmed table weights glimpse network conditions hidden state variable therein; glimpses turn help create better hidden state next glimpse proposal. positive feedback loop glimpses states leads weinet’s better overall accuracy table fast weights. also test weinet multiple auto-associative memories router network composing parallel memories non-linearly could lead improvement. mnist task proper purpose glimpses carry different kinds information sometimes glimpse contains stroke digit helpful classiﬁcation sometimes glimpse contains nothing useful. telling apart useful useless glimpses help ﬁlter noisy inputs lead better classiﬁcation performance. memory cell hope process different kinds glimpses differently types explicitly separated dedicated memory cells processed. four loops auto-associative-memory weinet shown figure within inner loop ﬁxed glimpse dynamically generated glimpse representation fed. case router component used drive glimpses different memory cells. however using multiple memory cells creates model non-identiﬁability problem learning kind softmax-based memory addressing mechanism highly unstable observed neural turing machines account nonidentiﬁability guide router distribute informative glimpses memory non-informative glimpses another. tell apart informative simple heuristics calculate many pixels glimpse non-black pixels. threshold number pixels determine much non-black pixels glimpse accepted informative glimpse. determined cross-validation. additionally initialized gaussian differentiate memory content help distinguish memory content. also glimpse router additional input function changed vgt) glimpse vector size generated glimpse networks rg×h using dynamic attention memory cells weinet could achieve accuracy improvement error rate using single autoassociative memory. using three memory cells test accuracy increase explained increased network capacity makes training harder difﬁculty proper thresholds separate glimpses types. also tried weinet machine comprehension task results report appendix. quences intelligent input composition. weinet introduces multiple auto-associative memory network encode complex input patterns. working memory architecture.", "year": 2017}