{"title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.", "text": "abstract—we train generative ’up-convolutional’ neural networks able generate images objects given object style viewpoint color. train networks rendered models chairs tables cars. experiments show networks merely learn images heart rather meaningful representation models allowing assess similarity different models interpolate given views generate missing ones extrapolate views invent objects present training recombining training instances even different object classes. moreover show generative networks used correspondences different objects dataset outperforming existing approaches task. generative modeling natural images long standing difﬁcult task. problem naturally falls components learning distribution images generated learning generator produces image conditioned vector distribution. paper approach second subproblem. suppose given high-level descriptions images train generator. propose ’up-convolutional’ generative network task show capable generating realistic images. recent years convolutional neural networks become method choice many areas computer vision especially recognition recognition posed supervised learning problem convnets known perform well given large enough labeled dataset. work stick supervised training turn standard classiﬁcation upside generate images given high-level information. instead learning mapping sensor inputs condensed abstract representation object identity position generate images high-level descriptions. given models train neural network capable generating projections models given model number viewpoint optionally additional transformation parameters color brightness saturation zoom etc. generative networks accept input high-level values produce images. train standard backpropagation minimize euclidean reconstruction error generated image. perfectly would fail produce reasonable results confronted inputs seen training. show networks train generalize previously unseen data various ways. namely show networks capable knowledge transfer within object class given limited number views chair network knowledge learned chairs infer remaining viewpoints; knowledge transfer classes network transfer knowledge views tables chairs; feature arithmetics addition subtraction feature vectors leads interpretable results image space; interpolation different objects within class classes; randomly generating object styles. review related work section describe network architecture training process section section compare different network architectures dataset sizes section test generalization abilities networks apply practical task ﬁnding correspondences different objects. finally section analyze internal representation networks. related work work generative models images typically addresses problem unsupervised learning data model generate samples latent representation. prominent examples line work restricted boltzmann machines deep boltzmann machines well plethora models derived rbms dbms undirected graphical models build probabilistic model data treat encoding generation joint inference problem. related approach convolutional deep belief networks making unpooling shapenets training variant cdbn generate models furniture. different approach train directed graphical models data distribution. includes wide variety methods ranging gaussian mixture models autoregressive models stochastic variations neural networks among rezende developed approach training generative model variational inference performing backpropagation latent gaussian representation. generative adversarial networks approach presented goodfellow models natural images using deconvolutional generative network similar architecture. unsupervised generative models extended incorporate label information forming semi-supervised conditional generative models fully unsupervised approaches work. examples include gated conditional rbms modeling image transformations training rbms disentangle face identity pose information using conditional rbms learning generative model digits conditioned digit class using variational autoencoders contrast work approaches typically restricted small models images often require expensive inference procedure training generating images. since publication conference paper work based resurgence research using neural networks generating images. line work includes increasing amount papers using large up-convolutional neural networks modelling realistic images using architectures similar derived ones presented paper denton radford train conditional convolutional generative models adversarial networks approach. networks capable generating high-ﬁdelity natural images. reed learn generate animations computer game characters based learned code represents image transformations. general difference approach prior work learning generative models assume highlevel latent representation images given supervised training. allows generate relatively large high-quality images pixels completely control images generate rather relying random sampling. downside course need label fully describes appearance image. modeling viewpoint variation often considered context pose-invariant face recognition recent work approached task neural network network takes face image input generates random view face together corresponding viewpoint. network fully connected hence restricted small images similarly generative models requires random sampling generate model description goal train neural network generate accurate images objects high-level description style orientation respect camera additional parameters color brightness etc. task hand hence inverse typical recognition task rather converting image compressed high-level representation need generate image given high-level parameters. formally assume given dataset examples targets input tuples consist three vectors one-hot encoding model identity azimuth elevation camera position parameters additional artiﬁcial transformations applied images. targets output image segmentation mask note predicting segmentation masks strict requirement good generative capabilities allows easily separate generated images background potentially replace ﬁgures predicted segmentation masks replace black background white. include artiﬁcial transformations described randomly generated parameter vector increase amount variation training data reduce overﬁtting analogous data augmentation discriminative training combination following transformations in-plane rotation translation zoom-in stretching horizontally vertically changing changing saturation changing brightness network architectures conceptually generative network formally refer looks like usual turned upside down. thought composition processing steps experimented several architectures shown figure layers ﬁrst build shared high dimensional hidden representation input parameters. within layers three input vectors ﬁrst independently fully connected layers neurons each outputs three streams concatenated. independent processing followed fully connected layers neurons each yielding response fourth fully connected layer fig. architecture -stream deep network generates pixel images. layer names shown above fully connected upconv upsampling+convolution conv convolution. followed rectiﬁed linear nonlinearity. experiments generated pixel images also experimented pixel images. difference architecture cases less up-convolution respectively. network training network parameters consisting layer weights biases trained minimizing error reconstructing segmented-out chair image segmentation mask lrgb lsegm loss functions image segmentation mask respectively weighting term trading two. experiments lrgb always squared euclidean distance lsegm tried choices squared euclidean distance negative log-likelihood loss preceded softmax layer. ﬁrst case second case. urgb segmentation mask usegm hidden representation experimented architectures them depicted figure image segmentation mask generated based shared feature representation. fully connected layer outputs -dimensional vector reshaped multichannel image upsampling+convolution layers ﬁlters upsampling followed convolutional layer ﬁlters. found adding convolutional layer up-convolution signiﬁcantly improves quality generated images. last upconv layer predicts image segmentation mask. alternative -stream architecture network splits streams right layer describe compare different architectures section order dense representation high dimensional image need unpool feature maps opposed pooling implemented usual cnns. similar deconvolutional layers used previous work illustrated figure perform unpooling simple nails upsampling replacing entry feature block entry value left corner zeros elsewhere. increases width height feature times. used networks. convolutional layer preceded upsampling operation think upsampling+convolution opposite convolution+pooling steps performed standard figure right. ﬁgure also illustrates practice upsampling convolution steps performed sequentially combined single operation. implementationwise operation equivalent backward pass usual convolutional layer stride model blender using azimuth angles elevation angles resulted images model. positions camera light source ﬁxed rendering. experiments paper used renderings models table models. renderings pixels additionally directly rendered corresponding segmentation masks. example renderings shown figure training parameters section describe details training compare different network architectures analyze effect dataset size data augmentation training. training details training networks built caffe implementation used adam momentum parameters regularization parameter mini-batch size started learning rate mini-batch iterations divided learning rate every iterations stopping iterations. initialized weights gaussian noise variance computed based input dimensionality suggested susillo experiments trained networks generating pixel images. viewpoint interpolation experiments section generated pixel images reduce computation time since multiple networks trained experiments. working cars tried generating images check deeper networks capable generating larger images successfully trained. observe complications training deeper networks. images generated architectures well ground truth shown figure reconstruction errors shown table clearly deeper s-s-deep network signiﬁcantly better others qualitatively quantitatively. reason used network experiments. network interpolate different objects. learned network however implements deterministic function high-level description generated image. shared structure multiple objects implicitly learned network explicitly accessible. used generate random objects blending objects training heuristics comes guarantees regarding quality generated images. principled generating objects train probabilistic generative model intermediate gaussian representation. replace independent processing stream class identity layer random samples drawn inference network learns capture underlying structure different chair images. inference network learned alongside generator described maximizing variational bound sample log-likelihood. full description probabilistic generative model training given appendix datasets training data generative networks used renderings models different objects chairs made public aubry well table models shapenet dataset. aubry provide renderings aligned chair models rendered viewpoints azimuth angles elevation angles ﬁxed distance chair. found dataset includes many near-duplicate models models differing color low-quality models. removing ended reduced dataset models used experiments. cropped renders small border around chair resized common size pixels padding white necessary keep aspect ratio. example images shown figure training network also used segmentation masks training examples produced subtracting monotonous white background. experimented training size analyzed effect data augmentation. used cars experiments since models available. keeping network architecture ﬁxed varied training size. example generated images shown figure column corresponds different number models training networks except rightmost column trained without data augmentation. standard model much difference difﬁcult models smaller training leads better reconstruction details. effect data augmentation qualitatively similar increasing training size. reconstruction errors shown table support observations. data augmentation leads worse reconstruction details expected lead better generalization. check this tried morph model another linearly interpolating one-hot input style vectors. result shown figure note network trained without augmentation better models images training fails interpolate smoothly. show networks successfully model complex data demonstrate generalization abilities generating images unseen training viewpoints object styles. also show application generative networks ﬁnding correspondences objects training set. modeling transformations figure shows network able generate chairs signiﬁcantly transformed relative original images. shows different type transformation. images central column non-transformed. even presence large transformations quality generated images basically good without transformation. image quality typically degrades little case unusual chair shapes chairs including details armrests interestingly network successfully models zoom-out even though never presented zoomed-out images training. network easily deals extreme color-related transformations problems representing large spatial changes especially translations. generation quality cases could likely improved complex architecture would allow transformation parameters explicitly affect feature maps convolutional layers perhaps fashion similar gregor jaderberg interpolation viewpoints section show network able generate previously unseen views interpolating views present training data. demonstrates network internally learns representation chairs enables judge chair similarity known examples generate previously unseen views. experiment used network reduce computational costs. randomly separated chair styles fig. generation chair images activating various transformations. shows transformation translation rotation zoom stretch saturation brightness color. middle column shows reconstruction without transformation. subsets ’source set’ styles ’target set’ remaining chairs. varied number viewpoints style either subsets together target trained generative network before. second setup idea network knowledge chairs learned source generate missing viewpoints chairs target set. figure shows representative examples angle interpolation. views target effect knowledge transfer already visible interpolation smoother details preserved better example middle column. starting views network without knowledge transfer fails produce satisfactory interpolation knowledge transfer works reasonably well even view presented training however case details armrest shape lost. figure plot average squared error generated missing viewpoints target without transfer clearly presence viewpoints source dataset dramatically improves performance target especially small numbers available viewpoints. might suppose network simply learns views chairs source then given limited number views chair ﬁnds similar sense among known models simply returns images chair. check fig. examples view interpolation pair rows knowledge transfer second without leftmost rightmost images views presented network training intermediate ones network hence result interpolation. number different views chair available training image quality worse ﬁgures used network. fig. reconstruction error unseen views chairs target depending number viewpoints present training. blue viewpoints available source dataset green number viewpoints available source target datasets case evaluated performance naive nearest neighbor approach. image target found closest match source given views interpolated missing views linear combinations corresponding views nearest neighbors. ﬁnding nearest neighbors tried similarity measures euclidean distance images descriptors. results shown figure interestingly although yields semantically much meaningful nearest neighbors similarity performs much better numerically. performance nearest neighbor method always worse network knowledge transfer suggesting network learns linearly combining known chairs especially many viewpoints available target set. elevation transfer extrapolation chairs dataset contains renderings elevation angles tables elevations available. show transfer information elevations class another. trained network chairs tables generated images chairs elevations present training. baseline network trained solely chairs. results shown figure network trained chairs generalize unseen elevation angles almost trained tables able generate unseen views chairs well. drawback generated images always precisely correspond desired elevation example second model figure still result suggests network able transfer understanding object structure object class another. network trained chairs tables fairly well predict views tables previously unseen elevation angles. figure shows network generate images previously unseen elevations interestingly presence chairs training helps better extrapolate views tables. hypothesis network trained object classes forced model kind objects also general geometry. helps generating reasonable views elevation angles. hypothesize modeling even object classes single network would allow learn universal class-independent representation shapes. interpolation styles remarkably generative network imagine previously unseen views given object also invent objects interpolating given ones. obtain interpolations simply linearly change input label vector class another. representative examples morphings chairs cars shown figures respectively. morphings object class sorted subjective morphing quality networks produce naturally looking morphings even challenging cases. even morphed objects dissimilar fig. elevation angle knowledge transfer. pair rows trained chairs bottom trained chairs tables green background denotes elevations presented training. also possible interpolate objects. figure shows morphing three chairs triple upper triangle lower triangle table. network successfully combines features three chairs. networks interpolate objects class morph objects different classes other? inter-class difference larger intra-class variance hence successfully interpolate classes network close large different classes. check network trained chairs tables capable this. results shown figure quality intermediate images slightly worse intra-class morphings shown above overall good especially considering training network seen anything intermediate chair table. feature space arithmetics previous section seen feature representation learned network allows smooth transitions even three different objects. property used transfer properties object onto another performing simple arithmetics fig. examples morphing different chairs morphing row. leftmost rightmost chairs present training intermediate ones invented network. rows ordered decreasing subjective quality morphing bottom. feature space? figure shows indeed possible. simple subtraction addition feature space change armchair chair similar style chair stick back identical chair solid back. found exact layer arithmetic performed matter results basically identical manipulate input style vectors outputs layers random chair generation section show results generating random chair images using ideas brieﬂy outlined section particular experiment networks trained fully supervised manner using training objective section networks trained variational bound objective described appendix mentioned above principled perform sampling using networks trained supervised manner. nonetheless natural heuristics used obtain quasi random chairs. ﬁrst observe style input network probability distribution styles training time concentrated single style however interpolation experiments seen network also generates plausible images given inputs several non-zero entries. suggests generating random images using random distributions input network. tried families distributions computed softmax gaussian fig. examples morphing different cars morphing row. leftmost rightmost chairs present training intermediate ones invented network. rows ordered decreasing subjective quality morphing bottom. fig. random chairs generated different methods. pair rows generated chairs bottom nearest neighbors training set. softmax gaussian input layer interpolations several chairs training gaussian noise stochastic networks trained variational bound loss gaussian noise networks trained usual loss. text details. exemplary results experiments shown figure generated image closest chair dataset according euclidean distance shown. generated method respectively. results good generated chairs similar higher essentially copy chairs training set. produced method respectively. generated chairs quite diverse similar chairs training set. model trained variational bound objective directly allows sample assumed prior generate images draws. simply replace activations style stream random gaussian noise. results shown figure difference control figure also show chairs generated network trained without variational bound objective. procedure guaranteed result visually appealing images since hidden layer activations restricted particular regime training found result sharp chair images. however high standard deviations generated chairs diverse. overall heuristics combining several chairs variational-bound-based training lead generating images roughly similar quality diversity. however second approach advantageous allows generating images simply gaussian distribution principled potentially promising improvement better optimized combined kinds stochastic networks. correspondences ability generative interpolate different chairs allows dense correspondences different object instances even appearance dissimilar. given chairs training dataset used s-s-deep network generate morphing consisting images computed optical resulting image sequence using code brox compensate drift reﬁned computed optical recomputing step frames initialized concatenated per-frame ﬂows. concatenation reﬁned optical ﬂows gives global vector ﬁeld connects corresponding points chair images. order quantitatively evaluate quality correspondences created small test image pairs. analyze performance detail separated ’simple’ pairs ’difﬁcult’ pairs exemplar pairs shown figure manually annotated several keypoints ﬁrst image pair asked people manually mark corresponding points second image pair. used mean keypoint positions second images ground truth. test time measured performance different methods computing average displacement predicted keypoints second images given keypoints ﬁrst images. also manually annotated additional validation image pairs tune parameters methods table show performance algorithm compared human performance baselines sift deformable spatial pyramid average basic approach used outperforms baselines thanks intermediate samples produced generative neural network. interestingly sift problems difﬁcult pairs algorithm not. suggests errors method largely contrast changes drift optical depend difﬁculty image pair. approaches hence complementary similar objects direct matching fairly accurate dissimilar ones intermediate morphings helpful. analysis network shown networks model objects extremely well. analyze inner workings networks trying insight source success. network used section. activating single units analyze neural network visualize effect single neuron activations. although method allow judge network’s actual functioning involves clever combination many neurons still gives rough idea kind representation created different network layers. fig. images generated single neuron activations feature maps layers network. bottom upconv upconv stream. relative scale images correct. bottom images pixel approximately half chair size. model tailored generate images highlevel neuron activations allows activate single neuron higher layers forwardpropagate image. results procedure different layers network shown figures corresponds different network layer. leftmost image generated setting neurons layer zero images activating randomly selected neuron. figure ﬁrst rows show images produced activating neurons feature maps class stream keeping viewpoint transformation inputs ﬁxed. results clearly look chair-like show much variation suggests larger variations achievable activating multiple neurons. last rows show results activating neurons feature maps. feature maps contain joint class-viewpointtransformation representations hence viewpoint ﬁxed anymore. generated images still resemble chairs much less realistic. expected away inputs less semantic meaning activations. middle bottom figure notice neuron seems generate zoomed chair. looking neurons carefully found indeed ’zoom neuron’ moreover transformation specialized neuron effect shown figure increasing activation neurons keeping activations ﬁxed results transformation generated image. quite surprising information transformations propagated without change fully connected layers. seems potential transformed fig. effect specialized neurons layer shows result increasing value single neuron given feature maps real chair. effects neurons bottom translation upwards zoom stretch horizontally stretch vertically rotate counter-clockwise rotate clockwise increase saturation decrease saturation make violet. versions chair already contained features ’transformation neurons’ modify give relevance activations corresponding required transformation. corresponding weights connecting specialized neurons next layer shown figure output channels responsible spatial transformations image others deal color brightness. fig. neural network weights corresponding transformation neurons shown figure shows weights connected neuron order figure selected subset interesting channels shown. layers produce edge-like images neurons higher deconvolutional layers generate blurry ’clouds’ opposed results zeiler fergus classiﬁcation network max-unpooling. explanation naive regular-grid upsampling network cannot slightly shift small parts precisely arrange larger meaningful structures. hence must another generate details. next subsection show achieved combination spatially neighboring neurons. rather activating single neurons keeping others ﬁxed zero network normally generate image analyze hidden layer activations either looking modifying observing results. example approach already used figure understand effect ’transformation neurons’. present results direction here. first study blurry ’clouds’ generated single high-level deconvolutional neurons form perfectly sharp chair images. start feature maps chair spatial extent next keep active neurons region around center feature gradually increasing size region means nearly single-neuron activation level whole image level. outcome shown figure clearly interaction neighboring neurons important central region many neurons active image sharp periphery blurry. interesting effect visible images sharply legs chair second last image appear larger image. suggests highly non-linear suppression effects activations neighboring neurons. second interesting observations made taking closer look feature maps upconv layer exhibit regular patterns shown figure feature maps correspond ﬁlters look near-empty figure explanation patterns compensate high-frequency artifacts originating ﬁxed ﬁlter sizes regular-grid upsampling. supported last figure shows happens generated image feature maps zero. fig. selected feature maps pre-output layer stream. feature maps correspond ﬁlters look near-empty figure middle close-ups feature maps. bottom generation chair feature maps zero left unchanged note high-frequency artifacts left pair images. shown supervised training convolutional neural networks used discriminative tasks also generating images given high-level style viewpoint lighting information. network trained generative task merely learn generate training samples learns generic implicit representation allows smoothly morph different object views object instances intermediate images meaningful. moreover trained stochastic regime creative invent chair styles based random noise. experiments suggest networks trained reconstruct objects different classes develop understanding shape geometry. expect training single network object classes would also succeed constitutes interesting direction future work. fascinating relatively simple architecture proposed already able learn complex behaviors. where experiments simply take sample data point. equation optimized using standard stochastic gradient descent since derivative respect parameters computed closed form. detailed explanation back-propagate sampling procedure refer kingma give additional details training objective used experiment random chair generation section phrase problem generating chairs learning probabilistic generative model. urgb usegm denote expanding part generator assume full knowledge view transformation parameters chair identity assume exists distribution latent states capture underlying manifold chair images generators urgb usegm generate corresponding image using input. deﬁne mapping obtained replacing independent processing stream class identity dimensional random vector layer deﬁne likelihood segmentation mask transformation denotes gaussian distribution mean covariance simplify formulation assuming diagonal covariance structure since distributions appearing derivation conditioned augmentation parameters view parameters omit following simplify notation. marginal likelihood image segmentation mask since priori knowledge regarding structure i.e. know shared underlying structure different chairs posses replace approximate inference distribution parameterize layer fully connected neural network predicting mean variance distribution. refer parameters network following recent examples neural networks literature jointly train approximate inference network generator networks maximizing variational lower bound log-likelihood zeiler fergus visualizing understanding convolutional networks eccv zeiler taylor fergus adaptive deconvolutional networks high level feature learning iccv chang funkhouser guibas hanrahan huang savarese savva song xiao shapenet information-rich model repository tech. rep. arxiv. shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding arxiv preprint arxiv. alexey dosovitskiy received specialist ph.d. degrees mathematics moscow state university respectively. ph.d. thesis ﬁeld functional analysis related measures inﬁnite-dimensional spaces representations theory. summer spent three months computational vision neuroscience group university t¨ubingen. since september postdoctoral researcher computer vision group university freiburg germany. current main research interests computer vision machine learning optimization. jost tobias springenberg student machine learning university freiburg germany supervised martin riedmiller. prior starting tobias studied cognitive science university osnabrueck earning went obtain computer science university freiburg focusing representation learning deep neural networks computer vision problems. research interests include machine learning especially representation learning learning efﬁcient control strategies robotics. maxim tatarchenko maxim tatarchenko received honors applied mathematics russian state technological university mati spent three years developing numerical algorithms satellite navigation company ’gpscom’ moscow. proceeded master studies university freiburg close obtaining degree. january going join computer vision group prof. thomas brox student. main research interest computer vision special focus deep generative models images. thomas brox received ph.d. degree computer science saarland university germany spent years postdoctoral researcher university bonn years university california berkeley. since heading computer vision group university freiburg germany. research interests computer vision particular video analysis deep learning computer vision. prof. brox associate editor ieee transactions pattern analysis machine intelligence international journal computer vision. received longuet-higgins best paper award koendrink prize fundamental contributions computer vision.", "year": 2014}