{"title": "Adaptive Learning Rate via Covariance Matrix Based Preconditioning for  Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Adaptive learning rate algorithms such as RMSProp are widely used for training deep neural networks. RMSProp offers efficient training since it uses first order gradients to approximate Hessian-based preconditioning. However, since the first order gradients include noise caused by stochastic optimization, the approximation may be inaccurate. In this paper, we propose a novel adaptive learning rate algorithm called SDProp. Its key idea is effective handling of the noise by preconditioning based on covariance matrix. For various neural networks, our approach is more efficient and effective than RMSProp and its variant.", "text": "abstract. adaptive learning rate algorithms rmsprop widely used training deep neural networks. rmsprop oﬀers eﬃcient training since uses ﬁrst order gradients approximate hessian-based preconditioning. however since ﬁrst order gradients include noise caused stochastic optimization approximation inaccurate. paper propose novel adaptive learning rate algorithm called sdprop. idea eﬀective handling noise preconditioning based covariance matrix. various neural networks approach eﬃcient eﬀective rmsprop variant. adaptive learning rate algorithms widely used eﬃcient training deep neural networks. rmsprop follow-on methods used many deep neural networks convolutional neural networks since easily implemented high memory eﬃciency. empirical success rmsprop could explained using hessian-based preconditioning hessian matrix represents curvature loss function; hessian-based preconditioning locally changes curvature loss function. training deep neural networks pathological curvatures saddle points cliﬀs slow progress ﬁrst order gradient descent stochastic gradient descent hessian-based preconditioning improves condition curvature thus enhances speed. however hessian-based preconditioning incurs high computation cost generally computes inverse matrix hessian. since rmsprop approximates hessian-based preconditioning using ﬁrst order gradients achieves eﬃcient training. addition rmsprop easy implement. therefore terms practical rmsprop variants adadelta adam still seen powerful approach training deep neural networks. however ﬁrst order gradients used rmsprop include noise caused stochastic optimization techniques mini-batch setting. batch setting since model inputs ﬁxed iteration parameter updates change gradients. hand mini-batch setting since inputs ﬁxed iteration gradients also changed randomly selecting inputs iteration. change mini-batch setting seen noise. since rmsprop uses noisy ﬁrst order gradients approximate hessianbased preconditioning approximation inaccurate. indicates paper proposes novel adaptive learning rate algorithm called sdprop. idea covariance matrix based preconditioning instead hessianbased preconditioning. covariance matrix derived assuming distribution noise observed gradients. since distribution eﬀectively captures noise sdprop eﬀectively capture changes gradients caused random input selection iteration. interestingly theoretical analysis reveals sdprop uses information directions past gradients adapting learning rate rmsprop variants magnitudes gradients. experiments compare sdprop rmsprop. sdprop needs fewer training iterations rmsprop reach ﬁnal training loss cifar cifar- mnist datasets. addition sdprop outperforms adam state-of-the-art algorithm based rmsprop several datasets. approach also eﬀective rmsprop training recurrent neural network deep fully-connected neural networks. brieﬂy review background paper. first describe basic algorithm stochastic optimization mini-batch setting. second review rmsprop. finally explain relationship hessian-based preconditioning rmsprop. stochastic gradient descent. many learning algorithms minimizing loss function respect parameter vector popular algorithm mini-batch setting. minimize iteratively updates mini-batch samples follows time sample mini-batch time ﬁrst order gradient respect i-th parameter given xt−. applies equation sample mini-batch gradient descent applies equation data batch setting. although includes noise random selection mini-batch uses training phase. since uses part data computing rmsprop. rmsprop popular algorithm based training neural networks. adadelta adam follow-up methods rmsprop. rmsprop rapidly reduces loss function adapting learning rate sgd. updating rule rmsprop follows equation past rmsprop yields small learning rate large. empirically idea eﬃciently reduces loss function deep neural networks. follow-up methods adadelta adam based idea. convex optimization regret analysis used explain eﬃciency methods non-convex optimization deep neural networks empirical success rmsprop could explained using hessian-based preconditioning. brieﬂy review relationship hessian-based preconditioning rmsprop following next section. hessian-based preconditioning. kind pathological curvature loss function slows progress therefore important capture curvature order eﬃciently train deep neural networks. number hessian estimates extent curvature pathological. condition number deﬁned σmax/σmin σmax σmin largest smallest singular values respectively. function less pathological curvature condition number small value. function equally curves small condition number. therefore increase eﬃciency training reducing hessian condition number hessian-based preconditioning locally transforms original parameter anparameter hessian small condition number. preconditioning matrix gives transformations ˆθ=d/θ transformed parameter. using function transformed function smaller condition number eﬃciently train model applying ﬁrst order gradient descent updating rule ˆθt=ˆθt−−α∇ since =d−/∇f following form hessian transformed function given ˆh=thd−/. d/=h smaller condition number identity matrix. case equation corresponds newton method. however exists positive-semideﬁnite. since deep neural networks many saddle points hessian indeﬁnite newton method unsuitable training deep neural networks. hand diagonal equilibration equation could explained using hessian-based preconditioning comparison equation equation corresponds i-th element diagonal precondi indicates tioning matrix. addition empirical results suggest approximates i-th element diagonal equilibration matrix used eﬃciently train deep neural networks thus rmsprop interpreted hessian-based preconditioning using approximated diagonal equilibration matrix minibath setting. therefore since rmsprop eﬃcient escaping saddle points rmsprop follow-up methods achieve high eﬃciency. ﬁrst order gradients described preliminary section. howorder gradients include noise input randomly selected iteration. since ﬁrst order gradients equation equation square roots uncentered variances tain noise diﬃcult eﬀectively approximate hessian-based preconditioning. order eﬀectively handle noise replace hessian-based preconditioning covariance matrix based preconditioning. represent magnitude oscillation ﬁrst order gradients include noise. speciﬁcally i-th j-th column element represents covariance i-th j-th ﬁrst order gradient. therefore i-th ﬁrst order gradient strongly correlates j-th ﬁrst order gradient large absolute value. hand represents variance i-th ﬁrst order gradient. therefore large value ﬁrst order gradient strongly oscillates i-th dimension. intuitively large oscillations i-th dimension incur high variance updating directions ineﬃcient progress plain sgd. however diﬃcult reduce oscillation since result noise induced mini-batch setting. reduce oscillation using motivation behind approach; plain eﬃciently progresses control oscillation utilizing paper propose preconditioning control oscillation. hessian-based preconditioning reduces condition number hessian preconditioning reduces condition number transforming identity matrix. describe approach next section. covariance matrix based preconditioning. previous section suggests large values diagonal prevent eﬃcient progress sgd. therefore could control values diagonal improve eﬃciency sgd. covariance matrix based preconditioning transforms identity matrix whose size hyper-parameter positive value. since element diagonal represents variance ﬁrst order gradient hold variance constant value variance larger value reduced therefore eﬃciently progresses transform ﬁrst describe approach used transform instead transformed easy transform describe later. hessian-based preconditioning transforms ﬁrst order gradients yield =d−/∇f preconditioning matrix. preconditioning matrix reduces condition number hessian described preliminary section. unlike previous approach execute preconditioning transformation gp=d−ˆgt. transformation transformed ﬁrst order gradient ﬁrst order gradient deﬁned equation since transformation aﬃne transformation generated gaussian distribution equation following distribution since assumed positive semi-deﬁnite matrix eigen values equal higher thus setting )−=u σ−/u gaussian covariance term equation d−=. show role learning rate derive sdprop next section. since compute ﬁrst order gradients time incrementally compute covariance matrix although theorem based property positive semi-deﬁnite matrix. order incrementally compute positive semi-deﬁnite matrix online updating rule follows note hessian-based preconditioning cannot control oscillation ﬁrst order gradients. transformation results distribution covariance matrix uncontrollable. addition since hessian positive semi-deﬁnite matrix diﬃcult compute h−/. therefore covariance matrix based preconditioning inherently diﬀers hessian-based preconditioning. idea preconditioning suitable hessian-based preconditioning handling oscillation triggered noise ﬁrst order gradients. algorithm. since deep neural networks large number parameters idea described previous section incurs large memory consumption number parameters. addition costs time compute using eigenvalue decomposition avoid problems employ diagonal preconditioning matrix d=diag/. since approach needs diagonal terms memory computation costs although approach ignores correlation ﬁrst order gradients suﬃcient control oscillation dimension. diagonal represents variance oscillation described previous section. picking diagonal equation updating rule moving average ﬁrst order gradients i-th parameter time hyper-parameter decay rate moving average exponentially moving variance ﬁrst order gradients i-th parameter time equation decay rate initialized exponentially moving variance. respectively. stable computation small positive value. equation corresponds equation call algorithm sdprop equation includes standard deviation although includes bias imposed initialization remove bias notice takes role learning rate equation rmsprop. therefore equation divides learning rate square root centered variance equation rmsprop divides learning rate square root uncentered variance words rmsprop follow-up methods adam adapt learning rate magnitude gradients adapt variance gradients. although rmsprop sdprop similar updating rules totally diﬀerent goals described previous sections. rmsprop executes hessian-based preconditioning sdprop executes covariance matrix based preconditioning. performed experiments compare sdprop rmsprop adam stateof-the-art algorithm based rmsprop. shows adam eﬃcient eﬀective approach rmsprop adadelta integrating momentum rmsprop. first show eﬃciency eﬀectiveness approach using cnn. second since sdprop eﬀectively handles oscillation described previous section evaluate sdprop using small mini-batches suﬀer noise ﬁrst order gradients. third show eﬃciency eﬀectiveness sdprop rnn. fourth demonstrate eﬀectiveness sdprop layered fully-connected neural network diﬃcult train many sadle points. eﬃciency eﬀectiveness cnn. investigate eﬃciency eﬀectiveness sdprop. used datasets assess classiﬁcation images; cifar- cifar- svhn mnist. experiments conducted -layered relu activation function. loss function negative likelihood. compared sdprop rmsprop adam. sdprop loss settings rmsprop lowest loss adam achieves lowest loss mini-batch size number epochs training loss evaluate algorithms optimize training criterion. figure shows training losses dataset. cifar- cifar- svhn sdprop yielded lower losses rmsprop adam early epochs. mnist although training loss sdprop adam nearly reached sdprop reduces loss faster adam. sdprop needs fewer training iterations rmsprop reach ﬁnal training loss cifar- cifar- mnist. suggests idea covariance matrix based preconditioning eﬃcient eﬀective hessian-based preconditioning mini-batch setting rmsprop adam approximate hessian-based preconditioning described preliminary section. since sdprop captures noise eﬀectively reduces loss even gradients noisy. next experiment investigate performance sdprop terms eﬀectiveness noise using noisy ﬁrst order gradients. sensitivity mini-batch size. previous experimental results show sdprop eﬃcient eﬀective existing methods well handles noise idea practice. words sdprop expected eﬀectively train model even small mini-batch sizes incur noisy ﬁrst order gradients therefore investigated sensitivity sdprop existing methods mini-batch size. main purpose experiment reveal performance attribute sdprop result suggests sdprop used devices scant memory must small mini-batches. compared sdprop rmsprop adam using mini-batch sizes used cifar- dataset -class image classiﬁcation task. used previous section. hyper-parameters also previous section; tuned grid search. number epochs table shows ﬁnal training accuracies. sdprop outperforms rmsprop adam mini-batch size values examined. speciﬁcally although small minibatch size incurs noisy ﬁrst order gradients sdprop obviously achieves eﬀective training unlike rmsprop adam. addition table shows superiority approach rmsprop adam increases mini-batch size falls. example mini-batch size approach percent higher accuracy rmsprop percent accurate mini-batch size indicates covariance matrix based preconditioning eﬀectively handles noise ﬁrst order gradients. eﬃciency eﬀectiveness rnn. evaluated eﬃciency eﬀectiveness sdprop recurrent neural network experiment predicted next character using previous characters characterlevel rnn. used subset shakespeare dataset source code linux kernel dataset size internal state preprocessing dataset followed mini-batch size settings rmsprop. training criterion cross entropy. used gradient clipping learning rate decay. gradient clipping popular approach scaling gradients manually setting threshold; prevents gradients exploding training threshold decayed learning rate every tenth epoch factor rmsprop following sdprop also decayed rmsprop. figure shows results shakespeare dataset source code linux kernel. sdprop reduces training loss faster rmsprop. since sdprop eﬀectively handles noise induced mini-batch setting eﬃciently train models rnn. layered fully-connected neural network. section performed experiments evaluate eﬀectiveness sdprop training deep fullyconnected neural networks. suggests number saddle points exponentially increases dimensions parameters. since deep fully-connected networks typically parameters higher dimension models optimization problem many saddle points. problem challenging slowly progresses around saddle points used deep fully-connected network hidden layers hidden units relu activation functions. used mnist dataset -class image classiﬁcation task. setting used evaluating eﬀectiveness high dimensional parameters. note mnist suﬃcient evaluation because unlike fully-connected networks saturate accuracy experiment. purpose evaluate eﬀectiveness setting high dimensional parameter. thus suﬃcient evaluate effectiveness accuracy saturated. training criterion negative likelihood. mini-batch size initialized parameters gaussian mean standard deviation following compared sdprop rmsprop. sdprop tried combinations hyper-parameters using shows averages best worst training accuracies setting. result shows sdprop achieves higher accuracy rmsprop best setting. addition diﬀerence best worst accuracy sdprop smaller rmsprop. since sdprop eﬀectively handles randomness noise reduce result uncertainty. results show sdprop eﬀectively trains models high dimensional parameters. proposed sdprop eﬀective eﬃcient training deep neural networks. approach utilizes idea using covariance matrix based preconditioning eﬀectively handle noise present ﬁrst order gradients. experiments showed that various datasets models sdprop eﬃcient eﬀective existing methods. addition sdprop achieved high accuracy even ﬁrst order gradients noisy.", "year": 2016}