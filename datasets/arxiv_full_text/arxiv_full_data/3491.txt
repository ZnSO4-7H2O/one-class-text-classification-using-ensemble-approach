{"title": "Hierarchical loss for classification", "tag": ["cs.LG", "cs.CV", "stat.ML", "62H30, 68T05, 65K10"], "abstract": "Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called \"loss\" or \"win\") used in textual or visual classification/recognition via neural networks seldom view a sheepdog as more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes.", "text": "abstract failing distinguish sheepdog skyscraper worse penalized failing distinguish sheepdog poodle; sheepdogs poodles breeds dogs. however existing metrics failure used textual visual classiﬁcation/recognition neural networks seldom view sheepdog similar poodle skyscraper. deﬁne metric that inter alia penalize failure distinguish sheepdog skyscraper failure distinguish sheepdog poodle. unlike previously employed possibilities metric based ultrametric tree associated given tree organization semantically meaningful hierarchy classiﬁer’s classes. metrics classiﬁer accuracy used neural network methods lecun bengio hinton seldom account semantically meaningful organizations classes; metrics neglect instance sheepdogs poodles dogs dogs cats mammals mammals birds reptiles amphibians vertebrates below deﬁne metric amount winnings classiﬁcation accounts given organization classes tree. optimization want maximize equivalently minimize loss caution experiments indicate plain stochastic gradient descent optimization random starting points stuck local optima; even hierarchical serve good metric success ﬁgure merit accuracy classiﬁer. approach detailed special case general methods hofmann kosmopoulos references. particular special cases discussed binder kawanabe brefeld hofmann chang costa deng deng kosmopoulos wang zhou liew allocate diﬀerent weights diﬀerent leaves leveraging ultrametric trees present paper related topic hierarchical classiﬁcation reviewed silla freitas redmon farhadi example); however present paper considers classiﬁcation given hierarchy’s ﬁnest-level classes. design options classiﬁcation ﬁnest-level classes circumscribed classiﬁcation easier implement interface existing codes. remainder present paper following structure section constructs hierarchical loss win. section details reﬁnements useful optimizing based hierarchical loss win. section illustrates evaluates hierarchical loss several numerical experiments. section draws conclusions proposes directions future work. concretely suppose want classify input many classes classes leaves tree organizes semantically meaningful hierarchy. suppose classiﬁer maps input output probability distribution leaves hopefully concentrated leaf corresponding correct class input. deﬁne probability node tree probabilities leaves falling node leaf falls node node path root leaf. deﬁne amount winnings weighted probabilities nodes along path root leaf corresponding correct class. calculate across nodes path root leaf corresponding correct class including root leaf weighting probability ﬁrst node weighting second node weighting third node weighting fourth node ﬁnal leaf. probability ﬁnal leaf weighted weight double-count ﬁnal leaf. justify double-counting shortly. compute probability node given probability distribution leaves propagate leaf probabilities tree follows. begin storing zero node tree. then leaf thus probability ﬁnal leaf large small whenever probability ﬁnal leaf irrespective ﬁnal leaf means weights form ultrametric tree phylogenetics discussed example reece justiﬁes double-counting ﬁnal leaf. facilitate optimization gradient-based methods detail compute gradient hierarchical respect input distribution relaxing constraint distribution leaves probability distribution probabilities leaves nonnegative algorithms speciﬁed yield value function distribution leaves. without constraint distribution leaves probability distribution actually linear function distribution product input distribution another vector gradient respect input therefore vector product. entry gradient vector entry equal distribution leaves consists zeros except value leaf; clearly equal series truncated number terms equal number nodes path root correct leaf path root leaf coincide remark forced choose single class corresponding leaf given hierarchy optimizing hierarchical loss ﬁrst identify node greatest probability among nodes coarsest level node greatest probability among nodes falling ﬁrst node selected node greatest probability among nodes falling second node selected select leaf. leaf select corresponds class choose. figure summarizes pseudocode procedure selecting single best leaf. order provide appropriately normalized results input hierarchical loss needs probability distribution arbitrary collection numbers. softmax provides good standard means converting collection real numbers proper probability distribution. recall softmax sequence numbers normalized sequence exp/z exp/z exp/z exp. notice normalized numbers lies generating probability distribution leaves softmax optimize based logarithm introduced rather itself. case omitting contribution root objective value gradient makes sense ensuring hierarchy results training usual cross-entropy loss. henceforth omit contribution root hierarchical wins losses report multiply resulting minimal maximal possible values become taking logarithm also makes sense joint probability stochastically independent samples product probabilities individual samples making averaging logarithm function make sense averaging function directly. said taking logarithm emphasizes highly misclassiﬁed samples desirable misclassifying samples acceptable. indeed logarithm even single sample inﬁnite average logarithm also inﬁnite irrespective values samples. whether hierarchy full training logarithms wins stringent whereas wins without logarithms meaningful metrics success ﬁgures merit. make good sense train logarithm works really hard accommodate learn hardest samples make metric success ﬁgure merit robust uninteresting outliers. thus training logarithm make good sense without logarithm metric success ﬁgure merit testing validation stage. illustrate hierarchical loss performance using supervised learning text classiﬁcation fasttext joulin experiments hashed bigrams million buckets trained using stochastic gradient descent setting learning rate start values indicated tables decaying linearly course training work joulin starting point stochastic gradient descent random. learning rate also known step size step length update iteration stochastic gradient descent. using hierarchy inform training follow section maximizing hierarchical calculated without contribution root implementation couples software joulin python prototype. industrial deployment would require acceleration python prototype codes suﬃcient estimating ensuing gains accuracy illustrating ﬁgure merit providing proof principle. particular experiments indicate gains accuracy training hierarchical loss meager except special circumstances detailed summarized conclusion section pending development suggested section main present hierarchical metric success ﬁgure merit good notion accuracy least training plain stochastic gradient descent coupled backpropagation. tables columns training loss rate epochs list following three parameters training form loss function used training initial learning rate tapers linearly course training total number sweeps data performed training training loss refers training using usual cross-entropy loss negative natural logarithm hierarchical using hierarchy labelable classes leaves attached root training loss refers training using hierarchical loss using full hierarchy. training loss refers training using negative natural logarithm hierarchical using full hierarchy. training loss coarse refers training using usual cross-entropy loss classiﬁcation coarsest classes hierarchy values reported tables learning rate number epochs yielded among best results accuracies discussed following paragraphs. columns one-hot hierarchy display average testing samples hierarchical results one-hot encoding class chosen according remark section columns softmax hierarchy display average testing samples hierarchical results softmax fasttext joulin columns −log hierarchy display average testing samples negative natural logarithm hierarchical results softmax fasttext columns cross entropy display average testing samples usual cross-entropy loss negative natural logarithm hierarchical results softmax using hierarchy labelable classes leaves attached root columns coarsest accuracy display fraction testing samples coarsest classes containing classes chosen classiﬁcation correct classifying sample exactly class remark section columns parents’ accuracy display fraction testing samples parents classes chosen classiﬁcation correct classifying sample exactly class; parents coarsest classes tables experiments reported tables pertain hierarchies levels columns ﬁnest accuracy display fraction testing samples classiﬁed correctly classifying exactly ﬁnest-level class remark last lines tables remind reader best classiﬁer would higher one-hot hierarchy higher softmax hierarchy lower −log hierarchy lower cross entropy higher coarsest accuracy higher parents’ accuracy higher ﬁnest accuracy. table reports results rcv-v lewis dataset includes hierarchy classes sample dataset comes associated least class labels whether class internal node tree leaf. sample dataset consists ﬁltered tokenized text reuters news articles described lewis labels associated internal nodes original hierarchy viewed leaves fall internal nodes classifying lower-level nodes. hierarchy hence duplicate every internal node other class node other class leaf. discard every sample dataset associated label swap training testing sets furthermore randomly permute samples training testing sets subsample samples testing training. hierarchy coarsest classes parents leaves used training leaves used. embedded words bigrams -dimensional space following joulin training classiﬁer coarsest classes embedded words bigrams -dimensional space. data optimizing based hierarchical loss yields worse accuracy according metrics considered compared optimizing based standard cross-entropy loss table reports results rcv-v section retaining training sample class label. training thus consists samples hierarchy coarsest classes parents leaves used training leaves used. embedded words bigrams -dimensional space following joulin training classiﬁer coarsest classes embedded words bigrams -dimensional space. data optimizing based negative natural logarithm hierarchical yields better accuracy according metrics considered compared optimizing based standard cross-entropy loss except negative natural logarithm hierarchical cross-entropy loss table reports results yahoo answers subset introduced zhang zhao lecun dataset includes classes sample dataset comes associated exactly class labels. sample dataset consists normalized text questions answers given website devoted q&a. nontrivial hierarchy grouped classes superclasses. embedded words bigrams -dimensional space following joulin training classiﬁer coarsest classes embedded words bigrams -dimensional space. table reports results dbpedia subset introduced zhang zhao lecun dataset includes classes sample dataset comes associated exactly class labels. sample dataset consists normalized text dbpedia articles nontrivial hierarchy grouped classes superclasses. embedded words bigrams -dimensional space following joulin dataset includes classes sample dataset consists normalized text lead section wikipedia article associated type sub-species species genus family order names removed associated wikipedia article ﬁnestlevel classes chose uniformly random leaf dbpedia taxonomic tree sample training reserving leaves testing hierarchy coarsest classes parents leaves tree leaves tree. embedded articles’ words bigrams -dimensional space following joulin optimizing hierarchical without logarithm wholly ineﬀective always resulting assigning ﬁnest-level class input samples taking logarithm hierarchical absolutely necessary train successfully. training classiﬁer coarsest classes embedded words bigrams -dimensional space. data optimizing based negative logarithm hierarchical yields much better coarsest accuracy hierarchical wins optimizing based standard cross-entropy loss optimizing based standard cross-entropy loss yields much better ﬁnest accuracy cross entropy. optimizing based negative natural logarithm hierarchical accuracy coarsest aggregates reaches attained optimizing coarse classiﬁcation directly. subtree includes coarsest classes ﬁnest-level classes reserved sample ﬁnest-level class training; samples testing chose uniformly random form testing set. sample dataset consists normalized tokenized text extracts wikipedia popular crowdsourced online encyclopedia. hierarchy coarsest classes parents leaves used training leaves used. embedded words bigrams -dimensional space following joulin training classiﬁer coarsest classes embedded words bigrams -dimensional space. data optimizing based hierarchical loss yields worse accuracy according metrics except accuracy coarsest aggregates compared optimizing based standard crossentropy loss optimizing based negative natural logarithm hierarchical accuracy coarsest aggregates approaches maximum attained optimizing coarse classiﬁcation directly. experiments optimizing hierarchical loss using plain stochastic gradient descent backpropagation could helpful relative optimizing usual crossentropy loss mainly training samples class particular provides limited partial solution problem open world classiﬁcation classiﬁcation testing includes samples classes represented training set. also direct relevance problem personalization often involves limited data individual class experiments reported summarized follows relative training usual cross-entropy loss training negative logarithm hierarchical hurt respects section helped respects section improved coarse accuracy much optimizing directly coarse classiﬁcation section hurt respects section improving coarse accuracy nearly much optimizing directly coarse classiﬁcation made essentially diﬀerence sections thus whether optimizing hierarchical loss makes sense depends data associated hierarchy. even optimizing hierarchical loss using plain stochastic gradient descent backpropagation rather ineﬀective least relative might possible. trained using stochastic gradient descent random starting point prone getting stuck local optima. extent hierarchical loss collapses many classes hierarchy aggregate superclasses parameters optimized within aggregates tied closely together optimization plain stochastic gradient descent unlikely discover beneﬁts tying plain stochastic gradient descent together parameters optimizing independently. optimizing hierarchical loss would presumably eﬀective using hierarchical process optimization could good subject future work. hierarchical optimization could alter stochastic gradient descent explicitly hierarchical process could involve regularization terms penalizing variance parameters associated leaves coarse aggregate. time being hierarchical loss useful metric success gauging performance fully trained classiﬁer semantically meaningful ﬁgure merit.", "year": 2017}