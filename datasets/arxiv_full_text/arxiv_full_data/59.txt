{"title": "Piecewise Latent Variables for Neural Variational Text Processing", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.5.1; I.2.7"], "abstract": "Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.", "text": "advances neural variational inference facilitated learning powerful directed graphical models continuous latent variables variational autoencoders. hope models learn represent rich multi-modal latent factors real-world data natural language text. however current models often assume simplistic priors latent variables uni-modal gaussian distribution incapable representing complex latent factors efﬁciently. overcome restriction propose simple highly ﬂexible piecewise constant distribution. distribution capacity represent exponential number modes latent target distribution remaining mathematically tractable. results demonstrate incorporating latent distribution different models yields substantial improvements natural language processing tasks document modeling natural language generation dialogue. introduction development variational autoencoder framework paved learning largescale directed latent variable models. signiﬁcant progress diverse machine learning applications ranging computer vision natural language processing tasks hoped framework enable learning generative processes real-world data including text audio images disentangling representing underlying latent factors data. however latent factors real-world data often highly complex. example topics newswire text responses conversational dialogue often posses latent factors follow non-linear multi-modal distributions nevertheless majority current models assume simple prior form multivariate gaussian distribution order maintain mathematical computational tractability. often highly restrictive unrealistic assumption impose structure latent variables. first imposes strong uni-modal structure latent variable space; latent variable samples generating model cluster around single mean. second forces latent variables follow perfectly symmetric distribution constant kurtosis; makes difﬁcult represent asymmetric rarely occurring factors. constraints latent variables increase pressure down-stream generative model turn forced carefully partition probability mass latent factor throughout intermediate layers. complex multi-modal distributions distribution topics text corpus natural language responses dialogue system unimodal gaussian prior inhibits model’s ability extract represent important latent structure data. order learn expressive latent variable models therefore need ﬂexible tractable priors. prior distribution based piecewise constant distribution. derive analytical tractable form applicable variational autoencoder framework propose differentiable parametrization evaluate effectiveness distribution utilized prior approximate posterior across variational architectures natural language processing tasks document modeling natural language generation dialogue. show piecewise constant distribution able capture elements target distribution cancaptured simpler priors uni-modal gaussian. demonstrate state-ofthe-art results three document modeling tasks show improvements dialogue natural language generation. finally illustrate qualitatively piecewise constant distribution represents multi-modal latent structure data. idea using artiﬁcial neural network approximate inference model dates back early work hinton colleagues researchers later proposed markov chain monte carlo methods scale well slowly well variational approaches require tractable factored distribution approximate true posterior distribution others since proposed using feed-forward inference models initialize mean-ﬁeld inference algorithm training boltzmann architectures recently variational autoencoder framework proposed kingma welling rezende closely related method proposed mnih gregor framework allows joint training inference network directed generative model maximizing variational lower-bound data log-likelihood facilitating exact sampling variational posterior. work extends framework. respect document modeling neural architectures shown outperform wellestablished topic models latent dirichlet allocation researchers successfully proposed several models involving discrete latent variables success discrete latent variable models able partition probability mass separate regions serves main motivations investigating models ﬂexible continuous latent variables document modeling. recently miao proposed continuous latent variables document modeling. researchers also investigated latent variable models dialogue modeling dialogue natural language generation success discrete latent variable models task also motivates investigation ﬂexible continuous latent variables. closely related proposed approach variational hierarchical recurrent encoder-decoder neural architecture latent multivariate gaussian variables. parallel work zhao also proposed latent variable model dialogue modeling speciﬁc goal generating diverse natural language responses. researchers explored ﬂexible distributions latent variables vaes autoregressive distributions hierarchical probabilistic models approximations based mcmc sampling complimentary approach; possible combine piecewise constant latent variables. parallel work multiple research groups also proposed vaes discrete latent variables promising line research however approaches often require approximations inaccurate applied larger scale tasks document modeling natural language generation. finally discrete latent variables inappropriate certain natural language processing tasks. majority work vaes propose parametrize multivariate gaussian distribtions. however unrealistic assumption critically hurt expressiveness latent variable model. appendix detailed discussion. motivates proposed piecewise constant latent variable distribution. piecewise constant distribution propose learn latent variables parametrizing using piecewise constant probability density function allow represent complex aspects data distribution latent variable space non-smooth regions probability mass multiple modes. start introducing neural variational learning framework. focus modeling discrete output variables context natural language processing applications. however framework easily adapted handle continuous output variables. neural variational learning sequence tokens conditioned continuous latent variable further additional observed variable conditions then distribution words model parameters. model ﬁrst generates higher-level continuous latent variable conditioned given generates word sequence unsupervised modeling documents excluded words assumed independent other conditioned note true posterior approximation intractable called encoder sometimes recognition model inference model parametrized distribution prior model available information framework employs re-parametrization trick allows move derivative lower-bound inside expectation. accomplish this parametrized transformation ﬁxed parameter-free random distribution drawn random distribution. here transformation parametrized example might drawn standard straightforward show piecewise constant distribution pieces capable representing bi-modal distribution. vector piecewise constant variables represent probability density modes. figure illustrates variables help model complex multi-modal distributions. order compute variational bound need draw samples piecewise constant distribution using inverse cumulative distribution function further need compute divergence prior posterior. inverse divergence quantities derived appendix training must compute derivatives variational bound expressions involve derivatives generation might example produced lstm encoder applied dialogue history) shared across latent variable dimensions. matrices hprior vectors bprior learnable parameters. posterior distribution previous work shown better parametrize posterior distribution linear interpolation prior distribution mean variance estimate mean variance based observation interpolation controlled gating mechanism allowing model turn on/off latent dimensions µpost =µprior σpost =σprior piecewise constant parametrization parametrize piecewise prior parameters using exponential function applied linear transformation conditioning information aprior model previously proposed document modeling latent variables gaussian. since original nvdm uses gaussian latent variables refer g-nvdm. propose novel models building g-nvdm. figure joint density plot pair gaussian piecewise constant variables. horizontal axis corresponds univariate gaussian variable. vertical axis corresponds piecewise constant variable. indicator functions derivatives zero everywhere except changing points derivative undeﬁned. however probability sampling value exactly changing point effectively zero. thus derivatives zero. similar approximations used training networks rectiﬁed linear units. current output sequence model must generate observed conditioning information. task contains additional conditioning information embedded example dialogue natural language generation represents embedding dialogue history document modeling gaussian parametrization µprior σprior prior mean variance µpost σpost approximate posterior mean variance. gaussian latent variables prior distribution mean variances encoded using linear transformations hidden state. particular prior distribution covariance encoded diagonal covariance matrix using softplus function gregor miao learn prior mean variance ﬁxed standard gaussian previous work. increases ﬂexibility model makes optimization easier. addition gating mechanism approximate posterior gaussian variables. gating mechanism allows model turn latent variable computing ﬁnal posterior parameters. furthermore miao alternated optimizing approximate posterior parameters generative model parameters optimize parameters simultaneously. dialogue model variational hierarchical recurrent encoderdecoder model previously proposed dialogue modeling natural language generation model decomposes dialogues using two-level hierarchy sequences utterances sub-sequences tokens n’th utterance dialogue utterances. m’th word n’th utterance vocabulary given -of-|v binary encoding. number words n’th utterance. utterance model generates latent variable conditioned latent variable model generates next utterance model parameters. vhred consists three modules encoder context decoder rnn. encoder computes embedding utterance. embedding context computes hidden state summarizing dialogue context utterance hcon state represents additional conditioning information used compute prior distribution ﬁrst model propose uses piecewise constant latent variables instead gaussian latent variables. refer model p-nvdm. second model propose uses combination gaussian piecewise constant latent variables. models sample gaussian piecewise constant latent variables independently concatenates together vector. refer model h-nvdm. vocabulary document words. represent document matrix -of-|v binary encoding i’th word document. model encoder component compresses document vector continuous distributed representation upon approximate posterior built. document modeling word order information taken account additional conditioning information available. therefore model uses bag-of-words encoder deﬁned multi-layer perceptron enc. based preliminary experiments choose encoder two-layered parametrized rectiﬁed linear activation functions approximate posterior model parameter matrix post piecewise latent variables parameter matrices post gaussian means variances. prior model parameter vector bprior piecewise latent variables vectors bprior gaussian means variances. initialize bias parameters zero order start centered gaussian piecewise constant priors. encoder adapt priors learning progresses using gating mechanism turn on/off latent dimensions. parameter matrix parameter vector corresponding bias word learned. output probability distribution combined divergences compute lower-bound appendix sample given input decoder computes output probabilities words next utterance. model trained maximizing variational lower-bound factorizes independent terms sub-sequence distribution approximate posterior distribution parameters computed similarly prior distribution conditioned encoder hidden state next utterance. original vhred model used gaussian latent variables. refer model g-vhred. ﬁrst model propose uses piecewise constant latent variables instead gaussian latent variables. refer model p-vhred. second model propose takes advantage representation power gaussian piecewise constant latent variables. model samples gaussian latent variable zgaussian piecewise tent variable zpiecewise independently conditioned context hidden state evaluate proposed models types natural language processing tasks document modeling dialogue natural language generation. models trained back-propagation using variational lower-bound loglikelihood exact log-likelihood. ﬁrst-order gradient descent optimizer adam gradient clipping table test perplexities three document modeling tasks -newgroup reuters corpus cade perplexities calculated using samples estimate variational lower-bound. h-nvdm models perform best across three datasets. document modeling tasks three different datasets document modeling experiments. first news-groups dataset second reuters corpus using version contained selected term vocabulary. previous work transform original word frequencies using equation original word frequency. third test document models text non-english language brazilian portuguese cade dataset datasets track validation bound subset vectors randomly drawn training corpus. training models trained using minibatches examples each. learning rate used. model selection early stopping conducted using validation lowerbound estimated using stochastic samples validation example. inference networks used units hidden layer cade rcv. experimented latent random variables class models found latent variables performed best validation set. h-nvdm vary number components used investigating effect pieces ﬁnal quality model. number approximate posterior learned test example order tighten variational lowerbound. h-nvdm also performed best evaluation across three datasets conﬁrms performance improvement piecewise components. appendix details. table examine highest ranked words given query term space using decoder parameter matrix. piecewise variables appear signiﬁcant effect uncovered model.in case space hybrid pieces seems value senses word–one related outer space another related dimensions depth height width within things exist move hand g-nvdm appears capture outer space sense table word query similarity test newsgroups query ‘space retrieve nearest words word embedding space based euclidean distance. h-nvdm- associates multiple meanings query gnvdm associates frequent meaning. hidden units chosen preliminary experimentation smaller models. set-up therefore report perplexities topic model document neural auto-regressive estimator neural variational document model ﬁxed standard gaussian prior results table report test document perplexity exp. variational lower-bound approximation based samples done first note best baseline model competitive prior posterior models learnt together opposed ﬁxed prior next observe integrating proposed piecewise variables yields even better results document modeling experiments substantially improving baselines. importantly reuters datasets increasing number pieces reduces perplexity. thus achieved state-of-theart perplexity news-groups task best knowledge better perplexities cade tasks compared using state-of-the-art model like g-nvdm. also evaluated converged models using nonparametric inference procedure separate table ubuntu evaluation using metrics w.r.t. activities entities. g-vhred p-vhred h-vhred outperform baseline hred. g-vhred performs best w.r.t. activities hvhred performs best w.r.t. entities. finally visualized means approximate posterior latent variables t-sne projection. shown figure g-nvdm h-nvdm- learn representations disentangle topic clusters -ng. however g-nvdm appears dispersed clusters outliers compared h-nvdm-. although difﬁcult draw conclusions based plots ﬁndings could potentially explained gaussian latent variables ﬁtting latent factors poorly. dialogue modeling task evaluate vhred natural language generation task goal generate responses dialogue. difﬁcult problem extensively studied recent literature dialogue response generation recently gained signiﬁcant amount attention industry high-proﬁle projects google smartreply microsoft xiaoice even recently amazon announced alexa prize challenge research community goal developing natural engaging chatbot system evaluate technical support response generation task ubuntu operating system. well-known ubuntu dialogue corpus consists million natural language dialogues extracted ubuntu internet relayed chat channel. technical problems discussed span wide range software-related hardwarerelated issues. given dialogue history conversation user technical support assistant model must generate next appropriate response dialogue. example turn technical support assistant model must generate appropriate response helping user resolve problem. evaluate models using activityentity-based metrics designed speciﬁcally ubuntu domain metrics compare activities entities model generated responses reference responses; activities verbs referring high-level actions entities nouns referring technical objects activities entities model response overlaps reference response likely response lead solution. training models trained maximize log-likelihood training examples using learning rate mini-batches size variant truncated backpropagation. terminate training procedure model using early stopping estimated using stochastic sample validation example. evaluate models generating dialogue responses conditioned dialogue context model latent variables median values generate response using beam search size select model hyperparameters based validation using activity metric described earlier. often difﬁcult train generative models language stochastic latent variables latent variable models therefore experiment reweighing divergence terms variational lower-bound values addition this linearly increase divergence weights starting zero ﬁnal value ﬁrst training batches. finally weaken decoder randomly replacing words inputted decoder unknown token probability. steps important effectively training models latter used previous work bowman serban tablished models task lstm language model hred model’s encoder uses bidirectional encoder forward backward rnns hidden units. context encoder hidden units decoder lstm decoder hidden units. encoder context rnns layer normalization also experiment additional rectiﬁed linear layer applied inputs decoder rnn. hyper-parameters choose whether include additional layer based validation performance. hred well models word embedding dimensionality size g-hred compare g-vhred vhred gaussian latent variables g-vhred uses hyperparameters encoder context decoder rnns hred model. model gaussian latent variables utterance. p-hred ﬁrst model propose pvhred vhred model piecewise constant latent variables. number pieces latent variable. p-vhred also uses hyper parameters encoder context decoder rnns hred model. similar g-vhred p-vhred piecewise constant latent variables utterance. h-hred second model propose hvhred piecewise constant gaussian latent variables utterance. h-vhred also uses hyper-parameters encoder context decoder rnns hred. results given table latent variable models outperform hred w.r.t. activities entities. strongly suggests high-level concepts represented latent variables help generate meaningful goaldirected responses. furthermore type latent variable appears help different aspects generation task. g-vhred performs best w.r.t. activities occur frequently dataset. suggests gaussian latent variables learn useful latent representations frequent actions. hand h-vhred performs best w.r.t. entities often much rarer mutually exclusive dataset. suggests combination gaussian piecewise latent variables help learn useful representations entities could learned gaussian latent variables alone. conducted qualitative analysis model responses supports conclusions. appendix paper sought learn rich ﬂexible multi-modal representations latent variables complex natural language processing tasks. proposed piecewise constant distribution variational autoencoder framework. derived closed-form expressions necessary quantities required autoencoder framework proposed efﬁcient differentiable implementation incorporated proposed piecewise constant distribution model classes nvdm vhred evaluated proposed models document modeling dialogue modeling tasks. achieved state-of-the-art results three document modeling tasks demonstrated substantial improvements dialogue modeling task. overall results highlight beneﬁts incorporating ﬂexible multi-modal piecewise constant distribution variational autoencoders. future work explore natural language processing tasks data likely arise complex multi-modal latent factors. authors acknowledge nserc canada research chairs cifar research nuance foundation microsoft maluuba funding. alexander ororbia funded nacme-sloan scholarship. authors thank hugo larochelle sharing newsgroup dataset. authors thank laurent charlin sungjin ryan lowe constructive feedback. research enabled part support provided calcul qubec compute canada crook granell pulman. unsupervised classiﬁcation dialogue acts using dirichlet special interest group process mixture model. discourse dialogue pages hinton salakhutdinov. replicated softmax undirected topic model. bengio schuurmans lafferty williams culotta editors nips pages curran associates inc. galley brockett dolan. diversity-promoting objective function neural conversation models. north american chapter association computational linguistics pages lowe serban pineau. ubuntu dialogue corpus large dataset research unstructured multi-turn dialogue sysspecial interest group discourse tems. dialogue serban klinger tesauro talamadupula zhou bengio courville. multiresolution recurrent neural networks application dialogue response generation. thirty-first aaai conference serban sordoni bengio courville pineau. building end-to-end dialogue systems using generative hierarchical neural network models. thirtieth aaai conference serban sordoni lowe charlin pineau courville bengio. hierarchical latent variable encoder-decoder model generating dialogues. thirty-first aaai conference sordoni galley auli brockett mitchell dolan. neural network approach context-sensitive generation conversational responses. conference north american chapter association computational linguistics pages srivastava salakhutdinov hinton. modeling documents deep boltzmann machines. proceedings twenty-ninth conference uncertainty artiﬁcial intelligence pages zhao zhao eskenazi. learning discourse-level diversity neural dialog models asusing conditional variational autoencoders. sociation computational linguistics train model using re-parametrization trick need generate uniform. employ inverse transform sampling requires ﬁnding inverse cumulative distribution function derive addition sampling need compute kullback-leibler divergence prior approximate posterior distributions piecewise constant variables. assume prior posterior piecewise constant distributions. prior superscript denote prior parameters post superscript denote posterior parameters divergence prior posterior computed using integrals integral inside corresponds constant segment ||pθ] majority work vaes propose parametrize prior approximate posterior multivariate gaussian variable. however multivariate gaussian uni-modal distribution therefore represent mode latent space. furthermore multivariate gaussian perfectly symmetric constant kurtosis. properties problematic latent variables represent inherently multi-modal latent variables follow complex non-linear probability manifolds example. frequency topics news articles could represented continuous probability distribution topic island probability mass; sports politics topics might clustered separate island probability mass zero little mass them. uni-modal nature gaussian distribution never represent probability distributions. another example ambiguity uncertainty natural language conversations could similarly represented islands probability mass; given question install ubuntu laptop? model might assign positive probability mass speciﬁc unambiguous entities like ubuntu welldeﬁned procedures like installation using dvd. particular certain entities like ubuntu outdated entities occur rarely practice considered rare events. modeling complex multi-modal latent distributions mapping multivariate gaussian latent variables outputs i.e. conditional distribution highly non-linear order compensate simplistic gaussian distribution capture natural latent factors intermediate layer model. however difﬁcult learn non-linear mappings using variational bound incurs additional variance sampling latent variable consequently models likely converge solutions capture salient aspects latent variables turn leads poor output distribution. kl-terms interpreted regularizers parameter updates encoder model terms encourage posterior distributions similar corresponding prior distributions limiting amount information encoder model transmits regarding output. hadamard product operator combines gaussian piecewise variables decoder model. result using re-parametrization trick choice prior calculate latent variable samples non-linear activation function parametrized linear rectiﬁer news-groups experiments softsign function reuters cade. decoder model outputs probability distribution words conditioned case deﬁne softmax function computed decoder’s output used calculate ﬁrst term variational lower-bound prior posterior distributions used compute term variational lowerbound. lower-bound gaussian latent variables interpolation gating mechanism described main text approximate posterior. experimented mechanisms controlling gating variables deﬁning linear function encoder. however improve performance preliminary experiments. appendix training details piecewise constant variable interpolation conducted initial experiments interpolation gating mechanism approximate posterior piecewise constant latent variables. however found improve performance. hred model found additional rectiﬁed linear units layer decreased performance validation according activity metric. hence test hred without rectiﬁed linear units layer. hand vhred models found additional rectiﬁed linear units layer improved performance validation set. p-vhred found ﬁnal weight divergence terms performed best validation set. g-vhred h-vhred reweighing divergence terms ﬁnal value performed best validation set. conducted preliminary experiments pieces iterative inference document modeling experiments results conclusions depend tight variational lower-bound such theory possible models performing much better reported variational lower-bound test set. therefore non-parametric iterative inference procedure tighten variational lower-bound aims learn separate approximate posterior test example. iterative inference procedure consists simple stochastic gradient descent learning rate gradient rescaling used training. news-groups iterative inference procedure stopped test example bound improve iterations. reuters cade iterative inference procedure stopped bound improve iterations. iterative inference parameters model well generated prior ﬁxed. gradients variational lower-bound respect generated posterior model parameters used update posterior model document note form inference expensive requires additional meta-parameters remark simpler accurate approach inference might perhaps importance sampling. current examples appears hnvdm pieces returns general words. example evidenced table case government baseline seems value plural form word hybrid model actually approximate posterior analysis present additional analysis approximate posterior news-groups order understand models capturing. test example calculate squared norm gradient terms w.r.t. word embedding inputted approximate posterior model. higher squared norm gradients word inﬂuence posterior approximation every test example count words highest squared gradients separately multivariate gaussian piecewise constant latent variables. results shown table illustrate piecewise variables capture different aspects document data. gaussian variables originally sensitive words table. however hybrid model nearly temporal words gaussian variables sensitive strongly affect piecewise variables also capture words originally missed shift responsibility indicates piecewise constant variables better equipped handle certain latent factors. effect appears particularly strong case certain nationality-based adjectives g-nvdm could model multi-modality data degree work would primarily done model’s h-nvdm piecewise varidecoder. ables provide explicit mechanism capturing modes unknown target distribution makes sense model would learn piecewise variables instead thus freeing ubuntu experiments present test examples dialogue context model responses generated using beam search ubuntu models table examples qualitatively illustrate differences models. first observe hred tends generate highly generic responses compared latent variable models. supports quantitative results reported main text suggests modeling latent factors latent variables critical task. next observe h-vhred tends generate relevant entities commands mount command xserver-xorg static address pulseaudio examples hand g-vhred tends better generating appropriate verbs list install pastebin reboot examples example qualitatively p-vhred model appears perform somewhat worse g-vhred h-vhred. suggests gaussian latent variables important ubuntu task therefore best performance obtained combining gaussian piecewise latent variables together h-vhred model. twitter experiments also conducted dialogue modeling experiment twitter corpus extracted based public twitter conversations dataset split training validation test sets containing respectively dialogues each. average dialogue contains utterances words. pre-processed tweets using byte-pair encoding vocabulary consisting sub-words. trained models learning rate mini-batches size ubuntu experiments used variant truncated back-propagation apply gradient clipping. experiment g-vhred hvhred. similar bidirectional encoder forward backward rnns hidtable comparative test perplexities various document datasets note document probabilities calculated using samples estimate variational lower-bound. units. experiment context encoders hidden units hidden units reach better performance w.r.t. variational lower-bound validation set. encoder context rnns layer normalization experiment decoder rnns hidden units hidden units reach better performance. g-vhred model experiment latent multivariate gaussian variables dimensions dimensions reach better performance. h-vhred model experiment latent multivariate gaussian piecewise constant variables dimensions dimensions reach better performance. drop words decoder ﬁxed drop rate multiply terms variational lower-bound scalar starts zero linearly increases ﬁrst training batches. note unlike ubuntu experiments ﬁnal weight divergence exactly hypothesis piecewise constant latent variables able capture multi-modal aspects dialogue. therefore evaluate models analyzing information learned represent latent variables. test dialogue utterances condition model ﬁrst utterances compute latent posterior distributions using utterances. compute gradients terms multivariate gaussian piecewise constant latent variables w.r.t. word dialogue. since words vectors discrete compute squared gradients w.r.t. word embedding. higher squared gradients word inﬂuence posterior approximation every test dialogue count words highest squared gradients separately multivariate gaussian piecewise constant latent variables. results shown table piecewise constant latent variables clearly capture different aspects dialogue compared gaussian latent variables. piecewise constant variable approximate posterior encodes words related time events hand gaussian variable approximate posterior encodes words related sentiment acronyms punctuation marks emoticons also conduct similar analysis document models evaluated sub-section results found appendix. table approximate posterior word encodings p-kl bold every case piecewise variables showed greater word sensitivity gaussian variables w/in hybrid model. dialogue context installing ubuntu laptop something else partitioning mount point drive root home choose directory remind contents partition e.g. windows partition /windows harddrive full free space bought without windows preinstalled want create drives store ﬁles mean root home mount point /mount shown drop menu sorry mean /mount mean desktop extra drives mounted /media understand problem sorry english native language directory meaningful name preferably root example /ﬁles choose something like /ﬁles root /home home mount point storage partition wait /ﬁles thank problem installing ubuntu desktop tried desktop hangs alternate hangs alternate hangs tried using different drives could someonle please help making whats specs used universal installer exactly hangs specs core sapphire radeon video tried nomodeset option nothing also can’t witch console using ctrl thanks anyone know route ubuntu getting error adding route sure pastebin command pronlem <url idea empathy playing notiﬁcation sounds even though ticked preferences restarted check pulse application muted reason well sound settings sound effects turned sound settings didn’t realize controlled applications done time annoying favorite though recently pulse freezing desktop audio adjustable like seconds question setting drives running drives holds booting second drive data data home directories data drive encrypted luks current setup whenever reboot password entry screen decrypt data drive home directory works problem reboot remotely stuck can’t enter password keep home directory won’t enough room symbolic link encrypted drive isn’t mounted creates errors issues suggestions redirect message guys anyone know cannot connect external localhost works forwarded port router machine you’re trying access work machine sure rules hred could alternate install g-vhred might problem graphics card could also alternate install works p-vhred nomodeset nomodeset h-vhred thing command sudo dpkg-reconﬁgure xserver-xorg errors hred pastebin output route please g-vhred want pastebin pastebin output sudo route please pastebin output route p-vhred output route h-vhred static address need specify address gateway hred don’t think problem pulseaudio sure g-vhred thank help thanks suggestion p-vhred think pulseaudio don’t know h-vhred tell problem pulseaudio don’t know hred want data drive mount /etc/fstab g-vhred good place want clean install need reboot p-vhred problem h-vhred want data drive mount /etc/fstab hred need forward port router forward port server g-vhred trying pastebin output sudo netstat pastebin please p-vhred pastebin output sudo aptget install openssh-server h-vhred even working need port forward router table approximate posterior word encoding twitter. numbers computed counting number times word among words largest squared gradients gaussian divergence piecewise constant divergence", "year": 2016}