{"title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY"], "abstract": "While deep learning has had significant successes in computer vision thanks to the abundance of visual data, collecting sufficiently large real-world datasets for robot learning can be costly. To increase the practicality of these techniques on real robots, we propose a modular deep reinforcement learning method capable of transferring models trained in simulation to a real-world robotic task. We introduce a bottleneck between perception and control, enabling the networks to be trained independently, but then merged and fine-tuned in an end-to-end manner to further improve hand-eye coordination. On a canonical, planar visually-guided robot reaching task a fine-tuned accuracy of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 pixels), showing the potential for more complicated and broader applications. Our method provides a technique for more efficient learning and transfer of visuo-motor policies for real robotic systems without relying entirely on large real-world robot datasets.", "text": "figure present technique eﬃcient learning transfer visuo-motor policies planar reaching task simulated real environments using modular deep network however consistent issue faced approaches reliance large amounts data train models. example google researchers addressed problem developing \"arm farm\" robots collecting data parallel generalization forms another challenge many current systems brittle learned models applied robotic conﬁgurations diﬀer used training. leads question better learn transfer visuo-motor policies robots tasks reaching? deep learning signiﬁcant successes computer vision thanks abundance visual data collecting suﬃciently large real-world datasets robot learning costly. increase practicality techniques real robots propose modular deep reinforcement learning method capable transferring models trained simulation real-world robotic task. introduce bottleneck perception control enabling networks trained independently merged ﬁne-tuned end-to-end manner improve hand-eye coordination. canonical planar visually-guided robot reaching task ﬁne-tuned accuracy pixels achieved signiﬁcant improvement naive transfer showing potential complicated broader applications. method provides technique eﬃcient learning transfer visuomotor policies real robotic systems withrelying entirely large real-world robot datasets. advent large datasets sophisticated machine learning models commonly referred deep learning recent years created trend away handcrafted solutions towards data-driven ones. learning techniques shown signiﬁcant improvements robustness performance particularly computer vision ﬁeld. traditionally robotic reaching approaches based crafted controllers combine motion planners hand-crafted features localize target visually. recently learning approaches tackle problem presented decrease amount real world collection necessary approach capable transferring learned models real-world scenarios fraction real-world data typically required direct realworld learning approaches. particular propose modular deep reinforcement learning approach inspired atari game playing eﬃciently learn transfer visuo-motor policies simulated real environments benchmark visually-guided planar reaching task robotic introducing modular approach perception skill controller transferred individually robotic platform retaining ability ﬁne-tune end-to-end fashion improve hand-eye coordination real robot related work data-driven learning approaches become popular computer vision starting replace hand-crafted solutions also robotic applications. especially robotic vision tasks robotic tasks based directly real image data navigation object grasping manipulation seen increased interest. lack large-scale real-world datasets expensive slow acquire limit general applicability approach limited broader application. collecting datasets required deep learning sped using many robots operating parallel grasp attempts recorded deep network trained predict success probability sequence motions aiming grasping robotic manipulator -ﬁnger gripper. combined simple derivative-free optimization algorithm grasping system achieved success rate anexample dataset collection grasping approach self-supervised grasping learning real world force sensors used autonomously label samples training real-world trials using staged leaning method deep convolutional neural network achieved grasping success rate around impressive results achieved high cost terms dollars space time. deepmind showed deep reinforcement learning system able directly synthesize control actions computer games vision data result important exciting breaktransfer directly real robots real cameras observing real scenes fact modest image distortions simulation environment caused performance system fall dramatically. introducing real camera observing game screen even worse increasing interest create robust visuo-motor policies robotic applications especially reaching grasping. levine introduced cnn-based policy representation architecture added guided policy search learn visuo-motor policies allow reduce number real world training providing oracle impressive results achieved complex tasks hanging coat hanger inserting block tightening bottle cap. recently proposed simulate depth images learn transfer grasping skills real-world robotic arms adaptation real-world performed. transfer learning attempts develop methods transfer knowledge diﬀerent tasks reduce amount data collected real world transferring skills simulation real world attractive alternative. progressive neural networks leveraged improve transfer avoid catastrophic forgetting learning complex sequences tasks eﬀectiveness validated reinforcement learning tasks atari maze game playing. modular reinforcement learning approaches shown skill transfer capabilities simulation however methods realworld robotic applications still scarce require manually designed mapping information e.g. similaritybased approach skill transfer robots reduce number real-world images required method adapting visual representations simulated real environments proposed achieving success rate hook loop task times less real-world images end-to-end ﬁne-tuning using weighted losses aiming better hand-eye coordination end-toend ﬁne-tuning conducted combined network separate training using weighted task perception losses. end-to-end ﬁne-tuning replaced control network updated using perception network updated using weighted loss benchmark robotic reaching canonical planar reaching task benchmark evaluate feasibility modular training method. task deﬁned controlling robot end-eﬀector position operational space moves position target robot’s joint conﬁguration represented joint angles spaces related forward kinematics i.e. reaching controller adjusts robot conﬁguration minimize error robot’s current target position i.e. task target position conﬁguration represent scene conﬁguration physical meaning guarantees convenience collecting labelled training reward time step following behaviour policy determines action take state discount factor applied future rewards. deep neural network introduced approximate q-value function named deep network state therefore represented high-dimensional raw-pixel image since latent state features extracted convolutional layers however learned visuo-motor policies highlevel input transfer directly simulated real robots modular deep networks preliminary studies deep visuo-motor policies indicate convolutional layers focus perception i.e. extracting useful information visual inputs fully connected layers perform make learncontrol transfer perception control eﬃcient propose separate perception control modules connected bottleneck layer bottleneck forces network learn low-dimensional representation unlike autoencoders diﬀerence explicitly equate bottleneck layer minimal scene conﬁguration whose meaning introduced section values normalized interval bottleneck perception module learns estimate scene conﬁguration rawpixel image control module learns approximate optimal q-value function deﬁned determining appropriate action given scene conﬁguration i.e. improve performance combined network weighted end-to-end ﬁne-tuning method proposed since experimental results show naive end-to-end ﬁne-tuning using straight-forward loss function work well performance improvement training method perception perception network trained using supervised learning ﬁrst conducted simulation ﬁne-tuned small number real samples skill transfer networks ﬁrst convolutional layer initialized weights pre-trained googlenet observed converge faster achieve higher accuracy. googlenet three input channels compared single channel network weight conversion based standard grey-scale mapping necessary ﬁrst convolutional layer initialization. parts networks initialized random weights. threshold reaching target constant discount factor represents times consecutively smaller threshold determines task completion. reward function yield negative rewards getting close enough target. helps take account temporal costs policy search i.e. fewer steps better. giving positive rewards smaller threshold consecutive times reward function guide learner converge target rather pass reward function proves successful learning planar reaching claim optimality. designing good reward function active topic reinforcement learning. guiding learning k-gps q-learning ε-greedy method frequently used policy search. however experiments show ε-greedy works poorly planar reaching task using multiple therefore introduce kinematics-based controller guide policy search i.e. guide learning operational space controller joint-space controller selects actions data directly measured. consider robotic degrees freedom i.e. steering end-eﬀector position plane i.e. ignoring orientation. task setup real-world task employs baxter robot’s left reach arbitrarily placed blue target using vision. control three joints keeping others ﬁxed. time step possible actions chosen change robot conﬁguration joint increasing decreasing constant amount leaving unchanged. monocular webcam placed tripod observe scene providing raw-pixel image inputs simulator simple simulator created that given scene conﬁguration generates corresponding image. creates images using simplistic representation baxter target represented blue disc radius pixels image reach deemed successful robot reaches keeps end-eﬀector within pixels target’s centre four consecutive actions. experimental results show although simulator low-ﬁdelity therefore cheap fast data collection reaching skills learned transferred real robot. network architecture perception network task architecture shown figure consists convolutional fully-connected layer. images simulator webcam converted grey-scale downsized inputs network. beginning trial arm’s starting conﬁguration target position randomly generated. guarantee random target position reachable ﬁrst randomly select conﬁguration position end-eﬀector target position. also used desired conﬁguration guide policy search. iteration action selected either kinematic controller control network. training decreases linearly i.e. guidance gets weaker process. newly observed sample added network updated using mini-batch randomly selected experiments results perception control networks ﬁrst trained evaluated independently various conditions benchmark reaching task. comparisons made diﬀerent combined networks naively combined ﬁne-tuned networks. evaluations conducted simulated real scenarios baxter robot reaching observed camera. assessing robot perception understand eﬀect adapting perception real images trained networks planar reaching task diﬀerent training data shown table trained scratch purely simulation; trained scratch using real images; trained adapting diﬀerent percentages real images found mini-batches. training adaptation. image collection robot moved ﬁxed conﬁgurations uniformly distributed joint space. target rendered image random position create large number training samples. figure shows typical scene data capture ﬁnal dataset image scaling cropping target addition. increase robustness trained network image dataset augmented applying transformations original images training adaptation rmsprop adopted using mini-batch size learning rate networks trained scratch converged million update steps contrast adapted converged million update steps. performance evaluated using perception error deﬁned euclidean norm predicted ground-truth scene conﬁguration compared three diﬀerent scenarios simulator images uniformly distributed scene conﬁguration space images collected using real robot withheld training diﬀerent scene conﬁgurations live trials baxter results listed table mean standard deviation expected perception network performed well scenarios trained adapted poorly otherwise. network trained simulated images small error simulation poor performance real scenarios similarly network trained adapted real images results show fraction real images mini-batch important balancing real simulated environment performance real images presented training smaller real world experiment error became similarly simulation. particular smallest mean error simulation smallest real world live scenarios. however balancing best performance tested live baxter smaller slightly larger compared comparing performance simulation network adapted simulated images resulted much larger error sim. indicates presence simulated images adaptation prevents network forgetting skills learned. also observe that network adapted using real images smaller error trained scratch shows adaptation pre-trained network leads better performance networks except errors live trials baxter slightly larger real world testing although collected real world dataset augmented translations rotations training. indicates high sensitivity perception networks variations camera pose further test indication trained perception networks without data augmentation resulted signiﬁcantly poorer performance live trials. check sensible network behaviour investigated perception networks behaviour target present. trained networks output incorrect constant values target position prediction. images targets presented networks random mixture target positions output. however cases joint angles estimated accurately. part robot body occluded shown figure conﬁgurations still estimated well although slightly greater error cases. assessing robot control trained control networks simulation planar reaching task varying degrees freedom using ε-greedy k-gps policy search. case active; uses controls three joints. training used learning rate mini-batch size probability decreased within million training steps reaching million steps million steps dof. ε-greedy k-gps used figure shows learning curves indicating success rate network certain number training steps. data point reaching tests performed using criteria introduced section target positions initial conﬁgurations uniformly distributed scene conﬁguration space. also tested several control network architectures varying number hidden layers number units layer. qualitative results show network hidden layer enough reaching insuﬃcient cases. number units layer also inﬂuenced performance. proposed architecture worked best reaching; least hidden layers units needed. end-to-end network performance evaluated end-to-end performance combined networks simulated real-world scenarios using metrics euclidean distance error average accumulated reward simulated trials real trials. testing real scenarios virtual targets rendered image stream camera repeatability simplicity. comparison evaluated three networks end-toend ee-ft. combined network comprising consists ee-ft end-to-end ﬁne-tuning using weighted losses. perception control modules selected section section best performance individually. end-to-end ﬁne-tuning mainly conducted simulation. ﬁne-tuning used learning rate mini-batch size task perception losses respectively exploration possibility k-gps. parameters empirically selected. prevent perception module forgetting skills real scenarios real samples also used obtain δlp. similar samples mini-batch real scenarios i.e. weight updating step real simulated samples used. error distances pixels compared. results listed table dmed median third quartile network perfect perception assumed added baseline. ε-greedy converged success rate around million steps case k-gps ε-greedy converged around took million million steps respectively. reaching converged million million steps respectively. results show k-gps feasible degrees freedom ε-greedy worked appropriately reaching degraded number increased. detailed comparison analyzed error distance euclidean distance end-eﬀector target reached converged network. reaching tests performed network simulation. results sumarized table shows k-gps achieved smaller error distances median dmed third quartile ε-greedy cases. evaluate performance control network real scenarios k-gps trained network directly transferred baxter. test joint angles taken robot’s encoders target position externally. achieved median distance error pixels consecutive reaching trials indicating robustness real-world sensing noise. end-to-end ﬁne-tuning ee-ft achieved much better performance ﬁne-tuned performance close control module controls using ground-truth sensing inputs. indicates proposed ﬁne-tuning approach signiﬁcantly improved hand-eye coordination. real world expected worked poorly since perception network experienced real scenarios. contrast ee-ft worked well achieved comparable performance simulation. note cost real world experiments trials similar results simulation beneﬁting end-to-end ﬁne-tuning ee-ft achieved smaller median distance error shows adaptation real scenarios kept presenting real samples compute perception loss. networks achieved success rate apart weighted end-to-end ﬁne-tuning approach also tried naively ﬁne-tuning combined networks using task loss work well performance improvement although many eﬀorts made searching appropriate hyper-parameters. target occlusions tested ee-ft setups shown figure case without occlusion real targets reached occlusions experienced network training cases targets reached larger distance errors case targets could reached increased error across cases shown attached video. conclusion paper demonstrated reliable vision-based planar reaching real robot using modular deep network trained simulation transference real robot utilizing small number real world images. proposed end-to-end ﬁne-tuning approach using weighted losses signiﬁcantly improved hand-eye coordination naively combined network ﬁne-tuning reaching accuracy improved work following observations ments show feasibility modular structure end-to-end ﬁne-tuning low-cost transfer visuomotor policies. ﬁne-tuning weighted losses combined network comprising perception control modules trained independently even achieve performance close control network alone scaling proposed techniques complicated tasks networks likely achievable appropriate scene conﬁguration representation. perception adaptation small number real-world images suﬃcient adapt pre-trained perception network simulated real scenarios benchmark task even simulator modest visual ﬁdelity. percentage real images mini-batch plays role balancing performance real simulated environments. presence simulated images ﬁne-tuning prevents network forgetting pre-mastered skills. adapted perception network also interesting robustness properties still estimate robot conﬁguration even presence occlusions directly experienced is/are zero multiple targets. control training k-gps guidance kinematic controller k-gps leads better policies shorter time ε-greedy producing trained control network robust real-world sensing noise. however kgps assume already knowledge task learn i.e. model task. introducing bottleneck perception control training networks independently merging ﬁnetuning promising line investigation robotic visual servoing manipulation tasks. current future work scaling complexity robot tasks characterizing performance approach. promising results obtained tabletop object reaching clutter using robotic velocity control mode acknowledgements research conducted australian research council centre excellence robotic vision additional computational resources services provided research support group queensland university technology brisbane australia. deng dong richard socher li-jia fei-fei. imagenet largescale hierarchical image database. ieee conference computer vision pattern recognition coline devin abhishek gupta trevor darrell pieter abbeel sergey levine. learning modular neural network policies multitask multi-robot transfer. ieee international conference robotics automation d’innocente fabio maria carlucci mirco colosi barbara caputo. bridging computer robot vision data augmentation case study object recognition. international conference computer vision systems tesca fitzgerald ashok goel andrea thomaz. similarity-based approach skill transfer. women robotics workshop robotics science systems conference stephen james andrew davison edward johns. transferring end-to-end visuomotor control simulation real world multistage task. annual conference robot learning kapil katyal i-jeng wang philippe burli. leveraging deep reinforcement learning reaching robotic tasks. ieee conference computer vision pattern recognition workshops alex ilya sutskever geoﬀrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages lecun. theoretical framework back-propagation. touretzky hinton sejnowski editors proceedings connectionist models summer school pages pittsburgh morgan kaufmann. sergey levine peter pastor sampedro alex krizhevsky deirdre quillen. learning hand-eye coordination robotic grasping deep learning large-scale data collection. international symposium experimental robotics koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature andrei rusu matej vecerik thomas rothörl nicolas heess razvan pascanu raia hadsell. sim-to-real robot learning pixels progressive nets. annual conference robot learning szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. ieee conference computer vision pattern recognition pages tijmen tieleman geoﬀrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning josh tobin rachel fong alex jonas schneider wojciech zaremba pieter abbeel. domain randomization transferring deep neural networks simulation real world. ieee/rsj international conference intelligent robots systems adam sareh shirazi jürgen leitner niko sünderhauf michael milford upcroft. robustness analysis deep networks. australasian conference robotics automation eric tzeng coline devin judy hoﬀman chelsea finn pieter abbeel sergey levine kate saenko trevor darrell. adapting deep visuomotor representations weak pairwise constraints. workshop algorithmic foundations robotics ulrich viereck andreas kate saenko robert platt. learning visuomotor controller real world robotic grasping using simulated depth images. annual conference robot learning jürgen leitner michael milford upcroft peter corke. towards vision-based deep reinforcement learning robotic motion control. australasian conference robotics automation december fangyi zhang jürgen leitner michael milford peter corke. sim-to-real transfer visuo-motor policies reaching clutter domain randomization adaptation modular networks. technical report queensland university technology fangyi zhang jurgen leitner michael milford peter corke. tuning modular networks weighted losses hand-eye coordination. ieee conference computer vision pattern recognition workshops july", "year": 2016}