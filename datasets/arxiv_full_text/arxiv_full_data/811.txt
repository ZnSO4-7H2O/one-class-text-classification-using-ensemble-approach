{"title": "End-to-End Differentiable Proving", "tag": ["cs.NE", "cs.AI", "cs.LG", "cs.LO"], "abstract": "We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.", "text": "introduce neural networks end-to-end differentiable proving queries knowledge bases operating dense vector representations symbols. neural networks constructed recursively taking inspiration backward chaining algorithm used prolog. speciﬁcally replace symbolic uniﬁcation differentiable computation vector representations symbols using radial basis function kernel thereby combining symbolic reasoning learning subsymbolic vector representations. using gradient descent resulting neural network trained infer facts given incomplete knowledge base. learns place representations similar symbols close proximity vector space make similarities prove queries induce logical rules provided induced logical rules multi-hop reasoning. demonstrate architecture outperforms complex state-of-the-art neural link prediction model three four benchmark knowledge bases time inducing interpretable function-free ﬁrst-order logic rules. current state-of-the-art methods automated knowledge base completion neural link prediction models learn distributed vector representations symbols scoring fact triples subsymbolic representations enable models generalize unseen facts encoding similarities vector predicate symbol grandfatherof similar vector symbol grandpaof predicates likely express similar relation. likewise vector constant symbol lisa similar maggie similar relations likely hold constants simple form reasoning based similarities remarkably effective automatically completing large kbs. however practice often important capture complex reasoning patterns involve several inference steps. example father homer homer parent bart would like infer grandfather bart. transitive reasoning inherently hard neural link prediction models learn score facts locally. contrast symbolic theorem provers like prolog enable exactly type multi-hop reasoning. furthermore inductive logic programming builds upon provers learn interpretable rules data exploit reasoning kbs. however symbolic provers lack ability learn subsymbolic representations similarities large limits ability generalize queries similar identical symbols. connection logic machine learning addressed statistical relational learning approaches models traditionally support reasoning subsymbolic representations using subsymbolic representations trained end-to-end training data neural multi-hop reasoning models address aforementioned limitations extent encoding reasoning chains vector space iteratively reﬁning subsymbolic representations question comparison answers. many ways models operate like basic theorem provers lack crucial ingredients interpretability straightforward ways incorporating domain-speciﬁc knowledge form rules. approach problem inspired recent neural network architectures like neural turing machines memory networks neural stacks/queues neural programmer neural programmer-interpreters hierarchical attentive memory differentiable forth interpreter architectures replace discrete algorithms data structures end-toend differentiable counterparts operate real-valued vectors. heart approach idea translate concept basic symbolic theorem provers hence combine advantages ability reason vector representations predicates constants. speciﬁcally keep variable binding symbolic compare symbols using subsymbolic vector representations. concretely introduce neural theorem provers end-to-end differentiable provers basic theorems formulated queries prolog’s backward chaining algorithm recipe recursively constructing neural networks capable proving queries using subsymbolic representations. success score proofs differentiable respect vector representations symbols enables learn representations predicates constants ground atoms well parameters function-free ﬁrst-order logic rules predeﬁned structure. ntps learn place representations similar symbols close proximity vector space induce rules given prior assumptions structure logical relationships transitivity. furthermore ntps seamlessly reason provided domain-speciﬁc rules. ntps operate distributed representations symbols single hand-crafted rule leveraged many proofs queries symbols similar representation. finally ntps demonstrate high degree interpretability induce latent rules decode human-readable symbolic rules. contributions threefold present construction ntps inspired prolog’s backward chaining algorithm differentiable uniﬁcation operation using subsymbolic representations propose optimizations architecture joint training neural link prediction model batch proving approximate gradient calculation experimentally show ntps learn representations symbols function-free ﬁrst-order rules predeﬁned structure enabling learn perform multi-hop reasoning benchmark outperform complex state-of-the-art neural link prediction model three four kbs. section brieﬂy introduce syntax remainder paper. refer reader in-depth introduction. atom consists predicate symbol list terms. lowercase names refer predicate constant symbols uppercase names variables consider function-free ﬁrst-order logic rules term constant variable. instance atom predicate grandfatherof terms variable constant bart. consider rules form body possibly empty conjunction atoms represented list head atom. call rule free variables ground rule. variables universally quantiﬁed. call ground rule empty body fact. substitution {x/t /tn} assignment variable symbols terms applying substitutions atom replaces occurrences variables respective term given query prolog’s backward chaining algorithm substitutions high level backward chaining based functions called and. iterates rules uniﬁes goal respective rule head thereby updating substitution set. called since successful proof sufﬁces uniﬁcation succeeds calls prove atoms body rule. prove subgoals rule body ﬁrst applies substitutions ﬁrst atom proven calling proving remaining subgoals recursively calling and. function called atoms body need proven together example rule used following describe recursive construction ntps neural networks end-to-end differentiable proving allow calculate gradient proof successes respect vector representations symbols. deﬁne construction ntps terms modules similar dynamic neural module networks module takes inputs discrete objects proof state returns list proof states proof state tuple consisting substitution constructed proof neural network outputs real-valued success score proof. discrete objects substitution used construction neural network network constructed continuous proof success score calculated many different goals training test time. summarize modules instantiated discrete objects substitution set. construct neural network representing proof success score recursively instantiate submodules continue proof. shared signature modules domain controls construction network domain proof states number output proof states. furthermore denote substitution proof state denote neural network calculating proof success. pseudocode style functional programming language deﬁne behavior modules auxiliary functions. particularly making pattern matching check properties arguments passed module. denote sets euler script letters lists small capital letters lists lists blackboard bold letters refer prepending element list atom list predicate symbol terms rule seen list atoms thus list lists head list rule head. figure module mapping upstream proof state list proof states thereby extending substitution adding nodes computation graph neural network representing proof success. uniﬁcation atoms e.g. goal want prove rule head central operation backward chaining. non-variable symbols checked equality proof aborted check fails. however want able apply rules even symbols goal head equal similar meaning thus replace symbolic comparison computation measures similarity symbols vector space. module unify updates substitution creates neural network comparing vector representations non-variable symbols sequences terms. signature module domain lists terms. unify takes atoms represented lists terms upstream proof state maps proof state unify iterates list terms atoms compares symbols. symbols variable substitution added substitution set. otherwise vector representations non-variable symbols compared using radial basis function kernel hyperparameter experiments. following pseudocode implements unify. note matches every argument here refers proof state refers variable symbols substitution variable symbol symbol denotes embedding lookup non-variable symbol index unify parameterized embedding matrix r|z|×k non-variables symbols dimension vector representations symbols. furthermore fail represents uniﬁcation failure mismatching arity atoms. failure reached abort creation neural network branch proving. addition constrain proofs cycle-free checking whether variable already bound. note simple heuristic prohibits applying non-ground rule twice. sophisticated ways ﬁnding avoiding cycles proof graph rule still applied multiple times leave future work. example assume unifying atoms given upstream proof state latter input atom placeholders predicate constant neural network would output evaluated. furthermore assume grandpaof bart represent indices respective symbols global symbol vocabulary. then proof state constructed unify thus output score neural network high subsymbolic representation input close grandpaof input close bart. however score cannot higher upstream proof success score forward pass neural network note addition extending neural networks module also outputs substitution {q/abe} graph creation time used instantiate submodules. based unify deﬁne module attempts apply rules signature domain goal atoms domain integers used specifying maximum proof depth neural network. furthermore number possible output proof states goal given structure provided implement denotes rule given head atom list body atoms contrast symbolic method module able grandfatherof rule query involving grandpaof provided subsymbolic representations predicates similar measured kernel unify module. creation neural network dependent also structure goal. instance goal would result different neural network hence different number output proof states substitute example substitute{x/q y/i}) results signature domain lists atoms number possible output proof states list atoms known structure provided module implemented andk andk andk andk ﬁrst lines deﬁne failure proof either upstream uniﬁcation failure passed module maximum proof depth reached line speciﬁes proof success i.e. list subgoals empty maximum proof depth reached. lastly line deﬁnes recursion ﬁrst subgoal proven instantiating module substitutions applied every resulting proof state used proving remaining subgoals instantiating modules. example figure illustrates examplary computation graph constructed note constructed training used proving goals structure training test time index input predicate indices input constants. final proof states used proof aggregation underlined. ntps gradient descent instead combinatorial search space rules example done first order inductive learner speciﬁcally using concept learning entailment induce rules prove known ground atoms give high proof success scores sampled unknown ground atoms. representations unknown predicates indices respectively. prior knowledge transitivity three unknown predicates speciﬁed figure exemplary construction computation graph knowledge base. indices arrows correspond application respective rule. proof states subscripted sequence indices rules applied. underlined proof states aggregated obtain ﬁnal proof success. boxes visualize instantiations modules proofs fail cycle-detection call parameterized rule corresponding predicates unknown representations learned data. rule used proofs training test time given rule. training predicate representations parameterized rules optimized jointly subsymbolic representations. thus model adapt parameterized rules proofs known facts succeed proofs sampled unknown ground atoms fail thereby inducing rules predeﬁned structures like above. inspired rule templates conveniently deﬁning structure multiple parameterized rules specifying number parameterized rules instantiated given rule structure inspection training decode parameterized rule searching closest representations known predicates. addition provide users rule conﬁdence taking minimum similarity unknown decoded predicate representations using kernel unify. conﬁdence score upper bound proof success score achieved induced rule used proofs. section present basic training loss ntps training loss neural link prediction models used auxiliary task well various computational optimizations. training objective known facts given usually observe negative facts thus resort sampling corrupted ground atoms done previous work speciﬁcally every obtain corrupted ground atoms sampling ˆj˜i constants. corrupted ground atoms resampled every iteration training denote known corrupted ground atoms together target score negative log-likelihood proof success score loss function parameters given substitution list resulting proof state. prove known facts trivially uniﬁcation themselves resulting parameter updates training hence generalization. therefore training masking calculation uniﬁcation success known ground atom want prove. speciﬁcally uniﬁcation score temporarily hide training fact assume proven facts rules beginning training subsymbolic representations initialized randomly. unifying goal facts consequently noisy success scores early stages training. moreover maximum success score result gradient updates respective subsymbolic representations along maximum proof path take long time ntps learn place similar symbols close vector space make effective rules. speed learning subsymbolic representations train ntps jointly complex complex share subsymbolic representations feasible kernel unify also deﬁned complex vectors. responsible multi-hop reasoning neural link prediction model learns score ground atoms locally. test time used predictions. thus training loss complex seen auxiliary loss subsymbolic representations learned ntp. term resulting model ntpλ. based loss section joint training loss deﬁned lntpλk ntps described suffer severe computational limitations since neural network representing possible proofs predeﬁned depth. contrast symbolic backward chaining proof aborted soon uniﬁcation fails differentiable proving uniﬁcation failure atoms whose arity match detect cyclic rule application. propose optimizations speed ntps appendix. first make modern gpus batch processing many proofs parallel second exploit sparseness gradients caused operations used uniﬁcation proof aggregation respectively derive heuristic truncated forward backward pass drastically reduces number proofs considered calculating gradients consistent previous work carry experiments four benchmark compare complex ntpλ terms area precision-recall-curve countries mean reciprocal rank hitsm described below. training details including hyperparameters rule templates found appendix countries countries dataset introduced testing reasoning capabilities neural link prediction models. consists countries regions subregions facts neighborhood countries location countries subregions. follow split countries randomly training countries development countries test countries every test country least neighbor training set. subsequently three different task datasets created. tasks goal predict locatedin every test country regions access training atoms varies. ground atoms locatedin test country region removed since information test countries still contained task solved using transitivity rule locatedin locatedin locatedin. addition ground atoms locatedin removed test country term term term term term term term term term. blockpositionindex blockpositionindex. expeldiplomats negativebehavior. negativecomm commonbloc. intergovorgs intergovorgs. interacts_with subregion. location test countries needs inferred location neighboring countries locatedin neighborof locatedin. task difﬁcult neighboring countries might region rule always hold. addition ground atoms locatedin region training country test country neighbor also removed. location instance inferred using three-hop rule locatedin neighborof neighborof locatedin. kinship nations umls nations alyawarra kinship uniﬁed medical language system left animals dataset contains unary predicates thus used evaluating multi-hop reasoning. nations contains binary predicates unary predicates constants true facts kinship contains predicates constants true facts umls contains predicates constants true facts. since baseline complex cannot deal unary predicates remove unary atoms nations. split every training facts development facts test facts. evaluation take test fact corrupt ﬁrst second argument possible ways corrupted fact original subsequently predict ranking every test fact corruptions calculate hitsm. results different model variants benchmark shown table another method inducing rules differentiable automated completion introduced recently evaluation setup equivalent protocol however neural link prediction baseline complex already achieves much higher hits results thus focus comparison ntps complex. first note vanilla ntps alone work particularly well compared complex. outperform complex countries nations kinship umls. demonstrates difﬁculty learning subsymbolic representations differentiable prover uniﬁcation alone need auxiliary losses. ntpλ complex auxiliary loss outperforms models majority tasks. difference auc-pr complex ntpλ signiﬁcant countries tasks major advantage ntps inspect induced rules provide interpretable representation model learned. right column table shows examples induced rules ntpλ countries recovered rules needed solving three different tasks. umls induced transitivity rules. relationships particularly hard encode neural link prediction models like complex optimized locally predict score fact. combining neural symbolic approaches relational learning reasoning long tradition various proposed architectures past decades review). early proposals neural-symbolic networks limited propositional rules kbann c-ilp neural-symbolic approaches focus ﬁrst-order inference learn subsymbolic vector representations training facts neural prolog clip++ lifted relational neural networks tensorlog logic tensor networks spirit similar ntps need fully ground ﬁrst-order logic rules. however support function terms whereas ntps currently support function-free terms. recent question-answering architectures translate query representations implicitly vector space without explicit rule representations thus easily incorporate domainspeciﬁc knowledge. addition ntps related random walk path encoding models however instead aggregating paths random walks encoding paths predict target predicate reasoning steps ntps explicit uniﬁcation uses subsymbolic representations. allows induce interpretable rules well incorporate prior knowledge either form rules form rule templates deﬁne structure logical relationships expect hold another line work regularizes distributed representations domain-speciﬁc rules approaches learn rules data support restricted subset ﬁrst-order logic. ntps constructed prolog’s backward chaining thus related uniﬁcation neural networks however ntps operate vector representations symbols instead scalar values expressive. ntps learn rules data related systems foil sherlock meta-interpretive learning higher-order dyadic datalog systems operate symbols search discrete space logical rules ntps work subsymbolic representations induce rules using gradient descent. recently introduced differentiable rule learning system based tensorlog neural network controller similar lstms method scalable ntps introduced here. however umls kinship baseline already achieved stronger generalization learning subsymbolic representations. still scaling ntps larger competing scalable relational learning methods open problem seek address future work. proposed end-to-end differentiable prover automated completion operates subsymbolic representations. used prolog’s backward chaining algorithm recipe recursively constructing neural networks used prove queries speciﬁcally introduced differentiable uniﬁcation operation vector representations symbols. constructed neural network allowed compute gradient proof successes respect vector representations symbols thus enabled train subsymbolic representations end-toend facts induce function-free ﬁrst-order logic rules using gradient descent. benchmark model outperformed complex state-of-the-art neural link prediction model three four time inducing interpretable rules. overcome computational limitations end-to-end differentiable prover introduced paper want investigate hierarchical attention reinforcement learning methods monte carlo tree search used learning play chemical synthesis planning addition plan support function terms future. based furthermore interested applying ntps automated proving mathematical theorems either logical natural language form similar recent approaches thank pasquale minervini dettmers matko bosnjak johannes welbl naoya inoue arulkumaran anonymous reviewers helpful comments drafts paper. work supported google fellowship natural language processing allen distinguished investigator award marie curie career integration award. references maximilian nickel volker tresp hans-peter kriegel. factorizing yago scalable machine learning linked data. proceedings world wide conference lyon france april pages ./.. sebastian riedel limin andrew mccallum benjamin marlin. relation extraction matrix factorization universal schemas. human language technologies conference north american chapter association computational linguistics proceedings june westin peachtree plaza hotel atlanta georgia pages richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. pages kai-wei chang wen-tau bishan yang christopher meek. typed tensor decomposition knowledge bases relation extraction. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages bishan yang wen-tau xiaodong jianfeng deng. embedding entities relations learning inference knowledge bases. international conference learning representations kristina toutanova danqi chen patrick pantel hoifung poon pallavi choudhury michael gamon. representing text joint embedding text knowledge bases. proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages théo trouillon johannes welbl sebastian riedel éric gaussier guillaume bouchard. complex embeddings simple link prediction. proceedings international conference machine learning icml york city june pages hervé gallaire jack minker editors. logic data bases symposium logic data bases centre d’études recherches toulouse advances data base theory york plemum press. isbn ---x. stanley pedro domingos. statistical predicate invention. machine learning proceedings twenty-fourth international conference corvallis oregon june pages ./.. matt gardner partha pratim talukdar bryan kisiel mitchell. improving learning inference large knowledge-base using latent syntactic cues. proceedings conference empirical methods natural language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group pages matt gardner partha pratim talukdar jayant krishnamurthy mitchell. incorporating vector space similarity random walk inference knowledge bases. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages arvind neelakantan benjamin roth andrew mccallum. compositional vector space models proceedings annual meeting association knowledge base completion. computational linguistics international joint conference natural language processing asian federation natural language processing july beijing china volume long papers pages rajarshi arvind neelakantan david belanger andrew mccallum. chains reasoning entities relations text using recurrent neural networks. conference european chapter association computational linguistics yelong shen po-sen huang jianfeng weizhu chen. reasonet learning stop reading machine comprehension. proceedings workshop cognitive computation integrating neural symbolic approaches co-located annual conference neural information processing systems barcelona spain december edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce unbounded memory. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages armand joulin tomas mikolov. inferring algorithmic patterns stack-augmented recurrent nets. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages marcin andrychowicz misha denil sergio gomez colmenarejo matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. advances neural information processing systems annual conference neural information processing systems december barcelona spain pages jacob andreas marcus rohrbach trevor darrell klein. learning compose neural networks question answering. naacl conference north american chapter association computational linguistics human language technologies diego california june pages william yang wang william cohen. joint information extraction reasoning scalable statistical relational learning approach. proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing july beijing china volume long papers pages antoine bordes nicolas usunier alberto garcía-durán jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. pages guillaume bouchard sameer singh theo trouillon. approximate reasoning capabilities low-rank vector spaces. proceedings aaai spring symposium knowledge representation reasoning integrating symbolic neural approaches maximilian nickel lorenzo rosasco tomaso poggio. holographic embeddings knowledge graphs. proceedings thirtieth aaai conference artiﬁcial intelligence february phoenix arizona usa. pages lokendra shastri. neurally motivated constraints working memory capacity production system parallel processing implications connectionist model based temporal synchrony. proceedings fourteenth annual conference cognitive science society july august cognitive science program indiana university bloomington volume page psychology press liya ding. neural prolog-the concepts construction mechanism. systems cybernetics intelligent systems century. ieee international conference volume pages ieee manoel frança gerson zaverucha artur d’avila garcez. fast relational learning using bottom clause propositionalization artiﬁcial neural networks. machine learning ./s---. gustav sourek vojtech aschenbrenner filip zelezný ondrej kuzelka. lifted relational neural networks. proceedings nips workshop cognitive computation integrating neural symbolic approaches co-located annual conference neural information processing systems montreal canada december luciano seraﬁni artur d’avila garcez. logic tensor networks deep learning logical reasoning proceedings international workshop neural-symbolic data knowledge. learning reasoning co-located joint multi-conference human-level artiﬁcial intelligence york city july mitchell william cohen. random walk inference learning large scale knowledge base. proceedings conference empirical methods natural language processing emnlp july john mcintyre conference centre edinburgh meeting sigdat special interest group pages amarnag subramanya fernando pereira william cohen. reading learned syntactic-semantic inference rules. proceedings joint conference empirical methods natural language processing computational natural language learning emnlp-conll july jeju island korea pages rocktäschel sameer singh sebastian riedel. injecting logical background knowledge embeddings relation extraction. naacl conference north american chapter association computational linguistics human language technologies denver colorado june pages zhiting xuezhe zhengzhong eduard hovy eric xing. harnessing deep neural networks logic rules. proceedings annual meeting association computational linguistics august berlin germany volume long papers thomas demeester rocktäschel sebastian riedel. lifted rule injection relation embeddings. proceedings conference empirical methods natural language processing emnlp austin texas november pages steffen hölldobler. structured connectionist uniﬁcation algorithm. proceedings national conference artiﬁcial intelligence. boston massachusetts july august volumes. pages stefan schoenmackers jesse davis oren etzioni daniel weld. learning ﬁrst-order horn clauses text. proceedings conference empirical methods natural language processing emnlp october stata center massachusetts meeting sigdat special interest group pages rémi coulom. efﬁcient selectivity backup operators monte-carlo tree search. computers games international conference turin italy revised papers pages ./----_. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou vedavyas panneershelvam marc lanctot sander dieleman dominik grewe john nham kalchbrenner ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature ./nature. cezary kaliszyk françois chollet christian szegedy. holstep machine learning dataset higher-order logic theorem proving. international conference learning representations xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural proceedings thirteenth international conference artiﬁcial intelligence martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro gregory corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal józefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek gordon murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. corr abs/. complex state-of-the-art neural link prediction model represents symbols complex vectors. real denote real part imag imaginary part complex vector representing symbol index. scoring function deﬁned complex denotes element-wise multiplication sigmoid function. beneﬁt complex neural link prediction models rescal distmult using complex vectors subsymbolic representations capture symmetric well asymmetric relations. batch proving rn×k matrix subsymbolic representations uniﬁed representations rm×k. adapt uniﬁcation module calculate uniﬁcation success batched using vectors ones respectively square root taken elementwise. practice partition rules structure batch-unify goals rule heads partition time graphics processing unit furthermore substitution sets bind variables vectors symbol indices instead single symbol indices operations taken goal. ntps allow calculate gradient proof success scores respect subsymbolic representations rule parameters. backpropagating large computation graph give exact gradient computationally infeasible reasonably-sized consider parameterized rule assume given contains facts binary predicates. bound respective representations goal substituted every possible second argument facts proving ﬁrst atom body. moreover substitutions need compare facts proving second atom body rule resulting proof success scores. however note since operator aggregating success different proofs subsymbolic representations proofs receive gradients. overcome computational limitation propose following heuristic. assume unifying ﬁrst atom facts unlikely uniﬁcation successes successes attain maximum proof success unifying remaining atoms body rule facts uniﬁcation ﬁrst atom keep substitutions success scores continue proving these. means partial proofs contribute forward pass stage consequently receive gradients backward pass backpropagation. term heuristic. note cannot guarantee anymore gradient proof success exact gradient large enough close approximation true gradient. adam initial learning rate mini-batch size optimization. apply regularization model parameters clip gradient values subsymbolic representations rule parameters initialized using xavier initialization train models epochs repeat every experiment countries corpus times. statistical signiﬁcance tested using independent t-test. models implemented tensorflow maximum proof depth following rule templates number front rule template indicates often parameterized rule given structure instantiated. note rule template speciﬁes predicate representations body shared.", "year": 2017}