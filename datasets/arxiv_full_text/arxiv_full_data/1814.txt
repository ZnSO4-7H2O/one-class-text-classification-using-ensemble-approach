{"title": "Learning Representations of Emotional Speech with Deep Convolutional  Generative Adversarial Networks", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Automatically assessing emotional valence in human speech has historically been a difficult task for machine learning algorithms. The subtle changes in the voice of the speaker that are indicative of positive or negative emotional states are often \"overshadowed\" by voice characteristics relating to emotional intensity or emotional activation. In this work we explore a representation learning approach that automatically derives discriminative representations of emotional speech. In particular, we investigate two machine learning strategies to improve classifier performance: (1) utilization of unlabeled data using a deep convolutional generative adversarial network (DCGAN), and (2) multitask learning. Within our extensive experiments we leverage a multitask annotated emotional corpus as well as a large unlabeled meeting corpus (around 100 hours). Our speaker-independent classification experiments show that in particular the use of unlabeled data in our investigations improves performance of the classifiers and both fully supervised baseline approaches are outperformed considerably. We improve the classification of emotional valence on a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, which is competitive to state-of-the-art performance.", "text": "automatically assessing emotional valence human speech historically difﬁcult task machine learning algorithms. subtle changes voice speaker indicative positive negative emotional states often overshadowed voice characteristics relating emotional intensity emotional activation. work explore representation learning approach automatically derives discriminative representations emotional speech. particular investigate machine learning strategies improve classiﬁer performance utilization unlabeled data using deep convolutional generative adversarial network multitask learning. within extensive experiments leverage multitask annotated emotional corpus well large unlabeled meeting corpus speakerindependent classiﬁcation experiments show particular unlabeled data investigations improves performance classiﬁers fully supervised baseline approaches outperformed considerably. improve classiﬁcation emotional valence discrete -point scale -point scale competitive state-of-the-art performance. machine learning general affective computing particular rely good data representations features good discriminatory faculty classiﬁcation regression experiments emotion recognition speech. derive efﬁcient representations data researchers adopted main strategies carefully crafted tailored feature extractors designed particular task algorithms learn representations automatically data latter approach called representation learning received growing attention past years highly reliant large quantities data. approaches emotion recognition speech still rely extraction standard acoustic features pitch shimmer jitter mfccs notable exceptions work leverage strategies automatically learn representations emotional speech material based upon work supported u.s. army research laboratory contract number wnf--d-. opinions ﬁndings conclusions recommendations expressed material author necessarily reﬂect views government ofﬁcial endorsement inferred. learn strong representations speech seek leverage much data possible. however emotion annotations difﬁcult obtain scarce leverage usc-iemocap dataset comprises around hours highly emotional partly acted data speakers however improve learned representations emotional speech unlabeled speech data unrelated meeting corpus consists hours data meeting corpus qualitatively quite different highly emotional usc-iemocap data believe learned representations improve additional data. combination separate data sources leads semi-supervised machine learning task extend architecture deep convolutional generative neural network trained unsupervised fashion within work particularly target emotional valence primary task shown challenging emotional dimension acoustic analyses number studies apart solely targeting valence classiﬁcation investigate principle multitask learning. multitask learning related tasks learned along primary task tasks share parts network topology hence jointly trained depicted figure expected data secondary task models information would also discriminative learning primary task. fact approach shown improve generalizability across corpora remainder paper organized follows first introduce dcgan model discuss prior work section describe speciﬁc multitask dcgan model section introduce datasets section describe experimental design section finally report results section discuss ﬁndings section proposed model builds upon previous results ﬁeld emotion recognition leverages prior work representation learning. multitask learning effective prior experiments emotion detection. particular proposed multitask model emotion recognition which like investigated model activation valence targets work uses investigated multitask model based upon dcgan architecture described section implemented tensorflow. emotion classiﬁcation fully connected layer attached ﬁnal convolutional layer dcgan’s discriminator. output layer separate fully connected layers outputs valence label outputs activation label. setup shown visually figure setup model able take advantage unlabeled data during training feeding dcgan layers model also able take advantage multitask learning train valence activation outputs simultaneously. particular model trained iteratively running generator discriminator valence classiﬁer activation classiﬁer back-propagating error component network. loss functions generator discriminator unaltered remain shown section valence classiﬁer activation classiﬁer cross entropy loss equation since valence activation classiﬁers share layers discriminator model learns features convolutional ﬁlters effective tasks valence classiﬁcation activation classiﬁcation discriminating real generated samples. semi-supervised nature proposed multitask dcgan model utilize labeled unlabeled data. unlabeled data audio iemocap datasets. labeled data audio iemocap dataset comes labels activation valence measured -point likert scale three distinct annotators. although iemocap provides per-word activation valence labels practice labels generally change time given audio simplicity label audio clip average valence activation. since valence activation measured -point scale labels encoded -element one-hot vectors. instance valence represented vector one-hot encoding deep belief network architecture classify emotion audio input valence activation secondary tasks. experiments indicate multitask learning produces improved unweighted accuracy emotion classiﬁcation task. like proposed model uses multitask learning valence activation targets. unlike them however primarily interested emotion classiﬁcation valence classiﬁcation primary task. thus multitask model valence primary target activation secondary target. also experiments iemocap database like method speaker split differs theirs. leave-one-speaker-out cross validation scheme separate train test sets speaker overlap. method lacks distinct validation set; instead validate test set. experimental setup hand splits data distinct train validation test sets still speaker overlap. described greater detail section unsupervised learning part investigated model builds upon architecture known deep convolutional generative adversarial network dcgan. dcgan consists components known generator discriminator trained minimax setup. generator learns samples random distribution output matrices prespeciﬁed form. discriminator takes input either generator output real sample dataset. discriminator learns classify input either generated real training discriminator uses cross entropy loss function based many inputs correctly classiﬁed real many correctly classiﬁed generated. cross entropy loss true labels predictions deﬁned learned vector weights number samples. purposes computation labels represented numerical values real generated. then letting represent discriminator’s predictions real inputs cross entropy correct predictions real simpliﬁes case correct predictions ones. similarly letting represent discriminator’s predictions generated inputs cross entropy correct predictions generated simpliﬁes thus generator’s loss gets lower better able produce outputs discriminator thinks real. leads generator eventually produce outputs look like real samples speech given sufﬁcient training iterations. thought probability distribution representing likelihood correct label particular value. thus cases annotators disagree valence activation label represented assigning probabilities multiple positions label vector. instance label conceptually means correct valence either equal probability corresponding vector would fuzzy labels shown improve classiﬁcation performance number applications noted generally greater success fuzzy label method training neural network model valence label directly i.e. classiﬁcation task regression. pre-processing. audio data network models form spectrograms. spectrograms computed using short time fourier transform window size samples sampling rate equivalent spectrogram pixels high representing frequency range khz. varying lengths iemocap audio ﬁles spectrograms vary width poses problem batching process neural network training. compensate this model randomly crops region input spectrogram. crop width determined advance. ensure selected crop region contains least data cropping occurs using following procedure random word transcript audio selected corresponding time range looked random point within time range selected treated center line crop. crop made using region deﬁned center line crop width. early found noticeable imbalance valence labels iemocap data labels skew heavily towards neutral range. order prevent model overﬁtting distribution training normalize training data oversampling underrepresented valence data overall distribution valence labels even. investigated models. investigate impact unlabeled data improved emotional speech representations multitask learning emotional valence classiﬁcation performance. compared four different neural network models basiccnn represents bare minimum valence classiﬁer thus sets lower bound expected performance. comparison multitaskcnn indicates effect inclusion secondary task i.e. emotional activation recognition. comparison basicdcgan indicates effect incorporation unlabeled data training. fairness architectures three baselines based upon full multitaskdcgan model. basicdcgan example simply multitaskdcgan model activation layer removed fully supervised baselines built taking convolutional layers discriminator component multitaskdcgan adding fully connected layers valence activation output. speciﬁcally discriminator contains four convolutional layers; explicit pooling kernel stride size image size gets halved step. thus design four models convolutional structure. ensure potential performance gains stem larger complexity higher number trainable weights within dcgan models rather stem improved representations speech. experimental procedure. parameters model including batch size ﬁlter size learning rate determined randomly sampling different parameter combinations training model parameters computing accuracy held-out validation set. model kept parameters yield best accuracy held-out set. procedure ensures model fairly represented evaluation. hyper-parameters included crop width input signal convolutional layer ﬁlter sizes number convolutional ﬁlters batch size learning rates e−}. identiﬁed parameters model shown table evaluation utilized -fold leave-one-session-out validation. fold leaves sessions labeled iemocap data training entirely. left-out conversation speaker’s audio used validation speaker’s audio used test set. fold evaluation procedure follows model evaluated trained training full pass training accuracy computed validation set. process continues accuracy validation found longer increase; words locate local maximum validation accuracy. increase certainty local maximum truly representative model’s best performance continue iterations local maximum found look consecutive iterations lower accuracy values. course iterations higher accuracy value found treated local maximum search restarts there. best accuracy value found manner restore model’s weights iteration corresponding best accuracy evaluate accuracy test set. evaluation strategy. collected several statistics models’ performances. primarily interested unweighted per-class accuracy. addition converted network’s output probability distributions back numerical model’s prediction original form element vector probability distribution. used compute pearson correlation predicted actual labels. table evaluation metrics four models averaged across test folds. speaker-independent unweighted accuracies -class -class valence performance well pearson correlation reported. table confusion matrix -class valence classiﬁcation basicdcgan model. predictions reported columns actual targets rows. valence classes sorted negative positive. classes correspond numeric labels pre-processing needed obtain accurate measures. particular cases human annotators perfectly split correct label particular sample possibilities accepted correct predictions. instance correct label correct prediction could either measures -point labeling scale iemocap data originally labeled. however prior experiments evaluated performance valence classiﬁcation -point scale authors provide example this valence levels pooled single negative category valence level remaining untouched neutral category valence levels pooled single positive category. thus allow comparison models also report results -point scale. construct results taking results class comparison pooling described. table shows unweighted per-class accuracies pearson correlation coeffecients actual predicted labels model. values shown average values across test sets folds. results indicate unsupervised learning yields clear improvement performance. basicdcgan multitaskdcgan considerably better accuracies linear correlations compared fully supervised models. strong indication large quantities task-unrelated speech data improved ﬁlter learning layers dcgan discriminator. multitask learning hand appear positive impact performance. comparing models addition multitask learning actually appears impair performance multitaskcnn worse basiccnn three metrics. difference smaller comparing basicdcgan multitaskdcgan enough decidedly conclude multitask learning negative impact there certainly indication positive impact. observed performance basicdcgan multitaskdcgan using -classes comparable state-of-theart compared reported needs noted data test speaker’s session partner utilized training model. models contrast trained four sessions discussed further presented models trained spectrograms audio feature extraction employed whatsoever. representation learning approach employed order allow dcgan component model train vast amounts unsupervised report confusion matrix best performing model basicdcgan table noted negative class classiﬁed best. however appears class picked frequently model resulting high recall precision class highest score very positive confusion very negative valence very positive valence right corner interesting previously observed investigated unsupervised multitask learning improve performance emotional valence classiﬁer. overall found unsupervised learning yields considerable improvements classiﬁcation accuracy emotional valence recognition task. best performing model achieves -class case -class case signiﬁcant pearson correlation continuous target label prediction indication multitask learning provides advantage. results multitask learning somewhat surprising. valence activation classiﬁcation tasks sufﬁciently related multitask learning yield improvements accuracy. alternatively different neural network architecture needed multitask learning work. further alternating update strategy employed present work might optimal strategy training. iterative swapping target tasks valence/activation might created instabilities weight updates backpropagation algorithm. explanations; investigation warranted. lastly important note model’s performance approaching state-of-the-art employs potentially better suited sequential classiﬁers long short-term memory networks however basic lstm suited learn entirely unsupervised data leveraged proposed dcgan models. future work hope adapt technique using unlabeled data sequential models including lstm. expect combining work advantages sequential models result performance gains competitive today’s leading models potentially outperform them. purposes investigation takeaway unsupervised learning yields clear performance gains emotional valence classiﬁcation task represents technique adapted models achieve even higher classiﬁcation accuracies. scherer kane gobl schwenker investigating fuzzy-input fuzzy-output support vector machines robust voice quality classiﬁcation computer speech language vol. thiel scherer schwenker fuzzy-input fuzzyoutput one-against-all support vector machines international conference knowledge-based intelligent information engineering systems vol. lecture notes artiﬁcial intelligence springer. angeliki metallinou martin wollmer athanasios katsamanis florian eyben bjorn schuller shrikanth narayanan context-sensitive learning enhanced audiovisual emotion ieee transactions affective computing classiﬁcation vol. yoshua bengio aaron courville pascal vincent representation learning review perspectives ieee transactions pattern analysis machine intelligence vol. george trigeorgis fabien ringeval raymond brueckner erik marchi mihalis nicolaou stefanos zafeiriou adieu features? end-to-end speech emotion recognition using deep convolutional recurrent network ieee international conference acoustics speech signal processing ieee gary mckeown michel valstar roddy cowie maja pantic marc schroder semaine database annotated multimodal records emotionally colored conversations person limited agent ieee transactions affective computing vol. carlos busso murtaza bulut chi-chun kazemzadeh emily mower samuel jeannette chang sungbok shrikanth narayanan iemocap interactive emotional dyadic motion capture database language resources evaluation vol. jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal meeting corpus pre-announcement international workshop machine learning multimodal interaction. springer alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial networks arxiv preprint arxiv. sayan ghosh eugene laksana stefan scherer louisphilippe morency multi-label convolutional neural network approach cross-domain action unit detection affective computing intelligent interaction international conference ieee", "year": 2017}