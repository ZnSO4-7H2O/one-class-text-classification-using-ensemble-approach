{"title": "Long Short-Term Memory based Convolutional Recurrent Neural Networks for  Large Vocabulary Speech Recognition", "tag": ["cs.CL", "cs.NE"], "abstract": "Long short-term memory (LSTM) recurrent neural networks (RNNs) have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks (CNNs) have brought significant improvements to deep feed-forward neural networks (FFNNs), as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed state-of-the-art speech recognition performance.", "text": "stacked bidirectional lstm network trained connectionist temporal classiﬁcation phoneme recognition. subsequently lstm rnns successfully applied shown give state-of-the-art performance robust speech recognition task many large vocabulary speech recognition tasks literatures cnns applied frequency domain variability along time axis handled ﬁxed long time contextual window however original motivations rnns approach learn much context used prediction rather ﬁxed contextual window. therefore using recurrent connections rnns improve cnns natural choice. paper lstm based convolutional recurrent neural network architecture proposed combining cnns lstm rnns. proposed approach speech frame without adjacent context frames organized number local feature patches along frequency axis lstm network performed patch along time axis. words proposed network architecture convolutional operations handle variability along frequency axis recurrent operations handle variability along time axis. proposed network architecture considered introducing recurrent operations cnns introducing convolution operations lstm rnns. experiments conducted large vocabulary conversational telephone speech recognition task results shown proposed lstm crnns improve performance. modern feed-forward neural networks based hybrid acoustic modeling acoustic context windows frames typically used inputs. cyclic connections rnns exploit self-learnt amount temporal context makes principle better suited acoustic modeling. rnn-hmm hybrids studied almost twenty years shown give state-ofthe-art performance many tasks introducing lstm rnns recently. long short-term memory recurrent neural networks shown give state-of-the-art performance many speech recognition tasks able provide learned dynamically changing contextual window sequence history. hand convolutional neural networks brought signiﬁcant improvements deep feed-forward neural networks able better reduce spectral variation input signal. paper network architecture called convolutional recurrent neural network proposed combining lstm rnn. proposed crnns speech frame withadjacent context frames organized number local feature patches along frequency axis lstm network performed feature patch along time axis. train compare ffnns lstm rnns proposed lstm crnns various number conﬁgurations. experimental results show lstm crnns exceed stateof-the-art speech recognition performance. index terms speech recognition long short-term memory recurrent neural network convolutional neural networks recently hybrid context dependent deep neural network hidden markov model become dominant framework acoustic modeling speech recognition performance improvement conventional gaussian mixture model partially attributed powerful potential modeling complex correlations acoustic features. based hybrid cd-dnn-hmm framework many researches done various aspects sequence discriminative training network architectures speaker adaptive methods shown give signiﬁcant performance improvements. researches network architectures architectures attracted lots attentions convolutional neural networks long short-term memory based recurrent neural networks seminal work ossama proposed apply cnns frequency domain explicitly normalize speech spectral features achieve frequency invariance enforce locality features shown error rate reduction could obtained comparing fully-connected dnns phoneme recognition task. subsequently researchers applied idea large vocabulary speech recognition tasks hand graves proposed where activation function convolutional layers output vector feature patch. feature patch convolutional ﬁlter input nodes output nodes weights convolutional ﬁlter shared among feature patches. convolutional layer pooling layer added compute lower resolution representation convolutional layer activations sub-sampling. usually pooling function used pooling strategy literature variants pooling functions pooling stochastic pooling also evaluated. weight sharing pooling important concepts cnns helps reduce spectral variance input features. stacked convolution-pooling layers standard fully connected layers always added combine features different bands. descriptions lstm rnns cnns that lstm rnns provide dynamically changing contextual window weight sharing pooling cnns focus frequency shift invariance. motivated taking advantages paper network proposed attempts combine properties cnns lstm rnns. convolutional layer cnns viewed standard neural network layer operated local patches along frequency axis re-organized input feature. addition structure called network network proposed enhance model local patches within receptive ﬁeld replace ﬁlters conventional cnns micro network multilayer perceptron consisting multiple fully connected layers nonlinear activation functions. based understanding cnns order combining cnns rnns proposed network constructed replacing ﬁlters conventional cnns recurrent networks speciﬁcally lstm networks leads architecture illustrated figure proposed network called convolutional lstm lstm based convolutional recurrent neural network paper. where denotes weight matrices denotes bias vectors denotes hidden layer function. however rnns hard trained properly vanishing gradient exploding gradient problems described address problems long short-term memory proposed modern lstm architecture shown figure lstm recurrent hidden layer consists recurrently connected subnets known memory blocks. memory block contains self-connected memory cells three multiplicative gates control information. besides peephole weights connecting gates memory cell improve lstms ability learn precise timing counting internal states. equations lstm memory blocks follows where logistic sigmoid function respectively input gate forget gate output gate cell input activation cell state vectors size hidden vector diagonal weight matrices peephole connections. cell input cell output non-linear activation functions generally paper tanh. besides lstm projected network proposed separate linear projection layer lstm layer yield improved performance large vocabulary speech recognition task. capable modeling local frequency structures applying linear convolutional ﬁlters local feature patches representing limited bandwidth whole speech spectrum. order represent speech inputs frequency scale divided number local bands acoustic modeling always ﬁlter-bank features inputs. assuming whole input feature organized local patches patch frequency bands equations illustrated figure speech represented mel-scale log-ﬁlterbank coefﬁcients. speech frame withcontext frames organized local patches along frequency axis adjacent patches overlaps. patch represents limited bandwidth whole speech spectrum recurrent network performed patch along time axis. means that patch recurrent network receives previous outputs current inputs patch make decisions. equation recurrent network based crnns written proposed clstm like cnns inputs network organized number local feature patches. meanwhile lstm rnns input network need current feature frame without adjacent context frames. easy that convolution operations along frequency axis recurrent operations along time axis. frequency shift invariance embodies convolutional part dynamically changing contextual window embodies recurrent part. similar network architecture clstm multidimensional lstm comparing architectures found that clstm apply recurrent operation along adjacent frequency bands multi-dimensional lstm does. another related work introduced biological sequence data analyzing network architecture -dimensional convolutional layer followed lstm layer fully connected layer ﬁnal softmax layer understood stack convolutional layer lstm layer. train compare ffnns lstm rnns proposed lstm crnns large vocabulary speech recognition task hkust mandarin chinese conversational telephone speech recognition corpus collected transcribed hong kong university science technology contains -hour speech calls training calls development respectively. experiments around -hour speech randomly selected training used validate network training. original development corpus used test used training hyper-parameters determination processes. speech corpus represented frames mel-scale log-ﬁlterbank coefﬁcients along ﬁrstsecond-order temporal derivatives. ffnns concatenated features constructed concatenating current frame frames left right contexts. inputs lstm rnns lstm crnns current frames outputs converted pseudo likelihood state output probability hidden markov model framework. networks trained based alignments generated well-trained gmm-hmm systems senones cross-entropy objective function adopted. implement training multi-gpu devices. training truncated back-propagation though time learning algorithm adopted. sentence training split subsequences equal length adjacent subsequences overlapping frames computational efﬁciency operates parallel subsequences different utterances time. order train networks multi-gpu devices asynchronous stochastic gradient descent adopted. strategy introduced applied scale gradients. since information future frames helps making better decisions current frame also delayed output state labels frames. experiments learning rate training network decreased exponentially initial ﬁnal learning rates speciﬁc network stable convergence training. firstly ffnns lstm rnns various number conﬁgurations trained baseline results summarized table table necessary point that found that appropriate senones would bring performance improvements. thus paper senones senones leading slightly better experimental results table ×relu network hidden layers layer rectiﬁed linear units ×maxoutg network hidden layers layer maxout units group size denoted ×conv+×relu convolution-pooling layers relu layers. details convolutional layers units pooling size expected outperforms ffnns. table lstm network lstm layer lstm cells lstmp network lstmp layer lstm cells projected nodes. besides based research lstm based deep rnns constructed. lstm rnns yield better performance ffnns best performance among lstm rnns obtained using lstmp+×relu network lstmp layer followed relu layers. besides comparison proposed lstm crnns also trained networks construed simply stacking convolutional layers lstm layers unfortunately networks perform worse lstmp+×relu. since pooling importance concept cnns compared models without pooling proposed clstms shows discernible performance difference. however since models pooling layer smaller number parameters without pooling layer models following experiments pooling layers pooling size next explored performance function number lstm cells convolutional recurrent layers. table observe increase number lstm cells steadily decrease. able obtain comparable performance using lstm cells convolutional recurrent layer best baseline performance. literatures proposed lstmp make effective model parameters train acoustic models. similarly explored projection layer strategy clstm networks. results lstmp crnns shown table projection layer strategy seems provide performance improvements. however introducing projection layers total number parameters lstm cells reduced. speciﬁcally lstm cells clstmp network similar performance clstm smaller number parameters example clstm+pooling+×relu network clstmp+pooling+×relu network. increase lstm cells clstmp networks small changes total number parameters obvious reductions. literatures many studies shown performance improved using multiple lstm layers. besides multiple convolutional layers also improve cnns thus experiments conducted explore convolutional recurrent layers another recurrent layers lstm crnns. results different network structure conﬁgurations shown table table shows having convolutional recurrent layers also helps yields relative improvement performance baseline systems. noteworthy networks another lstmp layers clstm layer reduce relative improvement. summary experimental results show proposed clstm network exceed state-of-the-art performance. best performance obtained network constructed clstmp layer lstmp layer three relu layers. paper lstm based convolutional recurrent neural network architecture proposed acoustic modeling combining cnns lstm rnns constructed replacing ﬁlter conventional cnns recurrent ﬁlter speciﬁcally lstm based ﬁlter. proposed network considered introducing dynamically changing contextual window embedded lstm network conventional cnns introducing frequency shift invariance embedded convolutional structure lstm rnns. words proposed network contains convolutional operations along frequency axis recurrent operations along time axis. empirically evaluated proposed network ffnns lstm networks large vocabulary speech recognition task. experiments various conﬁgurations constructing deep networks compared. experimental results revealed that proposed lstm crnn improve performance delivering relative reduction signiﬁcantly comparing lstm networks shown give state-of-the-art performance tasks. however believe work preliminary study. future work includes training cltsm crnns using sequence discriminative training criterion experiments larger corpus. dahl deng acero context-dependent pretrained deep neural networks large-vocabulary speech recognition ieee trans. audio speech lang. processing vol. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing mag. vol. kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization interspeech geiger zhang weninger schuller rigoll robust speech recognition using long short-term memory recurrent neural networks hybrid acoustic modelling interspeech vinyals heigold senior mcdermott monga sequence discriminative distributed training long short-term memory recurrent neural networks interspeech chen network network iclr graves fern´andez schmidhuber multi-dimensional recurrent neural networks international conference artiﬁcial neural networks", "year": 2016}