{"title": "Irregular-Time Bayesian Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In many fields observations are performed irregularly along time, due to either measurement limitations or lack of a constant immanent rate. While discrete-time Markov models (as Dynamic Bayesian Networks) introduce either inefficient computation or an information loss to reasoning about such processes, continuous-time Markov models assume either a discrete state space (as Continuous-Time Bayesian Networks), or a flat continuous state space (as stochastic differential equations). To address these problems, we present a new modeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing Dynamic Bayesian Networks, allowing substantially more compact representations, and increasing the expressivity of the temporal dynamics. In addition, a globally optimal solution is guaranteed when learning temporal systems, provided that they are fully observed at the same irregularly spaced time-points, and a semiparametric subclass of ITBNs is introduced to allow further adaptation to the irregular nature of the available data.", "text": "many ﬁelds observations performed irregularly along time either measurement limitations lack constant immanent rate. discrete-time markov models introduce either ineﬃcient computation information loss reasoning processes continuous-time markov models assume either discrete state space continuous state space address problems present modeling class called irregular-time bayesian networks generalizing dynamic bayesian networks allowing substantially compact representations increasing expressivity temporal dynamics. addition globally optimal solution guaranteed learning temporal systems provided fully observed irregularly spaced time-points semiparametric subclass itbns introduced allow adaptation irregular nature available data. many ﬁelds observations performed irregularly along time either limitation measurement process lack constant immanent rate. thus interested general eﬃcient method model learn reason structured stochastic processes produce qualitative quantitative data irregularly spaced time-points. markov models used support tasks diﬀer represent time. irregular time settings kalman ﬁlters hidden markov models dynamic bayesian networks general require speciﬁcation constant time distance between consecutive observations. requirement leads computationally ineﬃcient learning inference modeled time granularity ﬁner time spent consecutive observations information loss opposite case. cases inference limited multiples modeled time granularity would otherwise require learning model. markov models represent time continuously hand handle well time irregularity suﬀer limitations. speciﬁcally continuous-time bayesian networks model state transitions thus assume discrete state space whereas stochastic diﬀerential equations assume continuous state space conditional independence among solving subprocesses typically sharing observation times among subprocesses. present modeling class designed better support probabilistic reasoning irregular time settings. speciﬁcally irregular-time bayesian networks generalize dynamic bayesian networks time slice span time interval accommodate interprocess delays time diﬀerences consecutive slices vary according available data inference needs. generalization precludes diﬀerent limitations appear continuous-time models discrete-time models. comparing latter allows substantial reduction model size leads corresponding increase computational eﬃciency introduces greater expressivity temporal dynamics property referred dynamicstochasticorder markovity. addition globally optimal solution guaranteed learning temporal systems provided time-points interest. fundamental notion stochastic processes generalized follows accommodate representation method. deﬁnition set. function random variable said irregular time. suggest speciﬁc probabilistic models sought time rather time-points interest need considered appropriately treated evidence clearly irregular-time stochastic processes generalize discrete-time stochastic processes given vector following proposition shows irregular time settings generalization imply model size reduction. varying coeﬃcient models regression models linear regressors coeﬃcients allowed change smoothly value variables called eﬀect modiﬁers formally using conditional independence notation varying coeﬃcient models typically take settings generalized linear models estimation methods developed varying coeﬃcient models mainly based local regression kernel method special case penalized splines generalize smoothing splines allow ﬂexible choices spline model basis functions model penalty. taking approach penalized splines model figure dbns itbns. transparent circles represent unobserved states shaded circles represent observed states opaque small circles represent knots. constant time diﬀerence between consecutive slices. itbn bottom time slices observation time-points knots changing dynamics consecutive slices delayed eﬀect nodes time slice. implied deﬁnition above time slices itbns needed points interest. relaxation comparing dbns signiﬁcant since reasoning even single time slice potentially hard. however choose itbn choice made dbn. fact itbns generalize dbns also implied facts itbns joint irregular-time processes much like dbns joint discrete-time processes irregular-time processes generalize discrete-time processes given suggests irregular-time models potentially compact discrete-time models potential increases number observations. compact irregular-time representation naturally supports computational savings memory time consumed learning inference tasks discussed next sections. however introduction irregulartime processes caters additional less trivial beneﬁts. first given memory consumption irregular-time model potentially support longer hindsight learning parameters discrete-time model contain less points. longer hindsight essential convergence parameter learning variables exist either model factorization data incompleteness. secondly similarly factored model incomplete data greater diﬀerence eﬀective model complexity discrete-time model irregular-time model. turn lead better learned model. third ability compute probability given evidence past step implies long-distance eﬀects directly expressed opposed case regular time models. consider irregular-time markov model property could referred stochasticdynamicorder markovity distances consecutive time-points depend data change time although property interpreted irregular-time analog higher-order markovity discrete-time models properties actually orthogonal. like regular p-order markov models irregular p-order markov models also consider recent points present point unlike former points irregularly distanced. fourth need choose single temporal granularity consequently need approximate real time stamps necessary discrete-time models. lastly irregular-time models support application ﬁltering demand otherwise computationally infeasible. already existing nodes. thus need consider hypothesis space scoring function unknown time oﬀsets. natural choice hypothesis space spanned time oﬀsets already appear data. similarly natural choice scoring function would prefers compact candidate structures; least amount invented nodes. additional structural entity appear regular-time models knots. general heuristics mentioned section support default selection knots marginal process given number knots process. number manifests complexity process methods exist selecting penalized splines. adaptation algorithms optimize standard criteria hypothesized numbers knots spline-based itbns thus straightforward. concreteness present following class conditional probability distributions itbns takes semiparametric settings varying coeﬃcient models. class enable optimal exploitation natural adaptation itbns available data. speciﬁcally greater model complexity manifested example number spline knots attributed times data available. stochastic processes. exist functions paramelearning parameters itbns require speciﬁc choice time granularity constant observation rate. additionally representation compactness supports longer hindsight learning potentially lead lower probabilistic model complexity turn introduce better learned model. speciﬁcally semiparametric approach supports optimal exploitation natural adaptation model available data. illustration typical settings learning itbns semiparametric cpds appears figure clearly complete temporal data regular notion implies irregularly complete data similar implication applies notion fully observed. however implication holds opposite direction. specifically process fully observed irregularly spaced time-points considered highly incomplete regular notion. rest section assume observation time vector given. slice known assumed diﬀerent learning structure bayesian network. applies learning structure fully observed itbns case ﬁxed time oﬀsets inside time slice deduced. however learning ﬁxed time oﬀsets irregularly incomplete data pose challenge oﬀsets arbitrary continuous learning cannot reduced ﬁnding right connectivity figure typical irregular-time gaussian linear bayesian settings. circles represent variables rectangles represent replications single-line arrows represent random relations double-line arrows represent ﬁxed relations. mean precision vectors parametrize varying coeﬃcients constitutes d-degree κ-knot penalized spline. constant precision parameter. vector constitutes eﬀects inside time-slice parents assume simplicity section remove assumption fully given observation times taken last section. thus inference itbns refer either basic tasks estimating unobserved states ﬁnding time observed slice. since itbns extend bayesian networks standard inference methods apply itbns well. however since deﬁnition itbns implies requirement constant time granularity time slices need constructed timepoints speciﬁc interest. speciﬁcally irregular-time generalized linear computation given parents randomly distanced time slice bounded number knots slices. speciﬁcally computational savings expected take place applying task smoothing states estimated given past future because inference involve many time slices. prediction future states estimated given present beneﬁcial itbns especially time-point interest future. ﬁltering task current state typically estimated online applied itbns lazy evaluation fashion data arrive special interest paid. similar application discrete-time models infeasible. given consecutive time slices interested know point time interval span third slice likely take place. since attribute probability model time diﬀerence nodes answer question would applying uniform discretization time given time slices estimation states time slice time interval. however case satisfactory solution requires discretization relatively length time interval solution quite costly alternative eﬃcient sought. suggest reduce problem stochastic root ﬁnding subtracting requested value node additional slice estimated value node. stochastic root ﬁnding algorithms based ﬁxed nonlinear root ﬁnding algorithms handle randomness function question using simulation. note function similarly refer estimated process mean variance quantile derivative case continuous-valued processes state probability case discrete-valued processes. since itbns less time slices dbns inference algorithms potentially yield better performance. example denote number hidden nodes time slice; transition figure itbn. glucose levels observed depend time since last meal started minus time absorption blood measurements meals occur irregularly spaced time-points package created learn infer generalized linear itbns using interface bugs successfully tested several irregularly sampled univariate processes. purpose section highlight general qualitative properties itbns including expressivity. demonstrate ﬁtting itbn depicted figure data glucose levels time plotted figure data contain records glucose levels collected subjects irregularly spaced time points reference starting time last meal. itbn learning scheme chooses appropriate knot locations given requested model complexity expressed number knots variable. figure illustrates estimated densities parameters control itbn glucose data discrete-time analog model. clear illustration data multi-modal parameters discrete-time models unimodal irregular-time models means greater likeliquantitative temporal bayesian networks augment standard dbns time nets allow representation qualitative time intervals occurrence events. problem learning models addressed. solutions stochastic diﬀerential equations suggested serve cpds dbns issue learning cpds addressed integration regular cpds. continuous-time particle filter online simulation-based inference method accounts hybrid state space continuousvalued subprocesses cannot factor others solve stochastic diﬀerential equations. recently integration bayesian graphical modeling semiparametric regression studied research interaction disciplines seems working mainly direction. integration eﬀorts yielded extensions statistical regression models whereas belief models known enjoyed semiparametric approach. longitudinal data analysis semiparametric approach used alternative markov models typically restricted exponential correlation structures irregular time settings. speciﬁcally time-varying coeﬃcient models time modify parameters control them rather homogeneously parametrized states. introduced irregular-time bayesian networks modeling class generalizes dbns better support inference learning irregular time settings. speciﬁcally time slice span time interval accommodate ﬁxed delays eﬀects parent processes child processes time diﬀerences consecutive slices vary according available data inference needs. several issues pointed here. although need choose speciﬁc time granularity lifted number knots still need sought. learning irregular-time non-gaussian cpds intractable standard approximation methods applied. addition however small adaptive number time slices leading fewer inference transitions guaranteeing globally optimal learning weaker conditions time slice still needs fully estimated. finally itbns allow long-distance eﬀects directly expressed representation method time random vector introduce several methodological questions follows. first whether time really believed jump observation next. interpretation attributes ontological rather epistemological semantics irregular-time representation. argue true semantics potential eﬀectiveness modeling following second whether sample path aﬀect model underlying process produced words whether sampling process aﬀect modeling sampled process. clearly sampling process already great aﬀect ﬁtting model sampled process however implicit aﬀect thus modeling aﬀect regarded self introspective reﬂective bayesian. however interested modeling extent serves inference purposes. third adding removing fully unobserved time slices prevented aﬀecting predictions potentially leading inconsistent inference; words modiﬁcations time model maintain inferred results. question masks belief true time model always pragmatic belief. note changing time granularity discrete-time models clearly similar eﬀects. fourth whether dbns generalize itbns time diﬀerence either regular nodes cpds. itbns indeed generalize dbns shown generalization goes direction. time diﬀerences cannot serve regular nodes dbns because attributed prior probability model belong time slice. time diﬀerences also cannot serve cpds dbns random. several directions paper continued. adapting variational methods itbns conducting appropriate evaluation. another studying application ideas presented undirected graphical models non-markovian models markov decision models. also believe ideas used generalize object-oriented relational ﬁrst-order extensions dbns. paper proposed method represent time potentially leading improvements respect inference computational complexity model probabilistic complexity dynamics’ expressivity adaptivity model data. best knowledge paper also ﬁrst suggest application semiparametric approach belief models rather extension statistical models graphical bayesian approach.", "year": 2012}