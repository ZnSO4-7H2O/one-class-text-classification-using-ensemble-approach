{"title": "Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image  Statistics", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We present a theoretical analysis of Gaussian-binary restricted Boltzmann machines (GRBMs) from the perspective of density models. The key aspect of this analysis is to show that GRBMs can be formulated as a constrained mixture of Gaussians, which gives a much better insight into the model's capabilities and limitations. We show that GRBMs are capable of learning meaningful features both in a two-dimensional blind source separation task and in modeling natural images. Further, we show that reported difficulties in training GRBMs are due to the failure of the training algorithm rather than the model itself. Based on our analysis we are able to propose several training recipes, which allowed successful and fast training in our experiments. Finally, we discuss the relationship of GRBMs to several modifications that have been proposed to improve the model.", "text": "present theoretical analysis gaussian-binary restricted boltzmann machines perspective density models. aspect analysis show grbms formulated constrained mixture gaussians gives much better insight model’s capabilities limitations. show grbms capable learning meaningful features two-dimensional blind source separation task modeling natural images. further show reported difﬁculties training grbms failure training algorithm rather model itself. based analysis able propose several training recipes allowed successful fast training experiments. finally discuss relationship grbms several modiﬁcations proposed improve model. studies probainspired hierarchical bilistic models used deep hierarchical architectures learn high order statistics data k¨oster hyv¨arinen]. widely used architecture deep believe network usually trained stacked restricted boltzmann machines bengio erhan al.]. since original formulation rbms assumes binary input values model needs modiﬁed order handle continuous input values. common replace binary input units linear units independent gaussian noise known gaussian-binary restricted boltzmann machines gaussian-bernoulli restricted boltzmann machines al.] ﬁrst proposed training grbms known difﬁcult several modiﬁcations proposed improve training. used sparse penalty training allowed learn meaningful features natural image patches. trained grbms natural images concluded difﬁculties mainly existence high-frequency noise images prevents model learning important structures. illustrated terms likelihood estimation grbms already outperformed simple mixture models. researchers focused improving model view generative models ranzato hinton courville roux al.le roux heess shotton winn]. suggested failure grbms training algorithm proposed modiﬁcations overcome difﬁculties encountered training grbms. studies shown failures grbms empirically knowledge analysis grbms apart preliminary work accounts reasons behind failures. paper extend work consider grbms perspective density models i.e. well model learns distribution data. show grbm regarded mixture gaussians already mentioned brieﬂy previous studies theis courville al.] gone unheeded. formulation makes clear grbms quite limited represent data. however argue fact necessarily prevent model learning statistical structure data. present successful training grbms two-dimensional blind source separation problem natural image patches results comparable independent component analysis based analysis propose several training recipes allowed successful fast training experiments. finally discuss relationship grbms mentioned modiﬁcations model. denotes energy function known statistical physics deﬁnes dependence temperature parameter usually ignored setting value play important role inference partition function normalizes probability distribution integrating possible values intractable cases. training using gradient descent partition function usually estimated using sampling methods. however even sampling remains difﬁcult dependencies variables. special case energy function contains terms combining different hidden different visible units. viewed graphical model lateral connections within visible hidden layer results bipartite graph. implies hidden units conditionally independent given visibles vice versa allows efﬁcient sampling. values visible hidden units usually assumed binary i.e. common extend continuous data grbm assumes continuous values visible units binary values hidden units. energy function wang al.] deﬁned models like bms. data ˜xl} observations assumed independent identically distributed goal optimal parameters maximize likelihood data i.e. maximize probability data generated model practical reasons often considers logarithm likelihood maximum likelihood since monotonic function. log-likelihood deﬁned practice ﬁnite i.i.d. samples used approximate expectations training data estimate ﬁrst term i.i.d. samples unknown model distribution estimate second term. since able compute conditional probabilities rbms efﬁciently gibbs sampling used generate samples. gibbs-sampling guarantees generate samples model distribution inﬁnite long. impossible ﬁnite number sampling steps used instead. procedure known contrastive divergence algorithm even shows good results cd-gradient approximation given perspective density estimation performance model assessed examining well model estimates data distribution. therefore take look model’s marginal probability distribution visible units formalized product experts mixture gaussians equation illustrates written product factors referred product experts expert consists isotropic gaussians variance ﬁrst gaussian placed visible bias second gaussian shifted relative ﬁrst times weight vector scaled factor depends every hidden unit leads expert mode corresponds state corresponding hidden unit. figure illustrate grbm-- viewed grbm-m denotes grbm visible hidden units. figure illustration grbm-- arrows indicate roles visible bias vector weight vectors. visualize experts grbm. blue circles indicate center gaussians expert. visualizes components grbm. denoted green circles four components results product experts. notice component sits right blue circle. since form similar mixture isotropic gaussians follow naming convention. gaussian distribution called component model distribution exactly conditional probability visible units given particular state hidden units. well mogs component mixing coefﬁcient marginal probability corresponding state also viewed prior probability picking corresponding component. total number components grbm exponential number hidden units figure example. locations components grbm independent case mogs. centered b+wh vector visible bias selected weight vectors. selection done corresponding entries taking value one. implies components exactly zero weights placed scaled independently. name ﬁrst order components anchor component respectively. higher order components determined choice anchor ﬁrst order components. indicates grbms constrained mogs isotropic components. general presumption analysis natural considered mixture independent super-gaussian sources analysis remaining dependencies. order able visualize grbms model natural image statistics mixture independent laplacian distributions example. calculated test data isotropic two-dimensional gaussian distribution true data distribution. results presented table conﬁrm conclusion grbms good terms illustrate grbms model statistical structure data looked probability distributions trained grbms. half recovered independent components figure example. illustrated plotting amari errors true unmixing matrix estimated model matrices i.e. unmixing matrix weight matrix grbm shown figure grbms estimated unmixing matrix quite well although grbms good ica. fact weight vectors grbms restricted orthogonal ica. remaining grbms weight vectors pointed opposite direction shown figure accordingly grbms failed estimate unmixing matrix terms density estimation solutions quality orthogonal ones. thus grbms able learn statistical structures data model data distribution pretty well. show components contribute model distribution randomly chose grbms calculated mixing coefﬁcients anchor ﬁrst order components shown table large mixing coefﬁcient anchor component indicates model likely reach hidden states none hidden units activated. general activated hidden units state less likely reached leads naturally sparse representation data. figure illustration log-probability densities. data plotted blue dots. grbm-- learned independent components. grbm-- learned independent component opposite directions. grbm--. isotropic three components. arrows indicate weight vectors grbm crosses denote means components. comparing contribution second order component insigniﬁcant probability distribution grbm four components almost three components. also seen figure comparing grbm-- dimensional three isotropic components denoted mog- although mog-- component fewer grbm-- probability distributions almost same. contrast images common underlying structure could used code efﬁciently pixel-wise representation. showed sparse coding efﬁcient coding scheme addition biological plausible model simple cells primary visual cortex. showed independent components provide comparable representation natural images. want test empirically whether grbms generate biological plausible results like sparse coding ica. used imlog natural image database randomly sampled grey scale image patches size pixels. data whitened using zero-phase component analysis afterwards divided training testing image patches. followed training recipes mentioned section since training grbm natural image patches trivial task. figure show learned weight vectors namely features ﬁlters regarded receptive ﬁelds hidden units. fairly similar ﬁlters learned similar experiment calculated anchor ﬁrst order mixing coefﬁcients shown table coefﬁcients much smaller compared anchor ﬁrst order coefﬁcients grbms dimensional case. however figure amari errors real unmixing matrix estimations grbms. extends lower upper quantile values data line median. whiskers extend show range reliable data points. outlier points marked base line amari errors real unmixing matrices random matrices provided. still signiﬁcantly large considering total number components case similar two-dimensional experiments activated hidden units state less likely reached leads naturally sparse representation. support statement plotted histogram number activated hidden units training sample shown figure also examined results grbms over-complete case i.e. grbm--. prominent difference ﬁlters compared complete case shown figure compare ﬁlters complete over-complete case estimated spatial frequency location orientation ﬁlters spatial frequency domains figure figure respectively. achieved ﬁtting gabor function form used figure spatial layout size ﬁlters described position size bars. denotes center position orientation ﬁtted gabor function within grid. thickness length propotional spatial-frequency bandwidth. training grbms reported difﬁcult al.]. based analysis able propose recipes improve success speed training grbms natural image patches. depend data distribution therefore improve training general. preprocessing data important especially model highly restricted like grbms. whitening common preprocessing step natural images. removes ﬁrst second order statistics data zero mean unit variance directions. allows trainfigure polar plot frequency tuning orientation learned ﬁlters. crosshairs describe selectivity ﬁlters given /-bandwidth spatial-frequency orientation components grbms isotropic gaussians model would several components modeling covariances. whitened data spherical covariance matrix distribution modelled already fairly well single component. components used model higher order statistics claim whitening also important preprocessing step grbms. initial choice model parameters important optimization process. using prior knowledge optimization problem help derive initialization improve speed success training. learning right scaling usually slow since weights biases determine position scaling components. ﬁnal stage training grbms whitened natural images ﬁrst components scaled extremely compared anchor component. therefore usually speed training process initialize parameters ﬁrst order scaling factors already small. considering equation able speciﬁc ﬁrst order scaling factor initializing hidden bias scaling determined ideally chosen close unknown ﬁnal scaling factors. practice choice showed good performance cases. learning rate hidden bias much smaller learning rate weights. choice hyper-parameters signiﬁcant impact speed success training grbms. successful training acceptable number updates learning rate needs sufﬁciently big. otherwise learning process becomes slow algorithm converges local optimum components placed data’s mean. learning rate chosen gradient easily diverge resulting number overﬂow weights. effect becomes even crucial model dimensionality increases grbm visible hidden units diverges already learning rate therefore propose restricting weight gradient column norms meaningful size prevent divergence. since know components placed region data need weight norm bigger twice maximal data norm. consequently natural bound also holds gradient practice chosen even smaller. allows choose learning rates even large models therefore enables fast stable training. practice restrict norm update matrix rather gradient matrix also restrict effects momentum term etc. since components placed data naturally restricted makes weight decay useless even counter productive since want weights grow certain norm. thus recommend weight decay regularization. momentum term adds percentage gradient current gradient leads robust behavior especially small batch-sizes. early stage training gradient usually varies large momentum therefore used prevent weights converging zero. late stage however also prevent convergence practice momentum reduced zero ﬁnal stage training recommended. using gradient approximation rbms usually trained described section quality approximation highly depends samples used estimating model expectation ideally i.i.d. gibbs sampling usually mixing rate means samples tend stay close previously presented samples. therefore steps gibbs sampling commonly leads biased approximation gradient. order increase mixing rate suggested persistent markov chain drawing samples model distribution referred persistent contrastive divergence proposed parallel tempering selects samples persistent markov chain different scaling energy function. particular analyzed algorithm training grbms proposed modiﬁed version experiments methods lead meaningful features comparable differ convergence speed shown figure used original algorithm together weight restrictions temperatures stepsize although better performance also much higher computational cost shown table analyzed problem view generative models argued failure grbms model’s focus predicting mean intensity pixel rather dependence pixels. model covariance matrices time proposed mean-covariance addition conventional hidden units figure evolution grbm whitened natural image dataset using temperatures. learning curves average trials. learning rate initial momentum term multiplied ﬁfth epoch gradient restricted hundredth maximal data norm weight decay used. group hidden units dedicated model covariance visible units. view density models mcrbms regarded improved grbms additional hidden units used depict covariances. conditional probabilities mcrbm given view generative models another explanation failure grbms provided although agree poor ability grbms modeling covariances argue deﬁciency binary nature hidden units. order overcome limitation developed spike-and-slab splits binary hidden unit binary spike variable real valued slab variable conditional probability visible units given shown grbms capable modeling natural image patches reported failures training procedure. showed also grbms could learn meaningful ﬁlters using sparse penalty. penalty changes objective function introduced hyper-parameter. paper provide theoretical analysis grbm showed product experts formulation rewritten constrained mixture gaussians. representation gives much better insight capabilities limitations model. two-dimensional blind source separation task problem demonstrate grbms model data distribution. experiments grbms capable learning meaningful features problem modeling natural images. cases results comparable ica. contrast features restricted orthogonal form over-complete representation. however success training grbms highly depends training setup proposed several recipes based theoretical analysis. generalized datasets directly applied like gradient restriction. experience maximizing imply good features vice versa. prior knowledge data distribution beneﬁcial modeling process. instance recipes based prior knowledge natural image statistics center peaked heavy tails. interesting topic integrate prior knowledge data distribution model rather starting modeling scratch. considering simplicity easiness training proposed recipe believe grbms provide possible modeling natural images. since grbms usually used ﬁrst layer deep belief networks successful training grbms therefore improve performance whole network.", "year": 2014}