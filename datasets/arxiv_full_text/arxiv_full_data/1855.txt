{"title": "Bayesian Paragraph Vectors", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Word2vec (Mikolov et al., 2013) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences. In this work, we propose a novel interpretation for neural-network-based paragraph vectors by developing an unsupervised generative model whose maximum likelihood solution corresponds to traditional paragraph vectors. This probabilistic formulation allows us to go beyond point estimates of parameters and to perform Bayesian posterior inference. We find that the entropy of paragraph vectors decreases with the length of documents, and that information about posterior uncertainty improves performance in supervised learning tasks such as sentiment analysis and paraphrase detection.", "text": "wordvec proven successful natural language processing capturing semantic relationships different words. built single-word embeddings paragraph vectors ﬁxed-length representations pieces text arbitrary lengths documents paragraphs sentences. work propose novel interpretation neural-network-based paragraph vectors developing unsupervised generative model whose maximum likelihood solution corresponds traditional paragraph vectors. probabilistic formulation allows beyond point estimates parameters perform bayesian posterior inference. entropy paragraph vectors decreases length documents information posterior uncertainty improves performance supervised learning tasks sentiment analysis paraphrase detection. paragraph vectors recent method embedding pieces natural language text ﬁxed-length real-valued vectors. extending wordvec framework paragraph vectors typically presented neural language models compute single vector representation paragraph. unlike word embeddings paragraph vectors shared across entire corpus instead local paragraph. interpreted latent variable expect higher uncertainty paragraphs short. recently barkan proposed probabilistic view wordvec motivated research combining wordvec priors inspired progress extend paragraph vectors probabilistic model. model speciﬁed modern inference tools like edward makes easy experiment different inference algorithms. experiments sec. conﬁrm intuition paragraph vectors higher posterior uncertainty paragraphs short show explicitly modeling uncertainty improves performance supervised prediction tasks. paragraph embeddings built word embeddings dimensionality reduction tools words large vocabulary dense vector representation. word embedding methods learn point estimate embedding vector barkan pointed skip-gram model negative sampling also known wordvec admits bayesian interpretation. bayesian skip-gram model allows uncertainty taken account principled lays basis proposed bayesian paragraph vector model. many tasks natural language processing require ﬁxed-length features text passages variable length sentences paragraphs documents generalizing embeddings single words several methods proposed dense vector representations paragraphs since paragraph embeddings local short pieces text expect high posterior uncertainty paragraphs short. work incorporate idea paragraph vectors bayesian skip-gram model order coherently infer uncertainty associated paragraph vectors. bayesian skip-gram model probabilistic interpretation wordvec left part figure shows generative process. word vocabulary model draws latent word embedding vector latent context embedding vector gaussian prior here embedding dimension hyperparameter. model constructs npairs labeled pairs words following twostep process. first proposal pair words drawn uniform distribution vocabulary. then model assigns proposal pair binary label bern vj)) sigmoid function. pairs label form so-called positive examples assumed correspond occurrences word context word somewhere corpus. so-called negative examples label correspond observation corpus. training model resort heuristics proposed create artiﬁcial evidence negative examples bayesian paragraph vectors direct extension bayesian skip-gram model. right part figure shows generative process. addition global word context embeddings model draws paragraph vector ndocs documents corpus. following mikolov context vector classify given pair words positive negative example. thus likelihood word pair document label znij collect evidence positive examples document forming pairs words here word class token runs tokens document runs small context window size exclude negative examples observed corpus. following mikolov construct artiﬁcial evidence negative pairs sampling noise distribution edward.models import bernoulli normal normal dtype=tf.float) scale=lam) normal dtype=tf.float) scale=lam) normal scale=phi) tf.nn.embedding_lookup tf.nn.embedding_lookup bernoulli axis=)) model global local latent variables. expect posterior global variables peaked therefore approximate global word embedding matrices point estimates. expect broader posterior distribution local paragraph vectors thus variational inference posterior fully factorized gaussian distribution. split inference stages. ﬁrst stage point estimate parameters. second stage perform paragraph vectors. ﬁrst stage goal train global variables stochastic gradient descent every minibatch contains single document ﬁxed negative examples ﬁrst maximize joint probability w.r.t paragraph vector local optimization noise free converges quickly constant learning rate. then perform single gradient step global variables gradient noisy minibatch sampling stochastic generation negative examples. reason decreasing learning rate used. finally reinitialize proceed next document. optimizing nested loop update step saves memory since need keep track document vectors time. second stage variational distribution paragraph vectors holding ﬁxed. black reparameterization gradients provided edward library. time generate negative examples update step avoid overﬁtting. stochastic optimization performed decreasing learning rate. also perform separate maximum posteriori estimate paragraph vectors serve baseline downstream classiﬁcation tasks. paragraph vectors often used input features supervised learning natural language processing section apply binary classiﬁcation tasks sentiment analysis paraphrase detection. posterior uncertainty decreases length paragraphs grows. also concatenating variational mean standard deviation features inferred improve classiﬁcation accuracy compared point estimates paragraph embeddings. imdb dataset sentiment analysis. contains movie reviews split training points labeled test points. positive negative labels balanced labeled subsets typical reviews consist several sentences. algorithm unsupervised inference algorithms described sec. using training data train logistic regression classiﬁer using paragraph vectors labeled training data only. frequent words vocabulary context window size embedding dimension hyperparameters prior figure entropy paragraph vectors function number words document. left movie reviews imdb dataset. right news clips dataset. left plot scale used horizontal axis account wide range document lengths. number negative examples document equal average number positive pairs documents. feature vectors classiﬁer point estimates paragraph vectors concatenation variational mean standard deviation table shows test accuracy inference methods. outperforms since takes account posterior uncertainty paragraph embeddings. fig. shows entropy paragraph vectors computed using posterior variance obtained document length grows entropy decreases makes intuitive sense since longer reviews speciﬁc. also test discriminative power microsoft research paraphrase corpus data point contains sentences extracted news sources goal predict whether paraphrases other. training contains sentence pairs paraphrases test contains pairs among paraphrases. hyperparameters sentiment analysis task except take words appearing vocabulary dataset much smaller. ﬁnding paragraph vectors train classiﬁer following kiros features constructed concatenating component-wise product absolute difference pair features classiﬁcation results table show outperforms map. relationship entropy document length shown fig. also similar imdb dataset. proposed bayesian paragraph vectors generative model paragraph embeddings. treated local latent variables paragraph vectors bayesian expected high uncertainty especially short documents. experiments conﬁrmed intuition showed knowledge posterior uncertainty improves performance downstream supervised tasks. addition experimented hamiltonian monte carlo inference preliminary results showed worse performance; plan investigate further. possible reason might ﬁxed negative examples document generating samples result overﬁtting noise. finally believe sophisticated models document embeddings would also beneﬁt bayesian treatment local variables. palangi deng shen chen song ward deep sentence embedding using long short-term memory networks analysis application information retrieval. taslp", "year": 2017}