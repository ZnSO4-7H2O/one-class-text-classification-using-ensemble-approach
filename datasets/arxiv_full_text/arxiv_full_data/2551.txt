{"title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex  Domains", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "High-dimensional observations and complex real-world dynamics present major challenges in reinforcement learning for both function approximation and exploration. We address both of these challenges with two complementary techniques: First, we develop a gradient-boosting style, non-parametric function approximator for learning on $Q$-function residuals. And second, we propose an exploration strategy inspired by the principles of state abstraction and information acquisition under uncertainty. We demonstrate the empirical effectiveness of these techniques, first, as a preliminary check, on two standard tasks (Blackjack and $n$-Chain), and then on two much larger and more realistic tasks with high-dimensional observation spaces. Specifically, we introduce two benchmarks built within the game Minecraft where the observations are pixel arrays of the agent's visual field. A combination of our two algorithmic techniques performs competitively on the standard reinforcement-learning tasks while consistently and substantially outperforming baselines on the two tasks with high-dimensional observation spaces. The new function approximator, exploration strategy, and evaluation benchmarks are each of independent interest in the pursuit of reinforcement-learning methods that scale to real-world domains.", "text": "paper propose techniques scaling reinforcement learning domains first present novel non-parametric function approximation scheme based gradient boosting method meant i.i.d. data adapted reinforcement learning. approach seems several merits. like deep-learning based methods succeed learning good function approximations builds powerful learning system. unlike deep-learning approaches however gradient boosting models amenable training prediction single laptop opposed reliant gpus. model naturally trained residuals recently shown helpful even deep learning literature furthermore boosting rich theoretical foundation supervised learning theory could plausibly extended reinforcement learning settings future work. second contribution give complementary exploration tactic inspired principle information acquisition uncertainty improves εuniform exploration incentivizing novel action applications. extremely simple design efﬁcient data demonstrate algorithm combining techniques called generalized exploratory q-learning backbone agent facing highly complex tasks visual observations. high-dimensional observations complex realworld dynamics present major challenges reinforcement learning function approximation exploration. address challenges complementary techniques first develop gradient-boosting style nonparametric function approximator learning q-function residuals. second propose exploration strategy inspired principles state abstraction information acquisition under uncertainty. demonstrate empirical effectiveness techniques ﬁrst preliminary check standard tasks much larger realistic tasks high-dimensional observation spaces. speciﬁcally introduce benchmarks built within game minecraft observations pixel arrays agent’s visual ﬁeld. combination algorithmic techniques performs competitively standard reinforcementlearning tasks consistently substantially outperforming baselines tasks highdimensional observation spaces. function approximator exploration strategy evaluation benchmarks independent interest pursuit reinforcement-learning methods scale real-world domains. introduction many real-world domains large state spaces complex dynamics requiring agent reason extremely high-dimensional observations. example case task figure agent must navigate highest location using visual input. developing efﬁcient effective algorithms environments critically important across variety domains. even relatively straightforward tasks like cause existing approaches ﬂounder; instance simple linear function approximation cannot scale visual input nonlinear function approximation deep qlearning tends relatively simple exploration strategies. chain much larger realistic tasks high-dimensional observation spaces. latter tasks built within game minecraft observations pixel arrays agent’s visual ﬁeld figure minecraft experiments made possible artiﬁcial intelligence experimentation platform describe detail below. standard tasks technique performs competitively large high-dimensional minecraft tasks method consistently quite substantially outperforms baseline. related work literature reinforcement learning vast focus related results speciﬁcally function approximation exploration strategies. general introduction function approximation important technique scaling reinforcement-learning methods complex domains. linear function approximators effective many problems complex non-linear models function approximation often demonstrate stronger performance many challenging domains unlike recent approaches based neural network architectures adopt gradient boosted regression trees nonparametric class regression models competitive performance supervised learning tasks. although similar ensemble approaches reinforcement learning applied previous work assume ﬁxed independently-trained agents rather boosting-style ensemble. work introduces interleaving boosting iterations data collection. iterative nature approximation resembles ofﬂine batch-style training fitted qiteration q-learner iteratively ﬁxed data. algorithm differs that iteration current q-function approximation guides subsequent data collection results used drive next update q-function. adaptive data collection strategy critical exploration problem central reinforcement learning experiments interleaved method signiﬁcantly outperforms fitted q-iteration. main algorithmic innovation exploration strategy reinforcement learning function approximation. approach similar work state abstraction learning agent constructs uses compact model world important difference algorithm uses compact model exploration only rather exploration policy learning. consequently model compression compromise expressivity learning algorithm still learn optimal behavior contrast typical state-abstraction approaches. number works propose exploration tactics function approximation. example train model predict future state current state proposed action similarity predicted state memory bank inform exploration decisions. another approach learn dynamics model either optimistic estimates uncertainty model provide exploration bonuses lastly exploration strategies theoretical guarantees domains certain metric structure structure must known priori unclear construct structure general. geql algorithm section present model-free reinforcementlearning algorithm generalized exploratory q-learning includes independent complementary components function-approximation scheme based gradient boosting exploration tactic based model compression. setting discounted model-free reinforcement-learning setting agent interacts environment goal accumulating high reward. time step agent observes state might represented high-dimensional vector visual input figure agent selects action whose execution modiﬁes state environment typically moving agent. finally agent receives real-valued reward process either repeats indeﬁnitely ﬁxed number actions. agent’s goal maximize long-term γt−rt process typically assumed deﬁne markov decision process meaning next state reached ﬁxed stochastic function depends previous state action executed; reward similarly depends simplicity assume development states fact fully observable. however many realistic settings agent observes might fully deﬁne underlying state; words environment might partially observable mdp. nevertheless practice often reasonable observations actually unobserved states especially observations rich informative. alternatively purpose could recent past window observations actions even entire past history. approach based q-learning standard technique. recall optimal function deﬁned state-action pair expected discounted reward trajectory begins state ﬁrst action taken subsequent actions chosen optimally general conditions function satisﬁes reward next state reached action executed state like function-approximation schemes constructs function approximates attempting observed data. shown algorithm build function iteratively series episodes using gradient-boosting approach. episode ﬁrst guide behavior agent mainly choosing actions seem beneﬁcial according occasionally taking exploration steps described shortly. agent observes series state-action-reward tuples next step observations improve speciﬁcally algorithm regressor residuals current approximation observed one-step look-ahead standard boosting methods. chosen minimize functions class weak regressors. typical example weak regressors might chosen regression trees highly ﬂexible effective efﬁciently trainable computed added function approximator thus also updating actions selected future episodes. function-approximation scheme several important advantages existing approaches. first using nonlinear base model regression trees agent able learn complex non-parametric approximation function crucial settings high-dimensional observations. time training procedure computationally efﬁcient updates occur batches trajectories discarded update. finally interleaving data collection using learned policy induced residual regression data collected intuitively improve quality informativeness dataset second novel component algorithm exploration technique borrows ideas state-abstraction literature. technique uses state-collapsing function maps state clusters relatively small. geql function trained clustering large dataset states instance using k-means algorithm associating state nearest cluster center. ideally optimalitypreserving compression would used characterized abstractions always easily computable. main idea technique keep track often action taken states clusters choose exploratory steps encourage choice actions taken less often current cluster. thus episode clusters action maintain count often action taken states cluster i.e. state table induces gibbs distribution actions deﬁned exp{−ρm temperature parameter controls uniformity distribution. concert current function approximator distribution deﬁne randomized choice actions step episode. specifically current state probability choose greedily selecting action maximizes probability take exploration step sampling call exploration strategy information acquisition uncertainty iauu. strategy shares beneﬁts work state abstraction without suffering drawbacks. main advantage state-collapsing function promotes applying infrequently-taken actions thereby encouraging agent visit regions state space. however contrast state-abstraction literature method remains robust misspeciﬁcation state-collapsing function since exploration compromise agent’s ability learn optimal behavior. finally iauu exploration adds minimal computational overhead space needed additional running time action remains iauu exploration tactic attached function approximation scheme also used q-learning tabular setting i.e. maintained explicitly table. case number modiﬁcations exploration strategy convergence properties q-learning retained. option modify exploration distribution vanishingly small amount uniform distribution. another option count exploration steps updating state-visitation table cases action taken state inﬁnitely often property sufﬁces ensure convergence q-learning tabular setting. experiments standard benchmarks section details evaluation standard reinforcement learning benchmarks blackjack n-chain. blackjack conducted experiments blackjack domain implemented exactly deﬁned section sutton barto experiment test hypothesis geql improve standard baselines even fairly simple domains without high-dimensional visual input. particular blackjack fully observable noise dimension short episodes small action space. algorithms parameters geql used depth regression trees weak regressors using python’s scikit-learn package test isolate effectiveness incremental boosting approach compared geql three baseline function approximators approximator used features experiments. batch-based regression approaches trained every episodes. similar geql depth regression trees batch approaches two. batchboost forest approximators used number total trees geql trained batch. episodes tree based approach gets total trees. batchboost forest trees retrained episodes worth data booster adds tree every episode. function approximators \u0001-uniform exploration iauu exploration. across experiments decayed that episode state clustering function used iauu learned individual task randomly sampling states random policy. results figure shows results trials episodes each. results indicate episodes minimal learning occurred linear approximator gradient-boosting approximator able learn play effectively batch approximators demonstrated learning though gradient-boosting approximator outperformed them. however exploration tactic negligible effect domain likely small action space short episodes brevity episode also explain linear approximator learned little many episodes. n-chain conducted experiments n-chain domain strens domain states numbered actions available applying forward action state advances agent state return action moves agent state applications forward action provide zero reward states except transitions state provides reward applications return action receive reward transitions. actions also stochastic probability opposite effect. task poses challenging exploration problem since agent must avoid greedily taking return action learn optimal behavior exploring last state chain. used typical setting task. algorithms tabular problem evaluated tabular methods. used tabular q-learner uniform exploration iauu exploration rmax algorithm model based algorithm strong sample complexity guarantees. results figure displays results trials. unsurprisingly rmax signiﬁcantly outperformed tabular q-learning strategies used since designed seek state-action applications helping quickly discover high reward chain. q-learning \u0001uniform exploration hand discover high reward last state chain exponentially small probability since agent favor greedy action must repeatedly explore advance chain. iauu exploration extremes; generalizes extremely large state spaces unlike rmax provides effective exploration \u0001-uniform. experiments visual domains section describes empirical evaluation highly challenging problems agent must reason images. used minecraft game platform provide environments tasks. minecraft blocks world player place blocks destroy blocks craft objects navigate terrain. size underlying state space grows exponentially number blocks allowed world typically order millions making planning learning infeasible tabular approaches. moreover night cycles weather cycles dramatically alter visual world well animals roam world underwater environments. size state space complex visual elements pose signiﬁcant challenges learning visual inputs. minecraft previously used planning research advocated general artiﬁcial intelligence experimentation platform experimental results enabled recently developed artiﬁcial intelligence experimentation platform designed experimentation within minecraft. provides ﬂexible easy-to-use minecraft engine allows full control agent including action execution perception well precise design minecraft world agent operates visual grid world task hand crafted grid world inspired classical reinforcement learning task visual hill climbing task built minecraft’s random world generator. running minecraft runs around frames second though agents execute around actions second. visual system rich world minecraft reinforcement learning agents require additional preprocessing observation. employ classical computer vision techniques preprocessing visual images minecraft game. input visual pipeline image minecraft agent’s view distracting elements removed. data ﬁve-minute random exploration minecraft world speciﬁcally every frame received game engine agent performs surf key-point detection stores points dataset. minutes agent performs k-means clustering space points reduce dimensionality key-point space interest. training done ofﬂine experiments conducted. visual system used algorithms. system trained separately task. task frame agent receives partitions frame grid. partition agent ﬁnds point similar key-point centers computes distance. distance used feature cell. ultimate feature vector concatenation partition’s key-point distances occupancy grid since image available vision system based agent’s ﬁrst-person perspective agent’s immediate surroundings partially occluded. immediate surroundings crucial decision making augment agent occupancy grid cells agent touching. occupancy grid contains corresponding cell adjacent agent contains block solid water otherwise. binary features along point distances vision system comprise state feature vector available agent. state-collapsing function state-collapsing function train another k-means instance maps given state object lower dimension. minecraft agent explore another minutes saving every frame. saved frame agent computes feature vector described concatenates occupancy grid agent’s surrounding cells storing vectors data set. minutes agent performs k-means data features reduce feature space lower dimension. training also done ofﬂine experiments conducted iauu algorithms state-collapsing function task. visual grid world ﬁrst task consider visual grid world task. here environment consists grid. agent always starts must navigate using movements north east south west. rotation state agent observes bitmap image agent’s view preprocessed using vision system augmented occupancy grid. reward function negation agent’s euclidean distance away goal. example agent distance goal agent receives reward. transitions deterministic. optimal policy achieve reward roughly directly proceeding goal. algorithms parameters blackjack used four exploration strategies regression problems solved episode linear gradient approximators using data recent episode. batchboost forest approximators completely retrained every episodes using recent episodes data. before depth number results figure shows results trials episodes episode seconds. results demonstrate gradient-boosting approximator dramatically faster learning task linear approximator booster also outperforms batch approximators similar statistical signiﬁcance apart batchboost baseline uniform exploration combination gradient boosting iauu exploration best average performance improvement gradient boosting \u0001-uniform statistically signiﬁcant. speed minecraft engine scaling experiments challenging. nevertheless results clearly show geql major improvement previous baselines challenging task. since know reward optimal policy case also checked reward policy learned booster episodes. found average rewards booster iauu uniform explorations respectively. note optimal policy gets reward close best baseline batchboost uniform exploration picks reward around task. hence conclude geql learns signiﬁcantly better policy baselines task. visual hill climbing second task call visual hill climbing especially difﬁcult. core variant non-convex optimization environment. visual grid world state preprocessed bitmap image agent’s view along occupancy grid. agent must climb highest hill navigating highly complex world includes animals lakes rivers trees caverns clouds pits variety different terrain types. example snapshot agent’s task pictured figure especially challenging exploration problem agent’s view restricted ﬁnite horizon agent direction time. large hills also partially occluded trees animals agent’s hills. scaling hills involves steps jumps larger agent make action meaning agent cannot scale hill even gets there. agent move forward direction facing turn left degrees turn right degrees jump units perform combination jumps units moves forward. agent receives reward increasing elevation units reward decreasing elevation units small reward proportional current height relative level. agent’s initial elevation agent reaches elevation receive additional reward. transitions deterministic state partially observable repeated application action particular observation result dramatically different future observations. results figure displays results trials episodes episode exactly seconds. again results indicate gradient booster able learn better policy approximators iauu exploration tactic helps. indeed gradient booster non-negligible learning occur; given complex domain extremely promising learn reasonable policy visualize performance agent better plot elevation proﬁle policy learned booster iauu exploration time figure notice agent barely increases elevation initial quarter episodes reliably reaching much better altitudes last quarter indicating identify succeeding task. figure visual hill climbing elevation proﬁle elevation throughout episode averaged episodes indicated range booster forest iauu exploration. elevations respect starting elevation. elevation effective agent acknowledgement would like thank katja hoffman matthew johnson david bignell hutton members platform development team without whose support work would possible. references david abel david ellis hershkowitz gabriel barth-maron stephen brawner kevin o’farrell james macglashan stefanie tellex. goal-based acinternational conference automated tion priors. planning scheduling herbert andreas tinne tuytelaars gool. speeded-up robust features computer vision image understanding ronen brafman moshe tennenholtz. r-max-a general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research lars buitinck gilles louppe mathieu blondel fabian pedregosa andreas mueller olivier grisel vlad niculae peter prettenhofer alexandre gramfort jaques grobler design machine learning software experiences scikit-learn project. arxiv preprint arxiv. thomas dietterich. hierarchical reinforcement learning maxq value function decomposition. journal artiﬁcial intelligence research damien ernst pierre geurts louis wehenkel. tree-based batch mode reinforcement learning. journal machine learning research discussion paper described novel approaches function approximation well exploration reinforcement learning evaluated challenging tasks implemented within minecraft. encouraging performance methods suggests several exciting avenues future research. empirically performance gradient boosting coupled favorable computational properties appears promising would interesting compare computationally-intensive deep-learning based approaches future work. extending existing theory gradient boosting supervised reinforcement learning also natural question. terms exploration iauu certainly improves \u0001-uniform still limited state-collapsing function used suboptimal least-frequent action also happens bad. remains challenging better alternatives tractable reinforcement learning real-time decisions high-dimensional observations. finally minecraft provides attractive framework develop visual versions standard tasks. show examples here opportunity translate tasks stress highlight various learning abilities agent arthur guez david silver peter dayan. efﬁcient bayes-adaptive reinforcement learning using sample-based search. advances neural information processing systems lihong thomas walsh michael littman. towards uniﬁed theory state abstraction mdps. international symposium artiﬁcial intelligence mathematics llew mason jonathan baxter peter bartlett marcus frean. functional gradient techniques combining hypotheses. advances large margin classiﬁers volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems", "year": 2016}