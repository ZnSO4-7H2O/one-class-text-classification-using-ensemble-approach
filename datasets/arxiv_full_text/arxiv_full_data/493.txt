{"title": "Optimizing Neural Network Hyperparameters with Gaussian Processes for  Dialog Act Classification", "tag": ["cs.CL", "cs.NE", "stat.ML"], "abstract": "Systems based on artificial neural networks (ANNs) have achieved state-of-the-art results in many natural language processing tasks. Although ANNs do not require manually engineered features, ANNs have many hyperparameters to be optimized. The choice of hyperparameters significantly impacts models' performances. However, the ANN hyperparameters are typically chosen by manual, grid, or random search, which either requires expert experiences or is computationally expensive. Recent approaches based on Bayesian optimization using Gaussian processes (GPs) is a more systematic way to automatically pinpoint optimal or near-optimal machine learning hyperparameters. Using a previously published ANN model yielding state-of-the-art results for dialog act classification, we demonstrate that optimizing hyperparameters using GP further improves the results, and reduces the computational time by a factor of 4 compared to a random search. Therefore it is a useful technique for tuning ANN models to yield the best performances for natural language processing tasks.", "text": "systems based artiﬁcial neural networks achieved state-of-the-art results many natural language processing tasks. although anns require manually engineered features anns many hyperparameters optimized. choice hyperparameters signiﬁcantly impacts models’ performances. however hyperparameters typically chosen manual grid random search either requires expert experiences computationally expensive. recent approaches based bayesian optimization using gaussian processes systematic automatically pinpoint optimal near-optimal machine learning hyperparameters. using previously published model yielding state-of-the-art results dialog classiﬁcation demonstrate optimizing hyperparameters using improves results reduces computational time factor compared random search. therefore useful technique tuning models yield best performances natural language processing tasks. artiﬁcial neural networks recently shown stateof-the-art results various tasks including language modeling named entity recognition text classiﬁcation question answering machine translation unlike popular non-annbased machine learning algorithms support vector machines conditional random ﬁelds anns automatically learn features useful tasks thereby requiring manually engineering features. learning rate mini-batch size) architecture anns commonly contain hyperparameters makes challenging optimize. therefore published ann-based work tasks rely basic heuristics manual random search sometimes even optimize hyperparameters. although report state-of-the-art results withoptimizing hyperparameters extensively argue results improved properly optimizing hyperparameters. despite this main reasons previous works thoroughly optimize hyperparameters represent signiﬁcant time investment. however optimize efﬁciently hyperparameters perform well within reasonable amount time shown paper. like anns machine learning algorithms also hyperparameters. widely used methods hyperparameter optimization machine learning algorithms manual grid search bergstra yoshua show random search good better grid search ﬁnding hyperparameters within small fraction computation time suggest random search natural baseline judging performance automatic approaches tuning hyperparameters learning algorithm. however above-mentioned methods tuning hyperparameters downsides. manual search requires human experts arbitrary rules thumb grid random searches computationally expensive recently systematic approach based bayesian optimization gaussian process shown effective automatically tuning hyperparameters machine learning algorithms latent dirichlet allocation svms convolutional neural networks deep belief networks well tuning hyperparameters features approach model’s performance hyperparameter combination modeled sample resulting tractable posterior distribution given previous experiments. therefore posterior distribution used optimal hyperparameter combination next based observation. fig. model. sequence words corresponding utterance transformed vector using consisting convolution layer pooling layer utterance classiﬁed two-layer feedforward network tanh softmax activation functions. hyperparmeters optimize circled ﬁlter size number ﬁlters dropout rate history sizes ﬁgure grey rows represent zero paddings. work demonstrate application gaussian process optimize hyperparameters task namely dialog classiﬁcation whose goal assign dialog utterance. model makes good candidate hyperparameter optimization since simple model architectural hyperparameters optimized architectural hyperparameters interpetable give insights task hand. using model show optimizing hyperparameters improves state-of-the-art results datasets reduces computational time factor compared random search. model dialog classiﬁcation introduced brieﬂy described section used optimize hyperparameters model presented section colon notation represents sequence vectors convolution operation consecutive word vectors starting word outputs scalar feature bias term. tanh perform convolution operations different ﬁlters denote resulting features whose dimensions comes distinct ﬁlter. repeating convolution operations window consecutive words utterance obtain c−h+. utterance representation computed pooling layer element-wise maximum c−h+. training dropout probability applied utterance representation sequential utterance classiﬁcation utterance representation given architecture utterance sequence length sequence input two-layer feedforward neural network classiﬁes utterance. hyperparameters history sizes used ﬁrst second layers respectively optimized using ﬁrst layer takes input ui−d+ outputs number classes classiﬁcation task i.e. number dialog acts. uses tanh activation function. similarly second layer takes input yi−d+ outputs softmax activation function. ﬁnal output represents probability distribution classes utterance element corresponds probability utterance belongs class. utterance assigned class highest probability. hyperparameter optimization using hyperparameter combinations considered function mapping hyperparameter combinations real-valued performance metric learning algorithm using given hyperparameter combination. interest lies efﬁciently ﬁnding hyperparameter combination yields near-optimal performance paper bayesian optimization hyperparameters using call search. grid search brute-forcefully evaluating deﬁned grid selecting best one. random search randomly selects evaluates performance process repeated satisfactory found. manual search expert tries hyperparameter combinations based prior experience settling good one. contrast methods mentioned above search chooses hyperparameter combination evaluate next exploiting previous evaluations. achieve this assume prior distribution function gaussian process allows construct probabilistic model using previous evaluations calculating posterior distribution tractable manner. model computed used choose optimal hyperparameter combination evaluate next. search describe distribution functions. deﬁned collection random variables ﬁnite number joint gaussian distribution. completely speciﬁed mean function covariance function also called kernel deﬁned case f-score test evaluated model using given hyperparameter combination -dimensional vector consisting ﬁlter size number ﬁlters dropout rate history sizes training inputs outputs test inputs outputs respectively. note known unknown. goal distribution given order select among hyperparameter combination likely yield highest f-score. choice kernel impacts predictions. investigate different kernels linear cubic absolute exponential e|x−x| squared exponential e−.|x−x| initialize search needs compute fscore certain number randomly chosen hyperparameter combinations investigate optimal number iterate following steps speciﬁed maximum number iterations reached. first hyperparameter combination test highest f-score predicted second compute actual f-score move training set. process outlined algorithm fig. performance search different kernels random search hyperparameter optimization dstc mrda swda. x-axis represents number hyperparameter combinations f-score computed y-axis shows best f-score achieved least hyperparameter combinations. data point averaged runs speciﬁed search strategy. evaluate random searches dialog classiﬁcation task using dialog state tracking challenge icsi meeting recorder dialog switchboard dialog datasets. dstc mrda swda respectively contain utterances labeled different dialog acts mrda). train/test splits provided along datasets validation chosen randomly except mrda speciﬁes validation set. given hyperparameter combination trained minimize negative log-likelihood assigning correct dialog acts utterances training using stochastic gradient descent adadelta update rule gradient descent step weight matrices bias vectors word vectors updated. regularization dropout applied pooling layer early stopping used validation patience epochs. initialize word vectors -dimensional word vectors pretrained wordvec google news dstc -dimensional word vectors pretrained glove twitter swda. search ﬁnds near-optimal hyperparameters faster random search. figure compares searches different kernels random search natural baseline hyperparameter optimization algorithms datasets f-score evaluated using hyperparameters found search converges near-optimal values signiﬁcantly faster random search regardless kernels used. example swda computing f-scores different hyperparameter combinations search reaches average whereas random search obtains random search requires computing f-scores reach search therefore reduces computational time factor signiﬁcant improvement considering computing average f-scores runs extra hyperparameter combinations takes days geforce titan gpu. squared exponential kernel converges slowly others. even though search kernel choice faster random search kernels result better performance others. best kernel choice depends choice dataset squared exponential kernel consistently converges slowly illustrated figure across fig. impact number initial random hyperparameter combinations search. x-axis represents number hyperparameter combinations f-score computed y-axis shows best f-score achieved least hyperparameter combinations. data point averaged runs speciﬁed search strategy. fig. finding near-optimal hyperparameter combinations swda. figure shows many times runs search strategy found hyperparameter combination among best performing hyperparameter combinations. figure shows many times runs search strategy found best hyperparameter combination evaluating hyperparameter combinations. number initial random points impacts performances. mentioned section search starts computing f-score certain number randomly chosen hyperparameter combinations. figure shows impact number three datasets. optimal number seems around average i.e. hyperparameter search space. number might fail optimal hyperparameter combinations performs signiﬁcantly worse mrda swda. conversely number high unnecessarily delays convergence. search often ﬁnds near-optimal hyperparameters quickly. evaluating f-scores hyperparameter combinations search ﬁnds best hyperparameter combinations almost time swda shown figure even frequently dstc mrda. computing hyperparameter combinations search ﬁnds best time random search stumbles upon less time. fig. heatmap f-scores swda number ﬁlters dropout rate vary. f-scores averaged possible values hyperparameters result f-scores lower ones figure fig. parallel coordinate plot hyperparameter combinations dstc hyperparameter combination -dimensional search space shown polyline vertices parallel axes represents hyperparameter. position vertex axis indicates value corresponding hyperparameter. color polyline reﬂects f-score obtained using hyperparameter combination corresponding polyline. simple heuristics optimal hyperparameters well. compared previous state-of-the-art results model optimized manually search found optimal hyperparameters improving f-score dtsc mrda swda respectively. hyperparameters optimized varying hyperparameter time keeping hyperparameters ﬁxed. figures demonstrate optimizing hyperparameter independently might result suboptimal choice hyperparameters. figure illustrates optimal choice hyperparameters impacted choice hyperparameters. example higher number ﬁlters works better smaller dropout probability conversely lower number ﬁlters yields better results used larger dropout probability. figure shows that instance ﬁrst ﬁxed number ﬁlters optimized dropout rate would found optimal dropout rate then ﬁxing dropout rate would determined optimal number ﬁlters thereby obtaining f-score best f-score paper addressed commonly encountered issue tuning hyperparameters. towards purpose explored strategy based automatically pinpoint optimal near-optimal hyperparameters. showed search requires times less computational time random search three datasets improves state-ofthe-art results efﬁciently ﬁnding optimal hyperparameter combinations. choices kernels number initial random points impact performance search ﬁndings show efﬁcient random search regardless choices. search used ordinal hyperparameter; therefore useful technique developing models tasks. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa natural language processing scratch journal machine learning research vol. guillaume lample miguel ballesteros sandeep subramanian kazuya kawakami chris dyer neural architectures named entity recognition arxiv preprint arxiv. matthieu labeau kevin l¨oser alexandre allauzen non-lexical neural architecture ﬁne-grained tagging proceedings conference empirical methods natural language processing lisbon portugal september association computational linguistics. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts recursive deep models semantic compositionality sentiment treebank proceedings conference empirical methods natural language processing citeseer vol. yoon convolutional neural networks sentence classiﬁcation proceedings conference empirical methods natural language processing. association computational linguistics phil blunsom edward grefenstette kalchbrenner convolutional neural network modelling sentences proceedings annual meeting association computational linguistics. proceedings annual meeting association computational linguistics sequential short-text classiﬁcation recurrent convolutional human language technologies neural networks conference north american chapter association computational linguistics naacl wang eric nyberg long short-term memory model answer sentence selection question answering proceedings annual meeting association computational linguistics international joint conference natural language processing beijing china july association computational linguistics. jasper snoek hugo larochelle ryan adams practical bayesian optimization machine learning algorithms advances neural information processing systems pereira burges bottou weinberger eds. curran associates inc. james bergstra r´emi bardenet yoshua bengio bal´azs k´egl algorithms hyper-parameter optimization advances neural information processing systems shawe-taylor zemel bartlett pereira weinberger eds. curran associates inc. franck dernoncourt kalyan veeramachaneni unamay oreilly gaussian process-based feature selection wavelet parameters predicting acute hypotensive episodes physiological signals ieee international symposium computer-based medical systems franck dernoncourt elias baedorf kassis mohammad mahdi ghassemi hyperparameter selection secondary analysis electronic health records springer international publishing andreas stolcke klaus ries noah coccaro elizabeth shriberg rebecca bates daniel jurafsky paul taylor rachel martin carol ess-dykema marie meteer dialogue modeling automatic tagging recognition conversational speech computational linguistics vol. seokhwan luis fernando d’haro rafael banchs jason williams matthew henderson fourth dialog state tracking challenge proceedings international workshop spoken dialogue systems adam janin baron jane edwards ellis david gelbart nelson morgan barbara peskin thilo pfau elizabeth shriberg andreas stolcke icsi meeting corpus acoustics speech signal processing proceedings.. ieee international conference ieee vol. jurafsky elizabeth shriberg debra biasca switchboard swbd-damsl shallow-discoursefunction annotation coders manual institute cognitive science technical report tomas mikolov ilya sutskever chen greg corrado jeff dean distributed representations words phrases compositionality advances neural information processing systems jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings empiricial methods natural language processing vol.", "year": 2016}