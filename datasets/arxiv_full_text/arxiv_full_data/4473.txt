{"title": "TDLeaf(lambda): Combining Temporal Difference Learning with Game-Tree  Search", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with minimax search. We present some experiments in both chess and backgammon which demonstrate its utility and provide comparisons with TD(lambda) and another less radical variant, TD-directed(lambda). In particular, our chess program, ``KnightCap,'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating to a 2100 rating in just 308 games. We discuss some of the reasons for this success and the relationship between our results and Tesauro's results in backgammon.", "text": "paper present tdleaf variation algorithm enables used conjunction minimax search. present experiments chess backgammon demonstrate utility provide comparisons another less radical variant tddirected. particular chess program knightcap used tdleaf learn evaluation function playing free internet chess server improved rating rating games. discuss reasons success relationship results tesauro’s results backgammon. developed sutton roots learning algorithm samuel’s checkers program elegant algorithm approximating expected long term future cost stochastic dynamical system function current state. mapping states future cost implemented parameterised function approximator neural network. parameters updated online state transition batch updates several state transitions. goal algorithm improve cost estimates number observed state transitions associated costs increases. many authors discussed peculiarities backgammon make particularly suitable temporal difference learning self-play principle among speed play td-gammon learnt several hundred thousand games self-play representation smoothness evaluation backgammon position reasonably smooth function position making easier good neural network approximation stochasticity backgammon random game forces least minimal amount exploration search space. td-gammon original form searched one-ply ahead feel list appended with shallow search good enough humans. possible reasons this; either gain searching deeper contrast ﬁnding representation chess othello allows small neural network order moves one-ply near human performance difﬁcult task games reliable tactical evaluation difﬁcult achieve without deep search. requires exponential increase number positions evaluated search depth increases. consequently computational cost evaluation function hence chess othello programs linear functions. next section look reinforcement learning falls) subsequent sections look detail introduce variations theme td-directed tdleaf. ﬁrst uses minimax search generate better training data second tdleaf used learn evaluation function deep minimax search. popularly known best understood learning techniques fall category supervised learning. category distinguished fact input upon system trained correct output known. allows measure error train system. temporal difference learning perhaps best known reinforcement learning algorithms. provides using scalar rewards existing supervised training techniques used tune function approximator. tesauro’s td-gammon example uses back propagation train neural network function approximator managing process calculating necessary error values. vector partial derivatives respect parameters. positive parameter controls learning rate would typically annealed towards zero course long series games. parameter controls extent temporal differences propagate backwards time. this compare equation reinforcement learning differs substantially supervised learning correct output known. hence direct measure error instead scalar reward given responses series inputs. consider agent reacting environment denote possible environment states. time proceeds agent performing actions discrete time steps time agent ﬁnds environment state available actions axt. agent chooses action takes environment state probability determined series actions environment perhaps goal achieved become impossible scalar reward number actions series awarded agent. rewards often discrete success failure otherwise. ease notation assume series actions ﬁxed length assume agent chooses actions according function current state expected reward state given large state spaces possible store value every instead might approximate using parameterised function class example linear function splines neural networks etc. assumed differentiable function parameters close least generates correct ordering moves. another signiﬁcant difference paradigms nature data used training. supervised learning ﬁxed whilst reinforcement learning states occur training dependent upon agent’s choice action resulting scores propagated back tree choosing stage move leads best position player move. ﬁgure example game tree minimax evaluation. reference ﬁgure note evaluation assigned root node evaluation leaf node principal variation; sequence moves taken root leaf side chooses best available move. td-directed variant utilises minimax search allowing play guided minimax still deﬁnes temporal differences differences evaluations successive board positions occurring game equation denote evaluation obtained state applying leaf nodes depth minimax search parameter vector good approximation expected reward achieve apply algorithm ˜jd. sequence positions game deﬁne temporal differences problem equation necessarily differentiable function values even everywhere differentiable. values ties minimax search i.e. best move available positions along principal variation means principal variation unique. thus evaluation assigned root node evaluation number leaf nodes. fortunately mild technical assumptions behaviour shown states almost differentiable function note also continuous function whenever continuous function implies even pairs undeﬁned multi-valued. thus still arbitrarily choose particular value happens land points. based observations modiﬁed algorithm take account minimax search instead working root positions algorithm applied leaf positions found minimax search root positions. call algorithm tdleaf. consider term contributing sums equations parameter vector adjusted move predicted reward time closer predicted reward time contrast adjusts parameter vector away move predicted reward time step closer ﬁnal reward time step values zero interpolate behaviours. note equivalent gradient descent error function tesauro replicated work backgammon report results insensitive value commonly value around recent work beale smith however suggests domain chess greater sensitivity value perhaps proﬁtable dynamically tune successive parameter updates according algorithm should time lead improved predictions expected reward provided actions independent parameter vector shown linear algorithm converges near-optimal parameter vector unfortunately guarantee non-linear depends argument’s sake assume action taken state leads predetermined state denote approximation found choose actions state picking action whose successor state minimizes opponent’s expected reward strategy used td-gammon. unfortunately games like othello chess difﬁcult accurately evaluate position looking move ahead. programs games employ form minimax search. minimax search builds tree position examining possible moves computer position possible moves opponent possible moves computer predetermined depth leaf nodes tree evaluated using heuristic evaluation function fig. full breadth -ply search tree illustrating minimax rule propagating values. leaf nodes given score evaluation function scores propagated back tree assigning opponent’s internal node minimum children’s values internal nodes maximum children’s values. principle variation sequence best moves either side starting root node illustrated dashed line ﬁgure. note score root node evaluation leaf node principal variation. ties siblings derivative score respect parameters section describe several experiments tdleaf td-directed algorithms used train weights linear evaluation function chess program called knightcap. main experiment took knightcap’s evaluation function material parameters zero. material parameters initialised standard computer values. parameter settings knightcap started free internet chess server establish rating games played without modifying evaluation function blitz rating turned tdleaf learning algorithm learning rate value chosen arbitrarily high enough ensure rapid modiﬁcation parameters. playing initial weight setting knightcap blitz rating games fics knightcap’s rating improved point gain. much slower improvement original experiment makes clear starting near good weights important fast convergence. self-play learning self-play extremely effective tdgammon signiﬁcant reason stochasticity backgammon. however chess deterministic game self-play deterministic algorithm tends result large number substantially similar games. problem games seen selfplay representative games played practice however knightcap’s self-play games non-zero material weights different kind games humans level would play. demonstrate learning self-play knightcap effective learning real opponents another experiment material parameters initialised zero again time knightcap learnt playing itself. after games played resulting version good version learnt fics game match weight values ﬁxed. fics trained version points self-play version’s along code lgammon land also provided weights neural network. weights used lgammon playing first internet backgammon server lgammon achieved rating ranged signiﬁcantly mean rating across players convenience refer weights fibs weights. using lgammon fibs weights directly compare searching two-ply searching oneply observed two-ply stronger pointsper-game signiﬁcant difference backgammon. further analysis showed positions move recommended two-ply search differed recommended one-ply search. subsequently decided investigate well td-directed tdleaf search deeply might perform. experiment sought determine whether either td-directed tdleaf could better weights standard test this suitably modiﬁed algorithms account stochasticity inherent game took copies fibs weights product standard training games. trained copy using td-directed using tdleaf. network trained games played unmodiﬁed fibs weights games sides searching two-ply match score recorded. results ﬂuctuated around parity fibs weights training) statistically signiﬁcant change performance observed. suggests solution found either near optimal two-ply play. introduced tdleaf variant training evaluation function used minimax search. extra requirement algorithm leaf-nodes principal variations stored throughgame. presented experiments chess evaluation function trained on-line play mixture human computer opponents. experiments show importance on-line sampling need start near good solution fast convergence. compared training using leaf nodes training using root nodes chess linear evaluation function search backgammon hidden layer neural-network evaluation function -ply search. found signiﬁcant improvement training leaf nodes chess attributed substantially different distribution leaf nodes compared root nodes. improvement observed backgammon suggests optimal network -ply search close optimal network -ply search. theoretical side recently shown converges linear evaluation functions interesting avenue investigation would determine whether tdleaf similar convergence properties.", "year": 1999}