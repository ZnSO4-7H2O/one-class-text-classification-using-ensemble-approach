{"title": "Learning Multi-grid Generative ConvNets by Minimal Contrastive  Divergence", "tag": ["stat.ML", "cs.CV"], "abstract": "This paper proposes a minimal contrastive divergence method for learning energy-based generative ConvNet models of images at multiple grids (or scales) simultaneously. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We call this learning method the multi-grid minimal contrastive divergence. We show that this method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD.", "text": "figure synthesized images multi-grids. left right grid grid grid. synthesized image grid obtained step langevin sampling initialized synthesized image previous coarser grid beginning grid. maximum likelihood learning energy-based generative convnet model follows analysis synthesis scheme sample synthesized examples current model usually markov chain monte carlo update model parameters based difference observed training examples synthesized examples. probability distribution energy function learned model likely multi-modal training data highly varied. mcmc difﬁculty traverse different modes take long time converge. simple popular modiﬁcation maximum likelihood learning contrastive divergence learning observed training example obtain corresponding synthesized example initializing ﬁnite-step mcmc observed example. method scaled large training datasets using mini-batch training. however synthesized examples fair samples current model thus resulting bias learned model parameters. modiﬁcation persistent mcmc still initialized observed example initial learning epoch. however subsequent learning epoch paper proposes minimal contrastive divergence method learning energy-based generative convnet models images multiple grids simultaneously. grid learn energy-based probabilistic model energy function deﬁned bottom-up convolutional neural network learning model requires generating synthesized examples model. within iteration learning algorithm observed training image generate synthesized images multiple grids initializing ﬁnite-step mcmc sampling minimal version training image. synthesized image subsequent grid obtained ﬁnite-step mcmc initialized synthesized image generated previous coarser grid. obtaining synthesized examples parameters models multiple grids updated separately simultaneously based differences synthesized observed examples. call learning method multi-grid minimal contrastive divergence. show method learn realistic energy-based generative convnet models outperforms original contrastive divergence persistent paper studies problem learning energy-based generative convnet models images. model form gibbs distribution energy function deﬁned bottom-up convolutional neural network derived commonly used discriminative convnet direct consequence bayes rule unlike discriminative convnet generative convnet endowed gift imagination generate images sampling probability distribution model. result generative convnet learned unsupervised setting without requiring class labels. learned model used prior model image proﬁnite-step mcmc initialized synthesized example previous epoch. running persistent chains make synthesized examples less biased observed examples although persistent chains still difﬁculty traversing different modes learned model. address challenges constraint ﬁnite budget mcmc propose minimal contrastive divergence method learn energy-based generative convnet models multiple scales grids. speciﬁcally training image obtain multi-grid versions repeated down-scaling. method learns separate generative convnet model grid. within iteration learning algorithm observed training image generate corresponding synthesized images multiple grids. speciﬁcally initialize ﬁnite-step mcmc sampling minimal version training image synthesized image grid serves initialize ﬁnite-step mcmc samples model subsequent ﬁner grid. fig. illustration sample images sequentially grids steps langevin dynamics grid. obtaining synthesized images multiple grids models multiple grids updated separately simultaneously based differences synthesized images observed training images different grids. call learning method multi-grid minimal contrastive divergence initializes ﬁnite-step mcmc minimal version training image. advantages proposed method follows. ﬁnite-step mcmc initialized version observed image instead original observed image. thus synthesized image much less biased observed image compared original learned models coarse grids expected smoother models grids. sampling models increasingly ﬁner grids sequentially like simulated annealing process ﬁnite budget mcmc likely produce synthesized images close fair samples learned models. images models multiple grids correspond scene different viewing distances. thus models multiple grids corse-to-ﬁne sampling processes physically natural. unlike original persistent learned models equipped ﬁxed budget mcmc generate synthesized images scratch need initialize mcmc sampling one-dimensional histogram version training images. show proposed method learn realistic models images. learned models used image processing image inpainting. learned feature maps used subsequent tasks classiﬁcation. contributions paper follows. propose minimal method learning multi-grid energy-based generative convnet models. show empirically proposed method outperforms original persistent well single grid learning. importantly show small budget mcmc capable generating varied realistic patterns. deep energy-based models received attention deserve recent literature reliance mcmc sampling. hope paper stimulate research designing efﬁcient mcmc algorithms learning deep energy-based models. method modiﬁcation training energy-based models. general data distribution observed training examples learned model distribution multi-modal data distribution even multi-modal model distribution. ﬁnite-step mcmc initialized data distribution explore local modes around training examples thus ﬁnite-step mcmc close model distribution. also case persistent contrast method initializes ﬁnite-step mcmc minimal version original image sampling model grid initialized image sampled model previous coarser grid. model distribution coarser grid expected smoother model distribution ﬁner grid coarse mcmc likely generate varied samples close fair samples learned models. result learned models obtained method closer maximum likelihood estimate original multi-grid monte carlo method originated statistical physics work perhaps ﬁrst apply multi-grid sampling learning deep energy-based models. motivation multi-grid monte carlo reducing scale resolution leads smoother less multi-modal distribution. difference method multi-grid mcmc statistical physics latter distribution lower resolution obtained distribution higher resolution. work models different grids learned training images different resolutions directly separately. besides energy-based generative convnet model another popular deep generative model generator network maps latent vector follows simple prior distribution image top-down convnet. model usually trained together assisting model inferential model variational auto-encoder discriminative model generative adversarial networks focus paper training deep energy-based models without resorting different class models need concerned mismatch different classes models. unlike generator model energy-based generative convnet model corresponds directly discriminative convnet classiﬁer case observe unlabeled examples model single distribution treat positive distribution negative distribution. prior probability random example comes posterior probability random example comes log) i.e. logistic regression. exploited fact estimate logistic regression order exponentially tilt devised iterative algorithm treats current tilted logistic regression tends focus examples boundaries paper shall pursue maximum likelihood learning matches average statistical properties model distribution data distribution. discriminative convnet must learned supervised setting generative convnet model learned unlabeled data maximum likelihood resulting learning sampling algorithm admits adversarial interpretation. learning sampling algorithm suppose observe training examples unknown data distribution pdata. maximum likelihood learning seeks maximize log-likelihood function denotes expectation respect identity fθ]. expectation equation analytically intractable approximated mcmc langevin dynamics iterates following step exponential tilting trivial case effect exponential tilting reduces mean shifting convnet piecewise linear rectiﬁcation piecewise linear piecewise mean shifted gaussian general local energy minima sat. equivalence discriminative convnet model corresponds classiﬁer following sense suppose categories addition background category convnets share common lower layers. prior probability category posterior probability classifying example category softmax multi-class classiﬁer model data bottom-up convnet expressive enough create modes encode highly varied patterns. still lack in-depth understanding energy landscape. learning step updates increase langevin sampling step tends relax ˜yi} decrease zero temperature limit langevin sampling gradient descent decreases resulting learning sampling algorithm generalized version herding also related wasserstein critic actor energy-based model i.e. model generator critic. developed discriminative method updating model learning classiﬁer logistic regression distinguish observed {yi} synthesized ˜yi} tilt current model according logistic regression discussed subsection also analysis synthesis scheme well adversarial scheme except analysis performed classiﬁer instead critic. original mcmc sampling take long time converge especially learned multi-modal often case pdata usually multi-modal. order learn large datasets afford small budget mcmc i.e. within learning iteration mcmc small number steps. meet challenge proposed contrastive divergence method within learning iteration initialize ﬁnite-step mcmc current learning batch obtain synthesized example ˜yi. parameters updated according learning gradient thus name contrastive divergence. mθpdata close second divergence small estimate close maximum likelihood minimizes ﬁrst divergence. however likely pdata learned multi-modal. expected smoother pdata i.e. pdata colder language indexes time steps langevin dynamics step size gaussian white noise. langevin dynamics relaxes energy region noise term provides randomness variability. energy gradient relaxation form reconstruction error auto-encoder metropolishastings step added correct ﬁnite step size also hamiltonian monte carlo sampling generative convnet parallel chains langevin dynamics according obtain synthesized examples ˜n}. monte carlo approximation learning sampling steps involve derivatives respect respectively. derivatives computed efﬁciently back-propagation computations derivatives share chain rule steps. learning algorithm thus form alternating back-propagation. mode shifting adversarial interpretations subsection explains intuition adversarial interpretation learning sampling algorithm. skipped ﬁrst reading. learning sampling algorithm interpreted density shifting mode shifting. sampling step langevin dynamics settles synthesized examples ˜yi} energy regions high density regions major modes i.e. modes i=eθ energies high probabilities tends low. learning step seeks change energy function changing order increase effect shifting energy high density regions synthesized examples ˜yi} toward observed examples {yi} shifting major modes energy function synthesized examples toward observed examples until observed examples reside major modes model. major modes diffused around observed examples learning step sharpen focus observed examples. mode shifting interpretation related hopﬁeld network attractor network langevin dynamics serving attractor dynamics. important ensure modes encoding observed examples major modes high probabilities. energy landscape numerous major modes occupied observed examples modes imagine examples considered similar observed examples. even though maximum likelihood learning matches average statistical properties learning related score matching estimator auto-encoder showed tends auto-encoder learning step shifts local modes synthesized examples toward observed examples observed examples reside modes modes satisfy auto-encoder potential weakness explores local modes around observed examples local modes major modes high probabilities. ﬁtting mixture model correct component distributions incorrect component probabilities. persistent version initialize mcmc observed beginning learning epoch mcmc initialized synthesized obtained previous epoch. persistent still face challenge traversing exploring different local energy minima. original initializes mcmc sampling data distribution pdata. modify initializing mcmc sampling given distribution hope closer mθpdata. learning gradient approximately follows gradient ﬁnite-step mcmc given initial distribution resulting samples synthesized examples approximate expectation approximation made accurate using annealed importance sampling following idea simulated annealing smoother distribution unlike persistent ﬁnite-step mcmc non-persistent sometimes also referred cold start mcmc initialized given within learning iteration instead examples synthesized previous learning epoch. cold start version easier implement mini-batch learning. taking role whereas learning generator based mθqα modiﬁes accomplished minqα klqα) i.e. accumulates mcmc transitions close stationary distribution paper shall consider recruiting generator network need worry mismatch generator model energy-based model. words instead relying learned approximate direct sampler endeavor develop small budget mcmc sampling. propose minimal contrastive divergence method learning sampling generative convnet models multiple grids. image multigrid versions minimal version divide image grid squared blocks pixels. reduce block single pixel averaging intensity values pixels. down-scaling operation maps conversely also deﬁne upscaling operation expanding pixel block constant intensity obtain up-scaled version up-scaled identical original high resolution details lost. mapping linear projection onto orthogonal basis vectors corresponds block. up-scaling operation pseudo-inverse linear mapping. general even need integer existence linear mapping pseudo-inverse. sequence represents images scene different viewing distances. within learning iteration training image current learning batch initialize ﬁnite-step mcmc image sample current running steps langevin dynamics up-scaled version sampled previous coaser grid. that update model parameters based difference synthesized observed sampling scheme sampled directly one-dimensional histogram. expected smoother thus sampling scheme similar simulated annealing ﬁnite-step mcmc sequence probability distributions increasingly multi-modal hope reaching exploring major modes model distributions. learning process shifts major modes towards observed examples sharpening modes along order memorize observed examples major modes model distributions. model grid up-scaled version model random example grid up-scaled version distribution markov transition kernel l-step langevin dynamics samples learning gradient multi-grid minimal method grid approximately follows gradient difference divergences smoother distribution close creating details current resolution. original initializing mcmc data sampling multi-modal distribution initializing presumably even multi-modal distribution data expect resulting distribution close target theoretical analysis properties learning variants generally difﬁcult. paper study multi-grid empirically. carry qualitative quantitative experiments evaluate method respect several baseline methods evaluate advantage minimal multi-grid sampling. ﬁrst baseline single-grid starting image directly up-scale sample image using single generative convnet. baselines persistent initialize sampling observed images. training images resized since models three grids images different scales design speciﬁc convnet structure grid grid -layer network stride ﬁlters ﬁrst layer stride ﬁlters next layers; grid -layer network stride ﬁlters ﬁrst layer stride ﬁlters next three layers; grid -layer network stride ﬁlters ﬁrst layer stride ﬁlters second layer stride ﬁlters third layer. numbers channels grid grid grid. fully-connected layer channel output added every grid value scoring function batch normalization relu activations applied every convolution. iteration steps langevin dynamics grid step size networks trained simultaneously mini-batches size initial learning rate learning rate decayed logarithmically every iterations. persistent single-grid follow setting multi-grid except persistent single-grid langevin steps maintain mcmc budget multi-grid network structure grid baseline methods. synthesis diagnosis learn multi-grid models celeba places datasets. celeba dataset images cropped center randomly sample images training. places dataset learn models images single place category. fig. shows synthesized images celeba dataset forest road category places dataset. also show synthesized images generated models learned dcgan single-grid persistent cannot synthesize realistic images figure synthesized images models learned celeba forest road category places datasets. left right observed images images synthesized dcgan single-grid multi-grid persistent cannot synthesize realistic images results shown. show synthesis results. compared single-grid images generated multi-grid realistic. results multi-grid models comparable results dcgan. fig. shows synthesized images models learned another categories places dataset multi-grid number training images rock categories. monitor convergence multi-grid show l-norm gradients iterations learned celeba dataset fig. suggests learning stable. monitor model ﬁtting synthesis calculate values scoring function training. table shows results iterations training celeba dataset. randomly sample images included training dataset celeba testing images randomly sampled places negative examples. compared negative images scores training testing images higher close other. scores training synthesized images also close indicating synthesized images close fair samples. check diversity langevin dynamics sampling synthesize images initializing langevin dynamics image. shown fig. steps langevin dynamics sampled images image different other. learning feature maps classiﬁcation svhn dataset consists color images house numbers collected google street view. training consists images testing images. training learn models learned models generate images shown fig. evaluate feature maps learned multi-grid perform classiﬁcation experiment. procedure similar outlined multi-grid feature extractor. ﬁrst train models svhn training unsupervised way. learn classiﬁer labeled data based learned feature maps. speciﬁcally extract layer feature maps three grids train two-layer classiﬁcation feature maps. ﬁrst layer stride convolutional layer channels operated separately three feature maps. outputs three feature maps concatenated form -dimensional vector. fully-connected layer added vector. train classiﬁer using labeled examples randomly sampled training uniformly class distributed. shown table method achieves test error rate labeled images. within setting method outperforms persistent single-grid test method image inpainting. task learn conditional distribution models consists pixels masked consists pixels masked. training stage randomly place mask training image assume observed training. follow learning sampling algorithm algorithm except sampling step langevin step masked part image updated unmasked part remains ﬁxed observed. generalization pseudo-likelihood estimation corresponds case consists pixel. also considered form associative memory learning fully observed training images inpaint masked testing images masked parts observed. energy-based generative convnet corresponds directly convnet classiﬁer fundamental importance study models purpose unsupervised learning. work seeks facilitate learning models developing small budget mcmc initialized simple distribution sampling learned models. particular learn multi-stage models corresponding multi-stage reductions observed examples multi-stage models guide multi-stage mcmc sampling. multi-stage data reductions follow multigrid scheme although latter preserves convolutional structures networks.", "year": 2017}