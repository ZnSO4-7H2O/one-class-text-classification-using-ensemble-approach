{"title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation  Learning", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.", "text": "ence sentence next. cases example sentence text interpretable context relevant topic discussion. properties depend understanding local context includes relatively knowledge state world speciﬁc meanings previous sentences text. thus model successfully trained recognize discourse coherence must able understand meanings sentences well relate pieces knowledge world. hobbs presents formal treatment phenomenon. argues discourse interpreted coherent adjacent sentences must related kinds coherence relations. example sentence might followed another elaborates parallels contrasts treatment adequate cover full complexity language understanding allows hobbs show identifying relations depends upon sentence understanding coreference resolution commonsense reasoning. work presents novel objective function unsupervised training neural network sentence encoders. exploits signals paragraph-level discourse coherence train models understand text. objective purely discriminative allowing train models many times faster possible prior methods yields models perform well extrinsic evaluations. modern artiﬁcial neural network approaches natural language understanding tasks like translation summarization classiﬁcation depend crucially subsystems called sentence encoders construct distributed representations sentences. encoders typically implemented convolutional recursive recurrent neural networks operating sentence’s words characters early successes sentence encoder-based models tasks ample training data possible train encoders fully-supervised end-to-end setting. however recent work shown success using unsupervised pretraining unlabeled data improve performance methods extend lower-resource settings paper presents methods unsupervised pretraining train sentence encoders recognize discourse coherence. reading text human readers expectation coherploiting discourse coherence information kind train sentence encoders rely generative objectives require models compute likelihood word sentence training time. setting single epoch training typical text corpus take weeks making research difﬁcult making nearly impossible scale methods full volume available unlabeled english text. work propose alternative objectives exploit much coherence information greatly reduced cost. particular propose three fast coherencebased pretraining tasks show used together effectively multitask training evaluate models trained setting training tasks standard text classiﬁcation tasks. approach makes possible learn extract broadly useful sentence representations hours. work inspired directly skip thought approach kiros introduces paragraph-level discourse information unsupervised pretraining sentence encoders. since work three papers presented improvements method improved methods based techniques goals similar ours three involve models explicitly generate full sentences training time considerable computational cost. closely related work logeswaran present model learns order sentences paragraph. focus learning assess coherence show positive results measuring sentence similarity using trained encoder. alternately fastsent model hill designed work dramatically quickly systems like skip thought service goal standard sentence encoder replaced low-capacity cbow model. method well existing semantic textual similarity benchmarks insensitivity order places upper bound performance intensive extrinsic language understanding tasks. looking beyond work unsupervised pretraining hovy jurafsky representation learning systems directly model problem sentence order recovery focus primarily intrinsic evaluation rather transfer. wang train sentence representations context language modeling. addition treat discourse relations sentences latent variables show yields improvements language modeling extension document-context model outside context representation learning good deal work discourse coherence particular tasks sentence ordering coherence scoring. barzilay lapata provide thorough coverage work. work propose three objective functions paragraphs extracted unlabeled text. captures different aspect discourse coherence together three used train single encoder extract broadly useful sentence representations. binary ordering sentences many coherence relations inherent direction. example elaboration generally elaboration thus able identify coherence relations implies ability recover original order sentences. ﬁrst task call order consists taking pairs adjacent sentences text data switching order probability training model decide whether switched. table provides examples context really. ideas plans. never even caught sight them. candidate successors there’s nothing compares that. mister edwards drank did. anyway god’s getting revenge now. offered somewhere sleep. task along kind coherence relation assume involved. noted since relations unordered always possible recover original order based discourse coherence alone next sentence many coherence relations transitive nature sentences paragraph exhibit coherence. however adjacent sentences generally coherent distant ones. leads formulate next task given ﬁrst three sentences paragraph candidate sentences later paragraph model must decide candidate immediately follows initial three source text. table presents example task candidates coherent third sentence paragraph elaboration takes precedence progression conjunction prediction finally information coherence relation sentences sometimes apparent text case whenever second sentence starts conjunction phrase. form conjunction objective create list conjunction phrases group nine categories extract source text pairs sentences second starts listed conjunctions give system pair withphrase train recover conjunction category. table provides examples. section introduce training data methods present qualitative results comparisons among three objectives close quantitative comparisons related work. experimental setup train models combination data bookcorpus gutenberg project wikipedia. sentence word tokenization lower-casing identify paragraphs longer sentences extract next example each well pairs sentences order conjunction tasks. gives examples order conjunction next. despite recently become standard dataset unsupervised learning bookcorpus exhibit sufﬁciently rich discourse structure allow model fully succeed—in particular conjunction categories severely under-represented. this choose train models text three sources. precludes strict apples-toapples comparison published results goal extrinsic evaluation simply show method makes possible learn useful representations quickly rather demonstrate superiority learning technique given ﬁxed data unlimited time. consider three sentence encoding models simple sum-of-words encoding recurrent neural network bidirectional three fasttext pre-trained word embeddings apply highway transformation encoders trained jointly three bilinear classiﬁers three objectives perform stochastic gradient descent adagrad subsampling conjunction next factor respectively setting bigru model takes hours examples bookcorpus dataset least once. ease comparison train three models exactly hours. intrinsic qualitative evaluation table compares performance different training regimes along axes encoder architecture whether train model task joint model. expected complex bidirectional architecture required capture appropriate sentence properties although cbow still manages beat simple likely virtue substantially faster speed correspondingly greater number training epochs. joint training appear effective order next tasks beneﬁt information provided conjunction. early experiments external evaluation also show joint bigru model substantially outperforms single model. table supplementary material show nearest neighbors trained bigru’s representation space random seed sentences. select neighbors among held-out sentences. encoder appears especially sensitive high-level syntactic structure. table text classiﬁcation results including training time. +embed lines combine sentence encoder output pretrained word embeddings sentence. +unigram lines using embeddings learned target task without pretraining. +feats varies task. references hill kiros logeswaran overall system performs comparably sdae skip thought approaches drastically shorter training time. system also compares favorably similar discourseinspired method logeswaran achieving similar results msrp sixth training time. work introduce three training objectives unsupervised sentence representation learning inspired notion discourse coherence train sentence representation system competitive time times shorter comparable methods obtaining comparable results external evaluations tasks. hope tasks introduce paper prompt research discourse understanding neural networks well strategies unsupervised learning make possible unlabeled data train reﬁne broader range models language understanding tasks. kyunghyun bart merrienboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. emnlp doha qatar. bill dolan chris quirk chris brockett. unsupervised construction large paraphrase corpora exploiting massively parallel news sources. coling geneva switzerland. yunchen ricardo henao chunyuan xiaodong lawrence carin. unsupervised learning sentence representations using convolutional neural networks. corr abs/.. ryan kiros yukun ruslan salakhutdinov richard zemel raquel urtasun antonio torralba sanja fidler. skip-thought vectors. nips montreal quebec canada. tomas mikolov martin karaﬁ´at luk´as burget cernock´y sanjeev khudanpur. recurrent neural network based language model. interspeech makuhari chiba japan. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts recursive deep models semantic compositionality sentiment emnlp seattle washington treebank. usa. zichao yang diyi yang chris dyer xiaodong alexander smola eduard hovy. hierarchical attention networks document classinaacl diego california ﬁcation. usa. yukun ryan kiros richard zemel ruslan salakhutdinov raquel urtasun antonio torralba sanja fidler. aligning books movies towards story-like visual explanations watching movies reading books. iccv santiago chile. album features guest appearances kendrick lamar schoolboy chainz drake big. production original live rock blues jazz punk music composed arranged steve diane gioia. real drivers game gilles richard burns carlos sainz philippe piero tommi. rappers include young jeezy wayne freddie gibbs emilio rojas german rapper romeo miller. cache manifests also relative paths even absolute urls shown below. locales used translate different languages variations text replaced reference. nouns inﬂected possessive case preﬁx added. ratios commonly used compare banks assets liabilities banks constantly valued market values. home massachusetts controlled private society organized purpose board ﬁfteen trustees charge. group trusted servants family assigned search eastern area island area. city divided administrative wards grouped boroughs. wards elects councillor. served ambassador united nations commission status women. molly deemed potential eliminating jaclyn despite stellar portfolio. result elway connection erickson spent time year learning offense jack. result severe response czarist authorities insurrection leave poland. another unwelcome note struck needlessly aggressive board museum already mentioned. daniel pipes says primarily protocols elders zion whites spread charges wrote howard’s ﬁction kind wild west lands unbridled fantasy. said chancellor elaborately fought european solution refugee crisis sight. robert writing york post states that mellie show character personal name andes popular among illyrians southern pannonia much northern dalmatia identiﬁed chicano ﬁrst settlement people north america southern migration range stretches across northern western north america well across europe name dauphin river actually refers closely tied communities; members dauphin river ﬁrst nation. match lasted minute seconds second quickest bout division. results qualiﬁed grand prix ﬁnal placed overall. judge stated prosecution march charges. november reached ﬁnal ruhr open lost murphy.", "year": 2017}