{"title": "Neural Networks for Joint Sentence Classification in Medical Paper  Abstracts", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.", "text": "existing models based artiﬁcial neural networks sentence classiﬁcation often incorporate context sentences appear classify sentences individually. however traditional sentence classiﬁcation approaches shown greatly beneﬁt jointly classifying subsequent sentences conditional random ﬁelds. work present architecture combines effectiveness typical models classify sentences isolation strength structured prediction. model achieves state-of-theart results different datasets sequential sentence classiﬁcation medical abstracts. million scholarly articles published number articles published every year keeps increasing approximately half biomedical papers. repository human knowledge abounds useful information unlock promising research directions provide conclusive evidence phenomena become increasingly difﬁcult take advantage available information sheer amount. therefore technology assist user quickly locate information interest highly desired reduce time required locate relevant information. criteria interest. process easier abstracts structured i.e. text abstract divided semantic headings objective method result conclusion. however signiﬁcant portion published paper abstracts unstructured makes difﬁcult quickly access information interest. therefore classifying sentence abstract appropriate heading signiﬁcantly reduce time locate desired information. call sequential sentence classiﬁcation task order distinguish general text classiﬁcation sentence classiﬁcation context. besides aiding humans task also useful automatic text summarization information extraction information retrieval. paper present system based anns sequential sentence classiﬁcation task. model makes token character embeddings classifying sentences sequence optimization layer learned jointly components model. evaluate model nictapiboso dataset well dataset compiled based pubmed database. existing systems sequential sentence classiﬁcation mostly based naive bayes support vector machines hidden markov models conditional random ﬁelds often require numerous hand-engineered features based lexical based artiﬁcial neural networks require manual features trained automatically learn features based word well character embeddings. moreover ann-based models achieved state-of-the-art results various tasks. short-text classiﬁcation many models word embeddings recent works based character embeddings santos gatti word character embeddings. however existing works using anns short-text classiﬁcation context. contrast sequential sentence classiﬁcation sentence text classiﬁed taking account context. context utilized classiﬁcation could surrounding sentences possibly whole text. exception recent work dialog classiﬁcation utterance dialog classiﬁed dialog preceding utterances used system designed real-time applications mind. following denote scalars italic lowercase vectors bold lowercase matrices italic uppercase symbols. colon notations denote sequences scalars vectors respectively. hybrid token embedding layer hybrid token embedding layer takes token input outputs vector representation utilizing token embeddings well character embeddings. figure model sequential sentence classiﬁcation. token token embeddings character character embeddings character-based token embeddings hybrid token embeddings sentence vector sentence label vector sentence label. numbers parenthesis indicate dimensions vectors. token embeddings initialized glove embeddings pretrained wikipedia gigaword token embeddings direct mapping token vector pre-trained large unlabeled datasets using programs wordvec glove character embeddings also deﬁned analogous manner direct mapping character vector. sequence characters comprise token character ﬁrst mapped embedding resulting sequence input bidirectional lstm outputs character-based token embedding sentence label prediction layer sequence tokens given sentence corresponding embedding output hybrid token embedding layer. sentence label prediction layer takes input sequence vectors outputs element denoted reﬂects probability given sentence label achieve this sequence ﬁrst input bidirectional lstm outputs vector representation given sentence. vector subsequently input feedforward neural network hidden layer outputs corresponding probability vector pubmed assembled corpus consisting randomized controlled trials pubmed database biomedical literature provides standard sentence labels objectives background methods results conclusions. denotes number table dataset overview. classes vocabulary size. train validation test sets indicate number number abstracts followed number sentences parentheses. training model trained using stochastic gradient descent updating parameters i.e. token embeddings character embeddings parameters bidirectional lstms transition probabilities gradient step. regularization dropout rate applied characterenhanced token embeddings label prediction layer. table compares model several baselines well best performing model alta shared task competing research teams participated build accurate classiﬁer nictapiboso corpus. ﬁrst baseline classiﬁer based logistic regression using n-gram features extracted current sentence information surrounding sentences. second baseline uses model presented computes sentence embeddings sentence classiﬁes current sentence given preceding sentence embeddings well current sentence embedding. third baseline uses n-grams features output variable corresponds label sentence sequence considers entire abstract. baseline therefore uses preceding succeeding sentences classifying current sentence. lastly model presented developed label sequence optimization layer label sequence optimization layer takes sequence probability vectors label prediction layer input outputs sequence labels label assigned token order model dependencies subsequent labels incorporate matrix contains transition probabilities subsequent labels; deﬁne probability token label followed token label score label sequence deﬁned probabilities individual labels transition probabilities scores turned probabilities label sequences taking softmax function possible label sequences. training phase objective maximize probability gold label sequence. testing phase given input sequence tokens corresponding sequence predicted labels chosen maximizes score. experiments datasets evaluate model sentence classiﬁcation task using following medical abstract datasets sentence abstract annotated label. table presents statistics dataset. table f-scores test several baselines best published method literature model. since pubmed introduced work previous best published method dataset. presented results ann-based models fscores test highest f-score validation set. forward system performs better system worse unsurprising forward system uses information preceding sentences information succeeding sentences unlike crf. model performs better system system. hypothesize following four factors give edge model. human-engineered features unlike systems model rely human-engineered features. n-grams systems heavily rely n-grams model maps token token embedding feeds input rnn. helps combat data scarcity example chronic tendonitis chronic tendinitis different bigrams token embeddings similar since share meaning. structured prediction labels sentences abstract predicted jointly improves coherence predicted labels given abstract. joint learning model learned features token embeddings jointly sequence optimization. figure presents example transition matrix model trained pubmed rct. effectively reﬂects transitions different labels. example learned ﬁrst sentence abstract likely either discussing objective background token sentence pertaining methods typically followed sentence pertaining methods results article presented architecture classify sentences appear sequence. demonstrate jointly predicting classes sentences given text improves quality predictions yields better performance crf. model achieves state-of-the-art results datasets sentence classiﬁcation medical abstracts.", "year": 2016}