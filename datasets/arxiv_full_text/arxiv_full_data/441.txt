{"title": "Discrete Deep Feature Extraction: A Theory and New Architectures", "tag": ["cs.LG", "cs.CV", "cs.IT", "cs.NE", "math.IT", "stat.ML"], "abstract": "First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made---for the continuous-time case---in Mallat, 2012, and Wiatowski and B\\\"olcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection---including feature importance evaluation---complement the theoretical findings.", "text": "rett fashion) pre-speciﬁed unstructured random ﬁlters non-linearities used pooling operator employed given choice ﬁlters nonlinearities pooling operators lead vastly different performance results across datasets remarkable overall dcnn architecture allows impressive classiﬁcation results across extraordinarily broad range applications. therefore signiﬁcant interest understand mechanisms underlying universality. first steps towards addressing question developing mathematical theory dcnns feature extraction made—for continuous-time case—in speciﬁcally analyzed so-called scattering networks signals propagated layers employ directional wavelet ﬁlters modulus non-linearities intra-layer pooling. resulting wavelet-modulus feature extractor horizontally translation-invariant deformationstable properties signiﬁcance practical feature extraction applications. recently considered mallat-type networks arbitrary ﬁlters general lipschitz-continuous non-linearities continuous-time pooling operator amounts dilation. essence results vertical translation invariance lipschitz continuity feature extractor induced network structure rather speciﬁc choice ﬁlters non-linearities. band-limited thomas wiatowski michael tschannen aleksandar stani´c philipp grohs helmut b¨olcskei dept. zurich switzerland dept. math. university vienna austria first steps towards mathematical theory deep convolutional neural networks feature extraction made—for continuous-time case— mallat wiatowski b¨olcskei paper considers discrete case introduces convolutional neural network architectures proposes mathematical framework analysis. speciﬁcally establish deformation translation sensitivity results local global nature investigate certain structural properties input signal reﬂected corresponding feature vectors. theory applies general ﬁlters general lipschitz-continuous non-linearities pooling operators. experiments handwritten digit classiﬁcation facial landmark detection—including feature importance evaluation—complement theoretical ﬁndings. deep convolutional neural networks proven tremendously successful wide range machine learning tasks networks composed multiple layers computes convolutional transforms followed application non-linearities pooling operators. write cyclic translation operator. supremum norm continuoustime function supx∈r |c|. indicator function interval deﬁned cardinality denoted card. convolutional transform made ﬁlters {gλ}λ∈λ. ﬁnite index thought labeling collection scales directions frequency-shifts. ﬁlters gλ—referred atoms— learned pre-speciﬁed unstructured random ﬁlters pre-speciﬁed structured wavelets curvelets shearlets weyl-heisenberg functions. deﬁnition ﬁnite index set. collection {gλ}λ∈λ called convolutional bessel bound implies written linear combination elements n∈in absence lower bound therefore result failing extract essential features signal note however even learned ﬁlters likely satisfy least shall below existence lower bound however needed theory apply. signals cartoon functions lipschitz-continuous functions lipschitz continuity feature extractor automatically leads bounds deformation sensitivity. contributions. purpose present paper develop theory discrete dcnns feature extraction. speciﬁcally follow philosophy forward theory incorporates general ﬁlters lipschitz non-linearities lipschitz pooling operators. addition introduce analyze wide variety network architectures build feature vector subsets layers. leads notions local global feature vector properties globality pertaining characteristics brought union features across network layers locality identifying attributes made explicit individual layers. besides providing analytical performance results general validity also investigate certain structural properties input signal reﬂected corresponding feature vectors. speciﬁcally analyze deformation translation sensitivity properties feature vectors corresponding sampled cartoon functions simplicity exposition focus case throughout paper noting extension higher-dimensional case pose signiﬁcant difﬁculties. theoretical results complemented extensive numerical studies facial landmark detection handwritten digit classiﬁcation. speciﬁcally elucidate role local feature vector properties feature relevance study. notation. complex conjugate denoted write real imaginary part n-periodic discrete-time signals delta function supn∈in denote discrete fourier transform e−πikn/n circular convolution note isometrically isomorphic theory developed paper applies general pooling operators hn/s satisfy lipschitz property integer referred pooling factor determines size neighborhood values combined architecture consider ﬂexible following sense. layer feed feature vector either signals propagated layer ﬁltered versions thereof decide layer contribute feature vector. basic building blocks network triplets ﬁlters non-linearities pooling operators associated d-th network layer referred modules. emphasize triplets allowed different across layers. deﬁnition network layers {gλd}λd∈λd convolutional point-wise lipschitz-continuous non-linearity hnd+ lipschitz-continuous pooling operator denotes pooling factor d-th layer. then sequence triplets features generated wavelet ﬁlters modulus non-linearities without intra-layer pooling employing output-generating atoms low-pass characteristics describe frequency cepstral coefﬁcients sift-descriptors main results hold general signals provide reﬁned analysis class sampled cartoon functions. allows understand certain structural properties input signal presence sharp edges reﬂected feature vector. cartoon functions—as introduced continuous time —are piecewise smooth apart curved discontinuities along lipschitz-continuous hypersurfaces. hence provide good model natural images caltech- cifar- datasets images handwritten digits images geometric objects different shapes sizes colors baby school dataset. bounds deformation sensitivity cartoon functions continuous-time dcnns recently reported here analyze deformation sensitivity sampled cartoon functions passed discrete dcnns. deﬁnition function referred cartoon function written closed interval satisﬁes lipschitz property figure network architecture underlying feature extractor index convolutional associated d-th network layer. function output-generating atom d-th layer. root network corresponds function ind+ want output equal unﬁltered features propagated layer signal length want layer contribute feature vector. formally {gλd+}λd+∈λd+ noting {gλd+}λd+∈λd+ {χd} forms convolutional maxk∈ind+ atoms augmented {gλd+}λd+∈λd+ {χd} employed across consecutive layers sense generating output d-th layer according remaining atoms {gλd+}λd+∈λd+ propagating signals d-th layer layer according fig. slight abuse notation shall henceforth write ready deﬁne feature extractor based module-sequence output generated d-th network layer else. dimension overall feature vector determined pooling factors course layers contribute feature vector. remark argued figure left natural image typically governed areas little variation individual areas separated edges modeled curved singularities. middle image handwritten digit. right pixel values corresponding dashed middle image. inition cart conceptual importance moreover results easily generalized classes cart consisting functions containing multiple edges according also cart reduces class sampled lipschitzanalyze global local feature vector properties globality pertaining characteristics brought union features across network layers locality identifying attributes made explicit individual layers. lipschitz continuity guarantees pairwise distances input signals increase feature extraction. immediate implication lipschitz continuity robustness feature extractor w.r.t. additive bounded noise sense |||φω φω||| remark detailed proof theorem lipschitz continuity combined deformation sensitivity bound signal class consideration namely sampled cartoon functions establishes deformation sensitivity bound feature extractor. insight important practical ramiﬁcations shows whenever deformation sensitivity bounds signal class automatically deformation sensitivity guarantees corresponding feature extractor. shows small translations underlying analog signal lead small changes feature vector obtained passing resulting sampled signal discrete dcnn. shall translation sensitivity bound. analyzing impact deformations translations discrete feature vector generated sampled analog signal closely models real-world phenomena correspond different camera resolutions). note that iii) theorem speciﬁc cartoon functions apply signals strength results theorem derives fact condition underlying modulesequence easily practice. this ﬁrst note determined convolutional non-linearity pooling operator condition min{ which satisﬁed default enforced simply normalizing elements speciﬁcally max{bd gλd}λd∈λd hence satisﬁes normalization impact results theorem exists however tradeoff energy preservation deformation sensitivity ≤d≤d modulesequence corresponding bessel bounds lipschitz constants non-linearities lipschitz constants pooling operators outputgenerating atoms also follows moreover quantiﬁes impact deformations feature vector. practice desirable features become robust additive noise less deformationsensitive progress deeper network. formally vertical sensitivity reduction induced ensuring thanks accomplished choosing module-sequence ldrd χd−. note however owχdb/ also reduce signal energy contained features therefore tradeoff deformation sensitivity energy preservation. control tradeoff choice module-sequence come handy practice. ﬁnally turn interpreting translation covariance transsult owing condition lation covariance rough grid induced product pooling factors. absence pooling remark note scatnets translation-covariant rough grid induced factor corresponding coarsest wavelet scale. result hence spirit difference grid case induced pooling factors consider problem handwritten digit classiﬁcation evaluate performance feature extractor combination support vector machine results obtain competitive state-of-the-art literature. second line experiments perform assesses importance features extracted facial landmark detection handwritten digit classiﬁcation using random forests regression classiﬁcation respectively. results based dcnn different non-linearities pooling operators tensorized wavelets ﬁlters sensitive directions furthermore generate outputs layers low-pass ﬁltering. circular convolutions ﬁlters underlying tensorized wavelets efﬁciently implemented using algorithme trous reduce dimension feature vector compute features along frequency decreasing paths mallat retain child nodes scales larger maximum scale wavelets used refer detailed justiﬁcation approach scattering networks. mnist dataset handwritten digits comprises training test images size compare different network conﬁgurations deﬁned single module speciﬁcally consider haar wavelets reverse biorthogonal wavelets scales non-linearities described section pooling operators described section radial basis function kernel classiﬁcation. reduce dimension feature vectable classiﬁcation error percent handwritten digit classiﬁcation using different conﬁgurations wavelet ﬁlters nonlinearities pooling operators tors employ supervised orthogonal least squares feature selection procedure described penalty parameter localization parameter kernel selected -fold cross-validation combination wavelet ﬁlter non-linearity pooling operator. table shows resulting classiﬁcation errors test conﬁgurations employing rbio. wavelets tend yield marginally lower classiﬁcation error using haar wavelets. tanh logsig non-linearities max-pooling leads considerably lower classiﬁcation error pooling operators. conﬁgurations involving modulus relu non-linearities achieve classiﬁcation accuracy competitive state-of-theart based directional non-separable wavelets directions without intra-layer pooling. interesting separable wavelet ﬁlters employed implemented efﬁciently. experiment investigate importance features generated corresponding different layers wavelet scales directions different learning tasks namely facial landmark detection handwritten digit classiﬁcation. primary goal experiment illustrate practical relevance notion local properties established section facial landmark detection employ regressor handwritten digit classiﬁcation classiﬁer cases number trees select tree depth using out-of-bag error estimates impurity measure used learning node tests mean square error facial landmark detection gini impurity handwritten digit classiﬁcation. cases feature importance assessed using gini importance averaged trees. gini importance feature tree deﬁned denotes feature determined training phase test node number training samples impurity node denote left right child node respectively node feature extractor employ haar wavelets scales modulus non-linearity every network layer pooling ﬁrst layer average pooling uniform weights facial landmark detection. caltech faces data base images data base depicts faces different contexts data base contains annotations positions eyes nose mouth least face image. learning task estimate positions facial landmarks. annotations serve ground truth training testing. preprocess data follows. patches containing faces extracted images using viola-jones face detector discarding false positives patches converted grayscale resampled size feeding feature extractor procedure yields dataset containing total face images. select images uniformly random form training remaining images testing. train separate facial landmark. following report localization error i.e. -distance estimated ground truth landmark positions test fraction inter-ocular distance. errors obtained left right nose; mouth aside note values comparable ones reported conditional using patch comparison features handwritten digit classiﬁcation. experiment rely mnist dataset. training obtained sampling uniformly random images digit mnist training dataset complete mnist test set. train based unmodiﬁed images based images subject random uniform displacement pixels direction study impact offsets feature importance. resulting achieve classiﬁcation error respectively. discussion. figure shows cumulative feature importance handwritten digit classiﬁcation facial landmark detection. table shows corresponding cumulative feafigure average cumulative feature importance standard error facial landmark detection handwritten digit classiﬁcation. labels horizontal axis indicate layer index d/wavelet direction facial landmark detection features layer clearly highest importance feature importance decreases increasing layer index handwritten digit classiﬁcation using unshifted mnist images cumulative importance features second/third layer relative ﬁrst layer considerably higher facial landmark detection translated mnist images importance features second/third layer signiﬁcantly higher ﬁrst layer. explanation observation could follows classiﬁcation task small sensitivity translations beneﬁcial. according theory translation sensitivity indeed decreases increasing layer index average pooling used here. localization landmarks hand needs features covariant grid input image thus favoring features layers closer root. dantone gall fanelli gool realtime facial feature detection using conditional regression proc. ieee conf. comp. vision pattern forests. recog. davis mermelstein comparison parametric representations monosyllabic word recognition continuously spoken sentences. ieee trans. acoust. speech signal process. ranzato poultney chopra lecun efﬁcient learning sparse representations energybased model. proc. int. conf. neural information processing systems ranzato huang boureau lecun unsupervised learning invariant feature hierarproc. chies applications object recognition. ieee conf. comp. vision pattern recog. table classiﬁcation errors percent handwritten digit classiﬁcation using wavelet ﬁlters different non-linearities different pooling operators figure average cumulative feature importance standard error facial landmark detection. labels horizontal axis indicate layer index d/wavelet direction proceed iii). proof deformation sensitivity bound based ingredients. ﬁrst lipschitz continuity result stated second ingredient stated proposition appendix upper bound deformation error given remark already mentioned section excluding interval boundary points deﬁnition sampled cartoon functions cart necessary technical reasons. speciﬁcally without imposing exclusion expect deformation sensitivity results form seen follows. assume seek bound form proof. proof based judiciously combining deformation sensitivity bounds sampled components cart sampled indicator function ﬁrst bound stated lemma below reads follows young’s inequality remark emphasize also upperbounded follows fact {gλd+}λd+∈λd+ {χd} atoms convolutional bessel bound bd+. hence substitute bd+. ﬁrst lipschitz continuity result second ingredient again deformation sensitivity bound stated proposition appendix combining proof iii) theorem appendix c—then establishes completes proof iii).", "year": 2016}