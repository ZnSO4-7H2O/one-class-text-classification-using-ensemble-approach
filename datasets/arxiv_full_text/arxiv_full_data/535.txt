{"title": "Memory Visualization for Gated Recurrent Neural Networks in Speech  Recognition", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.", "text": "recurrent neural networks shown clear superiority sequence modeling particularly ones gated units long short-term memory gated recurrent unit however dynamic properties behind remarkable performance remain unclear many applications e.g. automatic speech recognition paper employs visualization techniques study behavior lstm performing speech recognition tasks. experiments show interesting patterns gated memory inspired simple effective modiﬁcations network structure. report modiﬁcations lazy cell update lstm shortcut connections residual learning. modiﬁcations lead comprehensible powerful networks. deep learning gained brilliant success wide spectrum research areas including automatic speech recognition among various deep models recurrent neural network particular interesting partly capability modeling complex temporal dynamics speech signals continuous state trajectory essentially overturns long-standing hidden markove model describes dynamic properties speech signals discrete state transition. promising results reported rnn-based known issue vanilla model training network generally difﬁcult largely attributed gradient vanishing explosion problem. additionally vanilla model tends forget things quickly. solve problems gated memory mechanism proposed researchers leading gated rnns rely trainable gates select important information receive memorize propagate. widely used gated structures long short-term memory proposed hochreiter gated recurrent unit proposed recently despite success gated rnns happened gated memory run-time remains unclear speech recognition. prevents deep understanding gating mechanism relative advantage different gated units understood neither intuitively systematically. paper utilize visualization technique study behavior gated rnns performing asr. focus evolution gated memory. interested difference popular gated units lstm terms duration memorization quality activation patterns. visualization behavior gated better understood return inspire ideas effective structures. paper reports simple modiﬁcations inspired visualization results experiments demonstrate result models powerful also comprehensible. rest paper organized follows section describes related work section presents experimental settings. visualization results shown section modiﬁcations inspired visualization results presented section entire paper concluded section visualization used several research areas study behavior neural models. instance computer vision visualization often used demonstrate hierarchical feature learning process deep conventional neural networks activation maximization composition analysis natural language processing another area visualization widely utilized. since word/tag sequences often modeled visualization focuses analysis temporal dynamics units rnns speech recognition visualization employed much partly displaying speech signals visual patterns straightforward images text. work know visualization conducted miao studied input forget gates lstm found correlated. visualization analysis presented paper differs miao’s work analysis based comparative study identiﬁes important mechanism good performance comparing behavior different gated structures terms activation patterns temporal memory traces. comparative analysis lstm conducted chung paper different chung’s work compare structures visualization rather reasoning. moreover analysis focuses group behavior individual units rather all-in-one performance. equations terms denote weight matrices diagonal. input symbol; represent respectively input forget output gates; cell unit output. logistic sigmoid function hyperbolic activation functions. denotes element-wise multiplication. ignore bias vectors formula simpliﬁcation. fbanks symmetric -frame window splice neighboring frames. number recurrent layers varies number units hidden layer units lstm gru. output layer consists units equal total number gaussian components conventional system used bootstrap model. kaldi toolkit used conduct model training performance evaluation training process largely follows nnet recipe. natural stochastic gradient descent algorithm used train model. results terms word error rate reported table ‘lstm’ denotes system lstms recurrent units ‘gru’ denotes system grus recurrent units. observe rnns based units perform slightly better based lstm units. section presents visualization results. limited space emphasis comparison lstm gru. detailed results analysis found associated technical report ﬁrst experiment investigates different gated rnns encode information different ways. lstm rnns units randomly selected hidden layer unit distribution cell values utterances computed. results shown fig. lstm rnns respectively. limited space ﬁrst fourth layers presented. lstm reset irregular values better visualization. observed cell values lstm concentrate zero values concentration decreases higher-level layer. pattern suggests lstm relies great positive negative cell values units represent information. contrast cells concentrate pattern clear higher-level layer. suggests relies contrast among cell values different units encode information. difference activation patterns suggests information distributed lstm. conjecture lead compact model better parameter sharing. related observation activations lstm cells unlimited absolute values cells rather large. cell values strictly constrained also derived since positive less cell initialized value cell remain range. constrained range values advantage model training partly avoid abnormal gradients often hinder training. noise segment inserted speech stream observe inﬂuence noise segment visualizing difference cell values caused noise insertion. results shown fig. second experiment investigates evolution cell activations performing recognition. achieved drawing cell vectors frames using t-sne tool decoding utterance. results shown fig. temporal traces four layers drawn plots bottom. interesting observation traces much smooth lstm gru. indicates lstm tends remember long-term memory novelty current time largely averaged past memory leading smooth temporal trace. experience quickly adopted memory tends change drastically. comparing memory traces different layers seen traces become smooth higher-level layers whereas trend clear lstm. suggests trade innovation memorization different layers low-level layers concentrates innovation high-level layers memorization becomes important. perhaps advantage analog human brain low-level features change abruptly high-level information keeps evolving gradually. seen units accumulate longer memory higher-level layers robust lstm noisy conditions. lstm impact noise lasts almost till cells even ﬁnal layer units supposed noise robust. impact lasts frames. demonstrates advantage double conﬁrms observation second experiment remembers less lstm. visualization results shown previous section demonstrate lstm possess different properties information encoding temporal evolution. differences easy tell model better particular task. speech recognition experimental results section seemingly demonstrate suitable. explained fact speech signals pseudo-stationary typical durations phones longer frames. means shorter memory likely advantage particularly noise difference lstm shown section updates cells ﬁnal step lstm updates cells computing output gates. study impact lazy update reorder computation lstm shown fig. recognition results presented table temporal trace lazy update shown fig. note ﬁnal lstm layer modiﬁed. results seen lazy update improve performance lstm. temporal trace seems modiﬁed lstm behaves like trace less smooth allowing quicker adoption input. demonstrates short-memory behavior possibly important factor good performance behavior closely related lazy cell update. another modiﬁcation inspired visualization result gates high-level layers show similar pattern implies cells high-level layers mostly learned residual. also conﬁrmed recent research residual borrow idea explicit results residual learning shown table temporal traces shown results show adding shortcut connections indeed introduces consistent performance gains lstm gru. temporal traces different layers seem consistent particularly evident third layer remember short-time events well. expected information quicker easier shortcut connections. paper presented visualization results gated rnns particular focused comparison lstm gru. results show gated rnns different ways encode information information distributed. moreover lstm possesses long-term memory also noise sensitive. inspired observations introduced modiﬁcations enhance gated rnns lazy cell update short connections residual learning provide interesting performance improvement. future work compare neural models different categories e.g. tdnn rnn. yajie miao jinyu yongqiang wang shi-xiong zhang yifan gong simplifying long short-term memory acoustic models fast training decoding ieee international conference acoustics speech signal processing ieee junyoung chung caglar gulcehre kyunghyun yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling arxiv preprint arxiv. daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society number epfl-conf-. zhiyuan tang ying dong wang yang feng shiyue zhang visualization analysis recurrent networks tech. rep. cslt tsinghua university http//cslt.org/mediawiki/images//a/visual.pdf. alex graves mohamed geoffrey hinton speech recognition deep recurrent neural netproceedings ieee international conworks ference acoustics speech signal processing ieee alex graves navdeep jaitly towards end-to-end speech recognition recurrent neural networks proceedings international conference machine learning hasim andrew senior franc¸oise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling proceedings annual conference international speech communication association kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio properties neural machine translation encoder-decoder approaches arxiv preprint arxiv. dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos deep speech end-to-end speech recognition english mandarin arxiv preprint arxiv. karen simonyan andrea vedaldi andrew zisserman deep inside convolutional networks visualising image classiﬁcation models saliency maps arxiv preprint arxiv.", "year": 2016}