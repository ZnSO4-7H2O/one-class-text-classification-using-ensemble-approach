{"title": "Combining policy gradient and Q-learning", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.", "text": "policy gradient efﬁcient technique improving policy reinforcement learning setting. however vanilla online variants on-policy able take advantage off-policy data. paper describe technique combines policy gradient off-policy q-learning drawing experience replay buffer. motivated making connection ﬁxed points regularized policy gradient algorithm q-values. connection allows estimate q-values action preferences policy apply q-learning updates. refer technique ‘pgql’ policy gradient q-learning. also establish equivalency action-value ﬁtting techniques actor-critic algorithms showing regularized policy gradient techniques interpreted advantage function learning algorithms. conclude numerical examples demonstrate improved data efﬁciency stability pgql. particular tested pgql full suite atari games achieved performance exceeding asynchronous advantage actor-critic q-learning. reinforcement learning agent explores environment reward signal learns optimize behavior maximize expected long-term return. reinforcement learning seen success several areas including robotics computer games online advertising board games many others. introduction reinforcement learning refer classic text sutton barto paper consider model-free reinforcement learning state-transition function known learned. many different algorithms model-free reinforcement learning fall families action-value ﬁtting policy gradient techniques. action-value techniques involve ﬁtting function called q-values captures expected return taking particular action particular state following particular policy thereafter. alternatives discuss paper sarsa q-learning although many others. sarsa on-policy algorithm whereby action-value function current policy reﬁned mostly greedy respect action-values. hand q-learning attempts qvalues associated optimal policy directly policy used generate data. q-learning off-policy algorithm data generated another agent replay buffer experience. certain conditions sarsa q-learning shown converge optimal q-values derive optimal policy policy gradient techniques policy represented explicitly improve policy updating parameters direction gradient performance online policy gradient typically requires estimate action-value function current policy. reason often referred actor-critic methods actor refers policy critic estimate action-value function vanilla actor-critic methods on-policy only although attempts made extend off-policy data paper derive link q-values induced policy policy policy ﬁxed point regularized policy gradient algorithm connection allows derive estimate q-values current policy reﬁne using off-policy data q-learning. show tabular setting regularization penalty small resulting policy close policy would found without addition q-learning update. separately show regularized actor-critic methods interpreted action-value ﬁtting methods q-values parameterized particular way. conclude numerical examples provide empirical evidence improved data efﬁciency stability pgql. highlight various axes along work compared others. paper entropy regularization ensure exploration policy common practice policy gradient alternative kl-divergence instead entropy regularizer constraint much deviation permitted prior policy natural policy gradient also interpreted putting constraint kl-divergence step policy improvement sallans hinton authors boltzmann exploration policy estimated q-values update using td-learning. heess extended actor-critic algorithm instead td-learning however updates combined done paper. azar authors develop algorithm called dynamic policy programming whereby apply bellman-like update action-preferences policy similar spirit update describe here. norouzi authors augment maximum likelihood objective reward supervised learning setting develop connection resembles develop policy q-values. works attempted combine off-policy learning primarily using action-value ﬁtting methods varying degrees success. paper establish connection actor-critic algorithms action-value learning algorithms. particular show td-actor-critic equivalent expected-sarsa boltzmann exploration q-values decomposed advantage function value function. algorithm develop extends actor-critic q-learning style update that decomposition q-values resembles update dueling architecture recently ﬁeld deep reinforcement learning i.e. deep neural networks represent action-values policy seen success examples section neural network pgql play atari games suite. consider inﬁnite horizon discounted ﬁnite state action space markov decision process state space action space rewards time period denoted policy mapping state-action pair probability taking action states policy induces probability distribution visited states probability seeing state-action pair dππ. reinforcement learning ‘agent’ interacts environment number times steps. time step agent receives state reward selects action policy point agent moves next state probability transitioning state state taking action continues agent encounters terminal state goal agent γtrt expectation respect initial state distribution state-transition probabilities policy discount factor that loosely speaking controls much agent prioritizes long-term versus short-term rewards. since agent starts knowledge action-values. action-value q-value particular state policy expected total discounted return taking action state following thereafter i.e. γtrt value state policy denoted γtrt expected total discounted return policy state optimal action-value function denoted satisﬁes maxπ policy achieves maximum optimal policy value function advantage function difference action-value value function i.e. represents additional expected reward taking action simply states policy advantage expectation next state reward optimal q-value function ﬁxed point optimal bellman equation i.e. π-bellman operator optimal bellman operator γ-contraction mappings sup-norm i.e. rs×a. fact show ﬁxed point operator unique value iteration converges i.e. initial value based reinforcement learning approximate q-values using function approximator. update parameters q-values close ﬁxed point bellman equation possible. denote approximate q-values parameterized q-learning updates q-values along direction esaq q)∇θq sarsa updates q-values along direction q)∇θq. online setting bellman operator approximated sampling bootstrapping whereby q-values state updated using q-values next visited state. exploration achieved always taking action highest q-value time step. common technique called ‘epsilon greedy’ sample random action probability starts high decreases time. another popular technique ‘boltzmann exploration’ policy given softmax q-values temperature i.e. exp/t common decrease temperature alternatively parameterize policy directly attempt improve gradient ascent performance policy gradient theorem states gradient respect parameters policy given expectation probability dππ. original derivation policy gradient theorem expectation discounted distribution states i.e. r{st however gradient update case assign weight states take long time reach therefore poor empirical performance. practice non-discounted distribution states frequently used instead. certain cases equivalent maximizing average policy performance even uses discount factor throughout paper non-discounted distribution states. online case common entropy regularizer gradient order prevent policy becoming deterministic. ensures agent explore continually. case update becomes denotes entropy policy regularization penalty parameter. throughout paper make entropy regularization however many results true choices regularizers minor modiﬁcation e.g. kl-divergence. note equation requires exact knowledge q-values. practice estimated e.g. discounted rewards along observed trajectory policy gradient still perform well section derive relationship policy q-values using regularized policy gradient algorithm. allows transform policy estimate q-values. show small regularization q-values induced policy ﬁxed point algorithm small bellman error tabular case. consider ﬁxed points entropy regularized policy gradient update deﬁne ﬁxed point longer update direction without violating constraints i.e. span vectors {∇θgs}. words ﬁxed point λs∇θgs lagrange multiplier ensures absorbed constants r|s|. solution equation strictly positive element-wise since must domain entropy function. tabular case represented single number state action pair gradient policy respect parameters indicator function i.e. obtain multiplying summing substituting equation following formulation policy exp/α words policy ﬁxed point softmax advantage function induced policy regularization parameter interpreted temperature. therefore policy derive estimate q-values since update unchanged per-state constant offsets. policy parameterized quantity sometimes referred action-preferences policy equation states action preferences equal q-values scaled additive per-state constant. r|s| lagrange multiplier associated constraint policy state. comparing equation measure expectation describe ﬁxed points. suggests interpretation ﬁxed points regularized policy gradient regression log-policy onto q-values. general case using approximation architecture interpret equation indicating error orthogonal cannot reduced changing parameters least locally. case equation unlikely hold solution however good approximation architecture hold approximately derive estimate q-values policy using equation estimate q-values next section. previous section made connection regularized policy gradient regression onto q-values ﬁxed point. section step further showing actor-critic methods interpreted action-value ﬁtting methods exact method depends choice critic. equivalence. policies identical since initialized parameterized assuming value function estimates initialized parameterized remains show updates equations identical. comparing assuming difference measure ﬁxed equal current policy therefore changes update. replacing makes updates identical case iterations policies always same. words slightly modiﬁed action-value method equivalent actor-critic policy gradient method vice-versa particular regularized policy gradient methods interpreted advantage function learning techniques since optimum quantity equal advantage function sarsa remark. action selected state would equivalent using bootstrap critic equation expected-sarsa take expectation q-values next state r+γv )−q. equivalent td-actor-critic value function provide critic given q-learning maxb would equivalent using optimizing critic bootstraps using q-value next state i.e. maxb reinforce critic monte carlo return state γtrt return trace truncated bootstrap performed n-steps equivalent n-step sarsa n-step q-learning depending form bootstrap bellman residual section show decreasing regularization penalty policy deﬁned corresponding q-value function functions shall show converges zero bounding sequence zero sequence converges zero. first since greedy respect q-values. bound exp/α) bellman residual converges zero decreasing words small enough q-values induced policy small bellman residual. moreover implies limα→ might expect. section introduce main contribution paper technique combine policy gradient q-learning. call technique ‘pgql’ policy gradient q-learning. previous section showed bellman residual small ﬁxed point regularized policy gradient algorithm regularization penalty sufﬁciently small. suggests adding auxiliary update explicitly attempt reduce bellman residual estimated policy i.e. hybrid policy gradient q-learning. ﬁrst present technique batch update setting perfect knowledge later discuss practical implementation technique reinforcement learning setting function approximation agent generates experience interacting environment needs estimate critic simultaneously policy. parameters necessarily equation unnecessary estimate constant since update invariant constant offsets although practice often estimated variance reduction technique full scheme simply combines updates policy regularized policy gradient update q-learning update assuming architecture provides policy value function estimate action-value critic parameter updates written notation) weighting parameter controls much update apply. case scheme reduces entropy regularized policy gradient. becomes variant q-learning architecture similar dueling architecture intermediate values produce hybrid two. examining update error terms trading off. ﬁrst term encourages consistency critic second term encourages optimality time. however since know standard policy gradient bellman residual small follows adding term reduces error make much difference ﬁxed point. updates complementary pointing general direction least away ﬁxed point. update also interpreted actor-critic update critic given weighted combination standard critic optimizing critic. another interpretation update combination expected-sarsa q-learning q-values parameterized advantage function value function. updates presented batch updates exact critic practice want scheme online estimate critic don’t necessarily apply policy gradient update time data source q-learning update. proposal scheme follows. agents interact environment encountering states rewards performing on-policy updates parameters using actor-critic algorithm policy critic updated online. time agent receives data environment writes shared replay memory buffer. periodically separate learner process samples replay buffer performs step q-learning parameters policy using scheme several advantages. critic accumulate monte carlo return many time periods allowing spread inﬂuence reward received future backwards time. furthermore replay buffer used store replay ‘important’ past experiences prioritizing samples replay buffer help reduce problems associated correlated training data generated agent exploring environment states likely similar time step next. also replay kind regularizer preventing policy moving satisfying bellman equation thereby improving stability similar sense policy ‘trust-region’ moreover batching replay samples update network leverage gpus perform updates quickly comparison pure policy gradient techniques generally implemented since perform q-learning using samples replay buffer generated policy performing off-policy learning. however q-learning known converge optimal q-values off-policy tabular case shown good performance off-policy function approximation case pgql updates equation modiﬁed ﬁxed point algorithm analysis longer valid. considering tabular case again still case policy before deﬁned however previously ﬁxed point satisﬁed corresponding q-values induced ηk)kqπ. appendix show decreasing tabular case. small induced q-values q-values estimated policy close still guarantee limit q-values optimal. words perturbed policy much addition auxiliary update. section discuss results running pgql grid world shown figure agent always begins square marked episode continues reaches square marked upon receives reward times receives reward. experiment chose regularization parameter discount factor figure shows performance traces three different agents learning grid world running initial random seed. lines show true expected performance policy start state calculated value iteration update. blue-line standard td-actor-critic maintain estimate value function generate estimate q-values critic. green line q-learning step update performed using data drawn replay buffer prior experience q-values parameterized equation policy softmax q-value estimates temperature line pgql step ﬁrst performs td-actor-critic update performs q-learning update grid world totally deterministic step size could large chosen step-size larger made pure actor-critic agent fail learn pgql q-learning could handle increase step-size possibly stabilizing effect using replay. clear pgql outperforms two. point along x-axis agents seen amount data would indicate pgql data efﬁcient either vanilla methods since highest performance practically every point. tested algorithm full suite atari benchmarks using neural network parameterize policy. ﬁgure show policy network augmented parameterless additional layer outputs q-value estimate. exception extra layer architecture parameters chosen exactly match asynchronous advantage actor-critic algorithm presented mnih turn reused many settings mnih speciﬁcally used exact learning rate number workers entropy penalty bootstrap horizon network architecture. allows fair comparison pgql since difference addition q-learning step. technique augmented following change actor-learner accumulated gradient policy update performs single step q-learning replay data described equation minibatch size q-learning learning rate chosen times actor-critic learning rate updates happen different frequencies different data sources). actor-learner thread maintained replay buffer last transitions seen thread. learning million agent steps results compare variant asynchronous deep q-learning. changes made q-learning make similar method tuning hyperparameters performance. exact network exploration policy softmax q-values temperature q-values parameterized equation q-value updates performed every steps minibatch method games used identical hyper-parameters. game tested times method hyper-parameters different random seeds. scores presented correspond best score obtained random start evaluation condition overall pgql performed best games performed best games q-learning best games. games methods tied. tables give mean median normalized scores percentage expert human normalized score across games tested algorithm random human-start conditions respectively. human-start condition agent takes control game randomly selected human-play starting points generally leads lower performance since agent found state training. cases pgql highest mean median median score exceeds human performance threshold. worth noting pgql worst performer game cases outright winner generally somewhere performance algorithms. figure shows sample traces games pgql best performer. cases pgql better data efﬁciency methods. ﬁgure show games pgql under-performed. practically every case pgql perform well better data efﬁciency early learning performance saturated collapsed. hypothesize cases policy reached local optimum over-ﬁt early data might perform better hyper-parameters tuned. made connection ﬁxed point regularized policy gradient techniques q-values resulting policy. small regularization shown bellman residual induced q-values must small. leads consider adding auxiliary update policy gradient related bellman residual evaluated transformation policy. update performed off-policy using stored experience. call resulting method ‘pgql’ policy gradient q-learning. empirically observe better data efﬁciency stability pgql compared actor-critic q-learning alone. veriﬁed performance pgql suite atari games parameterize policy using neural network achieved performance exceeding q-learning. thank joseph modayil many comments suggestions paper hubert soyer help performance evaluation. would also like thank anonymous reviewers constructive feedback. marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research matthew hausknecht peter stone. on-policy off-policy updates deep reinforcement learning. deep reinforcement learning frontiers challenges ijcai workshop timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. nips deep learning workshop. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature http//dx.doi.org/./ nature. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. mohammad norouzi samy bengio zhifeng chen navdeep jaitly mike schuster yonghui dale schuurmans. reward augmented maximum likelihood neural structured prediction. arxiv preprint arxiv. edwin pednault naoki bianca zadrozny. sequential cost-sensitive decision making reinforcement learning. proceedings eighth sigkdd international conference knowledge discovery data mining martin riedmiller. neural ﬁtted iteration–ﬁrst experiences data efﬁcient neural reinforcement learning method. machine learning ecml springer berlin heidelberg john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning david silver lever nicolas heess thomas degris daan wierstra martin riedmiller. deterministic policy gradient algorithms. proceedings international conference machine learning david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature richard sutton david mcallester satinder singh yishay mansour policy gradient methods reinforcement learning function approximation. advances neural information processing systems volume hado hasselt arthur guez david silver. deep reinforcement learning double qlearning. proceedings thirtieth aaai conference artiﬁcial intelligence harm seijen hado hasselt shimon whiteson marco wiering. theoretical empirical analysis expected sarsa. ieee symposium adaptive dynamic programming reinforcement learning ieee yin-hao wang tzuu-hseng chih-jui lin. backward q-learning combination sarsa algorithm q-learning. engineering applications artiﬁcial intelligence ziyu wang schaul matteo hessel hado hasselt marc lanctot nando freitas. dueling network architectures deep reinforcement learning. proceedings international conference machine learning demonstrate tabular case bellman residual induced q-values pgql updates converges zero temperature decreases guarantee vanilla regularized policy gradient notation policy ﬁxed point pgql updates i.e. induced q-value function qπα. first note apply argument show limα→ ˜qπα ˜qπα secondly equation write ˜qπα ˜qπα combining facts", "year": 2016}