{"title": "DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in  Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Deep neural networks have become widely used, obtaining remarkable results in domains such as computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, and bio-informatics, where they have produced results comparable to human experts. However, these networks can be easily fooled by adversarial perturbations: minimal changes to correctly-classified inputs, that cause the network to mis-classify them. This phenomenon represents a concern for both safety and security, but it is currently unclear how to measure a network's robustness against such perturbations. Existing techniques are limited to checking robustness around a few individual input points, providing only very limited guarantees. We propose a novel approach for automatically identifying safe regions of the input space, within which the network is robust against adversarial perturbations. The approach is data-guided, relying on clustering to identify well-defined geometric regions as candidate safe regions. We then utilize verification techniques to confirm that these regions are safe or to provide counter-examples showing that they are not safe. We also introduce the notion of targeted robustness which, for a given target label and region, ensures that a NN does not map any input in the region to the target label. We evaluated our technique on the MNIST dataset and on a neural network implementation of a controller for the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our approach identified multiple regions which were completely safe as well as some which were only safe for specific labels. It also discovered several adversarial perturbations of interest.", "text": "abstract. deep neural networks become widely used obtaining remarkable results domains computer vision speech recognition natural language processing audio recognition social network ﬁltering machine translation bio-informatics produced results comparable human experts. however networks easily fooled adversarial perturbations minimal changes correctly-classiﬁed inputs cause network misclassify them. phenomenon represents concern safety security currently unclear measure network’s robustness perturbations. existing techniques limited checking robustness around individual input points providing limited guarantees. propose novel approach automatically identifying safe regions input space within network robust adversarial perturbations. approach data-guided relying clustering identify well-deﬁned geometric regions candidate safe regions. utilize veriﬁcation techniques conﬁrm regions safe provide counter-examples showing safe. also introduce notion targeted robustness which given target label region ensures input region target label. evaluated technique mnist dataset neural network implementation controller next-generation airborne collision avoidance system unmanned aircraft networks approach identiﬁed multiple regions completely safe well safe speciﬁc labels. also discovered several adversarial perturbations interest. recent years advances deep neural networks enabled representation modeling complex non-linear relationships. paper study common classiﬁers take complex high dimensional input pass multiple layers transformations ﬁnally assign speciﬁc output label class. classiﬁers used variety applications including pattern analysis image classiﬁcation speech audio recognition self-driving cars; expected trend continue intensify neural networks also integrated safety-critical systems require high assurance guarantees. usefulness neural networks evident observed stateof-the-art networks highly vulnerable adversarial perturbations given correctlyclassiﬁed input possible input similar assigned different label instance image-recognition networks possible small amount noise image change classiﬁed network. worse still adversarial examples also found transfer across networks making possible attack networks black-box fashion without access weights. recent work demonstrated attacks carried practice vulnerability neural networks adversarial perturbations thus safety security concern essential explore systematic methods evaluating improving robustness neural networks attacks. date researchers mostly focused efﬁciently ﬁnding adversarial perturbations around select individual input points. problem typically cast optimization problem given network input minimizing words goal input close possible labeled differently. finding optimal solution optimization problem computationally difﬁcult various approximation approaches proposed. approaches gradient based whereas others optimization techniques also techniques focus generating targeted attacks adversarial perturbations result network classifying perturbed input speciﬁc target label various approaches ﬁnding adversarial perturbations successfully demonstrated weakness many state-of-the-art networks; however approaches operate individual input points unclear apply large input domains unless brute-force enumeration input values infeasible input domains. furthermore inherently incomplete techniques provide robustness guarantees fail adversarial input. orthogonal approaches also proposed training networks robust adversarial perturbations these provide formal assurances formal methods provide promising providing guarantees. recent approaches tackle neural network veriﬁcation casting solving problem. although typically slower aforementioned techniques veriﬁcation provide sound assurances adversarial examples exist within given input domain. still techniques provide guidance select meaningful regions within network expected behave consistently. although possible formulate naive notion global robustness checks points similar label inefﬁcient check also fails hold points legitimate boundaries regions. recent work uses reluplex check reﬁned version local global robustness conﬁdence score lables close inputs checked within acceptable parameter potentially handle situations approach. propose novel technique deepsafe formally evaluating robustness deep neural networks. notion underlying approach data-guided methodology determine regions likely safe enables characterizing behavior network partitions input space turn makes network’s behavior amenable analysis veriﬁcation. technique automatically perform following steps propose novel clustering algorithm automatically partition input domain regions points likely true label; regions checked robustness using existing veriﬁcation tool; veriﬁcation checks targeted robustness which given speciﬁc incorrect label guarantees input region mapped label; region result targeted veriﬁcation either safe adversarial example demonstrating unsafe; robustness target labels indicates region completely safe points within region mapped correct label. thus decompose robustness requirement number local proof obligations region. approach provides several beneﬁts discover input regions likely robust candidates safety checks; region found safe provide guarantees w.r.t points within region individual points previous techniques; discovered regions improve scalability formal veriﬁcation focusing search adversarial examples input space deﬁned region. note also regions used improve scalability above-mentioned approximate techniques similarly restricting search input points conﬁned region. usual notion safety might strong many introduce concept targeted robustness analogous targeted adversarial perturbations region input space safe w.r.t. speciﬁc target label indicates within region network guaranteed robust misclassiﬁcation speciﬁc target label. therefore even region network completely robust adversarial perturbations give guarantees safe speciﬁc targeted attacks. simple example consider used perception autonomous classiﬁes images semaphore green yellow. want guarantee never classify image green light light vice versa tolerable misclassify green light yellow still avoiding trafﬁc violations. supervised clustering algorithm kmeans modiﬁed purposes. output technique dense clusters within input space contains training inputs close known share output label. well-deﬁned safe regions cluster deﬁnes subset inputs ﬁnite number belong training data. point network expected display consistent behavior entire cluster. hence clusters considered safe regions adversarial perturbations exist backed data-guided rationale. therefore searching adversarial perturbations within cluster-based safe regions higher chance producing valid examples i.e. inputs whose misclassiﬁcation considered erroneous network behavior. size location boundaries clusters depend distribution training data help guide search instance often make sense search adversarial perturbations inputs boundaries clusters belonging different labels adversarial perturbations areas likely constitute acceptable network behavior. scalable veriﬁcation within cluster formal veriﬁcation prove network robust adversarial perturbations. veriﬁcation even simple neural networks np-complete problem difﬁcult practice. focusing clusters means veriﬁcation applied small input domains making feasible rendering approach whole scalable. further veriﬁcation separate clusters done parallel increasing scalability even further. targeted robustness approach focuses determining targeted safe regions analogous targeted adversarial perturbations. region input space safe w.r.t. speciﬁc target label indicates within region network guaranteed input speciﬁc target label. therefore even network completely robust adversarial perturbations able give guarantees safe speciﬁc targeted attacks. proposed basic approach additional interesting applications. example clusters used additional forms analysis e.g. determining whether network particularly susceptible speciﬁc kinds perturbations certain regions input space. also clustering approach black-box sense relies solely data distribution determine safe regions consequently applicable wide range networks various topologies architectures. finally later demonstrate straightforward incorporate approach user-speciﬁc information regarding adversarial perturbations encountered practice. helps guide search towards ﬁnding valid perturbations. remainder paper organized follows. section provide needed background clustering neural networks neural network veriﬁcation. section describe detail steps clustering-based approach followed evaluation section possible limitations approach discussed section related work discussed section conclude section focus particularly popular clustering algorithm called kmeans given data-points desired number clusters algorithm partitions points clusters variance within cluster minimal. metric used calculate distance points customizable typically euclidean distance manhattan distance points kmeans clustering iterative reﬁnement algorithm starts random points considered means clusters. iteration comprises mainly steps assign data-point cluster whose centroid closest respect chosen distance metric; re-calculate means clusters serve centroids. iterations continue assignment data-points clusters change. indicates clusters satisfy constraint variance within cluster minimal data-points within cluster closer points outside cluster. neural networks deep belief networks used pattern analysis image classiﬁcation speech/audio recognition perception modules self-driving cars typically objects domains high dimensional number classes objects need classiﬁed also high classiﬁcation functions tend highly non-linear input space. deep learning operates underlying rationale groups input parameters could merged derive higher level abstract features enable discovery linear continuous classiﬁcation function. neural networks often used classiﬁers meaning assign input output label/class. neural network thus regarded function assigns input output label denoted internally neural networks comprised multiple layers nodes called neurons node reﬁnes extracts information values computed nodes previous layer. typical layer neural network would consist following; ﬁrst layer input layer takes input variables second layer hidden layer neurons computes weighted input variables using unique weight vector bias value applies non-linear activation function result. last layer output layer uses softmax function make decision class input based values computed previous layer. neural networks trained tested ﬁnite sets inputs outputs expected generalize well previously-unseen inputs. seems work many cases network question designed part safety-critical system wish verify formally certain properties hold possible input. traditional veriﬁcation techniques often cannot directly applied neural networks sparked line work focused transforming problem format amenable existing tools solvers approach general sense could coupled veriﬁcation technique evaluation purposes used recently-proposed reluplex approach reluplex sound complete simplex-based veriﬁcation procedure speciﬁcally tailored achieve scalability deep neural networks. intuitively algorithm operates eagerly solving linear constraints posed neural network’s weighted sums attempting satisfy non-linear constraints posed activation functions lazy manner. often allows reluplex safely disregard many non-linear constraints bulk problem’s complexity stems from. reluplex used evaluating techniques ﬁnding defending adversarial perturbations also successfully applied real-world family deep neural networks designed operate controllers next-generation airborne collision avoidance system unmanned aircraft deepsafe approach section describe greater detail steps proposed approach clustering training inputs; cluster analysis; cluster veriﬁcation; processing possible adversarial examples. approach kmeans clustering algorithm perform clustering training inputs. training inputs mean inputs whose correct labels output classes known includes training validation test sets. note technique works even absence training data e.g. applying clustering randomly generated inputs labeled according given trained network. user need check labels valid. kmeans approach typically unsupervised technique meaning clustering based purely similarity data-points themselves depend labels. here however labels guide clustering algorithm generating clusters consistent labeling modiﬁed clustering algorithm starts setting number clusters input kmeans algorithm equal number unique labels. clusters obtained check whether cluster contains inputs label. kmeans applied cluster found contain multiple labels number unique labels within cluster. effectively breaks problematic cluster multiple sub-clusters. process repeated clusters contain inputs share single label. number clusters input parameter kmeans algorithm often chosen arbitrarily. approach take guidance training data customize number clusters domain consideration. pseudocode matlab implementation algorithm appears fig. every iteration count.csv maps index input dataset clusterindex.csv number unique labels instances correspond clusters whose instances correspond label named clusterfinalindex.csv. consider example training data labeled either stars circles. training data point characterised dimensions/attributes original kmeans algorithm partition training inputs groups purely based proximity w.r.t. attributes however groups stars circles together. modiﬁed algorithm creates partitions ﬁrst iteration however since cluster satisfy invariant contains training inputs label proceeds iteratively divide cluster subclusters invariant satisﬁed. creates clusters shown label star label circle. example typical domains image classiﬁcation even small change attribute values certain inputs could change label. modiﬁed clustering algorithm typically produces small dense clusters consistently-labeled inputs. underlying assumption approach cluster therefore constitutes safe region inputs labeled consistently. instance example algorithm creates relatively small clusters stars circles separating other. searching within clusters represent small neighborhoods inputs belonging label yield meaningful results searching regions within arbitrary distance input. clusters generated kmeans characterized centroid radius indicating maximum distance instance cluster centroid. inputs deep within cluster expected labeled consistently boundaries clusters low-density regions could different labels. order improve accuracy approach shrink clusters replacing average distance instance centroid denoted increases likelihood points within cluster consistently labeled deviation would constitute valid adversarial perturbation. training inputs within cluster label refer label cluster summarize main hypothesis behind approach immediately follows hypothesis point cluster assigned different label network constitutes adversarial perturbation. adversarial input example could follows; hypothetical process input transformation incorrectly bring inputs within cluster stars close circles thereby classifying circle. distance metric. similarity inputs within cluster determined distance metric used calculating proximity inputs. therefore important choose distance metric generates acceptable levels similarity domain consideration. approach assumes every input characterized attributes considered point euclidean space. euclidean distance commonly used metric measuring proximity points euclidean space. however recent studies indicate usefulness euclidean distance determining proximity points diminishes dimensionality increases manhattan distance found capture proximity accurately high dimensions. therefore experiments distance metric depending dimensionality input space distance metrics easily accommodated approach. clusters obtained result previous step characterize behavior network large chunks input space. analysis clusters provide useful insights regarding behavior accuracy network. listed cluster properties approach. density deﬁne density cluster number instances unit distance within cluster. cluster points average distance points centroid cluster’s density deﬁned n/r. cluster high density contains large number instances close proximity assume clusters hypothesis holds since seems undesirable network assign different label inputs within average distance centroid. however cannot said regarding cluster density encompasses fewer inputs belonging label spread out. therefore purpose robustness check disregard clusters density order increase chances examining valid safe regions hence detecting valid adversarial perturbations. centroid behavior centroid cluster considered representative behavior network cluster especially cluster’s density high. classiﬁer neural network assigns input score possible label representing network’s level conﬁdence input label intuitively targeted adversarial perturbations likely exist e.g. label whose level conﬁdence centroid second highest least likely label. approach label scores look targeted safe regions regions input space within network guaranteed robust misclassiﬁcation speciﬁc target label. identiﬁed analyzed clusters next reluplex tool verify formula representing negation hypothesis done targeted manner label basis. negated hypothesis shown hold region indeed safe respect label otherwise reluplex provides satisfying assignment constitutes valid adversarial perturbation. encoding shown here represents input point represent centroid radius label cluster respectively. represents label reluplex models network without ﬁnal softmax layer networks’ outputs correspond levels conﬁdence network assigns possible labels; score denote level conﬁdence assigned label point intuitively formula holds given exists point within distance assigned higher conﬁdence consequently property hold every within cluster score higher ensures targeted robustness network label network guaranteed misclassify input within region target label note overlapping clusters different labels uncertainty regarding desired labels clusters’ intersection. method reducing clusters’ radius serve exclude regions. property checked sequentially every possible denotes possible labels. property unsatisﬁable ensures complete robustness network inputs within cluster; i.e. network guaranteed misclassify input within region label. expressed formally shown below case many smt-based solves reluplex typically solves satisﬁable queries quickly unsatisﬁable ones. therefore order optimize performance test possible target labels descending order scores assigned centroid score). intuitively label nd-highest score likely yield satisﬁable query etc. distances reluplex. reluplex takes input conjunction linear equations certain piecewise-linear constraints. consequently straightforward model neural network query ability encode distance constraint equation depends distance metric used. piecewise linear encoded unfortunately cannot. dealing domains distance better measure proximity thus following approximation. perform clustering phase using distance metric described before cluster obtain radius verifying property however norm. cenl cenl guaranteed veriﬁcation conducted within cluster consideration adversarial perturbation discovered thus valid. veriﬁcation shows network robust however holds portion cluster checked. limitation waived using veriﬁcation technique directly supports enhancing reluplex support clusters scalability. main source computational complexity neural network veriﬁcation presence non-linear non-convex activation functions. however restricted small sub-domain input space functions present purely linear behavior case disregarded replaced linear constraint greatly simpliﬁes problem. consequently performing veriﬁcation within small domains beneﬁcial many activation functions often disregarded. approach naturally involves veriﬁcation queries small clusters tends helpful regard. reluplex built-in bound tightening functionality detect cases; leverage functionality computing lower upper bounds input variables within cluster provide part query reluplex. approach lends scalable veriﬁcation also parallelization. cluster involves stand-alone veriﬁcation queries veriﬁcation performed parallel. also checked independently every queries also performed parallel expediting process even processing possible adversarial perturbations solution target label indicates presence input within region network assigns higher score label check validity adversarial example needs done user/domain expert. note mean highest score; i.e. need targeted adversarial example cases speciﬁc constraints inputs considered valid adversarial examples. able successfully model domain-speciﬁc constraints acas generate valid adversarial perturbations evaluated proof-of-concept implementation deepsafe networks. ﬁrst network designed controller next-generation airborne collision avoidance system unmanned aircraft highly safety-critical system. second network digit classiﬁer popular mnist image dataset. acas family collision avoidance systems aircraft currently development federal aviation administration acas version unmanned aircraft control. intended airborne receive sensor information regarding drone nearby intruder drones issue horizontal turning advisories aimed preventing collisions. input sensor data includes distance ownship intruder; angle intruder relative ownship heading direction; heading angle intruder relative ownship heading direction; vown speed ownship; vint speed intruder; time loss vertical separation; aprev previous advisory. illustration. possible output actions follows clear-of-conﬂict weak right weak left strong right strong left. advisory assigned score lowest score corresponding best action. currently exploring implementation acas uses array deep neural networks. networks obtained discretizing parameters aprev network contains input dimensions treats aprev constants. applying approach. evaluated techniques acas networks. hidden layers relu activation nodes layer. network inputs known labels comprised dimensions classiﬁed output labels representing possible turning advisory unmanned aircraft. kmeans clustering performed using distance metric. yielded clusters input single-input clusters. clusters sorted descending order density. reluplex dense clusters timeout hours cluster. unsatisﬁable answer successfully obtained possible labels clusters deemed safe remaining clusters unsatisﬁable answer obtained labels solver timing rest additional clusters solver timed without returning concrete result labels. processing adversarial examples using domain-speciﬁc knowledge clusters reluplex detected adversarial perturbations inputs within cluster classiﬁed network belonging different class cluster label. however every example considered valid i.e. invalid sense constitute incorrect network behavior. consulted developers networks provided additional criterion order guide search towards perturbations likely valid adversarial perturbation share values vown vint parameters nearby correctly-classiﬁed inputs. incorporated information follows given initial check returns solution relulex perform additional checks. solves equation constrained domain. ﬁrst check slice cluster obtain centroid plane last dimensions ﬁxed values centroid. radius value used before. second check slice obtain maximum plane last dimensions ﬁxed -valued tuple appears maximum number times cluster. radius altered search space remains within cluster boundaries; obtained applying pythagoras theorem using radius distance centroid-plane maximum-plane w.r.t last three dimensions. ﬁrst parameters allowed vary before. clusters resulted unsat. rest adversarial examples found centroid plane and/or maximum planes. valid adversarial examples. acas domain experts stated acas fairly robust system adversarial examples found small-range border cases considered interesting crucial enough. however conﬁrmed valid adversarial examples nevertheless performed using distance metric. yielded clusters inputs single-input clusters. clusters sorted descending order density. reluplex clusters timeout hours cluster. explained section analysis targeted label different expected label check initiated search adversarial example received higher score search yielded adversarial examples target labels clusters save highest-density ones. clusters adversarial examples detected labels except lowest score adversarial examples validated manually converting images. results surprising mnist networks known susceptible adversarial perturbations adversarial examples shown fig. invalid adversarial examples validity adversarial perturbation depends accuracy cluster identifying regions input space ideally given label. clustering used approach typically generates small dense groups small neighborhoods around known inputs. hand attempts abstract input focusing certain features others order able assign unique label abstraction also enables generalization network inputs part training data. however process tends make network inaccurate inputs close neighborhoods known inputs. therefore although clustering cannot considered alternative classiﬁer input considered accurate oracle close neighborhoods known inputs. however impacted presence noise input space irrelevant attributes sieves out. invalid safety regions could scenario cluster network agree labels inputs within region however ideally need classiﬁed different label. happen training data representative enough. generalization experimental results current implementation solver reluplex used prototype tool supports piecewise-linear activation functions. could limit generalization experimental results networks types activation functions. vulnerability neural networks adversarial perturbations ﬁrst discovered szegedy model problem ﬁnding adversarial example constrained minimization problem. goodfellow introduced fast gradient sign method crafting adversarial perturbations using derivative models loss function respect input feature vector. show trained mnist cifar- classiﬁcation tasks fooled high success rate. extension approach applies technique iterative manner jacobian-based saliency attack proposed method targeted misclassiﬁcation exploiting forward derivative adversarial perturbation force model misclassify speciﬁc target class. carlini recently proposed approach could resisted state-ofthe-art networks using defensive distillation. optimization algorithm uses better loss functions parameters uses three different distance metrics. deepfool technique simpliﬁes domain considering network completely linear. compute adversarial inputs tangent plane point classiﬁer function. introduce non-linearity model repeat process true adversarial example found. deep learning veriﬁcation approach deﬁnes region safety around known input applies solving checking robustness. consider input space discretized alter input using manipulations minimal distance original generate possibly-adversarial inputs. guarantee freedom adversarial perturbations within discrete points explored. clustering approach potentially improve technique constraining discrete search within regions. paper presents novel data-guided technique search adversarial perturbations within well-deﬁned geometric regions input space correspond clusters similar inputs known share label. approach identiﬁes provides proof regions safety input space within network robust respect target labels. preliminary experiments acas mnist datasets highlight potential approach providing formal guarantees robustness neural networks scalable manner. re-training number single-input clusters clusters cardinality could indicate cases density enough training data region; density high noise number redundant attributes leads repeated splitting clusters. ﬁrst case could feedback re-training. second case indicator clustering probably carried higher layer abstraction smaller number relevant attributes. input techniques boundaries cluster spheres formed kmeans density areas. therefore network could assumed accuracy around boundaries. thus adversarial robustness checks around instances closer edge clusters could exhibit high number adversarial perturbations. analysis could help identify potential inputs techniques assessing local robustness could applied. solvers implementation used reluplex perform veriﬁcation approach general tools back-end solver. checking robustness deep neural networks active area research plan investigate integrate solvers become available. also plan investigate testing guided computed regions alternative veriﬁcation increased scalability price losing formal guarantees.", "year": 2017}