{"title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained.  In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an \"Option Graph\" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.", "text": "autonomous driving multi-agent setting host vehicle must apply sophisticated negotiation skills road users overtaking giving merging taking left right turns pushing ahead unstructured urban roadways. since many possible scenarios manually tackling possible cases likely yield simplistic policy. moreover must balance unexpected behavior drivers/pedestrians time defensive normal trafﬁc maintained. paper apply deep reinforcement learning problem forming long term driving strategies. note major challenges make autonomous driving different robotic tasks. first necessity ensuring functional safety something machine learning difﬁculty given performance optimized level expectation many instances. second markov decision process model often used robotics problematic case unpredictable behavior agents multi-agent scenario. make three contributions work. first show policy gradient iterations used variance gradient estimation using stochastic gradient ascent minimized without markovian assumptions. second decompose problem composition policy desires trajectory planning hard constraints goal desires enable comfort driving hard constraints guarantees safety driving. third introduce hierarchical temporal abstraction call option graph gating mechanism signiﬁcantly reduces effective horizon thereby reducing variance gradient estimation even further. option graph plays similar role structured prediction supervised learning thereby reducing sample complexity also playing similar role lstm gating mechanisms used supervised deep networks. endowing robotic ability form long term driving strategies referred driving policy enabling fully autonomous driving. process sensing i.e. process forming environmental model consisting location moving stationary objects position type path delimiters drivable paths semantic meaning trafﬁc signs trafﬁc lights around well deﬁned. sensing well understood deﬁnition driving policy underlying assumptions functional breakdown less understood. extent challenge form driving strategies mimic human drivers underscored ﬂurry media reports simplistic driving policies exhibited current autonomous test vehicles various practitioners order support autonomous capabilities robotic driven vehicle adopt human driving negotiation skills overtaking giving merging taking left right turns pushing ahead unstructured urban roadways. since many possible scenarios manually tackling possible cases likely yield simplistic policy. moreover must balance unexpected behavior drivers/pedestrians time defensive normal trafﬁc maintained. challenges naturally suggest using machine learning approaches. traditionally machine learning approaches planning strategies studied framework reinforcement learning general overview comprehensive review reinforcement learning robotics. using machine learning speciﬁcally raises concerns address paper. ﬁrst ensuring functional safety driving policy something machine learning difﬁculty given performance optimized level expectation many instances. namely given probability accident guarantee safety scaling variance parameters estimated sample complexity learning problem degree becomes unwieldy solve. second markov decision process model often used robotics problematic case unpredictable behavior agents multi-agent scenario. explaining approach tackling difﬁculties brieﬂy describe idea behind common reinforcement learning algorithms. typically performed sequence consecutive rounds. round agent observes state represents sensing state system i.e. environmental model mentioned above. decide action performing action agent receives immediate reward moved state st+. goal planner maximize cumulative reward planner relies policy maps state action. algorithms rely another mathematically elegant model markov decision process pioneered work bellman markovian assumption distribution fully determined given yields closed form expression cumulative reward given policy terms stationary distribution states mdp. stationary distribution policy expressed solution linear programming problem. yields families algorithms optimizing respect primal problem called policy search optimizing respect dual problem whose variables called value function value function determines expected cumulative reward start initial state pick actions according related quantity state-action value function determines cumulative reward start state immediately pick action pick actions according function gives rise crisp characterization optimal policy particular shows optimal policy deterministic function sense advantage model allows couple future present using function. given state value tells effect performing action moment entire future. therefore function gives local measure quality action thus making problem similar supervised learning. reinforcement learning algorithms approximate function function another. value iteration algorithms e.g. learning algorithm relies fact functions optimal policy ﬁxed points operators derived bellman’s equation. actor-critic policy iteration algorithms learn policy iterative iteration critic estimates based this actor improves policy. despite mathematical elegancy mdps conveniency switching function representation several limitations approach. first noted usually robotics able approximate notion markovian behaving state. furthermore transition states depends agent’s action also actions players environment. example context autonomous driving dynamic autonomous vehicle clearly markovian next state depends behavior road users necessarily markovian. possible solution problem partially observed mdps still assume markovian state observation distributed according hidden state. direct approach considers game theoretical generalizations mdps example stochastic games framework. indeed algorithms mdps generalized multi-agents games. example minimax-q learning nash-q learning approaches stochastic games explicit modeling players goes back brown’s ﬁctitious play vanishing regret learning algorithms also noted learning multi-agent setting inherently complex single agent setting. taken together context autonomous driving given unpredictable behavior road users framework extensions problematic least could yield impractical algorithms. comes categories algorithms handle markov assumption divide four groups algorithms estimate value function clearly deﬁned solely context policy based learning methods where example gradient policy estimated using likelihood ratio trick thereby learning iterative process iteration agent interacts environment acting based current policy estimation. policy gradient methods derived using markov assumption later necessarily required. algorithms learn dynamics process namely function takes yields distribution next state st+. known model-based methods clearly rely markov assumption. behavior cloning methods. imitation approach simply requires training examples form action human driver supervised learning learn policy clearly markov assumption involved process. problem imitation different human drivers even human deterministic policy choices. hence learning function small often infeasible. small errors might accumulate time yield large errors. ﬁrst observation policy gradient really require markov assumption furthermore methods reducing variance gradient estimator would require markov assumptions well. taken together algorithm could initialized imitation updated using iterative policy gradient approach without markov assumption. second contribution paper method guaranteeing functional safety driving policy outcome. given small probability accident corresponding reward trajectory leading accident much smaller thus generating high variance gradient estimator regardless means reducing variance detailed sec. variance gradient depends behavior reward also horizon required making decisions. proposal functional safety twofold. first decompose policy function composition policy desires trajectory planning hard constraints goal desires enable comfort driving hard constraints guarantees safety driving second following options mechanism employ hierarchical temporal abstraction call option graph gating mechanism signiﬁcantly reduces effective horizon thereby reducing variance gradient estimation even option graph plays similar role structured prediction supervised learning thereby reducing sample complexity also playing similar role lstm gating mechanisms used supervised deep networks. options skill reuse also recently studied hierarchical deep networks skill reuse proposed. finally sec. demonstrate application algorithm double merging maneuver notoriously difﬁcult execute using conventional motion path planning approaches. safe reinforcement learning also studied recently approach involves ﬁrst optimizing expected reward applying projection solution onto linear constraints. approach different approach. particular assumes hard constraints safety expressed linear constraints parameter vector case weights deep network hard constraints involve highly non-linear dependency therefore convex-based approaches applicable problem. begin setting notations geared towards deriving policy gradient method variance reduction making markov assumptions. follow reinforce likelihood ratio trick make modest contribution observation novel derivation markov assumptions environment required. state space contains environmental model around vehicle generated interpreting sensory information additional useful information kinematics moving objects previous frames. term state space order introduce terminology actually mean state vector agnostic sense without markov assumptions simply collection information around vehicle generated particular time stamp. denote action space point keep abstract later sec. introduce speciﬁc discrete action space selecting desires tailored domain autonomous driving. hypothesis class parametric stochastic policies denoted assume differentiable w.r.t. note chosen class policies part architectural design choice i.e. action time determined agnostic state particular given differentiability policy implemented deep layered network. words claiming optimal policy necessarily contained hypothesis class good enough policies modeled using deep network whose input layer consists theory depend nature hypothesis class design choices substituted example would correspond class recurrent neural networks deﬁne sequence state-action time period sufﬁcient long-term planning ¯sij denote sub-trajectory time stamp time stamp probability trajectory actions chosen according policy assumptions environment. total reward associated trajectory denoted function example discounted γtrt reward function used therefore gradient policy theorem follows standard likelihood ratio trick formula well known proof make observation markov assumptions environment required validity policy gradient estimator gradient policy theorem shows possible obtain unbiased estimate gradient expected total reward thereby using noisy gradient estimates stochastic gradient ascent/descent algorithm training deep network representing policy unfortunately variance gradient estimator scales unfavorably time horizon moreover probability critical corner cases probability accident immediate reward must satisfy turn variance random variable grows i.e. much larger high variance gradient detrimental effect convergence rate given nature problem domain extremely low-probability corner cases effect extremely high variance could bring policy solutions. approach variance problem along three thrusts. first base-line subtraction methods variance reduction. second deal variance corner cases decomposing policy learnable part non-learnable part latter induces hard constraints functional safety. last introduce temporal abstraction method gating mechanism call option graph ameliorate effect time horizon variance. section focus base-line subtraction derive optimal baseline generalize recent results non-markovian setting. next section deal variance corner cases. written matrix log)∇θi log) dimensional vector r∇θi log)∇θi log). estimate mini-batch episodes x−y. efﬁcient approach think problem ﬁnding baseline online linear regression problem separate process update online manner many policy gradient variants replace q-function assumes markovian setting. following lemma gives non-markovian analogue function. satisﬁes conditions lemma therefore also replace analogue so-called advantage function advantage function generalization often used actor-critique policy gradient implementations non-markovian setting considered paper advantage function complicated estimate therefore experiments estimators involve term estimated using online linear regression. previous section shown optimize reinforcement learning objective policy stochastic gradient ascent. recall deﬁned objective e¯s∼pθ expected reward. objectives involve expectation common machine learning. argue objective poses functional safety problem. consider reward function trajectories represent rare corner event would like avoid accident rest trajectories. concreteness suppose goal learn perform overtake maneuver. normally accident free trajectory would reward successful smooth takeovers penalize staying lane without completing takeover hence range sequence represents accident would like reward provide sufﬁciently high penalty discourage occurrences. question value ensure accident-free driving? observe effect accident additive term probability mass trajectories accident event. term negligible i.e. learner might prefer policy performs accident order fulﬁll takeover maneuver successfully often policy would defensive expense takeover maneuvers completed successfully. words want make sure probability accidents must since would like extremely small obtain must extremely large. recall policy gradient estimate gradient following lemma shows variance random variable grows larger hence even estimating objective difﬁcult alone gradient. lemma policy scalars probability probability then last approximation holds case discussion shows objective form cannot ensure functional safety without causing serious variance problem. baseline subtraction method variance reduction would offer sufﬁcient remedy problem would shifting problem high variance equally high variance baseline constants whose estimation would equally suffer numerical instabilities. moreover probability accident average sample least sequences obtaining accident event. immediately implies lower bound samples sequences learning algorithm aims minimizing therefore face fundamental problem whose solution must found architectural design formalism system rather numerical conditioning tricks. approach based notion hard constraints injected outside learning framework. words decompose policy function learnable part nonlearnable part. formally structure policy function maps state space desires maps desires trajectory function responsible comfort driving making strategical decisions cars over-taken given desired position host within lane forth. mapping state desires policy learned experience maximizing expected reward. desires produced translated cost function driving trajectories. function learned implemented ﬁnding trajectory minimizes aforementioned cost subject hard constraints functional safety. decomposition allows always ensure functional safety time enjoying comfort driving time. illustrate idea consider challenging driving scenario call double merge scenario double merge vehicles approach merge area left right sides side vehicle decide whether merge side not. successfully executing double merge busy trafﬁc requires signiﬁcant negotiation skills experience notoriously difﬁcult execute heuristic brute force approach enumerating possible trajectories could taken agents scene. begin deﬁning desires appropriate double merge maneuver. cartesian product following sets desired target speed host vehicle desired lateral position lanes units whole numbers designate lane center fraction numbers designate lane boundaries classiﬁcation labels assigned vehicles. vehicles assigned host vehicle give take maintain offset distance next describe translate desires cost function driving trajectories. driving trajectory represented location time experiments .sec cost assigned trajectory weighted individual costs assigned desired speed lateral position label assigned vehicles. individual costs descried below. figure double merge scenario. vehicles arrive left right side merge area. vehicles continue road vehicles merge side. dense trafﬁc vehicles must negotiate right way. given desired lateral position cost associated desired dist dist distance point lane position cost vehicles vehicle predicted trajectory host vehicle egocentric units earliest point exists distance small classiﬁed give-way would like meaning arrive trajectory intersection point least seconds vehicle arrive point. possible formula translating constraint cost .]+. likewise classiﬁed take-way would like translated cost .]+. finally classiﬁed offset would like translated cost penalizing distance trajectories. assigning weight aforementioned costs obtain single objective function trajectory planner naturally also objective cost encourages smooth driving. importantly hard constraints ensure functional safety trajectory. example allow roadway allow close summarize decompose policy mapping agnostic state desires mapping desires actual trajectory. latter mapping learned implemented solving optimization problem whose cost depends desires whose hard constraints guarantees functional safety policy. left explain learn mapping agnostic state desires topic next section. previous section injected prior knowledge order break problem ensure functional safety. alone system complying functional safety suffer high unwieldy variance reward ﬁxed splitting problem formulation mapping state space desires using policy gradient iterations followed mapping actual trajectory involve learning. necessary however inject even prior knowledge problem decompose decision making semantically meaningful components reasons. first size might quite large even continuous o}n). second gradient estimator involves ∇θπθ. mentioned above variance grows time horizon case value roughly high enough create signiﬁcant variance. approach follows options framework options graph represents hierarchical decisions organized directed acyclic graph special node called root graph. root node node incoming edges. decision process traverses graph starting root node reaches leaf node namely node outgoing edges. internal node implement policy function picks child among available children. predeﬁned mapping traversals options graph desires words traversal options graph automatically translated desire given node graph denote parameter vector speciﬁes policy picking child concatenation deﬁned traversing root graph leaf node using policy deﬁned pick child node. possible option graph double merge scenario depicted figure root node ﬁrst decides within merging area approaching need prepare cases need decide whether change lane stay lane. decided change lane need decide perform lane change maneuver possible push stay lane. determines desired lateral position natural example change lane lane lane sets desire lateral position stay sets desire lateral position push sets desire lateral position next decide whether keep same speed accelerate decelerate. finally enter chain like structure goes vehicles sets semantic meaning value sets desires semantic meaning vehicles obvious way. note share parameters nodes chain immediate beneﬁt options graph interpretability results. another immediate beneﬁt rely decomposable structure therefore policy node choose small number possibilities. finally structure allows reduce variance policy gradient estimator. next elaborate last point. mentioned previously length episode double merge scenario roughly steps. number comes fact hand would like enough time consequences actions hand dynamic driving must make decisions fast enough frequency options graph enables decrease effective value complementary ways. first given higher level decisions deﬁne reward lower level decisions taking account much shorter episodes. example already picked lane change nodes learn policy assigning semantic meaning vehicles looking episodes seconds second high level decisions need make decisions every seconds. instead either make decisions lower frequency implement option termination function gradient calculated every termination option. cases effective value order magnitude smaller original value. estimator every node depends value order magnitude smaller original steps immediately transfers smaller variance summarize introduced options graph breakdown problem semantically meaningful components desires deﬁned traversal dag. step along learner maps state space small subset desires thereby effectively decreasing time horizon much smaller sequences time reducing output space learning problem. aggregated effect reducing variance sample complexity learning problem. purpose section give sense challenging negotiation scenario handled framework. experiment involves propriety software modules data therefore regarded demonstration rather reproducible experiment. leave future work task conducting reproducible experiment comparison approaches. experimented double-merge scenario described section challenging negotiation task cars sides strong incentive merge failure merge time leads ending wrong side intersection. addition reward associated trajectory needs account success failure merge operation also smoothness trajectory control comfort level vehicles scene. words goal learner succeed merge maneuver also accomplish smooth manner without disrupting driving patterns vehicles. relied following sensing information. static part environment represented geometry lanes free space agent also observes location velocity heading every within meters away finally meters merging area agent receives side merge trajectory planner used optimization algorithm based dynamic programming. used option graph described figure recall deﬁne policy function every node option graph. initialized policy nodes using imitation learning. policy function associated every node option graph represented neural network three fully connected hidden layers. note data collected human driver contains ﬁnal maneuver observe traversal option graph. nodes infer labels data relatively straight forward manner. example classiﬁcation vehicles give-way take-way offset inferred future position host vehicle relative vehicles. remaining nodes used implicit supervision. namely option graph induces probability future trajectories train maximizing probability trajectory chosen human driver. fortunately deep learning quite good dealing hidden variables imitation process succeeded learn reasonable initialization point policy. videos. policy gradient updates used simulator self-play enhancement. namely partitioned agents sets used reference players used policy gradient learning process. learning process converged used reference players used learning process. alternating process switching roles sets continued rounds. resulting videos. haitham ammar rasul tutunov eric eaton. safe policy search lifelong reinforcement learning sublinear regret. journal machine learning research mariusz bojarski davide testa daniel dworakowski bernhard firner beat flepp prasoon goyal lawrence jackel mathew monfort muller jiakai zhang learning self-driving cars. arxiv preprint arxiv. ronen brafman moshe tennenholtz. r-max–a general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research evan greensmith peter bartlett jonathan baxter. variance reduction techniques gradient estimates reinforcement learning. journal machine learning research michael littman. markov games framework multi-agent reinforcement learning. proceedings eleventh international conference machine learning volume pages eric moulines francis bach. non-asymptotic analysis stochastic approximation algorithms machine learning. advances neural information processing systems pages deanna needell rachel ward nati srebro. stochastic gradient descent weighted sampling randomized kaczmarz algorithm. advances neural information processing systems pages john schulman philipp moritz sergey levine michael jordan pieter abbeel. highdimensional continuous control using generalized advantage estimation. arxiv preprint arxiv. richard sutton david mcallester satinder singh yishay mansour policy gradient methods reinforcement learning function approximation. nips volume pages csaba szepesvári. algorithms reinforcement learning. synthesis lectures artiﬁcial intelligence machine learning http//www.ualberta.ca/ ~szepesva/rlbook.html. taskar vassil chatalbashev daphne koller carlos guestrin. learning structured prediction models large margin approach. proceedings international conference machine learning pages chen tessler shahar givony zahavy daniel mankowitz shie mannor. deep hierarchical approach lifelong learning minecraft. arxiv preprint arxiv. note deriving expression make assumptions stands contrast markov decision processes assumed independent past given assumption make choice solely based comes architectural design choice hypothesis space policy functions. remainder proof employs standard likelihood ratio trick observation since depend parameters gets eliminated policy gradient. detailed sake completeness s∼pθ", "year": 2016}