{"title": "RRA: Recurrent Residual Attention for Sequence Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, we propose a recurrent neural network (RNN) with residual attention (RRA) to learn long-range dependencies from sequential data. We propose to add residual connections across timesteps to RNN, which explicitly enhances the interaction between current state and hidden states that are several timesteps apart. This also allows training errors to be directly back-propagated through residual connections and effectively alleviates gradient vanishing problem. We further reformulate an attention mechanism over residual connections. An attention gate is defined to summarize the individual contribution from multiple previous hidden states in computing the current state. We evaluate RRA on three tasks: the adding problem, pixel-by-pixel MNIST classification and sentiment analysis on the IMDB dataset. Our experiments demonstrate that RRA yields better performance, faster convergence and more stable training compared to a standard LSTM network. Furthermore, RRA shows highly competitive performance to the state-of-the-art methods.", "text": "residual attention. figure learning recurrent interaction hidden states apart enhanced residual connections. attention residual connections decides cell look back given timestep meanwhile controls individual contribution previous hidden states. example cell able look back past time steps semantic dependency word girl explicitly captured. contributions away states become zero inﬂuence future states gradient vanishing problem. hand weights matrix large gradient signal grows without bound learning diverges gradient exploding problem. alleviate effects gradient vanishing many methods proposed. long short-term memory seen successful among techniques. introduced memory cell lstm input forget output gates control whether store context information remove memory. allows lstm networks capture long-range relational dependencies input sequences compared regular rnn. gradient vanishing problem limited recurrent neural network also appear feedforward neural network particularly training deep networks. treat unfolded form shallow multiple timesteps equivalent deep network. residual learning provides novel learning scheme ultra-deep convolutional neural network introducing residual connections across layers. shortcut connections connect far-away layers ensure training error signal back-propagated higher layer lower layer directly alleviate gradient vanishing problem. inspired paper propose recurrent neural network residual attention learn long-range dependencies sequential data. propose residual connections across timesteps explicitly enhances interaction current state hidden states several timesteps apart. also allows training errors directly back-propagated residual connections effectively alleviates gradient vanishing problem. reformulate attention mechanism residual connections. attention gate deﬁned summarize individual contribution multiple previous hidden states computing current state. evaluate three tasks adding problem pixel-by-pixel mnist classiﬁcation sentiment analysis imdb dataset. experiments demonstrate yields better performance faster convergence stable training compared standard lstm network. furthermore shows highly competitive performance state-of-the-art methods. deep neural networks shown signiﬁcant improvements several application domains including image recognition natural language processing speech recognition recurrent neural networks particular type powerful capability processing complicated sequential data. using recurrent connections previous context information captured used predict next hidden state output. however training remains difﬁcult task gradient vanishing exploding problems especially needs learn long dependencies sequential inputs. main issue training using back-propagation time entails multiplying gradients large number times weights matrix contains small values gradient copyright association advancement artiﬁcial intelligence rights reserved. success residual learning computer vision tasks work reformulates residual learning recurrent network learning ultra-long range dependencies across timesteps sequence learning. different residual learning identity shortcut connection used input outputs stacked layers context sequence learning residual function) reformulate recurrent residual connection attention multiple precessing steps. results residual function attention across timesteps recurrent model attention weights. timestep computing current state reformulation ensures recurrent units ability look back past timesteps control relative contribution hidden state ht−k− current state even though attention mechanism widely studied machine translation image captioning object detection generative models basically sort attention models either layer-based network-based. allowed receive attended information previous layer separate network. casting attention mechanism recurrent residual connection recurrent unit provides natural sequence learning. explicitly looks back multiple preceding steps automatically decides much previous information seen weighting them. speciﬁc sequential pattern semantic dependencies words apart stronger adjacent words figure gives example intuitively supports assumption. word drawing explicitly involved predicting word obvious word girl would also make signiﬁcant semantic contribution. essentially sentence saying girl beautiful however regular rnns suffer difﬁculties capturing meaning. thus reasonable explicitly consider information several steps apart learning semantic meaning sequential data. work address problem casting attention mechanism residual connection timesteps recurrent network. beneﬁts recurrent residual attention fold enhances interactions hidden states several steps apart allows training error back-propagated across multiple timesteps. attention residual connection gives natural past hidden states selectively attend future states sequence learning. propose novel learning scheme sequential data reformulates residual learning attention recurrent network. code made publicly available soon. gate—attention gate deﬁned lstm proposed shows promising performance compared standard lstm network three benchmark tasks adding problem pixel-by-pixel mnist sentiment analysis. also outperforms matches state-of-the-art methods. rest paper structured follows section gives related work. section elaborate reformation residual learning attention recurrent manner. describe experiments discussions section conclude work section recurrent neural network powerful network architecture processing sequential data. widely used natural language processing speech recognition handwriting recognition recent years. allows cyclical connection reuse weights across different instances neurons associated different time steps. idea explicitly support network learn entire history previous states current states. property able arbitrary length sequence ﬁxed length vector. known difﬁcult training gradient vanishing problem. vanishing problem originally found lstm proposed prevent gradient vanishing training. therefore compare traditional lstm ability learn long-term dependencies inputs outputs. recently lstm became popular ﬁeld machine translation speech recognition sequence learning recently. another special type gated recurrent unit simpliﬁes lstm removing memory cell provides different prevent vanishing gradient problem. work falls category aims alleviate gradient vanishing learning ultra-long dependencies. residual learning previous work proven network depth crucial importance neural network architectures challenging train deeper networks. residual learning paves training networks. residual mapping layers enables networks substantially deep leads efﬁcient optimization importantly yields better performance. short-cut skip connections considered across multiple layers force direct information forward backward passes. this feedforward signals well feedback errors passed easily. adding residual connection across layers shown powerful capability computer vision input sequence target sequence respectively. input sequence length differ target sequence length hidden state model given hidden state input recurrent model standard variants. equation viewed general form recurrent learning algorithm able capture semantic dependencies across timesteps. example hidden state explicitly used outputting past hidden state implicitly involved. challenges existing rnns task needs model explicitly capture long-range semantic dependencies states several timesteps apart task described figure adding shortcut connection skip multiple timesteps enforcing direct information across timesteps explicitly previous hidden states computing future states. entails recurrent residual learning. recurrent residual learning overview reformulating recurrent network residual connection illustrated figure shortcut connection designed impose ﬂuent information across timesteps. residual connection recurrent network given timestep hidden state computed model weights receives regular rnn. keep receive form residual skip connection across timesteps. approximates residual function weights identity function ht−k ht−k hidden state time step. formulation computing hidden state besides ht−k explicitly considered. approximating equation returns back plain rnn. making weight multiple previous hidden states i.e. ht−...ht−k lead recurrent residual learning attention timesteps figure overview proposed methods. standard unfolded form. residual connections. recurrent network attention mechanism layers). residual attention timestep units able look back past states computing current state szegedy inspired this work incorporates residual connection across multiple precessing steps learn long complex dependencies sequential data. attention mechanism attention neural networks designed assign weights different input sequences equally original neural networks seen additional network widely incorporated different neural networks leading variety models formally attention model takes arguments e.g. h...hk context information returns weighted output summaries based related context weights corresponds relevances e.g. weights figure determines relative contributions ﬁnal output. current state-of-the-art attention methods either layer network based well studied recurrent manner. work reformulates attention residual connection recurrent network. section describes proposed approach learn recurrent residual attention sequential data. ﬁrstly introduce existing sequence learning recurrent network explain intuition extending recurrent network learn complex dependencies. describe reformulate residual connection followed casting attention mechanism recurrent residual connection. here lstm base recurrent network elaborate approach easily generalized plain gru. deﬁning attention gate rnns additional differentiable parameters residual connection introduced. optimization realized using standard back-propagation time regular rnns. section explore performance proposed multiple tasks including adding problem pixelby-pixel mnist image classiﬁcation sentiment analysis imdb dataset. implementation based theano. conducted experiments single titan memory. weights input-to-hidden layer hiddento-output layer initialized drawing uniform distribution internal weights orthogonally initialized attention weights randomly initialized. default attention window size means past hidden states considered every timestep. initial learning rate dropout rate used recurrent layer. gradients clipped prevent exploding gradients. models conﬁgured recurrent layer trained given number iterations without early stopping. experimental settings lstm same. adding problem task originally deﬁned testing ability capture long dependencies sequential data. task asked numbers randomly selected sequence. given sequence length element sequence pair consisting components ﬁrst actual number uniformly sampled second indicator decides whether ingore numbers sequence marked addition ﬁrst number placed leads sequence long-range dependency signiﬁcant remote inputs. naive strategy always predict target output regardless input sequences gives expected mean squared error used baseline beat. residual attention considered timestep viewed sliding attention window size timesteps. make past states selectively attend future state enforce residual attention effects memory cell directly gate—attention gate deﬁned lstm cell making lstm residual attention. equation reformulated input forget output gate respectively. memory cell sigmoid function. equations original lstm equation deﬁned attention gate summarizes relative contributions range ht−k−. hidden state used original attended step form residual connection across timesteps. attention weights normalized equation follow residual network length versions pixel-by-pixel mnist considered normal mnist pixel sequence read order left right bottom. pixel sequence randomly permuted. conﬁgured networks hidden units optimizer replaced rmsprop provides steady improvement task networks. training batch size lstm used baseline beat plain proved poor performance tasks figure reports test accuracy iterations. normal pixel-by-pixel mnist similar previous work lstm show good performance. achieves beats lstm besides shows able yield faster convergence stable improvement compared standard lstm. task conﬁgured challenging randomly permuted order pixels image. applying permutation image dependencies across pixels become longer original pixel order. requires models learn remember complicated dependencies across different timesteps. shown figure shows superior capability capturing long complicated dependencies. achieves lstm again faster convergence. compared recent proposed methods irnn urnn table achieves state-of-the-art performance normal permuted pixel-by-pixel mnist. noted urnn able beat lstm normal mnist conﬁgurations. nevertheless achieves sightly better performance normal mnist outperforms lstm permuted mnist certain margin. performance lstm test dataset varied sequence length lstm able consistently beat baseline around iterations approximately beats baseline iterations. increased task gets harder dependency target output relevant sequence inputs becomes remote requires model able capture longer dependencies. ﬁrst iterations lstm struggled minimize started beat baseline iterations signiﬁcantly faster lstm started beat baseline around iterations. pixel-by-pixel mnist task asked classify mnist digits suggested -by- image mnist treated sequential data recurrent network. leads pixel sequences sentiment analysis evaluate performance sentiment analysis conducted experiments imdb review dataset dataset consists movie reviews imdb. dataset split training testing. reviews training reviews labeled rest unlabeled testing reviews labeled. taskwe used labeled training reviews test binary sentiment classiﬁcation thus randomly guessing yields accuracy. different previous approaches e.g. bag-of-words latent dirichlet allocation etc. review sentences treated sequential data. task particularly challenging average review length longest review reach words. requires model strong ability capture longrange semantic dependencies among words. adadelta optimizer batch size tested different attention window size figure presents test error iterations original lstm different model trained around epochs without early stopping. dataset quite well since iterations considerably faster lstm. varied attention window size found test error sensitive different obtains sightly better results conjecture certain pattern sequence semantic contributions previous hidden states sufﬁcient compute current state. order compare recent methods recently reported baselines. table shows performance comparison. proves effectively learn good representations input word sequence sentiment classiﬁcation compared previous nonsequential representations e.g. classiﬁers. also highly competitive recent approaches lm-lstm sa-lstm noted models solely based proposed hidden units without using additional unlabeled data pre-training well wordvec embeddings. bidirectional performance model also visualized attention weights case respectively figure evolution normalized weights attention units suggests attention gate learns control relative contributions previous hidden states ht−k−. explicitly considered predicting contrast standard rnn/lstm variants history information indirectly considered ht−. discussion alleviates gradient vanishing bptt gradient vanishing gradient close sums gradient contribution every timestep dependency across timesteps cannot captured gradient contribution explicitly enforces short-cut connection across timestep directly passes error signal ht−k. attention residual connection enables control relative contribution across multiple timesteps alleviate gradient become particularly learning dependencies long complex sequence. experiments figure demonstrated stability learning long complex sequence. relation related work variants recently proposed address gradient vanishing problem recurrent networks. irnn composed relus initialized identity weight matrix urnn uses unitary hidden-to-hidden matrix generalizing orthogonal matrices complex domain. differently work focuses explicitly multiple previous hidden states residual connection attention. higher order proposed language modeling similar work differences existed uses regular form residual connection attention hornn directly considers hh−k. introduces much less parameters e.g. unit required consider past states introduces additional parameters hornn introduces millions weights compared plain rnn. recurrent weighted average also explores attention rnn. difference performs weighted average computing ﬂexible considering past states residual attention. limitation although shows ability capturing long-range dependencies across timesteps faster convergence stable training compared standard lstm multiple tasks also limitation training speed sightly slower standard lstms e.g. permuted mnist lstm took average epoch took took conjecture additional time spent compute derivative residual attention pass error signal current states states several step apart directly. however noted that experiments early stopping applied lstm ﬁnish training stop much earlier lstm. paper introduced learn long-term dependencies sequential data. residual shortcut connection effectively pass error signal across timesteps several apart away prevent gradient vanishing problem. deﬁned attention mechanism timesteps provides natural summarize individual contribution past hidden states predicting future hidden states. compared standard implementation lstm. shows superior performance stable training fast convergence adding problem pixel-by-pixel mnist classiﬁcation sentiment analysis. although without using additional mechanism e.g. wordvec embedding pre-training unlabeled data demonstrates competitive performance compared recent methods. future work extend different sequence learning scenarios including machine translation speech recognition etc..", "year": 2017}