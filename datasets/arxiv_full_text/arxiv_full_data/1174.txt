{"title": "Learning Paired-associate Images with An Unsupervised Deep Learning  Architecture", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities, or channels, such that input on one channel will correctly generate the associated response at the other and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of a bi-directional network and unsupervised learning from either paired or non-paired training examples.", "text": "paper presents unsupervised multi-modal learning system learns associative representation input modalities channels input channel correctly generate associated response vice versa. system develops kind supervised classiﬁcation model meant simulate aspects human associative memory. system uses deep learning architecture composed input/output channels formed stacked restricted boltzmann machines associative memory network combines channels. trained pairs mnist handwritten digit images develop hierarchical features associative representations able reconstruct image given paired-associate. experiments show multi-modal learning system generates models accurate back-propagation networks advantage bi-directional network unsupervised learning either paired non-paired training examples. humans learn knowledge environment data provided several forms modalities audio visual signals. psychologists deﬁne multi-modal learning learning knowledge multiple sensory modalities researchers shown people’s understanding concepts enhanced mixed-modality knowledge representations human brain adapted fuse associated sensory signals learn effectively efﬁciently. long-term goal research develop learning system simulates aspects multi-modal learning ability humans. particular investigate unsupervised learning methods create model capable generalization classiﬁcation input output modality another interested done without resorting form supervised learning suffers need labeled examples. deep learning sub-area machine learning typically uses restricted boltzmann machines type stochastic associative artiﬁcial neural network develop multilayer generative models deep learning architectures provide exciting substrate upon explore computational representational models knowledge acquired consolidated used prior work investigated dlas unsupervised learning methods develop models variety purposes including auto-associative memory pattern completion clustering well generalization classiﬁcation paper takes ﬁrst step toward developing multi-modal learning system examining capable learning paired-associate images input modalities must reconstruct matching image channel observes it’s paired image channel vice versa. system uses unsupervised learning develop associative memory model performs form classiﬁcation channel another. additionally learn paired-associate examples also non-paired independent examples sensory modality. experimentation shows quantitatively qualitatively system generates models accurately generates associated images compared models developed using traditional supervised back-propagation networks. artiﬁcial neural networks widely used solve classiﬁcation problems image speech recognition however many work fashion human nervous system. example back-propagation anns good modeling complex mapping relations input output data good reconstructing recalling pattern. humans ability recover complete information partial information; referred associative memory child watches tennis game learns appearance tennis ball racket. next time child sees picture tennis ball child recall image racket game. associations clearly major part learning world. associative anns inspired cognitive psychology designed mimic collections biological neurons store recall associative memories geoffrey hinton university toronto advocates using boltzmann machine associative networks simulating human brain structure. boltzmann machine trained patterns ability reconstruct patterns partial noisy pattern. however learning slow large boltzmann machines many weights fully connected network iterative sampling node activities required weight update. restricted boltzmann machine variant meant overcome long training times limiting number connections network using modiﬁed learning algorithm. rbms visible hidden layers neurons like however intralayer connections characterized bipartite graph settling equilibrium neuron turns probability neuron states neuron turns probability keep changing probabilities system computes activation energy vihjwij bias terms respective nodes global energy reduced quickly compared reduced number connections. goal training modify weights network establish energy states correspond training patterns visible nodes. similar input patterns energy states closer other whereas orthogonal patterns energy states distant other. method weight update research called contrastive divergence weights network initialized small random values. training data given visible neuron clamps states visible neurons frees states hidden binary neuron weight updated following formula ∆wij learning rate vihj expectation possible pairs visible hidden node values superscripts indicate expectation based training example reconstruction respectively. equation approximates gradient probability training example respect weight. weight updated global energy reduces threshold. probability neuron reconstruct input data training hidden layer weights learned feature distribution input space equal probability feature given input test ability recall pattern presented inputs test example visible units cause activations hidden units described humans tend organize ideas concepts hierarchically abstract concepts learned recalled composition simpler concepts approach makes sense world objects made parts turn composed smaller features. instance combination smaller parts like wheels frame. wheel made smaller features like tire rim. neuroscience studies conﬁrmed compositional structure seen human nervous system. mammalian brain uses deep learning architecture multiple levels abstraction corresponding different areas neocortex deep learning architectures sub-area machine learning places heavy emphasis hierarchical composition unsupervised learning methods. dlas developed stacking layers rbms another successfully used develop models recognizing hand-writing images digits manner simulates human visual cortex rbm-based systems capable unsupervised clustering unlabeled data based hierarchy features. shown figure hidden layer used input layer higher level highest level features used achieve classiﬁcation desired. subsequently researchers feel dlas develop hierarchy features fashion similar mammalian brain. dlas present looking systems learn. deep architectures used autoencoder model high-dimensional data images audio bengio reports deep architectures expressive shallow ones analyzing depth-breadth trade-off architecture representation perhaps importantly deep learning methods learn representative hierarchies directly data contrast approaches convolutional networks receptive ﬁelds modiﬁed back-propagation methods rely heavily known topological characteristics input space objective research develop learning system memorize recall multichannel data using associative memory network. learning system able recall pattern associative network sensory modality given data another sensory modality. long-term goal research create system learn concepts using sensory/motor modalities audio optical vocal consider problem learning paired-associate images input modalities propose network that training able generate paired image channel prompted image another channel. process meant simulate human sensory modalities associative memory provide insights classiﬁcation done using unsupervised learning approach. learning system composed major parts associative memory network associative sensory channel networks sensory channel networks designed recognition reconstruction sensory data. associative memory network ties sensory channel networks together simulates human associative memory. parts built using rbms. reduced representation recall capacity high fullyconnected determined unable recall patterns half visible neurons given correct pattern values thus used associative memory network additional steps required algorithm completed training. hinton weights network require tuning produce appropriate features layer weights model need ﬁnetuned. however ﬁne-tuning bi-directional weights destroy ability generate lower level features. protect accuracy generative model necessary untie weights layer channel associative memory network layer create sets weights recognition weights generative weights recognition weights used bottom-up pass receives input pattern generative weights used top-down pass reconstruct output pattern. generative weights left trained rbm. recognition weights ﬁne-tuned using back-ﬁtting algorithm associative memory network generate relatively accurate full associative memory features input channel. ﬁne-tune channel recognition weights neuron hidden layer neuron hidden layer used initial weight values gradient descent regression paired patterns. training pattern posterior probabilities {pi} hidden layer used input attribute posterior probabilities {pj} hidden layer used target output. posterior probabilities hidden layer computed using weights updated using gradient descent minimize error {pj} recognition weights pass input signal sensory channel associative memory network ﬁne-tuned generate full associative memory features channel generate appropriate output. back-ﬁtting multi-modal able achieve learning goal previously done supervised learning srivastava without supervised learning channels performance unlikely exceed traditional approach; however expect well. hierarchical feature learning sensory channels back-ﬁtting recognition weights expected make shortcomings purely unsupervised learning approach taking. sensory data always come pairs real life. example meowing image hear meowing without seeing cat. case sound meow audio signal image visual signal. sensory channels come together allow paired-associate learning individual channel representations learned improved upon separately. propose learning sensory modality non-paired examples help improve associative memories ability generate correct image channel given paired-associate other. would informative experiment test impact multi-channel learning system separately training sensory channels non-paired examples. three empirical studies carried using different data sets. ﬁrst third experiments used paired images mnist dataset handwritten numeric digits. second experiment used paired images synthetic dataset numeric digits. experiments pairs even digits associated each objective objective experiment compare unsupervised supervised approach learning paired-associate images. learning system trained handwritten digit image provided system generate paired digit image. material methods experiment uses dataset paired mnist handwritten digits learning domain. experiment repeated four times different training sets validation sets test sets. datasets contains paired-associate examples randomly selected mnist dataset. deep learning architecture rbms used develop unsupervised learning model problem. architecture accord figure channel network composed layers contains hidden neurons. hidden layers layers develop abstract features original images associative layer contains neurons. unsupervised uses back-ﬁtting ﬁne-tune weights associative layer algorithm training ﬁnished. training dlas training process sensory channel stops maximum iteration reached associative memory network trained iterations. validation sets used monitor back-ﬁtting avoid over-ﬁtting. digit part test example used test reconstruction corresponding even digit image vice versa. developed networks learn paired-associate mapping. network trained digit images even digits vice versa. networks architecture shown figure networks training validation testing dla. validation used prevent algorithm over-ﬁtting training set. accuracy reconstruction measured testing output images using hinton’s handwritten digits classiﬁcation software. software known classify mnist dataset handwritten digits errors pass input images reconstructed images hinton’s classiﬁer determine digit category. accuracy models based number correctly paired images. results discussion using hinton’s software reconstruction accuracy checked testing set. average results four replications experiments shown table average unsupervised generated images accurate anns generated images accurate. models equally well. suggests unsupervised models able achieve level accuracy supervised approach. figure shows examples reconstructed images produced dlas anns. images generated dlas clearer generated anns. suspect models able better differentiate features noise. investigated next experiment. objective objective experiment develop auto-associative models overcome noise injected synthetic training examples. unsupervised back-ﬁtting supervised anns developed noisy dataset quality regenerated images compared. material methods experiment uses synthetic dataset contains different sets paired images figure random noise added template image produce instances category total. ﬁrst images used training next used validation remaining used test set. layer contains neurons. training process sensory channel networks stops maximum iteration reached; associative memory network trains iterations. experiment networks developed learn paired-associate mapping. networks used architecture similar shown figure neurons layers neurons layer networks uses training validation test dla. accuracy reconstruction measured comparing similarity generated images corresponding template images test examples. rmse pixels reconstructed image corresponding template computed give average error examples results discussion rmse reconstructed images test shown table back-ﬁtting out-performs networks generating images presence noise. figure shows examples reconstructed images anns. generated images quite similar template images figure signiﬁcant noise generated images network. dlas attempt probabilistically differentiate features noises whereas anns attempt input pixels output pixels. features formed networks purpose mapping reconstruction original images. hence better choice objective construct noiseless category example form classiﬁcation. objective preceeding experiments used paired-associate examples develop neural network models however sensory data always come pairs real life. objective experiment accord section develop associative learning system paired associative examples independent non-paired examples. experiment designed test performance associative learning system improved separately training sensory channels non-paired examples. material methods experiment uses database mnist examples experiment experiment repeated four times different training sets validation sets test sets. repetition four models built using architecture different amounts training examples. ﬁrst model built paired-associate examples. second model built pairedassociate examples non-paired examples even digit images. third model built paired-associate examples non-paired examples even digit images non-paired examples digit images. last model built paired-associate examples. figure shows number paired non-paired examples training set. digits images used train channel even digits used train even channel paired-associate examples used develop associative memory. four models -layered architecture parameters validation sets test sets experiment back-ﬁtting validation sets used monitor overﬁtting. test sets used examine associative learning performance learning system. digits used test recall even digits vice versa. recalled images classiﬁed hinton’s classiﬁer examine accuracy models. results discussion performance four models recalling even digits digits digits even digits average shown figure error bars represent conﬁdence repeated studies. mean accuracy increases marginally model model means using non-paired examples better develop channels representation improve overall performance associative learning system. conjecture recognition weights generative weights channel optimized. improving recognition weight performance digits channel provide better features associative memory network generate corresponding even digits. better generative weights digits channel generate accurate digits even digits provided. general result suggests improving sensory channel networks multi-channel learning system contains channels improve recall involves channel. also important note reconstruction accuracy clearly increases model model demonstrates using paired-associate examples develop associative memory network improve performance system equivalent number non-paired examples. system three channels conjecture paired-associate examples channels beneﬁt entire associative memory network. paper presents recent work unsupervised multi-modal learning system develop associative memory structure combines input/output channels. long-term goal develop learning systems able learn conceptual representations multiple sensory input and/or motor output modalities manner similar humans. demonstrated unsupervised deep learning architecture reconstruct image mnist handwritten digit another paired handwritten digit. system develops kind supervised classiﬁcation model meant simulate aspects human associative memory. formed stacked restricted boltzmann machines trained contrastive divergence algorithm. associative memory network ties input/output channels together requires reﬁnement using back-ﬁtting technique increase recall accuracy visible neurons available channel. experimentation shows quantitatively qualitatively system develops models able reconstruct accurate paired images compared supervised back-propagation network models advantage unsupervised learning either paired non-paired training examples. future work different types sensory data used train multi-modal learning system audio signals. furthermore interested knowledge transfer dlas using unsupervised methods learning tasks modalities.", "year": 2013}