{"title": "Cooperative Training of Descriptor and Generator Networks", "tag": ["stat.ML", "cs.CV"], "abstract": "This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jumpstarting each other's Langevin sampling, and they can be naturally and seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.", "text": "paper studies cooperative training probabilistic models signals images. models parametrized convolutional neural networks ﬁrst network descriptor network exponential family model energy-based model whose feature statistics energy function deﬁned bottom-up convnet maps observed signal feature statistics. second network generator network non-linear version factor analysis. deﬁned top-down convnet maps latent factors observed signal. maximum likelihood training algorithms descriptor generator form alternating back-propagation algorithms involve langevin sampling. observe training algorithms cooperate jumpstarting other’s langevin sampling naturally seamlessly interwoven coopnets algorithm train nets simultaneously. begin story reader paper readily relate student writes initial draft paper. advisor revises submit revised paper review. student learns advisor’s revision advisor learns outside review. story advisor guides student student work. simultaneous training nets ﬁrst studied recent work nets belong major classes probabilistic models. exponential family models energy-based models markov random ﬁeld models probability distribution deﬁned feature statistics energy function computed signal bottom-up process. latent variable models directed graphical models signal assumed transformation latent factors follow known prior distribution. latent factors generate signal top-down process. classical example factor analysis classes models contrasted called classes models descriptive models generative models respectively. classes models beneﬁt high capacity multi-layer convnets. exponential family models energy-based models feature statistics energy function deﬁned bottom-up convnet maps signal features energy function usually linear combination features layer. call resulting model descriptive network figure chart algorithm training descriptor net. updating step based difference between observed examples synthesized examples. langevin sampling synthesized examples current model step time consuming. figure chart algorithm training generator net. updating step based observed examples inferred latent factors. langevin sampling latent factors current posterior distribution step time consuming. descriptor following built descriptive feature statistics. latent variable models directed graphical models transformation latent factors signal deﬁned topconvnet maps latent factors signal. call resulting model generative network generator following proposed model work generative adversarial networks article assume nets separate structures parameters. shall fuse maximum likelihood training algorithms nets. although training algorithm stand fusion beneﬁcial natural particular fusion fuels markov chain monte carlo sampling descriptor turns training generator unsupervised learning supervised learning. fusion leads cooperative training algorithm interweaves steps algorithms seamlessly. call resulting algorithm coopnets algorithm show train nets simultaneously. training algorithm descriptor alternates between following steps call algorithm figure illustration. step langevin revision sampling synthesized examples current model langevin dynamics call step langevin revision keeps revising current synthesized examples. step density shifting updating parameters descriptor based difference observed examples synthesized examples obtained step shift high probability regions descriptor synthesized examples observed examples. training algorithm generator alternates between following steps call algorithm figure illustration. step langevin inference observed example sample latent factors current posterior distribution langevin dynamics. call step langevin inference infers latent factors observed example order inferred latent variables explain reconstruct observed examples. step reconstruction updating parameters generator based observed examples inferred latent factors obtained inferred latent variables better reconstruct observed examples. training algorithm generator similar algorithm step mapped e-step step mapped m-step. unsupervised learning algorithm latent factors unobserved. compared step step also form density shifting current reconstructed examples towards observed examples requires inferring latent factors observed example. needs cooperation stem fact langevin sampling step step timeconsuming. algorithms cooperate jumpstart other’s langevin sampling resulting algorithm call coopnets algorithm naturally seamlessly interweaves steps algorithms minimal modiﬁcations. plain language descriptor needs dream hard step synthesis generator needs think hard step explaining-away reasoning. hand generator actually much better dreamer generate images direct ancestral sampling descriptor need think order learn. speciﬁcally following steps coopnets algorithm step generation generate initial synthesized examples using generator net. initial examples obtained direct ancestral sampling. step langevin revision starting initial synthesized examples produced step langevin revision dynamics number steps obtain revised synthesized examples. step density shifting before except revised synthesized examples produced step shift density descriptor towards observed examples. step langevin inference generator learn revised synthesized examples produced step revised synthesized example know values latent factors generate corresponding initial synthesized example step therefore simply infer latent factors known valfigure chart coopnets algorithm. part chart training descriptor similar algorithm figure except langevin sampling initialized initial synthesized examples supplied generator. part chart training generator also mapped algorithm figure except revised synthesized examples play role observed examples known generated latent factors used inferred latent factors given step initialize langevin inference dynamics step known values. step reconstruction before except revised synthesized examples inferred latent factors obtained step update generator. generator step generates thus reconstructs initial synthesized examples. step updates generator reconstruct revised synthesized examples. revision generator accounts revisions made langevin revision dynamics step figure shows chart coopnets algorithm. generator like student. generates initial draft synthesized examples. descriptor like advisor. revises initial draft running number langevin revisions. descriptor learns outside review form difference observed examples revised synthesized examples. generator learns descriptor revises initial draft reconstructing revised draft. tale nets. reason generator learn revised synthesized examples instead observed examples generator know latent factors generate observed examples think hard infer explaining-away reasoning. however generator knows latent factors generate initial synthesized example thus essentially knows latent factors learning revised synthesized examples. reconstructing revised synthesized examples generator traces accumulates langevin revisions made descriptor. cooperation thus beneﬁcial generator relieving burden inferring latent variables. generator hard learn observed examples directly descriptor problem learning observed examples needs compute bottom-up features deterministically. however needs synthesized examples shift density come easily. generator provide unlimited number examples learning iteration generator supplies completely batch independent examples demand. descriptor needs revise batch examples instead generating batch scratch. generator memorized cumulative effect past langevin revisions produce samples shot. cooperation thus beneﬁcial descriptor relieving burden synthesizing examples scratch. terms density shifting descriptor shifts density revised synthesized examples towards observed examples step generator shifts density initial synthesized examples towards revised synthesized examples step reconstructing latter. terms energy function descriptor shifts energy regions towards observed examples induces generator latent factors energy regions. achieves stochastically relaxing synthesized examples towards energy regions generator track synthesized examples. true essence learning nets still learn data. descriptor learns observed data teaches generator synthesized data. specifically collaborate communicate synthesized data following three versions generator generates initial draft. descriptor revises draft. generator reconstructs revised draft. work inspired generative adversarial networks generator paired discriminator net. nets play adversarial roles. work generator descriptor play cooperative roles feed synthesized data. learning nets based maximum likelihood learning process quite stable cooperative nature consistent direction maximum likelihood training algorithms. maximum likelihood training models charge training examples uniformly avoiding mode collapsing issue. maximum likelihood training also efﬁcient statistically discriminative learning sample size small. work similar recent work fact settings nets essentially same. work generator learned current descriptor minimizing kullback-leibler divergence generator current descriptor decomposed energy term entropy term latter approximated based quantities produced batch normalization work avoids mcmc sampling particular work involve revising reconstructing synthesized examples plays pivotal role method. work descriptor acts generator directly whereas work descriptor acts generator synthesized data. despite difference interesting connection work seeks minimize kullback-leibler divergence generator current descriptor langevin revision dynamics work runs markov chain current generator current descriptor automatically monotonically decrease kullback-leibler divergence distribution revised synthesized data distribution current descriptor important property markov chain underlies second thermodynamics so-called arrow time. revised synthesized data generator reconstruct effect langevin revisions. therefore learning method consistent fact energy entropy terms learning objective function correspond gradient noise terms langevin dynamics work langevin dynamics relieves need approximate intractable entropy term. indexes time steps langevin dynamics step size gaussian white noise term. langevin dynamics process stochastic relaxation. gradient term seeks reduce energy function noise term provides randomness. difference observed examples synthesized examples. details. gradient coincides gradient e/n. shift density regions ˜yi} {yi}. zero temperature langevin become gradient descent learning algorithm approximately solves following minimax problem work somewhat related generative stochastic network learns markov transition probability directly. work generator serves absorb cumulative effect past markov transitions descriptor reproduce shot. descriptor root exponential family model statistics energy-based model machine learning. signal image video sequence sound sequence. descriptor model form exponential tilting reference distribution dimensionality signal feature statistics energy function deﬁned convnet whose parameters denoted convnet bottom-up maps signal feature statistics. diagram step langevin revision steps langevin dynamics update i.e. starting current step follows equation step density shifting update computed according algorithm describes training algorithm. figure illustration. step needs compute step needs compute computations derivatives powered back-propagation. convnet structure computations derivatives share steps chain rule computations. parameter keeps changing learning process energy landscape local energy minima also keep changing. help langevin revision dynamics avoid trapped local energy minima. algorithm stochastic approximation algorithm except synthesized examples obtained ﬁnite number langevin steps learning iteration. convergence algorithm type maximum likelihood estimate established generator root factor analysis statistics latent variable model directed graphical model machine learning. generator seeks explain signal dimension vector latent factors dimension usually identity underlies algorithm expectation respect posterior distribution latent factors computed e-step. usefulness identify lies fact derivative complete-data loglikelihood right hand side obtained closed form. algorithm m-step step langevin inference steps langevin dynamics update i.e. starting current step follows equation step reconstruction update γtlgg learning rate computed according equation speciﬁcally step initialize synthesized examples generating examples generator require mcmc generator directed graphical model. speciﬁcally ﬁrst generate generate g+\u0001i current generator close current descriptor generated ˆyi} good initialization sampling descriptor i.e. starting supplied generator langevin dynamics step steps revised versions ˆyi}. ˜yi} used synthesized examples descriptor net. update according step algorithm order update generator treat produced step training data generator. since ˜yi} obtained langevin revision dynamics initialized produced generator known latent factors update learning supervised learning problem speciﬁcally non-linear regression ˆxi. latent factors generates thus reconstructs initial example ˆyi. updating want reconstruct revised examτ indexes time step step size notational simplicity continue denote noise term take derivative derivative derivative log-posterior since proportional function langevin inference solves penalized non-linear least squares problem reconstruct given current langevin inference process performs explaining-away reasoning latent factors compete explain current residual sampled observation langevin inference process monte carlo approximation updating solves non-linear regression problem learned enables better reconstruction inferred given inferred learning supervised learning problem algorithm describes training algorithm. figure illustration. step needs step needs compute compute computations derivatives powered back-propagation computations derivatives share steps chain rule computations. ˜yi. revise absorb revision generator shifts density ˆyi} ˜yi}. reconstruction error tell whether generator caught descriptor fully absorbing revision. diagrams double line arrows indicate generation reconstruction generator dashed line arrows indicate langevin dynamics revision inference nets. diagram right illustrates rigorous method initialize langevin inference step ˆxi} update step based ˜n}. algorithm describes cooperative training interweaves algorithm algorithm figure chart coopnets algorithm. experiments infer ˆxi. learning descriptor generator follows analysis synthesis principle three sets synthesized examples initial synthesized examples ˆyi} generated step revised synthesized examples ˜yi} produced step reconstructed produced step synthesized examples descriptor shifts density towards observed data generator shifts density towards evolution work descriptor. process stochastic relaxation settles synthesized examples energy regions descriptor. descriptor works associative memory recalled memory. serves feedback generator. reconstruction work generator seeks absorb feedback conveyed evolution descriptor test whether generator learns well checking whether close nets collaborate communicate synthesized data. step generate generate step starting steps langevin revision dynamics obtain step following equation step treat current training data infer ˆxi. rigorously starting steps langevin inference dynamics update step following equation step update γtlgg step update computed according equation except replaced multiple iterations step learn reconstruct ˜yi} allow generator catch descriptor. change shift energy regions local energy minima synthesized data observed data change latent factors energy regions local modes. temperature lowered gradually local energy minima correct probabilities. matconvnet coding. descriptor adopt structure bottom-up network consists multiple layers convolution linear ﬁltering relu nonlinearity down-sampling. adopt structure generator network top-down network consists multiple layers deconvolution linear superposition relu -layer descriptor ﬁrst layer ﬁlters sub-sampling rate pixels second layer ﬁlters sub-sampling third layer ﬁlters sub-sampling standard deviation reference distribution descriptor steps langevin revision dynamics within learning iteration langevin step size learning rate starting latent factors generator layers deconvolution kernels up-sampling factor layer. standard deviation noise vector learning rate number generator learning steps cooperative learning iteration. cooperative learning iterations train coopnets. figure displays results generating texture patterns. category ﬁrst image training image rest images generated learning algorithm. parallel chains ﬁrst example images presented. single chain rest examples synthesized images generated different iterations. even though single chain inﬁnite number chains iteration langevin revision dynamics image sampled generator. learn separate model object pattern. training images collected internet resized number training images category structure generator network adopt -layer descriptor net. ﬁrst layer ﬁlters sub-sampling second layers ﬁlters sub-sampling third layer fully connected layer single channel output. standard deviation reference distribution descriptor steps langevin revision dynamics within learning iteration langevin step size learning rate cooperative learning iterations train model. figure displays results. displays experiment ﬁrst images training images rest generated images. also conduct experiment human faces adopt -layer descriptor net. ﬁrst layer ﬁlters sub-sampling second layers ﬁlters sub-sampling third layer ﬁlters sub-sampling ﬁnal layer fully connected layer channels output. reduce langevin step size training data human faces randomly selected celeba dataset cooperative learning iterations. figure displays synthesized human faces descriptor net. quantitatively test whether learned good generator even though never seen training images directly training stage apply task recovering occluded pixels testing images. occluded testing image step algorithm infer latent factors change respect term squares observed pixels backpropagation computation. langevin steps initializing inferring completed image automatically obtained. design experiments randomly place mask testing image. experiments denoted respectively report recovery errors compare method different image inpainting methods. methods based markov random ﬁeld prior nearest neighbor potential terms differences respectively. methods interpolation methods. please refer details. table displays recovery errors experiments error measured pixel difference original image recovered image occluded region averaged testing images. fig. displays recovery results method. ﬁrst shows original images ground truth. second displays testing images occluded pixels. third displays recovered images generator trained coopnets algorithm training images. method outperforms methods terms recovery accuracy. conduct experiment synthesizing images categories imagenet ilsvrc dataset places dataset adopt -layer descriptor net. ﬁrst layer ﬁlters sub-sampling second layers ﬁlters sub-sampling third layer ﬁlters sub-sampling ﬁnal layer fully connected layer channels output. number langevin dynamics steps learning iteration step size learning rate category images training data resize images iterations. figure displays results categories show synthesized images generated method. figure shows categories. project page contains synthesis results. unique feature work networks feed synthesized data learning process. generator feeds descriptor initial version synthesized data. descriptor feedbacks generator revised version synthesized data. generator produces reconstructed version synthesized data. descriptor learns ﬁnite amount observed data generator learns virtually inﬁnite amount synthesized data. another unique feature work learning process interweaves existing maximum likelihood learning algorithms networks algorithms jumpstart other’s mcmc sampling. third unique feature work mcmc descriptor keeps rejuvenating chains refreshing samples independent replacements supplied generator single chain effectively amounts inﬁnite number chains evolution whole marginal distribution. powering mcmc sampling descriptive models main motivation paper bonus turning unsupervised learning generator supervised learning. coopnets algorithm descriptor learns observed examples generator learns descriptor synthesized examples. therefore descriptor driving force terms learning although generator driving force terms synthesis. order understand convergence learning start algorithm learning descriptor. bins monro except samples generated ﬁnite step mcmc transitions. according algorithm converges maximum likelihood estimate suitable regularity conditions mixing transition kernel mcmc schedule learning rate even number langevin steps ﬁnite small even number parallel chains ﬁnite small reason random ﬂuctuations caused ﬁnite number chains limited mixing caused ﬁnite steps mcmc mitigated learning rate sufﬁciently small. learning iteration estimated parameter descriptor. marginal distribution ˜yi} even though ﬁnite still probability according maximum likelihood estimate efﬁciency algorithm increases number parallel chains large leads accurate estimation expectation gradient equation afford larger learning rate faster convergence. come back coopnets algorithm. order understand descriptor helps training generator consider idealized scenario number parallel chains generator inﬁnite capacity iteration estimates maximum likelihood using synthesized data idealized scenario learned generator minimizing serving data distribution. eventually learned generator reproduce thus cooperative training helps learning generator. note learned generator reproduce distribution observed data pdata unless descriptor inﬁnite capacity too. conversely generator also helps learning descriptor coopnets algorithm. algorithm impractical make number parallel chains large. hand would difﬁcult small number chains explore state space. coopnets algorithm reproduces generate completely batch independent samples ˆyi} revise ˆyi} ˜yi} langevin dynamics instead running langevin dynamics batch ˜yi} original algorithm like implementing inﬁnite number parallel chains iteration evolves fresh batch examples iteration evolves chains. updating generator like updating inﬁnite number parallel chains memorizes whole distribution. even coopnets algorithm small e.g. viewed perspective algorithm thus idealization sound. information geometry point view {pd∀wd} manifold descriptor models distribution point manifold. maximum likelihood estimate projection data distribution pdata onto manifold {pg∀wg} manifold generator models distribution point manifold. maximum likelihood estimate projection data distribution pdata onto manifold notational simplicity slight abuse notation denote descriptor distribution denote generator distribution assume observed data size synthesized data size large enough shall work distributions populations instead ﬁnite samples. explained above assuming sound generator supply unlimited number examples. langevin revision dynamics runs markov chain towards markov transition kernel steps langevin revisions towards distribution revised synthesized data notation denotes marginal distribution obtained running markov transition distribution middle nets serves data distribution train generator i.e. project distribution manifold {pg∀wg} {wg} information geometry picture learning process alternates markov transition projection illustrated figure compared three sets synthesized data mentioned section corresponds corresponds figure learning generator alternates markov transition projection. family generator models illustrated black curve. distribution illustrated point. ﬁrst project pdata onto continue project onto therefore converges maximum likelihood estimate pdata data distribution converges maximum likelihood estimate serving data distribution. ﬁnite algorithm converge following ﬁxed points. ﬁxed point generator satisﬁes similar contrastive divergence except takes place pdata second kullback-leibler divergence. supposed close second kullback-leibler divergence supposed small hence algorithm closer maximum likelihood learning contrastive divergence. learned generator gradient descent objective function ﬁrst term negative entropy intractable second term expected energy tractable. learning method generator consistent learning objective thomas reduction kullback-leibler divergence projection learning generator consistent learning objective reducing monte carlo implementation work avoids need approximate intractable entropy term. marginal distribution mcmc parameter generator traces evolution marginal distribution mcmc absorbing cumulative effect past markov transitions traditional mcmc access monte carlo samples instead marginal distributions exist theoretically analytically intractable. however generator actually implement mcmc level whole distributions instead number monte carlo samples sense learning existing samples replace existing samples fresh samples sampling generator deﬁned learned illustrated two-way arrow effectively generator powers mcmc implicitly running inﬁnite number parallel chains. conversely mcmc drive evolution samples also drives evolution generator model. descriptor ﬁxed generator converge descriptor generator inﬁnite capacity. coopnets algorithm descriptor evolving because keep updating parameter descriptor learning observed examples. therefore markov chain non-stationary target distribution mcmc i.e. current descriptor keeps evolving markov transition kernel also keeps changing. initially distribution descriptor high temperature noise term dominates langevin dynamics. following descriptor generator gain high entropy high variance. learning proceeds parameters descriptor become bigger magnitude distribution becomes colder multi-modal langevin dynamics dominated gradient term. temperature patterns begin form. chasing descriptor generator begins latent factors local modes create patterns. possible generator maps small neighborhood factor space local mode energy landscape descriptor. observed local modes descriptor satisfy auto-encoder hence examples generated generator satisfy auto-encoder deﬁned descriptor stochastic relaxation descriptor cannot make much revision examples generated generator. embedding perspective generator embeds high-dimensional local modes descriptor low-dimensional factor space thus providing energy landscape descriptor. mcmc perspective learning process like simulated annealing tempering process generator traces process distribution illustrated diagram case simulated annealing necessary begin high temperature high entropy distribution cast sufﬁciently wide generator slow tempering schedule learning schedule case capture major modes. mcmc sampling evolving descriptor easier sampling ﬁxed descriptor energy landscape evolving descriptor keeps changing even destroying local modes deﬁned auto-encoder thus releasing markov chains trapping local modes. illustrate descriptive models generative models used frame model prototype descriptive models used sparse coding model prototype generative models. frame model sparse coding model involve wavelets wavelets play different roles models. frame model wavelets serve bottom-up ﬁlters extracting features. sparse coding model wavelets serve top-down basis functions linear superposition. roles uniﬁed orthogonal basis tight frame wavelets general cannot both. general setting descriptive model features explicitly deﬁned bottomprocess signal cannot recovered features explicit top-down process. generative model latent variables generate signal explicit top-down process inferring latent variables requires explaining-away reasoning cannot deﬁned explicit bottom-up process. efforts made integrate classes models understand entropy regimes still lack clear understanding roles scopes relationship. descriptor generator fundamental representations knowledge. descriptor provides multiple layers features proper serve associative memory content addressable memory conditional distribution signal given partial information explicitly deﬁned. compared full distribution conditional distribution easier sample less multi-modal full distribution. generator disentangles variations training examples provides non-linear dimension reduction. embeds highly non-linear manifold formed highdimensional training examples low-dimensional euclidean space latent factors linear interpolation factor space results highly non-linear interpolation signal space. could called descriptor energy-based model latter term carries much broader meaning probabilistic models e.g. fact energy-based probabilistic models often looked upon unfavorably intractable normalizing constant reliance mcmc. however probabilistic model appears coherent deﬁne associative memory conditional distributions given different partial informations always consistent other. difﬁculty mcmc sampling descriptor generator actually great help explored paper. probabilistic models belong related classes models. example energy-based models involve latent variables restricted boltzmann machine generalizations models undirected. also models latent variables signals dimension transformed other latent variables also considered features also models impose causal auto-regressive factorizations models beyond scope paper. right hand side involves bottom-up pass convolution top-down pass back-propagation computing derivative back-propagation actually deconvolution process autoencoder curious back-propagation becomes representation. also connects hopﬁeld associative memory hierarchical distributed representation. also noticed non-linearity bottom-up convnet deﬁnes rectiﬁed linear units binary activation piecewise gaussian modes gaussian pieces auto-encoding. speciﬁcally activation pattern relu units convnet partitions space different pieces within piece share activation pattern. linear piece energy function piecewise quadratic gives rise piecewise gaussian structure. langevin equation gradient term form reconstruction error local mode energy function satisﬁes auto-encoder mentioned above. langevin revision dynamics explore local modes stochastically relaxing current sample towards local mode. derived descriptor discriminator score soft-max multinomial logistic classiﬁcation reference distribution plays role negative baseline category. generator network viewed multi-layer recursion factor analysis. noted non-linearity top-down convnet deﬁnes relu model becomes piecewise linear factor analysis. speciﬁcally activation pattern relu units convnet partition factor space different pieces within piece share activation pattern. linear piece model piecewise linear factor analysis essentially piecewise gaussian. alternating back-propagation algorithms descriptor generator involve computations updating langevin sampling derivatives computed chain-rule back-propagation share computation derivative matrix chain rule diagonal matrix element-wise diagonal elements element-wise derivatives. computations give addition /∂wl /∂wl. therefore computations subset computations words back-propagation langevin dynamics by-product back-propagation parameter updating terms coding. alternating backpropagation algorithms nets similar coding. diag relu non-linearity diag makes vector diagonal matrix vector denotes binary vector whose elements indicate whether corresponding elements greater not. activation pattern relu units layer integral approximated laplace approximation around posterior mode maximizes compare energy descriptor laplace approximation generator. considered encoder recognizer inferring encoder recognizer learned wake-sleep algorithm variation auto-encoder recall used score soft-max discriminator. thus four nets discriminator descriptor generator recognizer connected. understand algorithm idealization maps prior distribution latent factors data distribution pdata learned deﬁne pdata pdatapg pdatapdata pdata posteriors observed data pdata. pdata considered data prior. data prior pdata close true prior sense kl|pg) kl|pg) right hand side minimized maximum likelihood estimate hence data prior pdata especially close true prior words posteriors data points pdata tend pave whole prior based m-step data point seeks reconstruct inferred latent factors words m-step seeks since posteriors tend pave prior overall m-step seeks prior whole training data pdata. course mapping observed cannot exact. fact actually maps d-dimensional patch around d-dimensional local patches observed examples patch d-dimensional manifold form d-dimensional observed examples interpolations. algorithm process density shifting mapping pdata shifts towards pdata. coopnets algorithm avoid dealing posterior distributions observed examples data prior pdata. instead generate directly prior distribution instead using observed pave prior. density shifting accomplished tracking langevin revisions made descriptor. deng dong socher richard li-jia fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee denton emily chintala soumith fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems dosovitskiy springenberg brox learning generate chairs convolutional neural networks. ieee international conference computer vision pattern recognition goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv. honglak grosse roger ranganath rajesh andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. icml pascanu razvan montufar guido bengio yoshua. number response regions deep feed forward networks piece-wise linear activations. arxiv preprint arxiv. radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. rezende danilo mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. jebara tony xing eric icml jmlr workshop conference proceedings younes laurent. convergence markovian stochastic algorithms rapidly decreasing ergodicity rates. stochastics international journal probability stochastic processes zhou bolei lapedriza agata xiao jianxiong torralba antonio oliva aude. learning deep features advances scene recognition using places database. neural information processing systems", "year": 2016}