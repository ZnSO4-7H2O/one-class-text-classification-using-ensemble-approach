{"title": "A probabilistic methodology for multilabel classification", "tag": ["cs.AI", "cs.LG", "68T37, 68T10", "I.2.6; I.2.3; H.3"], "abstract": "Multilabel classification is a relatively recent subfield of machine learning. Unlike to the classical approach, where instances are labeled with only one category, in multilabel classification, an arbitrary number of categories is chosen to label an instance. Due to the problem complexity (the solution is one among an exponential number of alternatives), a very common solution (the binary method) is frequently used, learning a binary classifier for every category, and combining them all afterwards. The assumption taken in this solution is not realistic, and in this work we give examples where the decisions for all the labels are not taken independently, and thus, a supervised approach should learn those existing relationships among categories to make a better classification. Therefore, we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers, by using a combination procedure with a classifier trained on the co-occurrences of the labels. We show an exhaustive experimentation in three different standard corpora of labeled documents (Reuters-21578, Ohsumed-23 and RCV1), which present noticeable improvements in all of them, when using our methodology, in three probabilistic base classifiers.", "text": "multilabel classiﬁcation relatively recent subﬁeld machine learning. unlike classical approach instances labeled category multilabel classiﬁcation arbitrary number categories chosen label instance. problem complexity common solution frequently used learning binary classiﬁer every category combining afterwards. assumption taken solution realistic work give examples decisions labels taken independently thus supervised approach learn existing relationships among categories make better classiﬁcation. therefore show generic methodology improve results obtained independent probabilistic binary classiﬁers using combination procedure classiﬁer trained co-occurrences labels. show exhaustive experimentation three diﬀerent standard corpora labeled documents present noticeable improvements them using methodology three probabilistic base classiﬁers. keywords multilabel classiﬁcation label dependency probabilistic classiﬁers text classiﬁcation work present novel solution multilabel categorization problem. kind problems subset categories assigned instance. multilabel classiﬁcation problems arise natural information processing concretely subﬁeld automatic document categorization nature important number text corpora multilabel kind. example news articles often belong category collections composed articles reuters agency). domains multiple labels assigned metadata give better description documents hand multilabel instances occur commonly internet many blog applications blog posts example categorized arbitrary number labels. furthermore collaborative environments users tags multilabel ordinary process. fact sometimes word label used instead category synonyms. recently multilabel classiﬁcation useful diﬀerent domains like instance analysis musical emotions scene image categorization protein gene function prediction medical diagnosis although instance given associated labels assigned whole normal approach solve problem ignore fact concentrate obtaining good solutions individual binary problems here propose solution takes account inter-category dependence trying natural associations order improve ﬁnal categorization results. content paper organized follows ﬁrst recall well-known problem multilabel supervised categorization reviewing previous works area together brief explanation semantic adding multiple labels instance instead one. based probabilistic foundations approach presented section results diﬀerent models. extensive experimentation test collections coming text categorization ﬁeld carried section prove validity proposal. finally light results previously obtained several conclusions future works pointed section stated follows given catetermined size. formally gories input instance space labeled data composed instances assigned labels yi}i=...n learning multilabel classiﬁer means inferring function words function able assign non-empty subsets labels unlabeled instance. order cope exponential output space classical approach consists dividing problem binary independent problems therefore learning binary classiﬁers although naive solution works reasonably well multilabel classiﬁcation literature used baseline. approach called binary relevance method often criticized ignoring existing correlations among labels. given fact many multilabel problems come multi-tagged collections likely tags associated content also presence/absence another tag. explain examples phenomenon section roughly speaking main motivation paper look carefully results individual binary classiﬁers modify results given model previously trained label assignment vectors explicitly captures relationships among categories therefore improves classiﬁcation results. shown approach utilizes simple powerful independence assumption results model complex binary relevance method still cheap enough. hundred references partially related multilabel categorization last years. ﬁrst sight possible solutions problem basically consist transforming single-label adapting learning procedure work multiple labels time. taxonomy given naming former solutions problem transformation methods algorithm adaptation latter ones. contribution adapts learning procedure multilabel learning review approaches second kind. almost typical classiﬁcation algorithms multilabel version. example adaptation entropy formula tree learning algorithm proposed multilabel classiﬁcation. lazy algorithms ﬁeld variations k-nn algorithm presented kind problems also k-nn combined logistic regression classiﬁer cope multiple labels. course variations algorithm shown intraclass dependencies improvement deﬁnition margin multilabel hand methods dealing multilabel classiﬁcation within probabilistic framework therefore closer approach. generative model trained using training data completed algorithm computed probable vector categories assigned document. subset reuters- used experimentation noticeable improvements shown. generative model also presented here main assumption words documents belonging several categories characterized mixture characteristic words related categories assumption conﬁrmed experimentation. ﬁrst second order models built learning algorithms proposed alternatives. experiments carried webpages gathered yahoo.com server. presented results good improve methods naive bayes k-nn. recently proposed novel method called probabilistic classiﬁer chains exploits label dependence showing method outperforms others terms loss functions. claimed extensive experimentation artiﬁcial real datasets. section enumerate three clear examples showing possible reasons manual indexer assign multiple labels. although pretend exhaustive think three presented examples common occur easily multilabel problems mixture topics. instance matches abstract description several categories. case example medical paper deals several topics represented mesh keywords. contextualization. tags added order context label used. example scene classiﬁcation picture ﬁshermen working coast motril tagged people contextualized town order distinguish submarine photos. overlapping labels. subsets labels admit instances belonging them. example music classiﬁcation inconceivable songs tagged baroque reggae although combinations ﬂamenco jazz possible. pure multilabel phenomenon ﬁrst one. second third denote occurrence labels based content instance also occurrence labels. second case label added contextualize previously given labels. occurrence certain subset labels increases likelihood although ﬁrst phenomenon initially captured diﬀerent binary classiﬁers second third tackled looking labels training content instances. therefore contribution state ﬁnal labeling instance certain category combination result binary classiﬁer evidence given categories binary classiﬁers taking account existing relationships among categories captured training set. fact issue label dependence recently shown crucial multilabel learning modeled probabilistic framework give rich language describe procedure. instance. suppose categories cp}. every category deﬁne binary random variable cj}. assume probabilistic binary classiﬁers based content instances thus conditional probability represents probability instance labeled assuming perfect knowledge underlying probability distribution classiﬁers deﬁne labeling rules follows classify alternatively classify every category shall also deﬁne random vector binary variables represents labeling instance classiﬁers. shall note particular value vector thus every component vector binary corresponds variable otherwise. model start making reasonable simplifying assumption given true value category events ﬁnding certain instance certain labeling categories instance independent ¿from point view certain category value known equation assumes values associated categories probabilistically independent content instance assumption might seem unrealistic basically performed naive bayes classiﬁer shall show later that case naive bayes classiﬁer assumption neither intuitive much realistic result good classiﬁcation performance. nevertheless stressed assumption mean independence labels independence labels content given label. clearly expressed term explicitly model clear dependence label labels represented term general used sense analogy classic naive bayes assumption previous labelings document along content considered general features assumption means apply features. taking account binary classiﬁcation holds values simply obtained probabilistic binary classiﬁer category prior probabilities estimated number instances belong class total number instances. probabilities estimated learning process labels training data every instance features binary values telling instance belongs categories model need train binary classiﬁers content instances binary classiﬁers labels assigned instances. last point clariﬁed model. given instance classify easily compute category values however obtain probability would need know true assignments labels model shall make second noticed model summed really need assumption random vectors made binary variables. fact required binary variables ones. thus equation applied equally variables continuous. represent degree belief labels assigned instance. note that order extended version model need classiﬁer compute capable dealing continuous inputs whole process summed figure note computational load respect binary relevance method low. initially apart binary classiﬁers based content label classiﬁer needs trained class. however label classiﬁers usually cheap train work input space possessing features classiﬁcation part binary relevance method needs example classiﬁed binary classiﬁers. scores classiﬁers obtained extra classiﬁcations needed label classiﬁers additional computations needs performed summarized seen neither much additional memory space computational power needed perform approach. therefore scalability issue. shown method deal relationships among labels using probabilistic classiﬁers. presented methodology. concrete problem decisions made underlying models used ﬁrst content classiﬁers second label classiﬁers. choice relies heavily kind problem selected suitable previous experimentation working methods. also note choices independent certain criterion. course exhaustive giving long list experiments. aware many diﬀerent collections combinations classiﬁers could selected tried make good experimentation selecting restricted representative classiﬁers. first reuters- instance) collection news articles. used famous split divides documents training test categories assigned documents test removed terms given. number categories mesh thesaurus huge often chosen subset categories root categories hierarchy. documents belong subtree categories discarded resulting corpus called ohsumed-. methodology followed finally corpus relatively recent corpus also based reuters news stories. contains many documents documents preprocessed stopwords removed terms already stemmed. number categories named topic codes standard split also provided called lyrl gives training documents test removed categories appear training ﬁrst cases stopwords list used consists stopwords smart retrieval system also english stemming algorithm snowball package applied resulting words. reuters- also marks removed. order reduce size lexicon terms occurring less three documents removed reuters- ohsumed- less documents done document preprocessing stage made daurolab subsection discuss diﬀerent evaluation measures used experimentation. following taxonomy considered selected label-based measures example-based ranking measures. shall present chosen measures along brief explanation them discussion performance aspects considered one. measures speciﬁcally designed multilabel problems. account performance multilabel task available selected hamming loss subset loss. note that measures losses lower value have better classiﬁcation hamming loss computes normalized hamming distance assigned labels predicted ones. certain instance predicted labels true labels hamming loss equal classical measures binary classiﬁcation problems. made cases hard categorization then used suitable measures task. deﬁned every category precision recall stand true positives false positives false negatives j-th category. selected f-measure adapted categorization macro micro averaged versions details. implemented ranking measure error evaluates many times label ranked relevant labels instance. measure important takes partially account ranking perform evaluation. real-world cases multilabel classiﬁcation predicted labels suggested human indexer using ranking procedure would important labels list relevant. order have ﬁrst baseline secondly basic content classiﬁer used afterwards model considered three diﬀerent classiﬁers widely used literature. usually obtains discrete results better results diﬀerent nature. ﬁrst case selected multinomial naive bayes binary version second case k-nn classiﬁer used normalizing output order obtain value interpreted probability. finally linear probabilistic output performed platt’s algorithm chosen. recall ﬂexibility procedure high probabilistic classiﬁer could used instead three. k-nn classiﬁer best performing value selected based previous experimentation existing works. thus chose reuters ohsumed- value used also following references performed feature selection reuters feature selection performed ohsumed- noted ﬁrst hand based previous experimentations chosen logistic regression-based classiﬁer model posterior probability distribution category given correct labels categories namely selected classiﬁer instead proposals three reasons ﬁrst deal real inputs. second discriminative method joint distribution finally simple fast learn works reasonably well almost environments. order accurate estimates dimensionality problem high method selected learn weights bayesian logistic regression concretely proposed gaussian priors. implementation used included weka package default parameters details). ﬂexibilities logistic regression classiﬁer model distributions real inputs. approximations input vector classiﬁer propose diﬀerent models called seems reasonable that content classiﬁers perform well model accurate model shall discuss point afterwards. complement previous added results obtained using linear support vector machine probabilistic outputs thus input vector output values binary classiﬁers real-valued outputs linear classiﬁer therefore transformed probabilities model learned training data mentioned algorithm. order show validity approach used linear kernel without modiﬁcation default parameters included canonical implementation weka class. well known linear svms tend perform reasonably well classiﬁcation tasks although generally outperformed kernelized counterparts. selected simplest model illustrate improve measures baseline binary classiﬁers without much eﬀort. present results reuters- ohsumed- denotes multinomial naive bayes k-nn nearest neighbors classiﬁer linear support vector machine probabilistic outputs. terms ‘blr’ ‘smo’ refers label classiﬁers used used term distinguish label classiﬁer content classiﬁer. classiﬁer denotes proposal binary real inputs presented corresponding label classiﬁer explained before. also results base classiﬁer shown comparison purposes. cases proposal version algorithm improved least measures baseline which measures improved. results noticeable even reaching gain case macro measure respect moreover statistical signiﬁcance tests every couple classiﬁers micro measure micro sign s-test performed macro measure chose macro s-test. presented constitute nowadays standard comparing kind experiments. nevertheless noticed s-test speciﬁcally designed measure taking account true positives true negatives also note macro s-test take account amount improvement number categories measure improved leading sometimes counter-intuitive results p-value less show tables sign denote baseline plus signiﬁcantly better baseline alone. signs used indicate fact p-value sign point signiﬁcant diﬀerence systems. table displays summary results showing number times models better signiﬁcantly better worse signiﬁcantly worse baseline. table number times baseline classiﬁer plus label classiﬁer either better/signiﬁcantly better/worse/signiﬁcantly worse corresponding baseline classiﬁer alone respect micro macro technique clearly improves classiﬁcation results baseline. macro experiments measures improved baselines. micro ones also good improvements found especially version classiﬁer improves baseline cases classiﬁer went worse presenting general smaller deltas worsening micro results rcv. comparing approaches model performs general worse continuous version regardless label classiﬁer used. binarization procedure removes information represented granularity assignment well captured classiﬁer general methodology seems beneﬁt classiﬁcation less populated categories without harming frequent categories. produces fact that general macro measures heavily lifted whereas micro measures improved lesser degree. improvement micro measure corpus quite small noted baseline value close best result obtained lewis sophisticated threshold tuning algorithm perhaps results better high value diﬃcult get. results show diﬀerent patterns measures. error behaves really good showing good performance three collections improving baseline versions almost cases. rest measures tend work well naive bayes subset loss improved cases ohsumed- cases reuters respectively. seems that cases better measure better smo. table number times baseline classiﬁer plus either better/worse corresponding baseline classiﬁer alone respect diﬀerent example-based rank measures cases performance improvement shown. fact also present naive bayes improvement obtained. again presents better performance general. fact example-based measures show great improvements explained fact method tends designed produce great increments macromeasures measures averaged instance except fact hamming loss accounts number false positives negatives regardless many labels predicted well. summary results shown figure number times models better measure corresponding baseline shown. paper proposed quite general methodology better manage multilabel classiﬁcation problems. based explicitly taking account dependences among labels. proposed method starting point binary classiﬁers able produce probabilistic output. information merged principled information generated another binary classiﬁers case trained using information labels assigned instances training label-based classiﬁers like content-based ones built using variety learning methods provided output interpreted probabilistically. carried experiments three well-known multilabel document collections using three diﬀerent content-based baseline classiﬁers label-based classiﬁers experiments conﬁrm methodology often tends improve results obtained baseline classiﬁers. combination methods well-known thresholding techniques open question. ﬁrst obvious question happens thresholding algorithm applied method ﬁrst glance improve results how? combination worthwhile? also sophisticated variant method better thresholding function studied. even combination proposals. future work would like study synergies proposal improving multilabel classiﬁcation thresholding techniques. another open question following using approach diﬀerent independence assumption given could given leading models. particular would like explore methods independence assumption relaxed think complete independence finally would like methodology diﬀerent environment. selected document categorization validating model natural form multilabeled instances. future would like work diﬀerent datasets example musical patterns protein data social network data think also suitable method. acknowledgements study jointly supported spanish research programme consolider ingenio consejer´ıa innovaci´on ciencia empresa junta andaluc´ıa projects miprcv p-tic- respectively.", "year": 2012}