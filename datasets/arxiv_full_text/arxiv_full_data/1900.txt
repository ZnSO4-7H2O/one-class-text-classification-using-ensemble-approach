{"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "tag": ["cs.CV", "cs.CL", "cs.LG", "I.2.6; I.2.7; I.2.10"], "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html .", "text": "paper present multimodal recurrent neural network model generating novel image captions. directly models probability distribution generating word given previous words image. image captions generated according distribution. model consists sub-networks deep recurrent neural network sentences deep convolutional network images. sub-networks interact multimodal layer form whole m-rnn model. effectiveness model validated four benchmark datasets model outperforms state-of-the-art methods. addition apply m-rnn model retrieval tasks retrieving images sentences achieves signiﬁcant performance improvement state-of-the-art methods directly optimize ranking objective function retrieval. project page work www.stat.ucla.edu/˜junhua.mao/m-rnn.html. obtaining sentence level descriptions images becoming important task many applications early childhood education image retrieval navigation blind. thanks rapid development computer vision natural language processing technologies recent work made signiﬁcant progress task many previous methods treat retrieval task. learn joint embedding features sentences images semantic space. methods generate image captions retrieving sentence database. thus lack ability generating novel sentences describing images contain novel combinations objects scenes. work propose multimodal recurrent neural networks model address task generating novel sentences descriptions images task image sentence retrieval. whole m-rnn model contains language model part vision part multimodal part. language model part learns dense feature embedding word most recently adopt simple strategy boost performance image captioning task signiﬁcantly. details shown section code related data available https//github.com/mjhucla/mrnn-cr. previous version work appears nips deep learning workshop title explain images multimodal recurrent neural networks http//arxiv.org/abs/. observed subsequent arxiv papers also recurrent neural networks topic cite work. gratefully acknowledge them. figure examples generated top-ranked retrieved sentences given query image iapr dataset. sentences well describe content images. show failure case fourth image model mistakenly treats lake misses people. examples coco dataset found project page www.stat.ucla.edu/˜junhua.mao/m-rnn.html. dictionary stores semantic temporal context recurrent layers. vision part contains deep convolutional neural network generates image representation. multimodal part connects language model deep together one-layer representation. m-rnn model learned using log-likelihood cost function errors backpropagated three parts m-rnn model update model parameters simultaneously. experiments validate model four benchmark datasets iapr flickr flickr coco show method achieves state-of-the-art performance signiﬁcantly outperforming methods three tasks generating novel sentences retrieving images given sentence retrieving sentences given image. framework general improved incorporating powerful deep representations images sentences. deep model computer vision natural language. methods based deep neural network developed rapidly recent years ﬁeld computer vision natural language. computer vision krizhevsky propose deep convolutional neural networks layers outperform previous methods large margin image classiﬁcation task imagenet challenge network structure widely used computer vision e.g. girshick design object detection framework based work. recently simonyan zisserman propose layers performs substantially better alexnet. natural language recurrent neural network shows state-of-the-art performance many tasks speech recognition word embedding learning recently rnns successfully applied machine translation extract semantic information source sentence generate target sentences sutskever image-sentence retrieval. many previous methods treat task describing images retrieval task formulate problem ranking embedding learning problem frome socher ﬁrst extract word sentence features uses dependency tree recursive neural network extract sentence features) well image features. optimize ranking cost learn embedding model maps sentence feature image feature common semantic feature space. directly calculate distance images sentences. recently karpathy show object level image features based object detection results generate better results image features extracted global level. figure illustration simple recurrent neural network multimodal recurrent neural network architecture. simple rnn. m-rnn model. inputs model image corresponding sentence descriptions. represents words sentence. start sign wstart sign wend training sentences. model estimates probability distribution next word given previous words image. consists layers deep time frame. number layer indicates dimension layer. weights shared among time frames. generating novel sentence descriptions images. generally three categories methods task. ﬁrst category assumes speciﬁc rule language grammar. parse sentence divide several parts gupta mannem part associated object attribute image uses conditional random field model farhadi uses markov random field model). kind method generates sentences syntactically correct. second category retrieves similar captioned images generates descriptions generalizing re-composing retrieved captions third category methods related method learns probability density space multimodal inputs using example deep boltzmann machines topic models generate sentences richer ﬂexible structure ﬁrst group. probability generating sentences using model serve afﬁnity metric retrieval. method falls category. closely related tasks method work kiros built log-bilinear model alexnet extract visual features. needs ﬁxed length context whereas model temporal context stored recurrent architecture allows arbitrary context length. shortly several papers appear record breaking results karpathy fei-fei vinyals donahue fang chen zitnick many built recurrent neural networks. demonstrates effectiveness storing context information recurrent layer. work major difference methods. firstly incorporate two-layer word embedding system m-rnn network structure learns word representation efﬁciently single-layer word embedding. secondly recurrent layer store visual information. image representation inputted m-rnn model along every word sentence description. utilizes capacity recurrent layer efﬁciently allows achieve state-ofthe-art performance using relatively small dimensional recurrent layer. experiments show strategies lead better performance. method still best-performing approach almost evaluation metrics. input word layer recurrent layer output layer activation input recurrent output layers time denoted respectively. denotes current word vector simple -of-n coding representation mikolov calculated follows size adaptive length input sequence. recurrent layers connect sub-networks different time frames. accordingly backpropagation need propagate error recurrent connections back time structure multimodal recurrent neural network shown figure layers time frame word embedding layers recurrent layer multimodal layer softmax layer). word embedding layers embed one-hot input dense word representation. encodes syntactic semantic meaning words. semantically relevant words found calculating euclidean distance dense word vectors embedding layers. sentence-image multimodal models frome socher kiros pre-computed word embedding vectors initialization model. contrast randomly initialize word embedding layers learn training data. show random initialization sufﬁcient architecture generate state-of-the-art result. treat activation word embedding layer ﬁnal word representation three direct inputs multimodal layer. word embedding layers recurrent layer dimensions. calculation recurrent layer slightly different calculation simple rnn. instead concatenating word representation time recurrent layer activation time ﬁrst vector space together represents element-wise addition. rectiﬁed linear unit inspired recent success training deep structure computer vision ﬁeld krizhevsky differs simple sigmoid function adopted relu faster harder saturate overﬁt data non-linear functions like sigmoid. backpropagation time conducted sigmoid function vanishing exploding gradient problem appears since even simplest model large temporal depth previous work heuristics truncated bptt avoid problem. truncated bptt stops bptt time steps hand-deﬁned hyperparameter. good properties relu need stop bptt early stage leads better efﬁcient utilization data truncated bptt. recurrent layer dimensional multimodal layer connects language model part vision part m-rnn model layer three inputs word-embedding layer recurrent layer image representation. image representation activation layer alexnet layer vggnet though framework image features. activation three layers multimodal feature space together obtain activation multimodal layer simple m-rnn models softmax layer generates probability distribution next word. dimension layer vocabulary size different different datasets. train m-rnn model adopt log-likelihood cost function. related perplexity sentences training given corresponding images. perplexity standard measure evaluating language model. perplexity word sequence calculated follows length word sequence denotes perplexity sentence given image probability generating word given previous words wn−. corresponds activation softmax layer model. cost function model average log-likelihood words given context words corresponding images training sentences plus regularization term. calculated perplexity denotes number sentences number words training receptively denotes length sentences represents model parameters. training objective minimize cost function equivalent maximize probability generating sentences training using model. cost function differentiable backpropagation learn model parameters. trained m-rnn model three tasks sentences generation image retrieval sentence retrieval sentence generation process straightforward. starting start sign wstart arbitrary number reference words model calculate probability distribution next word sample probability distribution pick next word. practice selecting word maximum probability performs slightly better sampling. that input picked word model continue process model outputs sign wend. retrieval tasks model calculate probability generating sentence probability treated afﬁnity sentence retrieval task trickier might sentences high probability perplexity image query solve problem kiros uses perplexity sentence conditioned averaged image feature across training reference perplexity normalize original perplexity. different them normalized probability normalization factor marginal probability images sampled training set. approximate constant ignore term. strategy leads much better performance kiros experiments. normalized probability equivalent probability symmetric probability vggnet imagenet dataset recently karpathy show using rcnn object detection results combined alexnet features performs better simply treating image whole frame. experiments show method performs much better karpathy image features used better comparable results even sophisticated features based object detection. update vision part model according gradient backpropagated multimodal layer. paper image features deep network training stage shortage data. future work apply method large datasets ﬁnetune parameters deep network training stage. m-rnn model trained using baidu’s internal deep learning platform paddle allows explore many different model architectures short period. hyperparameters layer dimensions choice non-linear activation functions tuned cross-validation flickrk dataset ﬁxed across experiments. takes average generate sentence single core cpu. test method four benchmark datasets sentence level annotations iapr flickr flickr coco iapr tc-. dataset consists around images taken different locations around world. contains images different sports actions people animals cities landscapes etc. image provides least sentence annotation. average sentence annotations image. adopt standard separation training testing previous works kiros images training images testing. flickrk. dataset consists images extracted flickr. image provides sentence annotations. adopt standard separation training validation testing provided dataset. images training images validation images testing. flickrk. dataset recent extension flickrk. image also provides sentences annotations. consists crowd-sourced captions describing images. grammar style annotations dataset similar flickrk. follow previous work used images testing. dataset well flickk dataset originally used image-sentence retrieval tasks. coco. current release recently proposed dataset contains training images validation images. image provides sentences annotations. randomly sampled images validation images testing currently released validation set. dataset partition coco flickrk available project page sentence generation. following previous works sentence perplexity bleu scores evaluation metrics. bleu scores originally designed automatic machine translation rate quality translated sentences given several reference sentences. similarly treat sentence generation task translation content images sentences. bleu remains standard evaluation metric sentence generation methods images though drawbacks. images reference sentences might contain possible descriptions image bleu might penalize correctly generated sentences. please details calculation bleu scores task supplementary material section sentence retrieval image retrieval. adopt evaluation metrics previous works frome karpathy tasks sentences retrieval image retrieval. measurement. recall rate correctly retrieved groundtruth given candidates. higher usually means better retrieval performance. since care top-ranked retrieved results scores smaller important. another metric median rank ﬁrst retrieved groundtruth sentence image. lower usually means better performance. iapr datasets additional evaluation metrics conduct fair comparison previous work please details supplementary material section results sentence generation task shown table ours-rnn-base serves baseline method m-rnn model. architecture m-rnn except image representation input. conduct fair comparison follow experimental settings kiros calculate bleu scores perplexity. evaluation metrics necessarily correlated following reasons. mentioned section perplexity calculated according conditional probability word sentence given previous reference words. therefore strong language model successfully captures distributions words sentences perplexity without image content. content generated sentences might uncorrelated images. table although baseline method generates perplexity bleu score indicating fails generate sentences consistent content images. www.stat.ucla.edu/˜junhua.mao/m-rnn.html bleu outputted implementation slightly lower recently released coco caption evaluation toolbox different tokenization methods sentences. reevaluate method using toolbox current version paper. table results median rank flickrk dataset. -alexnet denotes image representation based alexnet extracted whole image frame. -rcnn denotes image representation extracted possible objects detected rcnn algorithm. retrieval tasks since publicly available results dataset report scores method table future comparisons. result shows top-ranked retrieved sentences top-ranked retrieved images groundtruth. also adopt additional evaluation metrics compare method kiros supplementary material section dataset widely used benchmark dataset image sentence retrieval. different methods shown table compare model several state-ofthe-art methods sdt-rnn devise deepfe various image representations. model outperforms methods large margin using image representation also list performance methods using sophisticated features table -avg-rcnn denotes methods features average activation objects detection conﬁdence threshold. deepfe-rcnn karpathy uses fragment mapping strategy better exploit object detection results. results show using features improves performance. even without help object detection methods however method performs better methods almost evaluation metrics. develop framework using better image features based object detection future work. generated sentences using m-rnn-alexnet model dataset respectively. table properties recurrent layers recent methods. lrcn stack four dimensional lstm layers. achieves state-of-the-art performance using relatively small dimensional recurrent layer. lstm treated sophisticated version rnn. compare method several state-of-the-art methods recently released dataset devise deepfe mnlm dmsm lrcn deepvs results retrieval tasks sentence generation task shown table table respectively. also summarize properties recurrent layers adopted recent methods table select word maximum probability time sentence generation process table many comparing methods uses beam search scheme keeps best candidates. beam search scheme lead better performance practice using model. method vggnet image representation outperforms state-of-the-art methods including recently released methods almost evaluation metrics. note dimension recurrent layer model relatively small compared competing methods. shows advantage efﬁciency method directly inputs visual information multimodal layer instead storing recurrent layer. mrnn model vggnet performs better alexnet indicates importance strong image representations task. generated sentences coco datasets novel also validate method test coco evaluation server results shown table evaluate model greedy inference well beam search inference. represents results using reference sentences represents results using reference sentences. validate importance different components m-rnn model train several variants original m-rnn model compare performance. particular show two-layer word embedding system outperforms single-layer version strategy directly inputting visual information multimodal layer substantially improves performance limited space details experiments section supplementary material main paper. recently devlin proposed nearest neighbor approach retrieves captions nearest images training ranks captions according consensus caption w.r.t. rest captions output ranked one. inspired method ﬁrst adopt m-rnn model transposed weight sharing strategy denoted m-rnn-shared) generate hypotheses using beam search scheme. speciﬁcally keep best candidates sentence generation process model generates sign wend. best candidates approximately probable sentences generated model treated hypotheses. experiments since gives diversiﬁed hypotheses without much outliers validation set. generating hypotheses target image retrieve nearest neighbors image feature space training calculate consensus scores hypotheses w.r.t. groundtruth captions nearest neighbor images rerank hypotheses according scores types image features nearest neighbor image search ﬁrst original image features extracted vggnet ﬁrst resize image short side pixels. extract features windows figure sample images nearest neighbors retrieved types features. compared original vggnet features features reﬁned m-rnn model better capturing richer accurate visual information. resized image. finally average pool features make dimensional feature. second type feature reﬁned m-rnn model. calculated weight matrix image representation multimodal layer scaled hyperbolic tangent function. show sample images nearest neighbors figure compared original vggnet features features reﬁned m-rnn model capture richer accurate visual information. e.g. target image second contains woman bunch bananas. original vggnet features retrieve images bananas them. suppose nearest neighbor images training reference. follow devlin calculate consensus score hypotheses. difference devlin treat captions nearest neighbor images hypotheses hypotheses generated m-rnn model. speciﬁcally hypothesis calculate mean similarity hypothesis captions nearest neighbor images. table results m-rnn-shared model applying consensus reranking using nearest neighbors references compared original m-rnn model validation coco test server. consensus score hypothesis mean similarity score nearest captions. similarity hypothesis nearest neighbor reference captions deﬁned sentence-level bleu score sentence-level cider cross-validate hyperparamters bleu-based similarity optimal respectively. cider-based similarity optimal respectively. show results model validation coco testing server table bleu-based consensus reranking improvement points validation points coco test terms bleu score. cider-based consensus reranking improvement points validation points coco test terms cider. also show oracle performance hypotheses upper bound consensus reranking. speciﬁcally image validation rerank hypotheses according scores w.r.t groundtruth captions. results oracle reranking shown table oracle performance surprisingly high indicating still room improvement m-rnn model reranking strategy. propose multimodal recurrent neural network framework performs state-of-the-art three tasks sentence generation sentence retrieval given query image image retrieval given query sentence. model consists deep deep sub-networks interact multimodal layer. m-rnn powerful connecting images sentences ﬂexible incorporate complex image representations sophisticated language models. thank andrew chang huang duohao haoyuan jason eisner useful discussions technical support. also thank comments suggestions anonymous reviewers iclr nips deep learning workshop. acknowledge center minds brains machines partially funded award ccf- -cs. kyunghyun merrienboer bart gulcehre caglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. devlin jacob cheng fang gupta saurabh deng xiaodong zweig geoffrey mitchell margaret. language models image captioning quirks works. arxiv preprint arxiv. devlin jacob gupta saurabh girshick ross mitchell margaret zitnick lawrence. exploring nearest neighbor approaches image captioning. arxiv preprint arxiv. donahue jeff hendricks lisa anne guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor. long-term recurrent convolutional networks visual recognition description. arxiv preprint arxiv. fang gupta saurabh iandola forrest srivastava rupesh deng doll´ar piotr jianfeng xiaodong mitchell margaret platt john captions visual concepts back. arxiv preprint arxiv. farhadi hejrati mohsen sadeghi mohammad amin young peter rashtchian cyrus hockenmaier julia forsyth david. every picture tells story generating sentences images. eccv kuznetsova polina ordonez vicente berg tamara choi yejin. treetalk composition compression trees image descriptions. transactions association computational linguistics tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. arxiv preprint arxiv. junhua yang wang jiang huang zhiheng yuille alan. learning like child fast novel visual concept learning sentence descriptions images. arxiv preprint arxiv. mitchell margaret xufeng dodge jesse mensch alyssa goyal amit berg alex yamaguchi kota berg tamara stratos karl daum´e hal. midge generating image descriptions computer vision detections. eacl russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge young peter alice hodosh micah hockenmaier julia. image descriptions visual denotations similarity metrics semantic inference event descriptions. table performance comparison different versions m-rnn models flickrk dataset. models adopt vggnet image representation. figure details models. section compare different variants m-rnn model show effectiveness two-layer word embedding strategy input visual information multimodal layer. word embedding system. intuitively word embedding layers capture high-level semantic meanings words efﬁciently single layer word embedding. input multimodal layer offers useful information predicting next word distribution. validate efﬁciency train three different m-rnn networks m-rnn-noembinput m-rnnonelayeremb m-rnn-emboneinput. illustrated figure m-rnn-noembinput denotes m-rnn model whose connection word embedding layer multimodal layer off. thus multimodal layer inputs recurrent layer image representation. m-rnn-onelayeremb denotes m-rnn model whose word embedding layers replaced single dimensional word-embedding layer. much parameters word-embedding layers m-rnn-onelayeremb original m-rnn dictionary size large. m-rnnemboneinput denotes m-rnn model whose connection word embedding layer multimodal layer replaced connection word embedding layer multimodal layer. performance comparisons shown table table shows original m-rnn model word embedding layers connection word embedding layer multimodal layer performs best. veriﬁes effectiveness word embedding layers. connect vision language part model. train three variants m-rnn models image representation inputted recurrent layer m-rnn-visualinrnn m-rnn-visualinrnn-both m-rnn-visualinrnn-both-shared. m-rnn-visualinrnn input image representation word embedding layer later models input image representation multimodal layer word embedding layer weights connections shared m-rnn-visualinrnn-both-shared. please details models figure table shows original m-rnn model performs much better models indicating effective directly input visual information multimodal layer. practice harder train variants train original m-rnn model keep learning rate small avoid exploding gradient problem. increasing dimension recurrent layer replacing lstm might solve problem. explore issue future work. retrieval results dataset addition also adopt exactly evaluation metrics kiros plot mean number matches retrieved groundtruth sentences images respect percentage retrieved sentences images testing set. sentence retrieval task kiros uses shortlist images nearest neighbors query image feature space. shortlist strategy makes task harder similar images might similar descriptions often harder subtle differences among sentences pick suitable one. recall accuracy curves respect percentage retrieved images sentences shown figure ﬁrst method bowdecaf strong image based bag-of-words baseline second third models multimodal deep models. m-rnn model signiﬁcantly outperforms three methods task. bleu score proposed papineni originally used evaluation metric machine translation. calculate bleu-n score ﬁrst compute modiﬁed n-gram precision compute geometric mean length multiply brevity penalty length reference sentence length generated sentence. strategy fang computed whole testing corpus. multiple reference sentences length reference closest length candidate used compute", "year": 2014}