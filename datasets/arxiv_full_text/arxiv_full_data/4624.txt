{"title": "Low-complexity modular policies: learning to play Pac-Man and a new  framework beyond MDPs", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper we propose a method that learns to play Pac-Man. We define a set of high-level observation and action modules. Actions are temporally extended, and multiple action modules may be in effect concurrently. A decision of the agent is represented as a rule-based policy. For learning, we apply the cross-entropy method, a recent global optimization algorithm. The learned policies reached better score than the hand-crafted policy, and neared the score of average human players. We argue that learning is successful mainly because (i) the policy space includes the combination of individual actions and thus it is sufficiently rich, (ii) the search is biased towards low-complexity policies and low complexity solutions can be found quickly if they exist. Based on these principles, we formulate a new theoretical framework, which can be found in the Appendix as supporting material.", "text": "ab\u0000e give e\u0004\bab\u0000e a\u0004\u0004\u0006\u0003xi\u0001a\bi\u0003\u0002 \u0003b\bai\u0002 be\b\be\u0006 \u0007a\u0001\u0004\u0000i\u0002g di\u0007\b\u0006ib \bi\u0003\u0002. wi\u0000\u0000 f\u0006\u0003\u0001 fa\u0001i\u0000y \u0004a\u0006a\u0001e\be\u0006ized di\u0007\b\u0006ib \bi\u0003\u0002\u0007\b de\u0002\u0003\bed \u0006ibe a\u0000g\u0003\u0006i\bh\u0001 \bha\b i\be\u0006a\bive\u0000y i\u0001\u0004\u0006\u0003ve\u0007 \u0004a\u0006a\u0001e\be\u0006\u0007 di\u0007\b\u0006ib \bi\u0003\u0002 av\u0003id\u0007 \u0004\u0006\u0003b\u0000e\u0001 \u0001aki\u0002g \u0003\u0001\u0004\u0006\u0003\u0001i\u0007e \u0004\u0006efe\u0006\u0007 \u0000a\u0006ge i\u0001\u0004\u0006\u0003ve\u0001e\u0002\b\u0007\b d\u0003e\u0007 \u0000\u0003w\b d\u0003e\u0007 high ei\bhe\u0006 thi\u0007 \u0003\u0001\u0004\u0006\u0003\u0001i\u0007e hieved f\u0003\u0000\u0000\u0003w\u0007 be\u0007\b \u0007a\u0001\u0004\u0000e\u0007. h\u0003\u0003\u0007e\u0007 \u0006a\bi\u0003 \u0004\u0006\u0003vided \bha\b \u0007a\u0001\u0004\u0000e\u0007 a\u0006\u0006a\u0002ged thi\u0007 \u0003\u0006\u0006e\u0007\u0004\u0003\u0002d\u0007 \u0007e\b\bi\u0002g h\u0003\u0003\u0007e\u0007 di\u0007\b\u0006ib \bi\u0003\u0002 f\u0006\u0003\u0001 di\u0007\b\u0006ib \bi\u0003\u0002 fa\u0001i\u0000y \bha\b a\u0004\u0004\u0006\u0003xi\u0001a\be\u0007 be\u0007\b be\u0007\b \u0001i\u0002i\u0001izi\u0002g di\u0007\ba\u0002 \u0002if\u0003\u0006\u0001 di\u0007\b\u0006ib \bi\u0003\u0002 \u0003ve\u0006 e\u0000i\be \u0007a\u0001\u0004\u0000e\u0007. \u0001ea\u0007 di\u0007\ba\u0002 \u0004\u0003\u0000i \u0007\u0000\u0003\b\u0007. \u0007\u0000\u0003\b \u0000\u0000ed wi\bh f\u0006\u0003\u0001 \u0000eba\u0007e wi\bh \u0004\u0006\u0003babi\u0000i\by \u0000ef\b e\u0001\u0004\by wi\bh \u0004\u0006\u0003babi\u0000i\by \u0007\u0000\u0003\b \u0004\u0006i\u0003\u0006i\by f\u0006\u0003\u0001 e\u0000e\u0001e\u0002\b \bhi\u0007 \u0007e\b\b ided \bha\b \u0007\u0000\u0003\b \u0000\u0000ed\b \bhe\u0002 \u0004a\u0006\bi \u0007\u0000\u0003\b \u0004\u0003\u0000i \u0003\u0002\bai\u0002 \u0000e\u0007\b \u0004\u0003\u0007\u0007ib\u0000y \u0004\u0003\u0007\u0007ib\u0000e \u0001axi\u0001 \u0001be\u0006 a\u0000\u0000\u0003wed \u0004\u0003\u0000i b\u0007e\b a\u0004\u0004\u0000i ab\u0000e be\u0000\u0003\u0002gi\u0002g i\u0002dex b\u0007e\b a\u0004\u0004\u0000i ab\u0000e \u0004\u0006i\u0003\u0006i\bie\u0007. a\u0000\u0000\u0003wed \u0004\u0006i\u0003\u0006i\by a\u0004\u0004\u0000y \u0001e\bh\u0003d\b de\u0002e di\u0007\b\u0006ib \bi\u0003\u0002 \u0003ve\u0006 e\u0004i\u0007\u0003de de\u0002\u0003\be \u0004\u0006\u0003babi\u0000i\by \bha\b wi\u0000\u0000 \u0007e\u0000e \u0007ed\b |rn| h\u0003\u0003\u0007e f\u0006\u0003\u0001. \u0004\u0006\u0003babi\u0000i\by d\u0006aw f\u0006\u0003\u0001 i\u0002de\u0004e\u0002de\u0002\b be\u0006\u0002\u0003 di\u0007\b\u0006ib \bi\u0003\u0002\u0007 wi\bh di\u0006e a\u0004\u0004\u0000y", "year": 2006}