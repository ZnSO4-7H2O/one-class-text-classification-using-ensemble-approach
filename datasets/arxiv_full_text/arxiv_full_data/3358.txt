{"title": "A Statistical Test for Joint Distributions Equivalence", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We provide a distribution-free test that can be used to determine whether any two joint distributions $p$ and $q$ are statistically different by inspection of a large enough set of samples. Following recent efforts from Long et al. [1], we rely on joint kernel distribution embedding to extend the kernel two-sample test of Gretton et al. [2] to the case of joint probability distributions. Our main result can be directly applied to verify if a dataset-shift has occurred between training and test distributions in a learning framework, without further assuming the shift has occurred only in the input, in the target or in the conditional distribution.", "text": "provide distribution-free test used determine whether joint distributions statistically different inspection large enough samples. following recent efforts long rely joint kernel distribution embedding extend kernel two-sample test gretton case joint probability distributions. main result directly applied verify dataset-shift occurred training test distributions learning framework without assuming shift occurred input target conditional distribution. detecting dataset shifts occur fundamental problem learning need re-train system adapt data making wrong predictions. strictly speaking training data test data dataset shift occurs hypothesis sampled distribution wanes work provide statistical test determine whether shift occurred given samples training testing set. cope complexity joint distribution literature emerged recent years trying approach easier versions problem distributions assumed differ factor. example covariate shift when decomposition ppy|xqppxq ppy|xq qpy|xq ppxq qpxq. prior distribution shift conditional shift others deﬁned similar way. good reference reader want consider qui˜nonero-candela moreno-torres often happens assumptions strong hold practice require expertise data distribution hand cannot given granted. recent work long tried tackle question without making restricting hypothesis changing training test distributions. developed joint distribution discrepancy measuring distance joint distributions regardless everything else. build maximum mean discrepancy introduced gretton noticing joint distribution mapped tensor product feature space kernel embedding. main idea behind measure distance distributions comparing embeddings reproducing kernel hilbert space rkhs hilbert space functions equipped inner products x¨¨yh norms ||h. context work elements space probability distributions evaluated means inner products fpxq kpx¨qyh thanks reproducing property. kernel function takes care embedding deﬁning implicit feature mapping kpx¨q φpxq always xφpxq φpxqyh viewed measure similarity points characteristic kernel used embedding injective uniquely preserve information distribution according seminal work smola kernel embedding distribution ppxq given exrkpx¨qs exrφpxqs required tools place introduce jdd. deﬁnition unit ball rkhs. samples distributions respectively note that conversely long don’t square norm biased empirical estimation obtained replacing population expectation empirical expectation computed samples yqpx ymqqu samples jddbpf fgpf test null hypothesis would expect zero empirical converging towards zero samples acquired. following theorem provides bound deviations empirical ideal value zero. deviations happen practice large want reject null hypothesis. theorem deﬁned sec. sec. null hypothesis holds simplicity probability least consequence null hypothesis rejected signiﬁcance level satisﬁed. preserving interestingly type errors probability decreases zero rate convergence properties found kernel two-sample test gretton warn reader result obtained neglecting dependency sec. following deeper discussion. validate proposal handcraft joint distribution starting mnist data follows. sample image speciﬁc class deﬁne pair observation vertical horizontal projection histograms sampled image. fig. depicts process. number samples obtained described manner deﬁned belong class. easy distribution joint. experiments employed kernel known characteristic i.e. induces one-to-one embedding. formally distributed according indistinctly parameters experimentally accordingly kernels bounded ﬁrst experiment obtain tpxi yiqui sampling images class number similarly collect iqui applying rotation sampled images class. course samples come distribution null hypothesis rejected. opposite increases absolute value expect increase well point exceeding critical value deﬁned fig. illustrates behavior sampled increasingly different distributions. deepen analysis fig. study behavior critical value changing signiﬁcance level sample size figure show exemplar image drawn mnist dataset. observation projection histograms along axis i.e. obtained summing values across rows right depicts behavior measure samples drawn different distribution w.r.t. speciﬁcally distribution rotated images. rotation controlled parameter. green line shows critical value rejecting null hypothesis proof theorem based mcdiarmid’s inequality deﬁned joint distributions. result considered random variables distributions independent others despite clearly rarely case. however empirical experiments show encouraging results suggesting test could safely applied evaluate equivalence joint distributions broad independence cases. figure left depicts critical value test signiﬁcance level sample size change. cooler colors correspond lower values threshold. surprisingly conclusive desirable tests obtained either lowering increasing complementary shows convergence rate test threshold increasing size sample value worth noticing elbow convergence curve found around order prove test ﬁrst need introduce mcdiarmid’s inequality modiﬁed version rademacher average respect m-sample obtained joint distribution. theorem function exist denotes expectation random variables denotes probability variables. deﬁnition unit ball rkhs domain kernels bound yqpx ymqu i.i.d. sample drawn according probability measure i.i.d. taking values t´`u equal probability. deﬁne joint rademacher average fully exploit mcdiarmid’s inequality also need bound expectation jddb. similarly gretton exploit symmetrisation means ghost sample i.e. observations whose sampling bias removed expectation", "year": 2016}