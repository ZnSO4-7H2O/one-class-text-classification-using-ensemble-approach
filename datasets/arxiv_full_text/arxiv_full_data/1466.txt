{"title": "graph2vec: Learning Distributed Representations of Graphs", "tag": ["cs.AI", "cs.CL", "cs.CR", "cs.NE", "cs.SE"], "abstract": "Recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs. However, many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors. While the aforementioned approaches are naturally unequipped to learn such representations, graph kernels remain as the most effective way of obtaining them. However, these graph kernels use handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are hampered by problems such as poor generalization. To address this limitation, in this work, we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs. graph2vec's embeddings are learnt in an unsupervised manner and are task agnostic. Hence, they could be used for any downstream task such as graph classification, clustering and even seeding supervised representation learning approaches. Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels.", "text": "graph kernels handcrafted features. graph kernels prominent ways catering aforementioned graph analytics tasks. graph kernels evaluate similarity pair graphs recursively decomposing atomic substructures deﬁning similarity function substructures subsequently kernel methods could used performing classiﬁcation/clustering. however kernels exhibit critical limitations many provide explicit graph embeddings. renders using general purpose algorithms operate vector embeddings neural networks etc.) unusable graph data. substructures leveraged kernels ‘handcrafted’ i.e. determined manually speciﬁc welldeﬁned functions help extracting substructures graphs. however noted yanardag vishwanathan handcrafted features used large datasets graphs leads building high dimensional sparse non-smooth representations thus yield poor generalization. note replacing handcrafted features ones learnt automatically data could help aforementioned limitations. fact related domains text mining computer vision feature learning based approaches outperformed handcrafted ones signiﬁcantly across many tasks learning substructure embeddings. recently several approaches proposed learn embeddings graph substructures nodes paths subgraphs embeddings used directly substructure based analytics tasks node classiﬁcation community detection link prediction. however substructure representation learning approaches incapable learning representations entire graphs hence cannot used tasks graph classiﬁcation. recent works representation learning graph structured data predominantly focus learning distributed representations graph substructures nodes subgraphs. however many graph analytics tasks graph classiﬁcation clustering require representing entire graphs ﬁxed length feature vectors. aforementioned approaches naturally unequipped learn representations graph kernels remain effective obtaining them. however graph kernels handcrafted features hence hampered problems poor generalization. address limitation work propose neural embedding framework named graphvec learn data-driven distributed representations arbitrary sized graphs. graphvec’s embeddings learnt unsupervised manner task agnostic. hence could used downstream task graph classiﬁcation clustering even seeding supervised representation learning approaches. experiments several benchmark large real-world datasets show graphvec achieves signiﬁcant improvements classiﬁcation clustering accuracies substructure representation learning approaches competitive state-of-the-art graph kernels. graph-structured data ubiquitous nowadays many domains social networks cybersecurity biochemo-informatics. many analytics tasks domains graph classiﬁcation clustering regression require representing graphs ﬁxed-length feature vectors facilitate applying appropriate machine learning algorithms. instance vectorial representations programs’ call graphs could used detect graphs patchy-san proposed recently. oﬀer excellent performances supervised learning tasks pose critical limitations reduce usability deep neural network based representation learning approaches require large volume labeled data learn meaningful representations. obviously obtaining datasets challenge requires mammoth labeling effort. representations thus learnt speciﬁc particular task cannot used transferred tasks problems. instance graph embeddings chemical compounds mutag dataset details) learnt using speciﬁcally designed predict whether compound mutagenic eﬀect bacterium. hence embeddings could used tasks predicting properties compounds. circumvent limitations similar mentioned substructure representation learning approaches need completely unsupervised approach succinctly capture generic characteristics entire graphs form embeddings. best knowledge techniques available. hence driven motivation work take ﬁrst steps towards learning task-agnostic representations arbitrary sized graphs unsupervised fashion. approach. propose develop inspired neural embedding framework named graphvec. success recently proposed neural document embedding models extend learn graph embeddings. document embedding models exploit words/word sequences compose documents learn embeddings. analogically graphvec propose view entire graph document rooted subgraphs around every node graph words compose document extend document embedding neural networks learn representations entire graphs. best knowledge graphvec ﬁrst neural embedding approach learns representations whole graphs oﬀers following advantages graph kernels substructure embedding approaches unsupervised representation learning graphvec learns graph embeddings completely unsupervised manner i.e. class labels graphs required learning embeddings. allows readily graphvec embeddings plethora applications labeled data diﬃcult obtain. task-agnostic embeddings since graphvec leverage task-speciﬁc information representation learning process embeddings provides generic. allows embeddings across analytics tasks involving whole graphs. fact graphvec embeddings could used seed supervised representation learning approaches data-driven embeddings unlike graph kernels graphvec learns graph embeddings large corpus graph data. enables graphvec circumvent aforementioned disadvantages handcrafted feature based embedding approaches. graph embeddings them framework samples considers non-linear substructures namely rooted subgraphs learning embeddings. considering non-linear substructures known preserve structural equivalence hence ensures graphvec’s representation learning process yields similar embeddings structurally similar graphs. experiments. determine graphvec’s accuracy eﬃciency supervised unsupervised learning tasks several benchmark large real-world graph datasets. also perform comparative analysis several stateof-the-art substructure representation learning approaches graph kernels. experiments reveal graphvec achieves signiﬁcant improvements classiﬁcation clustering accuracies substructure embedding methods highly competitive state-of-theart kernels. speciﬁcally real-world program analysis tasks namely malware detection malware familial clustering graphvec outperforms state-of-the-art substructure embedding approaches respectively. mark real-world datasets demonstrate graphvec could signiﬁcantly outperform substructure representation learning algorithms highly competitive state-of-the-art graph kernels graph classiﬁcation clustering tasks. matrix representations graphs denoted r|g|×δ. nodes edges. assigns unique label alphabet every node otherwise considered unlabeled. instance weisfeiler-lehman kernel uses non-linear substructures computing kernels across graphs demonstrated outperform linear substructure kernels random walk kernel shortest path kernel many tasks goal learn distributed representations graphs extending recently proposed document embedding techniques multi-relational data. hence section review related background language modeling word document embedding techniques. modern neural embedding methods wordvec simple eﬃcient feed forward neural network architecture called skipgram learn distributed representations words. wordvec works based rationale words appearing similar contexts tend similar meanings hence similar vector representations. learn target word’s representation model exploits notion context context deﬁned ﬁxed number words surrounding target word. negative sampling simple eﬃcient algorithm helps alleviate problem train skipgram model. negative sampling selects small subset words random target word’s context updates embeddings every iteration instead considering words vocabulary. training ensures following word appears context another skipgram training converges semantically similar words mapped closer positions embedding space revealing learned word embeddings preserve semantics. forward extension wordvec learning embeddings words word sequences proposed mikolov docvec uses instance skipgram model called paragraph vector-distributed words capable learning representations arbitrary length word sequences sentences paragraphs even whole large documents. speciﬁcally given documents ...dn} sequence words wli} sampled document docvec ment word sampled i.e. respectively. model works considering word occurring congraphvec consider graphs analogical documents composed rooted subgraphs which turn analogical words special language extend document embedding models learn graph embeddings. background word document embeddings presented previous section important intuition extend graphvec view entire graph document rooted subgraphs around every node graph precise versions docvec namely pv-distributed memory pv-dbow. pv-dm instance skipgram model. learns document embeddings combining optimizing words occur within ﬁxed length context window corresponding documents. evidently pv-dm related proposed technique hence refrain discussing further. figure docvec’s skipgram model given document samples words considers co-occurring context uses learn representation. graphvec given graph samples rooted subgraphs around diﬀerent nodes occur uses analogous docvec’s context words thus learns representation. juncture important note substructures nodes walks paths could also considered atomic entities compose graph instead rooted subgraphs. however reasons make rooted subgraphs amenable learning graph embeddings higher order substructure. compared simpler lower order substructures nodes rooted subgraphs encompass higher order neighborhoods oﬀers richer representation composition graphs. hence embeddings learnt sampling higher order substructures would reﬂect compositions graphs better. non-linear substructure. compared linear substructures walks paths rooted subgraphs capture inherent non-linearity graphs better. fact evident considering graph kernels well. instance weisfeiler-lehman kernel based non-linear substructures oﬀer signiﬁcantly better performance many tasks linear substructure based kernels random walk shortest path kernels establishing mentioned analogy documents words graphs rooted subgraphs respectively utilize document embedding models learn graph embeddings. main expectation structurally similar graphs close embedding space. sense similar deep graph kernels graphvec’s embeddings provide means arrive data-driven graph kernel. similar document convention required input corpus graphs graphvec learn representations. given dataset graphs graphvec considers rooted subgraphs around every node vocabulary. subsequently following docvec skipgram training process learn representations graph dataset. train skipgram model mentioned fashion need extract rooted subgraphs assign unique label rooted subgraphs vocabulary. deploy relabeling strategy presented algorithm intend learn dimensional embeddings graphs dataset epochs. begin randomly initializing embeddings graphs dataset subsequently proceed extracting rooted subgraphs around every node graphs iteratively learn corresponding graph’s embedding several epochs steps represent core approach explained detail following subsections. facilitate learning graph embeddings rooted subgraph around every node graph extracted fundamentally important task approach. extract subgraphs follow wellknown relabeling process lays basis kernel subgraph extraction process explained separately algorithm algorithm takes root node graph subgraph extracted degree intended subgraph inputs sgvocab |sgvocab| large line algorithm procalculating hibitively expensive. hence follow negative sampling strategy calculate posterior probcle algorithm given graph rooted subgraphs context ...} graphs negative samples {sgn ...sgnk} sgvocab |sgvocab| intuitively negative samples subgraphs present graph whose embedding learnt vocabulary subgraphs. number negative samples hyper-parameter could empirically tuned. eﬃcient training evtrained embeddings corresponding subgraphs updated. subsequently update embeddings negative samples instead whole vocabulary. given pair graphs training makes embeddings closer composed similar rooted subgraphs similar time distances embeddings graphs composed similar subgraphs. stochastic gradient descent optimizer used optimize parameters line algorithm derivatives estimated using back-propagation algorithm. learning rate empirically tuned. could obtain embeddings graphs feed general purpose classiﬁers nueral networks svms perform classiﬁcation. juncture important note graph embedding procedure graph kernels substructure embeddings oﬀer ﬂexibility. speciﬁcally case methods kernel matrices computed using them could used conjunction kernel classiﬁers general purpose classiﬁers could used. graph clustering. given graph clustering goal group similar graphs together. graphvec’s embeddings could used along general purpose clustering algorithms k-means relational clustering algorithms aﬃnity propagation achieve this. again aforementioned limitations graph kernels substructure embeddings could used relational clustering algorithms. evaluate graphvec’s accuracy eﬃciency graph classiﬁcation clustering tasks. besides experimenting benchmark datasets also evaluate approach real-world graph analytics tasks ﬁeld program analysis namely malware detection malware familial clustering large malware datasets. research questions. speciﬁcally intend address following research questions graphvec compare state-of-the-art substructure representation learning approaches graph kernels graph classiﬁcation tasks terms accuracy eﬃciency benchmark datasets graphvec compare aforementioned state-of-the-art approaches realworld graph classiﬁcation task namely malware detection detection graphvec compare aforementioned state-of-the-art approaches real-world graph clustering task namely malware familial clustering. comparative analysis. proposed approach compared representation learning techniques namely nodevec subvec graph kernel techniques namely kernel deep kernel nodevec neural embedding framework learns feature representation individual nodes graphs. experiments obtain embeddings entire graphs using nodevec average nodes graph. subvec framework learns representations arbitrary subgraphs. therefore obtaining representation whole graphs using subvec straightforward procedure. kernel handcrafted feature based kernel decomposes graphs rooted subgraphs computes kernel values based them. besides kernel values also yields explicit vector representations graphs. deep explanations obtaining kernel matrix kernel representation learning variant kernel learns embeddings rooted subgraphs similar subgraphs similar embeddings. thus kernel values obtained using subgraph embeddings would unaﬀected limitations handcrafted features diagonal dominance. datasets. five benchmark graph classiﬁcation datasets namely mutag proteins used experiment. datasets belong chemobio-informatics domains speciﬁcations datasets used given table mutag data chemical compounds labeled according whether mutagenic eﬀect speciﬁc bacteria. dataset comprises compounds classes indicate carcinogenicity rats. proteins collection graphs whose nodes represent secondary structure elements edges indicate neighborhood amino-acid sequence space. datasets consist graphs respectively representing balanced subsets datasets chemical compounds screened activity non-small cell lung cancer ovarian cancer cell lines respectively. experiment datasets train classiﬁer samples chosen random evaluate performance test remaining samples. hyperparameters classiﬁers tuned based -fold cross validation training set. representation learning methods used common embedding dimensions arrived empirically. evaluation metrics. experiment repeated times average accuracy used determine eﬀectiveness classiﬁcation. eﬃciency determined terms time consumed building graph embeddings training testing durations reported directly related proposed method. accuracy. results obtained graphvec benchmark datasets summarized table results evident proposed approach outperforms representation learning kernel based techniques datasets comparable accuracy remaining datasets following inferences made table. technique could model local similarity within conﬁned neighborhood fails learn global structural similarities helps classify similar graphs together. especially evident results larger datasets proteins nodevec’s accuracy general results could conclude substructure embeddings techniques excel substructure based analytics tasks nodevec’s node classiﬁcation link prediction performances) extending directly tasks involving whole graphs yields sub-par accuracies. datasets. mainly fact strategy sample graph substructures learn embeddings particularly ill-suited obtaining embedding large graphs. subvec samples random walk given graph subsequently learns representations using ﬁxed length linear context skipgrams sampled walk. prevents subvec learning meaningful representations entire graph sampling random walk enough cover neighborhoods graph. ultimately prevents method appropriately modeling structural similarities across graphs reﬂects poor performance. also inference reinforced fact subvec accuracies decrease increase size graphs cater tasks graph classiﬁcation consistently provides good results datasets. deep kernel performs better kernel datasets addresses limitations latter kernel’s handcrafted features achieves better generalization. embedding appropriately models local global similarities among graphs consistently yields good results datasets. particular outperforms state-of-the-art methods mutag proteins dataset obtains slightly lesser accuracies datasets kernels. eﬃciency. pre-training phase compute vectors substructures graphs required aforementioned methods except kernel. hand kernel requires phase extract rooted subgraph features build handcrafted embeddings. case former approaches pre-training crucial step helps capturing latent substructure similarities graphs thus aids outperform handcrafted feature techniques. therefore important study cost pre-training. results pre-training/feature extraction durations methods study shown figure understandably kernel scalable method obtaining graph embeddings involve learning representations. nodevec learns embeddings lower order entities conﬁned explorations neighborhoods around hence takes less time pretraining. subvec learns graph embeddings sampling linear substructures running several iterations skipgram algorithm them. results signiﬁcantly high pretraining durations. deepwl kernel learns rooted subgraph embeddings using skipgram. takes much lesser duration subvec latter’s length sampled random walk much longer number samples rooted subgraphs former. finally approach learns embeddings higher order structures remains less scalable nodevec much scalable deep kernel subvec. fact that approach runs skipgram training limited number times approaches several times ﬁxed length linear context window. eﬃciency results experiments real-world datasets discussed subsequent subsections follow pattern discussed above. hence refrain discussing eﬃciency results after. embedding approaches reap considerable leverage exploiting volume data handcrafted approaches. therefore highly essential evaluate performance proposed method large real-world datasets showcase true potentials. experiment conﬁgurations. consider large-scale android malware detection problem given dataset dependency graphs malicious benign android apps task represent vectors train classiﬁers identify malicious ones. datasets statistics presented table evidently adgs much larger benchmark graphs. dataset comprises adgs contain nodes edges unique node labels average. training comprises samples chosen random remaining samples used test evaluate models. experiment repeated times results averaged. deep kernel exhibit excellent performance large-scale dataset. especially experiment range outperform techniques under comparison pronounced experiments benchmark datasets. again representation learning approaches nodevec subvec perform worse ill-suited learning embeddings entire graphs. goal experiment demonstrate eﬃcacy graphvec’s embedding downstream analytics task class labels graphs. task would appropriate evaluating comparing methods leverage task-speciﬁc information process learning representations. families belong task group samples belonging family cluster. consider dataset comprises android malware apps belonging families. statistics dataset presented table dataset large families corresponding malware samples considered clustering helps mitigate imbalance cluster sizes. embeddings kernels adgs belonging families built relational clustering algorithm namely used cluster them. evaluation metric. order quantitatively measure malware familial clustering accuracy standard clustering evaluation metric namely adjusted rand index used. values range ease understanding report percentage value. higher means higher correspondence groundtruth data. compared approaches highly signiﬁcantly. particular outperforms substructure embedding techniques kernels reinforces ﬁndings inferred classiﬁcation experiments. summary. summarizing inferences three experiments could trivially extending node subgraph representation learning approaches build graph embeddings yield sub-par results learning graph embedding data leads accurate results building using handcrafted features. since graphvec appropriately designed achieves excellent accuracies graph analytics tasks reasonably good eﬃciency. paper presented graphvec unsupervised representation learning technique learn embedding graphs arbitrary sizes. largescale experiments involving benchmark graph classiﬁcation datasets demonstrate graph embeddings learnt approach outperform substructure embedding approaches signiﬁcantly comparable graph kernels. since graphvec data-driven representation learning approach true potentials revealed trained large volumes graphs. evaluated real-world applications involving large graph datasets graphvec outperforms state-of-the-art graph kernels withcompromising eﬃciency overall performance. make code data used within work available grover aditya jure leskovec. nodevec scalable feature learning networks. proceedings sigkdd international conference knowledge discovery data mining.", "year": 2017}