{"title": "A Minimum Relative Entropy Principle for Learning and Acting", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is an agent that has been designed specifically for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert.", "text": "paper proposes method construct adaptive agent universal respect given class experts expert agent designed speciﬁcally particular environment. adaptive control problem formalized problem minimizing relative entropy adaptive agent expert suitable unknown environment. agent passive observer optimal solution well-known bayesian predictor. however agent active past actions need treated causal interventions stream rather normal probability conditions. shown solution variational problem given stochastic controller called bayesian control rule implements adaptive behavior mixture experts. furthermore shown mild assumptions bayesian control rule converges control suitable expert. behavior environment control signal fully known designer choose agent produces desired dynamics. instances problem include hitting target cannon known weather conditions solving maze controlling robotic manufacturing plant. however behavior plant unknown designer faces problem adaptive control. example shooting cannon lacking appropriate measurement equipment ﬁnding unknown maze designing autonomous robot martian exploration. adaptive control turns diﬃcult non-adaptive counterpart. good policy carefully trade explorative versus exploitative actions i.e. actions identiﬁcation environment’s dynamics versus actions control desired way. even environment’s dynamics known belong particular class optimal agents available constructing corresponding optimal adaptive agent general computationally intractable even simple problems thus ﬁnding tractable approximations major focus research. recently proposed reformulate problem statement classes control problems based minimization relative entropy criterion. example large class optimal control problems solved eﬃciently problem statement reformulated minimization deviation dynamics controlled system uncontrolled system work similar approach introduced. class agents given agent solves diﬀerent environment adaptive controllers derived minimum relative entropy principle. particular construct adaptive agent universal respect class minimizing average relative entropy environmentspeciﬁc agent. however extension straightforward. syntactical diﬀerence actions observations taken account formulating variational problem. speciﬁcally actions treated interventions obeying rules causality distinction made variational problem unique solution given stochastic control rule called bayesian control rule. control rule particularly interesting translates adaptive control problem on-line inference problem applied forward time. furthermore work shows mild assumptions adaptive agent converges environment-speciﬁc agent. paper organized follows. section introduces notation sets adaptive control problem. section formulates adaptive control minimum relative entropy problem. initial na¨ıve approach need causal considerations motivated. then bayesian control rule derived revised relative entropy criterion. section conditions convergence examined proof given. section illustrates usage bayesian control rule multi-armed bandit problem undiscounted markov decision problem. section discusses properties bayesian control rule relates previous work literature. section concludes. following agent environment formalized causal models sequences. agent environment coupled exchange symbols following standard interaction protocol discrete time observation control signals. treatment dynamics fully probabilistic particular actions observations random variables contrast decision-theoretic agent formulation treating observations random variables proofs provided appendix. more deﬁned one-way inﬁnite sequences based alphabet tuples written parentheses strings aaa. substrings following shorthand notation used string runs index written aiai+ ak−ak. similarly string starting ﬁrst index. also symbols underlined glue together like ao≤i aoao aioi. function meant taken w.r.t. base unless indicated otherwise. interactions. possible symbols drawn ﬁnite sets. denote inputs denote outputs interaction set. string ao≤t ao<tat interaction string similarly one-sided inﬁnite sequence aoao interaction sequence. interaction strings length denoted sets interaction strings sequences denoted respectively. valid here models true probability distribution interaction system probability producing action given history ao<t predicted probability observation given history ao<tat. hence sequence input stream sequence output stream. contrast roles actions observations reversed case system thus sequence output stream sequence input stream. model interaction fairly general many interaction protocols translated scheme. convention given interaction system agent constructed designer environment controlled agent. figure illustrates setup. intuitively means agent knows statistics environment’s future behavior past particular knows eﬀects given controls. environment known designer agent build custom-made policy resulting generative distribution produces interaction sequences desirable. done multiple ways. instance controls chosen resulting policy maximizes given utility criterion; resulting trajectory interaction system stays close enough prescribed trajectory. formally environment unknown task designing adaptive control problem. appropriate agent constitutes adaptive control problem. speciﬁcally work deals case designer already class agents tailored class possible environments. formally assumed going drawn probability {qm}m∈m possible systems interaction starts countable set. furthermore {pm}m∈m systems tailored interaction system generative distribution produces desirable interaction sequences. designer construct system behavior close possible main goal paper show problem adaptive control outlined previous section reformulated universal compression problem. informally motivated follows. suppose agent implemented machine interfaced environment whenever agent interacts environment agent’s state changes necessary consequence interaction. change state take place many possible ways updating internal memory; consulting random number generator; changing physical location orientation; forth. naturally design agent facilitates interactions complicates others. instance agent designed explore natural environment might incur memory footprint recording natural images memory-ineﬃcient recording artiﬁcially created images. abstracts away inner workings machine decides encode state transitions binary strings minimal amount resources bits required implement state changes derived directly associated probability distribution context adaptive control agent constructed minimizes expected amount changes necessary implement state transitions equivalently maximally compresses experience. thereby compression taken stand-alone principle design adaptive agents. coding theory problem compressing sequence observations unknown source known adaptive coding problem. solved constructing universal compressors i.e. codes adapt on-the-ﬂy source within predeﬁned class. codes obtained minimizing average deviation predictor true source constructing codewords using predictor. subsection procedure used derive adaptive agent formally deviation predictor true distribution measured relative entropy ﬁrst approach would construct agent minimize total expected relative entropy constructed follows. deﬁne history-dependent relative entropies action observation behavior described follows. given time maintains mixture systems weighting given mixture coeﬃcients whenever action observation produced weights updated according bayes’ rule. addition issues action suggested system drawn randomly according weights however important problem arises fact system passively observing symbols also actively generating them. subjective interpretation probability theory conditionals play role observations made agent generated external source. interpretation suits symbols issued environment. however symbols generated system require fundamentally diﬀerent belief update. intuitively diﬀerence explained follows. observations provide information allows agent inferring properties environment. contrast actions carry information environment thus incorporated diﬀerently belief agent. following section illustrate problem simple statistical example. causality study functional dependencies events. stands contrast statistics which abstract level said study equivalence dependencies amongst events. causal statements diﬀer fundamentally statistical statements. examples highlight diﬀerences many smokers lung cancer? opposed smokers lung cancer?; assign opposed newtonian physics. study causality recently enjoyed considerable attention researchers ﬁelds statistics machine learning. especially last decade signiﬁcant progress made towards formal understanding causation subsection provide essential tools required understand causal interventions. in-depth exposition causality reader referred specialized literature. illustrate need causal considerations case generated symbols consider following thought experiment. suppose statistician asked design model simple time series decides bayesian method. assume collects ﬁrst observation computes posterior probability density function parameters model given data using bayes’ rule likelihood given understands nature diﬀerent informative change belief state bayesian model non-informative thus reﬂection model’s belief state. hence would never condition bayesian model. mathematically seems imply statistician told source waiting simulated data point order produce next observation depend hands obtains observation using bayes’ rule posterior parameters thus order explain equation posterior given observed data generated data intervene order account fact non-informative given words statistician deﬁning value herself changed regime brings series mathematically expressed redeﬁning pdf. essential ingredients needed carry interventions. first needs know functional dependencies amongst random variables probabilistic model. provided causal model i.e. unique factorization joint probability distribution random variables encoding causal dependencies. general case deﬁnes partial order random variables. previous thought experiment causal model joint given conditional pdfs causal models contain additional information available joint probability distribution alone. appropriate model given situation depends story told. note intervention lead diﬀerent results causal models diﬀer. thus causal model following discussion previous section adaptive agent going constructed minimizing expected relative entropy time treating actions interventions. based deﬁnition conditional probabilities equation total expected relative entropy characterize using interventions going deﬁned. assuming environment chosen ﬁrst symbol depends importantly interventions index intervened probability distributions derived base probability distribution. hence ﬁxed intervention sequences form indexes probability distributions observation sequences this deﬁnes criteria indexed intervention sequences clear solution. deﬁne history-dependent intervened relative entropies action observation particular represents knowledge state past actions already issued next action known yet. then averaging previous relative entropies pasts yields again knowledge state time represented averages taken treating past actions interventions. finally deﬁne total expected relative entropy time averaged possible draws environment behavior diﬀers important aspect given time maintains mixture systems weighting systems given mixture coeﬃcients contrast updates weights whenever observation produced environment. update follows bayes’ rule treats past actions interventions dropping evidence provide. addition issues action suggested system drawn randomly according weights perhaps surprisingly theorem says optimal solution variational problem precisely predictive distribution actions observations treating actions interventions observations conditionals i.e. solution would obtain applying standard probability causal calculus. provides teleological interpretation agent akin na¨ıve agent constructed section adaptive control formalized problem designing agent unknown environment chosen class possible environments. environment-speciﬁc agents known bayesian control rule allows constructing adaptive agent combining agents. resulting adaptive agent universal respect environment class. context constituent agents called operation modes adaptive agent. represented causal models interaction sequences i.e. condiindex parameter characterizing operation mode. probability distribution input stream called hypothesis operation mode. following collects essential equations bayesian control rule. particular rule stated using recursive belief update. section develop suﬃcient conditions convergence provide proof convergence. simplify exposition analysis limited case controllers ﬁnite number input-output models. taking action collecting observation leads state sets states transitions represented enclosed areas similar venn diagram. choosing particular policy environment amounts partially controlling transitions taken state space thereby choosing probability distribution state transitions probability mass concentrates certain areas state space choosing policy thought choosing subset environment’s dynamics. following policy represented subset state space illustrated above. given observation models associated policies given action models sake simplifying interpretation policy diagrams assume existence state space function mapping histories states. assumption policies figure realization divergence processes associated controller operation modes divergence processes diverge whereas stay dotted bound. hence posterior probabilities vanish. fact statistical properties depend particular policy applied; hence given divergence process diﬀerent growth rates depending policy indeed behavior divergence process might depend critically distribution actions used. example happen divergence process stays stable policy diverges another. context bayesian control rule problem aggravated time step policy applied determined stochastically. speciﬁcally true operation mode random variable depends realization ao≤t drawn {aoτ}τ given conditions kept constant. deﬁnition plays role policy used sample actions time steps clearly realization divergence process decomposed general divergence process complex virtually classes distributions interest control well beyond assumptions i.i.d. stationarity. increased complexity jeopardize analytic tractability divergence process predictions asymptotic behavior made anymore. speciﬁcally growth rates divergence processes vary much realization realization posterior distribution operation modes vary qualitatively realizations. hence needs impose stability requirement akin ergodicity limit class possible divergence-processes class analytically tractable. purpose following property introduced. figure illustrates property. boundedness property going used construct results section. ﬁrst important result posterior probability true input-output model bounded below. wants identify operation modes whose posterior probabilities vanish enough characterize modes whose hypothesis match true hypothesis. figure illustrates problem. here three hypotheses along associated policies shown. share prediction made region diﬀer region hypothesis diﬀers everywhere others. assume true. long apply policy hypothesis make wrong predictions thus divergence process diverge expected. however evidence accumulated. applies policy long enough time controller eventually enter region hence accumulate counter-evidence long enough mean? executed short period controller risks visiting disambiguating region. unfortunately neither right policy right length period known beforehand. hence agent needs clever time-allocating strategy test policies ﬁnite time intervals. motivates following deﬁnition. probability sub-divergence pr{τ words agent apply m∗’s policy time step probability least strategy expected sub-divergence grows unboundedly core note demanding strictly positive probability execution time step guarantees agent possible ﬁnite time-intervals. following theorem shows posterior probabilities operation modes core vanish almost surely. even operation mode core i.e. given essentially indistinguishable m∗’s control still happen diﬀerent policies. figure shows example this. hypotheses share region diﬀer region addition operation modes policies respectively conﬁned region note operation modes core other. however policies diﬀerent. means unclear whether multiplexing policies time ever disambiguate hypotheses. undesirable could impede convergence right control law. section proof convergence bayesian control rule true operation mode provided ﬁnite operation modes. convergence result hold necessary conditions assumed boundedness consistency. ﬁrst boundedness imposes stability divergence processes partial inﬂuence policies contained within operation modes. condition regarded ergodicity assumption. second consistency requires hypothesis makes predictions another hypothesis within relevant subset dynamics hypotheses share policy. relevance formalized core operation mode. concepts proof strategies strengthen intuition potential pitfalls arise context controller design. particular could show asymptotic analysis recast study concurrent divergence processes determine evolution posterior probabilities operation modes thus abstracting away details classes distributions. extension results inﬁnite sets operation modes left future work. example could think partitioning continuous space operation modes essentially diﬀerent regions representative operation modes subsume neighborhoods consider multi-armed bandit problem problem stated follows. suppose -armed bandit i.e. slot-machine levers. pulled lever provides reward drawn bernoulli distribution bias speciﬁc lever. reward obtained probability reward probability −hi. objective game maximize time-averaged reward iterative probabilities {si}n pulls. continuum range stationary strategies parameterized indicating probabilities pulling lever. diﬃculty arising bandit problem balance reward maximization based knowledge already counts number times reward obtained pulling lever number times reward obtained respectively. observe summation discrete operation modes replaced integral continuous space conﬁgurations. last expression posterior distribution lever biases given product beta distributions. thus sampling action amounts ﬁrst sample operation mode obtaining bias beta distribution parameters choosing action corresponding highest bias maxj simulation bayesian control rule described compared agents ε-greedy strategy decay gittins indices test consisted bandits levers whose biases drawn uniformly figure comparison -armed bandit problem bayesian control rule ε-greedy agent using gittins indices runs averaged. panel shows evolution average reward. bottom panel shows evolution percentage times best lever pulled. beginning run. every agent play runs time steps each. then performance curves individual runs averaged. ε-greedy strategy selects random action small probability given εα−t otherwise plays lever highest expected reward. parameters determined empirically values several test runs. adjusted maximize average performance last trials simulations. gittins method indices computed horizon using geometric discounting i.e. close approximate time-averaged reward. results shown figure seen ε-greedy strategy quickly reaches acceptable level performance seems stall signiﬁcantly suboptimal level pulling optimal lever time. improved using value decays time. contrast gittins strategy bayesian control rule show essentially asymptotic performance diﬀer initial transient phase gittins strategy signiﬁcantly outperforms bayesian control rule. least three observations worth making here. first gittins indices pre-computed oﬀ-line. time complexity scales quadratically horizon computations horizon steps took several hours machines. contrast bayesian control rule could applied without pre-computation. second even though gittins method actively issues optimal information gathering actions bayesian control rule passively samples actions posterior distribution operation modes methods rely convergence underlying bayesian estimator. implies methods information bottleneck since bayesian estimator requires amount information converge. thus active information gathering actions aﬀect utility transient phase permanent state. eﬃcient algorithms bandit problems found literature markov decision process deﬁned tuple state space; action space; probability action taken state lead state immediate reward obtained state action interaction proceeds time steps time action issued state leading reward state starts next time step stationary closedloop control policy assigns action state. mdps always exists optimal stationary deterministic policy thus needs consider policies. undiscounted mdps average reward time step ﬁxed policy initial state deﬁned limt→∞ assumption markov chain policy ergodic. here assume mdps ergodic stationary policies. following q-notation watkins optimal policy characterized terms optimal average reward optimal relative q-values state-action pair solutions following system non-linear equations setup allows straightforward solution bayesian control rule possible known solution accordingly operation modes given obtain likelihood model inference realize equation rewritten predicts instantaneous reward mean instantaneous reward plus noise term given qm-values average reward labeled likelihood model easily devise conjugate prior distribution apply standard inference methods actions determined sampling operation modes posterior executing action suggested corresponding intervention models. resulting algorithm similar bayesian q-learning diﬀers actions selected. illustrate behavioral statistics algorithms. upper lower calculated ﬁrst last time steps randomly chosen runs. probability state color-encoded arrows represent frequent actions taken agents. panel presents curves obtained averaging runs. simulation tested mdp-agent grid-world example. give intuition achieved performance results contrasted achieved r-learning. used r-learning variant presented singh together uncertainty exploration strategy corresponding update equations learning rates. exploration strategy chooses ﬁxed probability pexp action maximizes constant represents number times action tried state thus higher values enforce increased exploration. mahadevan grid-world described especially useful test analysis algorithms. purposes particular interest easy design experiments containing suboptimal limit-cycles. figure panel illustrates goal state. step agent move adjacent space agent reaches goal state next position randomly square grid start another trial. also oneway membranes allow agent move direction other. experiments membranes form inverted cups agent enter side leave bottom playing role local maximum. transitions stochastic agent moves correct square probability free adjacent spaces probability rewards assigned follows. default reward agent traverses membrane obtains reward reaching goal state assigns parameters chosen simulation following. mdp-agent chosen hyperparameters precision r-learning chosen learning rates exploration constant total runs carried algorithm. results presented figure table r-learning learns optimal policy given suﬃcient exploration whereas bayesian control rule learns policy successfully. figure learning curve r-learning initially steeper bayesian controller. however latter attains higher average reward around time step onwards. attribute shallow initial transient phase distribution operation modes also reﬂected initially random exploratory behavior. idea work extend minimum relative entropy principle i.e. variational principle underlying bayesian estimation problem adaptive control. coding point view work extends idea maximal compression observation stream whole experience agent containing agent’s actions observations. minimizes amount bits write saving/encoding extension non-trivial important caveat coding sequences unlike observations actions carry information could used inference adaptive coding actions issued decoder itself. problem inference ones actions logically inconsistent leads paradoxes seemingly innocuous issue turned intricate investigated intensely recent past researchers focusing issue causality work contributes body research providing evidence actions cannot treated using probability calculus alone. causal dependencies carefully taken account minimizing relative entropy leads rule adaptive control called bayesian control rule. rule allows combining class task-speciﬁc agents agent universal respect class. resulting control simple stochastic control rule completely general parameter-free. analysis paper shows control rule converges true control mild assumptions. maker chooses policy maximizing expected utility outcomes equivalent choosing kronecker delta function probability distribution policies. case conditional probabilities coincide hence formalization actions interventions observations conditions perfectly compatible decision-theoretic setup fact generalizes decision variables status intervened random variables. bayesian control rule essentially bayesian predictor thereby entails modeling paradigm. designer deﬁne class hypotheses environments construct appropriate likelihood models choose suitable prior probability distribution capture model’s uncertainty. similarly sufﬁcient domain knowledge analogous procedure applied construct suitable operation modes. however many situations diﬃcult even intractable problem itself. example design class operation modes pre-computing optimal policies given class environments. formally class hypotheses modeling environments class policies. given however computing optimal policy many cases intractable. cases remedied characterizing operation modes optimality equations solved probabilistic inference example agent section recently applied similar approach adaptive control problems linear quadratic regulators problem bayesian inference problem. hence problems typically associated bayesian methods carry agents constructed bayesian control rule. problems analytical computational nature. example many probabilistic models posterior distribution closed-form solution. also exact probabilistic inference general computationally intensive. even though large literature eﬃcient/approximate inference algorithms particular problem classes many suitable on-line probabilistic inference realistic environment classes. tive) expected utility given environment class minimizing expected relative entropy given class operation modes. such bayesian control rule bayes-optimal controller. indeed easy design experiments bayesian control rule converges exponentially slower bayes-optimal controller maximum utility. consider following simple example environment k-state consecutive actions reach state reward interception b-action leads back initial state. consider second environment like ﬁrst actions interchanged. bayes-optimal controller ﬁgures true environment actions consider bayesian control rule optimal action environment environment uniform prior operation modes stays uniform posterior long reward observed. hence bayesian control rule chooses time-step equal probability. policy takes actions accidentally choose length bayesian control rule optimal too. bayes-optimal controller converges time bayesian control rule needs exponentially longer. remedy problem might allow bayesian control rule sample actions operation mode several time steps rather randomizing controllers every cycle. however considers non-stationary environments strategy also break down. conideas underlying work unique bayesian control rule. following selection previously published work recent bayesian reinforcement learning literature related ideas found. literature important amount work relating compression intelligence particular even proposed compression ratio objective quantitative measure intelligence compression also used basis theory curiosity creativity beauty extensively literature bayes-optimal predictors mixed. bayes-mixtures also used universal prediction control case idea using mixtures expert-controllers previously evoked models like mosaic-architecture universal learning bayes mixtures experts reactive environments studied wyatt examines exploration strategies mdps learning automata probability matching amongst others. particular wyatt discusses theoretical properties extension probability matching context multi-armed bandit problems. there proposed choose lever according likely optimal shown strategy converges thus providing simple method guiding exploration. derive control laws underlies kl-control methods developed todorov kappen there shown large class optimal control problems solved eﬃciently problem statement reformulated minimization deviation dynamics controlled system uncontrolled system. related idea conceptualize planning inference problem approach based equivalence maximization expected future return likelihood maximization applicable mdps pomdps. algorithms based duality become active ﬁeld current research. example rasmussen deisenroth fast model-based techniques used control continuous state action spaces. work introduces bayesian control rule bayesian rule adaptive control. feature rule special treatment actions based causal calculus decomposition adaptive agent mixture operation modes i.e. environmentspeciﬁc agents. rule derived minimizing expected relative entropy true operation mode carefully distinguishing actions observations. furthermore bayesian control rule turns exactly predictive distribution next action given past interactions would obtain using probability causal calculus. furthermore shown agents constructed bayesian control rule converge true operation mode mild assumptions boundedness related ergodicity; consistency demanding indistinguishable hypotheses share policy. presented bayesian control rule solve adaptive control problems based minimum relative entropy principle. thus bayesian control rule either regarded principled approach adaptive control novel optimality criterion heuristic approximation traditional bayes-optimal control. since takes similar form bayes’ rule adaptive control problem could translated on-line inference problem actions sampled stochastically posterior distribution. important note however problem statement formulated usual bayes-optimal approach adaptive control same. future relationship problem statements deserves investigation. thank david wingate zoubin ghahramani jos´e aliste jos´e donoso humberto maturana anonymous reviewers comments earlier versions manuscript and/or inspiring discussions. thank ministerio planiﬁcaci´on chile b¨ohringer-ingelheim-fonds funding. proof proof follows line argument solution equation crucial diﬀerence actions treated interventions. consider without loss equation note relative entropy written diﬀerence logarithms term depends varied. therefore integrate term write constant yields ﬁrst equality obtained applying bayes’ rule second using chain rule probabilities. second equality follows using causal factorization joint probability distribution. last equality applies interventions simpliﬁed term constant inspection equation sees encodes independent normal distributions immediate reward means indexed triples ×a×x words given rewards drawn normal distribution unknown mean known variance suﬃcient statistics given number times transition action mean rewards obtained transition. conjugate prior distribution well known given normal distribution hyperparameters inference carried sampling posterior distribution equation actions issued agent by-products inference process. derive approximate gibbs sampler introduce following symbols stand parameter removing respectively; matrices collecting values posterior hyperparameters respectively; maxa shorthand. conditional distribution q-values diﬃcult obtain enters posterior distribution linearly non-linearly however within operations amounts treating constant within single gibbs step conditional distribution approximated expect approximation hold resulting update rule constitutes contraction operation forms basis stochastic approximation algorithms result gibbs sampler draws values normal distributions. cycle adaptive controller carry several gibbs sweeps obtain sample improve mixing markov chain. however experimental results shown single gibbs sweep state transition performs reasonably well. parameter vector drawn bayesian control rule proceeds taking optimal action given equation note entries transitions occurred need represented explicitly; similarly q-values visited states need represented explicitly.", "year": 2008}