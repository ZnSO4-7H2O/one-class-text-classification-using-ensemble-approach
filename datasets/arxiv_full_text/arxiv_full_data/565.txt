{"title": "Incremental Adaptation Strategies for Neural Network Language Models", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without overfitting the small adaptation data.", "text": "today acknowledged neural network language models outperform backlanguage models applications like speech recognition statistical machine translation. however training models large amounts data take several days. present efﬁcient techniques adapt neural network language model data. instead training completely model relying mixture approaches propose methods continued training resampled data insertion adaptation layers. present experimental results environment post-edits professional translators used improve system. methods fast achieve signiﬁcant improvements without over-ﬁtting small adaptation data. language model plays important role many natural language processing applications namely speech recognition statistical machine translation long time back-off n-gram models considered state-ofthe-art particular large amounts training data available. alternative approach based high-dimensional embeddings words idea perform probability estimation space. means meaningful interpolations expected. projection probability estimation jointly learned neural network models also called continuous space language models seen surge popularity conﬁrmed many studies systematically outperform back-off n-gram models signiﬁcant margin speech recognition. many variants basic approach proposed last years e.g. recurrent architectures lstm recently neural networks also used translation model system ﬁrst translations systems entirely based neural networks proposed however best knowledge systems static i.e. trained large representative corpus changed adapted data conditions. ability adapt changing conditions important property operational system. need adaptation occurs instance system translate daily news articles order account changing environment. another typical application integration system tool want improve systems help user corrections. finally also want adapt generic particular genre topic lack large amounts speciﬁc data. various adaptation schemes proposed classical systems best knowledge limited works involving neural network models. interested setting needs adapted small amount data representative domain change overall system perform better domain future. task corresponds concrete needs real-world applications translation document human several days. human translator assisted system proposes translation hypothesis speed work work adapt cslm translapaper open-source matecat tool closely integrated system already adapted task source sentence system proposes eventual match translation memory translation system. human translator decide either post-edit them perform translation scratch. work want postedited sentences adapt systems translation quality improved next day. means system adapted speciﬁc translation project. important particularity task small amount adaptation data usually around three thousand words day. paper organized follows. next sections summarize basic notions statistical machine translation continuous space language models. present tasks results. paper concludes discussion directions future research. popular approaches adapt system mixture models e.g. data selection. former case separate trained available corpora merged interpolation coefﬁcients estimated minimize perplexity in-domain development corpus. known linear mixture models. also integrate various corpusspeciﬁc separate feature functions usual log-linear model system. data selection aims extracting relevant subset available training data. approach proposed turned effective many settings. adaptation models environment also investigated several studies e.g. data. curriculum learning aims presenting training data particular order improve generalization could also used perform adaptation data. couple papers investigate adaptation context particular application namely image processing speech recognition. could instance mention recent work investigated transfer features convolutional networks research perform speaker adaptation phoneme classiﬁer based traps also publications investigate adaptation neural network language models recent. insertion additional adaption layer perform speaker adaptation proposed park earlier idea explored speech recognition afﬁne transform output layer. adaptation data selection studied several variants curriculum learning explored adapt recurrent sub-domain area speech recognition finally early applications used rescore n-best list speed-up rescoring process adapt estimate inﬂuence history. statistical approach machine translation models automatically estimated examples. assume want translate sentence source language sentence target language then fundamental equation applying bayes rule translation model estimated bitexts bilingual sentence aligned data language model monolingual data target language. popular approach phrasebased models translate short sequences words together extended purposes. major challenge neural network handle words output layer since softmax normalization would costly large vocabularies. various solutions proposed short-lists class decomposition hierarchical decomposition work short-lists adaptation scheme could equally applied solutions. mentioned above popular successful adaptation schemes standard backlms data selection mixture models. could also applied cslms. practice would mean train completely cslm data selected adaptation process train several cslms e.g. generic task-speciﬁc combine linear log-linear way. however full training cslm usually takes substantial amount time often several hours even days function size available training data. building several cslms combining would also increase translation time. therefore propose compare cslm adaptation schemes efﬁcient performed couple minutes. underlying idea techniques train models slightly change existing cslm order account training data. ﬁrst method perform continued training cslm mixture adaptation data original training data. second method adaptation layers inserted neural network outlined figure additional layer initialized identity matrix weights layer updated. idea previously proposed framework speech recognition system build work explore different variants technique. interesting alternative keep original architecture modify layer e.g. weights tanh layers figure variant explored future work. translation probabilities phrase pairs usually estimated simple relative frequency. normally -gram back-off model. log-linear approach commonly used consider models instead translation language model automatic evaluation system remains open question many metrics proposed. study bleu score measures n-gram precision between translation human reference translation higher values mean better translation quality. basic architecture cslm shown figure words ﬁrst projected onto continuous representation remaining part network estimates probabilities. usually tanh hidden softmax output layer used recent studies shown deeper architecture perform better three tanh hidden softmax output layer depicted figure type architecture well known reader referred literature details e.g. task improve system closely integrated open-source tool post-edits provided professional human translators. tool algorithms update standard phrase-based systems including back-off language models developed framework european project matecat consider translation legal texts english german french. available resources language pair summarized table system based moses toolkit built according following procedure ﬁrst perform data selection parallel monolingual corpora order extract data representative development set. case interested translation legal documents. data selection well established method community. performed language translation model using methods described respectively. train -gram back-off phrasebased system using standard moses parameters. coefﬁcients feature functions optimized mert maximize bleu score development data. system used create distinct hypotheses source sentence. feature function corresponding probability generated cslm hypothesis coefﬁcients optimized. usually referred n-best list rescoring. call ﬁnal system domain-adapted since optimized translate legal documents. system used assist human translators translate large document legal domain. typically process work human translations injected system hope perform better rest document translated e.g. second work. procedure repeated several days document rather large usually humans able translate approximately words day. call procedure project-adaptation. -gram back-off built selected data perplexity domain-speciﬁc development data. given fact cslm efﬁciently trained long context windows used -gram experiments. means hope capture long range dependencies german. projection layer cslm dimension followed three tanh hidden layers size softmax output layer neurons shortlist accounts around tokens used corpus. initial learning rate exponentially decreased iterations. network converged epochs perplexity i.e. relative reduction. total training time less hours nvidia gpu. table gives bleu score baseline domain-adapted systems. analyze project adaptation techniques split another legal document part ﬁrst part containing around words used adapt system cslm aiming improve translation performance second part named note performance itself adaptation limited interest since could quite easily overtrain model data. hand informative monitor performance domaingeneric development set. ideally improve performance i.e. future text project adaptation data slightly loss generic development data. table comparative bleu scores english/german systems. italic values parenthesis information only. biased since reference translations used training. table english/german system number examples seen cslm epoch. domain adapted system randomly resample examples epoch. project-adapted system experimented various mixtures generic project speciﬁc data don’t want train data since would result strong over-ﬁtting. overﬁt data keep good performance domain-speciﬁc set. achieve this continued training networks mixture data. adaptation data always used small fractions domain-selected data randomly sampled epoch adaptation data accounts respectively. since networks trained small amounts data overall adaptation process takes minutes. statistics data used epoch detailed table show important perform adaptation cslm mixture generic adaptation data prevent overﬁtting. insert hidden layers neurons using linear hyperbolic tangent activation functions respectively. additional layers initialized identity matrix layers updated using backpropagation function. record perplexity adapted cslm used guideline selecting best networks integrate system lowest perplexity obtained keeping baseline network topology data constituted incremental training data perplexity decreases minor increase using larger table perplexities cslms hidden layer adapted bold values architecture column hidden layers. bold values last columns best perplexities respective test corpora. tanh shorthand notation hyperbolic tangent activation function. percentage proportion data total corpora networks trained iterations. lower part table summarizes results inserting adaptation layer linear tanh activation function three different slots respectively. conﬁguration explored different proportions baseline corpora clarity report interesting results. overall tendency using systematically leads over-ﬁtting network. several conclusions made tanh adaptation layer outperforms linear one; better insert adaptation layer network; updating weights inserted layer overﬁts less incremental training whole network perplexity decreases substantially observe slight improvefinally table lower part gives bleu scores project-adapted systems. cslm used bleu score increases achieved adapting translation back-off cslm adaptation schemes obtained quite similar bleu scores respectively insertion additional tanh layer slight advantage. overall adapted cslm yields improvement bleu point bleu domain-adapted system nicely shows effectiveness adaptation scheme applied couple minutes. dure different language pair english/french. community well known translation german hard task reﬂected bleu scores around hand baseline system english/french language pair bleu score well argue complicated improve system. addition investigate adaptation system cslm consecutive days human translator works corrects hypothesis corrections used adapt system second day. human corrections inserted system system third built adaptation scheme want verify whether methods robust quickly overﬁt adaptation data. number words three thousand. -gram cslm french target language shortlist used. training performed epochs. table english/french task proportion adaptation data e.g. create adaptation corpus consists data respectively remaining portions randomly resampled training data. task used incremental learning method yielded lowest perplexity english/german experiment. data consecutive days coming large document assumed domain only. therefore decided always available data preceding days adapt models. instance third data used build system fourth day. proportions corpus used continue training cslm given table perplexities various cslms ﬁrst observation rather high perplexity models day. shows importance project adaptation even domain related data available. adaptation allows decrease perplexity relative day. perplexities vary project days reduced every case demonstrates effectiveness adaptation method. order evaluate impact cslm adaptation system performed various translation experiments. results provided table bleu scores various systems using baseline adapted cslms presented. tests three different human translators sake clarity provide detailed results translator only. observed tendencies similar translators. first cslm improves bleu score baseline systems bleu points e.g. adapting whole system data improves signiﬁcantly translation quality e.g. without changing cslm. proposed adaptation scheme cslm achieves additional important improvements average bleu points. gain relatively constant days. comparison also give bleu scores using four reference translations three human translators independent translation provided european commission. table bleu scores obtained baseline project-adapted baseline cslm adapted cslm. ﬁrst value every cell bleu score obtained respect reference translation human translator; second calculated respect references created professional translators independent reference. translator also achieves generic improvements. also shows adaptation process beneﬁcial improving state-of-the-art systems already perform well certain tasks. paper presented thorough study different techniques adapt continuous space language model small amounts data. case want integrate user corrections statistical machine translation system performs better similar texts. task corresponds concrete needs real-world applications translation document human several days. human translator assisted system proposes translation hypothesis speed work work adapt cslm translations already performed human translator show system performs better remaining part document. explored adaptation strategies continued training existing neural network insertion adaptation layer weight updates limited layer only. cases network trained combination adaptation data portion similar size randomly sampled original training data. means avoid overﬁtting neural network adaptation data. overall adaptation data small less words leads fast training neural network language model couple minutes standard gpu. provided experimental evidence effectiveness approach large tasks translation legal documents english german french respectively. cases signiﬁcantly improvement translation quality observed.", "year": 2014}