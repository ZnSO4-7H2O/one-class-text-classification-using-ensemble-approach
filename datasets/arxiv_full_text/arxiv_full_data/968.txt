{"title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially. Here we show that training very deep feed-forward networks (FFNs) is not as difficult as previously thought. Unlike when back-propagation is applied to a recurrent network, application to an FFN amounts to multiplying the error gradient by a different random matrix at each layer. We show that the successive application of correctly scaled random matrices to an initial vector results in a random walk of the log of the norm of the resulting vectors, and we compute the scaling that makes this walk unbiased. The variance of the random walk grows only linearly with network depth and is inversely proportional to the size of each layer. Practically, this implies a gradient whose log-norm scales with the square root of the network depth and shows that the vanishing gradient problem can be mitigated by increasing the width of the layers. Mathematical analyses and experimental results using stochastic gradient descent to optimize tasks related to the MNIST and TIMIT datasets are provided to support these claims. Equations for the optimal matrix scaling are provided for the linear and ReLU cases.", "text": "training deep networks important open problem machine learning. many difﬁculties norm back-propagated error gradient grow decay exponentially. show training deep feed-forward networks difﬁcult previously thought. unlike backpropagation applied recurrent network application amounts multiplying error gradient different random matrix layer. show successive application correctly scaled random matrices initial vector results random walk norm resulting vectors compute scaling makes walk unbiased. variance random walk grows linearly network depth inversely proportional size layer. practically implies gradient whose log-norm scales square root network depth shows vanishing gradient problem mitigated increasing width layers. mathematical analyses experimental results using stochastic gradient descent optimize tasks related mnist timit datasets provided support claims. equations optimal matrix scaling provided linear relu cases. since early appreciated deep neural networks suffer vanishing gradient problem term vanishing gradient refers fact feedforward network backpropagated error signal typically decreases exponentially function distance ﬁnal layer. problem also observed recurrent networks errors back-propagated time error signal decreases exponentially function distance back time current error. vanishing gradient adding many extra layers ffns time points rnns usually improve performance. although applied feedforward recurrent networks analysis vanishing gradient problem based recurrent architecture recurrent network back-propagation time involves applying similar matrices repeatedly compute error gradient. outcome process depends whether magnitudes leading eigenvalues matrices tend greater less one. eigenvalue magnitudes greater produce exponential growth less produces exponential decay. magnitude leading eigenvalues tightly constrained useful non-vanishing interestingly analysis different randomly initialized matrices layer. error gradient computed different matrix applied every level back-propagation. small difference result wildly different behavior magnitude gradient norm ffns compared rnns. show correctly initialized ffns suffer vanishing gradient problem less drastic previously thought namely magnitude gradient scales square root depth network. different approaches training deep networks studied applied pre-training better random initial scaling better optimization methods speciﬁc architectures orthogonal initialization etc. further topic deep networks difﬁcult train also area active research here address vanishing gradient problem using mathematical analysis computational experiments study training error optimized deep-networks. analyze norm vectors result successive applications random matrices show analytical results hold empirically back-propagation equations nonlinear ffns hundreds layers. present test basic heuristic initializing networks procedure call random walk initialization random walk norms backpropagated errors. vector hidden activations linear transformation biases depth function element-wise nonlinearity normalize derivative condition scale factor matrices. assume network layers layer width assume elements initially drawn i.i.d. gaussian distribution zero mean variance otherwise elements elements initialized zero. deﬁne inputs outputs. assume task deﬁned network standard objective function deﬁning deﬁned across-all-layer gradient magnitude ratio solving vanishing gradient problem amounts keeping order proposal appropriately adjusting course matrices change learning initial conﬁguration network learning made changes. discuss make initial adjustment show experimentally sufﬁcient maintain useful gradients even learning. matrices initially random think variables deﬁned equation random variables. then given equation proportional product random variables according central limit theorem products random variables approximately log-normal distributed large implies distribution longtailed. applications neural network optimization want procedure regularize cases resulting optimizations making progress willing tolerate occasional pathological case resulting failed optimization. means interested catering tails distribution. avoid issues associated tails choose instead consider logarithm equation equation means think result random walk step walk given random variable goal random walk initialization chose make walk unbiased. equivalently choose make close zero possible. equation describes evolution logarithm error-vector norm output-layer vector back-propagated network. actual application back-propagation algorithm would computed propagating input forward network comparing network output desired output particular task trained. well neural network optimization experiments would like begin studying vanishing gradient problem broader context particular allows general discussion independent particular task trained. this study happens randomly chosen vectors back-propagated rather studying speciﬁc vectors result forward propagation error computation. among things implies uncorrelated matrices network. similarly want make analytic statements apply networks speciﬁc network. accomplish this average realizations matrices applied back-propagation. presenting analytic results show provide excellent approximations applied back-propagation calculations speciﬁc networks trained perform speciﬁc tasks. deﬁnition relu nonlinearity effectively zeros approximately half rows gaussian assumption applies non-zero rows. tanh nonlinearity rely numerical rather analytic results thus need assumption. gaussian ˜wδ/|δ| independent distribution value vector fact ˜wδ/|δ| gaussian vector means need consider properties vectors different network layers making calculations much easier. also implies distributed squared magnitude gaussian vector. precisely elements matrix variance write distributed according distribution degrees freedom. relu case closely related factor equal either sets fraction rows leaves rows unchanged. given zero-mean initialization values occur probability thus derivative relu function sets rows leaves rows gaussian entries. value drawn element binomial distribution squares random variables variance write before case distributed according means doubly stochastic variable ﬁrst drawn binomial distribution drawn similarly average must done binomial distributions. complication procedure using relu networks general small layer activity units. remove cases numerical studies average nonzero values compute leading order using taylor series above expanding around case obtain however unlike linear case expression good approximation entire range. instead computed numerically simple analytic expressions results obtain computing averages tanh nonlinearity difﬁcult though possible attempt this. instead report numerical results below. general expect optimal value tanh case greater glinear derivative tanh function reduces variance rows compared less grelu multiple rows random walks generate values according equation shown panel figure linear network case optimal value given equation used producing unbiased random walk linear increase variance random walk across layers well predicted variance computed equation figure sample random walks random vectors back-propagated linear network. many samples random walks equation determined equation starting vectors well matrices generated randomly step random walk. mean instantiations close zero optimal value used. variance random walks layer value predicted equation also explored numerical simulation degree equations good approximations dependence results shown figure figure shows predicted value function layer width nonlinearity. point averaged random networks form given equations random vectors whose elements unit variance. bottom figure shows growth magnitude comparison ﬁxed function scaling parameter nonlinearity. point averaged random instantiations equations back-propagated equations results show predicted optimal values equations match data well provide numerical estimate optimal value tanh case. addition range serviceable values larger tanh linear relu cases saturation nonlinearity compensating growth gwd. figure numerical simulation best function using equations using random vectors black shows results numerical simulations shows predicted best values equations linear relu tanh. bottom numerical simulation average function using equations results equations indicated arrows. guidelines provided. general methodology used random walk initialization according values given equations linear relu cases respectively. tanh values shown good practice shown figure figure scaling input distribution also adjusted zero mean unit variance dimension. poor input scaling effect back-propagation derivative terms equation number early layers randomness initial matrices washes poor scaling. slight adjustment helpful based actual data distribution real-world data normal distribution. similar reasoning initial scaling ﬁnal output layer need adjusted separately back-propagating errors affected initialization ﬁnal output layer. summary random walk initialization requires tuning three parameters input scaling ﬁrst handle transient effects inputs errors last generally tune entire network. important three assess quality training error deep nonlinear ffns random walk initialization experiments mnist timit datasets standard deﬁned equations particular studied classiﬁcation problem mnist timit using cross-entropy error multiclass classiﬁcation studied reconstruction mnist digusing auto-encoders using mean squared error. timit study input features frames studies focused exclusively training error effect depth generalization different problem train deep ffns ﬁrst place. general experimental procedure limit number parameters e.g. parameters distribute matrices biases layer network. classiﬁcation experiments used constant width layers experiments actual number parameters ﬁrst value parameter limit plim constant integer value possible. thus network deeper layers also became narrow. example mnist dataset plim mnist auto-encoder experiments plim code layer linear units. size layer surrounding middle encoding layer chosen picking constant increase layer size total number parameters ﬁrst number plim integral layer width layers. example layer sizes layer sizes auto-encoder studies used tanh nonlinearity varied parameter experiment layer. experiments compared depth another varied learning rates quite ensure fairness shallow deep networks. particular varied minimal maximal learning rates experiment. essence exponential learning rate schedule function depth minimal maximal values exponential hyper-parameters. precisely denote maximum depth experiment dmax single experiment dmax λout learning rate hyper-parameters input output layers respectively. exponential learning rate schedule decay scale function depth took form aspect learning rate scheme shallower networks overly penalized tiny learning rates early layers. decay starts layer getting learning rate λout goes backwards ﬁrst layer gets learning rate λdmax−d. means networks shallow dmax could much larger λin; dmax λin. experiments λout λout also tested standard λout deep networks varying learning rates function depth important although study depth here. finally learning rates layers uniformly decayed multiplicative factor training epoch. beyond setting learning rate schedules bells whistles. trained networks using standard stochastic gradient descent minibatch size epochs full training dataset. also used gradient clipping cases gradient became large although uncommon. combination hyper-parameters varied learning rates depths values resulted roughly optimizations panel displayed figure figure employed ﬁrst experiments determine whether training real-world dataset would affected choosing according random walk initialization. trained many networks described mnist dataset. results shown figure tanh relu nonlinearities. namely tanh smallest training error depths good agreement figure relu smallest training error results good agreement analytic calculations results shown figure figure training error mnist function simulation used parameter limit error shown scale. parameter varied x-axis color denotes various values training error tanh function learning rate combinations. learning rate hyper-parameters λout visually distinguished. upper left except showing minimum training error learning rate combinations. left relu nonlinearity. nonlinearities experimental results good agreement analytical experimental predictions. goal second experiments assess training error function words increased depth actually help decrease objective function? focused tanh nonlinearity many believe relu easier function use. demonstrated utility correctly scaling used variety values general optimal range results mnist classiﬁcation shown figure best training error depth large learning rate. tied second place depths showed next lowest training error. mnist auto-encoder experiments shown figure shallow network achieved best training error. however even depths roughly greater training error thus demonstrating effectiveness initialization scheme. mostly stunt trained networks layers classify mnist. parameter limit resulting layer width results shown figure best networks able achieve performance training mistakes. also tried random walk initialization timit dataset results similar mnist. best training error among depths tested depth depth essentially tied. summary depth improve training error tasks examined experiments nevertheless provide strong evidence initialization reasonable deep nonlinear ffns trained real-world data. figure performance results using random walk initialization mnist timit. network parameter limit regardless depth. training error shown plot. legend color coding λout shown right. values varied values λout shown different markers. varied values also used averaged over. classiﬁcation training error mnist function λout. mnist auto-encoder reconstruction error function hyper-parameters. experiments mnist training error shown function training epoch. hyper-parameters λout varied sense difﬁculty training deep network. value used combat pathological curvature. classiﬁcation training error timit dataset. values averaged over. results presented imply correctly initialized ffns values figure equation linear networks equation relu networks successfully trained real datasets depths upwards layers. importantly simply increase decrease ﬂuctuations norm back-propagated errors. derived equations correct linear relu cases. experiments explicitly used avoided regularization reason random walk initialization incompatible methods used train deep networks including second-order optimization different architectures regularization methods. study revealed number points training deep networks. first careful biases. throughout experiments initialized biases zero though always allowed modiﬁed. part biases hurt results. however care must taken learning rates optimization biases quickly match target mean across examples. happens careful initialization destroyed forward progress optimization cease. second learning rates deep networks important. seen figure exact learning rate scheduling made huge difference performance. third suspect extremely deep networks curvature error landscape extremely problematic. means network sensitive changes ﬁrst layer effective optimization experimental results show even though depth clearly improve training error initialization scheme nevertheless effective allowing training deep networks forward. namely almost models correctly chosen broken mismatch learning rate hyper-parameters architecture reached zero near-zero training classiﬁcation error extremely reconstruction error regardless depth. research necessary determine whether difﬁcult different tasks make deep feedforward networks useful applied settings. regardless results show initializing deep feedforward networks random walk initialization according figure described section calculation optimal values opposed easily implemented sensible default initialization. glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems sutskever ilya martens james dahl george hinton geoffrey. importance initialization momentum deep learning. proceedings international conference machine learning", "year": 2014}