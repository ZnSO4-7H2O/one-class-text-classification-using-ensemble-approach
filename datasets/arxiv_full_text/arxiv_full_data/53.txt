{"title": "Optimising The Input Window Alignment in CD-DNN Based Phoneme  Recognition for Low Latency Processing", "tag": ["cs.CL", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction.", "text": "many methods feature extraction also compute time derivatives features require number frames past future. often include three context frames ﬁrst three second derivatives total assume spaced feature vectors; input neural network estimates state probabilities include window context frames. typical values future past frames correspond latency. cases context extend whole utterance e.g. application convolutional neural networks; used hybrid recurrent neural networks hmms speciﬁcally designed latency processing. feature extraction based frequency cepstral coefﬁcients without time derivatives receive future feature frame input thus limiting latency size feature extraction window. purpose study however limited evaluating effect varying look-ahead length viterbi decoder system. nearly recent methods based acoustic models employ symmetric context windows input therefore affected certain latency. time delayed neural networks used asymmetric context windows cases. similarly context windows frames left context used. however aware systematic detailed investigation effect context window asymmetry. study want determine relationship latency model performance. order this analyse performance recogniser proposed varies alignment input context window shifted back forward time. intent report state-of-the-art results give indication relative effects shifting input window. also report results phoneme error rate timit data want precise control shifts time therefore require carefully annotated data. present systematic analysis performance phonetic recogniser window input features symmetric respect current frame. recogniser based context dependent deep neural networks hidden markov models objective reduce latency system reducing number future feature frames required estimate current output. tests performed timit database show performance degrade input window shifted frames past compared common practice corresponds improving latency settings. tests also show best results obtained symmetric window commonly employed asymmetric window eight past future context frames although observation conﬁrmed data sets. reduction latency suggested results critical speciﬁc applications real-time synchronisation telepresence also beneﬁcial general applications improve human-machine spoken interaction. recent years development deep neural models based restricted boltzman machines pretraining revitalised artiﬁcial neural networks automatic speech recognition well many ﬁelds extensive reviews). factor determines usability applications based speech recognition latency system. dialogue systems e.g. long latencies disrupt natural turntaking human-machine conversation. speciﬁc applications even critical. typical example involves systems drive movements avatar real time support telepresence experiments based kaldi pdnn+kaldi recipes performed timit corpus. standard speaker training used training. utterances removed prevent bias similarity utterances. training divided training validation regularisation back propagation training procedure. results reported -speaker core test set. channels ﬁlterbank features computed using hamming window increments. inputs dnns experiments context windows consecutive frames. length context window seems optimal application according also following results hidden layers size softmax output layer represents distribution posteriors different senones resulting topology network therefore learning rate initialised dropped half whenever difference previous epoch current epoch drops certain threshold. optimised acoustic scale decoder running decoder development different scales. scales form optimal values acoustic scale always contained extreme values tested suggesting correspond real optima also table section optimal values used decode test data. insertion penalty experiments optimised. decoding phone classes mapped classes evaluation. baseline results correspond input window centered respect current frame context frames either side. corresponds notation zero shift positive shift corresponds shift context window future. tested shifts increments frame. additionally tested shifts frames. every shift value whole training evaluation procedure repeated. interesting notice shifts context window contain current frame recogniser predict current phoneme exclusively based context. used kaldi feature extraction selection senones alignment senone transcriptions speech data. speed deep neural networks training used nvidia titan gpus. also used symbolic computations software theano well optimised symbolic algebra gpus. generative training rbms took minutes epoch entire training ﬁne-tuning back propogation took minutes full pass. fig. illustration method sequence speech feature frames constitutes input neural network. context window necessarily symmetric respect current frame illustration shift applied. senones given sequence input feature vectors. probability estimations used hidden markov model combination bigram phoneme-level language model phonetic recognition. senones alignment speech utterances dataset determined training context dependent recogniser based gaussian mixture models. number senones reduced decision tree based clustering. training procedure well described weights cd-dnn initialised using deep belief network stack restricted boltzmann machines trained generatively ﬁtting layers time means contrastive divergence procedure. ﬁnal output layer cd-dnn generalised softmax layer representing distribution senones. given generative initialisation full model ﬁne-tuned back-propagation training. input model context window successive frames ﬁlterbank feature vectors. feature vectors normalised zero mean unit variance. ﬁlterbank features instead mfccs reported achieve good results combination dnns without need time derivatives would increase latency viterbi decoder generate phoneme sequences phoneme-level bigram model estimated training set. acoustic scale used decoder tune acoustic language models optimised test independently development test optimal value used test set. decoder follows lattice generation pruning approach described differently previous studies vary alignment context window respect current frame train different recogniser alignment analyse performance terms phoneme error rate function window shift. fig. phoneme recognition performance function input window alignment time. left phoneme error rate right detail different kind errors. shift corresponds window centered current frame frames right context left context. windows shifts contain current frame. input window shift whereas right plot details different kind errors best occurs window shift i.e. window symmetric around current frame past future frames. performance degrade varying shift frames however starts increasing shift beyond frames network receive current feature vector input. consider positive shifts completeness observe similar behaviour although graph perfectly symmetric positive shifts generally slightly higher negative shifts amplitude. acoustic scale used decoding optimised development used test set. comparison table shows optimal values acoustic scale optimised development test set. cases optimal value obtained. cases different values obtained corresponding difference grater looking right plot figure observe number insertions relatively constant respect window shift. substitutions increase window shifted respect current frame errors vary window shifts deletions. table shows results selected window shifts including extreme cases reported figure clarity illustration. performance extreme shifts drops considerably degradation mostly accounted deletions substitutions. study presents systematic analysis effect shifting context input window cd-dnn+hmm phonetic recogniser respect current frame. goal investigating possibility reduce latency speech recogniser applications speciﬁc requirements results reported general interest. results timit database suggest context window slightly shifted back time superior compared symmetric context window used speech recognisers. however improvement performance small compared variability observation conﬁrmed testing data sets. interestingly results suggest shifting context window back time frames introduce noticeable degradation system performance. larger shifts introduce gradual progressively steeper degradation. consequence without modifying method reduce latency system least without degradation performance. reduce latency even degradation tolerated application. reduction latency although small size potentially improve usability many applications especially latency critical real-time synchronisation telepresence. mised experiments different window shifts deletion error always greater insertion error. future work investigate reduce effect window shift optimising insertion penalty shift. study speech recognition possibility generalise results outside scope phonetic recognition needs veriﬁed speciﬁc tests. example would interesting test systems longer time dependencies would affected window shifts similar way. giampiero salvi jonas beskow samer moubayed bj¨orn granstr¨om synface speech-driven facial animation virtual speech-reading support eurasip journal audio speech music processing sept. speakerindependent lips tongue visualization vowels acoustics speech signal processing ieee international conference giampiero salvi dynamic behaviour connectionist speech recognition strong latency constraints speech communication vol. july mohamed t.n. sainath dahl ramabhadran g.e. hinton m.a. picheny deep belief networks using disacoustics criminative features phone recognition g.e. dahl dong deng acero large vocabulary continuous speech recognition context-dependent acoustics speech signal processing dbn-hmms ieee international conference vijayaditya peddinti daniel povey sanjeev khudanpur time delay neural network architecture efﬁcient modeling long temporal contexts proc. interspeech andrew senior alexander gruenstein jeffrey accurate compact large vocabulary speech sorensen recognition mobile devices proc. interspeech navdeep jaitly exploring deep learning methods discovering features speech signals ph.d. thesis university toronto povey hannemann boulianne burget ghoshal janda karaﬁat kombrink motlicek yanmin qian riedhammer vesely ngoc thang generating exact lattices wfst framework acoustics speech signal processing ieee international conference march daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. dec. ieee signal processing society ieee catalog cfpsrw-usb. fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio theano features speed improvements deep learning unsupervised feature learning nips workshop", "year": 2016}