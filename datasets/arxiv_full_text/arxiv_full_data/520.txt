{"title": "Diverse Embedding Neural Network Language Models", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.", "text": "kartik audhkhasi∗ abhinav sethy∗ bhuvana ramabhadran watson research center yorktown heights {kaudhkhaasethybhuvana}us.ibm.com ∗kartik audhkhasi abhinav sethy joint ﬁrst authors. propose diverse embedding neural network novel architecture language models dennlm projects input word history vector onto multiple diverse low-dimensional sub-spaces instead single higherdimensional sub-space conventional feed-forward neural network lms. encourage sub-spaces diverse network training augmented loss function. language modeling experiments penn treebank data show performance beneﬁt using dennlm. diversity systems trained perform given machine learning task crucial obtaining performance improvement upon fusion problem language modeling aims design predictive models text exception. several language models proposed many years research simple n-gram models estimate conditional probability i-th word sentence given previous words parameterized researchers found fusing different kind n-gram language models together mikolov often signiﬁcantly improves performance. table shows perplexity -gram nnlms standard split penn treebank data interpolation nnlm -gram gives reduction perplexity single nnlm even though relatively close perplexities. correlation coefﬁcient posterior probabilities predicted word test models simple measure predict whether models diverse. posterior probabilities highly correlated models less diverse smaller gains expected fusion. posteriors n-gram nnlm correlation coefﬁcient signiﬁcantly lower correlation coefﬁcient pair randomly initialized nnlms. higher diversity nnlm n-gram combination results signiﬁcant perplexity improvement upon interpolation. almost n-gram models smoothed various ways assign non-zero probability estimates word phrases unseen training data. kneser-ney smoothing n-grams models paper. table table shows test perplexities several penn treebank test set. n-gram nnlm projects one-hot vectorof previous words onto x-dimensional linear sub-space. inputs projected vector hidden layer neurons outputs posterior probability words vocabulary. nn-randinit denotes interpolation randomly initialized independently-trained nnlms. last column shows average correlation coefﬁcient posteriors test words estimated ensemble. random initialization modifying neural topology terms embedding hidden layer size used build diverse nnlm models. table randomly initialized nnlms fused together provide improvement perplexity baseline. recurrent nnlm models different topologies fused signiﬁcant gains well demonstrated remarkable beneﬁt simple diversitypromoting strategies leads central question paper explicitly enforce diversity nnlm model training? show modifying nnlm architecture augmenting training loss function achieves that. fact nnlm learns low-dimensional continuous space embedding input words motivates architecture training proposed model diverse embedding neural network ﬁrst give overview conventional nnlms next section. section presents dennlm architecture training loss function. presents experiments results section conclude paper section feed-forward neural network converts one-hot encoding word history continuous low-dimensional vector representation figure shows schematic diagram typical nnlm. wi−n denote -in-v vectors history words. denote matrix projects history word vectors onto ddimensional vectors wi−n typically much smaller size vocabulary typical values resulting d-dimensional continuous representation input words feeds neural network hidden layer estimates -in-v vector target word hidden neuron activation function hyperbolic tangent logistic sigmoid output neuron activation function -way softmax. back-propagation algorithm trains nnlm often using stochastic gradient descent gradient computed several random subsets batches n-grams input training data. researchers proposed several variants feed forward nnlm especially model longer input word history. prominent examples include recurrent neural network bidirectional long-short term memory models offer improved performance slower difﬁcult train. even though restrict attention feed forward nnlms paper general principles proposed applicable nnlm architectures well. next section introduces diverse embedding nnlm figure ﬁgure shows schematic diagram -gram nnlm. matrix projects input history word vectors onto continuous space. representations pass hidden layer predict next word. diverse embedding nnlm aims learn multiple diverse representations input words rather single representation. ﬁrst important understand intuition representation context nnlm. given input word vectors wi−n consider d-dimensional vectors wi−n pairwise distances between vectors constitute representation input words. good representation captures contextual semantic similarity pairs words. similar words located close representation dissimilar words located apart. nnlm uses representation input words predict next word. natural ensure diversity nnlms diversity representation next section discusses intuitive score capture representational diversity. maximizing representational diversity nnlms ﬁrst requires objective score capture diversity. consider representation wi−n input words nnlm. since representation lies d-dimensional euclidean space pairwise angles words sufﬁcient uniquely deﬁne representation even afﬁne transformation translation rotation scaling. matrix pairwise cosine angles pairs points representation deﬁned matrix completely deﬁnes representation word history produced also independent dimensionality representation i.e. number rows useful comparing representations different dimensionality input data points. given representations computed using matrices deﬁne across representations raster-scans matrix column vector. note drep bounded cosine bounded representational diversity increases drep decreases. implementation distance diversity computation efﬁciency reasons consider distances randomly chosen words minibatch instead full vocabulary. experiments show using entire vocabulary diversity computation gave minor improvements perplexity expense much longer training time. currently exploring several potential ways compute diversity nnlms beyond representational diversity score presented paper. next section discusses dennlm architecture training loss function. discussed earlier dennlm attempts learn multiple diverse low-dimensional representations input word history. figure shows schematic diagram dennlm diverse representations. representations pass separate produce separate predictions next word. model merges predictions produce ﬁnal prediction. presence zero entries matrices implies dennlm smaller number parameters comparable nnlm. equivalence dennlm conventional nnlm makes implementation dennlm easy within conventional nnlm toolkits. merely constraining architecture nnlm guarantee learns multiple diverse representations. hence augment training loss function conventional nnlm additional terms promote diversity. negative log-likelihood cross entropy -way classiﬁcation loss function conventional nnlm. dennlm instead uses following augmented loss function ﬁrst term loss function mixture model loss ensures fused prediction accurate. using ﬁrst term trains mixture neural networks. however ensure high discrimination ability representations learned individual networks different representations capture modes data distribution. thus include second term motivated recent work deeply supervised neural networks authors augment conventional loss ﬁnal layer discriminative loses computed previous layers. second term plays similar role makes individual representations discriminative well. ﬁnal term representational diversity nnlm described section minimizing dennlm loss function gives representations jointly discriminative individually discriminative diverse. and/or regularization penalties loss function well often useful prevent over-ﬁtting case network size large compared number training ngrams. next section discusses experimental setup results. conducted language modeling experiments standard split penn treebank data used previous works upenn data vocabulary words contains approximately n-grams training set. used upenn data since well-studied language modeling also small enough conduct several experiments understanding dennlm model better. implemented dennlm theano trained optimizing augmented loss function using root-mean square propagation variant stochastic gradient descent. tuned hyper-parameters standard upenn heldset. table shows test perplexities baseline dennlms. kept model size comparable reducing size component nnlm. results show signiﬁcant improvement perplexity using dennlm -gram model single nnlm interpolation randomly initialized nnlms. posterior correlation coefﬁcients signiﬁcantly less correlation coefﬁcient randomly initialized nnlms. note dennlm signiﬁcantly outperforms standard nnlm size similar number parameters. also clearly better randomly initialized nnlm models. perplexity results table diverse feed-forward nnlms especially encouraging given fact advanced recurrent neural network gives perplexity upon interpolation -gram task diversity parameters performance fairly stable range minor differences around points perplexity. performance respect diversity weight also stable range model parameters explored different model sizes single model dennlm changing dennlm topology observed general dennlm total size single nnlm works well. achieve smaller gains around points perplexity increasing size dennlm. optimization parameters observed choice optimization algorithm signiﬁcant impact diversity models generated random initialization. found using rmsprop signiﬁcantly higher learning rate gradient clipping scaling bias updates training four randomly-initialized nnlms leads lower posterior posterior correlation compared standard optimization setting. higher diversity models translates lower perplexity interpolated nnlm comparable best results table indicates choice hyper-parameters optimizer settings building diverse models random initialization different ones used training models individually. understand impact hyper-parameters dennlm diversity perplexity performed thorough grid search learning rate rmsprop weight penalty dennlm connection weights. computed perplexity average cross-correlation posteriors individual models dennlm. figure shows scatter plot average posterior cross-correlation percent improvement dennlm perplexity best model’s perplexity. strong negative correlation ﬁgure shows diverse models give bigger improvement perplexity upon interpolation. highlights merit training diverse nnlm fact achieve diversity either informed objective functions exhaustive hyper-parameter search. latter becomes especially tedious deep complex neural networks trained data sets. work introduced neural network architecture training objective function encourages individual models diverse terms output distribution well underlying word representations. demonstrated usefulness well-studied upenn language modeling task. proposed training criterion general enough constrain nnlm models speciﬁc architecture. given promising results next step evaluate improved language models speech recognition spoken-term detection tasks. figure ﬁgure shows scatter plot average posterior cross-correlation percent improvement dennlm perplexity best model’s perplexity. observed strong correlation coefﬁcient variables indicating diverse nnlms give bigger reduction perplexity upon interpolation. also plan explore utility diverse representations measure semantic similarity sentence completion word embedding-based models shown effective. work supported intelligence advanced research projects activity department defense u.s. army research laboratory contract number wnf-c-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa dod/arl u.s. government. bergstra bastien breuleux lamblin pascanu delalleau desjardins warde-farley goodfellow bergeron bengio theano deep learning gpus python. learn workshop nips’", "year": 2014}