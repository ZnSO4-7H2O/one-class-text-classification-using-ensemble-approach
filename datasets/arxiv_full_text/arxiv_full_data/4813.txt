{"title": "Policy Gradient Methods for Off-policy Control", "tag": ["cs.AI", "cs.LG"], "abstract": "Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ, converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy. In control problems, one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function. In this paper, we present the first gradient-based learning algorithms for this problem, which rely on the framework of policy gradient in order to modify the behavior policy. We present derivations of the algorithms, a convergence theorem, and empirical evidence showing that they compare favorably to existing approaches.", "text": "oﬀ-policy learning refers problem learning value function behaving policy following diﬀerent policy. gradient-based oﬀ-policy learning algorithms tdc/gq converge even using function approximation incremental updates. however developed case ﬁxed behavior policy. control problems would like adapt behavior policy time become greedy respect existing value function. paper present ﬁrst gradient-based learning algorithms problem rely framework policy gradient order modify behavior policy. present derivations algorithms convergence theorem empirical evidence showing compare favorably existing approaches. fundamental concept reinforcement learning temporal diﬀerence learning introduced sutton td-learning methods used policy evaluation tries learn value given state ﬁxed policy. extension control case called q-learning value function deﬁned state-action pairs. control policy computed action values. ﬁrst q-learning algorithms proposed watkins dayan simultaneously searches evaluates policy varying action value estimates. watkins dayan’s q-learning algorithm oﬀ-policy algorithm policy searched evaluated strictly greedy respect current action values control agent uses ε-greedy policy. facilitates exploration agent allowed make random move probability obtain representative samples facilitate search policy generates high rewards. recently gradient-based oﬀ-policy learning algorithms introduced also proven convergent oﬀ-policy learning linear value function approximation. extension q-learning also convergent oﬀ-policy learning control policy ﬁxed. control case suﬃcient agent explore environment able search good policy. reason convergence cannot guaranteed non-stationary policy causes drift distribution transition samples generated. drift necessary agent good policy also cause oscillations value function estimates algorithm converge. sarsa also suﬀers problem guaranteed converge sub-space policies within sub-space value function estimates oscillate indeﬁnitely. paper present gradient-based td-learning algorithm similar also incorporates policy gradients correct drift distribution transitions sampled. similar policy gradient framework directly analyze interaction policy gradient distribution transitions sampled. result algorithm iterates sequence markov chains induced variation value function estimates therefore policies. makes algorithm similar policy iteration q-learning policy gradients consider ﬁnite state space ﬁnite action space. transition function stochastic reward function deﬁned discount factor consider linear function approximation case basis function deﬁne state-action value function r|s×a| vector state-action values similarly r|s×a| limt→∞ p{st exists. letting diagonal matrix limit distribution diagonal deﬁne norm ||v|| vdv. mean squared projected bellman error approach diﬀers view bellman operator stationary distribution state-action pairs parametric value function parameter stationary distribution assume algorithm shows resulting algorithm call policy-gradient q-learning. algorithm uses linear function approximation updates done number basis functions used. making transition want sample next action using parameter estimate rather updated estimate. tested method star baird counter example compared q-learning state version divergence q-learning monotonic known converge initialize parameter vector corresponding action transitions centre state remaining parameter entries discount factor experiments assume hard-coded policy ensures uniform exploration state-action pairs look control case actions selected using boltzmann policy probability selecting speciﬁc action updating done either sampling transitions according hard coded distributions either simulating trajectories mdp. sampled version sampled state according uniform distribution states action trajectory based experiments sampled seven start states uniformly executed transitions mdp. transitioning updating parameter vector measured mspbe using uniform stationary distribution states. figure shows mspbe mean squared td-error deﬁned parameter vector step simulation. presented gradient based td-learning algorithm incorporates policy gradients. resulting algorithm similar gq/tdc also correction term direction gradient target policy. analysis assumes dependency markov chain figure mspbe mstde q-learning baird counter example simulated trajectories. learning rates target policy temperature control policy temperature curve average repeated runs. parameter vector target policy. allows algorithm correctly step sequence diﬀerent markov chains account drift distribution transition data sampled changes parameter vector. next research direction extend method non-linear function approximation case. maei present ﬁrst gradient based algorithm converges case. able draw results work. derivation algorithm assumed bellman operator parametric parameter estimate lead additional policy gradient terms. assumptions made bellman operator value function terms mspbe objective non-linear function approximation case would obtain gradients value function here. however would analyze projection operator mspbe objective diﬀerently.", "year": 2015}