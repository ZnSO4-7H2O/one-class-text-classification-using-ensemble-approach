{"title": "On Learning to Think: Algorithmic Information Theory for Novel  Combinations of Reinforcement Learning Controllers and Recurrent Neural World  Models", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially \"learning to think.\" The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as \"mirror neurons.\" Experimental results will be described in separate papers.", "text": "paper addresses general problem reinforcement learning partially observable environments. large recurrent neural networks learned scratch drive simulated cars high-dimensional video input. however real brains powerful many ways. particular learn predictive model initially unknown environment somehow abstract planning reasoning. guided algorithmic information theory describe rnn-based designed same. rnnai trained never-ending sequences tasks provided user others invented rnnai curious playful fashion improve rnn-based world model. unlike previous model-building rnn-based machines dating back rnnai learns actively query model abstract reasoning planning decision making essentially learning think. basic ideas report applied many cases rnn-like system exploits algorithmic information content another. taken grant proposal submitted fall also explain concepts mirror neurons. experimental results described separate papers. neural networks partially observable environments direct indirect search program space deep learning supervised unsupervised learning gradient descent-based early controllers predictive world models early predictive world models combined traditional hierarchical multitask algorithmic transfer learning general reinforcement learning agents must discover without teacher interact dynamic initially unknown partially observable environment order maximize expected cumulative reward signals e.g. arbitrary priori unknown delays actions perceivable consequences. problem hard problem computer science since task computable description formulated framework e.g. artiﬁcial recurrent neural networks bill. typical consists many simple connected processors called neurons producing sequence real-valued activations. input neurons activated sensors perceiving environment neurons activated weighted connections wires previously active neurons neurons affect environment triggering actions. learning credit assignment ﬁnding real-valued weights make exhibit desired behavior driving car. depending problem neurons connected behavior require long causal chains computational stages stage transforms aggregate activation network often non-linear manner. unlike feedforward support vector machines rnns principle interact dynamic partially observable environment arbitrary computable ways creating processing memories sequences input patterns weight matrix program. without teacher reward-maximizing programs must learned repeated trial error. possible train small rnns weights using evolutionary algorithms search space weights policy gradients example evolutionary algorithms outperformed traditional dynamic programming -based methods partially observable environments e.g. however techniques insufﬁcient solving complex control problems involving high-dimensional sensory inputs video scratch. program search space networks size required tasks simply large. however search space often reduced dramatically evolving compact encodings neural networks e.g. lindenmeyer systems graph rewriting cellular encoding hyperneat techniques general early work used universal assembler-like languages encode later coefﬁcients discrete cosine transform latter method compressed search used successfully evolve controllers million weights drive simulated video game based solely high-dimensional video stream —learning control visual processing scratch without unsupervised pre-training vision system. ﬁrst published deep learner learn control policies directly high-dimensional sensory input using facilitate learning task controllers certain types supervised learning unsupervised learning based gradient descent techniques. particular ul/sl used compress search space build predictive world models accelerate discussed later. ﬁrst review relevant algorithms maximize differentiable objective functions researchers almost invariably backpropagation discrete graphs nodes differentiable activation functions typical applications include fnns time similar methods rnns e.g. bptt suffer fundamental deep learning problem ﬁrst discovered analyzed standard activation functions cumulative backpropagated error signals decay exponentially number layers explode hence early fnns layers. similarly early rnns could generalize well short long time lags relevant events. years several ways overcoming fundamental deep learning problem explored. example deep stacks unsupervised rnns fnns help accelerate subsequent supervised learning bptt also distill compress knowledge teacher student forcing student predict hidden units teacher long short-term memory alleviates fundamental deep learning problem ﬁrst architecture international contests e.g. connectionist temporal classiﬁcation widely used gradient-based method ﬁnding weights maximize probability teacher-provided label sequences given streams real-valued input vectors. example used baidu break important speech recognition record many recent state-of-the-art results sequence processing based lstm learned control robots used benchmark records prosody contour prediction textto-speech synthesis large vocabulary speech recognition machine translation ctc-trained lstm greatly improved google voice available billion smartphone users. nevertheless least applications rnns sometimes yield better results gradient-based lstm alternative differentiable memory proposed today’s faster computers gpus mitigate fundamental deep learning problem fnns particular many recent computer vision contests fully supervised max-pooling convolutional consist alternating convolutional max-pooling layers topped standard fully connected output layers. weights trained backpropagation ensembles gpubased mpcnns achieved dramatic improvements long-standing benchmark records e.g. mnist numerous competitions achieved ﬁrst human-competitive even superhuman results well-known benchmarks e.g. many recent variations improvements supervised transfer learning dataset another speed learning. combination convolutional lstm best results automatic image caption generation perhaps well-known application tesauro’s backgammon player learned achieve level human world champions playing itself. uses reactive policy based simplifying assumption markov decision processes current input agent conveys information necessary compute optimal next output event decision. policy implemented gradient-based trained method temporal differences play learns board states predictions expected cumulative reward selects actions leading states maximal predicted reward. similar approach employed play several atari video games directly pixel video input using neural fitted q-learning based experience replay even better results approaches cannot work realistic partially observable environments memories previous inputs stored priori unknown time intervals. triggered work partially observable markov decision problems traditional techniques based dynamic programming combined gradient descent methods train value-function approximator maps entire event histories predictions expected cumulative reward lstm used robots gradient-based used reduce controller’s search space feeding compact codes high-dimensional inputs example applied real-world control tasks purely visual inputs compactly encoded hidden layers deep autoencoders combined unsupervised learning based slow feature analysis enabled humanoid robot learn skills video streams raam employed deep unsupervised sequence encoder important application gradient-based obtain predictive world model controller achieve goals efﬁciently e.g. cheap mental m-based trials opposed expensive trials real world ﬁrst combination dates back generalizing earlier similar controller/model systems based fnns compare related work tries learn predict inputs previous inputs actions. also temporarily used surrogate environment form coupled outputs become inputs whose outputs turn become inputs gradient descent technique used learn plan ahead training series m-simulated trials produce output action sequences achieving desired input events high real-valued reward signals active vision system used basic principle learn sequential shifts fovea detect targets visual scene thus learning rudimentary version selective attention. early systems however powerful rnns lstm. fundamental problem environment noisy usually learn approximate conditional expectations predicted values given parts history. certain noisy environments monte carlo tree sampling similar techniques applied plan successful future action sequences methods however simulating possible futures time step time step without proﬁting human-like hierarchical planning abstract reasoning often ignores irrelevant details. early sec. also combined traditional temporal difference methods based markov assumption processing history actions observations predict future inputs rewards internal states used inputs temporal difference-based predictor cumulative predicted reward maximized appropriate action sequences. systems described actually collapsed cumulative reward predictor predictive world model hierarchical multitask algorithmic transfer learning work nn-based hierarchical without predictive world models published since early particular gradient-based subgoal discovery rnns decomposes tasks subtasks submodules numerous alternative techniques proposed frameworks feudal options directly address problem automatic subgoal discovery hq-learning automatically decomposes problems partially observable environments sequences simpler subtasks solved memoryless policies learnable reactive subagents. related methods include incremental evolution hierarchical evolution hierarchical policy gradient algorithms recent organizes potentially deep nnbased sub-modules self-organizing -dimensional motor control maps inspired neurophysiological ﬁndings methods above however assign credit hierarchical fashion limited ﬁxed schemes improved adapted problem-speciﬁc ways. next sections describe novel systems overcome drawbacks above-mentioned methods. general methods incremental multitask algorithmic transfer learning nn-speciﬁc include evolutionary adate system success-story algorithm selfmodifying policies running general-purpose computers optimal ordered problem solver learns algorithmic solutions problems inspecting exploiting solutions problems asymptotically time-optimal. powerplay incrementally learns become general algorithmic problem solver continually searching space possible pairs tasks modiﬁcations current solver ﬁnds powerful solver that unlike unmodiﬁed solver solves previously learned tasks plus least simpliﬁes/compresses/speeds previous solutions without forgetting any. early rnn-based systems mentioned sec. learn predictive model initially unknown environment. real brains seem still superior present artiﬁcial systems many ways. seem exploit model smarter ways e.g. plan action sequences hierarchical fashion types abstract reasoning continually building earlier acquired skills becoming increasingly general problem solvers able deal large number diverse complex tasks. describe rnn-based artiﬁcial intelligences designed learning think. fnns traditionally linked concepts statistical mechanics information theory programs general computers rnns call framework algorithmic information theory given universal programming language universal computer algorithmic information content kolmogorov complexity computable object length shortest program computes since program computer translated functionally equivalent program different computer compiler program constant size kolmogorov complexity objects hardly depends particular computer used. computable objects given size however hardly compressible since relatively programs much shorter. similar observations hold practical variants according given universal computer whose programs encoded strings mutual information programs expressed length shortest program computes given ignoring additive constant depending solution problem fast solution problem small fast much shorter asymptotically optimal universal search solution given generally ﬁrst thus solve much faster search scratch argument broad applicability. rnns similar general parallel-sequential computers vector learnable realvalued parameters trained algorithm perform certain well-deﬁned task environment. frozen. goal train parameters learning algorithm perform another well-deﬁned task whose solution share mutual algorithmic information solution task. facilitate this simply allow learn actively inspect reuse algorithmic information conveyed consider trial makes attempt solve given task within series discrete time steps learning algorithm experience gathered trial modify order improve performance later trials. trial give opportunity explore exploit ignore interacting follows sense query answer denote vectors real values; denote computable functions. time denote current states respectively. represent current neural activations fast weights dynamic variables change information processing. sense current input environment part encodes current output environment another memory previous events parts intersect sense also encode current query current answer thus representing interface learnable parameters computable function inﬂuence ﬁxed parameters computable functions previous events including queries answers transmitted learnable according argument provided conveys substantial algorithmic information task trainable interface allows address extract exploit information quickly small compared ﬁxed search space example suppose learned represent videos people placing toys boxes summarize videos textual outputs. suppose task learn control robot places toys boxes. although robot’s actuators quite different human arms hands although videos video-describing texts quite different desirable trajectories robot movements expected convey algorithmic information task perhaps form connected high-level spatio-temporal feature detectors representing typical movements hands elbows independent size. learning addresses extracts information partially reuses solve robot’s task much faster learning solve task scratch without access setups sec. special cases general scheme present sec. simple insight suggests many partially observable environments possible greatly speed program search letting learn access query exploit arbitrary computable ways program typically much bigger gradient-based used model compress agent’s entire growing interaction history failed successful trials. note sec. implement kinds well-known computable types reasoning e.g. hierarchical reuse subprograms analogy etc. perhaps even expect learn exploit human-like abstract thought. follows denote positive integer constants positive integer variables assuming ranges implicit given contexts. i-th component real-valued vector denoted rnnai’s life span discrete sequence time steps tdeath. beginning given time step normal sensory input vector reward input vector example parts represent pixel intensities incoming video frame components reﬂect external positive rewards negative values produced pain sensors whenever measure excessive temperature pressure. sense rm+n denote concatenation vectors total reward time time step rnnai produces output action vector inﬂuence environment thus future sense given time rnnai’s goal maximize rm+n+o denote concatenation sense out. denote sequence all) time able retrain components observations ever made rnnai stores entire growing lifelong sensory-motor interaction history including inputs actions reward signals observed successful failed trials including initially looks like noise later turn regular. normally done feasible today. data holy never discarded line mathematically optimal general problem solvers remarkably even human brains enough storage capacity store years sensory input reasonable resolution many rnn-like models used build general computers e.g. neural pushdown automata quickly modiﬁable differentiable external memory based fast weights closely related rnn-based meta-learners using sloppy convenient terminology refer rnns. typical implementation uses lstm network large -dimensional inputs video images ﬁrst ﬁltered lstm. cnn-lstm combination still rnn. brieﬂy summarize information processing standard rnns. using notation similar previous survey denote positive integer variables assuming ranges implicit given contexts. also denote positive integers. given moment described connected graph units unu} directed edges connections nodes. input layer input units subset fully connected rnns units connections non-input units. rnn’s behavior program determined real-valued possibly modiﬁable parameters weights episode information processing partially causal sequence real values called events. index used much ﬁne-grained index sec. single time step involve numerous events. either input environment activation unit directly depend current topology-dependent indices representing incoming causal connections links. function encode topology information event index pairs weight indices. example non-input case real-valued xkwv typically nonlinear real-valued activation function tanh. functions combine additions multiplications many activation functions possible. sequence directly affect certain outgoing connections links represented current outs indices ink. non-input events called output events. many refer different time-varying activations unit e.g. rnns. episode weight reused topology-dependent ways. weight sharing across space and/or time greatly reduce nn’s descriptive complexity number bits information required describe training algorithms rnns rnnais discussed later. several novel implementations described sec. make variable size called world model learns compactly encode growing history example predictive coding trying predict input component given history actions observations. goal discover algorithmic regularities data learning program compresses data better lossless manner. example details speciﬁed sec. initialize weights. freeze weights cannot change learns. execute trial generating ﬁnite action sequence prolongs history actions observations. actions exploit various ways train weights prolonged history generate action sequences higher expected reward using methods sec. unfreeze weights re-train sleep phase better predict/compress prolonged history; sec. stopping criterion goto central objective unsupervised learning compress observed data goal compress agent’s entire growing interaction history failed successful trials e.g. predictive coding input units receive time tdeath output units produce prediction pred rm+n sense address details training sleep phase step algorithm consider given weights default initialization unit activations. example making compress history following. given train replaying semi-ofﬂine training sequentially feeding input units standard fashion given calculates pred prediction sense. standard error function minimized gradient descent weights would pred sense deviations predictions however goal minimize total prediction error instead avoid erroneous discovery regular patterns irregular noise ait’s sound dealing overﬁtting measure compression performance number bits required specify plus bits needed encode observed deviations predictions example whenever incorrectly predicts certain input pixels perceived video frame pixel values encoded separately cost storage space. best time-bounded variant thereof integer variables bitsm bitsh denote estimates number bits required encode current deviations predictions observations current history respectively. example obtain bitsh naively assume simple bell-shaped zero-centered probability distribution ﬁnite number possible real-valued prediction errors sensei) encode −logpe bits large errors considered unlikely cost bits small ones. obtain bitsm naively multiply current number non-zero modiﬁable weights small integer constant reﬂecting weight precision. alternatively assume simple bell-shaped zero-centered probability distribution ﬁnite number possible weight values encode −logpw bits. large absolute weight values considered unlikely cost bits small ones alternatives ignore possibility entire weight matrix might computable short computer program advantage easy calculate. moreover since general computer itself least principle chance learning equivalents short programs. step algorithm starts small history grows small bitsm bitsh step uses sequential network construction regularly changes size adding pruning units connections whenever helps improve bitsm bitsh history changes kept otherwise discarded. given history instead re-training sleep phase re-train parts thereof selecting trials randomly otherwise replay retrain standard fashion this however unit activations need stored beginning trial. facilitate task certain environments frame sensory input stream ﬁrst separately compressed autoencoders autoencoder hierarchies based cnns fnns used sensory preprocessors create less redundant sensory codes compressed codes trained predict inputs compressed codes. predictions decompressed evaluate total compression performance bitsm bitsh fnn-rnn combination representing describe ways using world model sec. facilitate task controller especially systems sec. overcome drawbacks early systems mentioned sec. setups present sec. viewed special cases general scheme sec. start details approach whose principles date back early given rnn-like sec. implement traditional machine based markov assumption processing history actions observations predict future inputs internal states used inputs predictor cumulative expected future reward. speciﬁcally step algorithm consider trial lasting time tdeath. used preprocessor follows. beginning given time step trial hidden denote vector current hidden unit activations state rm+n+h denote concatenation sense hidden pred. pred initialized default values e.g. zero vectors.) machine h-dimensional inputs o-dimensional outputs. time state computes action out. computes sense hidden values hidden pred. executed environment obtain next input sense. parameters weights trained maximize reward standard method q-learning similar methods note methods evaluate input events pairs input output events. simplest cases linear perceptron fact built-in memory case fundamental restriction since recurrent trained predict normal sensory inputs also reward signals. state must contain historic information relevant maximize future expected reward provided data history already contains relevant experience learned compactly extract represent regular aspects. approach different other previous combinations traditional rnns rnns value function approximators directly predict cumulative expected reward instead trying predict sensations time step time step. system present section separates hard task prediction partially observable environments comparatively simple task markovian assumption current input contains information relevant achieving goal. approach essentially sec. except trained evolutionary algorithms applied policy gradient methods compressed search; sec. input units output units. time state computes out; computes hidden pred; executed obtain sense. rnn-based systems early could principle plan ahead performing numerous fast mental experiments predictive world model instead time-consuming real experiments extending earlier work reactive systems without memory however work well deterministic environments even there would simulate many entire alternative futures time step time step action sequence maximizes reward. method seems different much smarter hierarchical planning methods humans apparently learn identify exploit relevant problem-speciﬁc abstractions possible future events; reasoning abstractly efﬁciently ignoring irrelevant spatio-temporal details. consider sec. standard and/or multiplicative learnable connections units units typically huge unsupervised units units connections said belong collectively form called standard activation spreading sec. activations initialized default values beginning trial. trained tasks line step algorithm using search methods sec. connections however change—only connections mean? means relatively small candidate programs given time think feeding sequences activations reading activations interacting environment. since general computers programs query edit invoke subprograms arbitrary computable ways connections. given problem according argument greatly accelerate search problem-solving weight vector provided mutual algorithmic information program high expected many cases since environment-modeling program reﬂect many regularities useful prediction coding also decision making. simple novel approach much general previous computable restricted ways letting feedforward model simulating entire possible futures step step propagating error signals temporal difference errors backwards instead give program search opportunity discover sophisticated computable ways exploiting code abstract hierarchical planning analogybased reasoning. example represent previous observations implemented lstm network develop high-level abstract spatio-temporal feature detectors active thousands time steps long memories useful predict future observations however learn directly invoke corresponding abstract units inserting appropriate pattern sequences might short-cut typical subsequent abstract representations ignoring long input sequences normally required invoke thus quickly anticipating possible positive outcomes pursued negative outcomes avoided. note perfect predictor. example won’t able predict noise. instead learned approximate conditional expectations future inputs given history far. naive exploiting probabilistic knowledge would plan ahead naive step-by-step monte-carlo simulations possible m-predicted futures execute action sequences maximize expected reward predicted simulations. however won’t limit system naive approach. instead task learn address useful problem-speciﬁc parts current reuse problem solving. sure alternative letting learn access program c-owned connections weights units treating current weights additional real-valued inputs this however typically result much larger search space many variants general scheme described sec. intelligently exploit cost bits information often still much cheaper sense learning good program scratch previous non-rnn ait-based work algorithmic transfer learning self-invented recursive code previous solutions sped search code complex tasks factor numerous topologies possible adaptive connections back. although applications hard exploit might prefer ignore environments certain topologies greatly proﬁt weights frozen step algorithm weights learn make attend history information represented state ignore information instead innards computable ways. facilitated introducing special unit ˆuall instead time easily force completely ignore environmental inputs thinking ways. variant approach sec. incrementally trains never-ending series tasks continually building solutions previous problems instead learning problem scratch. principle done incremental evolution hierarchical evolution hierarchical policy gradient algorithms asymptotically optimal ways algorithmic transfer learning given task trained several previous tasks hierarchical/incremental methods freeze current weights enlarge adding units connections trained task. process reduces size search space task giving weights opportunity learn frozen parts subprograms. incremental variants compressed search directly search potentially large weight space frequency domain representing weight matrix small fourier-type coefﬁcients. searching coefﬁcients added already learned responsible solving previous problems weight matrix tuned incrementally indirectly given current problem ait-based oops style impose growing time limits programs tested solution found. humans even infants invent tasks curious creative fashion continually increasing problem solving repertoire even without external reward teacher. seem intrinsic reward creating experiments leading observations obey previously unknown allows better compression observations—corresponding discovery temporarily interesting subjectively novel regularity example video falling apples greatly compressed predictive coding gravity discovered. likewise video-like image sequence perceived moving ofﬁce greatly compressed constructing internal model ofﬁce space model allows re-computing entire high-resolution video compact sequence low-dimensional coordinates directions. model speciﬁed fewer bits information needed store pixel data long video. even model precise relatively extra bits required encode observed deviations predictions model. even mirror neurons easily explained by-products history compression sec. animal acts animal observes action performed another. mutual algorithmic information shared perceptions similar actions performed various animals efﬁcient rnn-based predictive coding proﬁts using feature detectors encode shared information thus saving storage space. given combinations sec. motivate become efﬁcient explorer artiﬁcial scientist adding standard external reward solving user-given tasks another intrinsic reward generating novel action sequences allow improve compression performance resulting data ﬁrst glance repeatedly evaluating compression performance entire history seems impractical. heuristic overcome focus improvements recent trial regularly re-training randomly selected previous trials avoid catastrophic forgetting. related problem incremental program search difﬁcult identify parts responsible improvements huge black box-like monolithic implement self-modularizing computation cost-minimizing winner-take-all possible keep track parts used encode parts history. evaluate weight changes affected parts stored history re-tested search facilitated tracking parts affected parts penalizing programs time consumed tests search biased prefer programs conduct experiments causing data yielding quickly veriﬁable compression progress program search prefer change weights used compress large parts history expensive verify ﬁrst implementations simple principle described work powerplay framework incrementally searches space possible pairs tasks modiﬁcations current program ﬁnds powerful program that unlike unmodiﬁed program solves previously learned tasks plus simpliﬁes/compresses/speeds previous solutions without forgetting any. certain conditions accelerate acquisition external reward speciﬁed user-deﬁned tasks. introduced novel combinations reinforcement learning controller rnn-based predictive world model general systems implement principles algorithmic opposed traditional information theory. rnns rnn-like systems. actively exploited arbitrary computable ways whose program search space typically much smaller learn selectively probe reuse internal programs plan reason. basic principles limited apply kinds active algorithmic transfer learning another. combining gradient-based rnns rnns create qualitatively type self-improving general purpose connectionist control architecture. rnnai continually build upon previously acquired problem solving procedures self-invented resembles scientist’s search novel data unknown regularities preferring still-unsolved quickly learnable tasks others.", "year": 2015}