{"title": "Learning Mid-Level Features and Modeling Neuron Selectivity for Image  Classification", "tag": ["cs.CV", "cs.LG", "cs.NE", "cs.RO"], "abstract": "We now know that mid-level features can greatly enhance the performance of image learning, but how to automatically learn the image features efficiently and in an unsupervised manner is still an open question. In this paper, we present a very efficient mid-level feature learning approach (MidFea), which only involves simple operations such as $k$-means clustering, convolution, pooling, vector quantization and random projection. We explain why this simple method generates the desired features, and argue that there is no need to spend much time in learning low-level feature extractors. Furthermore, to boost the performance, we propose to model the neuron selectivity (NS) principle by building an additional layer over the mid-level features before feeding the features into the classifier. We show that the NS-layer learns category-specific neurons with both bottom-up inference and top-down analysis, and thus supports fast inference for a query image. We run extensive experiments on several public databases to demonstrate that our approach can achieve state-of-the-art performances for face recognition, gender classification, age estimation and object categorization. In particular, we demonstrate that our approach is more than an order of magnitude faster than some recently proposed sparse coding based methods.", "text": "convolutional deep belief networks deconvolutional networks take features improve classiﬁcation performance generating mid-level features low-level ones operations sparse coding pooling learn mid-level features methods usually make hierarchical architecture layer accumulates information layer beneath form complex features. despite similarity mainly differ design nonlinearity important part good classiﬁcation performance spatial pyramid matching based methods apply sparse coding max-pooling nonlinearity. focuses sparse coding pooling unpooling nonlinearity cdbn uses sparse coding quasi maxpooling predictive sparse decomposition introduces nonlinear absolute value rectiﬁcation local contrast normalization however pointed coates feature-learning approaches slightly better others difference methods leads accuracy gain. moreover complex methods easily outweighed simpler ones carefully consider speciﬁc factors receptive ﬁeld size density extracted low-level features. therefore instead designing complicated approaches learn mid-level features paper propose efﬁcient mid-level feature learning method consists simple operations k-means convolution pooling vector quantization random projection shown fig. comparison sift hmax explain midfea produces desirable features argue might need spend much time learning lowlevel feature descriptors. know mid-level features greatly enhance performance image learning automatically learn image features efﬁciently unsupervised manner still open question. paper present efﬁcient mid-level feature learning approach involves simple operations k-means clustering convolution pooling vector quantization random projection. explain simple method generates desired features argue might need spend much time learning low-level feature extractors. furthermore boost performance propose model neuron selectivity principle building additional layer mid-level features feeding features classiﬁer. show ns-layer learns category-speciﬁc neurons bottom-up inference top-down analysis thus supports fast inference query image. extensive experiments several public databases demonstrate approach achieve state-of-the-art performances face recognition gender classiﬁcation estimation object categorization. particular demonstrate approach order magnitude faster recently proposed sparse coding based methods. image classiﬁcation performance relies quality image features. low-level features include common hand-crafted ones sift learned ones building blocks unsupervised model figure left panel overall ﬂowchart proposed framework. speciﬁcally proposed feed-forward midfea learns midlevel features hierarchical architecture neuron selectivity layer transforms features high-level semantic representation linear classiﬁer. right panel demonstration local descriptor assembling window help max-pooling local descriptor captures salient orientations within cuboid. effectively boost learning performance. according studies neural science neurons tend selectively respond visual signals speciﬁc categories. hence build additional neuron-selectivity layer mid-level features demonstrated fig. neurons ﬁred selectively semantically signals speciﬁc categories. modeling property structured sparse learning problem supports top-down analysis bottom-up inference ns-layer improves performance notably. summary contributions two-fold. propose simple efﬁcient method learn mid-level features. give explanation approach generates desirable features argue might need spend much time learning low-level features. best knowledge ﬁrst time neuron selectivity modeled mid-level features boost classiﬁcation performance. model builds ns-layer support fast inference appealing property real-world application. extensive experiments demonstrate framework achieves state-of-the-art results several databases also runs faster related methods order magnitude. begin describing mid-level feature learning approach section followed proposed ns-layer section present experimental validation section conclude section concept mid-level features ﬁrst introduced meaning features built low-level ones remain close image-level information without need high-level structured image description. typically mid-level features learned sparse coding techniques low-level hand-crafted ones sift however despite promising performance accuracy extracting low-level descriptors requires significant amounts domain knowledge human labor. computation also time-consuming lacks ﬂexibility. result researchers searching alternative methods learn features system efﬁcient effective. impressive unsupervised feature learning methods developed cdbn autoencoders despite differences however empirical validation conﬁrms several rules thumb. first nonlinearity mid-level feature learning leads improved performance. second even complicated feature learning methods better others factors help simpler algorithms outperform complex ones including densely extracted local descriptors suitable receptive ﬁeld size. third despite differences methods consistently learn low-level features resemble gabor ﬁlters even simplest k-means produce similar extractors inspired studies present efﬁcient midlevel feature learning approach called midfea consists soft convolution max-pooling local descriptor assembling mid-level feature generation shown fig. different mid-level feature learning methods adopts sift learns adaptive low-level mid-level features performs faster. soft convolution. midfea ﬁrst runs k-means clustering training generate low-level feature extractors. ﬁlters derived however complicated nonlinear functions analytical sparse decompositions instead convolve image ﬁlters feature maps seen rd-order tensor left panel fig. worth noting that different simple convolution soft convolution adaptively generates sparse feature maps. sconv consists several steps convolution normalization thresholding demonstrated fig. several advantages sconv. first convolutional behavior equals exhaustively dealing possible patches image amounts densest local descriptor extraction. second normalization along third mode preserves local contrast information counting statistic orientations thus makes resultant maps resistant illumination changes shown fig. third sparse property means trivial information background noises ﬁltered thresholding. seen fig. comparison dense sift feature maps. max-pooling. adopt max-pooling operation obtain robustness sconv feature maps. suppose ﬁlters generate feature maps max-pooling operates cuboid size meaning non-overlapping neighborhood every pair previous feature maps location. pooling leads single value maximum within volume. macro perspective maps half size compared previous ones. simple operation reduces size feature maps also captures salient information neighborhood eliminating trivial orientations. worth noting also uses max-pooling nonlinearity. however top-down analysis method requires much time derive feature maps sparse coding. feed-forward thus performs fast. local descriptor assembling. low-level local descriptor concretely sconv ﬁrst convolves image lowlevel ﬁlters followed normalization feature maps along third mode comparative range; thresholds maps element-wisely mean produces sparse ones; ﬁnally normalizes sparse maps along third mode sake subsequent operations. test several choices threshold maps using mean threshold maps always produce good results. figure demonstration soft convolution three images person different illumination conditions; convolutional feature maps image displayed three different ﬁlters; normalized maps thresholded maps normalized maps assembled resulted sconv feature maps demonstrated right panel fig. detail splitting feature overlapping patches produce times maps. hence feature maps generate ones now. better perception feature maps please refer sift feature maps sparse coding based densely extract sift descriptors patches centered every pixel generate feature maps size image. mid-level feature generation. encode descriptors vector quantization dictionary learned hand. then max-pooling codes predeﬁned partitions image concatenate pooled codes large vector image representation. concatenated vector usually thousands dimensions random projection dimensionality reduction normalize reduced vector unit length ﬁnal mid-level feature. note random project cheap perform involve large matrix factorization opposed dimensionality reduction methods. moreover even random project improve discrimination reduced data produce performance-guaranteed results contrast hand-crafted low-level descriptors sift hmax learned adaptively within data domain unsupervised manner. despite main difference model shares similarities hardwired features. example sift captures eight ﬁxed orientations image patches this also captures subtle important information convenience. example image object categorization partitioned spatial-pyramid scales partitions different different tasks details presented experiments. tures. words speciﬁc sparse patterns different class labels. presenting produce structured activations ﬁrst consider topfeedback analysis activations inspiration neural science successful applications computer vision ﬁeld paper choose simple linear decoder fulﬁll goal rp×d weight matrix controls linear decoder. back idea neuron selectivity reﬂected appropriate constraints unify top-down analysis bottom-up inference formulation below note input mid-level features normalized encoder function bounded range therefore without losing generality considering decoder linear combination bases reconstruct mid-level features constrain columns unit euclidean length i.e. constraint crucial reﬂects neuron selectivity property. hence enforce class-speciﬁc constraint activations. words particular neurons actively respond signals speciﬁc class others stay unﬁred. property modeled structured sparse learning problem. instead explicitly allocating neurons class implicit model property imposing norm eliminate besides activations class resemble other different classes ought different possible. activations class class denoted force similar minimizing mean vector matrix activations class. time differentiate activations different classes drive activations independent taking constraints lagrangian multiplier have adaptivity ﬂexibility learning process. moreover descriptor also resembles hmax feature derived feed-forward pathway incorporate convolution max-pooling. hmax built two-layer architecture sparsity feature maps produce complicated resilient features deeper architecture soft convolution. comparisons methods actually low-level descriptors count statistical information related local orientations. therefore need incorporate low-level feature learning stage pipeline high-level tasks along learning supervised manner. additionally max-pooling also adopted adaptive nonlinear transformation. however sparse feature maps calculated convolutional sparse coding means maps negative values hard interpret. therefore adaptive considers absolute values. contrary proposed soft convolution provides non-negative elements feature maps model interpretable w.r.t capturing statistic information orientations. mid-level features generated purely unsupervised manner. sake classiﬁcation propose build additional layer mid-level features boost performance. layer models neuron selectivity principle neural science means certain neurons actively respond signals speciﬁc category others stay unﬁred. therefore call layer neuron selectivity layer output classiﬁer classiﬁcation. denote mid-level feature input image belongs classes. would like build ns-layer neurons. then principle mathematically modeled structured sparse learning problem. given speciﬁc mid-level feature ns-layer neurons selectively respond feature generate activations turn encoder function ﬁlter rd×p derive activations paper logistic function generate element-wise activations constraint deﬁned several meanings. help ﬁrst term sufﬁciently large decays third term zero. means neurons automatically break separate parts part corresponds speciﬁc class. large enough neurons respond stimuli class identical behavior. means second term enforces mechanism strong classiﬁer. instead forcing penalty rigorously three parameters proper values allow intra-class variance preserved prevent overﬁtting training process neurons shared across categories combination ﬁred neurons ensures discrimination compactness. arrive ﬁnal objective function lagrangian multiplier mathematically proposed ns-layer reﬂected objective function seen fast inference sparse coding different methods predicts structured sparse codes moreover decoder term constraints seen discriminative dictionary learning improve classiﬁcation performance analytical manner. joint sigmoid encoder sparse codes forced non-negative. desirable property models intuition combining parts form whole opposed classic sparse coding includes negative elements cancel section extensive experiments evaluate proposed midfea ns-layer feature learning image classiﬁcation performance. first study midfea ns-layer classiﬁcation accuracy gains controlled way. then highlight efﬁciency framework according inference time object categorization. moreover carry classiﬁcation comparisons speciﬁcally classiﬁcation comparisons ﬁrst subset database face recognition gender classiﬁcation. database consists male female subjects subject images captured sessions illumination expression changes. face recognition ﬁrst images session person used training rest testing; gender classiﬁcation ﬁrst male ﬁrst female individuals used training rest testing. also test framework estimation fg-net database images spanning consistent literature leave-one-person-out setting evaluation. finally evaluate framework object categorization caltech caltech throughout experiments linear toolbox classiﬁer choose settings learn low-level features i.e. adaptively learning ﬁlters soft convolution size partitions spatial pooling different different tasks demonstrate along experiments. moreover classiﬁcation accuracy face recognition gender classiﬁcation object categorization mean absolute error estimation. demonstrate superiority midfea nslayer compare model controlled selftaught learning method seen three-layer network sparse codes mid-level features. face recognition database used comparison tens thousands face images downloaded internet used unsupervised feature learning. vary number dictionary bases number unlabeled face images record fig. classiﬁcation accuracies well obtained linear image. ﬁgure consistent literature neurons lead better performance unlabeled data learns reliable dictionary sufﬁcient unlabeled data available learn dictionary certain amount bases accuracy eventually saturate. order magnitude. three methods original sift low-level feature descriptor thus need seconds extract sift features image pixels. even using fast sift extraction approach local descriptor generation still time slower ours. furthermore scspm adopt sparse coding locality-restricted coding hence running time required. especially sorting process required coding local descriptor much slower. considering main steps model amenable parallelization gpu-based implementation expect applications real world. evaluate model facial attributes recognition face recognition gender classiﬁcation database estimation fg-net database. linear image acts baseline fair comparison database choose several state-of-the-art methods source codes online available including fddl lc-ksvd random face input ﬁrst three methods reproduce results. like framework output also projected dimension random matrix linear svm. face recognition gender classiﬁcation midfea learns mid-level features codewords single layer partition spatial pooling. moreover ns-layer learns neurons tasks respectively. detailed comparisons listed table learned neurons w.r.t tasks displayed fig. table proposed midfea ns-layer performance outperforms compared ones. furthermore shown fig. even tasks share database learned neurons nslayer capture speciﬁc characteristics according task. intuitively demonstrates reason proposed ns-layer works classiﬁcation. display learned neurons hereafter dimensionality reduction random projection. moreover spatial pooling waived here neurons averaged w.r.t image projected back input space sake demonstration. figure demonstration proposed midfea nslayer accuracy gains comparison self-taught learning seen three-layer network sparse codes mid-level features. x-axis indicates amount unlabeled data used learning bases number bracket shows dimensions mid-level features however ns-layer mid-level features produced notable gain obtained. moreover features generated midfea much better performance achieved. surprise ns-layer built midfea best performance recorded. conclude that proposed midfea learns robust features represent image ns-layer boosts ﬁnal classiﬁcation performance. highlight efﬁciency framework study inference time -pixel image matlab dual-core .ghz -bit ram. main steps framework include softthreshold convolution pooling local descriptor assembling spatial pyramid pooling random projection inference classiﬁer. midfea learns features bottom-up manner costs much less time top-down methods adaptive speciﬁcally adaptive needs minute produce feature maps seconds kernel classiﬁer. much slower almost orders magnitude involves multiple iterations decomposing image multi-layer feature maps. additionally compare model three feed-forward methods kernel scspm table summarizes detailed comparisons demonstrate method performs much faster compared ones table comparison detailed inference time stand vector quantization sparse coding spatial pyramid respectively. fair comparison mid-level features methods reduced dimension random projection. several state-of-the-art methods compared here including ages ohrank mtwgp recent casvr except ages methods uses images active appearance model generate -word codebook midfea deﬁne partition single layer ×pixel overlapping grids spatial pooling. results listed table demonstrate model performs slightly better best performance ever reported recent ca-svr sophisticatedly designed deal imbalanced data problem e.g. images years above. ohrank also deals sparse data performs slow showed method resembles uses biologically-inspired feature mid-level features which however generated shallower architecture. neurons shown fig. demonstrate model reveals information wrinkles face. figure visual comparison local descriptor feature maps model scspm three images caltech. nine learned ﬁlters presented shows three original images whose feature maps generated model scspm presented panel note last image average feature maps. averaged sift distributes attention uniformly image mainly focuses object. -pixel resolution preserved aspect ratio. compared methods include recent unsupervised feature learning methods well-known methods hand-crafted sift features. former methods include cdbn adaptive latter include kspm scspm methods midlevel features generated -word codebook sparse coding reduced dimension random projection. model ns-layer learns neurons total database respectively assuming average neurons associate speciﬁc category. classic -layer-pyramid partition pooling used. detailed comparisons listed table caltech test time image table caltech standard deviations trials. easy method outperforms sophisticated sift descriptor. importantly inference speed model much faster compared ones order magnitude. actually assemble sift descriptor every possible patch image feature maps scspm. therefore compare feature maps scspm intutable accuracies inference time caltech. timing comparison mid-level features methods reduced dimension random projection. observe reduction effect accuracies methods. time bracket achieved improved sift extraction method itively superiority model. fig. displays learned low-level feature extractors shows feature maps three images furthermore average feature maps show averaged last column fig. easy sift feature maps scspm incorporates cluttered background focuses object discards noisy region large extent. attribute proposed soft convolution step. admittedly better performances reported literature datasets. example accuracy achieved caltech sift feature sparse coding intersection kernel; sohn obtain caltech much larger codebook complicated sparse coding technique sift feature; varma achieve state-of-the-art caltech reported multiple hand-crafted feature types sophisticated techniques w.r.t dataset. however model achieves impressive performance simply learning features purely unsupervised manner low-level mid-level. optimal mechanism task classiﬁcation. therefore based framework sophisticated task-driven methods combined address speciﬁc vision-based problems. discuss crucial parameters model including moreover number neurons ns-layer also studied. fig. presents curve accuracy parameter database face classiﬁcation easy classiﬁcation accuracy sensitive parameters remains stable large range them. well hyper-parameters reveal terms objective function indeed brings performance gains. furthermore show accuracies neuron number ns-layer gender classiﬁcation fig. data sufﬁcient task accuracy peaks small number neurons demonstrates effectiveness ns-layer serves classiﬁcation high level. paper present simple efﬁcient method learning mid-level features. comparison sophisticated hand-crafted features explain proposed method produces desired features. argue might need spend much time learning lowlevel feature extractors. furthermore propose build additional layer higher level classiﬁcation. layer models principle neuron selectivity neural science. given image midfea produces midlevel features quickly feed-forward process ns-layer also supports fast bottom-up inference. result model performs faster others order magnitude achieves comparable even state-of-the-art classiﬁcation performance demonstrated experiments. despite effectiveness proposed feature learning method performance gains public databases especially caltech object categorization still remains marginal. reason obvious foreground objects large changes appearance translation scale. therefore sophisticated features like paper explicit mechanism deal changes still solicited references bienenstock elie cooper leon munro paul theory development neuron selectivity orientation speciﬁcity binocular interaction visual cortex. journal neuroscience donahue jeff yangqing vinyals oriol hoffman judy zhang ning tzeng eric darrell trevor. decaf deep convolutional activation feature generic visual recognition. icml fei-fei fergus perona pietro. learning generative visual models training examples incremental bayesian approach tested object categories. cviu georghiades athinodoros belhumeur peter kriegman david many illumination cone models face recognition variable lighting ieee transactions pattern analysis pose. machine intelligence honglak grosse roger ranganath rajesh andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. icml wang donghui kong shu. classiﬁcation-oriented dictionary learning model explicitly learning particularity commonality across categories. pattern recognition then normalize column unit euclidean length. note step pose negative affects overall performance newly updated dependent adaptively deal scaling change. algorithm algorithmic summary layer require training class initialize randomly stop criterion reached update gradient update gradient update gradient whilereturn learned usually good initialization variables lead fast convergence. example linear decoder dictionary pre-trained among category. rp×d neurons response data class. then merely k-means clustering among data pool class obtain then activations initialized initialized speciﬁcally datum calculate vector element however initialization method lack ﬂexibility allocation category must predeﬁned hand. therefore also simply initialized variables non-negative random matrices serve purpose symmetry breaking. empirically observe random initialization mean inferior performance requires time converge. that even framework deep architecture ns-layer incorporates hidden layer hence random initialization works quite well. limited space paper explain advantages proposed mid-level feature learning method especially soft convolution stated paper signiﬁcant advantages sconv vector/matrix elements equaling appropriate size. cannot derived directly update them. simple derivations denoting element-wise operation gradient illumination-invariant feature maps obtained proposed soft convolution requires several simple operations convolution normalization soft thresholding. pipeline sconv following study illumination invariance property extended yaleb database demonstration. database contains frontal face images individuals challenging varying illumination conditions expressions. therefore illustrate proposed soft convolution generating illumination-invariant feature maps. moreover low-level feature extractors shown fig. randomly select three different persons demonstration show three different images different illumination person fig. fig. fig. detail ﬁgure original image convolutional feature maps normalized convolutional maps soft-threshold maps normalized maps softthreshold ones max-pooling feature maps respecalso evaluate framework database face recognition. consistent settings literature face recognition database half images individual randomly selected training rest test. method leads average accuracy. best knowledge best performance ever reported setting database. second highlight generated feature maps ﬁlter background noises extent. property brought proposed soft convolution max-pooling. detail trivial weak orientation responses feature maps removed soft threshold operation strong ones kept strengthened normalization. show property display full feature maps appear experimental section paper. three images caltech database randomly selected come three different categories faces motorbikes panda here so-called feature maps consist assembled local descriptors patch. similarly compared sift feature maps scheme consist sift descriptors patches centered every pixel image therefore image generates feature maps. three sets feature maps images display fig. fig. fig. respectively. ﬁgure shows original gray-scale image averaged feature maps sift maps demonstrated respectively. ﬁgures sift distributes attention uniformly image mainly focuses target object constructing mid-level features. figure ﬁrst person. original image convolutional feature maps normalization convolutional feature maps soft-threshold maps normalization soft-threshold maps max-pooling. figure second person. original image convolutional feature maps normalization convolutional feature maps soft-threshold maps normalization soft-threshold maps max-pooling. figure third person. original image convolutional feature maps normalization convolutional feature maps soft-threshold maps normalization soft-threshold maps max-pooling. figure faces image. original image average maps sift descriptor average maps proposed local descriptor full sift feature maps full maps proposed descriptors. figure motorbikes image. original image average maps sift descriptor average maps proposed local descriptor full sift feature maps full maps proposed descriptors. figure panda image. original image average maps sift descriptor average maps proposed local descriptor full sift feature maps full maps proposed descriptors.", "year": 2014}