{"title": "Generalized Thompson Sampling for Contextual Bandits", "tag": ["cs.LG", "cs.AI", "stat.ML", "stat.OT", "62L05", "I.2.6"], "abstract": "Thompson Sampling, one of the oldest heuristics for solving multi-armed bandits, has recently been shown to demonstrate state-of-the-art performance. The empirical success has led to great interests in theoretical understanding of this heuristic. In this paper, we approach this problem in a way very different from existing efforts. In particular, motivated by the connection between Thompson Sampling and exponentiated updates, we propose a new family of algorithms called Generalized Thompson Sampling in the expert-learning framework, which includes Thompson Sampling as a special case. Similar to most expert-learning algorithms, Generalized Thompson Sampling uses a loss function to adjust the experts' weights. General regret bounds are derived, which are also instantiated to two important loss functions: square loss and logarithmic loss. In contrast to existing bounds, our results apply to quite general contextual bandits. More importantly, they quantify the effect of the \"prior\" distribution on the regret bounds.", "text": "thompson sampling oldest heuristics solving multi-armed bandits recently shown demonstrate state-of-the-art performance. empirical success great interests theoretical understanding heuristic. paper approach problem different existing efforts. particular motivated connection thompson sampling exponentiated updates propose family algorithms called generalized thompson sampling expert-learning framework includes thompson sampling special case. similar expert-learning algorithms generalized thompson sampling uses loss function adjust experts’ weights. general regret bounds derived also instantiated important loss functions square loss logarithmic loss. contrast existing bounds results apply quite general contextual bandits. importantly quantify effect prior distribution regret bounds. thompson sampling oldest heuristics solving stochastic multi-armed bandits embodies principle probability matching. given prior distribution underlying unknown reward generating process well past observations rewards maintain posterior distribution optimal. thompson sampling selects arms randomly according current posterior distribution. unpopular decades algorithm recently shown state-of-the-art empirical studies found success important applications like news recommendation online advertising addition advantages robustness observation delay simplicity implementation compared dominant strategies based upper conﬁdence bounds despite empirical success theoretical understanding ﬁnite-time performance thompson sampling limited recently. ﬁrst result provided non-contextual k-armed bandits prove nontrivial problem-dependent regret bound prior arm’s expected reward beta distribution. later improved bounds found setting match asymptotic regret lower bound contextual bandits pieces work available best knowledge. analyze linear bandits gaussian prior used weight vector space gaussian likelihood function assumed reward function. authors able show regret grows order factor away known matching lower bound contrast establish interesting connection ucb-style analysis bayes risk thompson sampling based probability-matching property. observation allows authors obtain bayes risk bound based novel metric known margin dimension arbitrary function class essentially measures fast upper conﬁdence bounds decay. analysis although interesting important better understanding thompson sampling seems hard generalized general contextual bandits. furthermore none existing theory able quantify role prior plays controlling regret although practice better domain knowledge often available construct good priors accelerate learning. paper attempts address limitations prior work different angle. based connection thompson sampling exponentiated update rules propose family contextual-bandit algorithms called generalized thompson sampling expert-learning framework expert corresponds contextual policy selection. similar thompson sampling generalized thompson sampling randomized strategy following expert’s policy often expert likely optimal. different thompson sampling uses loss function update experts’ weights; thompson sampling special generalized thompson sampling logarithmic loss used. regret bounds derived certain conditions. proof relies critically novel application selfboundedness property loss functions competitive analysis. results instantiated square logarithmic losses important loss functions. bounds apply quite general sets experts also quantify impact prior distribution regret. beneﬁts come cost worse dependence number steps. however believe possible close involved analysis connection thompson sampling expert-learning likely lead interesting insights algorithms future work. note setup allows contexts chosen adversary general setting typical contextual bandits reader notice require reward binary instead choice make exposition simpler without sacriﬁcing loss generality. indeed also suggested reward received convert binary pseudo-reward follows probability otherwise. clearly bandit process remains essentially same optimal expert regrets. motivated prior work thompson sampling parametric function classes allow learner access experts makes predicts average reward associated prediction function expert arm-selection policy context simply greedy policy respect reward predictions maxa∈a setting naturally used capture parametric function classes example generalized linear models used predict weight vector expert. difference framework works discrete experts. using covering device however possible approximate continuous function class ﬁnite cardinality covering number. expectation refers possible randomization learner selecting existing analysis thompson sampling make realization assumption experts correctly predicts average reward. without loss generality expert; words clearly emphasized that paper loss function measure well expert predicts average reward given context selected arm. general loss function reward completely unrelated. details given later. notation above thompson sampling described follows. requires input prior distribution intuitively interpreted prior probability reward-maximizing expert. algorithm starts ﬁrst posterior distribution step algorithm samples expert based posterior distribution follows expert’s policy choose action. upon receiving reward weights updated wit+ rt)) negative log-likelihood. finally assume optimal expert drawn unknown prior distribution noted bayes risk expected -step bayes regret deﬁned considered authors prior used thompson sampling. general true prior unknown believe bayes risk deﬁned respect reasonable light almost inevitable misspeciﬁcatin priors practice. observation thompson sampling previous section bayes update rule viewed exponentiated update logarithmic loss receiving reward expert penalized mismatch prediction observed reward penalty happens logarithmic loss thompson sampling. therefore principle loss function general family algorithms. fact none existing regret analyses relies interpretation meant bayesian posteriors manages show strong regret bound thompson sampling. observations suggest promising performance thompson sampling bayesian nature also motivates develop general family algorithms known generalized thompson sampling. denote loss incurred reward prediction observed reward generalized thompson sampling performs exponentiated updates adjust experts’ weights follows randomly selected expert making decisions similar thompson sampling. addition algorithm also allows mixing exponentially weighted distribution uniform distribution controlled pseudocode given algorithm clearly generalized thompson sampling includes thompson sampling special case setting logarithmic loss another loss function considered paper square loss er|xahˆlii exists constant κp¯lt. shifted loss assumes values exists constant that er|xahˆlii κer|xahˆlii; namely second moment bounded constant ﬁrst moment shifted proof. first observe shifted loss ˆlit used generalized thompson sampling replace loss algorithm behaves identically. rest proof uses fact pretending generalized thompson sampling uses ˆlit weight updates. step weight changes according ﬁrst inequality condition inequality second inequality inequality conditioned observed context selected step take expectation expressions respect randomization observed reward leading assumption usually satisﬁed practice seems necessary derive ﬁnite-time guarantees. note assumption slightly weaker common assumption logarithmic loss bounded ﬁrst inequality triangle inequality; second inequality pinsker’s inequality; fourth inequality jensen’s inequality; ﬁfth inequality fact selected probability least γ/k; last equality equation condition immediately satisﬁed normalization deﬁnition above. condition difﬁcult verify. best knowledge result logarithmic loss found literature independent interest. example implies analysis square loss also applies logarithmic loss. following lemma states result formally. proof rather technical left appendix. paper propose family algorithms generalized thompson sampling analyze regret expert-learning framework. regret analysis provides promising alternative understanding strong performance thompson sampling interesting pressing research problem raised recent empirical success. compared existing analysis literature following beneﬁts. first results apply generally experts rather making speciﬁc modeling assumptions prior likelihood. second analysis quantiﬁes affects regret bound well bayes regret optimal experts drawn unknown prior similar pac-bayes bounds results combine beneﬁts good priors robustness frequentist approaches. proof generalized thompson sampling inspired online-learning literature however technique needed prove critical lemma relies self-boundedness loss function. similar property shown square loss only used different way. self-boundedness logarithmic loss appears best knowledge independent interest. generalized thompson sampling bears similarities regressor elimination algorithm crucial difference requires computationally expensive operation computing balanced distribution experts order control variance elimination process. contrast algorithm computationally much cheaper. operations generalized thompson sampling also related uses unbiased importance-weighted reward estimates exponentiated updates expert weights. practice seems natural prediction loss expert adjust weight rather using reward signals directly focused case ﬁnitely many experts setting motivated realistic case experts continuous discrete case considered thought approximation continuous case using covering device. expect similar results hold replaced covering number class. work suggests interesting directions future work. ﬁrst close current bound best problem-independent bound contextual bandits. second extend analysis continuous expert classes importantly agnostic case. finally interesting regret analysis thompson sampling obtain performance guarantees reinforcement-learning analogues section proves lemma regarding self-boundedness logarithmic loss sense described condition analysis involve step corresponding context selected arm. therefore simplify notation follows true expert predicts expert predicts binary reward bernoulli random variable success rate shifted logarithmic loss given function veriﬁed rather tedious calculations exists maximized making close either follows rather tedious calculations using assumption log-ratios bounded", "year": 2013}