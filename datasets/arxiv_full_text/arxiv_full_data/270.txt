{"title": "Adversarial Connective-exploiting Networks for Implicit Discourse  Relation Classification", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.", "text": "implicit discourse relation classiﬁcation great challenge lack connectives strong linguistic cues motivates annotated implicit connectives improve recognition. propose feature imitation framework implicit relation network driven learn another neural network access connectives thus encouraged extract similarly salient features accurate classiﬁcation. develop adversarial model enable adaptive imitation scheme competition implicit network rival feature discriminator. method effectively transfers discriminability connectives implicit features achieves state-of-the-art performance pdtb benchmark. discourse relations connect linguistic units clauses sentences form coherent semantics. identiﬁcation discourse relations beneﬁt variety downstream applications including question answering machine translation text summarization forth. connectives critical linguistic cues identifying discourse relations. explicit connectives present text simple frequency-based mapping sufﬁcient achieve classiﬁcation accuracy contrast implicit discourse relation recognition long seen challenging problem best accuracy still lower implicit case discourse relations lexicalized various attempts made directly infer underlying relations modeling semantics arguments ranging feature-based methods recent end-to-end neural models despite impressive performance absence strong explicit connective cues made inference extremely hard hindered improvement. fact even human annotators would make connectives relation annotation. instance popular penn discourse treebank benchmark data annotated ﬁrst inserting connective expression manually determining abstract relation combining implicit connective contextual semantics. therefore huge performance explicit implicit parsing well human annotation practice strongly motivates incorporate connective information guide reasoning process. paper aims advance implicit parsing making annotated implicit connectives available training data. recent work explored combination. zhou developed two-step approach ﬁrst predicting implicit connectives whose sense disambiguated obtain relation. however pipeline approach usually suffers error propagation method itself relied hand-crafted features necessarily generalize well. research leveraged explicit connective examples data augmentation work orthogonal complementary line. paper propose novel neural method incorporates implicit connectives principled adversarial framework. deep neural models relation classiﬁcation take intuition that sentence arguments integrated connectives would enable highly discriminative neural features accurate relation inference ideal implicit relation classiﬁer even though without access connectives mimic connective-augmented reasoning behavior extracting similarly salient features. therefore setup secondary network addition implicit relation classiﬁer building upon connectiveaugmented inputs serving feature learning model implicit classiﬁer emulate. methodologically however feature imitation problem challenging semantic induced adding connective cues. necessary develop adaptive scheme ﬂexibly drive learning transfer discriminability. devise novel adversarial approach enables self-calibrated imitation mechanism. speciﬁcally build discriminator distinguishes features counterpart networks. implicit relation network trained correctly classify relations simultaneously fool discriminator resulting adversarial framework. adversarial mechanism emerging method different context especially image generation domain adaptation adversarial framework unique address neural feature emulation models. besides best knowledge ﬁrst adversarial approach context discourse parsing. compared previous connective exploiting work method provides integration paradigm end-to-end procedure avoids inefﬁcient feature engineering error propagation. proposed adversarial model greatly improves standalone neural networks previous best-performing approaches. also demonstrate implicit recognition network successfully imitates extracts crucial hidden representations. begin brieﬂy reviewing related work section section presents proposed adversarial model. section shows substantially improved experimental results previous methods. section discusses extensions future work. surge interest implicit discourse parsing since release pdtb ﬁrst large discourse corpus distinguishing implicit examples explicit ones. large work focused direct classiﬁcation based observed sentences including structured methods linguistically-informed features end-to-end neural models combined approaches however lacking connective cues makes learning purely contextual semantics full challenges. prior work attempted leverage connective information. zhou also incorporate implicit connectives pipeline manner ﬁrst predicting implicit connective language model determining discourse relation accordingly. instead treating implicit connectives intermediate prediction targets suffer error propagation connectives induce highly discriminative features guide learning implicit network serving adaptive regularization mechanism enhanced robustness generalization. framework also end-to-end avoiding costly feature engineering. another notable line aims adapting explicit examples data synthesis multi-task learning word representation work orthogonal complementary methods implicit connectives annotated implicit examples. figure architecture proposed method. framework contains three main components implicit relation network i-cnn sentence arguments connective-augmented relation network a-cnn whose inputs augmented implicit connectives discriminator distinguishing features networks. features ﬁnal classiﬁer relation classiﬁcation. discriminator i-cnn form adversarial pair feature imitation. test time implicit network i-cnn classiﬁer used prediction. adversarial method gained impressive success deep generative modeling domain adaptation generative adversarial nets learn produce realistic images competition image generator real/fake discriminator. professor forcing applies similar idea improve longterm generation recurrent neural language model. approaches extend framework controllable image/text generation. salimans propose feature matching trains generators match statistics real/fake examples. features extracted discriminator rather classiﬁer networks case. work differs since consider context discriminative modeling. adversarial domain adaptation forces neural network learn domain-invariant features using classiﬁer distinguishes domain network’s input data based hidden feature. adversarial framework distinct besides implicit relation network construct second neural network serving teacher model feature emulation. best knowledge ﬁrst employ idea adversarial learning context discourse parsing. propose novel connective exploiting scheme based feature imitation derive adversarial framework achieving substantial performance gain existing methods. proposed approach generally applicable tasks utilizing indicative side information. give discussions section discourse connectives indicators discourse relation. annotation procedure pdtb implicit relation benchmark annotators inserted implicit connective expressions adjacent sentences lexicalize abstract relations help ﬁnal decisions. model aims making full provided implicit connectives training time regulate learning implicit relation recognizer encouraging extraction highly discriminative semantics arguments improving generalization test time. method provides novel adversarial framework leverages connective information ﬂexible adaptive manner efﬁciently trained end-to-end standard back-propagation. basic idea proposed approach simple. want implicit relation recognizer predicts underlying relation sentence arguments without discourse connective prediction behaviors close connectiveaugmented relation recognizer provided discourse connective addition arguments. connective-augmented recognizer analogy annotator help connectives human annotation process discriminator differentiate reasoning behaviors implicit network i-cnn augmented network a-cnn. speciﬁcally binary classiﬁer takes inputs latent feature derived either i-cnn acnn given appropriate data output estimates probability comes connective-augmented a-cnn rather i-cnn. training procedure system trained alternating optimization procedure updates components interleaved manner. section ﬁrst present training objective component give overall training algorithm. denote parameters implicit network i-cnn classiﬁer respectively. model trained correctly classify relations training data produce salient features close connectiveaugmented ones. ﬁrst objective fulﬁlled minimizing usual cross-entropy loss cross-entropy loss predictive distribution ground-truth label achieve objective minimizing discriminator’s chance correctly telling apart features implicit recognizer would improved learning informed annotator. specifically want latent features extracted models match closely possible explicitly transfers discriminability connective-augmented representations implicit ones. instead manually selecting closeness metric take advantage adversarial framework constructing two-player zero-sum game implicit recognizer rival discriminator. discriminator attempts distinguish features extracted relation models implicit relation model trained maximize accuracy implicit data time confuse discriminator. next ﬁrst present overall architecture proposed approach develop training procedure components realized deep neural networks detailed modeling choices discussed section pair input output implicit relation classiﬁcation pair sentence arguments underlying discourse relation. training example also includes annotated implicit connective best expresses relation. figure shows architecture framework. neural model implicit relation classiﬁcation extracts latent representation arguments denoted feeds feature classiﬁer ﬁnal prediction ease notation also denote latent feature data second relation network takes inputs sentence arguments along implicit connective induce connectiveaugmented representation obtains relation prediction note ﬁnal classiﬁer used networks feature representations networks ensured within semantic space enabling feature emulation presented shortly. mentioned above classiﬁer implicit network forcing uniﬁed feature space networks. combine objectives eqs.- relation classiﬁers minimize joint loss balancing parameters calibrating weights classiﬁcation losses feature-regulating loss. practice pretrain implicit augmented networks independently minimizing respectively. adversarial training process found setting gives stable convergence. connective-augmented features ﬁxed pre-training stage. algorithm summarizes training procedure interleave optimization iteration. practical details provided section instantiate modules neural networks differentiable perform optimization efﬁciently standard stochastic gradient descent back-propagation. discriminator implicit relation network follow minimax competition drives improve implicit feature representations close connective-augmented latent representations encouraging implicit network extract highly discriminative features sentence arguments relation classiﬁcation. alternatively adaptive regularization implicit model which compared pre-ﬁxed regularizors -regularization provides ﬂexible self-calibrated mechanism improve generalization ability. figure neural structure i-cnn. sets convolutional ﬁlters shown corresponding features blue respectively. weights ﬁlters input arguments tied. presented adversarial framework implicit relation classiﬁcation. discuss model realization component. components framework parameterized neural networks. distinct roles modules framework lead different modeling choices. relation classiﬁcation networks figure illustrates structure implicit relation network i-cnn. convolutional network common architectural choice discourse parsing. network takes inputs word vectors tokens sentence argument maps argument intermediate features shared convolutional layer. resulting representations concatenated pooling layer select salient features ﬁnal representation. ﬁnal classiﬁer simple fully-connected layer followed softmax classiﬁer. connective-augmented network a-cnn similar structure i-cnn wherein implicit connective appended second sentence input. difference i-cnn adopt average k-max pooling takes average top-k maximum values pooling window. reason prevent network solely selecting connective induced features would case using pooling instead force also attend contextual features derived arguments. facilitates homogeneous output features networks thus facilitates feature imitation. experiments ﬁxed discriminator discriminator binary classiﬁer identify correct source input feature vector. make strong rival feature imitating network model discriminator multi-layer perceptron enhanced gated mechanism efﬁcient information shown figure demonstrate effectiveness approach quantitatively qualitatively extensive experiments. evaluate prediction performance pdtb benchmark different settings. method substantially improves diverse previous models especially practical multi-class classiﬁcation task. perform in-depth analysis model behaviors show adversarial framework successfully enables implicit relation model imitate learn discriminative features. experiment setup pdtb largest manually annotated discourse relation corpus. dataset contains implicit relation instances total three levels senses level- class level- type level- subtypes. level consists four major relation classes comparimake extensive comparison prior work implicit discourse relation classiﬁcation evaluate popular experimental settings multi-class classiﬁcation nd-level types oneversus-others binary classiﬁcations st-level classes describe detailed conﬁgurations following respective sections. focus analysis multiclass classiﬁcation setting realistic practice serves building block complete discourse parser shared tasks conll- model training provide detailed model training conﬁgurations supplementary materials mention here. throughout experiments i-cnn a-cnn contains sets convolutional ﬁlters ﬁlter sizes selected set. ﬁnal singlelayer classiﬁer contains neurons. discriminator consists fully-connected layers gated pathways layer layer layer adversarial model training critical keep balance progress players. simple strategy iteration optimizes discriminator implicit relation network randomly-sampled minibatch. found enough stabilize training. neural parameters trained using adagrad initial learning rate balancing parameters initialization stage weights connective-augmented network a-cnn ﬁxed. shown capable giving stable good predictive performance system. mainly focus general multi-class classiﬁcation problem alternative settings adopted prior work showing superiority model previous state arts. perform in-depth comparison carefully designed baselines providing empirical insights working mechanism proposed framework. broader comparisons also report performance one-versus-all setting. multi-class classiﬁcations ﬁrst adopt standard pdtb splitting convention following denoted pdtb-lin sections used training test sets respectively. frequent types relations selected task. training instances annotated relation types considered multiple instances annotations. test time prediction matches gold types considered correct. test contains examples. please refer details. alternative slightly different multi-class setting used denoted pdtb-ji sections used training test sets respectively. resulting test contains examples. also evaluate setting thorough comparisons. table shows classiﬁcation accuracy settings. model achieves state-of-the-art performance greatly outperforming previous methods various modeling paradigms including linguistic feature-based model pure neural methods combined approach obtain better insights working mechanism method compare carefully selected baselines shown rows word-vector sums word vectors sentence representation showing base effect word embeddings. standalone convolutional exact architecture implicit relation network. model trained within proposed framework provides signiﬁcant improvement showing beneﬁts utilizing implicit connectives training time. ensemble neural architecture proposed framework except input a-cnn augmented implicit connectives. essentially ensemble implicit recognition networks. method performs even inferior single model. conﬁrms necessity exploiting connective information. multi-task convolutional augmented additional task simultaneously predicting implicit connectives based network features. straightforward incorporating connectives method slightly improves stand-alone falling behind approach large margin. indicates proposed feature imitation effective scheme making implicit connectives. last -reg also implements feature mimicking imposing distance penalty implicit relation features connective-augmented features. simple model obtained improvement previous best-performing systems settings validating idea imitation. however contrast ﬁxed regularization adversarial framework provides adaptive mechanism ﬂexible performs better shown table. one-versus-all classiﬁcations also report results four one-versus-all binary classiﬁcations comparisons prior work. follow conventional experimental setting selecting sections training test sets. detailed data statistics provided figure test-set performance three components training epochs. relation networks a-cnn i-cnn measured multi-class classiﬁcation accuracy discriminator evaluated binary classiﬁcation accuracy. pdtb-lin setting ﬁrst epochs initialization stage bottom pdtb-ji setting ﬁrst epochs initialization. figure visualizations extracted hidden features implicit relation network i-cnn connective-augmented relation network a-cnn multi-class classiﬁcation setting networks trained without adversary networks trained within framework epoch features epoch implicit relation network successfully imitates connective-augmented features adversarial game. visualization conducted t-sne algorithm expansion relation obtain comparable scores best-performing methods relations respectively. notably feature imitation scheme greatly improves leverages implicit connectives intermediate prediction task. provides additional evidence effectiveness approach. qualitative analysis take closer look modeling behavior framework investigating process adversarial game training well feature imitation effects. figure demonstrates training progress different components. a-cnn network keeps high predictive accuracy implicit connectives given showing importance connective cues. rise-and-fall patterns accuracy discriminator clearly show competition implicit relation network i-cnn training goes. ﬁrst iterations accuracy discriminator increases quickly late stage accuracy drops around showing discriminator getting confused i-cnn i-cnn network keeps improving terms implicit relation classiﬁcation accuracy gradually ﬁtting data simultaneously learning increasingly discriminative features mimicking a-cnn. system exhibits similar learning patterns different settings showing stability training strategy. ﬁnally visualize output feature veci-cnn a-cnn using t-sne tors method figure without feature imitation extracted features networks clearly separated contrast shown figures feature vectors increasingly mixed training proceeds. thus framework successfully driven i-cnn induce similar representations a-cnn even though connectives present. developed adversarial neural framework facilitates implicit relation network extract highly discriminative features mimicking connective-augmented network. method achieved state-of-the-art performance implicit discourse relation classiﬁcation. besides implicit connective examples model naturally exploit enormous explicit connective data improve discourse parsing. feature imitation scheme also generally applicable conincorporate indicative side information text available training time enhanced inference. framework shares similar spirit iterative knowledge distillation method train student network mimic classiﬁcation behavior knowledgeinformed teacher network. approach encourages imitation feature level instead ﬁnal prediction level. allows approach apply regression tasks interestingly context student teacher networks different prediction outputs e.g. performing different tasks transferring knowledge beneﬁcial. besides adversarial mechanism provides adaptive metric measure drive imitation procedure. references biran kathleen mckeown. aggregated word pair features implicit discourse relation disproceedings annual ambiguation. meeting association computational linguistics soﬁa bulgaria pages chlo´e braud pascal denis. comparing word representations implicit discourse relation classiﬁcation. proceedings conference empirical methods natural language processing. lisbon portugal pages learning connective-based word representations implicit discourse relation identiﬁcation. proceedings conference empirical methods natural language processing. austin texas pages jifan chen zhang pengfei xipeng xuanjing huang. implicit discourse relation detection deep architecture gated relevance network. proceedings annual meeting association computational linguistics berlin germany pages chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems. pages xilun chen athiwaratkun kilian weinberger claire cardie. adversarial deep averaging networks cross-lingual sentiment classiﬁcation. arxiv preprint arxiv. yaroslav ganin evgeniya ustinova hana ajakan pascal germain hugo larochelle franc¸ois laviolette mario marchand victor lempitsky. domain-adversarial training neural netjournal machine learning research works. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adadvances neural information versarial nets. processing systems. pages zhiting xuezhe zhengzhong eduard hovy eric xing. harnessing deep proceedings neural networks logic rules. annual meeting association computational linguistics berlin germany pages zhiting zichao yang ruslan salakhutdinov eric xing. deep neural networks massive learned knowledge. proceedings conference empirical methods natural language processing austin usa. yangfeng jacob eisenstein. vector enough entity-augmented distributed semantics discourse relations. transactions association computational linguistics yangfeng gholamreza haffari jacob eisenstein. latent variable recurrent neural network discourse-driven language models. proceedings conference north american chapter association computational linguistics human language technologies diego california pages yangfeng gongbo zhang jacob eisenstein. closing domain adaptation explicit implicit discourse relations. proceedings conference empirical methods natural language processing. lisbon portugal pages alex lamb anirudh goyal ying zhang saizheng zhang aaron courville yoshua bengio. professor forcing algorithm training recurrent networks. advances neural information processing systems. pages zhengyu niu. leveraging synthetic discourse data multi-task learning implicit discourse relation recognition. proceedings annual meeting association computational linguistics soﬁa bulgaria pages junyi jessy marine carpuat nenkova. assessing discourse factors inﬂuence quality machine translation. proceedings annual meeting association computational linguistics baltimore maryland pages yujia kevin swersky richard zemel. progenerative moment matching networks. ceedings international conference machine learning lille france pages maria liakata simon dobnik shyamasree saha colin batchelor dietrich rebholz-schuhmann. discourse-driven content model summarising scientiﬁc articles evaluated complex question answering task. proceedings conference empirical methods natural language processing. seattle washington pages ziheng min-yen hwee recognizing implicit discourse relations penn proceedings discourse treebank. conference empirical methods natural language processing. singapore pages yang sujian recognizing implicit discourse relations repeated reading neural netproceedings works multi-level attention. conference empirical methods natural language processing. austin texas pages tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionaladvances neural information processing ity. systems. pages emily pitler annie louis nenkova. automatic sense prediction implicit discourse relations text. proceedings joint conference annual meeting association computational linguistics international joint conference natural language processing suntec singapore pages lianhui zhisong zhang zhao. implicit discourse relation recognition contextaware character-enhanced embeddings. proceedings coling international conference computational linguistics technical papers. osaka japan pages lianhui zhisong zhang zhao. stacking gated neural architecture implicit discourse relation classiﬁcation. proceedings conference empirical methods natural language processing. austin texas pages nianwen hwee sameer pradhan rashmi prasad christopher bryant attapol rutherford. conll- shared task shallow discourse parsing. proceedings nineteenth conference computational natural language learning shared task beijing china pages attapol rutherford nianwen xue. improving inference implicit discourse relations proclassifying explicit discourse connectives. ceedings conference north american chapter association computational linguistics human language technologies. denver colorado pages nianwen hwee sameer pradhan bonnie webber attapol rutherford chuan wang hongmin wang. conll- shared task shallow discourse parsing. proceedings twentieth conference computational natural language learning shared task berlin germany pages salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems. pages biao zhang deyi xiong jinsong rongrong hong duan zhang. variational neural discourse relation recognizer. proceedings conference empirical methods natural language processing. austin texas pages zhi-min zhou zheng-yu jian chew tan. predicting discourse connectives implicit discourse relation recogproceedings international nition. conference computational linguistics beijing china pages table lists ﬁlter conﬁgurations convolutional layer i-cnn a-cnn different tasks tuned sets. described section paper following convolutional layer pooling layer i-cnn average k-max pooling layer a-cnn. ﬁnal single-layer classiﬁer contains neurons uses tanh activation function. discriminator consists fully-connected layers gated pathways layer layer layer size layer ﬁxed experiments.", "year": 2017}