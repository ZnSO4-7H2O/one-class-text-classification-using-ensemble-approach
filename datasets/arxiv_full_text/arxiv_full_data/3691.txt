{"title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "tag": ["cs.DS", "cs.AI", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.", "text": "existing methods retrieving k-nearest neighbours suffer curse dimensionality. argue caused part inherent deﬁciencies space partitioning underlying strategy used existing methods. devise strategy avoids partitioning vector space present novel randomized algorithm runs time linear dimensionality space sub-linear intrinsic dimensionality size dataset takes space constant dimensionality space linear size dataset. proposed algorithm allows ﬁne-grained control accuracy speed per-query basis automatically adapts variations data density supports dynamic updates dataset easy-toimplement. show appealing theoretical properties demonstrate empirically proposed algorithm outperforms locality-sensitivity hashing terms approximation quality speed space efﬁciency. k-nearest neighbour method commonly used classiﬁer subroutines complex algorithms wide range domains including machine learning computer vision graphics robotics. consequently ﬁnding fast algorithm retrieving nearest neighbours subject sustained interest among artiﬁcial intelligence theoretical computer science communities alike. work past several decades produced rich collection algorithms; however suffer shortcoming ambient intrinsic dimensionality increases running time and/or space complexity grows rapidly; phenomenon often known curse dimensionality. finding solution problem proven elusive conjectured fundamentally impossible rapid growth volume dimensionality data become increasingly important devise fast algorithm scales high dimensional space. argue difﬁculty overcoming curse dimensionality stems part inherent deﬁciencies space partitioning strategy underlies existing algorithms. space partitioning divide-and-conquer strategy partitions vector space ﬁnite number cells keeps track data points cell contains. query time exhaustive search performed data points cells containing query point. strategy forms basis existing algorithms including trees locality-sensitive hashing strategy seems natural sensible suffers critical deﬁciencies dimensionality increases. volume region vector space grows exponentially dimensionality either number size cells must increase exponentially number dimensions tends lead exponential time space complexity dimensionality. addition space partitioning limits algorithm’s ﬁeld view cell containing query point; points adjacent cells hope retrieved. consequently query point falls near cell boundaries algorithm fail retrieve nearby points adjacent cells. since number cells exponential dimensionality intractable search cells dimensionality high. popular approach used spill trees mitigate effect partition space using overlapping cells search points cells contain query point. ambient dimensionality refer original dimensionality space containing data order differentiate intrinsic dimensionality measures density dataset deﬁned precisely later. ratio surface area volume grows dimensionality number overlapping cells must used increases dimensionality; result running time space usage becomes prohibitively expensive dimensionality increases. complications arise variations data density across different regions space. partitioning cells sparse regions space empty query point lies sparse region points retrieved. partitioning coarse cell dense regions space contain many points query point lies dense region many points true nearest neighbours must searched. phenomenon notably exhibited whose performance highly sensitive choice hash function essentially deﬁnes implicit partitioning. good partitioning scheme must therefore depend data; however data-dependent partitioning schemes would require possibly expensive preprocessing prohibit online updates dataset. fundamental limitations space partitioning interesting question possible devise strategy partition space still enables fast retrieval nearest neighbours? paper present strategy retrieving k-nearest neighbours avoids discretizing vector space call dynamic continuous indexing instead partitioning space discrete cells construct continuous indices imposes ordering data points closeness position serves approximate indicator proximity vector space. resulting algorithm runs time linear ambient dimensionality sub-linear intrinsic dimensionality size dataset requiring space constant ambient dimensionality linear size dataset. unlike existing methods algorithm allows ﬁne-grained control accuracy speed query time adapts varying data density on-the-ﬂy permitting dynamic updates dataset. furthermore algorithm easy-to-implement rely complex specialized data structure. extensive work past several decades produced rich collection algorithms fast retrieval k-nearest neighbours. space partitioning forms basis majority algorithms. early approaches store points deterministic tree-based data structures trees r-trees x-trees effectively partition vector space hierarchy half-spaces hyperrectangles voronoi polygons. methods achieve query times logarithmic number data points work well low-dimensional data. unfortunately query times grow exponentially ambient dimensionality number leaves tree need searched increases exponentially ambient dimensionality; result high-dimensional data algorithms become slower exhaustive search. recent methods like spill trees trees virtual spill trees extend approaches randomizing dividing hyperplane node. unfortunately number points leaves increases exponentially intrinsic dimensionality dataset. effort researchers considered relaxing problem allow \u0001approximate solutions contain point whose distance query point differs true nearest neighbours factor treebased methods proposed setting; unfortunately running time still exhibits exponential dependence dimensionality. another popular method locality-sensitive hashing relies hash function implicitly deﬁnes partitioning space. unfortunately struggles datasets varying density cells sparse regions space tend empty cells dense regions tend contain large number points. result fails return point queries requires long time others. motivated development data-dependent hashing schemes based k-means spectral partitioning unfortunately methods support dynamic updates dataset provide correctness guarantees. furthermore incur signiﬁcant pre-processing cost expensive large datasets. data-dependent algorithms outside framework also proposed work constructing hierarchy clusters using k-means viewed performing highly data-dependent form space partitioning. algorithms guarantees approximation quality running time known. class methods rely space partitioning uses local search strategy. starting random data point methods iteratively data point closer query previous data point. unfortunately performance methods deteriorates presence signiﬁcant variations data density since take long navigate dense region space even query. methods like navigating nets cover trees rank cover trees adopt coarse-to-ﬁne strategy. methods work maintaining coarse subsets points varying scales progressively searching neighbourhood query decreasing radii increasingly ﬁner scales. sadly running times methods exhibit exponential dependence intrinsic dimensionality. algorithm algorithm data structure construction require dataset points number simple indices constitute composite index number composite indices function construct proposed algorithm relies construction continuous indices data points support fast searching online updates. onedimensional random projections basic building blocks construct simple indices orders data points projections along random direction. index desired properties data points efﬁciently retrieved updated index implemented using standard data structures storing ordered sequences scalars like self-balancing binary search trees skip lists. ordered arrangement data points exploits important property k-nearest neighbour search problem often overlooked sufﬁces construct index approximately preserves relative order true k-nearest neighbours data points terms distances query point without necessarily preserving pairwise distances. observation enables projection much lower dimensional space johnson-lindenstrauss transform show following section high probability one-dimensional random projection preserves relative order points whose distances query point differ signiﬁcantly realgorithm algorithm k-nearest neighbour retrieval require query point binary search trees/skip lists associated projection vectors {}j∈l∈ maximum tolerable failure probability function query}jl array size entries initialized combine simple indices form composite index points ordered maximum difference simple indices positions point query simple index. composite index enables fast retrieval small number data points referred candidate points close query point along several random directions therefore likely truly close query. composite indices explicitly constructed; instead simply keeps track number constituent simple indices encountered particular point returns point soon constituent simple indices encountered point. query time retrieve candidate points composite index order returned stopping condition satisﬁed omitting points previously retrieved composite indices. exhaustive search performed candidate points retrieved composite indices identify subset points closest query. please refer algorithms precise statement construction querying procedures. figure examples order-preserving order-inverting projection directions. projection direction within shaded region inverts relative order vectors length projection projection directions outside region preserves size shaded region depends ratio lengths vectors. projection vectors whose endpoints shaded region would order-inverting. projection vectors whose endpoints shaded region would invert order long vectors relative short vector. best viewed colour. algorithm able automatically adapt changes data density dynamic updates made dataset without requiring pre-processing estimate data density construction time. also unlike existing methods number retrieved candidate points controlled per-query basis enabling user easily trade accuracy speed. develop versions algorithm data-independent datadependent version differ stopping condition used. former number candidate points indirectly preset according global data density maximum tolerable failure probability; latter number candidate points chosen adaptively query time based local data density neighbourhood query. analyze algorithm show failure probability independent ambient dimensionality running time linear ambient dimensionality sub-linear intrinsic dimensionality size dataset space complexity independent ambient dimensionality linear size dataset. first examine effect projecting d-dimensional vectors dimension motivates proposed algorithm. interested probability distant point appears closer nearby point projection; probability simple index approximately preserves order points distance query point. consider displacement vectors between query point data points probability equivalent probability lengths vectors inverting projection. probability least long projection proof. assuming collinear consider two-dimensional subspace spanned denote vector denote components limit attention analysis. parameterize terms angle relative denote also denote angle relative then lengths would inverted projected along occurs close orthogonal illustrated figure also note probability inverting relative order small much longer hand sphere corresponding toi⊆|i|=ki∈i andvs theorem letvl thatvl probability subset vectors fromvl projection cos−vs ∈i∈]i⊆|i|=ki∈i observe θ|cos θ|cos sides obtain i⊆|i|=ki∈i i⊆|i|=ki∈i i∈]i⊆|i|=ki∈i i⊆|i|=ki∈i ∈i⊆|i|=ki∈i point |bp| |bp| |bp| intuitively represents lower bound increase radius number points within ball doubled. close dataset dense neighbourhood since could many points alequidistant retrieving nearest neighbours considered hard since would difﬁcult tell points true nearest neighbours without computing distances points exactly. probability high similar length corresponds case data points almost equidistant query point. consider sequence vectors ordered length applying random one-dimensional projection likely perturb ordering locally preserve ordering globally. also deﬁne related notion global relative sparsity derive number iterations outer loop querying function executed bound running time independent query deﬁnition dataset global relative sparsity |bp| |bp| |bp| note dataset global relative sparsity local relative sparsity every point. global relative sparsity closely related notion expansion rate introduced speciﬁcally dataset global relative sparsity )-expansion latter quantity known expansion rate. denote expansion rate quantity known intrinsic dimension since dataset uniformly distributed intrinsic dimensionality would match ambient dimensionality. intrinsic dimension dataset global relative sparsity apply results obtained analyze algorithm. consider event algorithm fails return correct k-nearest neighbours occur true k-nearest neighbour contained sl’s entails points true k-nearest neighbours closer query true k-nearest neighbours projections uml. analyze probability occurs derive parameter settings ensure algorithm succeeds high probability. please refer supplementary material proofs following results. lemma relative k−log probability candidate points retrieved given composite index include true k-nearest neighbours constant theorem dataset global relative sparsity k−log algorithm returns correct k-nearest neighbours probability least result suggests choose k−log ensure algorithm succeeds high probability. next analyze time space complexity algorithm. proofs following results found supplementary material. theorem algorithm takes dk−/d time retrieve k-nearest neighbours query time denotes intrinsic dimension dataset. theorem algorithm takes time preprocess data points construction time. theorem algorithm requires time insert data point time delete data point. theorem algorithm requires space addition space used store data. conceptually performance proposed algorithm depends factors likely index returns true nearest neighbours points algorithm stops retrieving points index. preceding sections primarily focused former; section take closer look latter. strategy used data-independent version algorithm stop preset number iterations outer loop. although simple strategy leaves much desired. first order number iterations requires knowledge global relative sparsity dataset rarely known priori. computing either expensive case datasets infeasible case streaming data global relative sparsity change data points arrive. importantly unable take advantage local relative sparsity neighbourhood query. method capable adapting local relative sparsity could potentially much faster query points tend close manifold points dataset resulting dataset sparse neighbourhood query point. ideally algorithm stop soon retrieved true nearest neighbours. determining case amounts asking exists point seen lying closer query points seen. ﬁrst sight nothing known unseen points seems possible better exhaustive search rule existence point computing distances unseen points. somewhat surprisingly exploiting fact projections associated index random possible make inferences points never seen. leveraging ideas statistical hypothesis testing. iteration outer loop perform hypothesis test null hypothesis complete k-nearest neighbours retrieved. rejecting null hypothesis implies accepting alternative hypothesis true k-nearest neighbours retrieved. point algorithm safely terminate guaranteeing probability algorithm fails return correct results bounded signiﬁcance level. test statistic upper bound probability missing true k-nearest neighbour. resulting algorithm require prior knowledge dataset terminates earlier dataset sparse neighbourhood query; reason refer version algorithm data-dependent version. concretely algorithm retrieves candidate points computes true distances query maintains list points closest query among points retrieved composite indices far. ˜pmax denote closest candidate point retrieved composite indices farthest candidate point retrieved composite index respectively. number candidate points exceeds algorithm checks show correctness running time algorithm below. proofs following results found supplementary material. theorem data-dependent algorithm returns correct k-nearest neighbours query probability least theorem ative metric interest close points returned algorithm query rather whether returned points true k-nearest neighbours empirically evaluate performance setting. distance metric interest euclidean distance compare exact euclidean uses hash functions designed euclidean space. compare performance proposed algorithm cifar- mnist datasets consist colour images various real-world objects grayscale images handwritten digits respectively. reshape images vectors dimension representing pixel intensity particular location colour channel image. resulting vectors dimensionality case cifar- case mnist dimensionality consideration higher traditional tree-based algorithms handle. combine training test dataset total instances cifar- instances mnist. randomize instances serve queries using cross-validation. speciﬁcally randomly select instances serve query points designate remaining instances data points. algorithm used retrieve approximate k-nearest neighbours query point among data points. procedure repeated folds different split query data points. compare number candidate points algorithm requires achieve desired level approximation quality. quantify approximation quality using approximation ratio deﬁned ratio radius ball containing approximate k-nearest neighbours radius ball containing true k-nearest neighbours. smaller approximation ratio better approximation quality. dimensionality high exhaustive search must performed candidate points time taken compute distances candidate points query dominates overall running time querying operation. therefore number candidate points viewed implementationindependent proxy running time. hash table constructed depends desired level approximation quality construct different hash table level approximation quality. hand indices constructed proposed method speciﬁc particular level approximation quality; instead approximation quality controlled query time varying number iterations outer loop. indices used levels approximation quality. therefore evaluation scheme figure comparison speed proposed method cifar- mnist. curve represents mean number candidate points required achieve varying levels approximation quality folds shaded area represents standard deviation. comparison space efﬁciency proposed method cifar- mnist. height represents average amount memory used method achieve performance shown adopt recommended guidelines choosing parameters used hashes table tables. proposed method used cifar- mnist found work well practice. figures plot performance proposed method cifar- mnist datasets retrieving nearest neighbours. purposes retrieving nearest neighbours mnist challenging dataset cifar-. because instances mnist form dense clusters whereas instances cifar- visually diverse dispersed space. intuitively query falls inside dense cluster points many points close query difﬁcult distinguish true nearest neighbours points slightly farther away. viewed differently true nearest neighbours tend extremely close query mnist denominator computing approximation ratio usually small. consequently returning points slightly farther away true nearest neighbours would result large approximation ratio. result proposed method require candidate points mnist cifar- achieve comparable approximation ratios. proposed method achieves better performance levels approximation quality. notably performance degrades drastically mnist surprising since known difﬁculties datasets large variations data density. hand proposed method requires fewer candidate points achieve appaper delineated inherent deﬁciencies space partitioning presented strategy fast retrieval k-nearest neighbours dynamic continuous indexing instead discretizing vector space proposed algorithm constructs continuous indices imposes ordering data points closeby positions approximately reﬂect proximity vector space. unlike existing methods proposed algorithm allows granular control accuracy speed per-query basis adapts variations data density on-the-ﬂy supports online updates dataset. analyzed proposed algorithm showed runs time linear ambient dimensionality sub-linear intrinsic dimensionality size dataset takes space constant ambient dimensionality linear size dataset. furthermore demonstrated empirically proposed algorithm compares favourably terms approximation quality speed space efﬁciency. acknowledgements. thanks berkeley vision learning center natural sciences engineering research council canada ﬁnancial support. authors also thank anonymous reviewers feedback. arya sunil mount david netanyahu nathan silverman ruth angela optimal algorithm approximate nearest neighbor searching ﬁxed dimensions. journal berchtold stefan ertl bernhard keim daniel kriegel seidl thomas. fast nearest neighbor search high-dimensional space. data engineering proceedings. international conference ieee datar mayur immorlica nicole indyk piotr mirrokni vahab locality-sensitive hashing scheme based p-stable distributions. proceedings twentieth annual symposium computational geometry houle michael nett michael. rank-based similarity search reducing dimensional dependence. pattern analysis machine intelligence ieee transactions indyk piotr motwani rajeev. approximate nearest neighbors towards removing curse dimensionality. proceedings thirtieth annual symposium theory computing krauthgamer robert james navigating nets simple algorithms proximity search. proceedings ﬁfteenth annual acm-siam symposium discrete algorithms society industrial applied mathematics ting moore andrew yang gray alexander investigation practical approximate nearest neighbor algorithms. advances neural information processing systems nister david stewenius henrik. scalable recognition vocabulary tree. computer vision pattern recognition ieee computer society conference volume ieee paulev´e lo¨ıc j´egou herv´e amsaleg laurent. locality sensitive hashing comparison hash function types querying mechanisms. pattern recognition letters wang xueyi. fast exact k-nearest neighbors algorithm high dimensional search using k-means clustering neural networks triangle inequality. international joint conference ieee below present proofs results shown paper. ﬁrst prove intermediate results used derive results paper. throughout proofs {p}n closest point query given projection direction associated simple index disalso consider ranking points {pi}n tance projection nondecreasing order. points ranked others appear earlier ranking. lemma probability simple indices composite index points exist bours ranked them proof. given simple index refer points true k-nearest neighbours ranked extraneous points. furthermore categorize extraneous points either reasonable silly. extraneous point reasonable k-nearest neighbours silly otherwise. since reasonable extraneous points must least silly extraneous points. therefore event extraneous points exist must contained event silly extraneous points exist. probability silly extraneous points exists given simple index. theorem take i=k+ probability least silly extraneous points i=k+ p−q. lemma dataset global relative sparconstituent sity simple indices composite index fewer true k-nearest neighpoints exist bours ranked them least proof. refer points ranked positions true k-nearest neighbours true positives false positives. additionally refer points ranked positions true k-nearest neighbours false negatives. true k-nearest neighbours positions must least false negative. since true positives must least false positives. since false positives true k-nearest neighbours ranked false negative true knearest neighbour apply lemma taking obtain lower bound probability existence fewer false positives constituent simple indices composite index simple index fewer false positives positions must contain true k-nearest neighbours. since true constituent simple indices true k-nearest neighbours must among candidate points iterations outer loop. failure probability therefore proof. lemma ﬁrst points retrieved given composite index include true knearest neighbours probability algorithm fail must occur composite indices. since composite index constructed independently algorithm fails probability must succeed probability least since makes theorem algorithm takes dk−/d time retrieve k-nearest neighbours query time denotes intrinsic dimension dataset. proof. computing point since along ujl’s takes time searching binary search trees/skip lists tjl’s takes time. total number candidate points retrieved k−log γ)). computing distance candidate point query point takes dk−log time. closest points candidate points using selection algorithm like quickselect takes k−log since time taken compute time average. distances query point dominates entire algorithm takes dk−log time. since rewritten dk−/d proof. computing projections points along ujl’s takes time since constants. inserting points self-balancing binary search trees/skip lists takes time. proof. order insert data point need compute projection along ujl’s insert binary search tree skip list. computing projection takes time inserting entry self-balancing binary search tree skip list takes time. order delete data point simply remove binary search trees skip lists takes time. proof. additional information needs stored binary search trees skip lists. since entries stored binary search tree/skip list additional space required proof. analyze probability algorithm fails return correct k-nearest neighbours. denote true k-nearest neighbour missed. algorithm fails given composite index given composite index ﬁnishing fetching true k-nearest neighbours. since number composite indices constant total number candidate points retrieved composite indices retrieving k-nearest neighbours stopping condition satisﬁed algorithm would continue retrieving points. analyze number additional points algorithm retrieves terminates. terms number candidate points retrieved true k-nearest neighbours far. suppose algorithm already retrieved candidate points retrieve candidate point. since candidate point must different makes former inequal\u0001. true stopping condition would satisﬁed algorithm must terminated point earlier. rearranging former inequality order hold must least therefore number among candidate points retrieved said index. words composite index must returned points implying least constituent simple index returns points means points must appear closer projection associated simple displacement vectors candidate points farther displacement vector probability occurring given constituent simple index composite index probability occurs constituent simple inproof. order bound running time bound total number candidate points retrieved stopping condition satisﬁed. divide execution algorithm stages analyze algorithm’s behaviour ﬁnishes retrieving true k-nearest neighbours. ﬁrst bound number candidate points algorithm retrieves before ﬁnding complete k-nearest neighbours. lemma exist fewer points k-nearest neighbours ranked constituent simple indices given composite index least", "year": 2015}