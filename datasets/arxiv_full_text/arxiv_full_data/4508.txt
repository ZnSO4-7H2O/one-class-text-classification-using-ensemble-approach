{"title": "Loop corrections for message passing algorithms in continuous variable  models", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model. Using the exactness of the averages for belief propagation for Gaussian models, a different way of obtaining the covariances is found, based on Belief Propagation on cavity graphs. We discuss the relation of this loop correction algorithm to Expectation Propagation algorithms for the case in which the model is no longer Gaussian, but slightly perturbed by nonlinear terms.", "text": "paper derive equations loop corrected belief propagation continuous variable gaussian model. using exactness averages belief propagation gaussian models diﬀerent obtaining covariances found based belief propagation cavity graphs. discuss relation loop correction algorithm expectation propagation algorithms case model longer gaussian slightly perturbed nonlinear terms. message passing techniques graphical models allow computation marginal probabilities time interval scaling polynomially model size. discovery consequently revolutionized several ﬁelds applications past years error correcting codes vision probably prominent examples. many cases corresponding graphs loopy implying either error resulting application loopy belief propagation negligible particular model tolerated particular purpose serves. cases sophisticated reﬁnements necessary taking account loop errors. finding optimal treatment loop errors motivates active ﬁeld research diﬀerent solutions applying diﬀerent model classes developed. models involving many short loops like regular lattices type approaches work well tree approaches latter also applied correct incidental large loop. unifying frameworks like region graphs lead general strategies selecting basic clusters underlying approaches general model classes. recent analysis shown local update equations interpreted zero order term expansion cavity connected correlations. quantities parameterizations cavity distributions i.e. distribution neighbor variables central variable removed graph. bethe approximation recovered cavity distribution assumed factorize whereas ﬁrst order correction local update equations obtained takes account pair cumulants estimation pair cumulants possible extra runs allowing polynomial time algorithms reducing errors order applying algorithms running time scales extra factor although scaling seems heavy large beneﬁt approach require selection basic clusters underlying tree-structures since takes account eﬀect loops contribute nontrivial correlations cavity distribution once. loop correction strategy applicable class models perturbative expansion around bethe approximation makes sense i.e. models large loops relatively weak interactions. principal requirement magnitude pair variable cumulants cavity distributions order smaller single variable cumulants third order cumulants even smaller etc. however heuristics based strategy allow good algorithms performing well outside parameter regimes approach developed discrete variable models abstract versus practical level paper apply idea graphical models continuous variables. derive loop corrected belief propagation equations simple tractable gaussian models yielding message passing scheme that besides correct average marginals also yields correct variances. besides discuss approaches potentially applicable cases extra function approximations necessary relation expectation propagation. by-product loop corrected belief propagation equations algorithm calculates exact covariance matrices gaussian models like discussed without explicitly using linear response. viewpoints argued meaning nevertheless lead diﬀerent strategies optimization approximation improvement results. second viewpoint starting point algorithms like expectation propagation ﬁrst seen basic view loop correction strategies. since expectation propagation applies well continuous variable cases loop correction schemes sense applied continuous variable cases might instructive derive corresponding equations compare expectation propagation approaches. motivation mind ﬁrstly analyze loop correction scheme gaussian models. given scheme discuss possible generalizations suitable cases model longer tractable. thus variables gaussian local potential average variance interact pairwise manner variables interaction jij. obviously ψjk. denote neighborhood variable following analysis based loop correction equations applied discrete binary variables. current generalization continuous variables straightforward application ideas. write expression joint probability variables model interaction removed diﬀerent ways. ﬁrst terms cavity distribution variable i.e. joint distribution neighbors model removed second terms cavity distribution variable measures reduce functions mentioned cavity distributions unknown functions interest. clear however speciﬁed enough local equations solve full cavity distributions restrict ourselves moment gaussian models able perform integrations exact local message passing equations. note general type model local computations insuﬃcient used basis approximation appropriate approximating functions chosen characterized ﬁnite parameters. notice belief propagation recovered chooses approximate cavity parameterization includes exact result graph tree since nontrivial correlations variables cavity absent. loops graph corrections parameterization desirable. various parameterizations corrections possible principle suggested expand cavity distributions cumulants expansion appropriate either interactions weak loops long. gaussian model cavity distributions completely speciﬁed averages covariances including second order cumulants yields exact equations. following investigate structure corresponding equations identify exact correction gaussian belief propagation. appropriate parameterization cavity distribution decomposed covariance matrix diagonal part oﬀ-diagonal part dimensions cavity set. bethe approximation cavity distributions factorize corresponds neglecting oﬀ-diagonal components matrices vectors found consistency equations. following denote vector consistency equations gaussian integration cavity distribution number pairs equations equal number variables cavity set. thus given covariance matrix diagonals determined second equation subsequently average values determined ﬁrst equation. marginal distributions follow directly since variables known. substituting using response propagation possible estimate covariances leads improvement results small binary case gaussian case results response propagation exact procedure thus yield exact results provided response propagation belief propagation converge. equations allow explicitly interpret meaning belief propagation messages write expressions error. indeed messages equation represent averages variances cavity distributions i.e. model absence variable. interesting side result respect comes observation averages calculated belief propagation exact algorithm converges. follows message calculated equation equal graph variable removed calculated ordinary belief propagation. next section observation together similar arguments obtain exact results ordinary belief propagation variables alone. form loop corrected belief propagation equations imposes relation errors messages gaussian models. comparing result equations without cavity covariances show i.e. average variable graph without obtained running graph without variable thus running original graph running graph without calculate using equation writing equations suggest inverting matrices calculating correlation matrices growing graphs might useful application. subsequently attaching variables graph running ﬁnds full correlation matrix runs procedure described cost runs halved since graph growing along runs. however overlook fact equations introduce large number additions multiplications total computational complexity inverting sparse matrix similar well-known methods. fact loop corrections form able correct total error variances course gaussian nature model. discrete models exact parameterizations full distribution local marginals general possible loop corrections able increase accuracy bethe approximation. thus formalism might seem promising basis extensions models exactly tractable possibly alternative related algorithms like expectation propagation since viewed special case hope generalizations loop corrections equations relation applicable cases function approximations become necessary. however speciﬁc form equations much depends choice approximating family functions chooses. equivalence corresponds family approximate functions fully factorizes variables model loop corrected strategies expect relationship approaches based larger local neighborhoods. investigate relation deriving equations models general nonlinear single-variable potentials i.e. ψie−vi might expect vision problems nonlinear observation functions. gaussian loop corrections approach seems rather similar approach includes full gaussian approximating target distribution. standard formalism approach choose approximate distribution subscript stands gaussian represents gaussian contribution full joint probability. remaining terms relate approximations single node potentials following manner thus costly computations inversion equation one-dimensional integrations large models inversions become prohibitive otherwise scheme seems eﬃcient since cavity covariances computed separately implicitly present inversions optimal respect minimization kl-divergence. subsection discuss possible generalization loop correction scheme model discussed previous subsection. model nonlinear single-variable potentials tackled starting loop correction scheme beginning paper slightly generalizing case ψie−vi. formalism beginning paper still applied gaussian parameterization distributions approximation. given estimates covariance matrices hand easy check equations reduce equations loop correction i.e. case equivalent full gaussian approach previous subsection well since treatments exact limit. hand take updates somewhat similar completely factorizing gaussian approximate target distribution slight diﬀerence fact propagated expectations parameterize approximate cavity distributions actual target marginal distributions. thus kl-divergence approximate factorizing cavity distribution minimized approximate target distribution. diﬀerence vanishes algorithm reduces factorizing gaussian approximate joint distribution which limit equivalent ordinary optimizing approximations marginal moments objective algorithm approach subsection obviously optimal since instead moments cavity distributions optimized integrals calculate messages. inspired observations regarding optimization marginal moments target approximation derive alternative consistency equations starting expressions actual marginals integrations include full sets neighboring factors. again approximate cavity distributions gaussians reduces fully factorizing gaussian leads equations equivalent suitable choice make ﬁxed points equations equivalent full-gaussian equations beginning section since approaches optimize marginal moments variable given gaussian interaction matrix rest model. however beneﬁt full gaussian gaussian interaction matrix optimized albeit cost inversion iteration loop corrected approach desires estimate input updated. formalism approximation cavity covariance matrix obtained applying linear response algorithm graph variable removed. entries variables follow reasoning estimate covariance matrix zeroth order enables ﬁrst order corrected version expectation propagation algorithm equations note that given values single node variances obtained running including integrations graph without variable fast algorithm responses involve integrations. thus cost cavity covariance matrix determined factorizing updates graph without central variable responses quickly obtained. estimating obviously costly number inversions matrices dimension full gaussian approach inversion necessary update present approach might become cheaper current model seems practicle. possibly however complex models loop correction approach beneﬁcial topic investigated. paper derived loop corrected belief propagation equations continuous variable models. particular worked exactly tractable case gaussian models derived exact message passing equations. role connected correlations cavity distribution discussed taken oﬀ-diagonal parts covariance matrix obtained preprocessing algorithms. moreover using fact ordinary converges produces exact marginal averages various relations messages obtained leading alternative update schemes invert covariance matrix. models involving nonlinear terms approximation algorithms needed order compute marginals discussed relations expectation propagation approaches loop correction strategies particular model class nonlinear potentials involve single variables. loop correction approaches continuous variable models become attractive alternative strategies like expectation propagation grow costly. hand however heavily penalized cost preprocessing stage necessary estimate cavity connected correlations.", "year": 2007}