{"title": "Efficient Methods for Unsupervised Learning of Probabilistic Models", "tag": ["cs.LG", "cs.AI", "cs.IT", "cs.NE", "math.IT", "physics.data-an"], "abstract": "In this thesis I develop a variety of techniques to train, evaluate, and sample from intractable and high dimensional probabilistic models. Abstract exceeds arXiv space limitations -- see PDF.", "text": "high dimensional probabilistic models used many modern scientiﬁc engineering data analysis tasks. interpreting neural spike trains compressing video identifying features microarrays recognizing particles high energy physics rely upon ability model complex structure high dimensional space. despite great promise high dimensional probabilistic models frequently computationally intractable work practice. thesis develop solutions overcome intractability primarily context energy based models. common cause intractability model distributions cannot analytically normalized. probabilities computed constant making training exceedingly diﬃcult. solve problem propose ‘minimum probability learning’ variational technique parameter estimation models. utility training technique demonstrated case ising model hopﬁeld auto-associative memory independent component analysis model natural images deep belief network. second common diﬃculty training probabilistic models arises parameter space ill-conditioned. makes gradient descent optimization slow impractical alleviated using natural gradient. show natural gradient related signal whitening provide speciﬁc prescriptions applying learning problems. also diﬃcult evaluate performance models cannot analytically normalized providing particular challenge hypothesis testing model comparison. overcome this introduce method termed ‘hamiltonian annealed importance sampling’ eﬃciently estimates normalization constant non-analytically-normalizable models. method used calculate compare likelihoods several state probabilistic models natural image patches. finally many tasks performed trained probabilistic model involve generating samples model distribution typically computationally expensive process. introduce modiﬁcation hamiltonian monte carlo sampling reduces tendency sampling trajectories double back themselves enables statistically independent samples generated rapidly. thank advisor bruno olshausen countless thoughtful inspiring conversations giving freedom pursue interests; mentor mike deweese long nights working innumerable helpful conversations; tony bell identifying interesting questions; fritz sommer many interesting conversations supply reading material; jack culpepper peter battaglino charles cadieu jimmy wang chris hillar kilian koepsell amir khosrowshahi koester pierre garrigues rest redwood center diverse esoteric interests many many fascinating conversations useful feedback. scientists engineers increasingly confront large complex data sets defy traditional modeling analysis techniques. example ﬁtting well-established probabilistic models physics population neural activity recorded retina cortex currently impractical populations neurons similar diﬃculties occur many ﬁelds including computer science genomics physics thus development techniques train evaluate sample complex probabilistic models fundamental importance many scientiﬁc engineering disciplines. thesis identiﬁes addresses number important diﬃculties encountered working models. focus energy-based models cannot normalized closed form posing unique challenges learning. begin chapter review probabilistic models current state parameter estimation methods techniques estimating intractable normalization constants. chapter section summary contributions made thesis improve ability evaluate train work challenging probabilistic models. section provide formalism writing data model distributions introduce canonical kullback-leibler divergence objective parameter estimation present number relevant parameter estimation techniques. figure data model distributions viewed vectors dimensionality equal number possible states value entry equal probability corresponding state. illustrates data distribution. case equal number observations states observations states states probability illustrates model distribution parameterized probability assigned state determined energy assigned state normalization constant data distribution represented vector state superscript represents time system dynamics example variable binary system would four entries representing fraction data states unfortunately involves evaluating includes system states equation similar integral case continuous state space. intractable systems reasonable size instance involving terms binary system. reason exact parameter estimation frequently impractical. many approaches exist approximate parameter estimation including mean ﬁeld theory expansions variational bayes techniques variety sampling numerical integration based methods approaches closely relate techniques introduced thesis include contrastive divergence developed hinton welling carreira-perpi˜n´an hyv¨arinen’s score matching besag’s pseudolikelihood lyu’s minimum contraction minimum velocity learning framework proposed movellan contrastive divergence variation steepest gradient descent maximum likelihood objective function. rather integrating full model distribution approximates partition function term gradient averaging distribution obtained taking markov chain monte carlo steps away data distribution frequently abbreviated cd-k number mcmc steps taken away data distribution. qualitatively imagine data distribution contrasted distribution evolved small distance towards model distribution whereas would contrasted true model distribution traditional mcmc approaches. although guaranteed converge right answer even ﬁxed point proven eﬀective fast heuristic parameter estimation cd-k update rule written score matching method learns parameters probabilistic model continuous state space using derivatives energy function evaluated data distribution. sidesteps need explicitly sample integrate model distribution. score matching minimizes expected square distance score function respect spatial coordinates given data distribution similar score function given model distribution. score function gradient likelihood. number connections made score matching learning techniques score matching objective function written pseudolikelihood approximates joint probability distribution collection random variables computationally tractable product conditional distributions factor distribution single random variable conditioned others. approach often leads surprisingly good parameter estimates despite extreme nature approximation. recent work suggests pseudolikelihood consistent estimator model parameters meaning data distribution form model distribution limit inﬁnite data exact correct distribution recovered. pseudolikelihood objective function written probability distribution dimension state space conditioned remaining dimensions. clarity written pseudolikelihood objective function continuous state space deﬁned continuous discrete state spaces. minimum velocity learning approach recently proposed movellan recasts number ideas behind treating minimization initial dynamics away data distribution goal rather surrogate rather directly minimize diﬀerence data model movellan’s proposal introduce system dynamics model equilibrium distribution minimize initial probability away data dynamics. model looks exactly like data probability model data similar probability tend minimal. movellan applies intuition speciﬁc case distributions continuous state spaces evolving diﬀusion dynamics recovers score matching objective function velocity minimum velocity learning diﬀerence average drift velocities particles diﬀusing model distribution particles diﬀusing data distribution. minimum contraction involves applying special class mapping data model distributions minimizing amount mapping shrinks divergence data model distributions. divergence data model distributions becomes similar less room contraction mapping shrink contraction objective becomes smaller. like minimum probability minimum contraction appears generalization number existing parameter estimation techniques based local information model distribution. generating samples probability distributions high dimensional state spaces frequently extremely expensive. hamiltonian monte carlo family techniques fast sampling continuous state spaces work extending state space include auxiliary momentum variables simulating hamiltonian dynamics physics order traverse long iso-probability trajectories rapidly explore state space. sampling alternates drawing momentum marginal distribution simulating hamiltonian dynamics joint system described hamiltonian dynamics described diﬀerential equations hamiltonian dynamics conserve total energy thus joint probability preserve volume joint space samples proposed integrating equations accepted probability also traversed large distance previous sample. thus allows independent samples rapidly drawn factorial samples recovered discarding variables taking marginal distribution additional issues must addressed implementation involve choosing numerical integration scheme dynamics correctly accounting discretization errors. annealed importance sampling sequential monte carlo method allows partition function non-analytically-normalizable distribution estimated unbiased fashion. accomplished starting distribution known normalization gradually transforming distribution interest chain markov transitions. practicality depends heavily chosen markov transitions. review derivations importance sampling annealed importance sampling. extension annealed importance sampling better incorporate hamiltonian monte carlo presented chapter evaluate exactly must support everywhere does. unfortunately unless signiﬁcant mass everywhere does takes impractically large number samples accurately approximate annealed importance sampling extends state space series vectors transforms proposal distribution distribution setting distribution multiplying series markov transition distributions thesis attempt solve several problems arise probabilistic modeling. began chapter reviewing existing techniques working intractable probabilistic models. signiﬁcant problems working probabilistic models majority cannot analytically normalized. therefore probabilities assign states cannot exactly computed expensive even approximate. training model intractable normalization constant extremely diﬃcult using traditional methods based sampling. present alternative technique parameter estimation models minimum probability flow chapter direct mapping annealed importance sampling jarzynski equality nonequilibrium thermodynamics follows mapping reversibility quasistatic processes variance made transition suﬃciently gradual. quality ising model application storing memories hopﬁeld autoassociative memory evaluation estimation quality independent component analysis model deep belief network diﬃculties training probabilistic models stem conditioning model’s parameter space well inability analytically normalize model. chapter review conditioned parameter space undermine learning present novel interpretation natural gradient common technique dealing conditioning. addition present tricks speciﬁc prescriptions applying natural gradient learning problems. even probabilistic model trained remains diﬃcult objectively judge compare performance models unless normalized. address this chapter hamiltonian annealed importance sampling presented. method used eﬃcient likelihood estimation combines hamiltonian monte carlo annealed importance sampling applied compare likelihoods several state probabilistic models natural image patches. finally many tasks commonly performed probabilistic models instance image denoising inpainting speech recognition require samples model distribution. generating samples high computational cost often making bottleneck machine learning task. chapter extension sampling introduced reduces frequency sampling trajectories double back themselves thus enables statistically independent samples generated rapidly. additional research involving high dimensional probabilistic models incorporated thesis includes developing multilinear generative models natural scenes training groups describe transformations occur natural video exploring statistical structure scans breast tissue applying super-resolution algorithm images mars exploration rover panoramic camera photometric modeling martian dust modeling camera systems mars exploration rover discussed chapter probabilistic learning techniques require calculating normalization factor partition function probabilistic model question least calculating gradient. overwhelming majority models known analytic solutions calculation intractable. chapter present technique parameter estimation probabilistic models even cases normalization factor cannot calculated. material chapter taken list observations state system. introducing deterministic dynamics guarantee transformation data distribution model distribution minimizing divergence data distribution distribution results running dynamics short time formalism used introduced section monte-carlo algorithms rely core concepts statistical physics ﬁrst conservation probability enforced master equation time evolution distribution probability ﬂows state state ﬁrst term equation captures probability states state second captures states dependence results requirement chosen figure illustration parameter estimation using minimum probability panel axes represent space probability distributions. three successive panels illustrate sequence parameter updates occur learning. dashed curves indicate family model distributions parametrized black curves indicate deterministic dynamics transform data distribution model distribution maximum likelihood learning model parameters chosen minimize kullback–leibler divergence data distribution model distribution however divergence minimized instead distribution obtained initializing dynamics data distribution evolving inﬁnitesimal time represent graphically parameter updates pull towards also tend pull towards figure dynamics minimum probability learning. model dynamics represented probability matrix determine probability ﬂows empirical histogram sample data points equilibrium distribution model suﬃciently long time. example four possible states system consists pair binary variables particular model parameters favor state whereas data falls states. form chosen equation coupled satisfaction detailed balance ergodicity introduced section guarantees unique eigenvector eigenvalue zero eigenvalues real negative. states equilibrium probability state state equals probability satisﬁed detailed balance guarantees distribution ﬁxed point dynamics. sampling monte carlo methods performed choosing consistent equation stochastically running dynamics equation note need restrict dynamics deﬁned real physical process diﬀusion. underconstrained equation. introducing additional constraint invariant addition constant energy function choose following form non-diagonal entries determines states allowed directly exchange probability other. non-zero also sampled proposal distribution rather deterministic scheme case takes role proposal distribution section extremely sparse theoretically guarantee convergence model distribution non-zero elements must chosen that given suﬃcient time probability pair states uniquely zero equal. implies consistency data comes model class limit inﬁnite data minimized exactly true addition convex models exponential family models whose energy functions linear parameters objective additionally provides upper qualitatively informative states connect data states probable model. discrete state spaces nearest neighbor connectivity schemes work extremely well because learning converges states near data states become states probable model. although motivated technique using systems large ﬁnite number states generalizes continuous state spaces. become continuous functions populated stochastically extremely sparsely preserving cost. continuous state spaces estimated parameters much sensitive choice practically implemented continuous state spaces using persistent particle extensions section hamiltonian monte carlo sample connected states. probability transitioning state state single markov chain monte carlo step equation obvious similarities learning gradient equation thus steepest gradient descent resembles updates mcmc sampling/rejection step replaced weighting factor note diﬀerence form provides well-deﬁned objective function. important consequence existence objective function readily utilize general purpose oﬀ-the-shelf optimization packages gradient descent would tailored applied part accounts dramatic diﬀerence learning time cases objective function. full derivation presented appendix unlike applicable parametric model including discrete systems require evaluating third order derivative result unwieldy expressions. learning scheme blind regions state space directly connected data states. ﬂexibly thoroughly connect states treat probability connection state state rather binary indicator function. case constraints required probability distribution seen substitution form equation still satisﬁes detailed balance. additional motivations form linear factor approximated using samples contribution included linear factor function solely ratio normalization term cancels out. recent work shown persistent particle techniques outperform sample driven learning techniques. direct analogy persistent contrastive divergence nearest neighbor schemes setting connectivity function work nearly well continuous state spaces discrete state spaces persistent works quite well continuous state spaces pmpf particularly applicable continuous state space case. order modify work persistent samples ﬁrst take advantage restricted form rewrite objective function. proposed connectivity function depends destination state initial state nested sums equation factored apart. case depend write simply objective function becomes informative states connect learning probable model distribution. therefore useful learning make similar possible. eﬀective learning procedure alternates updating resemble current estimate model distribution model parameters using samples ﬁxed connectivity function deﬁning sequence estimated parameter vectors proposed connectivity distributions indicates learning iteration learning procedure becomes similar proposal distribution step gn−. signiﬁcant time thus saved generating samples initializing samples taking small number sampling steps. full procedure persistent using pmpf -dimensional continuous states space parameter estimation procedure given steps below. list samples learning step number samples typically number observations |d|. presented novel general purpose framework called minimum probability learning parameter estimation probabilistic models outperforms current techniques learning time accuracy. works parametric model withhidden state variables including continuous discrete state space systems avoids explicit calculation partition function employing deterministic dynamics place slow sampling required many existing approaches. provides simple well-deﬁned objective function minimized quickly using existing higher order gradient descent techniques. furthermore objective function convex models exponential family ensuring global minimum found gradient descent cases. inspired minimum velocity approach developed movellan reduces technique well score matching forms contrastive divergence special cases dynamics. chapter demonstrate experimentally eﬀectiveness minimum probability flow learning technique presented chapter matlab code implementing several cases presented chapter available unless stated otherwise minimization performed using l-bfgs implementation minfunc material chapter taken ising model long storied history physics machine learning recently found surprisingly useful model networks neurons retina ability ising models activity large groups simultaneously recorded neurons current interest given increasing number types data sets retina cortex brain structures. data consisted d-element binary samples generated swendsenwang sampling spin glass known coupling parameters. used square lattice non-diagonal nearest-neighbor elements using draws normal distribution variance diagonal elements column summed expected unit activations transition matrix elements figure shows mean square error estimated mean square error corresponding pairwise correlations function learning time four competing approaches mean ﬁeld theory corrections sampling steps iteration pseudolikelihood. parameter estimation minimum probability flow pseudolikelihood performed applying shelf l-bfgs implementation objective functions evaluated full training dataset trained stochastic gradient descent using minibatches size learning rate annealed linear fashion accelerate convergence. mean ﬁeld theory requires computation inverse magnetic susceptibility matrix which strong correlations often singular. regularized pseudoinverse used following manner technique known stochastic robust approximation using learning took approximately seconds compared roughly seconds pseudolikelihood upwards seconds -step -step note given suﬃcient training samples would converge exactly right answer learning ising model convex global minimum true solution. table shows relative performance convergence terms mean square error recovered weights mean square error resulting model’s correlation function convergence time. dramatically faster converge models tested exception mft+tap failed reasonable parameters. model data substantially better models. figure demonstration minimum probability flow outperforming existing techniques parameter recovery ising spin glass. time evolution mean square error coupling strengths methods ﬁrst seconds learning. note mean ﬁeld theory second order corrections actually increases error random parameter assignments case. mean square error coupling strengths ﬁrst seconds learning. mean square error coupling strengths entire learning period. mean square error pairwise correlations ﬁrst seconds learning ﬁrst seconds learning entire learning period respectively. every comparison ﬁnds better cases mft+tap shorter time table mean square error recovered coupling strengths mean square error pairwise correlations learning time versus mean ﬁeld theory correction -step -step contrastive divergence pseudolikelihood order allow additional comparison earlier work recovered coupling parameters unit fully connected ising model used paper faster solutions inverse pairwise ising problem figure shows average error predicted correlations function learning time samples. ﬁnal absolute correlation error used graciously provided broderick coauthors identical used synthetic data generation paper training performed samples match number samples used section iii.a. broderick core intel xeon learning converges seconds. broderick perform similar learning task -cpu grid computing cluster convergence time approximately seconds. demonstration learning complex discrete valued model trained layer deep belief network mnist handwritten digits. consists stacked restricted boltzmann machines hidden layer forms visible layer next. form figure demonstration rapid ﬁtting fully connected ising model minimum probability learning. mean absolute error learned model’s correlation matrix shown functions learning time unit fully connected ising model. convergence reached seconds samples. sampling-free application requires analytically marginalizing hidden units. rbms trained sequence starting bottom layer samples mnist postal hand written digits data set. ising case transition matrix populated connect every state states diﬀered single full derivation objective case found appendix training performed single step figure deep belief network trained using minimum probability learning four layer deep belief network trained mnist postal hand written digits dataset single step contrastive divergence samples deep belief network training mpf. reasonable probabilistic model handwritten digits learned. samples training uneven distribution digit occurrences suggests learned less representative model mpf. whitened natural image patches hateren database since likelihood gradient calculated analytically solved maximum likelihood learning compared resulting likelihoods. training techniques initialized identical gaussian noise trained data accounts similarity individual receptive ﬁelds found algorithms. average likelihood training techniques shown figure parameter estimation performed using persistent algorithm described section using hamiltonian monte carlo sample connectivity function motivated ising spin glass model statistical physics hopﬁeld introduced auto-associative neural-network storage retrieval binary patterns even today model various extensions provide plausible mechanism memory formation brain. however existing techniques training hopﬁeld networks suﬀer either limited pattern capacity excessive training time exhibit poor performance trained unlabeled corrupted memories. section show provides tractable neurally-plausible algorithm optimal storage patterns hopﬁeld network provide proof capacity network least pattern neuron. compared standard techniques hopﬁeld pattern storage shown superior eﬃciency generalization. another ﬁnding store many patterns hopﬁeld network highly corrupted samples them. discovery also corroborated background hopﬁeld network nodes consists symmetric weight matrix rn×n zero diagonal threshold vector possible states network length binary strings represent figure example hopﬁeld network. ﬁgure displays -node hopﬁeld network weight matrix zero threshold vector. binary state vector energy labeled y-axis diagram right. arrows states represent iteration network dynamics; i.e. updated order indicated clockwise arrow graph left. resulting ﬁxed states network indicated ﬁlled circles. figure learning memories corrupted samples. stored ﬁngerprints hopﬁeld network nodes minimizing objective large randomly generated noisy versions training original ﬁngerprints stored ﬁxed-points network. sample ﬁngerprint corruption used training. sample ﬁngerprint corruption. state network update dynamics initialized converged network dynamics equal original ﬁngerprint. e-h. diﬀerent ﬁngerprint. identical energy function ising spin glass. fact dynamics hopﬁeld network seen -temperature gibbs sampling energy function. fundamental property hopﬁeld networks asynchronous dynamical updates increase energy thus ﬁnite number updates initial state converges ﬁxed-point straightforward basic problem construct hopﬁeld networks given binary patterns memory denoising retrieval since corrupted versions patterns converge iterating learning rule updates network’s weights thresholds given training pattern call rule local learning updates three parameters computed access solely feedforward inputs thresholds otherwise call rule nonlocal. note stricter deﬁnition local sometimes used learning rule called local updating depends states feedforward inputs units unit necessarily feedforward input locally available since feedforward input compared threshold output chosen. therefore label learning rules utilize feedforward input local rules. locality rule important feature network training algorithm necessity theoretical models computation neuroscience. hopﬁeld deﬁned outer-product learning rule ﬁnding networks. local rule since binary states nodes required update coupling term training using patterns stored without errors n-node hopﬁeld network particular ratio patterns storable number nodes using rule memories neuron approaches zero increases. small percentage incorrect bits tolerated approximately patterns stored perceptron learning rule provides alternative method store patterns hopﬁeld network also local rule since updating requires unlike achieves optimal storage capacity possible collection patterns ﬁxed-points hopﬁeld network converge parameters ﬁxed-points. however training frequently takes many parameter despite connection ising model energy function common usage ising spin glasses build probabilistic models binary data aware previous work associative memories takes advantage probabilistic interpretation training. probabilistic interpretations used pattern recovery give eﬃcient algorithm storing least binary patterns strict local minima n-node hopﬁeld network prove algorithm achieves optimal storage capacity achievable network. also present novel local learning rule training neural networks. consider collection binary n-bit patterns stored strict local minima hopﬁeld network. collections patterns stored; instance nevertheless collection stored local minima hopﬁeld network strict local minimum energy function theorem binary vectors stored local minima hopﬁeld proof ﬁrst claim stored local minima hopﬁeld network objective satisﬁes suppose ﬁrst made strict local minima parameters inequality holds. particular uniform scaling parameters make energy diﬀerences arbitrarily large negative thus made less conversely suppose choice term positive numbers less implies energy diﬀerence satisﬁes thus strict local explain claim proves theorem. suppose stored local minima hopﬁeld network; then method producing parameter values objective arbitrarily close inﬁmum produce network objective strictly less therefore store next main result least patterns n-node hopﬁeld network stored minimizing make statement mathematically precise introduce notation. probability collection binary patterns problem determine exact critical value note perceptron asymmetric weight matrix achieve cover bound store arbitrary mappings stored mappings local minima associated energy function learned network equivalent figure shows fraction patterns made ﬁxed-points hopﬁeld network using function number randomly generated training patterns here binary nodes averaged trials. slight diﬀerence performance extraordinary number iterations required achieve perfect storage patterns near critical pattern capacity hopﬁeld network. also fig. close section deﬁning learning rule neural network. words minimum probability learning rule takes input training pattern moves parameters small amount direction steepest descent performed several experiments comparing standard techniques ﬁtting hopﬁeld networks minimizing objective function computations performed standard desktop computers used used limited-memory broyden-fletchergoldfarb-shanno algorithm minimize study eﬃciency method compared training time -node network fig. three techniques per. resulting computation times displayed fig. logarithmic scale. notice computation time signiﬁcantly increases near pattern capacity threshold hopﬁeld network. third experiment compared denoising performance per. four values -node hopﬁeld network determined weights thresholds storing randomly generated binary patterns using per. ﬂipped bits stored patterns dynamics converge recording converged pattern identical original pattern not. results shown demonstrate superior corrupted memory retrieval performance mpf. surprising ﬁnal ﬁnding investigation store patterns highly corrupted noisy versions without supervision. result explained illustrate experiment visually stored binary ﬁngerprints -node hopﬁeld network using large training samples corrupted ﬂipping random original bits; fig. details. figure shows fraction exact pattern recovery perfectly trained hopﬁeld network using rules function corruption start recovery dynamics various numbers patterns store. remark ﬁgure next include performance worse either per. figure shows fraction patterns fraction bits recalled trained networks function number patterns stored. training patterns presented repeatedly corruption presented novel technique storage patterns hopﬁeld associative memory. ﬁrst step method ising model using minimum probability learning discrete distribution supported equally binary target patterns. next learned ising model parameters deﬁne hopﬁeld network. show target patterns storable steps result hopﬁeld network stores patterns ﬁxed-points. also demonstrated resulting algorithm outperforms current techniques training hopﬁeld networks. shown improved recovery memories noisy patterns improved training speed compared training per. demonstrated optimal storage capacity noiseless case outperforming opr. also demonstrated unsupervised storage memories heavily corrupted training data. furthermore learning rule results method local; updating weights units requires states feedforward input. probabilistic interpretation hopﬁeld network used rule leads superior robustness noise graceful degradation case patterns stored ﬁxed points. probabilistic learning objective tries make observed data states probable also make unobserved states improbable. second aspect reduces probability mass assigned spurious minima attractive wells improving pattern recovery noisy initialization. additionally patterns presented stored probabilistic objective attempts carve broad minima energy landscape around clusters datapoints. many noisy examples template patterns presented lowest energy states tend center minima corresponding cluster data thus tend correspond template states. allows ﬁtting large hopﬁeld networks quickly investigations structure hopﬁeld networks posssible hope robustness speed learning technique enable practical hopﬁeld associative memories computational neuroscience computer science scientiﬁc modeling. diﬃculties training probabilistic models stem conditioning model’s parameter space well inability analytically normalize model. chapter review conditioned parameter space undermine learning present novel interpretation common technique dealing conditioning natural gradient. addition present tricks speciﬁc prescriptions applying natural gradient learning problems. material chapter taken natural gradient introduced allows eﬃcient gradient descent removing dependencies biases inherent function’s parameterization. several papers present topic thoroughly precisely remains diﬃcult idea head around however. intent chapter provide simple intuition natural gradient uses. natural gradient explained analogy widely understood concept signal whitening. knowledge ﬁrst time connection made signal whitening natural gradient. seen figure steepest gradient update steps move parameters direction nearly perpendicular desired direction. much sensitive changes step size much smaller instead much larger. addition independent other. move distribution nearly direction making movement perpendicular direction particularly diﬃcult. getting parameters fully converge steepest descent slow proposition shown figure pathological learning gradient illustrative general problem. model’s learning gradient eﬀected parameterization model well objective function minimized. eﬀects parameterization dominate learning. natural gradient technique remove eﬀects model parameterization learning updates. ﬁrst step towards compensating diﬀerences relative scaling cross-parameter dependencies shape parameter space ﬁrst described assigning measure distance metric. metric expressed symmetric matrix deﬁnes used calculate natural gradient. notice steepest descent takes circuitous slower path. divergence data distribution model function number gradient descent steps. descent using natural gradient converges quickly. arrows give gradient likelihood objective grid parameter settings. descent direction provided equation gradient likelihood objective terms whitened natural parameter space described section note steepest descent whitened space converges directly true parameter values φtrue objective function likelihood probability distribution measure information distance usually works well fisher information matrix frequently used metric. plugging example section resulting fisher information matrix connection covariance analogue inverse covariance matrix signal whitened given removing ﬁrst order dependencies scaling variance dimension unit length parameterization also whitened removing dependencies diﬀerences scaling dimensions captured figure example signal whitening. function similar procedure followed produce whitened parameterization wish parameters metric identity mahalanobis metric identity whitened signal. mean small step direction tend magnitude eﬀect objective leads symmetric zero-phase whitening. ﬁelds referred decorrelation stretch. equivalent rotating signal basis rescaling axis unit norm performing inverse rotation returning signal original orientation. unitary transformations steepest gradient descent steps terms descend objective function direct fashion steepest gradient descent steps terms illustrated figure steepest gradient natural gradient. almost always function problems parameterization white everywhere. long changes slowly though treated constant single learning step. suggests following algorithm learning natural parameter space space natural gradient ˜∇θj direction equivalent steepest gradient descent order ˜∇θj ﬁrst write terms write natural gradient update step terms practically usually treated constant many learning steps. allows natural gradient combined plug play fashion gradient descent algorithms like l-bfgs performing gradient descent rather surprise opportunity ﬁrst person read email receive gift drawn random complex probability distribution likely bottle wine. later respondents receive miniature version randomly drawn gift instance airline-sized wine bottle. kappen rodriguez jaakkola jordan haykin turning gradients natural gradients diﬃcult though fisher information depends gradient practically simply ignoring terms entirely using metric diagonal approximation square matrix size number parameters vector problems large impractically expensive compute apply. almost problems however natural gradient still improves convergence even oﬀ-diagonal elements neglected regularization even evaluating full easy problem still conditioned. dealing solving linear equations subject regularization rather using unstable matrix inverse entire ﬁeld study computer science. give simple plug play technique called stochastic robust approximation regularizing matrix inverse. replaced this general problem taking matrix inverses. matrix random elements noisy elements tend small eigenvalues. eigenvalues inverses eigenvalues thus tend large eigenvalues tend make elements large. even worse eigenvalues eigenvectors dominate smallest noisiest least trustworthy useful combine natural gradient gradient descent techniques. blindly replacing gradients natural gradients frequently causes problems ﬁxed value though natural parameter space order easily combine natural gradient gradient descent techniques ixed initial value perform gradient descent using preferred algorithm. signiﬁcant number update steps convert back update ixed value continue gradient descent space. techniques presented unique probabilistic models. natural gradient used context suitable metric written parameters. several approaches writing appropriate metric. descend objective function. wrong used gradient descent performed suboptimal problem steepest gradient descent used well. making educated guess rarely makes things worse frequently helps great deal. chapter introduce extension annealed importance sampling uses hamiltonian dynamics rapidly estimate normalization constants. demonstrate method computing likelihoods directed undirected probabilistic image models. compare performance linear generative models gaussian laplace priors product experts models laplace student’s experts mc-rbm bilinear generative model. matlab code implementing estimation technique presented chapter available material chapter taken introduced section would like probabilistic models assign probabilities data. unfortunately innocuous statement belies important diﬃcult problem many interesting distributions used widely across sciences cannot analytically normalized. historically training probabilistic models motivated terms maximizing probability data model minimizing divergence data model. however models impossible directly compute likelihood intractability normalization constant partition function. reason performance typically measured using variety diagnostic heuristics directly indicative likelihood. example image models often compared terms synthesis denoising inpainting classiﬁcation performance. inability directly measure likelihood made diﬃcult consistently evaluate compare models. recently growing number researchers given attention measures likelihood image models. annealed importance sampling hybrid annealed importance sampling chib-style estimator estimate likelihood variety mnist digits natural image patches modeled using restricted boltzmann machines deep belief networks. measures reduction multi-information statistical redundancy images undergo various complete linear transformations. produce estimates entropy inherent natural scenes uses kernel density estimates essentially address model evaluation. vector quantization compare diﬀerent image models though technique suﬀers compare severe scaling problems except speciﬁc contexts. true likelihoods number image models restricts analysis rare cases partition function solved analytically. work merge existing ideas annealed importance sampling hamiltonian dynamics single algorithm. insight makes algorithm eﬃcient previous methods adaptation work hamiltonian dynamics. extend state space include auxiliary momentum variables; however momenta change consistently intermediate distributions rather resetting beginning markov transition. make practical applications work clear method hamiltonian annealed importance sampling measure likelihood holdout data variety directed undirected probabilistic models natural image patches. hamiltonian monte carlo uses analogy physical dynamics particles moving momentum inﬂuence energy function propose markov chain transitions rapidly explore state space. expanding state space include auxiliary momentum variables simulating hamiltonian dynamics move long distances along iso-probability contours expanded state space. similar technique powerful context annealed importance sampling. additionally retaining momenta variables across intermediate distributions signiﬁcant momentum build proposal distribution transformed target. provides mixing beneﬁt unique formulation. momentum drawn forward chain described annealed importance sampling estimate given equation remains unchanged except replacement terms involving momentum combines advantages many intermediate distributions lower variance estimated improved mixing occurs momentum maintained many update steps. details hamiltonian monte carlo sampling techniques discussion speciﬁc steps leave invariant recommend models discussed linear constraints state spaces. dealt negating momentum reﬂecting position across constraint boundary every time leapfrog halfstep violates constraint. mean covariance restricted boltzmann machine analysis model analogue bilinear generative model. exact marginal energy function emcr taken released code rather paper. models trained pixel image patches taken random linearized images natural scenes hateren dataset extracted image patches ﬁrst logged mean subtracted. projected onto components whitened rescaling dimension unit norm. generative models trained using expectation maximization full training hamiltonian monte carlo algorithm used expectation step maintain samples posterior distribution. details. analysis models trained using lbfgs minimum probability learning objective function full training transition function based hamiltonian dynamics. details. regularization decay terms required model parameters. table average likelihood test data models. model ‘size’ column denotes number experts models mean covariance units mcrbm total number latent variables generative models. training data. unless otherwise noted likelihood estimated patches drawn test images using hamiltonian annealed importance sampling intermediate distributions particles. procedure takes seconds component analysis models tested below. generative models take approximately hours models unmarginalized auxiliary variables require full hais test datapoint. likelihood test data analytically computed three models outlined above linear generative gaussian prior product experts complete representation laplace student’s experts figures show convergence hamiltonian annealed importance sampling particles three models function number intermediate distributions. note student’s expert pathological case sampling based techniques several learned even ﬁrst moment student’s t-distribution inﬁnite. hastings rejection rules. second compare single hamiltonian leapfrog step intermediate distribution unit norm isotropic gaussian momentum. unlike hais however case randomize momenta update step rather allowing remain consistent across intermediate transitions. seen figures hais requires fewer intermediate distributions order magnitude more. training models diﬀerent sizes using hais compute likelihood able explore model behaves regard three somewhat diﬀerent characteristics shown figure model laplace expert relatively poor performance evidence able overﬁt training data; fact relatively weak sparsity laplace prior tend think thing learn oriented band-pass functions ﬁnely tile space orientation frequency. contrast student-t expert model rises quickly high level performance overﬁts dramatically. surprisingly mcrbm performs poorly number auxiliary variables comparable best performing model. explanation testing regime major structures designed model great beneﬁt. mcrbm primarily good capturing long range image structures suﬃciently present data components. although computational reasons evidence mcrbm overﬁt dataset likely power. expect fare better models scale sizeable images. finally excited superior performance bilinear generative model outperforms models small number auxiliary variables. suspect mainly high degree ﬂexibility sparse prior whose parameters learned data. fact comparable number hidden units outperforms mcrbm thought bilinear generative model’s ‘analysis counterpart’ highlights power model. models mcrbm chosen best performing datapoints figure linear models sparse priors experts leads large increase likelihood gaussian model. choice sparse prior similarly important model student’s experts performing nats better generative model laplace prior expert. although previous work suggested bilinear models outperform linear counterparts experiments show student’s performing within noise complex models. explanation relatively small dimensionality data advantage bilinear models linear expected increase dimensionality. another student’s models fact better previously believed. investigation underway. surprising performance student’s however highlights power usefulness able directly compare likelihoods probabilistic models. improving upon available methods partition function estimation made possible directly compare large probabilistic models terms likelihoods assign data. fundamental measure quality model especially model trained terms likelihood frequently neglected practical computational limitations. hope hamiltonian annealed importance sampling technique presented lead better relevant empirical comparisons models. figure subset basis functions ﬁlters learned model. bases linear generative model gaussian prior laplace prior; ﬁlters product experts model laplace experts student’s experts; bases bilinear generative model basis elements making single grouping ordered contrast modulated according strength corresponding weight mcrbm ﬁlters means single grouping showing pooled ﬁlters ordered contrast modulated according strength corresponding weight figure comparison hais alternate algorithms complete student’s model. scatter plot shows estimated likelihoods test data model diﬀerent numbers intermediate distributions blue crosses indicate hais. green stars indicate single hamiltonian dynamics leapfrog step distribution continuity momentum. dots indicate gaussian proposal distribution. dashed blue line indicates true likelihood minimum probability trained model. product student’s distribution extremely diﬃcult normalize numerically many moments inﬁnite. figure convergence hais linear generative model gaussian prior. dashed blue line indicates true likelihood test data model. solid blue line indicates hais estimated likelihood test data diﬀerent numbers intermediate distributions sampling critical many tasks involved learning working probabilistic models. discussed section hamiltonian monte carlo current state technique sampling high dimensional probabilistic models continuous state spaces. chapter extensions hamiltonian monte carlo allow rapid exploration state space presented. material chapter taken hamiltonian dynamics partial momentum refreshment style explore state space slowly otherwise would momentum reversals occur proposal rejection. cause trajectories double back themselves leading random walk behavior timescales longer typical rejection time leading slower mixing. present technique number momentum reversals reduced. accomplished maintaining exchange probability states opposite momenta reducing rate exchange directions direction. experiment illustrates reduced momentum ﬂips accelerating mixing particular distribution. doesn’t change probability state leapfrog integrator integrates hamiltonian dynamics hamiltonian using leapfrog integration integration steps stepsize assume constants write operator simply figure diagram illustrates possible transitions states using markov transition operator equation relevant states represented nodes labeled. possible transitions represented arrows labeled. section probability state detailed balance instead directly enforcing zero change probability state summing allowed transitions state. constraint analogous kirchhoﬀ’s current total current entering node seen equation deﬁnitions section illustrated figure state lose probability states gain probability states equating rates probability inﬂow outﬂow figure covariance samples function number intervening sampling steps standard rejection rejection fewer momentum reversals. reducing number momentum reversals causes faster mixing evidenced faster falloﬀ autocovariance. order demonstrate accelerated mixing provided technique samples drawn simple distribution standard rejection separate rejection momentum ﬂipping rates described above. cases leapfrog step length number integration steps momentum corruption rate corrupt half momentum unit stimulation time. samplers sampling steps. distribution used described energy function dimensional image distribution seen figure autocovariance returned samples seen function number intervening sampling steps figure sampling using technique presented rapid decay autocovariance consistent faster mixing. scientiﬁc progress driven ability build models world. investigating complex large systems tools available build probabilistic models frequently inadequate. thesis introduced several tools address pressing issues probabilistic modeling. minimum probability flow learning novel general purpose framework parameter estimation probabilistic models outperforms current techniques learning speed accuracy. works parametric model without hidden state variables including continuous discrete state space systems. avoids explicit calculation partition function employing deterministic dynamics place slow sampling required many existing approaches. provides simple well-deﬁned objective function minimized quickly using existing higher order gradient descent techniques. furthermore objective function convex models exponential family ensuring global minimum found gradient descent. extensions allow used conjunction sampling algorithms persistent particles even faster performance. minimum velocity learning score matching forms contrastive divergence special cases speciﬁc choices dynamics. natural gradient powerful concept diﬃcult understand traditional presentation. made connection natural gradient common concept signal whitening additionally provided cookbook techniques application natural gradient learning problems. lower barrier understanding using technique learning problems. natural gradient allow model ﬁtting performed quickly accurately situations previously impractical impossible. hamiltonian annealed importance sampling allows partition function non-analytically-normalizable probabilistic models computed many times faster competing techniques. improving upon available methods partition function estimation makes possible directly compare large probabilistic models terms likelihoods assign data. fundamental measure quality model frequently neglected literature practical computational limitations. hope hais lead meaningful comparisons models. improvements hamiltonian monte carlo sampling make many tasks averaging distribution practical complex computationally expensive probabilistic models. reduced time required generate independent samples distribution hamiltonian monte carlo reducing frequency momentum ﬂips cause sampler retrace steps. improve practicality sampling distribution lead frequent samples rather less accurate approximations maximum posteriori estimates. minimum probability learning objective function found taking ﬁrst order terms taylor expansion divergence data distribution distribution resulting running dynamics time appendix lower bound probability data states derived terms objective function. although bound theoretical interest evaluating requires calculating ﬁrst non-zero eigenvalue probability matrix typically intractable task equivalent computing mixing time markov chain monte carlo algorithm. mixing time monte carlo algorithm transition matrix corresponding upper lower bounded using however generally diﬃcult frequently little said monte carlo mixing times. equation score matching developed aapo hyv¨arinen method learns parameters probabilistic model using derivatives energy function evaluated data distribution sidesteps need explicitly sample integrate model distribution. score matching minimizes expected square distance score function respect spatial coordinates given data distribution similar score function given model distribution. number connections made score matching learning techniques show correct limit also reduces score matching. score matching thus equivalent connectivity function non-zero states inﬁnitesimally close other. noted score matching estimator closed-form solution model distribution belongs exponential family said limit. appendix derives objective function case ising model. section connectivity states diﬀer single ﬂip. section additional connection included states diﬀer bits. additional connection particularly beneﬁcial cases unit activity extremely sparse. code implementing ising model available extension connectivity function aids assigning correct relative probabilities data states states opposite side state space data even cases data lies small region state space. functions comparing relative probabilities data states states connected data states. region state space data lives data states connected blind region state space assign incorrect probability problem observed ﬁtting ising model sparsely active neural data. case assigns much probability states many units simultaneously. however additional connection added state state bits ﬂipped comparison states available many units simultaneously. extra connection better penalizes non-sparse states gets much better. matlab code available implements loop calculates change caused samples simultaneously. note loop could also performed samples change induced calculated matrix operations. code small batch size implementation would faster. clever programmer might replace loops matrix operations. craig abbey jascha sohl-dickstein bruno olshausen miguel eckstein john boone. higher-order scene statistics breast images. proceedings spie volume pages spie february johnson joseph kinch lemmon madsen maki malin mccartney mclennan mcsween ming moersch morris dobrea parker proton rice seelos soderblom soderblom sohl-dickstein sullivan wolﬀ wang. pancam multispectral imaging results spirit rover gusev crater. science august bell squyres arvidson arneson bass calvin farrand goetz golombek greeley grotzinger guinness hayes hubbard herkenhoﬀ johnson johnson joseph kinch lemmon madsen maki malin mccartney mclennan mcsween ming morris dobrea parker proton rice seelos soderblom soderblom sohl-dickstein sullivan weitz wolﬀ. pancam multispectral imaging results opportunity rover meridiani planum. science december bell joseph sohl-dickstein arneson johnson lemmon savransky. in-ﬂight calibration performance mars exploration rover panoramic camera instruments. journal geophysical research january cohen grossberg. absolute stability global pattern formation parallel memory storage competitive neural networks. ieee transactions systems cybernetics cover. geometrical statistical properties systems linear inequalities applications pattern recognition. electronic computers ieee transactions benjamin culpepper jascha sohl-dickstein bruno olshausen. building better probabilistic model images factorization. international conference computer vision j.p. grotzinger r.e. arvidson j.f. bell calvin b.c. clark d.a. fike golombek greeley haldemann k.e. herkenhoﬀ b.l. jolliﬀ a.h. knoll malin s.m. mclennan parker soderblom j.n. sohl-dickstein s.w. squyres n.j. tosca w.a. watters. stratigraphy sedimentology eolian depositional system burns formation meridiani planum mars. earth planetary science letters november hayes grotzinger edgar squyres watters sohl-dickstein. reconstruction eolian forms paleocurrents cross-bedded strata victoria crater meridiani planum mars. journal geophysical research april herkenhoﬀ squyres bell maki arneson bertelsen brown collins dingizian elliott goetz hagerott hayes johnson kirk mclennan morris scherr schwochert shiraishi smith soderblom sohl-dickstein wadsworth. athena microscopic imager investigation. journal geophysical research november christopher hillar jascha sohl-dickstein kilian koepsell. eﬃcient optimal binary hopﬁeld associative memory storage using minimum probability ﬂow. arxiv april hopﬁeld. neural networks physical systems emergent collective computational abilities. proceedings national academy sciences united states america hyv¨arinen. connections score matching contrastive divergence pseudolikelihood continuous-valued variables. computational statistics data analysis january jaakkola jordan. variational approach bayesian logistic regression models extensions. proceedings sixth international workshop artiﬁcial intelligence statistics january jeﬀrey johnson jascha sohl-dickstein william grundy raymond arvidson james bell phil christensen trevor graﬀ edward guinness kjartan kinch richard morris michael shepard. radiative transfer modeling dust-coated pancam calibration target materials laboratory visible/near-infrared spectrogoniometry. journal geophysical research october kjartan kinch jascha sohl-dickstein james bell jeﬀrey johnson walter goetz geoﬀrey landis. dust deposition mars exploration rover panoramic camera calibration targets. journal geophysical research april siwei lyu. unifying non-maximum likelihood learning objectives minimum contraction. shawe-taylor zemel bartlett pereira weinberger editors advances neural information processing systems pages iain murray ruslan salakhutdinov. evaluating probabilities high-dimensional latent variable models. advances neural information processing systems january marc’aurelio ranzato geoﬀrey hinton. modeling pixel means covariances using factorized third-order boltzmann machines. ieee conference computer vision pattern recognition january shlens field gauthier grivich petrusca sher litke chichilnisky. structure multi-neuron ﬁring patterns primate retina. journal neuroscience august jascha sohl-dickstein peter battaglino michael deweese. method parameter estimation probabilistic models minimum probability flow. physical review letters november jascha sohl-dickstein peter battaglino michael deweese. minimum probability flow learning. international conference machine learning november sommer dayan. bayesian retrieval associative memories storage errors. ieee transactions neural networks publication ieee neural networks council january tang jackson hobbs chen jodi smith hema patel anita prieto dumitru petrusca matthew grivich sher pawel hottowy wladyslaw dabrowski alan litke john beggs. maximum entropy model applied spatial temporal correlations cortical networks vitro. journal neuroscience january hateren schaaf. independent component filters natural images compared simple cells primary visual cortex. proceedings biological sciences march", "year": 2012}