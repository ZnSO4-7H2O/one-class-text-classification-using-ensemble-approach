{"title": "A Neural Transducer", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.", "text": "sequence-to-sequence models achieved impressive results various tasks. however unsuitable tasks require incremental predictions made data arrives tasks long input sequences output sequences. generate output sequence conditioned entire input sequence. paper present neural transducer make incremental predictions input arrives without redoing entire computation. unlike sequence-to-sequence models neural transducer computes next-step distribution conditioned partially observed input sequence partially generated sequence. time step transducer decide emit zero many output symbols. data processed using encoder presented input transducer. discrete decision emit symbol every time step makes difﬁcult learn conventional backpropagation. however possible train transducer using dynamic programming algorithm generate target discrete decisions. experiments show neural transducer works well settings required produce output predictions data come also neural transducer performs well long sequences even attention mechanisms used. recently introduced sequence-to-sequence model shown success many tasks sequences sequences e.g. translation speech recognition image captioning dialogue modeling however method unsuitable tasks important produce outputs input sequence arrives. speech recognition example online task users prefer seeing ongoing transcription speech receiving utterance. similarly instant translation systems would much effective audio translated online rather entire utterances. limitation sequence-to-sequence model fact output predictions conditioned entire input sequence. paper present neural transducer general class sequence-to-sequence learning models. neural transducer produce chunks outputs blocks inputs arrive thus satisfying condition online overview). model generates outputs block using transducer implements sequence-to-sequence model. inputs transducer come sources encoder recurrent state. words transducer generates local extensions output sequence figure high-level comparison method sequence-to-sequence models. sequence-tosequence model neural transducer emits output symbols data come transfers hidden state across blocks. conditioned features computed block encoder recurrent state transducer last step previous block. training alignments output symbols input sequence unavailable. overcoming limitation treat alignment latent variable marginalize possible values alignment variable. another approach generate alignments different algorithm train model maximize probability alignments. connectionist temporal classiﬁcation follows former strategy using dynamic programming algorithm allows easy marginalization unary potentials produced recurrent neural network however possible model since neural network makes next-step predictions conditioned input data alignment targets produced current step. paper show dynamic programming algorithm used compute \"approximate\" best alignments model. show training model alignments leads strong results. timit phoneme recognition task neural transducer achieve accuracy phoneme error rate close state-of-the-art unidirectional models. show good alignments made available model achieve per. past years many proposals made power ﬂexibility neural networks especially concept augmented memory augmented arithmetic units work concerned memory arithmetic components allows ﬂexibility model dynamically produce outputs data come work related traditional structured prediction methods commonplace speech recognition. work bears similarity hmm-dnn systems. important aspect approaches model makes predictions every input time step. weakness models typically assume conditional independence predictions output step. sequence-to-sequence models represent breakthrough assumptions made output sequence generated next step prediction conditioning entire input sequence partial output sequence generated figure shows high-level picture architecture. however seen ﬁgure models limitation wait speech utterance start decoding. property makes unattractive real time speech recognition online translation. bahdanau attempt rectify speech recognition using moving windowed attention provide mechanism address situation arises output produced windowed segment data. figure shows difference method sequence-to-sequence models. strongly related model sequence transducer model augments model combining transcription model prediction model. prediction model akin language model operates output tokens next step prediction model. gives figure overview neural transducer architecture speech. input acoustic sequence processed encoder produce hidden state vectors time step transducer receives block inputs step produces output tokens using sequence-to-sequence model input. transducer maintains state across blocks recurrent connections previous output time steps. ﬁgure shows transducer producing tokens block subsequence emitted block ymym+ym+. model expressiveness compared makes independent predictions every time step. however unlike model presented paper models sequence transducer operate independently model provide mechanism prediction network features time step would change transcription network features future vice versa. model effect generalizes model sequence sequence model. formulation requires inferring alignments training. however results indicate done relatively fast little loss accuracy even small dataset effort made regularization. further alignments given easily done ofﬂine various tasks model able train relatively fast without inference step. x···l input data time steps long represents features input time step block size i.e. periodicity transducer emits output tokens ˜y···s target sequence corresponding input sequence. further transducer produce sequence outputs ˜yi··· input block. sequence padded symbol added vocabulary. signiﬁes transducer proceed consume data next block. symbols produced block symbol akin blank symbol ctc. sequence ˜y···s transduced input various alignments. alignments output sequence ˜y···s input blocks. y···) alignment. note length length since block symbols however number sequences matching much larger corresponding possible alignments blocks. block element aligned inferred simply counting number symbols came index index last token emitted block. note thus =<e> block transducer using encoding input x···bw computed encoder label preﬁx y··· input transducer previous emission steps. describe detail next subsection. refer reader figure discussion. example shows transducer hidden layers units output step ﬁgure next step prediction shown block block index ﬁrst output symbol index last output symbol transducer computes next step prediction using parameters neural network following sequence steps recurrent neural network function computes state vector layer step using recurrent state vector last time step input current time step; fsof tmax softmax distribution computed softmax layer input vector fcontext ducer output step state current time step features +)···bw encoder current input block experimented different ways computing context vector without attention mechanism. described subsequently section note since encoder hw···bw actually function entire input x···bw far. correspondingly function labels emitted entire input seen far. similarly model context vector computed steps ﬁrst normalized attention vector computed state transducer next hidden states +···bw encoder current block linearly combined using used context vector. compute multi-layer perceptron computes scalar value pair transducer state encoder attention vector computed scalar values formally also experimented using simpler model fattention computed refer model dot-attention model. attention models shortcomings. firstly explicit mechanism requires attention model move focus forward output time step next. secondly energies computed inputs softmax function different input frames independent time step thus cannot modulate other softmax function. chorowski ameliorate second problem using convolutional operator affects attention time step using attention last time step. attempt address shortcomings using attention mechanism. model instead feeding softmax feed recurrent neural network hidden layer outputs softmax attention vector time step. thus model able modulate attention vector within time step across time steps. attention model thus general convolutional operator chorowski applied case context window size constant. refer model lstm-attention. since model produces small sequence output tokens block address mechanism shifting transducer block next. experimented three distinct ways this. ﬁrst approach introduced explicit mechanism end-of-blocks hoping transducer neural network would implicitly learn model training data. second approach added end-of-block symbols label sequence demarcate blocks added symbol target dictionary. thus softmax function equation implicitly learns either emit token move transducer forward next block. third approach model moving transducer forward using separate logistic function attention vector. target logistic function depending whether current step last step block not. latter term right hand side computed backpropagation using target model. however marginalization intractable combinatorial number alignments. alternatively gradient approximated sampling large noise learning gradients often biased leading models rarely achieved decent accuracy. instead attempted maximize probability equation computing term corresponding y···s highest posterior probability. unfortunately even exactly computationally infeasible number possible alignments combinatorially large problem ﬁnding best alignment cannot decomposed easier subproblems. algorithm ﬁnds approximate best alignment dynamic programming-like algorithm describe next paragraph. block output position algorithm keeps track approximate best hypothesis represents best partial alignment input sequence ˜y···j partial input x···bw hypothesis keeps track best alignment y··· represents recurrent states decoder last time step corresponding alignment. block hypotheses extended tokens using recurrent states compute h··· position highest probability hypothesis kept. alignment best hypothesis last block used training. theory need compute alignment sequence trained using model parameters time. practice batch alignment inference steps using parallel tasks cache alignments. thus alignments computed less frequently model updates typically every sequences. procedure ﬂavor experience replay deep reinforcement learning work exact inference scheme computationally expensive expression probability permit decomposition smaller terms independently computed. instead candidate would tested independently best sequence exponentially large number sequences would discovered. hence beam search heuristic best candidates. this output step keep heap alternative best preﬁxes extend symbol trying possible alternative extensions keeping best extensions. included beam search moving attention next input block. beam search ends either sequence longer pre-speciﬁed threshold token symbol produced last block. experimented neural transducer task adding three-digit decimal numbers. second number presented reverse order target output. thus model produce ﬁrst output soon ﬁrst digit second number observed. model able learn task small number units seen below model learns output digits soon required information available. occasionally model waits extra step output target symbol. show results four different examples block window size used used timit standard benchmark speech recognition larger experiments. ﬁlterbanks computed every inputs system. targets phones deﬁned timit dataset used stochastic gradient descent momentum batch size utterance training step. initial learning rate momentum used. learning rate reduced factor every time average prob validation decreased decrease applied maximum times. models trained epochs parameters epochs best prob used decoding. trained neural transducer three layer lstm coupled three lstm layer unidirectional encoder achieved timit test set. model used lstm attention mechanism. alignments generated model updated every steps momentum updates. interestingly alignments generated model similar alignments produced gaussian mixture model-hidden markov model system trained using kaldi toolkit even though model trained entirely discriminatively. small differences alignment correspond occasional phoneme emitted slightly later model compared gmm-hmm system. also trained models using alignments generated gmm-hmm model trained kaldi. frame level alignments kaldi converted block level alignments assigning phone sequence block last observed architecture model described achieved accuracy alignments. experiments assess properties model. order avoid computation associated ﬁnding best alignments experiments using gmm-hmm alignments. table shows comparison method basic implementation sequence-to-sequence model produces outputs block independent blocks concatenates produced sequences. here sequence-to-sequence model produces output conditioned state encoder block. models used encoder layers lstm cells without attention. standard sequence-to-sequence model performs signiﬁcantly worse model recurrent connections transducer across blocks clearly helpful improving accuracy model. table impact maintaining recurrent state transducer across blocks per. table shows maintaining state transducer across blocks leads much better results. experiment block size frames used. reported number median three different runs. figure shows impact block size accuracy different transducer variants used. section description {dotmlplstm}-attention models. models used lstm layer encoder lstm layer transducer. model sensitive choice block size attention used. however seen appropriate choice window size neural transducer without attention match accuracy attention based neural transducers. exploration conﬁguration lead improved results. attention used transducer precise value block size becomes less important. lstm-based attention model seems consistent compared attention mechanisms explored. since model performed best used conﬁguration subsequent experiments. figure impact number frames block attention mechanism per. figure shows validation function without different types attention mechanisms. number median value three experiments. table explores impact number layers transducer encoder per. three layer encoder coupled three layer transducer performs best average. four layer transducers produced results higher spread accuracy possibly difﬁcult optimization involved. thus best average achieved timit test set. results could probably improved regularization techniques reported pursue avenues paper. comparison previously published sequence-to-sequence models task used three layer bidirectional lstm encoder lstm cells direction achieved contrast best reported results using previous sequence-to-sequence models however result comes careful training techniques attempted work. given high variance results timit numbers quite promising. important side-effects model using partial conditioning blocked transducer naturally alleviates problem losing attention suffered sequence-to-sequence models. this sequence-to-sequence models perform worse longer utterances problem automatically tackled model block automatically shifts attention monotonically forward. within block model learns move attention forward step next attention mechanism rarely suffers size block number output steps block relatively small. result error attention block minimal impact predictions subsequent blocks. finally note increasing block size large input utterance makes model similar vanilla end-to-end models introduced model uses partial conditioning inputs generate output sequences. allows model produce output input arrives. useful speech recognition systems also crucial future generations online speech translation systems. useful performing transduction long sequences something possibly difﬁcult sequence-to-sequence models. applied model task addition phone recognition task showed produce results comparable state sequence-to-sequence models.", "year": 2015}