{"title": "Exact and approximate inference in graphical models: variable  elimination and beyond", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which is represented as a graph. However, the dependence between variables may render inference tasks intractable. In this paper we review techniques exploiting the graph structure for exact inference, borrowed from optimisation and computer science. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated. The so-called treewidth of the graph characterises this algorithmic complexity: low-treewidth graphs can be processed efficiently. The first message that we illustrate is therefore the idea that for inference in graphical model, the number of variables is not the limiting factor, and it is worth checking for the treewidth before turning to approximate methods. We show how algorithms providing an upper bound of the treewidth can be exploited to derive a 'good' elimination order enabling to perform exact inference. The second message is that when the treewidth is too large, algorithms for approximate inference linked to the principle of variable elimination, such as loopy belief propagation and variational approaches, can lead to accurate results while being much less time consuming than Monte-Carlo approaches. We illustrate the techniques reviewed in this article on benchmarks of inference problems in genetic linkage analysis and computer vision, as well as on hidden variables restoration in coupled Hidden Markov Models.", "text": "probabilistic graphical models offer powerful framework account dependence structure variables represented graph. however dependence between variables render inference tasks intractable. paper review techniques exploiting graph structure exact inference borrowed optimisation computer science. built principle variable elimination whose complexity dictated intricate order variables eliminated. so-called treewidth graph characterises algorithmic complexity low-treewidth graphs processed efﬁciently. ﬁrst message illustrate therefore idea inference graphical model number variables limiting factor worth checking treewidth turning approximate methods. show algorithms providing upper bound treewidth exploited derive ’good’ elimination order enabling perform exact inference. second message treewidth large algorithms approximate inference linked principle variable elimination loopy belief propagation variational approaches lead accurate results much less time consuming monte-carlo approaches. illustrate techniques reviewed article benchmarks inference problems genetic linkage analysis computer vision well hidden variables restoration coupled hidden markov models. graphical models formed variables linked stochastic relationships. enable model dependencies possibly high-dimensional heterogeneous data capture uncertainty. graphical models applied wide range areas elementary units locally interact other like image analysis speech recognition bioinformatics ecology name few. real applications large number random variables complex dependency structure involved. consequence inference tasks calculation normalisation constant marginal distribution mode joint distribution challenging. three main approaches exist evaluate quantities given distribution deﬁning graphical model compute exact manner; stochastic algorithm sample distribution estimates; derive approximation exact calculation possible. even appealing exact computation lead time memory consuming procedures large problems. second approach probably widely used statisticians modellers. stochastic algorithms monte-carlo markov chains gibbs sampling particle ﬁltering become standard tools many ﬁelds application using statistical models. last approach includes variational approximation techniques starting become common practice computational statistics. essence approaches type provide approximate answer exact problem whereas approaches type provide exact answer approximate problem. paper focus approaches type review techniques exact approximate inference graphical models borrowed optimisation computer science. computationally efﬁcient always standard statistician toolkit. characterisation structure graph associated graphical model enables determine exact calculation quantities interest implemented efﬁciently derive class operational algorithms. exact calculation cannot achieved efﬁciently similar analysis problem enables practitioner design algorithms compute approximation desired quantities associated acceptable complexity. provide reader elements understand power tools statistical inference graphical models. central algorithmic tool focus paper variable elimination concept section adopt uniﬁed algebraic presentation different inference tasks emphasise solved using particular case variable elimination scheme. consequently work done demonstrate variable elimination efﬁcient task passes ones. ingredient design efﬁcient algorithms based variable elimination clever distributivity algebraic operators. instance distributivity product enables write evaluating left-hand side equality requires multiplications addition evaluating right-hand side requires multiplication addition. similarly since a+max efﬁcient compute right-hand side algorithmic point view. distributivity enables minimise number operations. perform variable elimination associativity commutativity properties also required algebra behind semi-ring inference algorithms using distributivity property known published artiﬁcial intelligence machine learning literature different names sum-prod max-sum typical examples variable elimination procedures. variable elimination relies choice order elimination variables successive marginalisation maximisation operations. calculations performed according ordering applying distributivity. topology graph provides information optimally organise calculations minimise number elementary operations perform. example graph tree efﬁcient elimination order corresponds eliminating recursively vertices degree one. starts leaves towards root inner nodes higher degree successively become leaves. notion optimal elimination order inference arbitrary graphical model closely linked notion treewidth associated graph section reason inference algorithms based variable elimination best elimination order linear complexity number variables/nodes graph i.e. size graph exponential complexity treewidth. therefore treewidth main characterisation determine exact inference possible practice not. notion lead development several works solving apparently complex inference problems applied biology details methodological applied results provided conclusion section. concept treewidth proposed parallel computer science discrete mathematics graph minor theory discrete mathematics existence theorems establish exists algorithm computing treewidth graph complexity polynomial degree polynomial determined. however result tell derive implement algorithm apart speciﬁc cases trees chordal graphs series-parallel graphs section introduces reader several state-of-the-art algorithms provide upper bound treewidth together associated elimination order. algorithms therefore useful tools test exact inference achievable applicable derive exact inference algorithm based variable elimination. behaviour illustrated benchmarks borrowed combinatorial optimisation competitions. recently algorithms reinterpreted re-parameterise original graphical model updated different potential functions still representing join distribution explain section re-parametrisation used pre-processing tool obtain parameterisation inference becomes simpler. message passing perform re-parametrisation discuss alternative efﬁcient algorithms proposed context constraint satisfaction problems latter ones have best knowledge exploited context graphical models. emphasised above efﬁcient exact inference algorithms designed graphical models limited treewidth i.e. much less number vertices. although case many graphs principles variable elimination message passing tree applied graph leading heuristic inference algorithms. famous heuristics loopy belief propagation algorithm recall section result establishes variational approximation method. variational methods rely choice distribution renders inference easier. approximate original complex graphical model. approximate distribution chosen within class models efﬁcient inference algorithms exist models small treewidth review standard choices approximate distributions corresponds different underlying treewidth. finally section illustrates techniques reviewed article case coupled hidden markov model ﬁrst compare problem mode inference chmm devoted study pest propagation. exemplify different variational methods em-based parameter estimation chmm. models deﬁnition consider stochastic system deﬁned random variables variable takes values realisation denoted subset respectively subset random variables possible realisation state space respectively. joint probability distribution denote note focus discrete variables joint distribution said probabilistic graphical model indexed parts exists {ψb}b∈b maps called potential functions indexed expressed following factorised form normalising constant also called partition function. elements scopes potential functions arity potential function scopes potential functions involving variable denoted desirable property graphical models markov local independence expressed variable independent others conditionally variables x\\i. x\\iis called markov blanket neighbourhood denoted conditional independences represented graph vertex variable question encoding independence properties associated given distribution graph structure widely described discuss here. consider classical graph associated decomposition dictated edge drawn vertices exists representation graphical model actually rich representation instance cases represented graph namely clique size without loss generality could impose deﬁnition graphical model scopes correspond cliques example done deﬁning ψψψ. original structure lost costly store original potential functions. factor graph representation goes beyond limit representation graphical representation bipartite graph vertex potential function vertex variable. edges functions variables. edge present function vertex variable vertex variable scope potential function. figure displays examples graphical representations. several families probabilistic graphical models exist grouped directed undirected ones. classical directed framework bayesian network bayesian network potential functions conditional probabilities variable given parents. models trivially representation directed graph edge directed parent vertex child vertex undirected graphical representation obtained moralisation i.e. adding edge parents variables. undirected probabilistic graphical models equivalent markov random fields soon potential functions take values markov random ﬁeld potential function necessarily probability distribution required normalised deterministic graphical models. although terminology ’graphical models’ often used refer probabilistic graphical models idea describing joint interaction variables local functions also used artiﬁcial intelligence concisely describe boolean functions cost functions normalisation constraint. throughout article regularly refer deterministic graphical models explain algorithms devoted optimisation directly applied compute mode probabilistic graphical model. figure left right graphical representation directed graphical model potential functions deﬁne conditional probability variable given parents values; corresponding factor graph every potential function represented factor connected variables involved graphical representation undirected graphical model. impossible graph distinguish graphical model deﬁned unique potential function vertices model deﬁned pairwise potential functions pair corresponding factor graph unambiguously deﬁnes potential functions three pairwise potential functions. deterministic graphical model boolean potential functions potential function describes constraint variables. potential function takes value corresponding realisation said satisfy constraint. takes value realisation satisfy graphical model known ’constraint network’. describes joint boolean function variables takes value constraints satisﬁed. problem ﬁnding realisation satisﬁes constraints called solution constraint network ’constraint satisfaction problem’ framework used model solve combinatorial optimisation problems. wide variety software tools solve case cost functions take ﬁnite inﬁnite integer rational values inﬁnity enables express hard constraints ﬁnite values encode costs unsatisﬁed soft constraints. problem ﬁnding realisation minimum cost ’weighted constraint satisfaction problem’ np-hard. easy observe probabilistic graphical model translated weighted constraint network vice versa using simple transformation. therefore wcsp equivalent ﬁnding realisation maximal probability probabilistic graphical model. equivalence becomes possible exact wcsp resolution algorithms developed ﬁeld mode evaluation computation normalising constant probabilistic graphical model. instance viricel application problem protein design. computations probabilities potentials rely fundamental types operations. firstly multiplication used combine potentials deﬁne joint potential distribution. secondly max/min used eliminate variables compute marginals modes joint distribution subsets variables. precise identity basic operations important inference algorithms based variable elimination. therefore adopt presentation using generic operators emphasise property algorithms. denote combination operator elimination operator respectively. able apply variable elimination algorithm requirement deﬁnes commutative semi-ring. speciﬁcally semi-ring algebra offers distributivity instance corresponds distributivity product operation operation i.e. distributivity operation operation i.e. distributivity operation product operation i.e. extend deﬁnition abstract operators operators potential functions follows combine operator combination potential functions function classical counting optimisation tasks graphical models entirely written operators. simplicity denote sequence eliminations result insensitive order commutative semiring. similarly represents successive combination potential functions counting task. name group tasks involve summing state space subset variables includes computation partition function marginal distribution well entropy evaluation. marginal distribution associated joint distribution deﬁned combines functions using eliminates variables using marginal evaluation also interesting case variables observed. values observed values marginal conditional distribution computed restricting domains variables observed value. typically kind computational task required e-step algorithm parameter estimation models hidden data. maximum ⊕xb∈b respectively. computation mode require computation normalising constant however evaluating mode probability value does. another optimisation task interest computation max-marginals variable deﬁned maxxv therefore counting optimisation tasks interpreted instantiations combination operator elimination operator computational task expressed terms combination elimination operators namely respectively computational problem known sum-product problem artiﬁcial intelligence literature operator respectively max-sum problem practice means tasks solving e-step algorithm computing mode graphical model belong family computational problems. section exists exact algorithm solving general task exploits distributivity combination elimination operators perform operations smart order. generic algorithm known variable elimination bucket elimination deduce exact algorithms solve counting optimisation tasks graphical model instantiating operators deterministic graphical models. constraint satisfaction problem problem deﬁned using elimination operator combination operator booleans. weighted min-+ uses elimination operator combination operator. several variants exist including generic algebraic variants introduce example coupled hidden markov models seen extensions hidden markov chain models several chains interactions. section framework illustrate behaviour exact approximate algorithms based variable elimination. deﬁned sequences random variables length realisation variables observed states variables unknown model assumption made independent \\{i} \\{i} given hidden variable independences modelled pairwise potential functions ψhioi∀ furthermore hidden variable independent given hidden variable hi−. independences modelled pairwise potential functions ψhi−hi∀ model fully deﬁned specifying additional potential function model initial distribution. classical formulation potential functions normalised conditional probability distributions i.e. ψhi−hi ψoihi consequence normalising constant equal bayesian networks. consider hidden chain signals observed times denote variable corresponding observed signal time variable coupled framework assumes dependency hidden chains consecutive time steps indices chains depends depends noted results graphical structure displayed upon figure models considered series domains bioinformatics electroencephalogram analysis speech recognition chmm setting joint distribution hidden variables factorises ψinit initial distribution encodes local transition function encodes emission observed signal given corresponding hidden state. fairly comprehensive exploration models found potential function ψinit parameterised parameters denoted classical problem chmm iron estimate compute mode conditional distribution hidden variables given observations. estimation performed using algorithm mentioned previously e-step algorithm mode computation task belong family computational task graphical models. solved using variable elimination show next section. beforehand present reasonably simple example chmm used illustrate different inference algorithms introduced work. models dynamics pest spread landscape composed crop ﬁelds organised regular grid. spatial neighbourhood ﬁeld denoted four closest ﬁelds state crop corners grid). ﬁeld time state represents absence pest conditional probabilities ﬁeld. variable survival apparition pest ﬁeld parameterised parameters probability contamination outside landscape probability pest spreads infected ﬁeld ﬁeld consecutive times; probability ﬁeld persistent infection consecutive times. assume contamination events neighbouring ﬁelds independent. then number contaminated neighbours ﬁeld time contamination potential ﬁeld time writes hidden variables monitoring observations available. binary variable observed takes value pest declared present ﬁeld otherwise. errors detection possible. false negative observations occur since even pest there difﬁcult notice missed. opposite false positive observations occur pest mixed another one. deﬁne corresponding emission potential classical example variable elimination optimisation then formally describe variable elimination procedure general graphical model framework. element choice ordering sequential elimination variables. closely linked notion treewidth graphical representation model. explain complexity variable elimination algorithm fully characterised notion. also describe extension elimination blocks variables. didactic introduction exact inference graphical models variable elimination consider well studied stochastic process discrete hidden markov chain model classical inference task identify likely values variables given realisation variables problem compute maxh equivalently argument number possible realisations exponential nevertheless optimisation problem solved number operations linear using well-known viterbi algorithm algorithm based dynamic programming performs successive eliminations hidden variables starting iteratively considering hi’s ﬁnishing successively computes likely sequence hidden variables. using distributivity product operators elimination variable done rewriting potential function created maximising depends variable ht−. principle applied forth. simple application general variable elimination algorithm describe next section. trick behind variable elimination relies clever distributivity property. indeed evaluating requires fewer operations. hence eliminating second writing leads dealing fewer algebraic operations. since distributivity applies counting optimising tasks variable elimination applied tasks. also means variable elimination efﬁcient task also efﬁcient one. example principle variable elimination algorithm counting optimising consists eliminating variables expression problem like elimination ﬁrst variable performed merging potential functions involving applying operator potential functions. using commutativity associativity operators rewritten shows elimination results graphical model variable potential functions appear anymore. replaced potential involve depends neighbours graph associated graphical model sense similar original model. updated follows vertex removed neighbours connected together clique scope ψni. edges neighbours called ﬁll-in edges. instance eliminating variable graph figure potential functions replaced graph shown figure figure elimination variable replaces four pairwise potential functions involving variable potential involving four neighbours vertex original graph. edges created four vertices called ﬁll-in edges interpretation marginalisation maximisation ﬁnding mode distribution ﬁrst elimination step applied probability distribution deﬁned graphical model marginal distribution \\{i} original distribution complete elimination obtained successively eliminating variables result graphical model speciﬁes marginal distribution result model single constant potential function value instead last potential function obtained elimination last variable equal maximum non-normalised distribution. evaluating maximal probability graphical model obtained variable elimination algorithm changing deﬁnition needed) operator. lastly interested mode itself additional computation required. mode actually obtained induction \\{i} mode graphical model obtained elimination ﬁrst variable mode deﬁned value maximises maximisation straightforward derive take |λi| values. \\{i} obtained completing mode graphical model obtained elimination second variable stress procedure requires keep intermediary potential functions created successive eliminations. complexity intermediary potential functions variable elimination ordering prelude treewidth eliminating variable task computationally expensive computation intermediate ψni. requires compute product b∈bi several potential functions elements λni∪{i} state space xni∪{i}. time space complexity operation entirely determined cardinality |ni| indices maxj∈v |λj| time complexity space complexity complexity therefore exponential |ni| number neighbours eliminated variable current graphical model. total complexity variable elimination exponential maximum cardinality |ni| successive eliminations. however note linear means large necessarily problem access exact inference. graphical model changes elimination step number usually depends order variables eliminated. consequence prerequisite apply variable elimination decide ordering elimination variables. illustrated figure different orders lead different subsets. message choice order crucial. dictates efﬁciency variable elimination procedure. illustrate formalise intuition. understand viterbi algorithm efﬁcient algorithm mode evaluation hmc. graph associated comb-shaped hidden variables form line observed variable leaf comb possible design elimination order current variable eliminate unique neighbour graphical representation current model instance convention ﬁrst eliminated variable largest according ordering following elimination order eliminating variable using resulting graphical model fewer vertex previous ﬁll-in edge. indeed potential function function single variable since |ni| viterbi algorithm space complexity time complexity generally variable elimination efﬁcient i.e. leads transitional sets small cardinality graphical models whose graph representation tree. speciﬁcally graph structure always possible design elimination order current variable eliminate neighbour graphical representation current model. another situation variable elimination efﬁcient graph associated graphical model chordal size largest clique low. rationale interesting property explained intuitively here. figure edges created neighbours eliminated vertex. neighbourhood clique edge added. vertex whose neighbourhood clique called simplicial vertex. chordal graphs property exists elimination order vertices every vertex elimination process simplicial consequently exists elimination order ﬁll-in edges created. thus size transitional ni’s dictated size clique formed neighbours note tree chordal graph edges edges cliques. hence tree simplicial vertices vertices degree one. elimination degree vertices tree example simplicial elimination chordal graph. arbitrary graphs maximal scope size intermediate functions created variable elimination large memory time required storage computation quickly exceed computer capacities. depending chosen elimination order maximal scope reasonable computational point view large. again choice elimination order crucial. case chmm imagine different elimination orders either time slice time slice chain chain ﬁrst order starting oriented graph figure ﬁrst moralise then elimination variables last time step ﬁll-in edges. however eliminating variables temporal dependences chain create intermediate potential function depending variables successively eliminating temporal slices maximal size intermediate potential functions created second elimination order still starting moralised version create intermediate oriented graph eliminating variables successively potential function depending variables minimum front size k-tree number dimension also equal minmax clique number minus name few. treewidth also notion theory graph minors insist deﬁnitions. ﬁrst relies notion induced graph highlights close relationship ﬁll-in edges intermediate sets created variable elimination. second commonly used characterisation treewidth using so-called tree decompositions also known junction trees tools derive variable elimination algorithms. underlies block-by-block elimination procedure described section deﬁnition graph deﬁned vertices indexed edges. given ordering vertices induced graph gind deﬁned constructive follows. first gind vertices. edge oriented edge added gind going ﬁrst nodes according toward second. vertex considered following order deﬁned vertex treated oriented edge created pairs also called graph process computing sometimes referred playing elimination game simulates elimination using variable ordering graph chordal known every chordal graph least vertex directed) called perfect ordering gind elimination ordering figure graph elimination orders. left graph; middle induced graph associated elimination order vertices eliminated largest smallest. maximum size sets created elimination ﬁll-in edge added vertex eliminated; right induced graph associated elimination order maximum size sets created elimination ﬁll-in edges used. second notion enables deﬁne treewidth notion tree decomposition. intuitively tree decomposition graph organises vertices clusters vertices linked edges graph obtained tree. speciﬁc constraints vertices associated clusters decomposition tree required. constraints ensure resulting tree decomposition properties useful building variable elimination algorithms. trivial establish equivalence term exactly cardinality largest created variable elimination elimination order example figure middle right graphs induced graphs different orderings equal ﬁrst ordering second. easy example treewidth graph model tree equal established ﬁnding minimum treewidth ordering graph ﬁnding minimum treewidth tree decomposition computing treewidth graph equivalent complexity. arbitrary graph computing treewidth easy task. section dedicated question theoretical practical point view. treewidth therefore indicator answer driving subject review variable elimination efﬁcient given graphical model? instance principle variable elimination applied exact computation normalising constant markov random ﬁeld small lattice reeves pettitt regular graph known treewidth equal min. exact computation variable elimination possible lattices small value large). however well beyond computer capacities real challenging problems image analysis. case variable elimination used deﬁne heuristic computational solutions algorithm friel relies merging exact computations small sub-lattices original lattice. given graphical model tree decomposition graph possible alternative solve counting optimisation tasks eliminate variables successive blocks instead other. block block elimination procedure relies tree decomposition characterisation treewidth. underlying idea apply variable elimination procedure tree decomposition eliminating cluster tree step. first root cluster chosen used deﬁne order elimination clusters progressing leaves toward root. every eliminated cluster corresponds leaf current intermediate tree. potential function assigned cluster closest root. cluster always exists otherwise either running intersection property would satisﬁed graph decomposition would tree. precisely procedure starts elimination leaf cluster parent note assigned ci}. again commutativity distributivity used rewrite expression follows note variables indices ci\\cj eliminated even common cluster eliminated. instance example depicted figure ﬁrst eliminated cluster potential function depends variables cluster elimination continues cluster left. interest procedure intermediate potential function created cluster elimination scope much smaller treewidth leading better space complexity however time complexity increased. summary lowest achievable complexity performing variable elimination reached elimination orders cardinality intermediate sets smaller equal treewidth treewidth determined considering cluster sizes tree decompositions furthermore tree decomposition used build elimination order vice versa. indeed elimination order deﬁned using cluster elimination order based choosing arbitrary order eliminate variables indices subsets ci\\cj. conversely easy build tree decomposition given vertex ordering since induced graph gind chordal maximum cliques identiﬁed polynomial time. clique deﬁnes cluster tree decomposition. edges identiﬁed edges minimum spanning tree graph vertices edges weighed cj|. deterministic graphical models. knowledge notion treewidth properties ﬁrst identiﬁed combinatorial optimisation bertel´e brioshi coined dimension graph parameter later shown equivalent treewidth variable elimination related fourier-motzkin elimination variable elimination algorithm beneﬁts linearity handled formulas. variable elimination repeatedly rediscovered non-serial dynamic programming david-putnam procedure boolean satisﬁability problems bucket elimination wcsp viterbi forward-backward algorithms many more. exists situations choice elimination order deep impact complexity computations gauss elimination scheme system linear equations choleski factorisation large sparse matrices cases equivalence elimination decomposition also used already mentioned complexity counting optimisation tasks graphical models strongly linked treewidth underlying graph could guess optimal vertex ordering leading then would able achieve optimal complexity solving exactly tasks; recall maximal domain size variable graphical model. however ﬁrst obstacle overcome treewidth given graph cannot evaluated easily treewidth computation problem known np-hard spend time ﬁnding optimal vertex ordering computing probabilities underlying graphical model utility exact treewidth computation appears limited. therefore alternative line search look algorithms computing vertex ordering leading suboptimal width efﬁcient terms computational time. following describe empirically compare heuristics simultaneously provide vertex ordering upper bound treewidth. performing inference relying ordering still exact. optimal terms time complexity problems inference still performed reasonable time. broad class heuristic approaches greedy algorithms iterative approach variable elimination algorithm except manipulate graph structure. perform actual combination/elimination computation. starting empty vertex ordering initial graph repeatedly select next vertex ordering locally optimising following criteria vertex ﬁll-in edges simplicial vertex fast implementations minimum degree algorithms developed e.g. time complexity input graph vertices edges. minimum ﬁll-in heuristics tend slower compute yield slightly better treewidth approximations practice. moreover perfect elimination ordering exists heuristic thus recognises chordal graphs returns optimal treewidth particular case. easily established results bodlaender notice exists linear time algorithms detect chordal graphs maximum cardinality search greedy algorithm builds elimination order based cardinality already processed neighbours. however treewidth approximation return usually worse previous heuristic approaches. simple improve treewidth bound found greedy algorithms choose candidate vertices value selected criterion using second criterion minimum ﬁll-in ﬁrst maximum degree choose random iterate resulting randomised algorithms done kask compared mean treewidth upper bound found four approaches wcsp benchmarks used combinatorial optimisation problems various solver competitions. paritylearning optimisation variant minimal disagreement parity problem originally contributed dimacs benchmark used minizinc challenge linkage genetic linkage analysis benchmark geomsurf scenedecomp respectively geometric surface labelling scene decomposition problems computer vision problem possible vary number vertices potential functions. number instances problem well mean characteristics given table results reported figure .the randomised iterative minimum ﬁll-in algorithm used maximum iterations seconds compared maximum second used non-iterative approaches. minimum ﬁll-in algorithm performed better greedy approaches. randomised iterative version offers slightly improved performance price computation time. benchmark compared three exact methods task mode evaluation exploit either minimum ﬁll-in ordering randomised iterative version variable elimination and/or search elim exploit minimum ﬁll-in ordering and/or search used randomised iterative version. addition and/or search exploit tree decomposition depth first branch bound method order good trade-off memory space search effort. like variable elimination worst-case time complexity exponential treewidth. methods allocated maximum hour operon ghz. results reported figure show able solve problems methods ﬁxed time. however best performing method heavily depends problem category. paritylearning elim fastest method memory total instances table characteristics optimisation problems used benchmark. given problem several instances available corresponding different numbers variables different numbers potential functions. used less randomised iterative minimum ﬁll-in heuristic used and/or search preprocessing consumed ﬁxed amount time larger cost simple minimum ﬁll-in heuristics run. faster and/or search solve instances except problem categories perform comparison following implementation method. version elim implemented combinatorial optimisation solver toolbar version implemented combinatorial optimisation solver toulbar toulbar available www.inra.fr/ mia/t/toulbar. software inference competitions task. and/or search version implemented open-source version daoopt probabilistic inference challenge albeit different closed-source version tree-structured graphical models message passing algorithms extend variable elimination algorithm efﬁciently computing every marginals simultaneously variable elimination computes one. general graphical models message passing algorithms still applied. either provide approximate results efﬁciently exponential running cost. also present less classical interpretation message passing algorithms conceptually interesting view algorithms performing re-parametrisation original graphical model i.e. rewriting potentials without modifying joint distribution. instead producing external messages re-parametrisation produces equivalent figure left comparison treewidth upper bounds provided minimum degree minimum ﬁll-in randomized iterative minimum ﬁll-in categories problems right mode evaluation three exact methods exploiting minimum ﬁll-in ordering randomized iterative version. number instances solved within given time seconds elim and/or search message passing algorithms trees described extension variable elimination marginals max-marginals variables computed double pass algorithm. depict principle tree ﬁrst marginal computation. beginning ﬁrst pass leaf marked processed variables unprocessed. leaf successively visited potential considered message sent denoted µi→pa. message potential function messages moved upward nodes subgraph deﬁned unmarked variables. variable marked processed received messages. variable remains unmarked combination functions variable equal marginal unnormalised distribution variable. results directly fact operations performed forward pass message compute marginal another variable redirect tree using variable root. subtrees remain unchanged tree messages subtrees need recomputed. second pass message passing algorithm exploits fact messages shared several marginal computations organise computations clever order compute marginals variables enough second pass send messages root towards leaf. marginal computed combining downward messages upward messages arriving particular vertex. application well-known forward-backward algorithm formally message passing algorithm marginal evaluation tree messages µi→j deﬁned edge leaves-to-root-to-leaves order; messages edge direction. messages µi→j functions computed iteratively following algorithm then messages sent upward edges. message updates performed iteratively marked nodes unmarked neighbour edge message updates take following form max-product max-sum algorithms equivalently deﬁned tree exact computation max-marginal joint distribution logarithm algebraic language updates deﬁned formula take general form cases graph underlying model tree corresponding factor graph tree factors potentially involving variables cases message passing algorithm still deﬁned lead exact marginal value computations however complexity becomes exponential size largest factor minus message passing algorithm tree structured factor graph exploits idea shared messages case tree structured graphical models except different kinds messages computed figure left graphical model structure tree. right corresponding factor graph tree. applying message passing root variable variables leaves. left branch ﬁrst messages sent ν→{} followed µ{}→ factor graph graphical model tree two-pass message passing algorithm applied directly loops. general graphical models message passing approach generalised different ways. tree decomposition computed previously discussed section message passing applied resulting cluster tree handling cluster crossproduct variables following block-by-block approach. yields exact algorithm computations expensive space intensive typical example algorithm algebraic exact message passing algorithm edges termination condition met. algorithm returns approximations marginal probabilities quality approximation convergence steadystate messages guaranteed hence importance termination condition. however observed often provides good estimates marginals practice. deeper analysis loopy belief propagation algorithm postponed section possible message passing re-parametrisation technique. case computed messages directly used reformulate original graphical model equivalent graphical model graphical structure. equivalent mean potential functions deﬁne joint distribution original graphical model. several methods re-parametrisation proposed ﬁeld probabilistic graphical models ﬁeld deterministic graphical models share advantage re-parameterised formulation computed satisfy precise requirements. designed reparameterised potential functions contains information interest max-marginals approximation). also optimised order tighten bound probability assignment partition function originally naive bounds tightened non-naive ones re-parametrisation. additional advantage re-parametrised distribution context incremental updates perform inference based observation variables observations introduced incrementally. since re-parameterised model already includes result previous inferences interesting perform updated inference starting expression joint distribution original idea behind re-parametrisation conceptually simple message µi→j computed instead keeping message possible combine potential function involving µi→j using preserve joint distribution deﬁned original graphical model need divide another potential function involving message µi→j using inverse example computation max-marginals. illustrate re-parametrisation exploited extract directly max-marginals order potentials model. case divided µi→j multiplied µi→j. procedure replacing message deﬁnition obtain singleton marginals instead. table unnormalised probabilities eight possible states original reparameterised models. check re-parameterised version describes joint distribution original one. original re-parametrisation compute pairwise cluster joint distributions. possibility incorporate messages binary potentials order extract directly pairwise joint distributions described koller friedman replaced µi→j µj→i divided µj→i µi→j. example sum-prod messages computed re-parameterised pairwise potential shown equal marginal distribution tree-structured problems resulting graphical model said calibrated emphasise fact pairs binary potentials sharing common variable agree marginal distribution common variable loopy case exact approach using tree decomposition followed domains messages size exponential size intersection pairs clusters re-parametrisation create potentials size. messages included inside clusters. resulting cluster potential marginal joint distribution cluster variables. again re-parameterised graphical model tree-decomposition calibrated intersecting clusters agree marginals. exploited lauritzen-spiegelhalter jensen sum-product-divide algorithms besides interest incremental updates context local loopy approach used instead re-parameterisations change scopes provide re-parameterised model. estimates marginals original model read directly. re-parameterisations follow clever update rules provide convergent re-parameterisations maximising well deﬁned criterion. typical examples process sequential version tree re-weighted algorithm max-product linear programming algorithm aims optimising bound non-normalised probability mode. algorithms exact graphical models loops provided potential functions submodular re-parametrisation deterministic graphical models. re-parameterising message passing algorithms also used deterministic graphical models. known local consistency enforcing constraint propagation algorithms. side local consistency property deﬁnes targeted calibration property. side enforcing algorithm uses so-called equivalence preserving transformations transform original network equivalent network i.e. deﬁning joint function satisﬁes desired calibration/local consistency property. similar consistency usual form local consistency related unit propagation consistency exact trees usually incrementally maintained exact tree search using re-parametrisation. idempotency logical operators local consistencies always converge unique ﬁx-point. local consistency properties algorithms weighted csps closely related message passing map. however always convergent thanks suitable calibration properties also solve tree structured problems problems potential functions submodular. algorithms directly used tackle max-prod sum-prod problems mrf. re-parametrised often informative original one. instance simple conditions potential functions scope larger naive upper bound considerably tightened re-parameterising using soft-arc consistency algorithm mainly discussed methods exact inference graphical models. useful order variable elimination small treewidth available. many real life applications interaction network seldom tree-shaped treewidth large consequently exact methods cannot applied anymore. however drawn inspiration derive heuristic methods inference applied graphical model. meant heuristic method algorithm derived optimisation particular criterion latter rather termed approximation method. nevertheless shall alleviate distinction show good performing message passing-based heuristics sometimes interpreted approximate methods. marginalisation task widespread heuristics derived variable elimination message passing principles loopy belief propagation last decade better understanding heuristics reached re-interpreted particular instances variational approximation methods variational approximation distribution deﬁned best approximation class tractable distributions according kullback-leibler divergence. depending application several choices considered. connection variable elimination principles treewidth obvious ﬁrst sight. however emphasised cast variational framework. treewidth chosen variational distribution depends nature variables case discrete variables treewidth need cases class formed independent variables associated treewidth equal works consider class associated treewidth equal case continuous variables treewidth variational distribution original model general chosen class multivariate gaussian distributions numerous inference tools available. recall components variational approximation method kullbackleibler divergence choice class tractable distributions. explain interpreted variational approximation method. finally recall rare examples statistical properties estimator obtained using variational approximation established. section illustrate variational methods used derive approximate algorithms estimation chmm. measures dissimilarity between probability distributions symmetric hence distance. positive null equal. consider constrained belong family include solution minq∈q best approximation according divergence. called variational distribution. tractable distributions inference marginals mode normalising constant used approximations quantities derive mean ﬁeld approximation corresponding class fully factorised distributions qi}. since variables binary corresponds joint distributions independent bernoulli variables respective parameters =def namely write −xi. optimal approximation within class distributions characterised qi’s minimise denoting expectation respect interpreted equal conditional distribution neighbouring variables ﬁxed expected values distribution explains name mean ﬁeld approximation. note general equal marginal choice class indeed critical trade-off opposite desirable properties must large enough guarantee good approximation small enough contain distributions inference manageable. next section particular choice bethe class emphasised. particular enables link heuristics variational methods. choices possible used. instance structured mean ﬁeld setting distribution factorial hidden markov model approximated variational approach; multivariate hidden state decoupled variational distribution conditional distribution hidden states independent markov chains chowliu algorithm computes minimum distribution whose associated graph spanning tree graph amounts computing best approximation among graphical models treewidth equal finally alternative treewidth reduction choose variational approximation class exponential distributions. applied gaussian process classiﬁcation using multivariate gaussian approximation posterior distribution hidden ﬁeld. method relies algorithm algorithm minimised instead choice minimising depends computational tractability. bethe approximation consists applying arbitrary graphical model formula free energy used tree minimising variables constraint probability distributions marginal extension bethe approximation interpreted variational method associated family qbethe unnormalised distributions qdi− coherent sets order expressed order marginals. yedidia established ﬁxed points stationary points problem minimising bethe free energy equivalently class qbethe distributions. furthermore yedidia showed class distributions corresponding particular method possible deﬁne generalised algorithm whose ﬁxed points stationary points problem minimising drawback algorithm extensions associated theoretical bound error made marginals approximations. nevertheless increasingly used inference graphical models good behaviour practice implemented software packages inference graphical models like libdai opengm maximum-likelihood parameter estimation graphical model often intractable could require compute marginals normalising constants. computationally efﬁcient alternative monte-carlo estimates variational estimates obtained using variational approximation model. statistical point-of-view variational estimation approximation maximum-likelihood estimation resulting parameter estimates beneﬁt typical properties maximum-likelihood estimates consistency asymptotic normality. unfortunately general theory exists variational estimates results available speciﬁc models general point view bayesian context wang titterington wang titterington studied properties variational estimates. proved approximate conditional distribution centred true posterior mean small variance. celisse proved consistency variational estimates stochastic block model gazal empirically established accuracy bayesian counterpart. variational bayes estimates also proposed jaakkola jordan logistic regression approximate posterior also turns accurate. heuristic explanation positive examples that cases class used approximate conditional distribution sought asymptotically contain true conditional distribution. last section illustrate different discussed algorithms chmm framework perform practice marginal inference model parameters known concretely exploited algorithm perform parameter estimation. compared following inference algorithms problem computing marginals hidden variables chmm model pest propagation described section conditionally observed variables. simulated datasets following parameters values data following algorithms using libdai software junction tree loopy belief propagation mean ﬁeld approximation gibbs sampling runs burn-in iterations iterations. compared algorithms three criteria running time mean absolute difference true marginal probability state estimated hidden variables percentage hidden variables restored true value mode estimated marginal results presented increasing values number rows square grid ﬁelds table comparison junction tree loopy belief propagation mean field gibbs sampling inference algorithms chmm model pest propagation running time second; mean difference true estimated marginal state percentage hidden variables restored true value using mode marginals. beyond cannot computing diff-marg used marginals instead true marginals. results illustrate well fact approximate inference methods based principle variable elimination time efﬁcient compared monte-carlo methods still accurate. furthermore even naive variational method like mean ﬁeld interesting accurate marginal estimates required interested preserving mode. exact algorithm chmm examples incomplete data models involve variables variables observed. maximum likelihood inference model aims ﬁnding value parameters maximise likelihood observed data i.e. solve maxθ prθ. popular algorithm achieve task algorithm formulation reads iterative maximiq stands distribution hidden variables stands expectation arbitrary distribution algorithm consists alternatively maximising respect solution e-step since kullback-leibler divergence minimal even null case. replacing global trant). writing equivalent merging hidden variables given time step. corresponds graphical model given figure denoting number possible values hidden variables regular hidden markov model possible hidden states. mode computed exact manner either forward-backward recursion viterbi algorithm mode evaluation. procedures complexity exact calculation therefore achieved provided remains small enough becomes intractable number signals exceeds tens. several variational approximations algorithm complex graphical structure explicitly determining expensive perform exactly. ﬁrst approach derive approximate e-step seek variational approximation assuming restricted family tractable distributions described section choice critical requires achieving acceptable balance approximation accuracy computation efﬁciency. choosing typically amounts breaking dependencies original distribution tractable distribution. case chmm simplest distribution class fully factorised distributions figure graphical representation mean-ﬁeld approximation coupled hmm. observed variables indicated light grey since part variational distribution distribution hidden variables. alternative bethe approximation algorithm used provide approximation conditional marginal distributions singletons pairs variables approach proposed heskes advantage approach compared variational approximations based families provides approximation joint conditional distribution pairs hidden variables within time step instead assuming independent. figure graphical representation approximation coupled independent heterogeneous markov chain. observed variables indicated light grey since part variational distribution distribution hidden variables. tutorial variable elimination exact approximate inference introduction basic concepts variable elimination message passing links variational methods. introduces ﬁelds statisticians confronted inference graphical models. main message exact inference systematically ruled out. looking efﬁcient approximate method wise advice would evaluate treewidth graphical model. practice question easy answer. nevertheless several algorithms exist provide upper bound treewidth together associated variable elimination order even optimal ordering used perform exact inference bound small enough. examples treewidth graphical model successfully exploited perform exact inference problems apparently complex numerous. korhonen parviainen simpliﬁed np-hard problem learning structure bayesian network data underlying network treewidth. proposed exact score-based algorithm learn graph structure using dynamic programming. berg compared approach encoding algorithm framework maximum satisﬁability improved performances classical machine learning datasets networks nodes. akutsu tackled problem boolean acyclic network completion. speciﬁcally achieve smallest number modiﬁcations network distribution consistent binary observations nodes. authors established general np-completeness problem even tree-structured networks. however reported problems solved polynomial time network bounded treewidth in-degree enough samples ﬁndings applied obtain sparsest possible modiﬁcations activation inhibition functions signalling network hypothesised cell-state alteration colorectal cancer patients. xing introduced bayesian probabilistic graphical modelling genomic data analysis devoted identiﬁcation motifs cis-regulatory modules transcriptional regulatory sequences haplotype inference genotypes snps inference high-dimensional models hybrid distributions complex compute. author noted exact computation might feasible models bounded tree-width good variable ordering available. however question latter addressed approximate generalised mean ﬁeld inference algorithm developed. finally reader berger illustration notion treewidth help simplifying parametrisation many algorithms bioinformatics. reader interested testing inference algorithms presented article list provided kevin murphy even though slightly out-dated gives good idea variety existing software packages dedicated particular family graphical model reason variable elimination based technique inference graphical model well widespread outside communities researchers computer science machine learning probably exist software generic easy interface python matlab. obviously tutorial exhaustive since chose focus fundamental concepts. many important results treewidth graphical models several decades area still lively broaden discussion recent works tackle challenges related computation treewidth. offer efﬁcient algorithms graphical models bounded treewidth offer attractive target learn model best represents given sample. kumar bach problem learning structure undirected graphical model bounded treewidth approximated convex optimisation problem. resulting algorithm polynomial time complexity. discussed kumar bach algorithm useful derive tractable candidate distributions variational approach enabling beyond usual variational distributions treewidth zero optimisation exact techniques offered tree search algorithms branch bound recursively consider possible conditioning variables. techniques often exploit limited variable elimination processing prevent exhaustive search either using message-passing like algorithms compute bounds used pruning performing on-the-ﬂy elimination variables small degree beyond pairwise potential functions time needed simple update rules message passing becomes exponential size scope potential functions. however speciﬁc potential functions involving many variables exact messages computed reasonable time even context convergent message passing optimisation. done using polytime graph optimisation algorithms shortest path mincost algorithms. functions known global potential functions probabilistic graphical models global cost functions deterministic cost function networks. different problems appear continuous variables counting requires integration functions. again speciﬁc families distributions exact computations obtained distributions conjugate distributions. message passing several solutions proposed. instance recent message passing scheme proposed noorshams wainwright relies combination orthogonal series approximation messages stochastic updates. refer reader references noorshams wainwright state-of-the-art alternative methods dealing continuous variables message passing. variational methods also largely exploited continuous variables particular signal processing finally excluded monte-carlo methods scope review. however combination inference methods presented article stochastic methods inference area researchers start exploring. recent sampling algorithms proposed exact optimisation algorithms sample points high probability context estimating partition function. additional control sampling method needed avoid biased estimations hashing functions enforcing fair sampling randomly perturbed potential functions using suitable noise distribution recently monte-carlo variational approaches combined propose discrete particle variational inference algorithm beneﬁts accuracy former rapidity latter.", "year": 2015}