{"title": "Scalable and Robust Sparse Subspace Clustering Using Randomized  Clustering and Multilayer Graphs", "tag": ["cs.CV", "stat.ML"], "abstract": "Sparse subspace clustering (SSC) is one of the current state-of-the-art methods for partitioning data points into the union of subspaces, with strong theoretical guarantees. However, it is not practical for large data sets as it requires solving a LASSO problem for each data point, where the number of variables in each LASSO problem is the number of data points. To improve the scalability of SSC, we propose to select a few sets of anchor points using a randomized hierarchical clustering method, and, for each set of anchor points, solve the LASSO problems for each data point allowing only anchor points to have a non-zero weight (this reduces drastically the number of variables). This generates a multilayer graph where each layer corresponds to a different set of anchor points. Using the Grassmann manifold of orthogonal matrices, the shared connectivity among the layers is summarized within a single subspace. Finally, we use $k$-means clustering within that subspace to cluster the data points, similarly as done by spectral clustering in SSC. We show on both synthetic and real-world data sets that the proposed method not only allows SSC to scale to large-scale data sets, but that it is also much more robust as it performs significantly better on noisy data and on data with close susbspaces and outliers, while it is not prone to oversegmentation.", "text": "sparse subspace clustering current state-of-the-art methods partitioning data points union subspaces strong theoretical guarantees. however practical large data sets requires solving lasso problem data point number variables lasso problem number data points. improve scalability propose select sets anchor points using randomized hierarchical clustering method anchor points solve lasso problems data point allowing anchor points non-zero weight generates multilayer graph layer corresponds diﬀerent anchor points. using grassmann manifold orthogonal matrices shared connectivity among layers summarized within single subspace. finally k-means clustering within subspace cluster data points similarly done spectral clustering ssc. show synthetic real-world data sets proposed method allows scale large-scale data sets also much robust performs signiﬁcantly better noisy data data close susbspaces outliers prone oversegmentation. finding low-dimensional subspace best represents high-dimensional data points fundamental problem many ﬁelds machine learning computer vision. fact dimensionality reduction essential tool understanding preprocessing data sets. using assumption data points fewer degrees freedom ambient high dimension several methods developed discover underlying low-dimensional structure. principal component analysis popular method matter however classical methods neglect fact data often contains data diﬀerent intrinsic structures. example facial images multiple individuals varying illumination conditions belong multiple manifolds/subspaces approximated multiple subspaces instead leads general problem often referred subspace clustering ∗department computer engineering information technology amirkabir university technology tehran iran. acknowledges support ministry science research technology iran. email mabdolaliaut.ac.ir †department mathematics operational research facult´e polytechnique universit´e mons houdain mons belgium. acknowledges support f.r.s.-fnrs email nicolas.gillisumons.ac.be formally subspace clustering problem deﬁned follows given data points union unknown subspaces unknown intrinsic dimensions goal partition/cluster data points according underlying subspaces estimate subspace parameters corresponding cluster. past decades many methods proposed deal problem. usually classiﬁed four categories iterative statistical algebraic spectral-based methods. iterative methods formulate problem non-convex optimization problem optimize using two-step iterative approach similarly k-means given initial clustering alternatively calculate basis subspace assign point closest subspace. despite simple intuitive convergence guaranteed local minimum highly sensitive noise outliers. statistical approaches treat problem modelling data mixture gaussian distributions. similar iterative methods statistical methods sensitive noise outliers. algebraic methods polynomials data points using algebraic geometric reformulation problem. however methods category sensitive noise outliers also scale well increase dimension data points. spectral-based methods inspired classical spectral clustering techniques based using spectrum specially constructed similarity matrix data points. main diﬀerence methods category similarity matrix constructed. global spectral based approaches spectral curvature clustering tend construct better similarity matrices compared local based alternatives however advances sparse representations references therein) methods attracted attention within spectral-based approaches. idea data point expressed sparse linear combination data points within subspace. property often referred self-expressiveness. three main representative methods category sparse subspace clustering low-rank representation based clustering least square regression three approaches based following model corresponding adjacency matrix |c|t note important strength methods need know dimensions di’s subspaces estimate number clusters done spectral clustering strong theoretical guarantees noisy noiseless cases independent disjoint subspaces behavior presence noise disjoint subspaces still well understood computationally less demanding compared guaranteed preserve subspaces independent. despite theoretical guarantees empirical success algorithm practical large real-world data sets samples bottleneck resolution large-scale convex optimization problem computational cost operations. overcome issue propose scalable approach referred scalable robust based solving several problems form rows allowed non-zero order generate multilayer graph merge using technique computational cost linear sr-ssc scale much larger data sets. main contributions paper propose novel scalable approach extends clustering large-scale data sets. ingredients novel method randomized hierarchical clustering algorithm allows identify diﬀerent sets anchor points good representatives data multilayer graph technique summarizes information across several graphs vertices. show method signiﬁcantly improves performance challenging cases noisy close subspaces overshadows oversegmentation issue ssc. paper organized follows. section brieﬂy review discuss related works proposing scalable ssc-like methods. section present approach makes scalable large data sets robust refer scalable robust investigate properties performance sr-ssc using synthetic real-world data sets section section concludes paper. several methods literature already addressed scalability ssc. earliest attempts method referred scalable applies randomly selected subset data points assigns rest data points based obtained clusters. approach sensitive selection method suboptimal data points selected taken account generating clusters. instead solving lasso problems authors greedy algorithm namely orthogonal matching pursuit obtain sparse representation data point. even though faster original -based scalable samples greedy method weaker theoretical guarantees also computational cost still high requiring operations. iteration approximate given data point residual orthogonal previously selected data points. property enforces constraints mixture norms used take advantage subspace preserving norm dense connectivity norm. later oracle-based algorithm dubbed oracle guided elastic solver proposed identify support sample eﬃciently. recently nearest neighbor ﬁltering approach presented choose support sample eﬃciently compared orgen. approach lasso problem restricted nearest neighbors chosen support point. even though authors provided theoretical guarantees correct connections noisy noiseless cases approach requires nearest neighbors point requires operations. additionally approach likely oversegment subspaces. illustrate simple example similar suppose points chosen around four circles -dimensional subspace follows columns input matrix form solving equivalent ﬁnding extreme points closest face polytope data point proved choosing suﬃciently large value vertices corresponding closest face point conclusion need eﬃcient algorithm computationally cheaper ideally running number data points worsening weaknesses. next section present approach. proposed approach scalable robust prone oversegmentation. allowed non-zero similarly methods presented section clearly choice anchor points plays critical role. moreover computational cost method choosing anchor points otherwise would gain throwing away rest points however choosing anchor points prior knowledge diﬃcult task. reduce role anchor points propose framework constructs multilayer graph based several sets well-chosen anchor points using randomization. overall diagram proposed framework shown figure details steps discussed following sections namely selection anchor points summarization multilayer graph. figure diagram summarizing scalable robust input data points corresponding data matrix selection several sets anchor points using randomized hierarchical clustering technique construction adjacency graphs corresponding anchor points solving allowing rows non-zero correspond anchor point summarization information among diﬀerent graphs using multilayer graph technique cluster data points ﬁnal subspace representation using k-means. section describe proposed method selecting sets anchor points. instead selecting anchor points fully randomly select well-spread data good representatives data points. simple randomized top-down hierarchical clustering technique used. works follows. start data points root node. step satisfy properties well-balanced stable technique proposed using simple quantify property fact nodes well-balanced measured using stability measured using construction tree continued number leaf nodes tree reaches number anchor points needed. node select anchor point data point closest average node centroid node. computational cost split node points dimension operations extract anchors points overall procedure hence require operations. however unless unbalanced clusters data child node contain roughly half data points parent node expected computational cost actually rather operations practice operations since level contains data points levels cases). selects anchor points good representatives data since located nearby centroids clusters generated distant points likely diﬀerent clusters nearby points. example using random sampling performs worse satisfy ﬁrst property above; section research includes design approaches perform selection anchor points retaining three properties. vertex corresponding graph spectral clustering identify single cluster. side result randomized anchor point selection step duplicated data points ﬁltered selected anchor points simultaneously choosing number anchor points depends many factors number subspaces dimensions aﬃnity subspaces distribution data points subspace level noise. hence choosing single ‘good’ anchor points done example highly non-trivial. leverage diﬃculty propose choose several sets diﬀerent anchor points described previous section. select sets anchor points data sets indices similarly allowing anchor points non-zero weight compute constraint ensures anchor point uses self representation ssc. optimization problem solved example using admm algorithm provide completeness admm good choice purpose computational cost iteration slow convergence bottleneck since high precision necessary need know order magnitude entries –also data usually rather noisy hence make much sense solve high precision. similarly ssc. note sparse matrix less non-zero coeﬃcients. need combine information individual graphs layer. adopt method presented merge information diﬀerent layers proper representation strengthens connectivity/information majority graphs tend agree brieﬂy describe technique. problem merging multilayer graph combined problem merging diﬀerent subspaces grassmann manifold. individual graph compute p-dimensional subspace representation matrix rn×p whose columns optimization problem ﬁnds subspace representation satisﬁes goals. ﬁrst term makes sure connectivity information individual graph preserved within subspace second term distance metric incites subspace close subspaces corresponding graph parameter balances terms. solution problem ﬁnal subspace representation obtained ﬁnding eigenvectors corresponding smallest eigenvalues procedure long chosen reasonable range. note choosing amounts naive strategy summing adjacency matrices diﬀerent layers turn perform rather poorly. choose anchor points form dictionary using randomized hierarchical procedure described section solve obtain construct symmetrized adjacency matrix compute normalized laplacien matrix compute eigenvectors rn×p corresponding smallest eigenvalues analyze computational cost algorithm sr-ssc. ﬁrst analyze loop performed times. identifying anchor points requires operations; section main computational cost solving using algorithm performed step cost operations compute operations compute inverse operations compute assuming number iterations algorithm since total computational cost solving using algorithm constructing done directly requires operations. compute eigenvectors arpack sparse eigenvalue solver non-zero entries) requires operations remains form deﬁned compute eigenvectors corresponding global laplacian matrix contains non-zero elements non-zero elements) second term rank since columns. computing eigenvalue decomposition sparse non-zero entries) plus fact arpack based implicitly restarted arnoldi method reduces implicitly restarted lanczos method matrix symmetric arnoldi/lanczos adaptations power method ﬁnding eigenvalues corresponding eigenvectors large-scale structured sparse matrix based simple iterations using matrix-vector multiplications. hence capable obtaining eigenvectors matrix explicit stored form available long matrix-vector multiplication done eﬃciently. total computational cost sr-ssc operations linear hence scalable large data sets long remain small. value since di’s usually unknown good choice pick multiple guess average dimension subspaces e.g. choice parameters regularization parameter depends directly algorithm balances importance data ﬁtting term sparsity coeﬃcient matrix. smaller leads fewer wrong connections whereas larger value increases number true connections imposes natural trade-oﬀ. words want small possible making sure leads suﬃcient connectivity. suggested smallest value coeﬃcient vector sample would zero. value leads best results data dependent. data subspaces small intrinsic dimension regularization parameter small enforce sparsity subspaces high intrinsic dimension chosen larger relax sparsity. hence suggested regularization parameter order paper value prescribed value leads reasonable results without giving particular attention tuning parameter. section evaluate performance sr-ssc synthetic real-world data sets. experiments implemented matlab computer intel core code available https//sites.google.com/site/nicolasgillis/ code. section investigate performance sr-ssc diﬀerent conditions depending choice parameters number graphs number anchor points value synthetic data sets consists three -dimensional spaces random samples generated linear combinations weights linear combinations chosen random using gaussian distribution mean variance finally i.i.d. random gaussian noise zero mean standard deviation added data matrix similar data points normalized norm equal one. aﬃnity subspaces deﬁned average cosine angles subspaces; shown larger aﬃnity diﬃcult identify right subspaces. particular synthetic data sets decreasing value increases aﬃnity hence makes subspace clustering task challenging. evaluating performance sr-ssc synthetic data value regularization parameter chosen minimum value avoids zero coeﬃcient matrix hence ﬁxed budget larger vale imply smaller value number anchor points note choice budget favours sr-ssc fewer graphs since computational cost sr-ssc linear section made choice simplicity conservative sense using graphs increase computational cost ﬁxed budget. mutlilayered graph technique parameter balances connectivity graph distances subspace representations; section authors recommended eﬀect sr-ssc synthetic data described diﬀerent values trials. observe performance quite stable values around rather interesting note that clear drop performance sr-ssc. words merging naively information laplacians layer summing together work well. section investigate performance sr-ssc depending number graphs number anchor points graph diﬀerent aﬃnities subspaces noise levels. show selecting multiple sets anchor points signiﬁcantly improve performance sr-ssc subspace clustering problem gets challenging synthetic data sets described previous section figure shows average accuracy sr-ssc generated synthetic data sets diﬀerent values budget diﬀerent numbers graphs values performance ssr-ssc graph decreases budget increases; similar observation already reported fact aﬃnity increases chances choosing wrong connections increases well. however multilayered graph structure reduces eﬀect seeks connections agreed majority individual graphs. fact budget suﬃciently large using graph improves performance sr-ssc signiﬁcantly. example budget sr-ssc graph average accuracy graphs average accuracy observe increasing number layers number anchor points increases average accuracy also reduces variance signiﬁcantly. clearly increasing number layers chance choosing good anchor points hence constructing consistent graphs increases. indicates natural trade-oﬀ computational cost increase performance sr-ssc. noise main factor makes subspace clustering challenging. study eﬀect interesting compare performance randomized hierarchical clustering able identify well-spread anchor points picking anchor points uniformly random. figure completely random selection anchor points. comparing figure observe performance fully randomized selection data points worse. example budget accuracy goes graph graphs graphs graphs graphs stable. budget accuracy goes graphs graphs graphs graph stable. deviation outlier normalized unit norm done ‘this guarantees outlier detection trivially accomplished exploiting diﬀerences norms inliers outliers’. value budget evaluate average performance sr-ssc correctly clustering inliers diﬀerent layers graphs trials observe aﬃnity increases percentage allowed outliers maintain accuracy decreases. moreover observe adding layers graph leads robust stable performance percentage allowed outliers maintain important issue connectivity graphs corresponding data points within subspace. particular guarantee dimensions higher four points subspace form single connected component section show choosing anchor points representative samples oversegmentation issue alleviated. purpose points subspaces -dimensional space created similarly precisely consider points subspace chosen around orthogonal circles. half points ﬁrst subspace given point deﬁned non-zero weights correspond four points implies connections points within subspace similarly graph corresponding coeﬃcient matrix shown figure graph correfigure sponding coeﬃcient matrix standard four connected components. graph corresponding coeﬃcient matrix sr-ssc connected components. dots indicate anchor points. chance forming single connected component similarity graph increases. observe choosing samples data graph corresponding coeﬃcient matrix shown figure sr-ssc accuracy section performance sr-ssc evaluated using three large-scale challenging data sets handwritten digits object images forest data compare accuracy running times sr-ssc standard based admm three state-of-the methods sparse subspace clustering namely sssc orgen section brief description methods. orgen used code available http//vision.jhu.edu/code/. mnist database contains grey scale images handwritten digits size pixels. image using scattering convolution network feature vector dimension extracted projected dimension done nine data sets corresponding diﬀerent number digits formed around data points digit. also consider data sets similar digits chosen minimum value avoids zero coeﬃcient matrix chosen approach orgen sssc. investigate behaviour plotting confusion matrix orgen digits figure similarity digits adding digit methods oversegment cluster containing digit failing distinguish digits conﬁrmed performance unlike digit digit cluster forms well connected component digits also well separated. conﬁrms previous results synthetic data sets section showed proposed method performs well challenging case close subspaces overshadows oversegmentation eﬀect. table accuracy diﬀerent clustering methods diﬀerent sets digits mnist data set. value indicates memory. best accuracy indicated bold second best underlined. performance sr-ssc diﬀerent parameters shown tabel sets parameters observe adding anchor points increases accuracy almost cases. however performance quite stable among diﬀerent settings adding digit sr-ssc still able perform well even anchor points cluster. similar performances cases highlight fact multilayer framework improve performance long enough anchor points chosen individual layer. hence performance srssc depends main factors well-spread suﬃciently many anchor points suﬃciently many number layers best summarize informative shared connectivity among diﬀerent layers. performance algorithms compared challenging cifar data set. cifar consists -by- colored images objects. image converted grayscale scattering convolution network used extract feature vectors projected dimension using pca. diﬀerent sets data corresponding diﬀerent numbers clusters formed. cluster samples corresponding class selected randomly. consider layers graphs method anchor points selected graph. sr-ssc highest accuracy cases takes less minute cluster data points less minutes cluster points. performance sr-ssc slightly higher graphs compared graphs diﬀerence signiﬁcant. cifar challenging data linearity assumption subspaces violated explains relative accuracy variants. however sr-ssc still manages outperform variants. furthermore compared performances four extra combinations clusters bottom table clear drop performance clustering compared compared explained close aﬃnity custers compared terms computational time sr-ssc compares favorably sssc mnist. computational time orgen signiﬁcantly higher compared scalable approaches data set. adaptive support selection method orgen depends correlation data points oracle point. high correlation among data points cifar decreases amount discarded data points iteration plays crucial role computational cost algorithm. table accuracy diﬀerent clustering methods diﬀerent sets clusters cifar data set. value indicates memory. best accuracy indicated bold second best underlined. covertype data contains samples categories tree types. sample categorical/integer attributes derived data originally obtained geological survey usfs data. sparsity parameter algorithm regularization parameter orgen sr-ssc. results reported tables orgen achieves best accuracy much higher computational cost sr-ssc oﬀers best trade-oﬀ computational time accuracy. data accuracy sensitive number graphs anchor points. note paper proposed framework overcome scalability issue ssc. proposed framework referred scalable robust constructs multilayer graph solving lasso problems using diﬀerent sets dictionaries. fast hierarchical clustering method used select anchor points within dictionaries good representatives data set. screening large numbers data points drastically reduces computational cost memory requirements ssc. moreover multilayer structure ability obtain summarized subspace representation diﬀerent layers graphs emphasizes shared information among diﬀerent layers. experimental results synthetic large-scale real-world data sets showed eﬃciency sr-ssc especially challenging cases noisy data close subspaces. moreover choosing common representative points proposed framework ability overshadow oversegmentation problem ssc. course trade-oﬀ computational cost robustness sr-ssc carefully balanced. observed practice using graphs good choice number anchor point proportional number clusters dimensions. work include improvement steps sr-ssc namely selection sets anchor points summarization multilayer graph. example choosing diﬀerent sets anchor points independently would particularly interesting order make diﬀerent layers complementary possible. also studying theoretical properties sr-ssc would particular promising direction research. another direction research improve implementation sr-ssc; particular sr-ssc especially amenable parallelization constructions diﬀerent layers multilayer graph independent.", "year": 2018}