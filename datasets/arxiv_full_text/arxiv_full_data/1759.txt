{"title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization", "tag": ["stat.ML", "cs.CL", "cs.IR", "cs.LG"], "abstract": "Automated feature selection is important for text categorization to reduce the feature size and to speed up the learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches.", "text": "abstract—automated feature selection important text categorization reduce feature size speed learning process classiﬁers. paper present novel efﬁcient feature selection framework based information theory aims rank features discriminative capacity classiﬁcation. ﬁrst revisit information measures kullback-leibler divergence jeffreys divergence binary hypothesis testing analyze asymptotic properties relating type type errors bayesian classiﬁer. introduce divergence measure called jeffreys-multi-hypothesis divergence measure multi-distribution divergence multi-class classiﬁcation. based jmh-divergence develop efﬁcient feature selection methods termed maximum discrimination methods text categorization. promising results extensive experiments demonstrate effectiveness proposed approaches. great importance label contents predeﬁned thematic categories automatic also known automated text categorization. last decades growing number advanced machine learning algorithms developed address challenging task formulating classiﬁcation problem commonly automatic text classiﬁer built learning process prelabeled documents. documents need represented suitable general learning process. widely used representation words document represented vector features corresponds term phrase vocabulary collected particular data set. value feature element represents importance term document according speciﬁc feature measurement. challenge text categorization learning high dimensional data. hand tens hundreds thousands terms document lead high computational burden learning process. hand irrelevant redundant features hurt predictive performance classiﬁers text categorization. avoid issue curse dimensionality speed learning process necessary perform feature reduction reduce size features. common feature reduction approach text categorization feature selection paper concentrates subset original features selected input learning algorithms. last decades number feature selection methods proposed usually categorized following types approach ﬁlter approach wrapper approach ﬁlter approach selects feature subsets based general characteristics data without involving learning algorithms selected features. score indicating importance term assigned individual feature based independent evaluation criterion distance measure entropy measure dependency measure consistency measure. hence ﬁlter approach selects number ranked features ignores rest. alternatively wrapper approach greedily searches better features evaluation criterion based learning algorithm. although shown wrapper approach usually performs better ﬁlter approach much computational cost ﬁlter approach sometimes makes impractical. typically ﬁlter approach predominantly used text categorization simplicity efﬁciency. however ﬁlter approach evaluates goodness feature exploiting intrinsic characteristics training data without considering learning algorithm discrimination lead undesired classiﬁcation performance. given speciﬁc learning algorithm hard select best ﬁlter approach producing features classiﬁer performs better others discrimination viewpoint theoretical analysis. paper present feature selection method ranks original features aiming maximize discriminative performance text categorization naive bayes classiﬁers used learning algorithms. unlike existing ﬁlter approaches method evaluates goodness feature without training classiﬁer explicitly selects features offer maximum discrimination terms divergence measure. speciﬁcally contributions paper follows propose efﬁcient approach rank order features approximately produce maximum divergence. theoretical analysis shows divergence monotonically increasing features selected. analyze asymptotic distribution proposed test statistic leads distribution. introduce another simple effective feature ranking approach maximizing noncentrality measurement noncentral distribution. rest paper organized follows section introduce previous work naive bayes classiﬁers feature selection techniques automatic text categorization. section present theoretical framework feature selection using information measures. section introduce efﬁcient feature selection approaches naive bayes classiﬁers text categorization. experimental results given section along performance analysis compared state-of-the-art methods. conclusion future work discussion given section previous work document representation text categorization document commonly represented model bag-of-words feature vector i-th element corresponds measure i-th term vocabulary dictionary. given data ﬁrst generate vocabulary unique terms documents. then document feature vector formed using various feature models. typically value feature represents information particular term document. feature models widely used. ﬁrst binary feature model feature takes value either corresponding presence absence particular term document. distribution binary feature class usually modeled bernoulli distribution. multi-value feature model feature takes values ···} corresponding number occurrences particular term document thus also called term frequency distribution class usually modeled multinomial distribution model. note several feature models also exist literature normalized term frequency inverse document frequency probabilistic structure representation recent work learning vector representations words using neural networks shown superior performance classiﬁcation clustering ordering semantics words considered. naive bayes naive bayes classiﬁer widely used text categorization simplicity efﬁciency model-based classiﬁcation method offers competitive classiﬁcation performance text categorization compared data-driven classiﬁcation methods neural network support vector machine logistic regression k-nearest neighbors. naive bayes applies bayes’ theorem naive assumption pair features independent given class. classiﬁcation decision made based upon maximum-a-posteriori rule. usually three distribution models including bernoulli model multinomial model poisson model commonly incorporated bayesian framework resulted classiﬁers bernoulli naive bayes multinomial naive bayes poisson naive bayes respectively. extensive experiments real-life benchmarks shown usually outperforms large vocabulary size similar conclusions drawn also shown equivalent document length document class assumed independent reason naive bayes usually refers classiﬁer. paper concentrate formulation proposed feature selection method classiﬁer. methods easily extended classiﬁers. would best-known naive bayes classiﬁcation approaches using term frequency represent document. considering text categorization problem classes discrete variable class label taking values integer-valued feature vector corresponding term frequency. classiﬁer assumes number times term appears document satisﬁes multinomial distribution words document feature selection feature selection also called term selection widely adopted approach dimensionality reduction text categorization. given predetermined integer number terms selected feature selection approach attempts select terms original vocabulary. yang pedersen shown employment feature selection remove unique terms without hurting classiﬁcation performance much thus feature selection greatly reduce computational burden classiﬁcation. kohavi john used so-called wrapper feature selection approach feature either added removed step towards optimal feature subset selection. feature generated classiﬁer re-trained features tested validation set. approach advantage searching feature space greedy always able better feature subset sense improved classiﬁcation performance. however high computational cost makes prohibitive practical text categorization applications. alternative popular ﬁlter approach feature assigned score based importance measure ranked features highest scores kept. advantage approach easy implementation computational cost. rest section describe several state-of-the-art measures widely used text categorization. document frequency number documents term occurs simple effective feature selection approach. removes original feature space rare terms considered non-informative classiﬁcation. effectiveness approach also inspired researchers experiments remove terms occur hence given document ﬁrst count number times term appears generate feature vector according multinomial distribution likelihood observing conditioned class label document length calculated follows number times i-th term appears among documents class total number terms class avoid zero probability issue technique laplace smoothing prior information probability usually applied leads note document length commonly assumed independent document class simplify naive bayes classiﬁcation rule likelihood posterior probability respectively. otherwise leads general formulation posterior probability classiﬁcation given class-wise document length information incorporated bayesian fashion. document length information sometimes useful making classiﬁcation decisions e.g. classwise document length distributions different requires estimate given data set. follow common assumption document length constant experiments i.e. note solutions times training preprocessing stage. tens hundreds rare features removed step feature selection. tf-idf measure considers term frequency inverse document frequency calculate importance features recently generalized tf-idf measure proposed considering different level hierarchies among words effectively analyze tweet user behaviors. many ﬁlter approaches based information theory measures including mutual information information gain relevancy score chi-square statistic odds ratio expected cross entropy text coefﬁcient name few. describe measures below. denotes probability term appears document document belongs category probability term appears document probability document belongs category zero independent i.e. term useless discriminating documents belonging category expected cross entropy text proposed denotes probability term appear document document belongs category probability term appear document. unlike criterion criterion less inﬂuenced frequency terms usually performs much better criterion chi-square statistic proposed measure lack independence term category modeled chi-square distribution. considering negative evidence term document general statistic measure deﬁned document space divided categories complement pools remaining categories denotes probability term appear document also document belong category denotes probability term appears document document belong category modiﬁed measure termed coefﬁcient using negative evidence proposed galavotti deﬁned shown measure outperforms original chi-square measure several data sets notice almost ﬁlter approaches based information theory measures binary variables e.g. presence absence term document document belonging category unlike existing ﬁlter approaches proposed approaches make term occurrence measure term importance document hence richer information contained. meanwhile existing ﬁlter approaches rank features exploring intrinsic characteristics data based feature relevancy without considering discriminative information classiﬁers. difﬁcult select optimal feature subset discrimination theoretical way. paper feature selection approaches able involve learning algorithm maximizing discriminative capacity. theoretical framework feature selection follow information theory select feature subsets maximum discriminative capacity distinguishing samples among classes. ﬁrst introduce concepts information measures binary hypothesis testing present divergence measure multiple hypothesis testing divergence measures binary hypothesis testing considering two-class classiﬁcation problem ﬁrst class represented particular distribution saying class class test procedure classiﬁcation considered binary hypothesis testing sample drawn accept hypothesis sample drawn accept words also denote indicates bayesian classiﬁer’s discriminative capacity discriminating observation class favor similarly rule considering observation drawn class purpose feature selection determine informative features lead best prediction performance. hence natural select features maximum discriminative capacity classiﬁcation minimizing classiﬁcation error however j-divergence deﬁned binary hypothesis. next extend jdivergence multiple hypothesis testing jeffreys-multi-hypothesis divergence jensen-shannon divergence used measure multi-distribution divergence divergences individual distribution reference distribution calculated summed together. unlike j-divergence measure discrimination capacity hold. sawyer presents variant j-divergence variance-covariance matrix multiple comparisons separate hypotheses. here ﬁrst generalize j-divergence multi-distribution using scheme one-vs-all deﬁned follows deﬁnition p··· distributions. jeffreys-multi-hypothesis denotes expectation respect probability distribution speciﬁcally easy obtain kl-divergence measure discrete distributions commonly used text categorization. according kl-divergence multinomial distributions logarithm likelihood ratio measures information observation discrimination favor deﬁnition kl-divergence measure indicates mean information discrimination favor also illustrates indicator bayesian classiﬁer’s discriminative capacity discriminating observation class favor extension central limit theorem chernoff showed that large number observations type error probability incorrectly accepting asymptotically denotes independent observations. said larger value kl-divergence indicates lower type error inﬁnite number observations. note kl-divergence measure symmetric. alternatively kl-divergence combinations intractable particularly high dimensional data set. meanwhile practice need examine various values evaluate classiﬁcation performance using selected features. hence necessary assign importance score feature rank features. here start propose greedy approach rank features according discriminative capacity naive bayes. approach starts determine feature features produces maximum jmh-divergence single feature used classiﬁcation. determine discrminative feature feature build variables original i-th feature pool remaining features parameters superscript number indicate k-th step greedy approach denote distributions variables class class respectively. note also satisfy multinomial distribution different parameters. then calculate j-divergence step obtain j-divergences choose ﬁrst feature indexed leads maximum j-divergence then ﬁrst feature repeat process remaining features. speciﬁcally k-th step select k-th feature s··· sk−} feature index selected previous steps. again individual feature form variables xs··· xsk− ﬁrst variables original features last variable pool remaining similar one-vs-all strategy multi-class classiﬁcation problem build binary hypothesis testing detectors discriminates samples favor complement detector represented mixture distribution remaining classes coefﬁcients given prior probability class since kl-divergence detector measure discriminative capacity discrimination multi-distribution divergence able measure discrimination capacity classes. speciﬁcally note that since divergence multiple j-divergences holds properties j-divergence. example divergence alpositive deﬁnite i.e. equality also symmetric measure consider binary classiﬁcation problem ﬁrst extend feature selection method general multi-class classiﬁcation problem later. unlike existing feature selection methods compute score features based feature relevance class goal select features offer maximum discrimination classiﬁcation. expect improved classiﬁcation performance text categorization. two-class classiﬁcation problem know j-divergence indicates discriminative capacity discriminating classes data rule. hence formulate feature selection problem follows given features predetermined integer number features selected |b∗| discriminative features hence m-th step ranked feature index s··· produced. implementation greedy feature selection approach based maximum j-divergence two-class classiﬁcation given algorithm theorem maximum j-divergences algorithm monotonically increases i.e. proof theorem provided supplemental material. theorem indicates discriminative capacity increases features used classiﬁcation assumption term occurrence document satisﬁes particular multinomial distribution. note proposed greedy feature selection algorithm makes locally optimal choice step approximate global optimal solution selecting feature greedy approach considered wrapper approach. however unlike existing wrapper approaches greedy approach need evaluate classiﬁcation performance validation data retraining classiﬁer feature generated closed form kl-divergence given measure discriminative capacity classiﬁers. however greedy approach still computational complexity leads heavy computational load high-dimensional data set. next provide efﬁcient feature selection approach text categorization. efﬁcient feature selection approach algorithm best single feature selected ﬁrst step providing optimal starting point approximate optimal solution. step rank j-divergences features given instead greedy search efﬁcient ranked feature index e··· summarize efﬁcient feature selection algorithm algorithm compared algorithm proposed approach much efﬁcient feature score calculated once computational complexity jmhdivergence deﬁned feature subset note kl-divergence indicates discriminative capacity binary classiﬁer distinguish samples class samples remaining classes thus jmh-divergence able measure difﬁculty capacity discriminating samples among classes. efﬁcient feature selection method n-class classiﬁcation problem based maximum jmhdivergence presented algorithm value jmh-divergence used score feature. sorting feature scores descend output ranked feature index e··· multi-class classiﬁcation. computational complexity algorithm feature selection based statistics kl-divergence measure also known minimum discrimination information probability distributions suppose random sample observations test null hypothesis observation drawn class distribution alternative hypothesis observation drawn class efﬁcient approach evaluates importance individual feature measuring discriminative capacity single feature used classiﬁcation. note theorem also satisﬁed efﬁcient approach i.e. j-divergence measure increases features selected. meanwhile also note features selected algorithm necessarily ones selected algorithm example approximately hold ﬁrst second feature selection. considering ﬁrst feature selection example approximation depends value deﬁned multi-class classiﬁcation section extend efﬁcient feature selection method multi-class classiﬁcation problems. considering n-class classiﬁcation problem discriminative features selected maximizing jmh-divergence given also observe chi-squared statistic performs better real-life text data sets. therefore practice would like recommend approaches simplicity efﬁciency improved discrimination performance. experimental results analysis real-life data sets experiments test proposed feature selection approaches three benchmarks prepared deng text categorization -newsgroups reuters topic detection tracking three benchmarks widely used literature performance evaluation. -newsgroups benchmark consists documents collected postings different online newsgroups topics. reuters originally contains documents topics documents belong multiple topics. experiments modapte version reuters removing documents multiple labels. version consists documents topics. following work form three data sets named reuters- reuters- reuters- consisting documents ﬁrst topics respectively. benchmark consists documents taken newswires radio programs television programs also documents belong topics removed. extremely imbalanced data categories ﬁrst topics largest data size data set. data sets used experiments ignore words stoplist discard words appear less documents messages preprocessing stage. data sets except perform classiﬁcation ofﬁcially split training testing data sets. -fold cross validation performance evaluation reported results averaged runs. performance evaluation metrics following metrics evaluate classiﬁcation performance accuracy precision recall measure. accuracy metric widely used machine learning ﬁelds indicates overall classiﬁcation performance. precision percentage documents correctly classiﬁed estimate given observations assumed known. reject null hypothesis accept alternative hypothesis value statistic exceeds predetermined threshold. asymptotically statistic null hypothesis satisﬁes central chi-squared distribution degrees freedom satisﬁes non-central chisquared distribution alternative hypothesis noncentrality parameter given thus noncentrality parameter would also good sign indicate discriminative capacity discriminating therefore select features maximizing noncentrality parameter binary classiﬁcation. assumption number samples training data goes inﬁnity estimation training data multi-class classiﬁcation problem unlike algorithm feature noncentrality parameters distributions ranked feature index e··· produced sorting scores need note feature selection approach based upon chi-squared statistic equivalent approach algorithm inﬁnite training documents. assumption large numbers satisﬁed features selected lose discriminative capacity. demonstrated extensive experiments discrimination performance chi-squared statistic usually bounded approach algorithm however sometimes positive documents classiﬁed positive recall percentage documents correctly classiﬁed positive documents actually positive. metrics precision recall deﬁned denotes number true positive denotes number false positive denotes number false negative. metrics inverse relationship other. words increasing precision cost reducing recall vice versa. among measures attempt combine precision recall single measure measure popular deﬁned metrics precision recall measure originally deﬁned binary class. multi-class classiﬁcation follow several studies binary classiﬁers built individual class global measure obtained averaging measure class weighted class prior. results compare efﬁcient feature selection approaches maximum discrimination termed asymptotic statistic termed md-χ state-of-the-art feature ranking methods including document frequency expected cross entropy text statistic gss. carry experiments three benchmarks naive bayes used classiﬁers. compare performance feature selection methods evaluate classiﬁcation accuracy measure metric classiﬁers different number features ranging feature selection approaches naive bayes used classiﬁer. fig. shows results -newsgroups data set. shown performance improved features selected. proposed approaches commonly perform better others. zoomed-in ﬁgure proposed method performs better others. method worst data set. show results fig. data alt-comp subset -newsgroups categories alt.* categories comp.*. data md-χ best among others. comparison measure alt-comp given fig. shows proposed asymptotic statistic best approaches. comparing results fig. signiﬁcant difference performance behavior although alt-comp subset -newsgroups. example accuracy -newsgroups data even lower small subset features selected accuracy alt-comp data higher might indicate diverse feature characteristic -newsgroups data set. newsgroups data topics belong category closely related other e.g. rec.sport.baseball rec.sport.hockey comp.sys.mac.hardware comp.sys.ibm.hardware etc. also notice md-χ method performs better method fig. might always true data sets since method based asymptotic distribution statistic used method. possible explanation correlation among words holds discriminative information method assumes words independent other. however asymptotic distribution still hold large number. theoretical supports determine method performs better need study. fig. shows comparison results three data sets reuters reuters- reuters- reuters-. shown proposed approaches ﬁrst selected features achieve similar performance four existing approaches ﬁrst selected features. fig. shows results data set. classiﬁcation tasks data performed scheme -fold cross validation results fig. averaged across runs. shown fig. proposed md-χ outperform others respect metric accuracy. interesting notice that ﬁrst features selected obtains accuracy md-χ methods ﬁrst features need selected achieve classiﬁcation accuracy. future work analyze feature dependence develop feature selection algorithms weighting individual features aiming maximize discriminative capacity. furthermore incorporate feature selection approaches advanced machine learning algorithms imbalanced learning partial learning model enhance learning rare categories. references joachims text categorization support vector machines learning many relevant features ecml ruiz srinivasan automatic text categorization application text retrieval ieee transactions knowledge data engineering vol. sebastiani machine learning automated text categorization computing surveys vol. al-mubaid umair text categorization technique using distributional clustering learning logic ieee transactions knowledge data engineering vol. aphinyanaphongs peskin efstathiadis aliferis statnikov comprehensive empirical comparison modern supervised classiﬁcation feature selection methods text categorization journal association information science technology vol. g¨overt lalmas fuhr probabilistic description-oriented approach categorizing documents proceedings eighth international conference information knowledge management turian ratinov bengio word representations simple general method semi-supervised learning proceedings annual meeting association computational linguistics. association computational linguistics text categorization. fig. shows classiﬁcation results three reuters data sets performance improvement proposed approaches also seen. shows proposed approaches perform least well previously existing methods small feature size consistently better feature size increases. conclusions future works introduced feature selection approaches based information measures naive bayes classiﬁers aiming select features offer maximum discriminative capacity text classiﬁcation. also derived asymptotic distributions measures leads version chi-square statistic approach feature selection. compared existing feature selection approaches rank features exploring intrinsic characteristics data without considering learning algorithm classiﬁcation proposed approaches involve learning model feature ﬁltering process provides theoretical analyze optimality selected features. experiments conducted several benchmarks demonstrated promising performance improvement compared previously existing feature selection approaches. caropreso matwin sebastiani learnerindependent evaluation usefulness statistical phrases automated text categorization text databases document management theory practice galavotti sebastiani simi experiments feature selection negative evidence automated text categorization research advanced technology digital libraries kohavi john wrappers feature subset selection artiﬁcial intelligence vol. salton wong c.-s. yang vector space model automatic indexing communications vol. combarro montanes diaz ranilla mones introducing family linear measures feature selection text categorization ieee transactions knowledge data engineering vol.", "year": 2016}