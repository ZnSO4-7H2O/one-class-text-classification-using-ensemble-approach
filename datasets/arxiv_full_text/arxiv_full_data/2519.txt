{"title": "Clustering With Side Information: From a Probabilistic Model to a  Deterministic Algorithm", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.CO"], "abstract": "In this paper, we propose a model-based clustering method (TVClust) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data. Our method is based on a nonparametric Bayesian hierarchical model that combines the probabilistic model for the data instance and the one for the side-information. An efficient Gibbs sampling algorithm is proposed for posterior inference. Using the small-variance asymptotics of our probabilistic model, we then derive a new deterministic clustering algorithm (RDP-means). It can be viewed as an extension of K-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be specified a priori. Empirical studies have been carried out to compare our work with many constrained clustering algorithms from the literature on both a variety of data sets and under a variety of conditions such as using noisy side information and erroneous k values. The results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature.", "text": "paper propose model-based clustering method robustly incorporates noisy side information soft-constraints aims seek consensus side information observed data. method based nonparametric bayesian hierarchical model combines probabilistic model data instances side-information. eﬃcient gibbs sampling algorithm proposed posterior inference. using small-variance asymptotics probabilistic model derive deterministic clustering algorithm viewed extension k-means allows inclusion side information additional property number clusters need speciﬁed priori. compare work many constrained clustering algorithms literature variety data sets conditions using noisy side information erroneous values. results experiments show strong results probabilistic deterministic approaches conditions compared algorithms literature. consider problem clustering side information focusing type side information represented pairwise cluster constraints data instances. example clustering genomics data could prior knowledge whether proteins grouped together not; clustering pixels image would naturally impose spatial smoothness sense nearby pixels likely clustered together. side information shown provide substantial improvement clustering. example showed combining additional tags image visual features oﬀered substantial beneﬁts information retrieval khoreva showed learning combining additional knowledge oﬀers substantial beneﬁts image segmentation. despite advantages including side information best incorporate remains unresolved. often side-information real applications noisy usually based heuristic inexact domain knowledge treated ground truth complicates problem. paper approach incorporating side information perspective. model observed data instances side information sources data independently generated latent clustering structure hence call probabilistic model tvclust speciﬁcally tvclust combines mixture dirichlet processes data instances random graph constraints. derive gibbs sampler tvclust furthermore inspired jiang scale variance aforementioned probabilistic model derive deterministic model. seen generalization k-means nonparametric number clusters also uses side instance-level information since based dp-means algorithm uses relational side information call ﬁnal algorithm relational dp-means lastly experiments results presented investigate behavior algorithm diﬀerent settings compare existing work literature. plethora work aims enhance performance clustering side information either deterministic probabilistic settings. refer interested reader existing comprehensive literature reviews subarea basu k-means side information earliest eﬀorts incorporate instancelevel constraints clustering proposed wagstaﬀ cardie wagstaﬀ papers must-link cannot-link constraints considered modiﬁed k-means algorithm. limitation work side information must treated ground-truth incorporated models hard constraints. algorithms similar nature k-means proposed well incorporate soft constraints. include mpck-means bilenko constrained vector quantization error pelleg baras variant linear constrained quantization error pelleg baras unlike approaches algorithm derived using small variance asymptotics probabilistic model therefore derived principled fashion. moreover deterministic model doesn’t require input goal number clusters determines data. probablistic clustering side information motivated enforcing smoothness image segmentation orbanz buhmann proposed combining markov random field prior nonparametric bayesian clustering model. issue approach nature handle must-links cannot links. contrast model also based nonparametric bayesian clustering model handle types constraints. spectral clustering side information following long tail works spectral clustering techniques malik they’re works using techniques side information mostly diﬀering laplacian matrix constructed various relaxations objective functions. works include constrained spectral clustering wang davidson constrained -spectral clustering rangapuram hein supervised clustering considerable interest supervised clustering labeling instances finley joachims goal create uniform clusters instances particular class. work side cues improve quality clustering making full labeling unnecessary also make partial and/or noisy labels. non-parametric k-means recent work bridges probabilistic clustering algorithms deterministic algorithms. work kulis jordan jiang show properly scaling distributions components derive algorithm similar k-means without requiring knowledge number clusters instead requires another parameter dp-means much less sensitive parameter k-means similar technique derive proposed algorithm rdp-means. section introduce probabilistic model based multi-view learning multi-view learning datas consists multiple views approach consider following views refer data diﬀerent views underlying clustering structure. worth noting either view suﬃcient clustering existing algorithms. given data instances xi’s familiar clustering task many methods k-means model-based clustering applied. given side information many graph-based clustering algorithms normalized graph-cut spectral clustering applied. approach tries aggregate information views bayesian framework reach consensus cluster structure. given latent clustering structure data views modeled independently generative models modeled dirichlet process mixture model modeled random graph aggregating views particularly useful neither view fully trusted. previous work constrained k-means constrained assume rely constraint exactness tvclust uses soft manner robust errors. call link may-not link contrast aforementioned must-link cannot-link emphasize model tolerates noise side information. mixture dirichlet processes underlying clustering model data instances {xi}n denote model parameter associated observation modeled sample random distribution dirichlet process used prior assume unique values among denoted kronecker delta function n−ik number instances accumulated cluster excluding instance natural clustering eﬀect sense positive probability take existing value i.e. join clusters. eﬀect interpreted using chinese restaurant process metaphor aldous assigning cluster analogous customer choosing table chinese restaurant. customer join already occupied table start one. parameters prior distribution. given deﬁnitions likelihood conjugate prior posterior distribution exponential family distribution form prior distribution scaled parameters exponential families contain many popular distributions used practice. example gaussian families often used model real valued points correspond mean vector covariance matrix. base measure often chosen gaussian families conjugate prior normal-inverse-wishart distribution. another popular parametric family multinomial distribution often used model word counts text mining histograms image segmentation. distribution corresponds base measure often chosen dirichlet distribution conjugate prior. given summarize clustering structure matrix hn×n δθi. note confused represents side information viewed random realization based true clustering structure want infer based gibbs sampling scheme derived tvclust model spirit similar gibbs samplers gibbs sampler involves iteratively sampling full conditional distribution unknown parameter given parameters data. step sampling full conditional given others mixture discrete distribution point masses located continuous component. sampling discrete component involves evaluation likelihood function easily computed. focus sampling continuous component. first observe sampled continuous component therefore using analogy chinese restaurant interpretation interpret sampling distribution following way. instance friend instance similarly instances strangers number friends instance table number strangers table deﬁnition deﬁne strictly convex function domain convex diﬀerentiable relative interior gradient exists. given points bregman divergence deﬁned forster warmuth showed exists bijection exponential families bregman divergences. given connection banerjee derived k-means type algorithm ﬁtting probabilistic mixture model using bregman divergence rather euclidean distance. parameter exponential family log-partition function deﬁned exponential family bijection rewrite likelihood using bregman divergence legendere conjugate stress conditioning equivalent conditioning since bijection right side essentially nice intuition reparameterization likelihood data point related cluster components parameters distance measured using bregman divergence dψ∗. show objective function corresponds algorithm. objective function many local minimum algorithm minimizes greedy fashion. experimentally observed initialize small value increase iteration incrementally tightening constraints gives desirable result. proof follow similar argument kulis jordan simk) augmented distance. ﬁxed number clusters point gets assigned cluster smaller augmented distance thus decreasing value objective function. remove point existing cluster cluster centered data point increase value one. increases objective ﬁxed assignment points clusters ﬁnding cluster centers averaging assigned points minimizes objective function. thus objective function decreasing iteration. following spectral relaxation framework k-means objective function introduced kulis jordan apply reformulation framework given objective function consider following optimization problem section report experiments simulated data variety datasets image dataset. evaluation report -measure exactly deﬁned section bilenko adjusted rand index normalized mutual information rdp-means experiments terminate algorithm cluster assignments change iterations initialize ξrate dp-means rdp-means calculate based k-th furthest ﬁrst method explained kulis jordan although actual calculating practice less sensitive initialization compare constrained semi-supervised clustering techniques literature could online personal communications. experiments include results methods observed unstable behavior numerical instabilities. parameters algorithms default settings authors’ implementation. experiment three tasks. ﬁrst experiment evaluates algorithms two-dimensional simulated data showcases diﬃcult clustering problems order gain visual intuition algorithms perform. second experiment evaluates collection datasets repository commonly used evaluation clustering tasks iris wine ecoli glass balance. also study eﬀect varying parameters experiments. third task illustrates eﬀectiveness using side information image clustering task jiang evaluate datasets experimenting diﬀerent settings. vary percentage constraints sampled choose secondly noise letting parameter take values clusters choose deviation {±±± dataset average parameters present performance algorithms dataset averaged diﬀerent values parameters results summarized table show overall rdp-means best performance. average parameters varying amount noise analyze adding noise constraints aﬀects performance vary values algorithm dataset. mentioned previously probability choosing noisy constraints proporfirst note results k-means dp-means across diﬀerent noise rates since algorithms make constraints. another observation that mpckmeans best performance although performance drops signiﬁcantly. therefore method good option side information relatively pure. methods including rdp-means tvclust lcvqe drops well increasing noise level although drops tvclust rdp-means smaller. average parameters varying amount side information better understand eﬀect side-information unroll results table show performance function results shown table unsurprisingly adding constraints increases performance algorithms make them. interestingly best algorithms make constraints similar performance k-means dpmeans suggests could space improvement handling noisy constraints. eﬀect deviation true number clusters algorithms analyze dependent true number clusters usually unknown practice. here investigate sensitivity algorithms perturbations true value dp-means algorithm jiang said less sensitive choice since parameter weaker dependence choice similarly since rdp-means derived dp-means expected would relatively robust deviations actual notice performance dp-means clearly stable diﬀerent choices supports claim made jiang similarly rdp-means tvclust show stable results. mpckmeans generally works well unless underestimated. small variations results k-means possible random initialization datasets dropped value deviation also implementation lcveq used needs least clusters work. aware general available implementation algorithm. repeat experiment jiang images diﬀerent categories imagenet data sampled. image processed standard visual-bag-of-words sift applied images patches resulting sift vectors mapped visual works. sift feature counts used features image since features discrete counts modeled coming multinomial distribution. thus used corresponding divergence measure i.e. kl-divergence distance metric clustering. laplace smoothing smoothing parameter remove illconditioning also include clustering results using gaussian model show importance choosing appropriate distribution. result evaluation table clearly rdp-means best result makes side information. also investigate behavior rdp-means function percentage pairs sampled. result depicted figure case rate close zero model equivalent dp-means. ﬁgure shows that constraints performance model consistently increases. sample pairs able almost fully reconstruct true clustering without loss information. authors would like thank jiang providing data used image clustering. also thank daphne tsatsoulis eric horn shyam upadhyay adam volrath stephen mayhew helpful comments draft.", "year": 2015}