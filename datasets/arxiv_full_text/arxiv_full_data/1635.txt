{"title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, \\emph{ITransF}, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.", "text": "knowledge bases important resources variety natural language processing tasks suffer incompleteness. propose novel embedding model itransf perform knowledge base completion. equipped sparse attention mechanism itransf discovers hidden concepts relations transfer statistical strength sharing concepts. moreover learned associations relations concepts represented sparse attention vectors interpreted easily. evaluate itransf benchmark datasets— knowledge base completion obtains improvements mean rank hits metrics baselines additional information. knowledge bases wordnet freebase yago dbpedia useful resources many applications question answering information extraction however knowledge bases suffer incompleteness despite formidable sizes leading number studies automatic knowledge base completion link prediction. fundamental motivation behind studies exist statistical regularities intertwined facts stored multirelational knowledge base. discovering generalizable regularities known facts missing ones recovered faithful way. excellent generalization capability distributed representations a.k.a. embeddings popularized address task seminal work bordes proposes transe models statistical regularities linear translations entity embeddings operated relation embedding. implicitly transe assumes entity embeddings relation embeddings dwell vector space posing unnecessarily strong prior. relax requirement variety models ﬁrst project entity embeddings relationdependent space model translation property projected space. typically relation-dependent spaces characterized projection matrices unique relation. beneﬁt different aspects entity temporarily emphasized depressed effect projection. instance stranse utilizes projection matrices relation head entity tail entity. despite superior performance stranse compared transe prone data sparsity problem. concretely since projection spaces unique relation projection matrices associated rare relations exposed facts training resulting poor generalization. common relations similar issue exists. without restrictions number projection matrices logically related conceptually similar relations distinct projection spaces hindering discovery sharing generalization statistical regularities. previously line research makes external information textual relations web-scale corpus node features alleviating sparsity problem. parallel recent work proposed model regularities beyond local facts considering multirelation paths since number paths grows exponentially length side effect path-based models enjoy much training cases suffering less problem. paper propose interpretable knowledge transfer model encourages sharing statistic regularities between projection matrices relations alleviates data sparsity problem. core itransf sparse attention mechanism learns compose shared concept matrices relation-speciﬁc projection matrices leading better generalization property. without external resources itransf improves mean rank hits benchmark datasets previous approaches kind. addition parameter sharing clearly indicated learned sparse attention vectors enabling interpret knowledge transfer carried out. induce desired sparsity optimization introduce block iterative optimization algorithm. contributions work proposing novel knowledge embedding model enables knowledge transfer learning discover shared regularities; introducing learning algorithm directly optimize sparse representation knowledge transferring procedure interpretable; showing effectiveness model outperforming baselines benchmark datasets knowledge base completion task. denote entities denote relations. knowledge base completion given training triples head tail entities relation e.g. want predict missing facts translation phenomenon observed well trained word embeddings transe represents head entity relation tail entity vectors respectively trained deﬁne energy function better model relation-speciﬁc aspects entity transr uses projection matrices projects head entity tail entity relation-dependent space. stranse extends transr employing different matrices mapping head tail entity. energy function however relations abundant data estimate relation speciﬁc matrices training samples associated relations leading data sparsity problem rare relations. model discussed above fundamental weakness transr stranse equip relation unique projection matrices introduces parameters also hinders knowledge sharing. intuitively many relations share concepts other although stored independent symbols example relation award nominated describe person’s high-quality work wins award nomination respectively. phenomenon suggests relation actually represents collection real-world concepts concept shared several relations. inspired existence lower-level concepts instead deﬁning unique projection matrices every relation alternatively deﬁne small concept projection matrices compose customized projection matrices. effectively relation-dependent translation space reduced smaller concept spaces. general prior knowledge concepts exist composed form relations. therefore itransf propose learn information simultaneously data together knowledge embeddings. following idea ﬁrst present model details discuss optimization techniques training. energy function speciﬁcally stack concept projection matrices -dimensional tensor rm×n×n pre-speciﬁed number concept projection matrices dimensionality entity embeddings relation embeddings. relation select useful projection matrices tensor selection represented attention vector. energy function itransf deﬁned αααh normalized attention vectors used compose concept projection matrices convex combination. obvious stranse expressed special case model concept matrices attention vectors disjoint one-hot vectors. hence model space generalization stranse. note safely fewer concept matrices itransf obtain better performance though stranse always requires projection matrices. training consisting correct triples distribution corrupted triples deﬁned section max. note omitted dependence avoid clutter. normalize entity vectors projected entity vectors sparse attention vectors deﬁned αααh normalized vectors used composition. dense attention vector computationally expensive perform convex combination matrices iteration. moreover relation usually consist existing concepts practice. furthermore attention vectors sparse often easier interpret behaviors understand concepts shared different relations. motivated potential beneﬁts further hope learn sparse attention vectors itransf. however directly posing regularization attention vectors fails produce sparse representations preliminary experiment motivates enforce constraints αααt block iterative optimization though sparseness favorable practice generally np-hard optimal solution under constraints. thus resort approximated algorithm work. convenience refer parameters without sparse constraints sparse partition dense partition respectively. based notion high-level idea approximated algorithm iteratively optimize partitions holding ﬁxed. since parameters dense partition including embeddings projection matrices pre-softmax scores fully differentiable sparse partition ﬁxed simply utilize optimize dense partition. then core difﬁculty lies step optimizing sparse partition want following properties hold satisfying criterion seems highly resemble original problem deﬁned however dramatic difference parameters dense partition regarded constant cost function decoupled w.r.t. relation words optimal choice therefore need consider optimization single relation essentially assignment problem. note that however still coupled without basically reach situation backpack problem. principle explore combinatorial optimization techniques optimize jointly usually involve iterative procedure. avoid adding another inner loop algorithm turn simple fast approximation method based following single-matrix cost. corresponding energy function subscript denotes subsets relation intuitively measures given current tail attention vector αααt project matrix could chosen head entity implausible would hence mini gives analogously deﬁne single-matrix cost tail side symmetric way. then update rule follows derivation. admitr tedly approximation described relatively crude. show section proposed algorithm yields good performance empirically. leave improvement optimization method future work. recall need sample negative triple compute hinge loss shown given positive triple distribution negative triple denoted previous work generally constructs corrupted triples replacing head entity tail entity random entity uniformly sampled however uniformly sampling corrupted entities optimal. often head tail entities associated relation belong speciﬁc domain. corrupted entity comes domains easy model induce large energy true triple corrupted one. energy exceeds training signal corrupted triple. comparison corrupted entity comes domain task becomes harder model leading consistent training signal. motivated observation propose sample corrupted head tail entities domain probability whole entity probability choice relation-dependent probability speciﬁed appendix rest paper refer proposed sampling method domain sampling. table statistics used experiments. denote number entities relation types respectively. train valid test numbers triples training validation test sets respectively. knowledge base completion task evaluate model’s performance predicting head entity tail entity given relation entity. example predict head given relation tail triple compute energy function entity knowledge base rank entities according energy. follow bordes report ﬁlter results i.e. removing correct candidates ranking. rank correct entity obtained report mean rank hits lower mean rank higher hits mean better performance. initialize projection matrices identity matrices added small noise sampled normal distribution entity relation vectors itransf initialized transe following garc´ıa-dur´an minibatch convergence. employ bernoulli sampling method generate incorrect triples used wang served alter hyperparameters. margin dimension embedding fbk. batch size fbk. learning rate fbk. matrices matrices fbk. models implemented theano softmax temperature results analysis overall link prediction results reported table model consistently outperforms previous models without external information metrics fbk. even achieve much better mean rank comparable hits current state-of-the-art model employing external information. path information helpful models taking advantage path information outperform intrinsic models signiﬁcant margin. indeed facts easier recover help multi-step inference. example know barack obama born honolulu city united states easily know nationality obama united states. straightforward extending proposed model k-step path {ri}k deﬁne path energy function αααh concept association related path. plan extend model multi-step path future. performance rare relations proposed itransf design attention mechanism encourage knowledge sharing across different relations. naturally facts associated rare relations beneﬁt sharing boosting overall performance. verify hypothesis investigate model’s performance relations different frequency. overall distribution relation frequencies resembles word frequencies subject zipf’s law. since frequencies relations approximately follow power distribution note although explicitly exploit path information performs multi-step inference multiple usages external memory. allowed access memory prediction hits similar models without path information. table link prediction results datasets. higher hits lower mean rank indicates better performance. following nguyen shen divide models groups. ﬁrst group contains intrinsic models without using extra information. second group make additional information. results brackets another results stranse reported. order study performance relations different frequencies sort relations frequency training split buckets evenly bucket similar interval length frequency. within bucket compare model stranse shown figure itransf outperforms stranse signiﬁcant margin rare relations. particular last average hits increases showing great beneﬁts transferring statistical strength common relations rare ones. comparison relation shown appendix observe tha. also observe similar pattern although degree improvement less signiﬁcant. conjecture difference roots fact many rare relations disjoint domains knowledge transfer common concepts harder. interpretability addition quantitative evidence supporting effectiveness knowledge sharing provide intuitive examples show knowledge shared model. mentioned earlier sparse attention vectors fully capture association relations concepts hence knowledge transfer among relations. thus visualize attention vectors several relations figure words hyponym hypernym refer words speciﬁc general meaning respectively. example hyponym student student hypernym phd. concepts associated head entities relation also associated tail entities reverse relation. further instance hypernym special hypernym head entity instance tail entity abstract notion. typical example connection also discovered model indicated fact instance hypernym hypernym share common concept matrix. finally symmetric relations like similar head attention identical tail attention well matches intuition. figure hits relations different amount data. give relation equal weight report average hits relation instead reporting average hits sample bin. bins smaller index corresponding high-frequency relations. model compression byproduct parameter sharing mechanism employed itransf much compact model equal performance. figure plots average performance itransf number projection matrices together baseline models. reduce number matrices model performance decreases hits still outperforming stranse. similarly itransf continues achieve best performance reduce number concept project matrices sparseness desirable since contribute interpretability computational efﬁciency model. investigate whether enforcing sparseness would deteriorate model performance compare method another sparse encoding methods section. dense attention regularization although constrained model usually enjoys many practical advantages deteriorate model performance applied improperly. here show model employing sparse attention achieve similar results dense attention signiﬁcantly less computational burden. also compare dense attention regularization. coefﬁcient experiments apply softmax since vector softmax always compare models setting computation time results reported table generally itransf sparse attention slightly better comparable performance comparing dense attention. further show attention vectors nonnegative sparse encoding proposed model induce sparsity carefully designed iterative optimization procedure. apart approach utilize sparse encoding techniques obtain sparseness based pretrained projection matrices stranse. concretely stacking pretrained projection tensor matrices -dimensional r|r|×n×n similar sparsity induced solving -regularized tensor completion prob basically minad da|| plays role attention vectors model. details refer readers completeness compare model aforementioned approach. comparison summarized table benchmarks itransf achieves signiﬁcant improvement sparse encoding pretrained model. performance expected since objective function sparse encoding methods minimize reconstruction loss rather optimize criterion link prediction. ctransr enables relation embedding sharing across similar relations cluster relations training rather learning principled way. further solve data sparsity problem sharing projection matrices parameters. learning association semantic relations used related problems relational similarity measurement relation adaptation edge statistical strengths across similar models languages. example bharadwaj transfers models resource-rich languages resource languages parameter sharing common phonological features name entity recognition. zoph initialize models trained resource-rich languages translate low-resource languages. several works obtaining sparse attention share similar idea sorting values softmax keeping largest values. however sorting operation works gpu-friendly. block iterative optimization algorithm work inspired lightrnn allocate every word vocabulary table. word represented vector column vector depending position table. iteratively optimize embeddings allocation words tables. summary propose knowledge embedding model discover shared hidden concepts design learning algorithm induce interpretable sparse representation. empirically show model improve performance benchmark datasets without external resources previous models kind. future plan enable itransf perform multi-step inference extend sharing mechanism entity relation embeddings further enhancing statistical binding across parameters. addition framework also applied multi-task learning promoting ﬁner sharing among different tasks. thank anonymous reviewers graham neubig valuable comments. thank yulun paul mitchell abhilasha ravichander pengcheng chunting zhou suggestions draft. also appreciative great working environment provided staff lti.", "year": 2017}