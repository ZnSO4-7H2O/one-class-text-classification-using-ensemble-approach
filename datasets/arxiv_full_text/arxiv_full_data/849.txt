{"title": "Shared latent subspace modelling within Gaussian-Binary Restricted  Boltzmann Machines for NIST i-Vector Challenge 2014", "tag": ["cs.LG", "cs.NE", "cs.SD", "stat.ML", "62M45", "I.2.6; I.5.1"], "abstract": "This paper presents a novel approach to speaker subspace modelling based on Gaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is based on the idea of shared factors as in the Probabilistic Linear Discriminant Analysis (PLDA). GRBM hidden layer is divided into speaker and channel factors, herein the speaker factor is shared over all vectors of the speaker. Then Maximum Likelihood Parameter Estimation (MLE) for proposed model is introduced. Various new scoring techniques for speaker verification using GRBM are proposed. The results for NIST i-vector Challenge 2014 dataset are presented.", "text": "section basic deﬁnitions grbms covered grbm shared latent subspace corresponding generative model introduced proposed model including modiﬁcation contrastive divergence algorithm performed. section various scoring techniques described including log-likelihood ratio normalized cosine scoring. section train test datasets described. results nist i-vector challenge dataset given compared baseline state-of-the-art methods. section conclusions future work directions discussed. appendix section theoretical proofs presented. normalizing constant called partition function energy function. grbm continuous space discrete space depends visible bias hidden bias vector standard deviations connectivity matrix grbm modiﬁed simulate speaker channel variability. hidden variable divided speaker factor channel factor i.e. according this parameters split groups i.e. rewrite energy function expression using split parameters paper presents novel approach speaker subspace modelling based gaussian-binary restricted boltzmann machines proposed model based idea shared factors probabilistic linear discriminant analysis grbm hidden layer divided speaker channel factors herein speaker factor shared vectors speaker. maximum likelihood parameter estimation proposed model introduced. various scoring techniques speaker veriﬁcation using grbm proposed. results nist i-vector challenge dataset presented. index terms speaker recognition speaker veriﬁcation restricted boltzmann machines i-vector plda actual approaches text-independent automatic speaker veriﬁcation generally focus modelling speaker channel variability. background majority methods based factorising long-term distribution spectral features. standard method model distribution using gaussian mixture model trained large audio database referred universal background model joint factor analysis technique based decomposition supervector additive components belonging speaker channel subspace. speaker channel subspaces modeled using low-dimensional factors. i-vector approach based total variability model representing supervector lowdimensional space containing speaker channel information. probabilistic linear discriminant analysis applied handle inﬂuence channel variability i-vector space. plda deals decomposition ivectors speaker channel factors speaker factor i-vectors speaker paper examine alternative effectively model speaker subspace using restricted boltzmann machines idea close plda factor modelling based dividing hidden layer speaker channel factors speaker factor shared vectors speaker. proposed model uses gaussianbinary contrast model described gaussian-gaussian considered. proposed approach simply extended case binary-binary choice motivated ability using gaussian-binary binary-binary blocks internal modiﬁcation contrastive divergence algorithm enables compute second term gradient presented section gradient ﬁrst term considered. taking account derivatives energy function gradient takes following form denote indexing dimensions. additionally instead update logvariances naturally constrained stay positive posteriori probabilities latent factors expressions determined following relations proved appendix section paper modiﬁcation contrastive divergence algorithm presented below. enables compute approximately second part gradient expectation replaced mean ﬁnite samples distribution since hard samples complexity generative process approximate algorithm called m-steps i-vector. generative models depending number i-vectors corresponding speaker introduced. consider case i-vectors speaker correspondent n-order generative model. denote speaker data channel factors speaker factor norder model expressed follows assume labeled training speakers denoted {xk}k data i-vectors corresponds k-th speaker. hence generative models assumed parameters tied. estimate parameters using criterium standard approach rbms optimization objective function stochastic gradient descent approach widely used rbms since data pair speakers assumed independent normalized log-likelihood function takes form log-likelihood functions generative model width speaker’s cluster lost standard cosine scoring. general cosine score divided average cosine kyspk shown cossp kyspk. taking account expression normalized cosine score takes form plda model trained i-vectors projected onto subspace projected unit sphere plda handles residual channel variability using linear factor model scoring done using plda model nist i-vector machine learning challenge dataset chosen test efﬁciency proposed model. dataset consists labeled development labeled model i-vectors model unlabeled test since labels devset available challenge best results obtained methods allowed cluster devset apply plda experiments reformed dataset. preliminary i-vectors duration less seconds removed quality construct labeled trainset modelset testset modelsetcv testsetcv. speakers devset i-vectors united initial modelset assigned trainset i-vectors assigned modelset testset remaining speakers i-vectors form cross validation first i-vectors speaker’s form enrollment modelset remaining form testset. done cross validation set. eventually trainset contains speakers total i-vectors speakers i-vectors i-vectors modelset testset respectively. used mindcf measure system performance measure cross validation processing denote false acceptance false rejection rates varying threshold. trials consist possible pairs involving target speaker modelset test i-vector testset. whitened trainset used parameter estimation. parameters whitening computed trainset too. transform used trials. initial biases zero. following recommendations elements connectivity matrices generated using normal distribution zero mean standard deviation equal elements standard deviation vector case reestimation showed worse results. best performance obtained using speaker factor dimension equal channel factor dimension equal i-vector dimension equal used minibatch stochastic gradient descent algorithm learning contrastive divergence applied. algorithm scheme presented figure data speaker used initialize algorithm zero step. intermediate k-th step algorithm presented below. reconstruction visible data sampled using latent variables sampled using binarization uniformly distributed random thresholds following recommendations given veriﬁcation trial i.e. enrollment speaker’s vectors test vector target non-target hypotheses.the target hypothesis trial vectors share common speaker factor i.e. generated -order model. non-target hypothesis generated n-order model independent generated -order model. methods exist approximate computation partition function note values partition function inﬂuence performance system case speakers number enrollment vectors. apply standard cosine scoring i-vectors previously projected onto subspace denote speaker’s i-vector test. score cosine average speaker’s vector i-vectors projected speaker space grbm. addition linear fuse plda models presented. ﬁrst model uses i-vectors features second uses i-vectors projected coefﬁcients fuse estimated cross validation using logistic regression training weighted criterium seen figure fused scores outperform i-vector plda area retain performance area. used shared latent subspace grbm hidden layer separate speaker dependent speaker independent factors ivector space. approximate maximum likelihood parameters estimation presented. proposed model several scoring methods speaker veriﬁcation considered including novel log-likelihood scoring normalized cosine scoring. plda operating i-vectors projected grbm speaker space performed results comparable state i-vector plda approach. fuse plda models showed best results operating points. method projection grbm speaker space viewed stand-alone channel variability compensation technique. grbm shared latent subspace extended types used block deeper architectures. section proofs score expression section expressions derived. obtained expression posterior probability first derive joint latent variables data using deﬁnition research conducted stel computer systems ltd. support ministry education science russian federation unique applied scientiﬁc research rfmefix. data presented statements made views expressed solely responsibility authors. rate momentum zero weight decay. batch contained speakers. epoch speakers shufﬂed batches. took epochs achieve best mindcf cross validation set. case speakers belong batch took times iterations reach performance system. train plda model ivectors whitened trainset projected unit sphere found best speaker channel factor dimensions plda equal respectively. plda model trained i-vectors projected speaker channel factor dimensions equal respectively. increase channel factor dimension showed worse results. table figure demonstrate scoring strategies perform better challenge baseline. despite optimality log-likelihood grbm scoring show best results among grbm scoring strategies. perhaps speciﬁc i-vector data. considered normalized cosine scoring performs better standard cosine scoring. terms best result achieved plda trained kenny ouellet dehak gupta dumouchel study interspeaker variability speaker veriﬁcation audio speech language processing ieee transactions vol. larcher phoneticallyconstrained plda modeling text-dependent speaker veriﬁcation multiple short utterances acoustics speech signal processing ieee international conference ieee rajan afanasyev hautam¨aki kinnunen from single multiple enrollment i-vectors practical plda scoring variants speaker veriﬁcation digital signal processing vol. khoury shafey ferras marcel hierarchical speaker clustering methods nist i-vector challenge odyssey speaker language recognition workshop epfl-conf- brummer burget cernocky glembek grezl karaﬁat leeuwen matejka schwarz strasheim fusion heterogeneous speaker recognition systems stbu submission nist speaker recognition evaluation audio speech language processing ieee transactions vol.", "year": 2015}