{"title": "Poor starting points in machine learning", "tag": ["cs.LG", "cs.NE", "math.OC", "stat.ML"], "abstract": "Poor (even random) starting points for learning/training/optimization are common in machine learning. In many settings, the method of Robbins and Monro (online stochastic gradient descent) is known to be optimal for good starting points, but may not be optimal for poor starting points -- indeed, for poor starting points Nesterov acceleration can help during the initial iterations, even though Nesterov methods not designed for stochastic approximation could hurt during later iterations. The common practice of training with nontrivial minibatches enhances the advantage of Nesterov acceleration.", "text": "poor starting points learning/training/optimization common machine learning. many settings method robbins monro known optimal good starting points optimal poor starting points indeed poor starting points nesterov acceleration help initial iterations even though nesterov methods designed stochastic approximation could hurt later iterations. good option roll nesterov acceleration later iterations. common practice training nontrivial minibatches enhances advantage nesterov acceleration. scheme robbins monro long known optimal stochastic approximation/optimization provided starting point iterations good. poor starting points common machine learning higher-order methods help early iterations explained below. below elaborate section sutskever giving rigorous mathematical details somewhat similar given d´efossez bach flammarion bach particular discuss role noise level estimates objective function stress large minibatch size makes higherorder methods eﬀective iterations. presentation purely expository; disavow claim might make originality fact observations obvious experts stochastic approximation. concision full mathematical rigor assume reader familiar subjects reviewed bach remainder present paper following structure section sets mathematical notation stochastic approximation/optimization considered sequel. section elaborates principle accelerating optimization. section discusses minibatching. section supplements numerical examples bach sutskever section reiterates above concluding paper. cost generally nonnegative scalar-valued function whereas vectors real complex numbers. supervised learning vector contains input feature entries corresponding output target entries example regression input entries regressors outputs regressands; classiﬁcation outputs labels correct classes. training access directly access independent identically distributed samples probability distribution arises. course depends particular sample drawn deﬁniteness limit consideration algorithms access values ﬁrst-order derivatives/gradient respect assume cost calibrated absolute magnitude good absolute gauge quality reviewed example bach algorithm robbins monro minimizes test error using number samples optimal optimal within constant factor depends starting point despite kind optimality happens start much larger course optimization performance suﬀer applying higher-order method rough random objective estimating standard deviation high relative instance nesterov methods essentially across several iterations; values summed stochastically independent summation would actually increase variance. accordance optimality method robbins monro nesterov acceleration eﬀective estimates objective function dominantly deterministic objective function test error practice apply higher-order methods initial iterations starting point poor gradually transitioning original method robbins monro test error approaches lower limit stochastic variations estimates objective function derivatives become important. practical considerations detailed example lecun following many modern microprocessor architectures leverage parallelism inherent batch processing. parallel processors minimizing number samples drawn probability distribution underlying data eﬃcient possibility. rather updating parameters learned random individual samples online stochastic gradient descent common current practice draw number samples collection samples known minibatch update parameters simultaneously samples proceeding next minibatch. suﬃcient parallelism processing entire minibatch take little longer processing single sample. averaging estimates objective function derivatives samples minibatch yields estimates smaller standard deviations eﬀectively making parameter values poor nesterov acceleration eﬀective iterations. moreover minibatches provide estimates standard deviations estimates number initial iterations nesterov methods accelerate optimization made arbitrarily large setting size minibatches arbitrarily large. said smaller minibatch sizes typically require fewer samples total approach full convergence also suﬃciently many iterations stochastic variations estimates objective function derivatives become important continuing nesterov acceleration later iterations would counterproductive. again recommend applying higher-order methods initial iterations starting point poor noting larger minibatches make higherorder methods advantageous iterations point requiring turning nesterov acceleration. present section supplements examples bach sutskever experiments indicating higher-order method namely momentum form nesterov acceleration help training convolutional networks. training reported method robbins monro minibatches ﬁrst size sample iteration size samples iteration section third-to-last paragraph present section describe minibatching detail. figures report accuracies various conﬁgurations momentum learning rates; momentum appears accelerate training somewhat especially larger size minibatches. iteration subtracts parameters learned multiple stored vector multiple learning rate stored vector estimated gradient plus stored vector previous iteration latter stored vector scaled amount momentum index iteration updated vector parameters previous vector parameters updated stored auxiliary vector previous stored auxiliary vector mom. amount momentum training sample minibatch size minibatch ∂c/∂θ gradient cost respect parameters following lecun exactly done chintala architecture generating feature activations convolutional network consisting series stages stage feeding output next. last stage form multinomial logistic regression applying linear transformation inputs followed softmax detailed lecun thus producing probability distribution classes classiﬁcation. cost/loss negative natural logarithm probability assigned correct class. stage last convolves image input several learned convolutional kernels summing together convolved images inputs several output images takes absolute value pixel resulting image ﬁnally averages patch partition subtract mean pixel values input image processing convnet append additional feature activation feeding softmax obtained convnet namely standard deviation values pixels image. data subset imagenet russakovsky retaining classes images representing class samples training class testing set. restricting subset facilitated extensive experimentation images full color three color channels. neither augmented input data regularized cost/loss functions. used torch platform http//torch.ch computations. table details convnet architecture tested. stage speciﬁes positions indicated layers convnet. input channels speciﬁes number images input given stage sample data. output channels speciﬁes number images output given stage. input image convolved separate learned convolutional kernel output image kernel size speciﬁes size square grid pixels used convolutions. input channel size speciﬁes size square grid pixels constituting input image. output channel size speciﬁes size square grid pixels constituting output image. feature activations convnet produces feed linear transformation followed softmax detailed lecun minibatch size rather updating parameters learned randomly selected individual images training online stochastic gradient descent instead following randomly permute training partition permuted images subsets updating parameters simultaneously images constituting subsets processing series minibatches series. sweep entire training known epoch. lecun among others made terminology standard training convnets. horizontal axes ﬁgures count number epochs. figures present results minibatches size samples iteration; figure presents results minibatches size sample iteration. average precision fraction classiﬁcations correct choosing class input sample image test set. error test average samples test negative natural logarithm probability assigned correct class coeﬃcient variation estimate standard deviation divided mean estimate standard deviation divided rolling momentum coeﬃcient variation increases good idea. please beware experiments deﬁnitive even gains using momentum seem marginal. even section discusses minibatching eﬀectively reduces standard deviations estimates objective function making nesterov acceleration eﬀective iterations. though many settings method robbins monro optimal good starting points higher-order methods help early iterations optimization parameters optimized poor sense discussed above. opportunity accelerating optimization clear theoretically apparently observable numerical experiments. minibatching makes higher-order methods advantageous iterations. said higher order higher accuracy need same higher order guarantees accuracy increase faster rate higher order guarantees higher accuracy enough iterations. method) large transition asymptotically optimal method robbins monro becomes roughly unit magnitude. transition based estimates coeﬃcient variation coeﬃcient variation essentially inversely proportional large.", "year": 2016}