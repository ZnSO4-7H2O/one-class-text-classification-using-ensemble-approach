{"title": "Accelerated Sparse Subspace Clustering", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "State-of-the-art algorithms for sparse subspace clustering perform spectral clustering on a similarity matrix typically obtained by representing each data point as a sparse combination of other points using either basis pursuit (BP) or orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in practice while the performance of OMP-based schemes are unsatisfactory, especially in settings where data points are highly similar. In this paper, we propose a novel algorithm that exploits an accelerated variant of orthogonal least-squares to efficiently find the underlying subspaces. We show that under certain conditions the proposed algorithm returns a subspace-preserving solution. Simulation results illustrate that the proposed method compares favorably with BP-based method in terms of running time while being significantly more accurate than OMP-based schemes.", "text": "state-of-the-art algorithms sparse subspace clustering perform spectral clustering similarity matrix typically obtained representing data point sparse combination points using either basis pursuit orthogonal matching pursuit bp-based methods often prohibitive practice performance omp-based schemes unsatisfactory especially settings data points highly similar. paper propose novel algorithm exploits accelerated variant orthogonal least-squares efﬁciently underlying subspaces. show certain conditions proposed algorithm returns subspace-preserving solution. simulation results illustrate proposed method compares favorably bp-based method terms running time signiﬁcantly accurate omp-based schemes. massive amounts data collected recent information systems give rise challenges ﬁeld signal processing machine learning data analysis. challenge develop fast accurate algorithms lowdimensional structures large-scale high-dimensional data sets. task extracting low-dimensional structures encountered many practical applications including motion segmentation face clustering computer vision image representation compression image clustering hybrid system identiﬁcation systems theory settings data thought collection points lying union low-dimensional subspaces. goal subspace clustering organize data points several clusters cluster contains points subspace. subspace clustering drawn signiﬁcant attention past decade among various approaches subspace clustering methods rely spectral clustering analyze similarity matrix representing relations among data points received much attention simplicity theoretical rigour superior performance. methods assume data self-expressive i.e. data point represented linear combination points union subspaces. motivates search so-called subspace preserving similarity matrix establishes stronger connections among points originating similar subspace. form similarity matrix sparse subspace clustering method employs sparse reconstruction algorithm referred basis pursuit aims minimize ℓ-norm objective means convex optimization approaches interior point alternating direction method multipliers orthogonal matching pursuit used greedily build similarity matrix. rank subspace clustering approaches rely convex optimization techniques ℓ-norm nuclear norm regularizations singular value decomposition data build similarity matrix. finally presents algorithm constructs similarity matrix thresholding correlations among data points. performance self-expressiveness-based subspace clustering schemes analyzed various settings. shown subspaces disjoint bp-based method subspace preserving. take geometric point view study performance bp-based algorithm setting intersecting subspaces presence outliers. results extended omp-based sparse subspace clustering large-scale data computationally challenging. computational complexity state-of-the-art bp-based method rank representation methods often prohibitive practical applications. hand current scalable algorithms e.g. produce poor clustering solutions especially scenarios subspaces well separated. paper address challenges proposing novel self-expressiveness-based algorithm subspace clustering exploits fast variant orthogonal least-squares efﬁciently form similarity matrix ﬁnding sparse representation data point. analyze performance proposed scheme show scenarios subspaces independent proposed algorithm always ﬁnds solution subspacepreserving. simulation studies illustrate proposed algorithm signiﬁcantly outperforms state-of-the-art method terms runtime providing essentially better clustering accuracy. results rest paper organized follows. section formally states subspace clustering problem reviews relevant concepts. section introduce accelerated sparse subspace clustering algorithm analyze performance. section presents simulation results concluding remarks stated section column given subspace preserving solution constructs similarity matrix |c|⊤ data points. graph normalized laplacian similarity matrix used input spectral clustering algorithm turn produces clustering assignments. bold capital letters denote matrices bold lowercase letters represent vectors. matrix denotes entry column additionally submatrix contains columns indexed denotes subspace spanned columns projection operator onto orthogonal complement denotes moore-penrose pseudoinverse identity matrix. further vector ones denote uniform distribution collection data points rd×n data matrix representing data points. data points drawn union subspaces {si}n dimensions {di}n without loss generality assume columns i.e. data points normalized vectors unit norm. goal subspace clustering partition {y}n groups points belong subspace assigned cluster. sparse subspace clustering framework assumes data points satisfy self-expressiveness property formally stated below. deﬁnition collection data points {y}n satisﬁes self-expressiveness property data point linear representation terms points collection i.e. exist representation matrix section develop novel self-expressiveness-based algorithm subspace clustering problem analyze performance. propose approximate solution problem employing low-complexity variant orthogonal least-squares algorithm sparse representation data point thus construct note small predeﬁned parameter used stopping criterion proposed algorithm. algorithm drawn much attention recent years greedy heuristic iteratively reconstructs sparse signals identifying nonzero signal component time. complexity using classical subspace preserving although lower bp-based method might prohibitive applications involving large-scale data. propose fast variant referred accelerated signiﬁcantly improves running time accuracy classical ols. aols replaces aforementioned single component selection strategy procedure indices selected iteration leading signiﬁcant improvements computational cost accuracy. enable signiﬁcant gains speed aols efﬁciently builds collection orthogonal vectors uℓl}t represent basis subspace includes approximation sparse signal. order aols problem consider task ﬁnding sparse representation \\{j} containing indices data points nonzero coefﬁcients representation proposed algorithm sparse subspace clustering referred accelerated sparse subspace clustering ﬁnds iteraparticular starting tive fashion theorem {yi}n collection noiseless data points drawn union independent subspaces {si}n then representation matrix returned assc algorithm subspace preserving. proof theorem omitted brevity relies observation order select representation points assc ﬁnds data points highly correlated current residual vector. since subspaces independent assc chooses point drawn different subspace corresponding coefﬁcient zero assc meets terminating criterion hence points drawn subspace nonzero coefﬁcients ﬁnal sparse representation. remark shown subspaces independent ssc-bp ssc-omp schemes also subspace preserving. however illustrate simulation results assc robust respect dependencies among data points across different subspaces settings ssc-bp ssc-omp struggle produce subspace preserving matrix theoretical analysis setting left future work. evaluate performance assc algorithm compare bp-based omp-based schemes referred ssc-bp ssc-omp respectively. ssc-bp implementations based admm interior point methods available authors interior point implementation ssc-bp accurate admm implementation admm implementation tends produce sup-optimal solution iterations. however interior point implementation slow even relatively small problems. therefore simulation studies admm implementation ssc-bp provided authors scheme tested consider following scenarios random model subspaces high probability near-independent; setting used hybrid dictionaries generate similar data points across different subspaces turn implies independence assumption longer holds. scenarios randomly generate subspaces dimension ambient space dimension times obtain uℓl}i required subsequent iterations. procedure continued until krik iteration algorithm reaches predeﬁned maximum number iterations vector coefﬁcients used representing computed least-squares solution finally found construct |c|⊤ apply spectral clustering normalized laplacian obtain clustering solution. fig. performance comparison assc ssc-omp ssc-bp synthetic data perturbation. points drawn subspaces dimension ambient dimension subspace contains number points overall number points varied fig. performance comparison assc ssc-omp ssc-bp synthetic data perturbation terms points drawn subspaces dimension ambient dimension subspace contains number points overall number points varied subspace contains sample points vary therefore total number data points varied results averaged independent instances. scenario generate data points uniformly sampling unit sphere. second scenario sampling data point perturbation term addition comparing algorithms terms clustering accuracy running time following metrics deﬁned quantify subspace preserving property representation matrix returned algorithm subspace preserving rate deﬁned fraction points whose representations subspace-preserving subspace preserving error deﬁned fraction norms representation coefﬁcients associated points subspaces i.e. represents data points subspaces. results scenario illustrated fig. fig. respectively. fig. assc nearly fast ssc-omp orders magnitude faster ssc-bp assc achieves better subspace preserving rate subspace preserving error clustering accuracy compared competing schemes. regarding second scenario observe performance sscomp severely deteriorated assc still outperforms ssc-bp ssc-omp terms accuracy. further similar ﬁrst scenario running time assc similar ssc-omp methods much faster ssc-bp. overall fig. fig. illustrate assc algopaper proposed novel algorithm clustering high dimensional data lying union subspaces. proposed algorithm referred accelerated sparse subspace clustering employs computationally efﬁcient variant orthogonal least-squares algorithm construct similarity matrix assumption data point written sparse linear combination data points subspaces. assc performs spectral clustering similarity matrix clustering solution. analyzed performance proposed scheme provided theorem stating subspaces independent similarity matrix generated assc subspacepreserving. simulations demonstrated proposed algorithm orders magnitudes faster bpbased scheme essentially delivers better clustering solution. results also show assc outperforms state-of-the-art omp-based method especially scenarios data points across different subspaces similar. part future work would interest extend results analyze performance assc general setting subspaces arbitrary necessarily independent. moreover would beneﬁcial develop distributed implementations acceleration assc. m.-h. yang k.-c. kriegman clustering appearances objects varying illumination conditions computer vision pattern recognition proceedings. ieee computer society conference vol. ieee vidal soatto sastry algebraic geometric approach identiﬁcation class linear hybrid systems decision control proceedings. ieee conference vol. ieee s.-j. lustig boyd gorinevsky interior-point method large-scale ℓ-regularized least squares ieee journal selected topics signal processing vol. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. jan. robinson vidal scalable sparse subspace clustering orthogonal matching pursuit proceedings ieee conference computer vision pattern recognition favaro vidal ravichandran closed form solution robust subspace estimation clustering computer vision pattern recognition ieee conference ieee hashemi vikalo recovery sparse signals branch bound least-squares proceedings ieee international conference acoustics speech signal processing ieee hashemi vikalo sparse linear regression generalized orthogonal least-squares proceedings ieee global conference signal information processing ieee dec. soussen gribonval idier herzet joint k-step analysis orthogonal matching pursuit orthogonal least squares ieee transactions information theory vol.", "year": 2017}