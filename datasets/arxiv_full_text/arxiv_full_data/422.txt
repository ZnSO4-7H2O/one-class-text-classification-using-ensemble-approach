{"title": "Understanding and Comparing Deep Neural Networks for Age and Gender  Classification", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.IR", "cs.LG", "68"], "abstract": "Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects.  In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition.", "text": "recently deep neural networks demonstrated excellent performances recognizing gender human face images. however models applied black-box manner information provided facial features actually used prediction features depend image preprocessing model initialization architecture choice. present study investigating different effects. detail work compares four popular neural network architectures studies effect pretraining evaluates robustness considered alignment preprocessings cross-method test swapping intuitively visualizes model’s prediction strategies given preprocessing conditions using recent layer-wise relevance propagation algorithm. evaluations challenging adience benchmark show suitable parameter initialization leads holistic perception input compensating artefactual data representations. combination simple preprocessing steps reach state performance gender recognition. since supervision entered imagenet challenge large margin much progress made ﬁeld computer vision help deep neural networks improvements network architecture model performance steady fast-paced since artiﬁcial neural networks also revolutionized learning-based approaches research directions beyond classical computer vision tasks e.g. learning read subway plans understanding quantum many-body systems decoding human movement signals matching even exceeding human performance playing games texas hold’em poker various atari games super smash bros. automated facial recognition estimation gender using machine learning models held high level attention decades become ever relevant abundance face images especially social media platforms. introduction models domain largely replaced need hand crafted facial descriptors data preprocessing considerably increased possible prediction performances incredible rate. models successfully applied gender recognition also classiﬁcation emotional states previous three years alone recognition rates increased gender recognition rates reportedly recent challenging adience benchmark mirroring overall progress available benchmarks images groups data data ghallagher collection person data next indisputable performance gains across board probably important factor popularity architectures entry barrier provided intuitive generic building blocks oneﬁts-all applicability many learning problems importantly availability highly performing accessible software training testing deployment e.g. caffe theano tensorﬂow name supported powerful gpu-hardware. however recently dnns complex nonlinear learning machines used black-box manner providing little information aspect input causes actual prediction. efforts explaining complex models near past resulted several approaches methods allowing insights beyond performance ratings obtainable common benchmarks. welcome development critical applications autonomous driving medical domain often special importance know model decides does given certain input whether trusted outside laboratory settings paper compare inﬂuence model initialization weights pretrained real world data sets random initialization analyze impact image preprocessing steps model performance adience benchmark dataset different recent architectures. show suitable pretraining yield robust starting model weights compensating artefactual representation data cross-method test swapping. using layer-wise relevance propagation visualize choices made prior training affect classiﬁer interacts input pixel level i.e. provided input used make decision parts rectiﬁed performance gender recognition referred likely result report result slightly exceeding baseline. combination simple preprocessing steps reach state performance gender recognition human face images adience benchmark dataset. recent face image data sets adience benchmark published containing photos across subjects binary gender label label eight different groups partitioned splits. principle data capture images close real world conditions possible including variations appearance pose lighting condition image quality name few. conditions provide unconstrained challenging learning problem ﬁrst results adience benchmark achieved accuracy classiﬁcation accuracy gender classiﬁcation using pipeline including robust certainty based in-plane facial alignment step local binary pattern descriptors four patch descriptors dropout-svm classiﬁer reference classiﬁcation pipeline achieves accuracy classiﬁcation accuracy gender classiﬁcation ghallagher data set. authors introduce landmark-based alignment preprocessing step computes frontalized versions unconstrained face images slightly increases gender classiﬁcation accuracy adience data otherwise using classiﬁcation pipeline ﬁrst time model applied adience benchmark authors resort end-to-end training regime e.g. face frontalization preprocessing omitted model completely trained scratch order demonstrate feature learning capabilities neural network type classiﬁer. architecture used similar bvlc caffe reference model fourth ﬁfth convolution layers removed. best reported accuracy ratings increased classiﬁcation gender classiﬁcation using over-sampling prediction scheme crops taken sample instead sample best knowledge current state results gender predictions reported accuracy respectively. model winner chalearn looking people challenge uses vgg- layer architecture pretrained imdbwiki face data set. data also introduced comprised labelled face images collected imdb wikipedia. prior pretraining imdb-wiki data model initialized weights learned imagenet challenge authors attribute success model large amounts training data simple robust face alignment preprocessing step appropriate choice network architecture. accuracy achieved commercial system supposedly backed carefully labelled non-public training images. authors identify landmark-based facial alignment preprocessing critical factor achieve reported results. unfortunately details given model architecture use. authors compare results systems selectively list estimation competing methods authors also report gender recognition performance rather given early results performance recognition attempts replicate models referenced studies. recapitulating identify three major factors contributing performance improvements among models listed table changes architecture. prior knowledge pretraining. optional dataset preparation alignment preprocessing. following sections paper brieﬂy describe selection architectures investigate inﬂuence random weight initialization pretraining generic task-speciﬁc real world data sets well impact data preprocessing comparing afﬁne reference frame based alignment techniques coarse rotation-based alignment. size unconstrained nature data availability previous results adience benchmark data evaluation sandbox. dataset available rotation aligned version version images preprocessed using afﬁne in-plane alignment putting shown faces closer reference frame facial features. layer-wise relevance propagation give glimpse model’s prediction strategy visualizing facial features used prediction per-sample basis order explain major performance differences. evaluated architectures data preprocessing techniques weight initialization choices. models trained using caffe deep learning framework code based https//github.com/gillevi/ agegenderdeeplearning containing conﬁgurations reproduce results compare architectures model used bvlc caffe reference model googlenet vgg- state performance classiﬁcation reported adiencenet structurally similar caffenet main difference lying smaller convolution masks learned input layer less convolution layers present. number hidden units composing fully connected layers preceding output layer considerably lower adiencenet. vgg- consists convolution layers small kernel sizes interleaved similarly small pooling operations followed fully connected layers hidden units each fully connected output layer. fourth model evaluate googlenet connects series inception layers. inception layer realizes multiple convolution/pooling sequences different kernel sizes parallel feeding input tensor outputs concatenated along channel axis. compared vgg- architecture googlenet fast train evaluate slightly outperforming vgg- model imagenet challenge top- error classiﬁcation task choice made training classiﬁcation regarding data preprocessing. svm-based system improves upon introducing face frontalization preprocessing step goal rendering inputs pipeline invariant changes pose. landmarkbased preprocessing also identiﬁed important step obtaining reported model performances. employ simple rotation based preprocessing roughly aligns input faces horizontally trusting learning capabilities neural networks proﬁt increased variation data learn suitable data representations. adience benchmark data provides version data images roughly rotated horizontally aligned faces well afﬁne in-plane aligned version download. prepare training test sets versions using adapting original splits data preprocessing code available download github. also create mixed data union previous data sets double number training samples allows models trained provided alignment techniques simultaneously. invaluable beneﬁt architectures option pretrained models starting point training. compared random weight initialization using pretrained models starting points often results faster convergence overall better model results initializing model meaningful ﬁlters. paper compare models initialized random weights models starting weights trained data sets namely imagenet data imdb-wiki data sets whenever model weights readily available. replicate results train adiencemodel scratch since monotonically increasing nonlinear function neuron inputs neuron output learned weight bias parameters. behaviour described taking example single neuron neuron receives relevance quantity neurons upper layer redistributed input neurons lower layer proportionally contribution forward pass here quantity measuring contribution neuron activation neuron aggregation forward messages relevance score neuron consequently obtained pooling incoming relevance quantities ri←j neurons contributes relevance redistribution obtained equations general exact deﬁnitions depending neuron input’s type position pipeline models considered paper consist part relu-activated feature extraction layers towards bottom followed inner product layers serving classiﬁers therefore apply inner product layers \u0001-decomposition small epsilon matching sign added denominator numeric stability truthfully represent decisions made layers’ linear mappings consistently. since relu activations convolutional layers below serve gate ﬁlter weak activations apply decomposition formula weights either pretraining data available. instead comparable caffenet estimate results obtainable initialzing model imagenet weights. also train googlenet scratch initialized imagenet weights. excessive training time required vgg- model replicate results train models initialized available imagenet imdb-wiki weights. complement quantitative analysis section qualitative insights perception reasoning models explaining predictions made importance features decision input level. following success dnns desire understand inner workings black models vitalized research efforts dedicated increasing transparency complex models. several methods explaining individual predictions emerged since then robust computationally expensive occlusion-based samplingbased analysis sensitivity analysis backpropagation-type approaches among them. intensive study layer-wise relevance propagation found outperform considered competing approaches computing meaningful explanations decisions made classiﬁers. further method contrast sampling occlusion-based approaches computationally inexpensive applicable wide range architectures classiﬁer types therefore supportively complement quantitative results shown section visualize perception model interaction input evaluated training conditions. experiments current version toolbox provided authors. principled general approach decompose output decision function given input so-called relevance values component method operates iteratively model output inputs layer-by-layer backpropagation-style algorithm computing relevance scores hidden units interim. corresponds contribution input hidden variable true layers. method assumes decision handles activating inhibiting parts separately weights respectively since enforcing ensures conservation property equation theoretical insights decomposition types found relevance scores obtained pixel level sum-pool relevance values color channel axis. leaves value pixel visualize results using color centered zero since indicates neutral contribution input component identify components locally speaking global prediction. models vastly different ﬁlter sizes bottom layers. follow distributing neurons lower layers uniformly across respective inputs granularity visualizations models comparable. score trained models using oversampling evaluation scheme using average prediction crops sample. results gender prediction shown tables respectively. columns tables correspond described models; adiencenet caffenet googlenet vgg-. following previous work also report -off accuracy results accuracy obtained predicting least label adjacent correct prediction task. headers describe training evaluation setting ﬁrst value signiﬁes n-plane face alignment preprocessing step training testing stands otation based alignment describes results obtained rotation aligned in-plane aligned images ixed training images test used evaluation. second values describe weight initialization using imageet imdb-iki respectively. second value means model trained scratch random weight initialization. results tables list measured performance ﬁxed amount training steps. intermediate models might shown slightly better performance ignored favour comparability. attempt replicate results based code provided authors managed exceed reported results accuracy -off accuracy prediction accuracy gender prediction. expected structurally comparable caffenet architecture obtains relatable results learning problems random model weight initialization. compared relatively fast train caffenet model table result classiﬁcation accuracy percent using oversampling prediction. small numbers next accuracy score show -off accuracy least adjacent group predicted. table results gender classiﬁcation accuracy using oversampling prediction. bold values match exceed currently reported state results adience benchmark. googlenet model data preprocessing conﬁgurations trained scratch ﬁne-tuned based imagenet weights. replicate measurements verify observations made based models. here fully manage reach reported results despite using model pre-trained imdb-wiki data provided authors. however closely scrape reported results slight differences accuracy -off accuracy averaged splits data model trained mixed training set. evaluated settings shown figure observe overall trends choices architecture dataset composition preprocessing model initialization. settings caffenet architecture outperformed complex deep googlenet models. gender classiﬁcation comparable settings best vgg- models outperform best googlenet models. figure visualizes different characteristics input faces used classiﬁers predict figure plots ordered column-wise model architectures row-wise according prediction problem showing model performance training time given different initializations data preprocessing settings. bottom dashed lines plot show worst best reference accuracy results horizontal axis increasing training iterations. thick lines show results taken color coding corresponds data preprocessing shading model initialization blue color stands afﬁne n-plane alignment. violet lines correspond otation alignment. orange lines show model performances training ixed training set. translucent line color stands training random model initialization fully opaque solid lines show performance ﬁnetuning imagenet weights dashed lines correspond model initialization using imdb-wiki weights applied vgg- model. results averaged splits adience data set. observe model performance correlates network depth turn correlates structure observable heatmaps computed lrp. instance models recognize female faces dominantly hair line eyes males based bottom half face. caffenet model tends contentrate isolated aspects given input compared especially less certain prediction reﬂected stronger negative relevance. three models observe overall trend prediction problems in-plane alignment preprocessing step beneﬁcial classiﬁer performance compared rotation alignment. exception trend randomly initialized googlenet model loses percent accuracy prediction rotation alignment albeit still gaining performance measured -off prediction. reason better performance rotation aligned images justiﬁed potential dnns learn domain face images canonically meaningful sets features. face images aligned using technique presented difﬁcult. especially images children faces aligned reference frames suitable adults result head shapes uncharacteristic aspect ratios group even faulty alignments. figure demonstrates nature artefactual noise introduced data unsuitable alignment. models beneﬁt combining rotation aligned landmark aligned data sets training. effectively doubles training sizes also perhaps importantly allows learning robust feature models trained comsponds increase performance classiﬁcation problems dataset conﬁgurations. caffenet however slightly loses performance tuned group prediction beneﬁting gender prediction. better response googlenet compared caffenet initialized respective imagenet weights might caused quality initial parameters googlenet achieves top- error imagenet caffenet reaches evaluating incorrect test data tuned models trained rotation aligned images manage recover respective performance ratings compared models trained scratch evaluated correct data. googlenet model even exceeds performance architecture initialized randomly trained evaluated rotated images. measurable beneﬁcial effect appropriate pretraining visualized figures imagenet pretraining leads larger meaningful parts face prediction googlenet randomly initialized model picks single characteristics training correlate target class. includes eyebrows lips deﬁning female faces nose chin uncovered ears gender recognition. comparable results vgg- group estimation comparing pretraining imagenet imdb-wiki. model initialized imdb-wiki weights pretraining task estimation categories concentrates facial features themselves imagenet-initialized prone distraction background elements clothing items. facial features seen examples opposing classes respectively weaker models ﬁgures independent ensemble facial features leads less certain noisy decisions. problem gender recognition affected less weight initialization quality data preprocessing. here imdb-wiki pretraining might diminished effect ﬁrstly imagenet weights providing already good starting weights secondly pretraining objective orthogonal task gender recognition. fact recognition vgg- models initialized imagenet weights converged better parameters counterparts. figure reports prediction performances caffenet googlenet vgg- model evaluated settings averaged splits adience data set. recorded model scores time illustrate suitably initializing model largely outweighs problems introduced artefactual data experiments. next overall better model obtained convergence also observe considerably faster increase learning progress early training. figure bottom input image followed relevance maps best performing caffenet googlenet model gender prediction. colors identify parts image contributing predicted class. cold hues show evidence contradicting predicted class perceived model. smoother heatmaps consequence smaller ﬁlters stride bottom layers. bination landmark aligned rotation aligned images perform well test sets resulting preprocessing techniques. tables show results models trained combined evaluated rotation aligned test set. order underline effect increased robustness models trained diverse oration aligned training evaluated models trained nplane aligned images otation aligned test images vice versa. corresponding model performances listed tables models trained data prepared alignment technique evaluated test perform even worse early svm-based models despite competitive results combined training set. models trained in-plane aligned images difﬁculty predicting unseen setting models trained rotated images original facial pose proportions face image mostly preserved. vgg- model compared in-plane alignment mixed training worst best expected results. again mixed training data results better model in-plane alignment used. figure shows overview results training time. observations initialization figure samples taken rotation aligned variant adience data set. bottom in-plane aligned samples. left image pairs show faces taken group classiﬁed correctly rotation alignment placed least group predictor landmark-based alignment middle image left predicted group googlenet. in-plane alignment technique applied variant adience data tends elongate faces vertically. remaining image show misclassiﬁed misaligned samples picked random. table test swapping results prediction. performance considerably worse incorrect preprocessing used testing overﬁt feature sets. pretraining yield robust model parameters compensating deviating test statistics. recent deep neural network models able accurately analyze human face images particular recognize persons’ gender emotional state. complex non-linear structure however models often operate black-boxes recently unclear arrived predictions. paper opened black-box classiﬁer using layer-wise relevance propagation investigated facial features actually used gender prediction. compared different image preprocessing model initialization architecture choices challenging adience dataset discussed affect performance. using visualize models’ interactions given input samples demonstrate appropriate model initialization pretraining counteracts overﬁtting leading holistic perception input. combination simple preprocessing steps achieve state performance gender classiﬁcation adience benchmark data set. figure heatmaps googlenet models gender recognition. input images shown heatmaps pretrained imagenet shown heatmaps initialized randomly. ﬁnetuned model predicts based ensemble facial features whereas model starting random weights overﬁt isolated features characteristic target classes.", "year": 2017}