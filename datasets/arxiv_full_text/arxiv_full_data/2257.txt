{"title": "Distributed Bayesian Piecewise Sparse Linear Models", "tag": ["cs.AI", "cs.DC", "cs.LG", "stat.ML"], "abstract": "The importance of interpretability of machine learning models has been increasing due to emerging enterprise predictive analytics, threat of data privacy, accountability of artificial intelligence in society, and so on. Piecewise linear models have been actively studied to achieve both accuracy and interpretability. They often produce competitive accuracy against state-of-the-art non-linear methods. In addition, their representations (i.e., rule-based segmentation plus sparse linear formula) are often preferred by domain experts. A disadvantage of such models, however, is high computational cost for simultaneous determinations of the number of \"pieces\" and cardinality of each linear predictor, which has restricted their applicability to middle-scale data sets. This paper proposes a distributed factorized asymptotic Bayesian (FAB) inference of learning piece-wise sparse linear models on distributed memory architectures. The distributed FAB inference solves the simultaneous model selection issue without communicating $O(N)$ data where N is the number of training samples and achieves linear scale-out against the number of CPU cores. Experimental results demonstrate that the distributed FAB inference achieves high prediction accuracy and performance scalability with both synthetic and benchmark data.", "text": "ones often produce competitive accuracy state-of-the-art non-linear methods realworld datasets. addition representations often preferred domain experts. best models i.e. simple accurate simultaneously determine number pieces cardinality predictor. however simultaneous model selection essentially much computationally demanding restricted applicability middle-scale data sets. meanwhile analyzing large scale data size feature matrix easily exceeds memory capacity single computation node. recent trends distributed computational platforms large-scale machine learning shifting based distributed systems based distributed memories parameter server hadoop incurs substantial overhead load intermediate data disks computations distributed memory architectures able avoid need disk access storing data memory across computations. notably spark appears promising platforms enterprise data analytics many distributed machine learning algorithms spark recently developed paper proposes novel distributed algorithm learning piecewise linear models distributed memory architectures efﬁcient implementation spark. contributions summarized below. distributed learning algorithm paper develops distributed learning algorithm piecewise linear models model selection. technical contributions mainly two-fold. first algorithm linearly scales number distributed workers automates model selection problem taking advantages recently-developed techniques factorized asymptotic bayesian hierarchical mixture experts model selection piecewise linear models median selection subset aggregation estimator communicationefﬁcient distributed feature selection. second message algorithm independently processes data worker communication efﬁciency observe yields bias factorized information criterion abstract—the importance interpretability machine learning models increasing emerging enterprise predictive analytics threat data privacy accountability artiﬁcial intelligence society piecewise linear models actively studied achieve accuracy interpretability. often produce competitive accuracy state-of-the-art non-linear methods. addition representations often preferred domain experts. disadvantage models however high computational cost simultaneous determinations number pieces cardinality linear predictor restricted applicability middle-scale data sets. paper proposes distributed factorized asymptotic bayesian inference learning piece-wise sparse linear models distributed memory architectures. distributed inference solves simultaneous model selection issue without communicating data number training samples achieves linear scale-out number cores. experimental results demonstrate distributed inference achieves high prediction accuracy performance scalability synthetic benchmark data. importance interpretability transparency machine learning models increasing emerging enterprise predictive analytics threat data privacy accountability artiﬁcial intelligence society data mining machine learning academic community workshop named fat/ml held every year since momentum grown incorporating legal aspects machine learning agents government point view european union enforces gdpr requires consequences proﬁling informed data subject. hand interpretable models restrict model representations balance interpretability accuracy important research topics decades learning spark spreading industrial area including electric power telecommunication drug discovery movement importance spark data science platform community also growing several tutorials held last several conferences despite increasing attentions spark-based high-scale machine learning best knowledge little studies distributed learning piecewise linear models fab/hme models partition feature spaces using gating functions assign sparse expert partition. fab/hme model employs bernoulli gating function follows observation variable target variable represents model parameters numbers experts gating functions respectively. denotes j-th expert index contains indices gating nodes unique path root node j-th expert node. probability i-th gate expresses probability j-th path j-th expert left subtree i-th gate otherwise. experts either linear regression i.e. linear logistic regression i.e. fab/hme. derive asymptotic correction term bias leads better feature selection individual local models. practical design spark paper presents design algorithm spark helps fully utilize distributed computation resources. resilient distributed dataset designed perform iterative model optimization withshufﬂing data. further show sample-wise parallelization algorithm uses resources much efﬁciently expert-wise one. experimental results demonstrate algorithm design achieves high prediction accuracy high scalability synthetic benchmark data compared state-of-the-art spark machine learning libraries. piecewise linear models actively studied achieve interpretability accuracy. models include classical ones decision trees lasso advanced models hierarchical mixture experts bayesian treed linear models local supervised learning space partitioning informative projection ensembles supersparse linear integer models optimization piecewise linear models usually non-convex simultaneous optimizations partitions local models. partition-wise linear models addressed issue formulating structured-sparsity problem. fab/hmes induce sparseness tree structures cardinalities local models fully automate simultaneous model selection learning piecewise linear models inference jialei extended fab/hmes incorporated non-linearity local predictors gain better accuracy keeping certain level interpretability. ribeiro proposed locally approximate non-linear models linear models model agnostic interpretability. know applications sophisticated piecewise linear models limited middle scale datasets high computational costs. meanwhile spark appears promising platforms distributed machine learning algorithms. large quantity researches realize distributed machine learning algorithms spark logistic regression k-means admm dominant cluster detection graph algorithms high scale computing power automation hyper-parameter search spark also active research ﬁeld. furthermore spark gotten attention platform deep learning recently research outcomes continuously integrated spark machine learning library called mllib growing proposals cutting-edge technologies application ﬁeld machine like shared memory architecture. second challenge avoid huge communication overhead. particularly communications among worker nodes cause reallocation data distributed memory architectures. third map-reduce computational model popular modern distributed memory computation models balancing equalizing loads needed minimize synchronization overhead. hereafter denote dedicated server worker node worker nodes. assume training data distributed memories subscription denotes w-th worker node. denote sample index w-th worker node. algorithm algorithm executed worker nodes parallel dedicated server serial respectively. distributed computation necessary convergence determination described algorithm calculation consists parts expected log-likelihood regularization. former requires distributed worker nodes hence worker node computes expected log-likelihoods data memory summation needs collected dedicated server shown line algorithm step requires communication scalar values dedicated server individual worker nodes. latter computed dedicated server shown line algorithm fab/hme algorithm ﬁnds best parameters models maximizing factorized information criterion asymptotically accurate approximation bayesian marginal log-likelihood derived follows distribution entropy i-th gating index contains indices experts sub-tree i-th gating node. linear regression logistic regression. optimization conducted alternating optimizations like expectation-maximization algorithm. note equivalent l-norm induces sparsity model. applied forward-backward greedy algorithm least square experts tightest error bounds among state-of-the-art methods. easily extend idea logistic regression applying gradient foba algorithm theoretical error bounds general smooth convex functions. paper considers situations data size much larger data dimensionality designation distributed fab/hme algorithm architecture involves following three challenges. first memory capacity limitation single worker node load entire data target variational distributions. prohibits algorithm employing straightforward parallelization tlitsi sets samples whose γi-th dimension larger smaller contains indices expert nodes left sub-tree i-th gating node similarly deﬁned right sub-tree gating node. matrices w.r.t. dtmax elements collected dedicated server. then dedicated server i-th gate parameter computed described lines algorithm containing three scalar values distributed worker nodes. optimizing experts m-step distribute regularized optimization well-studied approaches using distributed gradient proximal methods applicable. address issue applying recently-developed median selection subset aggregation estimator algorithm expected numbers samples experts gates denoted collected dedφj therefore scalar values icated server communicated. line computed globally shared dedicated server. known exponentiated regularization derived eliminates redundant latent variables iterations non-effective experts eliminated model follows threshold value normalization shrinkage process automatically determines piecewise space partitioning structures bayesian-principled fashion. gate optimization requires split candidate points i.e. {ti|discretized domain dimension consistently aggregate distributed calculations worker nodes must share purpose computed beginning fabem algorithm basis algorithm first worker node maximum minimum values computed collected dedicated server. vector size transferred worker node dedicated server. computed basis lines algorithm {bd} distributed worker nodes. process communicates matrix size dtmax. resilience distributed dataset base spark distributed computation whose elements distributed computations spark performed i.e. spark processes element parallel. immutable partitioned collection records created persistent data rdds transformations. standard design might assign data instance element rdd. however major computations fab/hme algorithms rely matrix computations dfab designed distributed worker nodes. note majority voting weight aggregation lightweight computations comparison optimization processes executions dedicated server affect parallel performance. correction m-step fig. illustrates structure dfab. partition contains multiple elements data units functions applied element w-th partition consists tuple index element corresponds worker node index fig. illustrates execution dfab. denote t-th iteration whose element t-th calculation driver process invokes transformation closure executing step algorithm involves model parameters parameters. here executor computes intermediate outcome element updated note that eliminated experts identiﬁed driver process updated executors simply setting corresponding column zero order avoid reallocation itself. driver process invokes transformation closure executing steps algorithm note avoid recalculation expert-wise likelihood storing part element. here element updated calculation e-step obtain without re-allocating shufﬂing large scale data. although mappartition function collect part training data partition mappartition passes iterator samples closure causes many iterator accesses signiﬁcant computational overhead. subsection discusses advantages sample-wise parallelization expert-wise parallelization. although later might seem natural choice parallelizing fab/hme learning algorithm since expert optimizations processed independently observe issues. first fig. shows distribution processing time expert optimization heavy tailed. optimization long process time large number assigned instances varies across experts iterations. implies loads cores unbalanced fast ones wait slow ones shown fig. resulting poor usage. second number cores expert-wise parallelization bounded number experts. fab/hme learning procedure irrelevant experts automatically eliminated number experts decreases iterations. fig. illustrates example decreasing numbers active experts iterations simple simulation dataset. experiment used machine cores. algorithm utilized half cores however number active experts became step. fig. illustrates sample-wise parallelization expert optimization. sample-wise parallelization addresses issues. unbalance loads across experts time complexity expert optimization approximately linear w.r.t. sample size hence sample-wise parallelization equally distribute computational loads. addition degree parallelization since task processes optimization experts shrinkage decrease number experts accelerates overall computation rather decreasing usage. distributed operations dfab implemented chains transformations. dfab ﬁrst converts training data repeatedly transforms another calculate variational distribution model parameters driver process invokes processes dfab executors means transformers reduce calculation results collected driver process collect count functions. since distributed operations transformations dfab able obtain beneﬁts spark e.g. sophisticated task/job scheduling fault tolerance. driver process executors exchange model parameters dynamically-generated closures. driver distributes generating closure passed transformation function spark. dfab implements logic dfab ﬁrst-class parameterized functions. passing parameter parameterized function function generates another closure calculates logic parameter dfab passes closure derived transformation functions executes dfab logic executors. checkpointing intermediate rdds important w.r.t. correct execution performance improvement. dfab kind algorithm sometimes requires long iterations. dfab executes iterations chaining operations. long chain operations increases stack size remote procedure calls induces stack overﬂow worst spark maintain lineage operations keeping fault tolerance. furthermore likely cause garbage collection intermediately-generated elements result need re-computation. deal issues dfab makes checkpoints every iterations number determined basis empirical knowledge experiments. used spark worker nodes spark executors yarn server worker node employed intel xeon processors memory connected gbps ethernet. observed target variables standardized advance. ﬁrst demonstrate dfab improves execution speed negative impact accuracy comparing serial algorithm abbreviated sfab using artiﬁcial regression data used original fab/hme paper true tree structure illustrated fig. experts expert uses features. i-th gating node ﬁxed randomly selected respectively. j-th expert non-zero elements randomly sampled sampled uniform employed number initial experts sfab |w|. observed rmse values dfab smaller sfab. might caused samplewise parallelization reduced training variance original paper message algorithm also reported rmse dfab slightly increased increasing relatively small might compared predictive accuracy dfab regression sensor array household power consumption) classiﬁcation data sets repository distributed machine learning algorithms implemented spark mllib. brief task abstracts data sets described here; task sensor array data sets multivariate regression multiple responses based chemical sensing system consists array metal-oxide sensors external mechanical ventilator simulate biological respiration cycle. task household power consumption data predict amount power consumption every minutes house contains three meter’s measurements electric power consumption household one-minute sampling rate period almost years. task higgs data classiﬁcation problem distinguish signal process produces higgs bosons background process contains features total. used features limitation dfab implementation. task hepmass data separate particleproducing collisions background source. data produced monte carlo simulations contains features. spark mllib chose elasticnet decision tree random forests classiﬁcation regression baseline algorithms. note less interpretable evaluated state-of-the-art distributed non-linear model. used -loop cross validation -fold outer loops evaluating test prediction error -fold inner loops parameter selection. note fab/hmes need -loop cross validation dfab execute inner loop. employed number initial experts table summarizes test rmses regression data sets classiﬁcation errors classiﬁcation data sets. three regression data sets dfab outperformed others performed better dfab higgs data classiﬁcation. higgs data dfab generated fab/hmes active experts whose cardinalities much interpretable models learned consists trees -depth. summary results indicate dfab achieved better predictive accuracy distributed algorithms implemented spark mllib achieved competitive accuracy interpretable models non-linear models spark mllib. number samples executor becomes insufﬁcient. however difference accuracy became negligible larger experimental condition. fig. shows processing times dfab sfab implemented parallel gate optimization expert-wise parallelization sfab openmp results observed that iteration time sfab cores much less core dfab cores much less same. dfab cores showed average time iteration total processing time much less sfab core respectively. general larger achieves better improvement time iteration large |w|. larger leads longer processing time worker result diminishes overhead spark execution initial setup overhead executors rdds wait intermediate checkpointing. existed limitation performance improvement depending value performance improvement iteration worse hand performance improvement iteration greater implies determine adequate number partitions i.e. basis size training data. table test prediction errors numbers shown parentheses standard deviations. best second best methods highlighted bold bold italic faces respectively. sensor array dfab outperformed elasticnet dectree improvement reach level difference sensor array data caused difference convergence time derived difference training data even scale data similar other. dfab limitation scale computing performance household power consumption data. data three features resulted running optimization processes quite fast overhead spark computation became bottleneck. finally compared processing time dfab algorithm implementations spark mllib. experiments algorithm implementations. note processing time except dfab includes -fold inner loop. shown table dfab averagely takes longer time algorithms dfab solves non-convex optimization problems achieve high interpretability accuracy. however shown fig. execution performance scalability dfab better others. property dfab alleviates performance disadvantage dfab easily reduce execution time dfab adding worker nodes unthinkable solution cloud computing era. section demonstrates dfab empirically utilize computing resources efﬁciently. fig. illustrates resource consumption dfab runs artiﬁcial data results observed utilization workers roughly execution. decrease utilization occurred checkpointing rdds. however severely degrade performance execution. spikes disk network access occurred checkpointing rdds hdfs. hand average disk network accesses iterations without checkpointing less several hundreds kb/s. driver process issue notable disk accesses. means execution driver fully completed memory performance bottleneck. further driver process issued network trafﬁc rate tens mb/s start execution. network transmission arose distribution process training data. process ﬁnished driver process transferred insigniﬁcant amounts data workers less hundreds kb/s. diminish network transfer training data simple modiﬁcation implementation make workers read data hdfs directly. evaluate scale-out scalability dfab conducted experiments cluster tens hundreds virtual instances amazon services prepared scaled-out clusters large number worker nodes whose small number cores; consists c.large instances another consists r.large instances comparison also prepared scaled-up clusters small number worker nodes whose large number cores; consists c.xlarge instances another consists r.xlarge instances hardware speciﬁcation described table iii. evaluation used sensor array data set. c.large r.large clusters around times faster c.xlarge r.xlarge clusters dfab achieved good scale-out scalability c.large r.large clusters despite scaled-out architectures tend increase network communications huge number worker nodes. scale-out scalability obtained design dfab reduces network trafﬁc much possible avoid performance degradation. execution time c.large r.large clusters shorter c.xlarge r.xlarge clusters. good scaled-out scalability dfab exploits performance advantages hardware architecture automatically clock-up technology large total bandwidth memory access. paper proposed distributed fab/hme algorithm scalable algorithm learn interpretable accurate piecewise sparse linear models data. taking advantages inference message algorithm novel asymptotic bias correction dfab realizes fully automated model selection linearly scale-out capability data size. further presented design dfab spark rising distributed memory computation platform. dfab enables fully utilize without needing shufﬂe data optimization processes. experimental results demonstrated dfab achieves high prediction accuracy performance scalability synthetic public data. important future work explore possibility rebuilding dfab algorithm make asynchronous order improve efﬁciency computing resource utilization accelerate algorithm execution. meng bradley yavuz sparks venkataraman freeman tsai amde owen franklin zadeh zaharia talwalkar mllib machine learning apache spark mach. learn. res. vol. zaharia chowdhury dave mccauly franklin shenker stoica resilient distributed datasets fault-tolerant abstraction inmemory cluster computing usenix nsdi ustun trac`a rudin supersparse linear integer models predictive scoring systems aaai conference late-breaking developments field artiﬁcial intelligence zaharia borthakur sarma elmeleegy shenker stoica delay scheduling simple technique achieving locality fairness cluster scheduling proc. sigops european conf. comp. sys. available http//doi.acm.org/./. vavilapalli murthy douglas agarwal konar evans graves lowe shah seth saha curino o’malley radia reed baldeschwieler apache hadoop yarn another resource negotiator socc harnie vapirev wegner gedich steijaert wuyts meuter scaling machine learning target prediction drug discovery using apache spark proc. ieee/acm ccgrid", "year": 2017}