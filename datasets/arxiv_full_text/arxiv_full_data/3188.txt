{"title": "An Analysis of Random Projections in Cancelable Biometrics", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "With increasing concerns about security, the need for highly secure physical biometrics-based authentication systems utilizing \\emph{cancelable biometric} technologies is on the rise. Because the problem of cancelable template generation deals with the trade-off between template security and matching performance, many state-of-the-art algorithms successful in generating high quality cancelable biometrics all have random projection as one of their early processing steps. This paper therefore presents a formal analysis of why random projections is an essential step in cancelable biometrics. By formally defining the notion of an \\textit{Independent Subspace Structure} for datasets, it can be shown that random projection preserves the subspace structure of data vectors generated from a union of independent linear subspaces. The bound on the minimum number of random vectors required for this to hold is also derived and is shown to depend logarithmically on the number of data samples, not only in independent subspaces but in disjoint subspace settings as well. The theoretical analysis presented is supported in detail with empirical results on real-world face recognition datasets.", "text": "increasing concerns security need highly secure physical biometrics-based authentication systems utilizing cancelable biometric technologies rise. problem cancelable template generation deals trade-off template security matching performance many state-of-the-art algorithms successful generating high quality cancelable biometrics random projection early processing steps. paper therefore presents formal analysis random projections essential step cancelable biometrics. formally deﬁning notion independent subspace structure datasets shown random projection preserves subspace structure data vectors generated union independent linear subspaces. bound minimum number random vectors required hold also derived shown depend logarithmically number data samples independent subspaces disjoint subspace settings well. theoretical analysis presented supported detail empirical results realworld face recognition datasets. popularity last decades growing need cancelable biometric technologies. cancelable biometrics refers systematic intentional repeatable distortion biometrics features order prevent notion stolen biometrics. person’s biometrics stolen speciﬁc modality feature template used assigning biometric user compromised masquerading attacker thus giving attacker access privileges user’s resources. cancelable biometrics especially important need store biometric templates compromised virtually impossible user regenerate physical traits used creating templates enrollment. thus attempt reduce vulnerability security systems increased research activity areas cancelable biometrics problem deals trade-off template security matching performance. state-of-the-art algorithms successful generating high quality cancelable biometrics based random projection course random projection technique alone sufﬁcient generating highly secure discriminating biometric templates ﬁrst fundamental step occurs complex techniques class-preserving transforms template hashing etc. implemented. increasing technological advancements computational speed memory increasing volumes disparate data collected security purposes high dimensional feature vectors used many geometrically deﬁnition says margin between subspaces deﬁned maximum product unit vectors either subspace. vector pair maximize product known principal vector pair subspaces angle vectors called principal angle. notice implies subspaces maximally separated implies subspaces independent. deﬁned concepts goal learn subspace given dataset sampled union independent linear subspaces independent subspace structure property approximately preserved dataset. make idea concrete shortly. notice deﬁnitions independent subspaces separation margin apply explicitly well deﬁned subspaces. natural question deﬁne concepts datasets? deﬁne independent subspace structure dataset follows deﬁnition {xj}n class dataset data vectors data vectors belong class dataset independent subspace structure class data sampled linear subspace subspace independent. again deﬁnition speciﬁes data samples different classes belong independent subspaces. estimate margin subspaces these deﬁne subspace margin datasets follows deﬁnitions make idea independent subspace structure preservation concrete. speciﬁcally subspace structure preservation refer case originally given data vectors sampled union independent linear subspaces biometrics-driven security systems. however since computational time increases dimensionality real-life biometric systems highly susceptible performance degradation time. dimensionality reduction techniques employed overcome problem however applications perform tasks generating secure discriminating biometric templates subspace structure data preserved dimensionality reduction many techniques fail. foresee random projections core component future security systems using biometric modalities face recognition authentication. reason more paper formally deﬁne notion independent subspace structure datasets based deﬁnition show random projection preserves subspace structure data vectors generated union independent linear subspaces. thus technique employed cancelable transform project original biometric template subspace generate cancelable template maintaining discriminability. extensive number papers literature employed random projection data dimensionality reduction tasks k-means clustering classiﬁcation etc. papers shown respective tasks certain desired properties data vectors preserved random projection. however best knowledge general formal analysis linear subspace structure preservation random projections reported thus far; main thrust paper. deﬁnitions linear subspace dimensions represented using matrix rn×d columns form support subspace. vector subspace represented independent subspaces denoted subspace said independent subspaces exist non-zero vector linear combination vectors subspaces. formally subsequently projection projected data vectors also belong union independent linear subspaces. formally class dataset independent subspace structure class samples drawn subspace projected data vectors sets data vectors belong linear subspace subspaces indepen¯ random projections random projection gained signiﬁcant popularity recent years computational costs guarantees comes with. speciﬁcally shown cases linearly separable data data lies dimensional compact manifold random projection preserves linear separability manifold structure respectively given certain conditions satisﬁed. notice union independent linear subspaces speciﬁc case manifold structure hence results random projection manifold structure apply general case. however results derived general case results weak applied problem setting. further best knowledge prior analysis random projection margin independent subspaces. various applications random projection dimensionality reduction rooted following version johnson-lindenstrauss lemma lemma vector matrix rm×n element drawn i.i.d. standard gaussian distribution lemma states norm randomly projected vector approximately equal norm original vector. conventionally elements random matrix generated gaussian distribution proved indeed sparse random matrices achieve goal. aside relation adopting random projection preliminary steps providing template cancelability given cancelable biometric template constructed original template projection matrix initial cancelable template compromised template issued projection matrix replacement. lemma indicates discriminability original feature vector preserved template however conditions required still need investigated. studying conditions required independent subspace structure preservation multiclass problem ﬁrst state cosine preservation lemma simply states cosine angle ﬁxed vectors approximately preserved random projection. similar angle preservation theorem stated state difference presenting lemma. would like point cosine acute obtuse angles preserved random projection evident lemma. however cosine value close zero additive error inequalities distorts cosine signiﬁcantly projection. hand paper state obtuse angles preserved. evidence authors empirically show cosines negative value close zero. however already stated cosine values close zero well preserved. hence serve evidence obtuse angles preserved random projection show empirically otherwise true. notice case lemma error multiplicative hence length vectors preserved good degree invariantly vectors. general inner product vectors well preserved random projection irrespective angle vectors. analyzed using equation rewriting equation following form that holds high probability. clearly error term depends length vectors inner product between arbitrary vectors random projection well preserved. however special case inner product vectors length less preserved error term gets diminished case. ease representation analysis equation making cosine preservation lemma. examine conditions independent subspace structure preserved linearly separatable dataset. order independent subspace structure preserved dataset need conditions hold simultaneously. first data sampled subspace continue belong linear subspace projection. second subspace margin dataset preserved. remark denote data vectors drawn subspace rm×n denote random projection matrix deﬁned before. projection vectors continue along linear subspace span columns denote span theorem {xj}n class dataset independent subspace structure class margin subspace structure entire dataset preserved random projection using matrix rm×n recall discussions cosine preservation lemma cosine values close zero well preserved random projection. however error bound margin turns problem subspaces separated margin close zero implies principal angle almost orthogonal i.e. maximally separated. therefore circumstances projected subspaces also well separated. formally projection upper bounded tends practice much smaller quantity hence well analysis relates structure preservation datasets independent subspace structure hard bounds also apply datasets disjoint subspace structure i.e. subspace pairwise disjoint independent overall. sparse representation widely used classiﬁcation purposes various machine learning applications including face recognition tasks biometric security applications. idea based theory compressed sensing. theory claims system linear equations overcomplete dictionary sparse solution achieved solving basis pursuit algorithm measurement vector rm×n overcomplete dictionary variable want sparse solution. property useful classiﬁcation training samples columns overcomplete dictionary test sample solve optimization obtain sparse reconstruction coefﬁcient training samples. advantage representing test sample sparse linear combination training samples fewer non-zero coefﬁcients training samples recently sparse subspace clustering used subspace clustering applications. subspace clustering domain assumes individual class lies along linear independent subspace assumption want cluster given data samples cluster corresponds samples subspace. authors approach show that basis pursuit optimization guarantees correct reconstruction test sample using overcomplete dictionary training samples formally stated following theorem theorem rm×n matrix whose columns drawn union independent linear subspaces. assume points within subspace general position. point subspace solution problem sparse belongs subspace otherwise. denotes column matrix theorem gives sufﬁcient condition guaranteed recover correct coefﬁcients given test sample using property used algorithm clustering. however also clearly shows makes sense sparse representation task classiﬁcation assumption classes along independent linear subspaces. assumption widely used applications like face recognition motion segmentation. anteed preserve structure dataset. apply aforementioned algorithms much smaller feature space without losing accuracy simultaneously much faster. preceding section showed random projection preserves underlying structure datasets thus effectively used dimensionality reduction. notice advantage random projections three fold allows classiﬁcation/recognition algorithm faster; extremely inexpensive compute; yields classiﬁcation results accuraies original dimensions data. dimensionality reduction algorithms expensive terms computing projection vectors random projection needs element projection vectors sampled randomly independent data hand. nonadaptive nature random projection makes powerful dimensionality reduction tool. qualities indicate random projections becoming essential technique developing efﬁcient highly secure biometric applications. empirical analysis section present empirical evidence support theoretical analysis random projections work cancelable biometrics. perform experiments show cosine preservation subspace structure preservation random projections using different face recognition datasets. projection irrespective angle acute obtuse. however also stated cosine values close zero well preserved. here perform empirical analysis vectors varying angles arbitrary length verify same. order achieve this settings similar generate random projection matrices rm×n vary dimension original space. deﬁne empirical rejection probability cosine preservation similar acute angle randomly generate vectors arbitrary length ﬁxed cosine values obtuse angle similarly generate vectors ﬁxed cosine values {−.−.−.−.}. compute empirical rejection probability mentioned different values figure shows results vectors. ﬁgure notice rejection probability decreases absolute value cosine angle increases well higher value notice cosine values close zero rejection probability close even high dimensions. results corroborate theoretical analysis lemma section. compute empirical rejection probability mentioned different values figure shows results vectors. evident ﬁgure inner product vectors well preserved result line theoretical bound equation vector lengths experiment arbitrarily greater required number random vectors study number random vectors required subspace preservation varying different parameters. lower bound number random vectors required theorem hold given seen random projection lower dimensions effective sufﬁces. choice depends robustness algorithm towards noise trade-off noise number random vectors required. subspace structure preservation section goal show random projections achieve accuracy better least widely used dimensionality reduction technique report comparative analysis accuracy results performance times random projections pca. selected alone detailed analysis mainly found performance nonlinear dimensionality reduction techniques signiﬁcantly less techniques. testing extended yale dataset initially used laplacian eigenmaps reduce data dimensions. best performing reduction techniques yielded result compared close accuracies resulting random projections pca. fairness techniques make claim preserving original subspace structure data rather preintent showing random projections achieve accuracy better least sparse representation based classiﬁcation technique exploits subspace structure data. always better classiﬁcation algorithm exploits structure achieve higher accuracy. however compare different classiﬁcation algorithms show random projection computationally inexpensive dimensionality reduction tool performance guarantees supported theoretical analysis. cancelable biometrics face templates testbedof-choice generally assumed face images illumination variation along linear independent subspaces following datasets evaluation extended yale dataset consists frontal face images individuals images person. images taken constrained varying illumination conditions. crop images concatenate pixel intensity form feature vectors. train-test split evaluation. dataset pose illumination expression database consists images people different poses illumination conditions different expressions. however utilize ﬁrst classes dataset train-test split evaluation. cropped size pixels. pixel intensities concatenated form feature vectors. perform types experiments. first compare time taken dimensionality reduction random projections datasets. time time taken either algorithm compute it’s projection vectors project entire dataset projection vectors. results shown table extended yale dataset dataset respectively. results show random projections faster least order times. secondly show classiﬁcation accuracies datasets dimensionality reduction. results shown table extended yale dataset dataset respectively. clearly random projections performs better signiﬁcantly faster. used projection yields good accuracy. observation explained using lemma authors show given data lies along dimensional subspace needs random vectors. real applications value usually i.e. classes usually along dimensional subspace. thus surprising even small number random vectors yield high accuracy. major advantage random projections occurs streaming data constantly changing. also long data lies d-dimensional subspace stated lemma random projection vectors preserve length vectors subspace hence structure preservation results still hold true. thus results hold true ﬁxed size dataset also inﬁnite stream data vectors long sufﬁcient number random vectors used underlying data structure remains same. originally stated section random projections technique complete solution generating highly secure discriminating biometric templates. although random projection step secure brute-force attack original templates often realvalued high-dimensional projection matrix well protected attacker could construct pseudoinverse recover approximation original data. nevertheless advantages random projections namely allowing classiﬁcation/recognition algorithm faster; extremely inexpensive compute; yielding classiﬁcation results accuracies original dimensions data random projections quickly becoming essential early-step technique development efﬁcient highly secure biometric applications. conclusion paper presented formal analysis random projections essential initial step generating cancelable biometrics especially real-life scenario security discriminability cancelability required. using random projections dimensionality reduction ensures independent subspace structure datasets preserved. derived bound minimum number random vectors required hold concluded number depends logarithmically number data samples. arguments hold disjoint subspace settings well. side analysis also showed cosine values preserved random projection although describe work context cancelable biometrics discussion evaluations presented detailed analysis linear subspace structure preservation random projections irrespective task-at-hand. balcan maria-florina blum avrim vempala santosh. kernels features kernels margins lowth international condimensional mappings. ference algorithmic learning theory georghiades a.s. belhumeur p.n. kriegman d.j. many illumination cone models face ieee recognition variable lighting pose. trans. pattern anal. mach. intelligence alwyn david c.l. computation crypcommunicatographic keys face biometrics. tions multimedia security. advanced techniques network data protection volume lecture notes computer science springer berlin heidelberg xiaofei deng shuicheng zhang hongcomjiang. neighborhood preserving embedding. puter vision iccv tenth ieee international conference volume vol. hegde chinmay wakin michael baraniuk richard random projections manifold learning. platt john koller daphne singer yoram roweis nips. curran associates inc. ping hastie trevor church kenneth sparse random projections. proceedings sigkdd international conference knowledge discovery data mining york acm. terence baker simon bsat maan. pose illumination expression database. automatic face gesture recognition proceedings. fifth ieee international conference ieee teoh a.b.j. d.c.l. random multispace quantization analytic mechanism biohashing biometric random identity inputs. ieee trans. pattern anal. mach. intelligence", "year": 2014}