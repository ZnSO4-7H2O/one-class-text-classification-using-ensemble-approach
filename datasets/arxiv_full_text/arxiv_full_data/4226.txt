{"title": "Left-Right Skip-DenseNets for Coarse-to-Fine Object Categorization", "tag": ["cs.CV", "cs.AI"], "abstract": "Inspired by the recent neuroscience studies on the left-right asymmetry of the brains in the low and high spatial frequency processing, we introduce a novel type of network -- the left-right skip-densenets for coarse-to-fine object categorization. This network can enable both coarse and fine-grained classification in a single framework. We also for the first time propose the layer-skipping mechanism which learns a gating network to predict whether skip some layers in the testing stage. This layer-skipping mechanism assigns more flexibility and capability to our network for the categorization tasks. Our network is evaluated on three widely used datasets; the results show that our network is more promising in solving the coarse-to-fine object categorization than the competitors.", "text": "figure example images assess cerebral asymmetries spatial frequencies. hemispheric specialization left hemisphere /the right hemisphere predominantly involved local/global letter identiﬁcation. figures another question coarse-to-ﬁne sequence being processed human brains? recent biological studies postulate cerebral hemispheres differently involved spatial frequency processing i.e. left-right asymmetry brain. left hemisphere /the right hemisphere predominantly involved high/low spatial frequency processing respectively. illustrated fig. top-left ﬁgure large global letter made small local letters; top-right ﬁgure inspired recent neuroscience studies left-right asymmetry brains high spatial frequency processing introduce novel type network leftright skip-densenets coarse-to-ﬁne object categorization. network enable coarse ﬁne-grained classiﬁcation single framework. also ﬁrst time propose layer-skipping mechanism learns gating network predict whether skip layers testing stage. layer-skipping mechanism assigns ﬂexibility capability network categorization tasks. network evaluated three widely used datasets; results show network promising solving coarse-to-ﬁne object categorization competitors. recent success deep neural networks convolutional neural networks recurrent neural networks inspired researchers further explore mechanisms functions human brains. even though engineering-tailored surpass human performance several speciﬁc vision tasks simulating ﬂexibility complexity human brain system still hard current level nns; artiﬁcial general intelligence also mission impossible near future. however researchers believe biological plausibility guide design intelligence systems achieve neuroscience-level understanding; particularly focus visual understanding paper. though still lots arguments towards exactly visual analysis processed within brain considerable evidence showing visual analysis take place predominately default coarse-toﬁne sequence shown fig. interestingly coarse-to-ﬁne perception also proportional length cerebral circuit path i.e. time. example presenting image fig. short time coarse visual stimuli perceived sand umbrella usually spatial frequencies. nevertheless given longer time ﬁne-grained details scene image. bottom-left bottom-right corresponding sinusoidal grating corresponding images above. fig. given input visual stimuli high activated regions corresponding high spatial frequencies. additionally left-right asymmetry brain argued result evolution human beings initial functional conﬁgurations roughly same; cerebral asymmetries represented different synapsis connections distinctive ability processing high spatial frequency come evolution process. light understanding mimic hemispheric specialization ﬁrst time propose left-right skip-densenets coarse-to-ﬁne object categorization tasks. network enable coarse-to-ﬁne object categorization single framework. whole network structured fig. network branches referring respectively. branches roughly initialized layers structures. networks built upon skip-dense blocks transition layers. former abstraction block visual concept abstraction; latter aims manipulating size feature maps learned previous layer. unique connections built learning varying knowledge abstraction. functionality branch memorized given input supervised information learning stage. guide arrow refers information ﬂows coarse branch branch; indicates ﬁne-grained categorization tasks also information learned coarse categorization tasks. mechanism also observed human brain ﬁrst time introduce layer-skipping mechanism single input utilize part layers deep model purpose computation sparsity ﬂexibility. organisms like humans tend energy wisely task given visual stimuli recent study neuroscience also showed synaptic cross-layer connectivity common human neural system especially abstraction level. contrast state-of-the-art deep convolutional neural networks mechanism process input data entire network layers inference. computational intensive; also recent study found examples easy correctly classiﬁed without utilization deep networks testing stage. particular propose gating network learns predict whether skipping convolutional layer testing stage. networks evaluated three datasets coarse-to-ﬁne object recognition tasks. experimental results strongly support claims thus used validate proposed network. densenets coarse-to-ﬁne object classiﬁcation tasks. main novelties networks come several parts. left right branch network structure solve coarse-to-ﬁne classiﬁcation ﬁrstly proposed. network inspired recent theory neuroscience hemispheric specialization. left-right asymmetry brain widely studied psychological examination functional imaging primates recent research showed shape neurons density neurotransmitter receptor expression depend laterality presynaptic origin also proof terms information transfer indicates connectivity changes within left right inferotemporal cortexes result recognition learning learning also differs local population well theta-nested gamma frequency oscillations hemispheres greater synchronization theta across electrodes right left recently argued left hemisphere specializes controlling routine tends focus local aspects stimulus right hemisphere specializes responding unexpected stimuli tends deal global environment information refer recent survey paper deep architectures. starting notable victory alexnet imagenet classiﬁcation contest boomed exploration deep architectures. later proposed deep residual networks mapped lower-layer features deeper layers shortcut connection element-wise addition making training hundreds even thousands layers feasible. prior work highway networks devised shortcut connection input-dependent gating units. recently proposed compact architecture called densenet integrated shortcut connections make early layers concatenated later layers. simple dense connectivity pattern surprisedly achieves state-of-the-art accuracy fewer parameters. different shortcut connections used densenets resnets layer-skipping mechanism learnt predict whether skipping particular layer testing stage. feedback networks developed coarse-to-ﬁne representation recurrent convolutional operations previous iteration’s output gives feedback prediction next iteration. different additionally network also different previous branch networks siamese two-stream networks usually different input data branch solve coarse-to-ﬁne object categorization problems. conditional computation. evaluation deep models still time-consuming resource-intensive. efforts made bypass problem including network compression conditional computation often refers input-dependent activation neurons unit blocks resulting partial involvement fashion network. learns drop data points data blocks feature thus taken adaptive variant dropout. introduced stochastic times smooth neurons binary gates within deep neural network termed straightestimator whose gradient learned heuristically back-propagating threshold function. proposed ’standout’ technique uses auxiliary binary belief network compute dropout probability node. tackle problem selectively activating blocks units reinforcement learning. later used controller trained reinforce examine constrain intermediate activations network test-time. incorporates attention resnets learning image-dependent early-stop policy residual units layer level feature block level. layerskipping mechanism different points learns drop data points data block feature whilst gating network learns predict whether skipping layers. cases usually employs reinforcement learning methods in-differentiable loss function needs huge computational cost policy search algorithm; contrast gating network differentiable function used layer network. simulate left-right hemispheric specialization propose left-right densenets architecture shown fig. input image ﬁrstly processed ﬁrst convolutional layer shared subnets representing lowlevel visual signal processing module. subnet stacked mainly abstraction blocks transition layers iteratively. fig. deﬁne skip-dense block viewed level visual concept abstraction information ﬂows adaptively changed gating networks. transition layer convolutional pooling operations feature maps change channel number spatial size. guide link fig. indicates coarse/global branch help guide ﬁne/local also inspired coarse-to-ﬁne cortical model finally pooling linear classiﬁer added branch prediction. left right subnets equivalent general structure perform differently different grained level supervised information. branch coarse level object class information another branch level object class information. given input coarse/global path commonly shorter faster ﬁne/local since learned rougher information. hidden paths residual networks containing shortcut connections behave like ensembling numerous shallower paths thus removing layers networks would degrade performance seriously. similarly densenets containing dense connectivity pattern allow layer access preceding layers make individual layers additionally supervised shorter connections conclude resnets densenets improve data short paths throughout networks. allowing layer selection mechanism densenet containing dense layers obtain hidden paths. considering ensembling many hidden paths intuitively redundant per-input basis believe introducing path selection speedup inference process even enlarge capacity network ﬂexible dynamic path selection make evaluation targeted single input. besides mechanism increases complexity diversity nested convolutions still unexplored. note experimental evidence also indicate path optimization exist parvocellular pathways left right hemispheres high-pass signals processed. skip-dense block discuss structure skip-dense block fig. input pattern shallower abstraction level shortcut connection applied dense layer. option ﬂowing dense layer conditionally checked cheap gating network input-dependent. output pattern processed next transition layer entered next abstraction level. gating gating network introduced predict whether skipping convolutional layer terms input data pattern. also taken special type regularizations input data complex gating network inclined skip many layers; vice versa. here utilize fully-connected layer figure left-right skip-densenets architecture. left homogeneous subnets derived densenets process input visual information asymmetrically. branch predict coarse/global level object classes predict ﬁne/local level object classes. share prelimilary visual processing module namely ﬁrst convolutional layer here. subnet stacked mainly abstraction blocks transition layers iteratively. guide link delivers global context information coarse/global subnet ﬁne/local one. right skip-dense block contains several skipable densely connected convolutional layers controlled cheap afﬁliated gating network. slope variable threshold function regulated data distribution statistics previous outputs function training. gradually adjusting value proportion ‘unsettled’ output values lying appropriately scheduled training margin error. slope variable dense layer policy designing training threshold function estimator critical success skipping mechanism. luckily magnitude unit often determines importance categorization task cnns. based observation derive simple end-to-end training scheme entire network. speciﬁcally output threshold function multiplied unit convolutional layer output affects layer importance categorization. long threshold function differentiable gating network training incorporated back-propagation naturally smoothly. mutual adjustment regularization gatings dense layers abbreviate problem training difﬁculty hard conadjusted independently. ending training adjusted sloped variables gating functions large enough learned gating networks approximate binary decisions. evaluation compute dense layer gating output approaches skip gating output approaches gating networks make binary decisions inference achieve computational sparsity. merge merge preceding layers includes types operation channel concatenation element-wise addition. former used densenets claimed improve feature reuse efﬁciency. instance later identity mappings resnets. different merge operations heavily lead different behaviors deep architectures. pre-activation unit facilitates merge operations observation also used model. replace skipped convolutions feature maps ﬁlled zero values skipped convolutions merge evaluation make addition concatenation feasible. merged feature maps rescaled properly according gating results training test. different standard dropout layer dropping feature rescaling model dynamic data-dependent. transition layer transition layer contains convolution interaction cross channel information pooling downsampling feature maps needed. merge operations skip-dense blocks concatenation type convolution transition reduce number feature channels compression. merge operations addition type convolution transition increase number feature channels higher level information distillation. guide faster coarse/global subnet guide slower ﬁne/local subnet global context information objects higher levels. here select last transition layer delivering guiding information. speciﬁcally output feature penultimate skip-dense block global subnet copied concatenated output feature penultimate skip-dense block local subnet. intuitively injection feedback information coarse level beneﬁcial ﬁne-level object categorization. datasets. conduct experiments three datasets namely sb-mnist cifar- stanford cars inspired experiments build sb-mnist dataset randomly selecting images mnist ﬁgures using ﬁrst local ﬁgure construct second one. generate training images testing images building sb-mnist dataset. examples illustrated fig. iii. stanford cars dataset another ﬁnegrained classiﬁcation dataset. contains images classes describes properties maker model year car. terms basic types deﬁned categorized coarse classes including sedan coupe convertible pickup hatchback wagon. images resized top- mean accuracy reported ﬁne-grained coarse-level classiﬁcation. feedback instantiated using existing recurrent neural networks feedback based learning architecture representation formed iterative manner based feedback received previous iteration’s output. thus feedback better classiﬁcation performance standard cnns. densenet densenet connects layer preceding layers feed-forward fashion; design skip-dense block derived densenet. thus comparing densenet network components gating network skip unnecessary layers; branch structures solving coarse-to-ﬁne classiﬁcation. resnet extension traditional cnns learning residual layer enable network trained substantially deeper previous cnns. implementation details. mainly report conﬁguration cifar-. model structures smaller datasets similar. concat-type network start feature maps shared convolutional layer. branches built based densenet- namely growth rate skip-dense blocks. compression pooling applied transition layer. addition-type network start feature maps shared convolutional layer. branches separately contain skip-dense blocks dense layers each. feature maps doubled convolutions transition layers pooling applied every skip-dense blocks. networks merge types transition layer last skip-dense block average pooling follows instead. pooling operations gating networks average pooling. data preprocessing training procedure follow dropout rate fully connected layers default element-wise addition soft sigmoid function merge gating network respectively. main results discussion sb-mnist results dataset compared tab. local global indicates classiﬁcation small number number. dataset densenet resnet lenet running separately local global tasks. dataset gives general understanding task. classiﬁcation small number number image corresponds identiﬁcation task high spatial frequency processing spatial frequency processing comparing baselines method jointly solve tasks i.e. coarse-toﬁne processing. judging results tab. method greatly outperforms methods clear margin. largely reasons. firstly skip-dense block efﬁciently gradually learn visual concepts; secondly guide network transfers learned information coarse branch ﬁner branch. performance original version lenet greatly suffered number layers convolutional ﬁlters. cifar- compare results tab. compare feedback densenet resnet well previous works. local global indicate classiﬁcation tasks ﬁne-grained coarse-level individually. densenet resnet running separately local global tasks respectively. tab. network greatly outperforms baselines. note local global classiﬁcation tasks densenet resnet running separately whilst feedback network jointly. comparing feedback ours accuracy margins local global individually. table results cifar- dataset. depths resnet densenet feedback respectively. note virtual depth feedback reported network uses layers averagely layers skipped testing step. reported implies network better capability learning coarse-to-ﬁne information branches. interestingly even global task network still around improvement feedback densenet. possible explanation since global task relatively easy networks standard layers tend overﬁt training data; contrast gating network skip layers avoid problem overﬁtting degree. stanford cars show results tab. still three baselines compared dataset. local global tasks still refer classiﬁcation ﬁnegrained coarse-level classiﬁcation. firstly feedback hits accuracy global task higher results. possible reason good global performance feedback comes early prediction stage effectively avoid overﬁtting global task. hand gating network skip-dense block network also prevent overﬁtting skipping layers global task relatively easy. thus network still outperforms densenet resnet global task. finally guiding information coarse branch branch network achieve best performance local task outperforming feedback implies network better coarse-to-ﬁne processing ability feedback also prove effectiveness proposed framework. merge types gating functions compare different choices merge types gating functions. results compared tab. evaluation metrics used here namely accuracy skip ratio. skip ‘adtable results cifar- dataset. dition’‘hard’‘soft’ indicate channel concatenation element-wise addition soft sigmoid function hard sigmoid function. judging results tab. combination element-wise addition merge gating soft sigmoid best network conﬁguration. speciﬁcally notice soft sigmoid generally better hard sigmoid merge types cases. postulate probably good differential property soft sigmoid function better stabilized optimized back-propagation step. compare skip ratio local global tasks. testing stages coarse branch network skip much layers global tasks skipped layers sibling branch. largely fact global task easy using relative layers coarse branch good enough grasp coarselevel information. interestingly point also follows coarse-to-ﬁne perception recent neural science study spatial frequency information predominantly processed right hemisphere relatively shorter paths cerebral system. skip ratio accuracy verify ﬂexibility model vary threshold gating network affect accuracy local global tasks reported fig. ratios. expected optimal range skip ratio branches different even overlapped different grained level recognition tasks. global branch optimal skip ratio range. local branch optimal skip ratio range lower wider global branch. good thing learned gating networks dense layer make performance branches keep consistent under various user-deﬁned thresholds. also notice performance trained branches without layer skipping inferior case skipping dense layers also signiﬁcantly different standard dropout tranditional conditional computation. intriguing phenomenon suggests individual dense layer model learns specialized visual abstraction information. words skipping control gating networks critical success recognition. withskipping layer network become disordered. inspired recent study left right hemisphere specialization coarse-to-ﬁne perception proposed novel left-right skip-densenets coarse-to-ﬁne object categorization. design philosophy make network simultaneously classify coarse ﬁnegrained classes. additionally layer-skipping behaviour densely connected convolutional layers controlled auxillary gating networks ﬁrst time proposed. experiments three datasets validate performance show promising results network.", "year": 2017}