{"title": "An EM Based Probabilistic Two-Dimensional CCA with Application to Face  Recognition", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Recently, two-dimensional canonical correlation analysis (2DCCA) has been successfully applied for image feature extraction. The method instead of concatenating the columns of the images to the one-dimensional vectors, directly works with two-dimensional image matrices. Although 2DCCA works well in different recognition tasks, it lacks a probabilistic interpretation. In this paper, we present a probabilistic framework for 2DCCA called probabilistic 2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the parameters. Experimental results on synthetic and real data demonstrate superior performance in loading factor estimation for P2DCCA compared to 2DCCA. For real data, three subsets of AR face database and also the UMIST face database confirm the robustness of the proposed algorithm in face recognition tasks with different illumination conditions, facial expressions, poses and occlusions.", "text": "abstract recently two-dimensional canonical correlation analysis successfully applied image feature extraction. method instead concatenating columns images one-dimensional vectors directly works two-dimensional image matrices. although dcca works well diﬀerent recognition tasks lacks probabilistic interpretation. paper present probabilistic framework dcca called probabilistic dcca iterative based algorithm optimizing parameters. experimental results synthetic real data demonstrate superior performance loading factor estimation pdcca compared dcca. real data three subsets face database also umist face database conﬁrm robustness proposed algorithm face recognition tasks diﬀerent illumination conditions facial expressions poses occlusions. although many real-world applications encounter high dimensional data informative part data modeled dimensional space. moreover processing high-dimensional data time consuming process requires lots resources. tackle problems feature extraction used tool ﬁnding compact meaningful data representation. single-mode source data subspace learning methods conducted learn semantic description subspaces. examples methods principal component analysis linear discriminant analysis however observations sources share mutual information canonical correlation analysis popular approach dimensionality reduction. seeks lower-dimensional space sets variables maximally correlated projecting technique widely used diﬀerent ﬁelds pattern recognition computer vision bioinformatics etc. cca-based methods necessary vectorize image matrices. vectorization three main drawbacks breaking spatial structure image data cause losing potentially useful structural information among column/rows leading high-dimensional vector space small sample size problem turn makes diﬃcult calculate covariance matrices causing covariance matrices large turn makes eigen-decomposition large matrices time-consuming. overcome drawbacks two-dimensional introduced choi computes directions based image matrices. proposed dcca overcomes curse dimensionality signiﬁcantly reduces computational cost directly working images instead reshaping vectors. higher recognition accuracies reported using dcca compared using face databases time complexity improved. however associated probabilistic model observed data notably absent form feature extraction methods. probabilistic feature extraction algorithm could intuitively appealing many reasons bridge tipping bishop proposed probabilistic based latent variable model known factor analysis proposed ppca used framework many formulations also probabilistic models proposed bach jordan also proposed probabilistic interpretation estimate parameters proposed model using maximum likelihood expectation maximization. recently many inspiring research proceeded domain including kernel based semiparametric nonparametric methods domain feel work required. bridge probabilistic model dcca introduced safayani showed maximum likelihood estimation parameters leads dimensional canonical correlation directions. however propose based solution model. require explicit eigendecomposition covariance matrices. moreover using possible handle models incomplete data mixture models cluster labels missing values paper present probabilistic interpretation dcca referred pdcca together based solution estimate parameters model. proposed model handle small sample size problem eﬀectively. rest paper organized follows section brieﬂy reviews related algorithms pcca dcca necessary understand proposed algorithms work. proposed pdcca model introduced section section experiments synthetic data several face databases given evaluate performance proposed algorithm; ﬁnally paper concluded section imagine given sets random vectors realizations corresponding random vectors respectively. seeks transformation vectors correlation maximized. correlation error term assumed follow multivariate gaussian distributions zero mean inverse covariance matrix bach jordan proved maximum likelihood estimation parameters model would lead canonical directions. maximum likelihood estimates projection matrices given transforms right transforms projection data would form tivi. dcca ﬁnds left right transforms maximize correlation projected data. therefore objective function dcca formulated realization random matrix left transforms right transforms obtained iteratively solving equations convergence. eigenvectors associated largest eigenvalues determine left transform matrices eigenvectors associated largest eigenvalues determine right transform matrices using transform matrices possible project data high dimensional space lower dimensional feature space. section propose probabilistic two-dimensional embased solution ﬁnding parameters model. model observed data modeled two-dimensional matrices follows estimate parameters ﬁrst must calculate expectation loglikelihood take derivative expected log-likelihood respect parameter. unfortunately closed-form solution simultaneously. inspired decoupled probabilistic model employed obtain projection matrices separately using alternating optimization procedure. model ﬁrst assume value projection matrices e.g. right projeci= known. observations projected corresponding latent spaces. projection procedure probabilistic introduced section this left probabilistic model deﬁned left model latent matrix mean matrix left projected observations noise source left probabilistic model columns noise matrix follow normal distribution zero mean covariance matrix deﬁnition parameter left probabilistic model would estimated using expectation maximization procedure. estimation procedure explained later section. similar procedure parallel left probabilistic model right known. observations projected corresponding latent spaceshence right probabilistic model deﬁned apply algorithm decoupled probabilistic model e-step expectation likelihood left probabilistic model right probabilistic model computed separately. expected likelihood maximized respect parameters. following subsections describe optimize left right probabilistic model respectively. apply algorithm decoupled probabilistic model probabilistic models expectation likelihood function calculated e-step detail given appendix maximization step done maximizing respect values parameters estimated evaluated algorithm synthetic real data. synthetic data part veriﬁed implementation pdcca algorithm simple synthetic data randomly generated projected matrices. also compared algorithm dcca method projection matrices estimation real part evaluation proposed pdcca method used face recognition well-known face image databases umist database divided three subsets evaluating performance system regard diﬀerent illumination expression occlusion conditions. umist database used obtain performance dealing pose variation. section verify implementation proposed method simplest possible scenario. generate synthetic data projection matrices. estimate projection matrices using method compare true ones. know pdcca estimations uniform distribution interval consider ground truth projection matrices obtained using equation element residual matrices sampled gaussian distribution mean variance. synthetic data pdcca algorithm discussed figure calculate also dcca obtain corresponding projection matrices. compare obtained matrices algorithms ground truth projection matrices. cancel scale factors divide transform norm comparison. euclidean distance utilized compare normalized transforms. figure shows results. obvious worst-case distance value becomes best-case zero. depicted ground truth compared obtained dcca. experiment used generated samples examine eﬀect selections repeat experiment diﬀerent values sample numbers also noise variances. figure demonstrates results. observed ﬁgure pdcca estimation cases much closer ground truth compared dcca method. occlusions. individuals sessions images taken diﬀerent time periods. session contains images. experiments used ﬁrst session individuals second session images. collected face images people person diﬀerent face images collected images three diﬀerent illumination conditions; three diﬀerent expressions; three occlusions remaining images neutral expression occlusion known reference images experiments. examine performance proposed methods diﬀerent conditions partitioned collected images three subsets known ar-. individual contains four images three images diﬀerent lighting conditions remaining reference image. used test performance algorithms exist expression variation. involves four images individuals three images diﬀerent expressions last reference image. prepared test performance presence occlusion. again subset contains four images individuals three images taken glasses last reference image. figure shows exemplary face images woman respectively. image gray scaled resized normalized gorithms diﬀerent versions including ppca pcca dpca dlda dcca. based methods work data. also based methods supervised based algorithms unsupervised. task investigate well diﬀerent algorithms relate face images varying illumination conditions expressions occlusion correspondence reference face images. ppca pcca dcca dpca dlda pdcca used extract features facial images classiﬁer employed classiﬁcation. note based output type algorithms dcca dpca dlda pdcca frobenius distance used calculate distance feature matrices ppca pcca common euclidean distance measure adopted. furthermore noted since pcca suﬀers small sample size problem implementing using formulas introduced caused covariance matrices singular. solve problem dimension reduction using implementing algorithm. evaluate recognition accuracy used three-fold cross-validation. evident based algorithms need sets images training paper training sets called left training right training set. form training sets e.g. neutral images considered left training form right training three images individual diﬀerent illumination conditions selected randomly. images considered test images. procedure repeated three times time diﬀerent image among three images selected right training neutral images always used form left training set. test performance algorithm images right left training sets projected feature spaces using corresponding transforms. also test images projected feature spaces projection every test image. calculate distance projected test images projected training images. label training image nearest projection test image projection determine ﬁnal class test image. procedure iterates ﬁnal class images test set. ﬁnal classes compared real classes images recognition accuracy algorithm calculated. finally average recognition rate three round experiments recorded ﬁnal recognition accuracy. since based algorithms work data fair comparison used images training images test data neutral images always training data together images diﬀerent illuminations iterations. process repeated three times ﬁnal accuracy average three runs. figure shows test process pdcca. table demonstrate recognition accuracy evaluated algorithms experiments conducted respectively. tables dimension reduced feature space. note output dimensional algorithms therefore experiments iteration algorithm i.e. tmax signiﬁcantly reduces computational cost algorithm. tried iterations signiﬁcant improvement recognition rate. figure shows results iterations results support idea choosing tmax umist face database also known sheﬃeld face database consists images subjects. subjects diﬀerent races sexes appearances. subject images diﬀerent poses proﬁle experiment images samples subject used examine performance diﬀerent algorithms face orientation varies signiﬁcantly. figure shows images subject. select frontal image well seven randomly selected images training remaining images test set. training phase based methods frontal image always selected left training image seven images right training image. classiﬁer used classiﬁcation. procedure repeated twenty times average recognition rates algorithms reported. table shows recognition accuracy evaluated algorithms experiments conducted umist. test pdcca achieved slightly better performance compared dcca best. fact achieved best performance. test images class training images class leaded best performance supervised method. ignoring pdcca performance higher methods. since umist face dataset contains subjects discriminated features limited able compare results algorithms showed results larger values algorithms. experiments showed accuracy pdcca consistently better based methods i.e. pcca dcca. question sill remains diﬀerences statistically signiﬁcant?. section answered question evaluating experimental results using independent-samples t-test section also next section considered based algorithms including pcca dcca pdcca since goal paper compare functionality newly proposed based method based algorithms. desired signiﬁcance level null hypothesis signiﬁcant diﬀerence recognition rates pdcca pcca dcca respectively. reject null hypothesis whenever resulted ρ-value becomes lower case result considered statistically signiﬁcant. necessary note t-test dataset algorithm considered highest recognition rate. table shows ρ-value test. seen table pdcca signiﬁcantly outperforms algorithms null-hypothesis rejected cases. plicity considered equal i.e. analysis. table shows computational complexity algorithms. table number random samples dataset. noted types iteration corresponding methods; iteration necessary convergence part algorithm iteration alternating optimization procedure left right model. show former latter table. however experiments. paper proposed probabilistic model dimensional termed pdcca together em-based solution estimate parameters model. experimental results demonstrated functionality proposed method. proposed pdcca many advantages dcca signiﬁcant advantage ability extend mixture pdcca model. also possible develop probabilistic bayesian model pdcca gaining beneﬁts bayesian model. future works.", "year": 2017}