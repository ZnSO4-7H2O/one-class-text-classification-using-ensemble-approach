{"title": "The Neural Representation Benchmark and its Evaluation on Brain and  Machine", "tag": ["cs.NE", "cs.CV", "cs.LG", "q-bio.NC"], "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4. In our analysis of representational learning algorithms, we find that three-layer models approach the representational performance of V4 and the algorithm in [Le et al., 2012] surpasses the performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance comparable to that of IT for an intermediate level of image variation difficulty, and surpasses IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that exceeds our current estimate of IT representation performance. We hope that this benchmark will assist the community in matching the representational performance of visual cortex and will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "text": "requirement development effective learning representations evaluation comparison representations know effective. natural sensory domains community viewed brain source inspiration implicit benchmark success. however possible directly test representational learning algorithms directly representations contained neural systems. here propose benchmark visual representations directly tested neural representation multiple visual cortical areas macaque computer vision algorithm produces feature space tested. benchmark measures effectiveness neural machine representation computing classiﬁcation loss ordered eigendecomposition kernel matrix analysis neural representation visual area superior visual area indicating increase representational performance higher levels cortical visual hierarchy. analysis representational learning algorithms three-layer models approach representational performance algorithm surpasses performance impressively recent supervised algorithm achieves performance comparable intermediate level image variation difﬁculty surpasses higher difﬁculty level. believe result represents major milestone ﬁrst learning algorithm found exceeds current estimate representation performance. enable researchers utilize benchmark make available image datasets analysis tools neural measurements hope benchmark assist community matching representational performance visual cortex serve initial rallying point correspondence representations derived brains machines. primary goals representational learning produce algorithms learn transformations unstructured data produce representational spaces well suited problems interest visual object recognition auditory speech recognition. pursuit goal brain representations produces used source inspiration even suggested benchmark success ﬁeld. work attempt provide benchmark measure progress representational learning deﬁned measures success relative high-level visual cortex. machine learning signal processing communities achieved many successes incorporating insights neural processing even complete understanding neural systems lacking. initial formulations neural networks took explicit inspiration neurons might transform inputs david lowe original formulation sift algorithm cites inspiration complex cells primary visual cortex cortex concepts hierarchical processing intermediate features also history cross pollination computer vision neuroscience cross pollination also great inﬂuence ﬁeld neuroscience suggested ways investigate brain works suggesting speciﬁc hypotheses computational principles. work presented here architectures algorithms devised hierarchical neural networks serve concrete hypotheses computational mechanisms used visual cortex achieve fast robust object recognition. believe neuroscience ﬁeld needs concrete hypotheses hope latest representational learning algorithms void. measure representational efﬁcacy? quantitative evaluation progress made representational learning must address question. advocate kernel analysis formulated works believe kernel analysis main advantages. first measures accuracy representation function complexity task decision boundary. allows identify representations achieve high accuracy given complexity. also avoids measurement confound arises using cross-validated accuracy decision boundary’s complexity and/or constraints dependent size choice training dataset factors strongly affect accuracy scores. measuring accuracy affected complexity decision boundary kernel analysis allows explicitly take dependency account. second kernel analysis particularly advantageous comparisons models brain robust number samples used measurement. ability measure neural activity brain increased exponentially analysis simultaneous recording growth rate related number stimuli measured) still orders magnitude away dataset sizes achieved machine learning community. reason measures useful low-sample regime particularly important evaluating performance neural representations. kernel analysis exhibits property converges quickly function number samples used analysis. therefore measures representational efﬁcacy related kernel analysis utilize kernel analysis convergence properties explicit measurement accuracy versus complexity. general number methodologies might consider comparing algorithms neural responses. approach model neural variation directly approach valid scientiﬁcally pursuit understanding neural mechanisms lacks representational aspect. example details neural activity representational value insofar variation relate variable interested representing outside neural mechanism. therefore seek measure blends neural measurement representational tasks interest. approach downsides; troubling must choose speciﬁc aspect world represented neural system. hope chosen task neural system effectively represents ideally neural system optimized represent. major unaccomplished goal computational neuroscience determine representation formed brain ﬁnding mapping external factors neural response. methodology propose claim solved problem choosing aspects world brain optimized represent believe chosen reasonable task aspect visual environment category-level object recognition. relation scientiﬁc goal ﬁnding aspects world brain representing kernel analysis measure aspects world brain optimized represent attributes environment neural representation found perform well aspects brain optimized represent. however examination beyond scope paper. work builds series previous efforts measure representational efﬁcacy models brain. work nikolaus kriegeskorte colleagues example examined variation present neural populations visual stimuli presentations compared variation variation produced model feature spaces stimuli. work inﬂuenced pursuit ﬁnding mappings major downside purposes measure variations neural model spaces relevant particular task class-level object classiﬁcation. exist number published accounts neural datasets might useful type comparison seek measurements released often made handful images measures given typically cross-validated performance robust image counts kernel analysis metric here. comparing algorithms brain important choose carefully neural system measure type neural measurement make. work analyze ventral stream macaque monkey non-human primate species. using macaque visual cortex allows leverage extensive literature includes behavioral measurements neural anatomy extensive physiological measurements numerous cortical visual areas measurements using variety techniques single cell measurements fmri experiments indicate macaque visual abilities close humans ventral cortical processing stream relevant object recognition multi-unit recordings high-level visual areas exhibit responses increasingly robust object identity preserving variations considerations mind describe neural representation benchmark used judge representational efﬁcacy representational learning algorithms. importantly present measurement visual areas macaque cortex benchmark. measurements allow researchers test algorithms known high-performing representation. also provide evaluation thus facilitate long sought goal artiﬁcial intelligence achieve representations effective found brain. preliminary evaluation machine representations indicates coming close goal. paper organized follows. methods section describe images task neural measurements kernel analysis suggested protocol measuring algorithms. results section provide kernel analysis measurement number control models recently published high-performing neural network models. conclude discussion additional aspects neural system need investigated ultimately conclude representational learning algorithms effective brain. proposed benchmark utilizes image dataset composed seven object classes broken three levels variation present increasing levels difﬁculty. measure representational efﬁcacy feature space using kernel analysis measures classiﬁcation loss eigendecomposition representation’s kernel matrix representational task chosen class-level object recognition effect image variations object exemplar geometric transformations background. task deﬁned image generation process. image constructed ﬁrst choosing seven categories seven object exemplars category randomly chosen background image ﬁnally variation parameters drawn three distributions. three different variation parameter distributions systematically increase degree however discussion methodologies account dissimilarity matrices class-distance matrices. methodology produce single summary number accuracy-complexity curves achieve kernel analysis. variation sampled variation presents objects ﬁxed position scale pose medium variation high variation presents objects positions spanning image multi-octave scale dilation wide range poses. example images variation level shown figure figure example testing images variation level. variation level medium high variation show example images class example images animal class. images shown contain object instance thus showing image variability variation parameters backgrounds. animal images contain either object instance elephant object instance thus showing variability exemplar variation parameters background. resulting image several advantages disadvantages. advantageously procedure eliminates dependencies objects backgrounds found real-world images introduces controlled amount variability difﬁculty task used produce image datasets known difﬁcult current algorithms resulting images artiﬁcial quality them control allows scientiﬁcally investigate neural coding relation parameters. disadvantages using image expose contextual effects present real world used neural machine systems include relevant variations e.g. lighting texture natural deformations occlusion. view disadvantages opportunities future datasets neural measurements. measuring efﬁcacy representation seek measure favor representations allow simple task solution learned. measure turn work presented based theory presented provide brief description measure refer reader references additional details justiﬁcation. measurement procedure refer kernel analysis utilizes kernel principal component analysis determine much task question solved leading kernel principal components. kernel principal components analysis decompose variation representational space stimuli question. good representation high variability relation task question. therefore leading kernel principal components effective modeling task representational space effective task. contrast ineffective representational space little variation relevant task question variation relevant task contained eigenvectors corresponding smallest eigenvalues kernel principal component analysis. intuitively good representation learns simple boundary small number randomly-chosen examples poor representation makes complicated boundary requiring many examples following kernel analysis consists estimating ﬁrst components kernel feature space ﬁtting linear model low-rank representation minimize loss function task. subspaces formed ﬁrst components controls complexity model accuracy measured loss subspace refer dimensionality subspace complexity accuracy. thus curve provides measurement accuracy function model complexity given representational space. curves produced different representational spaces inform simplicity task representational space higher curves indicating problem simpler representation. advantages kernel analysis kernel method converges favorably limited number samples. braun show kernel projections obtained ﬁnite typically small number samples close multiplicative errors would obtained asymptotic case result especially important setting number images reasonably obtain neural measurements comparatively low. therefore kernel analysis provides methodology assessing representational effectiveness favorable properties image sample regime thousands images. next present speciﬁc computational procedure computing kernel analysis. given learning problem data points drawn independently evaluate representation deﬁned mapping case inputs images category labels denotes feature extraction process. diag d-dimensional approximation eigendecomposition. note dropped moment dependency solve learning problem using linear model corresponding subspace. problem least squares solution multi-way regression problem denoted remove dependence kernel value minimizes loss dimensionality argminσ finally convenience plot accuracy normalized complexity total dimensionality. note chosen squared error loss function multi-way classiﬁcation problem. might appropriate evaluate multi-way logistic loss function chosen least-squares loss computational simplicity provides stronger requirement representational space reduce variance within class increase variance classes allows distinguish representations identical terms separability certain dimensionality still differences feature mappings. kernel analysis deep boltzmann machines also uses mean squared loss function classiﬁcation problem setting. discussion above represents vector task labels images speciﬁc case category identity values assumed discrete binary values equations above. generalize case multiway categorization version common one-versus-all strategy. assuming distinct categories form label matrix evaluate neural representations machine representations measure kernel analysis curves area curves variation. testing image dataset consists seven object classes seven instances object class broken three levels variation images variation medium variation high variation. classes animals cars chairs faces fruits planes tables. measure statistical variation subsampling image variation parameters evaluate pre-deﬁned subsets images taking data variation level. within subset equalize number images class. representation maximize values gaussian kernel parameter chosen quantiles distance distribution. variation level representation procedure produces kernel analysis curve subsets compute mean standard deviation values. allow researchers utilize dataset provide following tools downloadable data testing images images containing seven object classes seven instances object class broken three levels variation images variation medium variation high variation. classes animals cars chairs faces fruits planes tables. computing features images sufﬁcient evaluate algorithm. image grayscale pixels. prevent over-ﬁtting candidate algorithms trained dataset parameter estimation involved model selection estimated independently testing images. training images images consisting object classes object instances object class images produced similar rendering procedure testing image set. training contains speciﬁc constituent objects background images common testing objects original seven categories addition categories. image therefore used independent model selection learning using either supervised unsupervised methods appendix training however optional. testing kernel analysis curves ka-auc values tools evaluate kernel analysis features produced model tested. tools datasets found http//dicarlolab.mit.edu/neuralbenchmark. collected multi-unit sites cortex multi-unit sites form neural feature vectors normalized responses background ﬁring rate variance within presentation images within variation. appendix details. evaluate number machine representations literature including several recent best breed representational learning algorithms visual representation models well feedforward three layer hierarchical model optimized training set. v-like evaluate v-like representation pinto al.’s model attempts capture ﬁrst-order account primary visual cortex computes collection locally-normalized thresholded gabor wavelet functions spanning orientation frequency. model simple baseline biologically-plausible representation sophisticated representations compared. high-throughput model class evaluate three layer hierarchical convolutional neural model class described model class. model class three layer model layer sequentially performs local ﬁltering thresholding saturation pooling normalization. choose high performing model class performed high-throughput search parameter space using kernel-analysis performance provided training image optimization criterion. performing model training evaluated testing appendix details. coates nips evaluate unsupervised feature learning model learns features millions unlabeled images collected internet. evaluate second layer complex cells dimensional feature space rescaling input images pixels computing model’s output grid nonoverlapping pixel windows. resulting output dimensional. icml evaluate model hierarchical locally connected sparse auto encoder pooling local contrast normalization trained unsupervised dataset million images downloaded internet ﬁne-tuned imagenet images labels. penultimate layer outputs network feature representation images resized model’s input dimensions pixels. krizhevsky nips evaluate deep convolutional neural network model ‘supervision’ described trained supervised learning imagenet fall release additional training lsvrc- dataset authors computed features penultimate layer model testing images cropping center pixels mimics procedure described feature logistic regression predict class labels. figure present kernel analysis curves obtained measured neural populations variation level. ka-auc values medium high variation respectively. variation level bootstrap analysis indicates ka-auc measurements signiﬁcantly different variation large difference might expected variation level test variability scale position pose variations neural responses tolerant higher variation sets medium high variation show increased separation reduced performance representations indicating increased difﬁculty task representations. however representation maintains high accuracy complexity even high variation condifigure kernel analysis curves panel shows kernel analysis curves variation level. accuracy minus loss plotted complexity normalized dimensionality eigendecomposition shaded regions indicate maximum minimum accuracy obtained testing subsets often smaller line thickness. tion. representation medium high variation shows sharp increase accuracy complexity indicating representation able accurately capture class-level object recognition task simple decision boundary. note kernel analysis measuretable kernel analysis results. representation measure ka-auc variation level testing subset. means testing subsets given table standard deviations parentheses. performing models highlighted. note measurements cortex current best estimates subject experimental limitations. figure present kernel analysis evaluation machine representations evaluated along neural representations comparison. corresponding ka-auc numbers presented table v-like model shows high accuracy complexity variation performs quite poorly medium variation high variation indicating tasks interesting tests object recognition problem. ht-l model highest performing representation variation achieves performance approaches medium variation high variation. model presented performs similarity v-like model variation levels. performance large variety images model trained relatively shallow architecture and/or mismatch testing image size pixel patches base model. model presented performs comparably variation surpasses medium variations. model performs comparably variation nearly matches performance medium variation surpasses representation high variation. interestingly model matches performance medium variation across entire complexity range exceeds across complexity range high variation. view result highly signiﬁcant ﬁrst model measured matches current estimate representation performance medium variation surpasses high variation. number issues related measurement macaque visual cortex including viewing time behavioral paradigm neural subsampling mapping neural recording neural feature necessary address determining ultimate representational measurement macaque visual cortex. presentation time images shown animals intentionally brief close typical ﬁxation time therefore interesting measure neural representational space changes increased viewing time especially considering natural viewing conditions typically allow longer ﬁxation times multiple ﬁxations. another aspect consider animals engaged passive viewing experimental procedure. actively performing task inﬂuence neural representation? question related commonly referred attentional phenomena current experimental techniques allow measure small portion neurons cortical area. analysis appendix suggests reaching saturation estimate ka-auc current neural sample sample biased spatially cortical sheet electrode grids. bias likely leads underestimate ka-auc. finally neural code topic heated debate neuroscience community mapping multi-unit recordings neural feature vector used analysis possible mapping. importantly mapping shown account human figure kernel analysis curves brain machine. brain representations others machine representations. panel shows kernel analysis curves variation level. accuracy minus loss plotted complexity normalized dimensionality eigendecomposition shaded regions indicate maximum minimum accuracy obtained testing subsets often smaller line thickness. behavioral performance however gain knowledge cortical processing best guess neural code evolve update neural representation benchmark accordingly. another aspect measurement address direct impact visual experience representations observed cortex. interestingly macaques involved studies little real-world experience number object categories used evaluation though beneﬁt millions years evolution years postnatal experience. however learning effects adult cortex well observed even passive viewing remaining unanswered questions exposure experimental protocol affected neural representation could neural representation enhanced increased exposure? related question training provided sufﬁcient achieve level performance testing set? positive example transfer expect algorithms leveraging massive amounts visual data produce best results testing set. algorithms data dependence informative. furthermore extent need build additional structure representations representational learning algorithms achieve representations equivalent found brain? could human neural representation measured better observe macaque cortex? volume cortical tissue related representational efﬁcacy likely human ventral stream would achieve even better performance. determining human homologues macaque visual cortex active investigation known primary visual cortex humans twice large macaque suggestive human visual representation even better metric scaling human visual cortex macaque optimizing representational aspects measuring here. summary suspect estimates representational performance macaque presented provide lower bound performance human visual system. address human visual representation fmri inference human representational space behavioral measurements. others neuroscience ﬁeld actively pursuing directions. analysis believe ﬁeld made signiﬁcant advances recent algorithms. intermediate level variation task advances quite evident recent representational learning algorithm surpasses representation surprisingly supervised algorithm matches representation advances also evident high level variation task algorithm narrowly better algorithm beats ample margin. informative measure elements models lead performance interesting purely unsupervised algorithms achieve similar performance. methodology proposed extended sensory domains representation critical neural representations thought effective. example possible deﬁne similar task protocols auditory stimuli measure neural responses auditory cortex. measurements would implications discovering effective auditory representations also provide data necessary validate representational learning algorithms effective multiple contexts. representational learning algorithms prove effective across domains serve hypotheses canonical cortical algorithm ‘holy grail’ artiﬁcial intelligence research. work supported u.s. national institute national science foundation defense advanced research projects agency c.f.c supported u.s. national institute thank adam coates quoc alex krizhevsky help evaluating models comments paper. braun. accurate error bounds eigenvalues kernel matrix. jmlr braun buhmann m¨uller. relevant dimensions kernel feature spaces. jmlr canolty ganguly kennerley cadieu koepsell wallis carmena. oscillatory dicarlo zoccolan rust. brain solve visual object recognition? neuron fabre-thorpe richard thorpe. rapid categorization natural images rhesus monkeys. montavon braun m¨uller. kernel analysis deep networks. jmlr ruff bodurka weerd bandettini kriegeskorte. categorical graded singleimage activation proﬁles human category-selective cortical regions. journal neuroscience oliva torralba. role context object recognition. trends cognitive sciences orban. higher order visual processing macaque extrastriate cortex. physiological reviews pinto cox. beyond simple features large-scale feature search approach unconstrained face riesenhuber poggio. hierarchical models object recognition cortex. nature neuroscience rosenblatt. perceptron probabilistic model information storage organization brain. using model class performed high-throughput search parameter space evaluating approximately parameter selections random training image sets. model parameter instantiation category-balanced subset randomly chosen images -image training set. instantiation evaluated kernel analysis protocol similar used testing object class labels training opposed present testing set. model instantiation also extracted features testing images standard kernel analysis protocol. evaluate transfer training testing examined well training scores predict testing scores comparing relative performance rankings training transfer testing set. figure shows results. performance training strongly correlated performance medium high variation components testing weakly correlated variation condition. might expected training contains variation similar high variation testing set. single best model training achieves high score testing relative models training range machine-learning representations. data indicates models trained using provided training perform favorably testing set. collected neural data across adult male rhesus monkeys using multi-electrode array recording system chronically implanted three arrays animal recorded best visually driven neural measurement sites animal another stimulus presentation recorded multi-unit neural responses images sites. stimuli presented screen time. image presented radius center screen half-gray background followed half-gray blank period. animal’s movement monitored video tracking system animal rewarded upon successful completion image presentations maintaining good ﬁxation center screen indicated small dot. presentations large movements discarded. experimental block recorded responses images certain variation level. within block image repeated three times variation medium high variation. resulted collection image repetitions medium high variation respectively. surgical experimental procedures accordance national institute health guidelines massachusetts institute technology committee animal care. convert neural responses neural representation following normalization process. image block compute vector ﬁring rates across measurement sites counting number spikes onset image site. subtracted background ﬁring rate ﬁring rate presentation half-gray background blank image evoked response. order minimize effect variable external noise normalize standard deviation site’s response block images medium high variation. variation stimuli divide three repetitions within block three separate sets containing complete images normalize standard deviation site’s response within set. finally neural representation calculated taking mean across repetitions image site producing scalar valued matrix neural sites images. post-processing procedure current best-guess neural code shown account human performance therefore possible develop effective neural decoding example inﬂuenced intrinsic cortical variability dynamics current experimental techniques allow measure small portion neurons cortical area. seek estimate kernel analysis metric would affected larger neural sample. figure estimate effect subsampling neural population measurement showing kaauc function number neural measurement sites. estimate asymptotic convergence neural representation variation level curve form be−ctd number neural sites parameters. provides estimate ka-auc entire neural population. estimated asymptotic values ka-auc’s variation medium variation high variation respectively. interestingly number neural sites measured already approaching asymptotic value. therefore given task speciﬁcation preprocessing procedure convergence estimate believe reaching saturation estimate ka-auc neural population found functional form well similar analysis performed computational representation subsampled number features included analysis. allowed estimate behavior ka-auc much larger feature spaces neural measurements. figure high-throughput model relationship training testing performance. panel shows scatter plot measured training ka-auc testing ka-auc variation level. lines indicate best linear dots best worst performing models training best performing model testing note value training ka-auc model. linear relationships observe indicate provided training informative testing set. figure effect sampling neural areas. estimate effect sampling neural sites testing ka-auc. panel shows effect variation level. best curves shown solid lines measured samples indicated ﬁlled circles. estimated asymptotes indicated dashed horizontal lines. text details.", "year": 2013}