{"title": "Online Learning under Delayed Feedback", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.", "text": "online learning delayed feedback received increasing attention recently several applications distributed web-based learning problems. paper provide systematic study topic analyze eﬀect delay regret online learning algorithms. somewhat surprisingly turns delay increases regret multiplicative adversarial problems additive stochastic problems. give meta-algorithms transform black-box fashion algorithms developed non-delayed case ones handle presence delays feedback loop. modiﬁcations well-known algorithm also developed bandit problem delayed feedback advantage meta-algorithms implemented lower complexity. paper study sequential learning feedback predictions made forecaster delayed. case example advertisement information whether user clicked certain come back engine delayed fashion selected waiting information user clicks engine provide users. also click information aggregated periodically sent module decides resulting delays. another example parallel distributed learning propagating information among nodes causes delays online learning proved successful many machine learning problems applied practice situations feedback delayed theoretical results non-delayed setup applicable delays present. previous work concerning delayed setting focussed speciﬁc online learning settings delay models thus comprehensive understanding eﬀects delays missing. paper provide systematic study online learning problems delayed feedback. consider partial monitoring setting covers settings previously considered literature extending unifying often improving upon existing results. particular give general meta-algorithms transform blackbox fashion algorithms developed non-delayed case algorithms handle delays eﬃciently. analyze delay eﬀects regret algorithms. interesting perhaps somewhat surprising result delay inﬂates regret multiplicative adversarial problems eﬀect additive stochastic problems. general meta-algorithms useful timespacecomplexity unnecessarily large. resolve problem work modiﬁcations variants algorithm stochastic bandit problems delayed feedback much smaller complexity black-box algorithms. rest paper organized follows. problem online learning delayed feedback deﬁned section adversarial stochastic problems analyzed sections modiﬁcation algorithm given section proofs well results kl-ucb algorithm delayed feedback provided appendix. formation. model forecaster make sequence predictions possibly based side information prediction receives reward feedback feedback delayed. formally given possible side information values possible predictions reward functions possible feedback values time instant forecaster receives side information then predicts value environment simultaneously chooses reward function ﬁnally forecaster receives reward time-stamped feedback n×h. particular element pair time index feedback value time index indicating time instant whose decision associated feedback corresponds note forecaster receive direct information rewards receives standard online learning feedback-set singleton feedback depends delayed model however feedback concerns decision time received time period t+τt prediction made i.e. delayed time steps. note corresponds non-delayed case. shown figure delayed case. note bandit full information problems also treated special partial monitoring problems. therefore last formulation problem. stochastic assumption made sequence generated talk adversarial model. stochastic setting consider case sequence independent identically distributed random variables. side information present real problem; finally diﬀerent assumptions delays. often assume i.i.d. sequence independent past predictions forecaster. stochastic setting also allow distribution depend table summary work online learning delayed feedback. shows regret delayed setting shows regret non-delayed setting. denotes matching lower bound. indicate maximum average respectively number consecutive time steps agent feedback term τconst indicates results constant delays only. work positive constants τmax denotes maximum delay. results presented paper shown boldface maximum number outstanding feedbacks ﬁrst time-steps. particular knowledge weinberger ordentlich ﬁrst analyze delayed feedback problem; considinformation setting ered adversarial ﬁxed known delay τconst. showed minimax optimal solution τconst independent optimal predictors subsampled reward sequences τconst prediction strategies used predictor used time instants approach forms basis method devised adversarial case langford showed usual conditions suﬃciently slowed-down version mirror descent algorithm achieves optimal decay rate average regret. mesterharm considered another variant full information setting using adversarial model delays label prediction setting forecaster predict label corresponding side information vector full information online prediction problem weinberger ordentlich showed regret increases multiplicative factor τconst work mesterharm important quantity becomes maximum/average deﬁned length largest time interval forecaster receive feedback. mesterharm also shows minimax regret adversarial case increases multiplicatively average increases additive fashion stochastic case maximum gap. agarwal duchi considered problem online stochastic optimization showed that random delays regret increases i.i.d. section provide black-box algorithms delayed feedback problem. assume exists base algorithm base solving prediction problem without delay. often specify assumptions underlying regret bounds algorithms assume problem consider diﬀers original problem delays. example adversarial setting base build assumption reward functions selected oblivious non-oblivious first consider adversarial case section section provide tighter bounds stochastic case. supb...bn∈b respectively supb...bn∈b algorithm weinberger ordentlich adversarial full information setting subsamples reward sequence constant delay τconst+ runs base algorithm base τconst subsampled sequences. weinberger ordentlich showed base enjoys regret bound algorithm ﬁxed delay case enjoys regret bound furthermore base minimax optimal non-delayed setting subsampling algorithm also minimax optimal delayed setting seen constructing reward sequence changes every τconst times. note weinberger ordentlich require condition however conditions imply concave function ﬁxed satisﬁed regret bounds aware algorithm weinberger ordentlich case delays constant partial monitoring setting. idea several instances non-delayed algorithm base needed instance free received feedback corresponding previous prediction instance busy waiting feedback. need make prediction existing instances free hence ready make another prediction. instance exists create used resulting algorithm call black-box online learning delayed feedback shown clearly performance bold depends many instances base need create many times instance used. denote number base instances created bold including time create instance beginning time instant instances waiting feedback. ﬁrst inequality follows since deterministic function delays last inequality follows jensen’s inequality concavity fbase. substituting taking expectation concludes proof. weinberger ordentlich thus generalizing result partial monitoring. know whether bound tight even base minimax optimal argument weinberger ordentlich lower bound work partial information setting theorem suppose non-delayed algorithm base used bold enjoys regret bound fbase. assume furthermore delays independent forecaster’s prediction expected regret bold time steps satisﬁes note function delay sequence function predictions hence reward sequence instance evaluated chosen obliviously whenever adversary bold oblivious. independence assumption outcomes sequences potential rewards feedh i.i.d. respectively backs setting also asprediction sume feedback reward sequences different predictions independent other. denote expected reward predicting maxi∈a optimal reward optimal prediction. moreover i{at denote number times gaps expected similarly adversarial setting build base algorithm base non-delayed case. advantage setting base consider permuted order rewards feedbacks wait actual feedback; enough receive feedback prediction. idea core algorithm queued partial monitoring delayed feedback base partial monitoring algorithm non-delayed case inside algorithm. feedback information coming environment stored separate queues prediction value. outer algorithm constantly queries base feedbacks predictions made available queues inner algorithm base runs main assumption section outcomes form i.i.d. sequence also independent predictions standard i.i.d. partial monitoring setting conventional multi-armed bandit setting recovered feedback reward last prediction previous section assume feedback delays independent outcomes environment. main result section shows assumptions penalty regret grows additive fashion delays opposed multiplicative penalty seen adversarial case. real prediction problem). feedback available outer algorithm keeps sending prediction real environment feedback prediction arrives. base simulated non-delayed environment. next lemma implies inner algorithm base actually runs non-delayed version problem experiences distributions lemma consider delayed stochastic problem deﬁned above. prediction denote feedback qpm-d receives predicting sequence i.i.d. sequence distribution sequence feedbacks prediction relate non-delayed performance base regret qpm-d need deﬁnitions. denote number feedbacks prediction received time instant number missing feedbacks making prediction time instant max≤t≤n git. number thermore times algorithm base predicted queried times. denote number steps inner algorithm base makes steps real problem. next relate well number times qpm-d base make speciﬁc prediction. shown lemma reordered rewards feedbacks i.i.d. distribution original feedback sequence t∈n. base algorithm base worked ﬁrst feedbacks therefore operated steps simulated environment reward feedback distributions without delay. hence ﬁrst sumalgorithms previous section provide easy convert algorithms devised nondelayed case ones handle delays feedback improvements achieved makes modiﬁcations inside existing non-delayed algorithms retaining theoretical guarantees. viewed white-box approach extending online learning algorithms delayed setting enables escape high memory requirements black-box algorithms arises methods previous section delays large. consider stochastic multi-armed bandit problem extend family algorithms delayed setting. modiﬁcation proposed quite natural common characteristics ucb-type algorithms enable uniﬁed extending performance guarantees delayed setting recall stochastic setting special case stochastic problem section feedback time instant distribution rewards prediction drawn i.i.d. manner. assume rewards diﬀerent predictions independent other. notation section several algorithms devised non-delayed stochastic problem based upper conﬁdence bounds optimistic estimates expected reward diﬀerent predictions. different ucb-type algorithms diﬀerent upper conﬁdence bounds choose time instant prediction largest ucb. bist denote prediction time instant number reward samples used computing estimate. non-delayed setting prediction ucb-type algorithm time instant given argmaxi∈a bitit. presence delays simply upper conﬁdence bounds rewards observed predict algorithms deﬁned easily shown enjoy regret guarantees compared non-delayed versions additive penalty depending delays. analyses regrets algorithms follow pattern upper bounding number trials suboptimal prediction using concentration inequalities suitable speciﬁc form ucbs use. note last term bound additive penalty diﬀerent assumptions bounded theorem proof theorem well similar regret bound delayed version kl-ucb algorithm found appendix analyzed eﬀect feedback delays online learning problems. examined partial monitoring case provided general algorithms transform forecasters devised non-delayed case ones handle delayed feedback. turns price delay multiplicative increase regret adversarial problems additive increase stochastic problems. believe ﬁndings qualitatively correct lower bounds prove also turns important quantity determines performance algorithms maximum number missing rewards. interesting note maximum number servers used multi-server queuing system inﬁnitely many servers deterministic arrival times. also maximum deviation certain type markov chain. found immediately applicable results ﬁelds think applying techniques areas could lead improved understanding hence improved analysis online learning delayed feedback. mesterharm chris on-line learning delayed label feedback. jain sanjay simon hansulrich tomita etsuji algorithmic learning theory volume lecture notes computer science springer berlin heidelberg gergely gy¨orgy andr´as szepesv´ari csaba antos andr´as. online markov decision processes bandit feedback. laﬀerty williams shawe-taylor zemel r.s. culotta advances neural information processing systems", "year": 2013}