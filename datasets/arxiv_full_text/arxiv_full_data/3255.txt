{"title": "Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction of nonnegative parts-based and physically meaningful latent components from high-dimensional tensor data while preserving the natural multilinear structure of data. However, as the data tensor often has multiple modes and is large-scale, existing NTD algorithms suffer from a very high computational complexity in terms of both storage and computation time, which has been one major obstacle for practical applications of NTD. To overcome these disadvantages, we show how low (multilinear) rank approximation (LRA) of tensors is able to significantly simplify the computation of the gradients of the cost function, upon which a family of efficient first-order NTD algorithms are developed. Besides dramatically reducing the storage complexity and running time, the new algorithms are quite flexible and robust to noise because any well-established LRA approaches can be applied. We also show how nonnegativity incorporating sparsity substantially improves the uniqueness property and partially alleviates the curse of dimensionality of the Tucker decompositions. Simulation results on synthetic and real-world data justify the validity and high efficiency of the proposed NTD algorithms.", "text": "compact representation high-dimensional data. important topic extensively studied last several decades particularly witnessed great success blind source separation techniques methods observation data modeled linear combination latent components possess speciﬁc diversities statistical independence temporal structure sparsity smoothness. properly exploiting diversities large family matrix-factorization-based methodologies proposed successfully applied variety areas. many applications data naturally represented tensors e.g. color images video clips fmri data. methodologies matricize data apply matrix factorization approaches give ﬂattened view data often cause loss internal structure information; hence favorable process data domain i.e. tensor domain obtain multiple perspective stereoscopic view data rather ﬂattened one. reason tensor decomposition methods proposed widely applied deal high-order tensors. widely used methods tucker decomposition applied pattern recognition clustering analysis image denoising etc. achieved great success. often observation data latent components naturally nonnegative e.g. text images spectra probability matrices adjacency matrices graphs data based impressions clicks ﬁnancial time series. data extracted components lose physical meaning nonnegativity preserved. regard nonnegative matrix factorization demonstrated powerful tool analyze nonnegative matrix data able give physically meaningful interpretable results. particularly ability learning local parts objects result received extensive study last decade found many important applications including clustering analysis sparse coding dependent source separation etc. nonnegative tensor data analysis nonnegative tucker decomposition also gained importance recent years inherits advantages also provides additional multiway structured representation data. fig. illustrates able give parts-based representation face images included abstract—nonnegative tucker decomposition powerful tool extraction nonnegative parts-based physically meaningful latent components high-dimensional tensor data preserving natural multilinear structure data. however data tensor often multiple modes large-scale existing algorithms suffer high computational complexity terms storage computation time major obstacle practical applications ntd. overcome disadvantages show rank approximation tensors able signiﬁcantly simplify computation gradients cost function upon family efﬁcient ﬁrst-order algorithms developed. besides dramatically reducing storage complexity running time algorithms quite ﬂexible robust noise well-established approaches applied. also show nonnegativity incorporating sparsity substantially improves uniqueness property partially alleviates curse dimensionality tucker decompositions. simulation results synthetic real-world data justify validity high efﬁciency proposed algorithms. behind observation data fundamental task data analysis widely studied ﬁelds signal image processing machine learning. although observation data large much lower number latent variables components capture signiﬁcant features original data. revealing components achieve objectives dimensionality reduction feature extraction obtain highly relevant manuscript received ...this work partially supported national natural science foundation china guangdong province natural science foundation guangdong province excellent thesis foundation jsps kakenhi guoxu zhou school automation guangdong university technology guangzhou china laboratory advanced brain signal processing riken brain science institute wako-shi saitama japan. e-mail zhouguoxuieee.org. laboratory advanced brain signal processing riken brain science institute wako-shi saitama japan systems research institute polish academy science warsaw poland. e-mail ciabrain.riken.jp. fig. illustration able give local parts-based representation tensor objects face database. face represented linear combination sparse basis faces. contrast basis images extracted basis faces possess multilinear structure rules without fully exploiting special multilinear structure tucker model turn suffers high computational complexity terms space time especially data tensor large-scale. therefore quite crucial develop efﬁcient algorithms able yield satisfactory results within tolerable time. taking account unconstrained tucker decompositions signiﬁcantly faster propose framework efﬁcient based unconstrained tucker decomposition data tensor paper. such frequent access original tensor avoided thereby leading considerably reduced computational complexity ntd. although basic idea based proceeding rank approximation brieﬂy introduced recent overview paper detailed derivations presented paper results uniqueness ntd. rest paper organized follows. section basic notation models introduced. section ﬁrst-order algorithms reviewed. section ﬂexible efﬁcient algorithms based low-rank approximation data introduced unique sparse discussed section simulations synthetic real-world data presented section demonstrate high efﬁciency proposed algorithms conclusions presented section vii. matrix matrix respectively. vector/matrix zeros. index positive integers larger ascending order i.e. nth-order nonnegative tensors size r-by-r· by-rn nonnegative matrix i.e. operator yielding nonnegative matrix tensor mode-n matricization tensor element-wise product division matrices tensors. moreover deﬁne kronecker product ri×j ri×j yields rii×jj entries ci+ij+j ri×j ri×j database. ﬁgure sample face image represented linear combination sparse basis images possess multilinear structure. unconstrained tucker decompositions often criticized lack uniqueness curse dimensionality indicates size core tensor increases exponentially dimension. compared unconstrained tucker decompositions likely unique provides physically meaningful components. moreover core tensor often sparse allows discover signiﬁcant links components partially alleviate curse dimensionality. unfortunately existing algorithms generally performed directly applying update population important broad range applications machine learning signal processing understand idea population consider simultaneously performing th-order sample tensors ri×i···×in− common component matrices. such sample tensor represented column vectorized sample mode-n matricization nth-order tensor obtained concatenating samples such samples represented linear combination sets common basis vectors contains extracted features. studied name population value decomposition without nonnegativity constraints. hence population extension extracting nonnegative common components multiblock higherdimensional data equipped extra ability learning localized parts objects alternative method performing feature extraction tasks referred nonnegative matrix factorization vectorizes sample form sample matrix represented factor matrices rank) rr×r×···×rn core tensor whose entries reﬂect interactions connections components different mode matrices. assume high-dimensional data often well approximated lower-rank representations. core tensor factor matrices required element-wisely nonnegative. nonnegativity factors brings effects resulting representation purely additive without subtractions factors often sparse contain many zero entries. effects equip nonnegative factorization methods ability learning localized parts objects. intuitive difference population basis vectors former outer product lower-dimensional vectors shown much lower number free parameters gives kind multilinear representation. multilinear representation widely exploited overcome over-ﬁtting problem discriminant analysis substantially improves sparsity basis vectors discussed later. mentioned algorithms based gradients major problem quite large. example veriﬁed complexity computing high hence direct implementation methods extremely time space demanding especially large-scale problems. rin×rn controls ×n∈in target complexity terms space time. fact consumes much less storage former consumes whereas consumes low-rank approximation first solve optimization problem generally block coordinate descent framework minimize cost function respect partial parameters time ﬁxing others. optimize consider equivalent form nonnegative least squares problems extensively studied context including multiplicative update algorithm hierarchical alternating least squares method active-set methods algorithm based accelerated proximal gradient algorithms ﬁrst-order information free searching step size. extend methods need compute following respective gradients dntd respect sampling sparsiﬁcation provide favorable scalability suitable large-scale problems. highly scalable tucker decomposition algorithm developed basis distributed computing randomization. methods assumed noise gaussian. otherwise robust tensor decomposition methods recommended data tensor contaminated outliers. practice best depends many factors e.g. whether data sparse dense scale data noise distribution etc. clever choice step size such cost function remains nonincreasing remains nonnegative. term contain negative elements apply method proposed descent direction ∂dntd replaced thereby leading following formula algorithm pseudocode algorithm based rules update parameters inefﬁcient manner update parameter main iteration. case however multiple updates used achieve sufﬁcient decrease cost function improve total efﬁciency motivated work nmf. course order achieve high efﬁciency exact convergence subproblem generally unnecessary iterations.) low-rank approximation unconstrained tucker decomposition data tensor plays role proposed two-step framework. high-order singular value decomposition method often serves workhorse purpose. although provides good tradeaccuracy efﬁciency involves eigenvalue decomposition large matrices hence suitable large-scale problems memory efﬁcient variant proposed iterative method provides improved scalability. decomposition mach method respectively based active method active method proposed applied solve roughly speaking methods involve solving inverse problems ∂vec nonnegativity constraints among them block principal pivoting achieved best performance multiple columns updated simultaneously active-setbased approaches converge fast stability alternating least squares semi-ntd sometimes component matrices and/or core tensor necessarily nonnegative natural extension semi-nmf factors updated least-squares solutions linear equation systems ∂dntd ∂dntd similar als-based method als-ntd method generally guarantee convergence. however many experimental results show method works quite well factors sparse. practice entries data tensor could severely contaminated noise hence could used simply missing. case intrinsic low-rank structure data often allows recovery missing multilinear rank nonnegative multilinear rank vector called multilinear rank)∀n. vector rank called nonnegative multilinear rank nonnegative tensor essential uniqueness call ×n∈in essentially unique ˆapd holds ×n∈in permutation matrix nonnegative diagonal matrix. deﬁnition essential uniqueness obtained.) proof note rrn×r exists simply ˜a=a then ×n∈in forms another rank+) contradicts assumption rank+) corollary ×n∈in essentially unique. corollary condition means trivial matrix product permutation matrix nonnegative scaling matrix. following proof proposition proof corollary obvious. proof suppose exists non-trivial matrix another ×n∈in then ×n∈in different ntds q−g. contradicts assumption essentially unique. proposition population g×n=n essentially unique positive population entries weight tensor entries either problem missing values. although many methods proposed tensor/matrix decompositions missing values straightforwardly extended ad-hoc two-step solution applied step weighted tucker decomposition performed minimizing cost function step performed using completed tensor yielded step notice weighted tucker decomposition approaches also allow obtain low-rank approximations accessing randomly sampled entries high-dimensional tensor useful technique deal large-scale problems although approaches proposed missing-values problem based random sampling attempt optimal approximation original data using partial data subtle difference ﬁrst category data samples used ﬁxed whereas second category data samples used shall carefully selected order achieve satisfactory accuracy high probability. using two-step framework scaled large-scale problems error governed quality step stated proposition tucker decompositions often criticized suffering major disadvantages curse dimensionality lack uniqueness. former means size core tensor increases exponentially respect order whereas latter fact unconstrained tucker decompositions essentially estimate subspace mode. section discuss nonnegativity help overcome limitations tucker decompositions particularly incorporating sparsity. best knowledge although several algorithms developed theoretical analysis uniqueness still missing. fig. evolution values versus iteration number algorithm runs. algorithms started initial settings. data tensor generated using ×n∈i entries drawn i.i.d. exponential distributions entries noise drawn standard gaussian distribution corollary describes special case high-order tensor achieved solving independent subproblems avoids nonnegative alternating least squares respect factors realized furthermore proof proposition know factors essentially uniquely recovered kronecker products. motivates extend idea mode reduction proposed ntd; nth-order tensor implemented performing rd-order tensor obtained reshaping original nth-order tensor followed kronecker product approximation procedure. rd-order tensor essentially unique original nth-order tensor also does. many results uniqueness comprehensive review) based sparsity factor matrices. among them puresource-dominant condition means signal exists least instant signal active strongly dominant popular uniqueness conditions proposition y=ab essentially unique satisﬁes pure-source-dominant condition i.e. diagonal scaling matrix permutation matrices identity matrix. pure-source-dominant condition also studied terms separable gained popularity recently proved highly scalable representative applications include topic discovery clustering analysis largescale datasets replace unique proposition corollary pure-sourcedominant condition obtain corresponding uniqueness conditions ntd. note pure-source-dominant condition essentially requires least factor matrix sufﬁciently sparse. requires core tensor component matrices sufﬁciently sparse. fact sparsity factor uniqueness also reﬂects learning-parts ability many zeros often exist factors. focus sparse ntd. sparse core tensor particular importance partially break curse dimensionality keeps signiﬁcant connections components different modes also improves uniqueness feature results. fact ideal case sparse all-zero except gii...i essentially reduced nonnegative polyadic decomposition essentially unique mild conditions sparse) facts suggest sparse core tensor quite useful practice. below focus improve sparsity core tensor imposing suitable constraints also applied improve sparsity component matrices similarly. popular approach penalty improve sparsity leading dsntd dntd nonnegative another approach frobenius norm penalty generally leads denser factor matrices sparse core tensor case subproblem respect strictly convex equivalent applying tikhonov regularization. consider partial model notice special kronecker product structure possessed nmf. below show kronecker product structure substantially improve sparsity basis matrix. lemma rn×. then zazb sasb means max. proposition rin×rn then +sa−sasa sa). proof exists rearrangement denoted satisfying vec)vec) equivalently rest proof obvious. proposition generally able provide sparse basis matrices nmf. sparsity stems sparsity factor matrix enhanced kronecker product operators. section performance proposed algorithms demonstrated using synthetic real-world data. simulations performed computer icpu .ghz memory running windows matlab codes proposed algorithms available http//bsp.brain.riken.jp/∼zhougx. fig. illustration performance affected ﬁnal results monte carlo runs randtucker used compress noisy data tensor. roughly speaking accurate leads better results. fig. msir values versus sparsity level factors averaged monte carlo runs additive gaussian noise seen factors sparse lra-ntd algorithms able recover true components high probability. data tensor generated using ×n∈i elements component matrices core tensor drawn independent exponential distributions mean parameter entries additive noise term drawn independent gaussian distributions. simulations using synthetic data nonnegative multilinear rank generate sparse core tensor component matrices entries uniformly sampled matrix/tensor zero meet speciﬁed sparsity. used performance indices measure approximation accuracy. ﬁrst index measures ﬁtting error basis solvers introduced section implemented algorithms mu-ntd halsntd bpp-ntd apg-ntd als-ntd versions without lra. implementation bpp-ntd borrowed code bpp-nmf solve subproblem. convergence speed different update rules. maximum iteration number subproblem total number iterations algorithms. comparison directly algorithm observation data without procedure compare performance suggested update rules evolution values versus iteration number shown fig. using different initial values. seen alsntd algorithm quite sensitive initial values mainly involves computation inverse probably ill-conditioned matrices iterations. although seems serious als-ntd. seems apgntd less sensitive initial values whereas halsntd often provides higher accuracy faster convergence speed. hence comparisons below focused three stable algorithms). except als-ntd algorithm algorithms converged consistently. comparison algorithms without different levels noise. algorithm stopping criterion lra-based algorithms used hosvd obtain noisy observation data. performance averaged monte carlo runs shown fig.. fig. lra-based algorithms often robust without lra. guess mainly algorithms without sensitive initial values noisy data nonnegative projection iterations. words quite helpful reduce noise consequently improve robustness algorithms. moreover expected lra-based algorithms signiﬁcantly faster without shown fig.. also investigated affect ﬁnal results ntd. experiment randtucker algorithm proposed used compress data tensor then lraapg applied perform using exactly initial settings. fig. shows results monte carlo runs demonstrated accurate roughly better performance ntd. note also ﬁnal values often higher suggesting nonnegativity constraints help remove noise improve estimation accuracy. investigation sparsity affects essential uniqueness applying developed algorithms data whose factors different levels sparsity. simulation time. gaussian noise added data tensor data procedure algorithms performed using hosvd average performance monte carlo runs shown fig.. ﬁgure sparsity factors higher msir values generally higher means corresponding ntds cases essentially unique existing algorithms able recover true components high probability. however factors sparsity level algorithms failed recover true components also achieved lower values. guess mainly caused local convergence algorithms. simulation results sparsity factors factor substantially improves essential uniqueness turn leads better data sticking local minima algorithms largely avoided. object clustering. experiment applied proposed algorithms clustering analysis objects selected columbia object image library coil- database consists images objects images taken different poses. simplicity considered ﬁrst categories randomly selected categories time form data tensor ×××k. then tensor decomposed proposed algorithms g×n∈i empirically setting denoting number features. used factor matrix features used k-means approach cluster objects. k-means repeated times mitigate local convergence issue. show superiority methods high-dimensional data analysis also used nenmf method method extract features vectorized samples. table performance comparison algorithms applied clustering analysis using first objects coil- objects averaged monte carlo runs. algorithms achieved higher clustering accuracy nenmf algorithm. randomly selected different subsets objects average performance plotted fig. listed table indicates approaches outperformed nenmf work ﬂattened data lra-based algorithms signiﬁcantly faster others. fig. observe approaches extracted sparse local parts-based basis. moreover core tensors obtained ntds generally sparse even without imposing additional sparsity constraints. example entries captured energy entire core tensor allows adoption efﬁcient sparse representations storage computation. note also include existing algorithms comparison much slower proposed algorithms analyzed section example iterations algorithm proposed consumed approximately achieving algorithms inefﬁcient update parameters iteration without acceleration. human face recognition. experiment applied proposed algorithms extract features human face recognition using yale databases. face images gray-scaled size time randomly selected sample images person training data whereas others used testing. ntd-based approaches decomposed training data using proposed algorithms empirically setting then unfolding matrix used basis matrix. comparison mahnmf nenmf also used learn basis matrix ﬂattened training data number features. basis matrix learnt training data nonnegative projection test sample onto basis matrix used features recognition finally classiﬁer included matlab used recognition using extracted features distance measured table comparison face recognition accuracy achieved algorithm using yale databases averaged five monte carlo runs. samples person used training data whereas others used testing. factorization-based methods especially amount training data relatively small. phenomenon also observed tensor-based discriminate analysis shows tensor-based methods could considerably alleviate overﬁtting problem also seen difference accuracy obtained standard algorithms lra-accelerated versions marginal fig. illustrate number features i.e. affected recognition accuracy using database. basically larger value often higher accuracy cost higher computational load. however performance approaches became unstable. guess unique anymore originally developed order give parts-based representation images perform dimensionality reduction physical domain. fig. shows basis images learnt nenmf mahnmf lrahals-ntd algorithms. database well known algorithms often tend give global representations rather parts-based ones shown fig. contrast lrahals-ntd algorithm extracted localized parts faces without fig. face-recognition accuracy averaged monte carlo runs dataset. samples used training others used test samples. lrahals-ntd algorithm used feature extraction varying number features ﬁxed fig. basis images learned nenmf mahnmf lrahals-ntd database. data whereas approaches often give global-based representations lrahals-ntd able give localized representation. powerful tool analyze multidimensional nonnegative tensor data giving sparse localized parts-based representation high-dimensional objects. paper proposed family ﬁrst-order algorithms based preceding data tensors. proposed algorithms ﬁrst-order information free line search search update steps procedure signiﬁcantly reduces computational complexity subsequent nonnegative factorization procedure terms time space also substantially improves robustness noise ﬂexibility algorithms. indeed incorporating various well-established techniques proposed algorithms could seamlessly implemented analyze data contaminated various types noise seamlessly. error bounds lra-based brieﬂy discussed preliminary results essential uniqueness provided focus relationship uniqueness nmf. discussed sparsity able improve uniqueness partially alleviate curse dimensionality tucker decompositions. simulations justiﬁed efﬁciency proposed lra-based algorithms demonstrated promising applications clustering analysis. maybank general tensor discriminant analysis gabor features gait recognition ieee transactions pattern analysis machine intelligence vol. oct. cichocki zdunek phan s.-i. amari nonnegative matrix tensor factorizations applications exploratory multi-way data analysis blind source separationpplications exploratory multi-way data analysis blind source separation. chichester john wiley sons rajwade rangarajan banerjee image denoising using higher order singular value decomposition ieee transactions pattern analysis machine intelligence vol. wang chan wang nonnegative leastcorrelated component analysis separation dependent sources volume maximization ieee transactions pattern analysis machine intelligence vol. lock nobel marron comment journal american statistical association vol. zhou zhao zhang adali cichocki linked component analysis matrices high order tensors applications biomedical data proceedings ieee accepted. zhang zhou zhao wang cichocki spatialtemporal discriminant analysis erp-based brain-computer interface ieee transactions neural systems rehabilitation engineering vol. l.-h. comon nonnegative approximations nonnegative tensors journal chemometrics vol. seung algorithms non-negative matrix factorization proc. advances neural information processing systems leen dietterich tresp eds. press cambridge park nonnegative matrix factorization based alternating nonnegativity constrained least squares active method siam journal matrix analysis applications vol. phan cichocki extended hals algorithm nonnegative tucker decomposition applications multiway analysis classiﬁcationer decomposition applications multi-way analysis classiﬁcation neurocomputing vol. sidiropoulos papalexakis faloutsos parallel algorithm tensor decomposition using randomly compressed cubes proc. ieee international conference acoustics speech signal processing huang sidiropoulos swami non-negative matrix factorization revisited uniqueness algorithm symmetric decomposition ieee transactions signal processing vol. zhou yang j.-m. yang minimum-volumeconstrained nonnegative matrix factorization enhanced ability learning parts ieee transactions neural networks vol. oct.", "year": 2014}