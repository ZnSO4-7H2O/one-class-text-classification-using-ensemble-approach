{"title": "Predictive State Temporal Difference Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.", "text": "propose approach value function approximation combines linear temporal difference reinforcement learning subspace identiﬁcation. practical applications reinforcement learning complicated fact state either high-dimensional partially observable. therefore methods designed work features state rather state itself success failure learning often determined suitability selected features. comparison subspace identiﬁcation methods designed select feature preserves much information possible state. paper connect approaches looking problem reinforcement learning large features marginally useful value function approximation. introduce algorithm situation called predictive state temporal difference learning. ssid predictive state representations pstd ﬁnds linear compression operator projects large features small preserves maximum amount predictive information. pstd uses bellman recursion estimate value function. discuss connection pstd prior approaches ssid. prove pstd statistically consistent perform several experiments illustrate properties demonstrate potential difﬁcult optimal stopping problem. examine problem estimating policy’s value function within decision process high dimensional partially-observable environment parameters process situation common strategy employ linear architecture represent unknown. value function linear combination features observations. popular family model-free algorithms called temporal difference algorithms used estimate parameters value function. least-squares algorithms exploit linearity value function optimal parameters least-squares sense time-adjacent samples features. unfortunately choosing good features hard. features must predictive future reward features must small relative amount training data learning prone overﬁtting. problem selecting small reasonable features approached number different perspectives. many domains features selected hand according expert knowledge; however task difﬁcult time consuming practice. therefore considerable amount research devoted problem automatically identifying features support value function approximation. much research devoted ﬁnding sets features dynamical system known state space large difﬁcult work with. example large fully observable markov decision process often easier estimate value function dimensional features using state directly. several approaches attempt automatically discover small features given larger description e.g. using spectral analysis state-space transition graph discover low-dimensional feature preserves graph structure partially observable markov decision processes extend mdps situations state directly observable circumstance agent plan using continuous belief state dimensionality equal number hidden states pomdp. number hidden states large dimensionality reduction pomdps achieved projecting high dimensional belief space lower dimensional one; course difﬁculty projection preserves decision quality. strategies ﬁnding good projections include value-directed compression non-negative matrix factorization resulting model compression predictive state representation observable operator model multiplicity automaton moving representations often compress pomdp large factor little loss accuracy examples exist arbitrarily large lossless compression factors practice often achieve large compression ratios little loss. drawback approaches enumerated ﬁrst assume dynamical system model known give ﬁnding compact representation value function. practice would like able good features without prior knowledge system model. kolter contend problem sparse feature selection standpoint. given large possibly-relevant features observations proposed augmenting lstd applying penalty coefﬁcients forcing lstd select sparse features value function estimation. resulting algorithm lars-td works well certain situations original large features contains small subset highly-relevant features. recently parr looked problem value function estimation perspective model-free model-based reinforcement learning model-free approach estimates value function directly sample trajectories i.e. sequences feature vectors visited states. model-based approach contrast ﬁrst learns model computes value function learned model. parr compared lstd modelbased method ﬁrst learn linear model viewing features proxy state compute value function approximate model. parr demonstrated approaches compute exactly value function formalizing fact recognized degree current paper build insight simultaneously ﬁnding compact features using powerful methods system identiﬁcation. first look problem improving lstd model-free predictive-bottleneck perspective given large features history devise method called predictive state temporal difference learning estimates value function bottleneck preserves predictive information intuitively approach beneﬁts lars-td ﬁnding small predictive features avoid overﬁtting make learning data-efﬁcient. however method differs identify small subspace features instead sparse subset features. hence pstd lars-td applicable different situations show experiments below pstd better many marginally-relevant features lars-td better highly-relevant features hidden among many irrelevant ones. second look problem value function estimation model-based perspective instead learning linear transition model features subspace identiﬁcation learn samples. compute value function bellman equations learned psr. approach substantial beneﬁt linear feature-to-feature transition model seem common uses outside paper psrs proposed numerous times merits strict generalization pomdps. example pstd features explicitly interpreted statistically consistent estimate true underlying system state. feasibility ﬁnding true value function shown depend linear dimension dynamical system equivalently dimensionality predictive state representation—not cardinality pomdp state space. therefore representation naturally compressed sense speeding convergence. improved methods also yield practical beneﬁts; demonstrate beneﬁts several experiments. first compare pstd lstd lars-td synthetic example using different sets features illustrate strengths weaknesses algorithm. next apply pstd difﬁcult optimal stopping problem pricing high-dimensional ﬁnancial derivatives. significant amount work gone hand tuning features problem. show that large number weakly relevant features hand-tuned features pstd predictive subspace performs much better competing approaches improving best previously reported result particular problem substantial margin. theoretical empirical results reported suggest that many applications lstd used compute value function pstd simply substituted produce better results. value function approximation start discrete time dynamical system states actions distribution initial states state transition function reward function discount factor seek policy mapping states actions. notion value function central importance reinforcement learning given policy value state deﬁned expected discounted rewards obtained starting state following policy well known value function must obey know transition function states sufﬁciently small directly solve value function execute greedy policy setting action state maximize right-hand side however consider instead harder problem estimating value function partially observable latent variable transition function unknown. situation receive information observations ﬁnite state element history knew transition model could infer belief distribution belief state instead; below discuss learn compressed belief state. partial observability hope predict reward conditioned history must choose actions function history instead possible histories. often large inﬁnite instead ﬁnding value separately history focus value functions linear features histories parameter vector feature vector history rewrite bellman equation immediately attempt estimate parameter solving linear system least squares sense indicates moore–penrose pseudoinverse. however solution biased since independent variables noisy pr]. words estimating value function parameters error-in-variables problem. least squares temporal difference algorithm provides consistent estimate independent variables right multiplying approximate bellman equation quantity viewed instrumental variable i.e. measurement correlated true independent variables uncorrelated noise estimates variables. value function parameter estimated follows amount data increases k+φh converge probability population values estimate matrix inverted consistent. therefore long matrix nonsingular estimate inverse also consistent estimate therefore converges true parameters probability although lstd provides consistent estimate value function parameters practice potential size feature vectors problem. number features large relative number training samples estimation prone overﬁtting. problem alleviated choosing small features contain information relevant value function approximation. however exception lars-td little work problem select features automatically value function approximation system model unknown; course manual feature selection depends not-alwaysavailable expert guidance. approach problem ﬁnding good features bottleneck perspective. given signal history case large features would like compression preserves relevant information predicting value function section improvement directly related spectral identiﬁcation psrs. ﬁrst need deﬁne precisely task predicting future. history ordered sequence action-observation pairs executed prior time deﬁne test length ordered sequence action-observation pairs aioi executed observed time prediction test history written probability test observations given intervene execute test actions tests write τn)t corresponding vector test predictions. generalize notion test feature future linear combination several tests sharing common action sequence. example tests lstd algorithm also theoretically justiﬁed result application bellman make feature feature executed intervene executed value stands indicator random variable taking value depending whether observe sequence observations prediction given linear combinations tests seem restrictive deﬁnition actually expressive represent arbitrary function ﬁnite sequence future observations. take collection tests picks possible realization sequence weight test value function conditioned realization. example observations mean restriction common action sequence necessary without restriction tests making feature could never executed once. move feature predictions however makes sense lift restriction linear combination feature predictions also feature prediction even features involved different action sequences. action sequences raise problems obtaining empirical estimates means covariances features future e.g. always possible sample particular feature’s value every time step feature choose sample step restrict features sample subsequent steps. order carry derivations without running problems repeatedly assume rest paper reset system every sample history independently distributed distribution bookkeeping would unnecessarily complicate derivations.) furthermore introduce language keep derivations simple vector features future pretend sample evaluate features starting single history even different elements require execute different action sequences. algorithms call sample instead following trick random vector correct expectation write different action sequences probability distribution sequences. pick single sample features depend action sequence zeros everywhere else. easy expected value sample vector correct probability selection weighting factor cancel out. write stand expectation. none tricks actually necessary experiments stopping problems simply execute continue action every step sequences continue actions every test feature. order predictive feature compression ﬁrst need determine would like predict. since interested value function approximation relevant prediction value function itself; could simply predict total future discounted reward given history. unfortunately total discounted reward high variance unless data learning difﬁcult. reduce variance including prediction tasks well. example predicting individual rewards future time steps strictly necessary predict total discounted reward seems highly relevant gives much immediate feedback. similarly future observations hopefully contain information future reward trying predict observations help predict reward better. finally speciﬁc application able problem-speciﬁc prediction tasks help focus attention relevant information example pathplanning problem might predict several goal states reach represent prediction tasks features future e.g. predict goal reach distinct observation goal state predict individual rewards individual rewards observations. write vector features future time i.e. events starting time continuing forward. instead remembering large arbitrary features history want small subspace features history relevant predicting features future. call subspace predictive compression write value function linear function predictive compression features. predictive compression reduced-rank regression deﬁne following empirical covariance matrices features future features histories truncated weighted covariance left singular vectors right singular vectors diagonal matrix singular values. number columns equal number retained singular values. deﬁne weighting different features future differently change approximate compression interesting ways. example section scaling future reward constant factor results value-directed compression—but unlike previous ways value-directed compressions need know model system ahead time. another example deﬁne lower triangular cholesky factor empirical covariance future preserve largest possible amount mutual information features future features history. equivalent canonical correlation analysis matrix becomes diagonal matrix canonical correlations futures histories. don’t wish reveal extra information adding additional observations instead corresponding feature predictions observations; predictions deﬁnition reveal additional information. save trouble computing predictions realized feature values rather predictions learning algorithms below cost extra variance expectation realized feature value expectation predicted feature value. possible compression lose predictive power. practice though noise full rank. know true rank choose ﬁrst singular values deﬁne subspace compression. choose smaller subspace results approximate compression selectively dropping columns corresponding small singular values trade compression predictive power. directions larger variance features future correspond larger singular values minimize prediction error truncating smallest singular values. contrast unscaled covariance attempt minimize reconstruction error features history since features history standardized multiply inverse cholesky factor. least squares solution still prone error-in-variables problem. variable still correlated true independent variables uncorrelated noise instrumental variable unbias estimate deﬁne additional empirical provided intuition predictive features better arbitrary features temporal difference learning. show additional beneﬁt modelfree algorithm equation circumstances equivalent model-based value function approximation method uses subspace identiﬁcation learn predictive state representations predictive state representation compact complete description dynamical system. unlike pomdps represent state distribution latent variable psrs represent state predictions tests. formally consists elements ﬁnite possible actions ﬁnite possible observations. core tests i.e. whose vector predictions sufﬁcient statistic predicting success probabilities tests. functions embody predictions initial prediction vector. work restrict linear psrs vector r|q|. finally core prediction functions linear linear said minimal tests linearly independent i.e. test’s prediction linear function tests’ predictions. since sufﬁcient statistic tests state i.e. remember instead itself. action observation update recursively write matrix rows normalizer deﬁned mt∞q addition parameters need additional deﬁnitions reinforcement learning reward function mapping predictive states immediate rewards discount factor weights importance future rewards present ones policy mapping predictive states actions. fully general e.g. want unit reward test instead equivalently deﬁned instead ordinary psrs work transformed psrs tpsrs generalization regular psrs tpsr maintains small number sufﬁcient statistics linear combinations test probabilities. tpsr maintains small number feature predictions instead test predictions. tpsrs exactly predictive abilities regular psrs invariant similarity transforms given invertible matrix transform mt∞s− smaos− without changing corresponding dynamical system since pairs cancel main beneﬁt tpsrs regular psrs that given core tests dimensional parameters found using spectral matrix decomposition regression instead combinatorial search. respect tpsrs closely related transformed representations ldss hmms found subspace identiﬁcation minimal core tests dynamical system cardinality equal linear dimension system. then larger core tests possible histories. vector before write features future time since core tests deﬁnition compute test prediction linear function since feature predictions linear combinations test predictions also compute feature prediction linear function deﬁne matrix r×|t embody predictions future features entry weight tests calculating prediction features deﬁne several covariance matrices equation terms observable quantities show matrices relate parameters underlying psr. relationships lead learning algorithm below. first deﬁne covariance matrix features histories therefore rank linear dimension system. also that since size true covariance probability next deﬁne σhaoh matrices action-observation pair represent covariance features history taking action observing following indicator variable whether observation step wish above-deﬁned matrices learn tpsr data. need make somewhat-restrictive assumption assume features history rich enough determine state system i.e. regression exact σshς−hhφh discuss relax assumption section also need matrix invertible; probability random matrix satisﬁes condition below useful choose scaled version described sec. using assumptions show useful identity σhaoh identity heart learning algorithm shows σhaoh contains hidden copy main tpsr parameter need learn. would like recover σshς−hhσhaohς course know σsh. fortunately though turns stand-in described below since matrix differs invertible transform show recover tpsr matrices σhaoh since tpsr’s predictions invariant similarity transform parameters algorithm learning algorithm simple simply replace true covariance matrix empirical estimate. since empirical estimates converge true values probability sample size increases learning algorithm clearly statistically consistent. finally ready show model-free pstd learning algorithm introduced section equivalent model-based algorithm built around learning. ﬁxed policy tpsr’s value function linear function state solution tpsr bellman equation exactly pstd algorithm shown that learn subspace identiﬁcation algorithm sec. compute value function bellman equation exact answer directly learned value function model-free pstd method. addition adding understanding methods important corollary result pstd statistically consistent algorithm value function approximation—to knowledge ﬁrst result method. pstd learning related value-directed compression pomdps learn tpsr data generated pomdp tpsr state exactly linear compression pomdp state compression exact approximate depending whether include enough features future whether keep nonzero singular values bottleneck. include reward feature future value-directed compression sense poupart boutilier desired tune degree value-directedness compression scaling relative variance features higher variance reward feature compared features value-directed resulting compression work signiﬁcantly diverges previous work pomdp compression important respect prior work assumes access true pomdp model make assumption learn compressed representation directly data. close connection subspace identiﬁcation psrs provides additional insight temequation made assumption features poral difference learning procedure. history rich enough completely determine state dynamical system. fact using theory developed possible relax assumption instead assume state merely correlated features history. case need introduce actioncovariance matrices ebao aohu proof details). value function parameter estimated σrhu aohu h)†)† σrhu aoh)† since longer value function h−φt time instead need learn full model designed several experiments evaluate properties pstd learning algorithm. ﬁrst experiments look comparative merits pstd respect lstd lars-td applied problem estimating value function reduced-rank pomdp. second experiments apply pstd benchmark optimal stopping problem show pstd outperforms competing approaches. evaluate pstd learning algorithm synthetic example derived problem value function policy partially observable markov decision process pomdp latent states policy’s transition matrix rank resulting belief distributions represented -dimensional subspace original belief simplex. reward given ﬁrst third latent state reward latent states system emits possible observations conﬂating information latent states. perform experiments comparing performance lstd lars-td pstd pstd formulated section different sets features used. case compare value function estimated algorithm true value function computed ﬁrst experiment execute policy time steps. split data overlapping histories tests length sample histories tests serve centers gaussian radial basis functions. evaluate basis function every remaining sample. then using features learned value function using lstd lars-td pstd linear dimension pstd linear dimension experiment pstd pstd lower mean squared error approaches. second experiment added random features good features attempted learn value function algorithms case lstd pstd difﬁculty ﬁtting figure experimental results. error bars indicate standard error. estimating value function small number informative features. pstd pstd well. estimating value function small informative features large random features. lars-td designed scenario dramatically outperforms pstd lstd however outperform pstd. estimating value function large semi-informative features. pstd able determine small compressed features retain maximal amount information value function outperforming lstd large margin. pricing high-dimensional derivative policy iteration. y-axis expected reward current policy iteration. optimal threshold strategy black lstd blue lstd cyan lars-td green pstd red. value function large number irrelevant features tests histories relatively small amount training data. lars-td designed precisely scenario able select relevant features estimate value function better substantial margin. surprisingly experiment pstd outperformed pstd bested even lars-td. third experiment increased number sampled features case feature somewhat relevant number features relatively large compared amount training data. situation occurs frequently practice often easy large number features least somewhat related state. pstd pstd outperform lars-td subspace subset selection methods outperform lstd large margin efﬁciently estimating value function derivatives ﬁnancial contracts payoffs linked future prices basic assets stocks bonds commodities. derivatives contract holder choices complex cases contract owner must make decisions—e.g. early exercise contract holder decide terminate contract time receive payments based prevailing market conditions. cases value derivative depends contract holder acts. deciding exercise therefore optimal stopping problem point time contract holder must decide whether continue holding contract exercise. stopping problems provide ideal testbed policy evaluation methods since easily collect single data sufﬁcient evaluate policy choose continue action forever. consider ﬁnancial derivative introduced tsitsiklis derivative generates payoffs contingent prices single stock. given holder exercise. exercise owner receives payoff equal current price stock divided price days beforehand. think derivative psychic call owner gets decide whether s/he would like bought ordinary -day european call option then-current market price days ago. assuming stock prices ﬂuctuate days market open parameters correspond annual growth rate detail standard brownian motion stock price evolves ρpt∇t σpt∇wt summarize relevant state vector dimension represents amount investment stock time would grow time process markov ergodic independent identically distributed. immediate reward exercising option immediate reward continuing hold option discount factor determined growth rate; corresponds assuming risk-free interest rate equal stock’s growth rate meaning investor gains nothing expectation holding stock itself. value derivative current state given supt goal calculate approximate value function wtφh value function generate stopping time min{t| sample sequence states calculate features state. perform policy iteration sample alternately estimating value function given policy using value function deﬁne greedy policy stop wtφh. within strategy main choices features estimate value function terms features. value function estimation used lstd lars-td pstd. case re-used -state sample trajectory iterations start beginning follow trajectory long policy chooses continue action reward step. policy executes stop action reward next state’s features restart policy steps future process fully mixed. feature selection fortunate previous researchers hand-selected good features data repeated trial error greatly expand features pstd synthesize small highquality combined features. speciﬁcally entire -step state vector squares components state vector several additional nonlinear features increasing total number features histories length tests length choose linear dimension tests value-directed reducing variance features except reward factor figure shows results. compared pstd lstd either hand-selected features full features well lars-td simple thresholding strategy case evaluated ﬁnal policy random trajectories. pstd outperformed competitors improving next best approach lars-td percentage points. fact pstd performs better best previously reported approach percentage points. improvements correspond appreciable fractions risk-free interest rate therefore signiﬁcant arbitrage opportunities investor doesn’t know best strategy consistently undervalue security allowing informed investor expected value. paper attack feature selection problem temporal difference learning. although well-known temporal difference algorithms lstd provide asymptotically unbiased estimates value function parameters linear architectures trouble ﬁnite samples number features large relative number training samples high variance value function estimates. reason real-world problems substantial amount time spent selecting small features often trial error remedy problem present pstd algorithm approach feature selection methods demonstrates insights system identiﬁcation beneﬁt reinforcement learning. pstd automatically chooses small features relevant prediction approaches feature selection bottleneck perspective value function approximation. ﬁnding small features preserves predictive information. focus predictive information pstd approach closely connected psrs appropriate demonstrate merits pstd compared popular alternative algorithms lars-td lstd synthetic example argue pstd effective approximating value function large number features contains least little information state. finally apply pstd difﬁcult optimal stopping problem demonstrate practical utility algorithm outperforming several alternative approaches topping best reported previous results.", "year": 2010}