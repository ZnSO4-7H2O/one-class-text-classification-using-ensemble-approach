{"title": "Kafnets: kernel-based non-parametric activation functions for neural  networks", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Neural networks are generally built by interleaving (adaptable) linear layers with (fixed) nonlinear activation functions. To increase their flexibility, several authors have proposed methods for adapting the activation functions themselves, endowing them with varying degrees of flexibility. None of these approaches, however, have gained wide acceptance in practice, and research in this topic remains open. In this paper, we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron. Leveraging over several properties of kernel-based models, we propose multiple variations for designing and initializing these kernel activation functions (KAFs), including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network. The resulting KAFs can approximate any mapping defined over a subset of the real line, either convex or nonconvex. Furthermore, they are smooth over their entire domain, linear in their parameters, and they can be regularized using any known scheme, including the use of $\\ell_1$ penalties to enforce sparseness. To the best of our knowledge, no other known model satisfies all these properties simultaneously. In addition, we provide a relatively complete overview on alternative techniques for adapting the activation functions, which is currently lacking in the literature. A large set of experiments validates our proposal.", "text": "neural networks generally built interleaving linear layers nonlinear activation functions. increase ﬂexibility several authors proposed methods adapting activation functions themselves endowing varying degrees ﬂexibility. none approaches however gained wide acceptance practice research topic remains open. paper introduce novel family ﬂexible activation functions based inexpensive kernel expansion every neuron. leveraging several properties kernel-based models propose multiple variations designing initializing kernel activation functions including multidimensional scheme allowing nonlinearly combine information diﬀerent paths network. resulting kafs approximate mapping deﬁned subset real line either convex nonconvex. furthermore smooth entire domain linear parameters regularized using known scheme including penalties enforce sparseness. best knowledge known model satisﬁes properties simultaneously. addition provide relatively complete overview alneural networks powerful approximators built interleaving linear layers nonlinear mappings latter step usually implemented using element-wise diﬀerentiable ﬁxed nonlinear function every neuron. particular current consensus shifted contractive mappings piecewise-linear functions allowing eﬃcient backpropagated error relatively inﬂexible architecture might help explaining extreme redundancy found trained parameters modern designing ways adapt activation functions themselves however faces several challenges. hand parameterize known activation function small number trainable parameters describing example slope particular linear segment immediate implement results small increase ﬂexibility marginal improvement performance general case hand interesting task devise scheme allowing activation function model large range shapes smooth function deﬁned subset real line. case inclusion hyper-parameters enables user trade greater ﬂexibility larger number parameters per-neuron. refer schemes general non-parametric activation functions since number parameters potentially grow without bound. three main classes non-parametric activation functions known literature adaptive piecewise linear functions maxout networks spline activation functions described depth section there also argue none approaches fully satisfactory meaning loses desirable properties paper propose fourth class non-parametric activation functions based kernel representation function. particular deﬁne activation function linear superposition several kernel evaluations dictionary expansion ﬁxed beforehand sampling real line. show later resulting kernel activation functions number desirable properties including computed cheaply using vector-matrix operations; linear respect trainable parameters; smooth entire domain; using gaussian kernel parameters local eﬀects resulting shapes; parameters regularized using classical approach including possibility enforcing sparseness norms. best knowledge none known methods possess properties simultaneously. call endowed kafs every neuron kafnet. importantly framing method kernel technique allows potentially leverage huge literature kernel methods either statistics machine learning signal processing here preliminarly demonstrate discussing several heuristics choose kernel hyper-parameter along techniques initializing trainable parameters. however much applied context discuss depth conclusive section. also propose bi-dimensional variant allowing information multiple linear projections nonlinearly combined adaptive fashion. addition contend reason rareness ﬂexible activation functions practice found lack cohesive treatment topic. paper provide relatively comprehensive overview selection proper activation function. particular divide discussion state-of-the-art three separate sections. section introduces common activation functions used classical sigmoid function recently developed self-normalizing unit swish function then describe section functions eﬃciently parameterized adaptive scalar values order enhance ﬂexibility. finally introduce three existing models designing non-parametric activation functions section them brieﬂy discuss relative strengths drawbacks serve motivation introducing model subsequently. rest paper composed four additional sections. section describes proposed kafs together several practical implementation guidelines regarding selection dictionary choice proper initialization weights. completeness section brieﬂy describes additional strategies improve activation functions going beyond addition trainable parameters model. large experiments described section ﬁnally main conclusions future lines research given section column vectors. operator standard norm euclidean obtain manhattan norm deﬁned generic vector |vk|. additional notations introduced along paper following review common choices selection describing methods adapt based training data. readability drop subscript letter denote single input function call activation. note that cases activation function last layer cannot chosen freely depends task proper scaling output. particular common select regression problems sigmoid function brieﬂy review common activation functions neural networks basis parametric ones next section. current wave deep learning activation functions used ‘squashing’ type i.e. monotonically non-decreasing functions satisfying cybenko proved universal approximation property class functions results later extended larger class functions hornik practice squashing functions found limited deep networks prone problem vanishing exploding gradients bounded derivatives relu main advantages. first gradient either making back-propagation particularly eﬃcient. secondly activations sparse beneﬁcial several points views. smoothed version relu called softplus also introduced glorot despite lack smoothness relu functions almost always preferred softplus practice. obvious problem relus that wrong initialization unfortunate weight update activation stuck irrespective input. referred ‘dying relu’ condition. circumvent problem maas introduced leaky relu function deﬁned another problem activation functions non-negative output values mean value always positive deﬁnition. motivated analogy natural gradient clevert introduced exponential linear unit renormalize pattern activations case generally chosen modiﬁes negative part relu function saturating user-deﬁned value computationally eﬃcient smooth gradient either negative values immediate increase ﬂexibility parameterize previously introduced activation functions ﬁxed number adaptable parameters neuron adapt activation function diﬀerent shape. long function remains diﬀerentiable respect parameters possible adapt numerical optimization algorithm together linear weights biases layer. ﬁxed number parameters limited ﬂexibility call parametric activation functions. note parameters initialized randomly adapted independently every neuron. speciﬁcally determines range output controls slope curve. trentin provides empirical evidence learning amplitude neuron beneﬁcial respect unit amplitude activation functions. similar results also obtained recurrent networks recently consider parametric version leaky relu coeﬃcient initialized everywhere adapted every neuron. resulting activation function called parametric relu simple derivative respect parameter layer hidden neurons introduces additional parameters compared parameters generalized tanh. importantly case regularization user careful regularize parameters would bias optimization process towards classical relu leaky relu activation functions. initialized randomly adapted training process. based analysis always exists setting linear weights avoids vanishing gradient problem. diﬀerently prelu however parameters regularized order avoid degenerate behavior respect linear weights extremely small linear weights coupled large values parameters activation functions. srelu composed three linear segments middle identity. diﬀerently prelu however cut-oﬀ points three segments also adapted. additionally function convex nonconvex shapes depending orientation left right segments making ﬂexible previous proposals. similar prelu four parameters regularized intuitively parametric activation functions limited ﬂexibility resulting mixed performance gains average. diﬀerently parametric approaches non-parametric activation functions allow model larger class shapes price larger number adaptable parameters. stated introduction methods generally introduce global hyper-parameter allowing balance ﬂexibility function varying eﬀective number free parameters potentially grow without bound. additionally methods grouped depending whether parameter local global eﬀect overall function former desirable characteristic. section describe three state-of-the-art approaches implementing non-parametric activation functions functions section spline functions section maxout networks section function introduced agostinelli generalizes srelu function summing multiple linear segments slopes cut-oﬀ points learned constraint overall function continuous hyper-parameter chosen user parameteri=. parameters randomly initialized neuron regularized regularization similarly pelu order avoid coupling small linear weights large coeﬃcients units. previous theorem implies piecewise-linear function approximated provided behavior linear large small possible drawback activation function introduces points non-diﬀerentiability neuron damage optimization algorithm. next class functions solves problem cost possibly larger number parameters. since polynomial degree pass exactly points polynomial activation function theory approximate smooth function. drawback approach parameter global inﬂuence overall shape output function easily grow large encounter numerical problems particularly large absolute values large improved polynomial expansions spline interpolation giving rise spline activation function originally studied vecci guarnieri later re-introduced modern context scardapane following previous advances nonlinear ﬁltering sequel adopt newer formulation. described vector parameters called knots corresponding sampling y-values equispaced grid points x-axis symmetrically chosen around origin sampling step value output value computed spline interpolation closest knot rightmost neighbors generally chosen equal giving rise cubic spline interpolation scheme. speciﬁcally denote index closest knot vector comprising corresponding knot neighbors. call vector span. also deﬁne value figure example output interpolation using neuron. knots shown markers overall function given light blue. given activation black control points green background active. control points before interpolate using b-basis matrix instead matrix resulting curve smoother guaranteed pass control points. knot limited local inﬂuence output making adaptation stable. resulting function also smooth fact approximate smooth function deﬁned subset real line desired level accuracy provided chosen small enough. drawback regularizing resulting activation functions harder achieve since regularization cannot applied directly values knots. guarnieri solved choosing large turn severely limiting ﬂexibility interpolation scheme. diﬀerent parameters regularized penalizing deviations values initialization. note straightforward initialize known ﬁxed activation functions described before. diﬀerently functions described maxout function introduced goodfellow replaces entire layer particular neuron instead computing single product obtain activation compute diﬀerent products separate weight vectors biases take maximum activation function function subset output previous layer. maxout neurons hidden layers called maxout network remains universal approximator according following theorem. advantage maxout function extremely easy implement using current linear algebra libraries. however resulting functions several points non-diﬀerentiability similarly units. addition number resulting parameters generally higher alternative formulations. particular increasing multiply original number parameters corresponding factor approaches contribute linearly number. additionally lose possibility plotting resulting activation functions unless input maxout layer less dimensions. example dimension shown fig. figure example maxout neuron one-dimensional input three linear segments shown light gray resulting activation shown shaded red. note maxout generate convex shapes deﬁnition. however plots cannot made inputs three dimensions. kernel methods dictionary elements generally selected training data. stochastic optimization setting means would grow linearly number training iterations unless proper strategy selection dictionary implemented simplify treatment consider simpliﬁed case dictionary elements ﬁxed adapt mixing coeﬃcients. particular sample values x-axis uniformly around zero similar method leave user-deﬁned hyper-parameter. additional beneﬁt resulting model linear adaptable parameters eﬃciently implemented mini-batch training data using highly-vectorized linear algebra routines. note vast literature kernel methods ﬁxed dictionary elements particularly ﬁeld gaussian processes figure examples kafs. cases sample uniformly points x-axis mixing coeﬃcients sampled normal distribution. three plots show three diﬀerent choices properties kernel methods kafs equivalent learning linear functions large number nonlinear transformations original activation without explicitly compute transformations. gaussian kernel additional beneﬁt thanks deﬁnition mixing coeﬃcients local eﬀect shape output function advantageous optimization. addition expression gaussian kernel approximate continuous function subset real line expression resembles one-dimensional radial basis function network whose universal approximation properties also well studied below depth additional considerations implementing model. note model simple derivatives back-propagation literature many methods proposed select bandwidth parameter performing kernel density estimation methods include popular rules thumb scott silverman problem kernel density estimation abscissa corresponds given dataset arbitrary distribution. proposed scheme abscissa chosen according grid optimal bandwidth parameter depends uniquely grid resolution. instead leaving bandwidth parameter additional hyper-parameter empirically veriﬁed following rule thumb represents good compromise smoothness ﬂexibility random initialization mixing coeﬃcients normal distribution fig. provides good diversity optimization process. nonetheless advantage scheme initialize kafs follow know activation function guarantee certain desired behavior. speciﬁcally denote vector desired initial values corresponding dictionary elements initialize mixing coeﬃcients using kernel ridge regression experiments also consider two-dimensional variant proposed denote d-kaf. roughly speaking d-kaf acts pair activation values instead single learns twodimensional function combine them. seen generalization two-dimensional maxout neuron instead constrained output maximum value among inputs. uniform grid plane considering positions uniformly spaced around dimensions. group incoming activation values pairs possible pair activations output increase parameters counter-balanced factors. firstly grouping activations halve size linear matrix subsequent layer. secondly generally choose smaller respect case i.e. found values enough provide good degree ﬂexibility. table provides comparison proposed models three alternative non-parametric activation functions described before. brieﬂy mention multidimensional variant explored solazzi uncini many authors considered ways improving performance classical activation functions necessarily require adapt shape numerical optimization require special care implemented. completeness brieﬂy review moving experimental section. equivalent taking average possible values seen training. similar dropout technique randomly deactivates neurons step training later rescales weights test phase. general several papers developed stochastic versions classical artiﬁcial neurons whose output depend random variables sampled execution idea resulting noise help guide optimization process towards better minima. notably provides link classical probabilistic methods generative networks networks trained using variational inference main challenge design stochastic neurons provide simple mechanism back-propagating error random variables without requiring deﬁnitionanactivationfunctionislocalifeachadaptableweightonlyaﬀectsasmallportionoftheoutputvalues. tableacomparisonoftheexistingnon-parametricactivationfunctionsandtheproposedkafandd-kaf.inour figure example noisy activation function. original sigmoid function together hard-thresholded version possible activation value outside saturated regime random half-normal noise increasing variance matching sign according algorithm gulcehre final noisy activation function computed gulcehre test time expected values returned. expensive sampling procedures minimal amount interference network. representative example noisy activation functions proposed gulcehre achieve combining activation functions ‘hard saturating’ regimes random noise outputs whose variance increases regime function saturates avoid problems sparse gradient terms. example given fig. another approach design vector-valued activation functions maximize parameter sharing. simplest case concatenated relu returns output values applying relu function modiﬁes maxout neuron returning input activations sorted order instead picking highest value only. multi-bias activation functions compute several activations values using diﬀerent bias terms apply activation function independently resulting values. network-in-network model nonparametric approach speciﬁc convolutional neural networks wherein nonlinear units replaced fully connected speciﬁc tasks audio modeling authors proposed hermite polynomials adapting activation functions similarly proposed functions expressed weighted several ﬁxed nonlinear transformations activation values i.e. hermite polynomials. however nonlinear transformations computed recurrence formula thus highly increasing computational load. section provide comprehensive evaluation proposed kafs d-kafs applied several cases. preliminary experiment begin comparing multiple activation functions relatively small classiﬁcation dataset section discuss several examples shapes generally obtained networks initialization strategies. consider large-scale dataset taken baldi section show layers kafs able signiﬁcantly outperform feedforward network hidden layers even considering parametric activation functions state-ofthe-art regularization techniques. section show kafs d-kaf provide increase performance also applied convolutional layers cifar- dataset. finally show section signiﬁcantly faster training higher cumulative reward reinforcement learning scenario using mujoco environments openai gym. provide open-source library replicate experiments implementation kafs d-kafs three separate frameworks i.e. autograd tensorflow pytorch publicly accessible web. unless noted otherwise experiments linearly preprocess input features substitute eventual missing values median values computed corresponding feature columns. full dataset randomly keep portion dataset validation another portion test. neural networks softmax https//gym.openai.com/ https//github.com/hips/autograd https//www.tensorflow.org/ http//pytorch.org/ https//github.com/ispamm/kernel-activation-functions/ activation function output layer trained minimizing average cross-entropy training dataset small -regularization term whose weight selected accordance literature. optimization adam algorithm mini-batches elements default hyper-parameters. epoch compute accuracy validation stop training whenever validation accuracy improving consecutive epochs. experiments performed using pytorch implementation machine intel xeon cuda back-end employing nvidia tesla accuracy measures test computed repeating experiments diﬀerent splits dataset initializations networks. weights linear layers always initialized using so-called ‘uniform strategy additional parameters introduced parametric non-parametric activation functions initialized following guidelines original papers. begin experiment ‘sensorless’ dataset investigate whether kafs d-kafs indeed provide improvements accuracy respect baselines visualizing common shapes obtained training. sensorless dataset standard benchmark supervised techniques composed examples input features representing electric signals used predict among diﬀerent classes representing operating conditions. partition using random validation another testing small regularization factor dataset found best performing ﬁxed activation function simple hyperbolic tangent. particular network hidden layers neurons achieves test accuracy best result obtained network three hidden layers achieves test accuracy simplicity dataset found improvements adding layers including dropout training following sections. best performing parametric activation function instead prelu improves results obtaining accuracy single hidden layer three hidden layers. figure examples trained kafs sensorless dataset. y-axis plot output value kaf. initialization shown dashed ﬁnal shown solid green. distribution activation values training shown reference light blue. although dataset relatively simple shapes obtain representative experiments performed provide selection fig. speciﬁcally initialization shown dashed line ﬁnal shown solid green line. understanding behavior functions also plot empirical distribution activations test using light blue background. shapes similar common activation functions discussed section although shifted x-axis correspond distribution activation values. example fig. similar fig. similar standard saturating function. also latter case ﬁnal shape somewhat determined initialization ﬁnal shapes general tend independent initialization case fig. another common shape radial-basis function fig. similar gaussian function centered mean empirical distribution. shapes however vastly complex these. example fig. show function acts standard saturating function main part activations’ distribution right-tail tends remove values larger given threshold eﬀectively acting sort implicit regularizer. fig. show without intuitive shape selectively amplify multiple regions activation space. finally fig. show interesting pruning eﬀect useless neurons correspond activation functions practically zero everywhere. this combined possibility applying -regularization allows obtain networks signiﬁcant smaller number eﬀective parameters. interestingly shapes obtained fig. seem necessary high performance networks artifact initialization. speciﬁcally obtain similar accuracies even initializing kafs close possible hyperbolic tangents following method described section obtain vastly inferior performance initialize kafs randomly prevent adaptation. points fact ﬂexibility adaptability seems intrinsic component good performance experiment following sections aspect return conclusive section. results also similar using d-kafs initialize elements axis using strategy kafs. scenario obtain test accuracy single hidden layer accuracy hidden layers. examples obtained shapes provided fig. section evaluate algorithms realistic large-scale case susy benchmark introduced baldi task predict presence super symmetric particles simulation collision experiment starting features describing simulation itself. overall dataset composed million examples last used test another validation. task interesting several reasons. nature data even tiny change accuracy generally statistically signiﬁcant. original paper baldi showed best obtained deep feedforward network hidden layers signiﬁcantly better results compared shallow network. surprisingly agostinelli later showed shallow network fact suﬃcient long uses non-parametric activation functions order replicate results proposed methods consider baseline network inspired baldi hidden layers neurons relu activation functions dropout applied last hidden layer probability comparison also consider architecture substitute relus selu prelu functions. selu also substitute standard dropout customized version proposed klambauer compare simpler networks composed hidden layers neurons employing maxout neurons units proposed kafs d-kafs following random initializations previous section. results terms amount trainable parameters given table table results diﬀerent activation functions susy benchmark. last four rows proposed d-kaf. standard deviation given brackets best result shown bold second best result underlined. networks ﬁxed parametric activation functions hidden layers. text full description architectures. cases even signiﬁcantly. however improvements still require several layers depth fail provide accurate results experimenting shallow networks. hand non-parametric functions able achieve similar results requiring hidden layers neurons. among them maxout achieve similar layer able beneﬁt addition second layer. d-kaf able signiﬁcantly outperform competitors overall best result obtained d-kaf network hidden layers. obtained signiﬁcant reduction number trainable parameters also described depth following section. although focus feedforward networks interesting question whether superior performance exhibited kafs d-kafs also obtained diﬀerent architectures convolutional neural networks investigate this train several cnns cifarsince compare diﬀerent architectures convolutional kernels train simple cnns made stacking convolutional ‘modules’ composed convolutive layer ﬁlters windows stride dropout layer probability consider cnns minimum modules maximum output last dropout operation ﬂattened applying linear projection softmax operation. training setup equivalent previous sections. consider diﬀerent choices nonlinearity convolutional ﬁlters using baseline proposed kafs d-kafs. order improve gradient initial stages training kafs case initialized strategy using target. results shown fig. show left ﬁnal test accuracy right number trainable parameters three architectures. interestingly kafs d-kafs able signiﬁcantly better results baseline i.e. even layers convolutions suﬃcient surpass accuracy obtained equivalent -layered network baseline activation functions. fig. obtained negligible increase number trainable parameters signiﬁcant decrease d-kaf. reason before nonlinearity d-kaf merging information coming diﬀerent convolutive ﬁlters eﬀectively halving number parameters required subsequent layer. concluding evaluate performance proposed activation functions applied relatively complex reinforcement learning scenario. particular consider representative mujoco environments openai platform task learn policy control highly nonlinear physical systems including pendulums bipedal robots. baseline open-source openai implementation proximal policy optimization algorithm learns policy function alternating gathering episodes interactions environment building policy itself optimizing surrogate loss function. hyper-parameters taken directly original paper without attempting speciﬁc ﬁne-tuning figure results reinforcement learning experiments terms average cumulative rewards. compare baseline algorithm equivalent architecture nonlinearities. details models hyperparameters provided main discussion. algorithm. policy function baseline implemented hidden layers hidden neurons tanh nonlinearities providing output mean gaussian distribution used select action. comparison keep overall setup ﬁxed replace nonlinearities neurons using initialization preceding sections. plot average cumulative reward obtained every iteration algorithms diﬀerent environments fig. policy networks implemented functions consistently learn faster baseline with several cases consistent improvement respect ﬁnal reward. paper extensively reviewing known methods adapt activation functions neural network proposed novel family non-parametric functions framed kernel expansion input value. showed functions combine several advantages previous approaches without introducing excessive number additional parameters. furthermore smooth entire domain operations implemented easily high degree vectorization. experiments showed networks trained activations obtain higher accuracy competing approaches number diﬀerent benchmark scenarios including feedforward convolutional neural networks. initial model made number design choices paper include ﬁxed dictionary gaussian kernel hand-picked bandwidth kernel. however many alternative choices possible dictionary selection strategies alternative kernels several others. respect intriguing aspect proposed activation functions provide link neural networks kernel methods opening door large number variations described framework.", "year": 2017}