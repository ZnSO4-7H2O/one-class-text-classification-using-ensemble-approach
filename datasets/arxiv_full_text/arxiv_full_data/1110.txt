{"title": "Learning compressed representations of blood samples time series with  missing data", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Clinical measurements collected over time are naturally represented as multivariate time series (MTS), which often contain missing data. An autoencoder can learn low dimensional vectorial representations of MTS that preserve important data characteristics, but cannot deal explicitly with missing data. In this work, we propose a new framework that combines an autoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts for missingness patterns in MTS. Via kernel alignment, we incorporate TCK in the autoencoder to improve the learned representations in presence of missing data. We consider a classification problem of MTS with missing values, representing blood samples of patients with surgical site infection. With our approach, rather than with a standard autoencoder, we learn representations in low dimensions that can be classified better.", "text": "abstract. clinical measurements collected time naturally represented multivariate time series often contain missing data. autoencoder learn dimensional vectorial representations preserve important data characteristics cannot deal explicitly missing data. work propose framework combines autoencoder time series cluster kernel kernel accounts missingness patterns mts. kernel alignment incorporate autoencoder improve learned representations presence missing data. consider classiﬁcation problem missing values representing blood samples patients surgical site infection. approach rather standard autoencoder learn representations dimensions classiﬁed better. application machine learning deep learning brought signiﬁcant impact healthcare industry improving diagnosis outcomes changing providing care patients main challenge machine learning asked solve discover relevant structural patterns clinical data usually concealed diﬃcult detect manually. important fraction electronic health records clinical measurements collected patients time represented multivariate time series several eﬀorts devoted learn informative compact representations improve quality analysis also manage large amounts data necessary train deep learning models furthermore characterized complex relationships across variables time must accounted analysis. however methods designed treat vectorial data cannot trivially extended capture relationships. autoencoder type neural network originally conceived non-linear dimensionality reduction algorithm exploited learn data representations deep architectures adopted time series data codes real-typed vectors lying lower dimensional space clinical measurements often recorded irregular frequencies change diﬀerent patients across variables time. hence discretizing ∗this work funded norwegian research council fripro grant next time resulting containing missing values missing values follow patterns reﬂect medical conditions patients decisions doctors therefore important included analysis. since cannot process data containing missing values usually replaced imputation techniques that however cannot capture patterns blanks trying introduce less bias possible. hand recently proposed method called time series cluster kernel computes unsupervised kernel similarity missing data. leverages conﬁgurations missingness patterns improve evaluation similarity. work propose completely unsupervised approach learning compressed representations presence missing data. towards utilize deep kernelized autoencoder recently proposed architecture embeds properties given prior kernel code representation kernel alignment. introducing prior kernel extend dkae framework time series. moreover tck’s properties relationships among learned codes accounts presence missing data yielding discriminative representation data. apply method classify blood samples relative patients site infections contracted surgery high percentage missing data. compare classiﬁcation results obtained representations learned standard ones dkae implementing alignment tck. results indicate learned codes provide compact vectorial representation classiﬁer achieves better results operates code space rather input space. time series cluster kernel exploits missing patterns compute similarities rather relying imputation methods introduce strong biases. implements ensemble learning approach wherein robustness hyperparameters ensured joining clustering results many gaussian mixture models form ﬁnal kernel. hence critical hyperparameters must tuned user. deal missing data gmms extended using informative prior distributions matrix built ﬁtting gmms time series range numbers mixture components provide partitions diﬀerent resolutions capture local global structures data. enhance diversity ensemble partition evaluated random subset attributes segments using random initializations randomly chosen hyperparameters. also provides robustness hyperparameters selection. built summing inner products pairs posterior distributions corresponding diﬀerent mts. simultaneously learn functions. ﬁrst encoder provides mapping input domain code domain i.e. hidden representation. second function decoder maps back single hidden layer encoding decoding function denote respectively sample input space hidden representation reconstruction. usually implemented sigmoid case inputs real-valued vectors squashing nonlinearity replaced linear activation. finally weights bias encoder decoder respectively. stacking hidden layers capable learning complex representations transforming inputs multiple nonlinear transformations. native formulation process vectorial data therefore ﬂattened uni-dimensional vector since process inputs lengths missing ﬁlled numeric values. reconstruction loss hyperparameter traditional loss code loss enforces similarity matrices rn×n kernel matrix given prior rn×n inner product matrix codes associated input data. depiction minimizing normalized frobenius distance indirectly include codes information captures missingness patterns improve quality learned codes presence missing data. dkae model trained using mini-batches. therefore training matrix generated codes associated elements mini-batch distance computed submatrix related entries mini-batch analyze blood measurements collected patients undergoing gastrointestinal surgery university hospital north norway years patient dataset represented blood samples extracted within days surgery. contain measurements variables alanine aminotransferase albumin alkaline phosphatase creatinine hemoglobine leukocytes potassium sodium thrombocytes. focus cohort classes patients ones without surgical site infections. dataset labels assigned according international classiﬁcation diseases nomesco classiﬁcation surgical procedures relative patients severe postoperative complications. missing data correspond measurements collected given patient observation period. patients less measurements excluded cohort. ended patients infections. ﬁrst datasets used training rest test set. evaluate eﬀect alignment kernel compare classiﬁcation results obtained codes learned standard dkae. missing values ﬁlled three diﬀerent imputation techniques zero imputation mean imputation lastvalue-carried-forward imputation codes classiﬁed k-nn equipped euclidean distance. also consider results yielded input space similarity dkae also report mean squared error encoder input decoder output. reconstruction guarantee learn better representation input implies accurate back-mapping code input space. without kernel alignment dkae zero imputation last-value-carried-forward obtain best worst classiﬁcation performance respectively. imputation method codes learned dkae classiﬁed accurately reconstruction error increase even codes aligned prior kernel. demonstrate importance embedding codes similarity information yielded captures missingness patterns. indeed patterns ignored relies solely imputation whose purpose missing entries introducing less bias possible. interesting notice classiﬁcation input space based similarity slightly less accurate classiﬁcation code space dkae. therefore dkae yields codes reduced dimensionality handled easily processed faster discriminated easier inputs simple classiﬁer. fig. visualize ﬁrst components test input code spaces. compute linear codes kernel matrix coloring depends ground truth label observe classes better separated code space dkae. interestingly dkae notice structure yield kpca input space kernel. demonstrate kernel alignment procedure successfully embed codes properties without compromising precision decoder reconstruction. underline using rather kpca avoid performing costly eigendecomposition also learn inverse mapping code input space provided decoder. paper proposed novel approach learning compressed vectorial representations missing values common clinical records. achieved combining deep kernelized autoencoder fig. projection test ﬁrst components using kpca input space code space dkae code space. yellow dots triangles represent infected non-infected patients respectively. similarity measure accounts missingness patterns. tackled classiﬁcation blood samples patients postoperative infections data high percentage missing data. results showed aligning codes kernel matrix embed representation important information relative missingess patterns data improve classiﬁcation outcome.", "year": 2017}