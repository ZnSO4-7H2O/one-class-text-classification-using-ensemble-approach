{"title": "Fix your classifier: the marginal value of training the last weight  layer", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Neural networks are commonly used as models for classification for a wide variety of tasks. Typically, a learned affine transformation is placed at the end of such models, yielding a per-class value used for classification. This classifier can have a vast number of parameters, which grows linearly with the number of possible classes, thus requiring increasingly more resources. In this work we argue that this classifier can be fixed, up to a global scale constant, with little or no loss of accuracy for most tasks, allowing memory and computational benefits. Moreover, we show that by initializing the classifier with a Hadamard matrix we can speed up inference as well. We discuss the implications for current understanding of neural network models.", "text": "neural networks commonly used models classiﬁcation wide variety tasks. typically learned afﬁne transformation placed models yielding per-class value used classiﬁcation. classiﬁer vast number parameters grows linearly number possible classes thus requiring increasingly resources. work argue classiﬁer ﬁxed global scale constant little loss accuracy tasks allowing memory computational beneﬁts. moreover show initializing classiﬁer hadamard matrix speed inference well. discuss implications current understanding neural network models. deep neural network become widely used model machine learning achieving state-ofthe-art results many tasks. common task models used perform classiﬁcation case convolutional neural networks used classify images semantic category. models currently considered standard visual tasks allowing better accuracy preceding approaches training models using inference requires large amounts memory computational resources thus extensive amount research done lately reduce size networks. used weight sharing speciﬁcation micikevicius used mixed precision reduce size neural networks half. jaderberg used rank approximations speed nns. hubara zhou used aggressive approach weights activations gradients quantized reduce computation training. although aggressive quantization beneﬁts smaller model size extreme compression rate comes loss accuracy. past work noted fact predeﬁned random projections used together learned afﬁne transformation achieve competitive results several tasks. study suggest reversed proposal common models used learn useful representation even without modifying ﬁnal output layer often holds large number parameters grows linearly number classes. convolutional neural networks commonly used solve variety spatial temporal tasks. cnns usually composed stack convolutional parameterized layers spatial pooling layers fully connected layers separated non-linear activation functions. earlier architectures cnns used fully-connected layers later stage network presumably allow classiﬁcation based global features image. despite enormous number trainable parameters layers added model known rather marginal impact ﬁnal performance network easily compressed reduced model trained simple means matrix decomposition sparsiﬁcation more modern architecture choices characterized removal fully connected layers found lead better generalization overall accuracy together huge decrease number trainable parameters. additionally numerous works showed cnns trained metric learning regime explicit classiﬁcation layer introduced objective regarded distance measures intermediate representations. hardt suggested all-convolutional network variant kept original initialization classiﬁcation layer ﬁxed negative impact performance cifar dataset. properties provide evidence fully-connected layers fact redundant play small role learning generalization. despite apparent minor role play fully-connected layers still commonly used classiﬁcation layers transforming dimension network features number required class categories therefore classiﬁcation model must hold number trainable parameters grows linear manner number classes. property still holds fully-connected layer replaced convolutional classiﬁer shown springenberg work claim common use-cases convolutional network parameters used ﬁnal classiﬁcation transform completely redundant replaced predetermined linear transform. show ﬁrst time property holds even largescale models classiﬁcation tasks recent architectures trained imagenet benchmark ﬁxed transform many cases allow huge decrease model parameters possible computational beneﬁt. suggest existing models modiﬁcation devoid classiﬁer weights help deployment models devices computation ability smaller memory capacity. moreover keep classiﬁer ﬁxed less parameters need updated reducing communication cost models deployed distributed systems. ﬁxed transform depend number classes allow models scale large number possible outputs without linear cost number parameters. also suggest ﬁnding might shed light importance preceding non-linear layers learning generalization. focus attention ﬁnal representation obtained network classiﬁer. denote representation assumed deep neural network input parameters e.g. convolutional network trained backpropagation. also trained back-propagation. input length different possible outputs required matrix training done using cross-entropy loss feeding network outputs softmax activation evaluate conjecture regarding importance ﬁnal classiﬁcation transformation replaced trainable parameter matrix ﬁxed orthonormal projection rn×c column ensured simple random sampling singular-value decomposition rows classiﬁer weight matrix ﬁxed equally valued norm beneﬁcial also restrict representation normalizing reside n-dimensional sphere allows faster training convergence network need account changes scale weights. face problem bounded causes convergence issues softmax function scale sensitive network affected inability re-scale input. similar phenomenon described vaswani respect softmax function used attention mechanisms. spirit amend issue ﬁxed also known softmax temperature. scale applied softmax inputs softmax difference single scale entries weight matrix contrast scale salimans kingma uses. keep additional vector bias parameters train using negative-loglikelihood criterion. explicitly classiﬁer output recall ﬁnal representation obtained network speciﬁc sample ground-truth label sample. observing behavior parameter time revealed logarithmic growth depicted graph interestingly behavior exhibited norm learned classiﬁer ﬁrst described hoffer linked generalization network. recently explained under-review work soudry convergence margin classiﬁer. suggest using single parameter enable simpler examination possible exploration phenomenon implications. note also found possible train network simple cosine angle loss suggest hadamard matrix ﬁnal classiﬁcation transform. hadamard matrix matrix entries either more orthogonal identity matrix. truncated hadamard matrix }c×n rows orthogonal ﬁnal classiﬁcation layer also note similarity using hadamard matrix ﬁnal classiﬁer methods weight binarization suggested courbariaux classiﬁer weights ﬁxed need -bit precision possible focus attention features preceding used well known cifar cifar datasets krizhevsky initial test-bed explore idea ﬁxed classiﬁer. cifar image classiﬁcation benchmark dataset containing training images test images. images color contain pixels. possible classes various animals vehicles. cifar holds number images size contains different classes. trained residual network cifar dataset. used network depth hyper-parameters used original work. compared variants original model learned classiﬁer version ﬁxed transformation used. results shown ﬁgure demonstrate although training error considerably lower network learned classiﬁer models achieve classiﬁcation accuracy validation set. conjecture ﬁxed parameterization network longer increase norm given sample’s representation thus learning label requires effort. happen speciﬁc seen samples affects training error. also compared using ﬁxed scale variable different values learned parameter. results depicted ﬁgure training validation error. seen similar validation accuracy obtained using ﬁxed scale value expense another hyper-parameter seek. experiments opted train parameter instead. experiments scale parameter regularized weight decay coefﬁcient used original classiﬁer. followed train model cifar dataset. used densenet-bc model huang depth layers continued train according original regime setting described network dataset. naturally higher number classes caused number parameters grow encompass whole model. validation accuracy ﬁxed-classiﬁer model remained equally good original model continued observe training curve. order validate results challenging dataset used imagenet dataset introduced deng imagenet dataset spans visual classes million samples. cnns used classify imagenet krizhevsky szegedy usually hidden representation leading ﬁnal classiﬁer least dimensions. architectural choice together large number classes causes size classiﬁer exceed millions parameters taking sizable share entire model size. evaluated ﬁxed classiﬁer method imagenet using resnet training regime hyper-parameters. using ﬁxed classiﬁer approximately -million parameters removed model accounting model parameters. following procedure trained densenet model ﬁxed classiﬁer reduced parameters. similarly results cifar dataset observed convergence speed approximately ﬁnal accuracy validation training sets. furthermore interested evaluating challenging models classiﬁer parameters constitutes majority amount. reason chose shufﬂenet architecture designed used memory limited computing platforms. shufﬂenet network contains million parameters million part ﬁnal classiﬁer. fixing classiﬁer resulted model million parameters. model trained found again converge similar validation accuracy original. interestingly method allowed imagenet training under-speciﬁed regime training samples number parameters. unconventional regime modern deep networks usually over-speciﬁed many parameters training samples moreover many recent theoretical results related neural network training even generalization usually assume over-speciﬁcation. table summarizes ﬁxed-classiﬁer results convolutional networks comparing originally reported results. offer drop-in replacement learned classiﬁer used train models ﬁxed classiﬁers replicate results. language modeling requires classiﬁcation possible tokens available task vocabulary interested ﬁxed classiﬁer used possible saving large number trainable parameters recent works already found empirically using weights word embedding classiﬁer yield equal better results using separate pair weights compliant ﬁndings linear classiﬁer largely redundant. examine reduction number parameters removed classiﬁer embedding weights replaced ﬁxed transform. trained language model wikitext dataset described merity using setting merity used recurrent model -layers lstm embedding hidden size vocabulary wikitext holds different words expected number parameters embedding classiﬁer -million. number makes parameters used whole model. found using random orthogonal transform yielded poor results compared learned embedding. suspect that oppose image classiﬁcation benchmarks embedding layer language models holds information words similarities relations thus requiring initialization. test intuition opted pre-trained embeddings using wordvec algorithm mikolov factorization suggested levy goldberg using ﬁxed wordvec embeddings achieve much better results. speciﬁcally less parameters fully learned model obtain somewhat worse perplexity. last couple years observe rapid growth number classes benchmark datasets contain example cifar imagenetk imagenetk language modeling therefore computational demands ﬁnal classiﬁer increase well considered less architecture chosen. work case introduced jft-m internal google dataset different classes. using resnet sized representation model parameters. means model parameters reside ﬁnal classiﬁcation layer. describes difﬁculty distributing amount parameters training servers need split sub-layers. also note fact training procedure needs account synchronization parameter update must incur non-trivial overhead. work help considerably kind scenario using ﬁxed classiﬁer removes need gradient synchronization ﬁnal layer. furthermore using hadamard matrix remove need save transformation altogether make efﬁcient allowing considerable memory computational savings. argue method works ability preceding layers network learn separable representations easily classiﬁed even classiﬁer ﬁxed. property affected ratio learned features number classes small we’ve experimenting cases example imagenet classiﬁcation using mobilenet-. reduced version resnet scenarios method converged similarly fully learned classiﬁer reaching ﬁnal validation accuracy. strengthening ﬁnding showing even cases ﬁxed classiﬁer provide equally good results. another possible issue appear possible classes highly correlated. ﬁxed orthogonal classiﬁer account kind correlation prove hard network learn case. suggest another reason difﬁculties experienced training language model using orthogonal ﬁxed classiﬁer word classes tend highly correlated instances. recent works suggested connection generalization capabilities models various norm-related quantities weights. results might potentially simpliﬁed model since single scalar variable seems relevant parameter model ﬁxed classiﬁers might simpliﬁed binarized neural networks activations weights restricted propagations. case norm last hidden layer constant samples constant absorbed scale constant need per-sample normalization also plan explore efﬁcient ways learn word embedding similar redundancy classiﬁer weights suggest simpler forms token representations low-rank sparse versions allowing similar beneﬁts ﬁxed transformations suggested. work suggested removing parameters classiﬁcation layer used deep neural networks. showed empirical results suggesting keeping classiﬁer ﬁxed cause little decline classiﬁcation performance common balanced datasets cifar imagenet allowing noticeable reduction trainable parameters. argue ﬁxing last layer reduce computational complexity training well communication cost distributed learning. furthermore using hadamard matrix classiﬁer might lead computational beneﬁts properly implemented save memory otherwise spent large amount transformation coefﬁcients. datasets tend become complex time believe resource hungry afﬁne transformation remain ﬁxed training least partially. also found efﬁcient methods create pre-deﬁned word embeddings explored require huge amount parameters possibly avoided learning task. based ﬁndings recommend future research focus representations learned non-linear part neural networks ﬁnal classiﬁer seems highly redundant. research leading results received funding taub foundation european research council european unions horizon program grant agreement speedinftradeoff. jane bromley isabelle guyon yann lecun eduard s¨ackinger roopak shah. signature veriﬁcation using siamese time delay neural network. advances neural information processing systems matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition andrew howard menglong chen dmitry kalenichenko weijun wang tobias weyand marco andreetto hartwig adam. mobilenets efﬁcient convolutional neural networks mobile vision applications. arxiv preprint arxiv. huang zhuang laurens maaten kilian weinberger. densely connected convolutional networks. proceedings ieee conference computer vision pattern recognition itay hubara matthieu courbariaux daniel soudry el-yaniv yoshua bengio. quantized neural networks training neural networks precision weights activations. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems paulius micikevicius sharan narang jonah alben gregory diamos erich elsen david garcia boris ginsburg michael houston oleksii kuchaev ganesh venkatesh mixed precision training. arxiv preprint arxiv. salimans diederik kingma. weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems mahdi soltanolkotabi adel javanmard jason lee. theoretical insights optimization landscape over-parameterized shallow neural networks. arxiv preprint arxiv. christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition shuchang zhou zekun xinyu zhou yuxin yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. arxiv preprint arxiv.", "year": 2018}