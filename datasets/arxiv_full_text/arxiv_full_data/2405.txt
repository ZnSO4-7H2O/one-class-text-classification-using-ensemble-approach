{"title": "Classifying Options for Deep Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different \"option heads\" on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer.", "text": "paper combine method hilearning—the options erarchical framework—with different option heads policy network supervisory network choosing different options. utilise setup investigate effects architectural constraints subtasks positive negative transfer across range network capacities. empirically show augmented lower sample complexity simultaneously learning subtasks negative transfer without degrading performance learning subtasks positive transfer. introduction recent advances reinforcement learning focused using deep neural networks represent state-action value function policy function successes methods largely attributed representational power deep networks. tempting create ultimate end-to-end solution general problems kinds powerful models. however general methods require large amount samples order learn effective policies. order reduce sample complexity domain knowledge construct algorithm biased towards promising solutions. deep q-network end-to-end reinforcement learning algorithm achieved great success variety video games work explore possibility imposing structural priors onto adding domain knowledge whilst trying limit reduction generality dqn. classical reinforcement learning literature large amount research focusing temporal abstraction problems known umbrella hierarchical reinforcement learning. options framework augments admissible actions temporally-extended actions called options. context option closed-loop controller execute either primitive actions options according policy. ability options gives agent many advantages temporally-extended exploration allowing planning across range time scales ability transfer knowledge different tasks. another approach maxq decomposes task hierarchy subtasks. decomposition allows maxq agents improve performance towards achieving main objective recursively solving smaller problems ﬁrst. options maxq frameworks constructed prior domain knowledge implemented naturally. approaches viewed constructing main policy smaller sub-policies implemented prior knowledge either structural constraints behaviour subpolicies themselves. consider class problems task broken reward-independent subtasks human expert. task decomposition done subtasks share state space explicitly partitioned action space similarly partitioned knowledge also incorporated bound actions available option policy. domain knowledge decompose composition smaller representations derived prior work hierarchical reinforcement learning. although evidence implicitly learn options investigate whether beneﬁt constructing options explicitly. related work option heads inspired osband policy network similar ours without additional supervisory network. network’s bootstrap heads used different motivations. train heads different initialisations task. allows network represent distribution qfunctions. exploration done deeply sampling head using policy whole episode experiences shared across heads. motivation using option heads however allowing temporally abstracted actions akin options framework concretely decompose policy combination simpler sub-policies. motivates supervisory network discussed subsection although focus options framework notion subtasks alternative view multitask learning. context deep reinforcement learning recent work generalised distillation applied classiﬁcation order train dqns several atari games parallel distillation uses trained teacher networks provide extra training signals student network originally technique used distill knowledge large teacher network smaller student network form model compression. multitask learning parisotto able keep student architecture teacher dqns whilst rusu created larger network additional fully connected layer. latter explicitly separated network different controllers game called architecture multi-dqn architecture combination policy distillation algorithm multi-dist. studies showed teacher networks could enable learning effective policies several atari games parallel— standard multidqn architectures unable perform well across games without teacher networks. note bootstrapped multi-dqn similar structures several heads either directly indirectly shared convolutional layers. baselines evaluating actor-mimic framework multitask convolutional architecture bootstrapped dqn. although working architecture goals different incorporating options framework. ﬁrst major difference work supervisory network allows infer subtask attempted time step evaluation. conversely multitask setting different tasks typically clearly separated. secondly method rely teacher networks instead focusing whose augmented training signal based knowledge current subtask. respect latter point focus analysis controlling capacity networks opposed scaling parameters linearly number tasks another notable success subtask learning multiple independent sources reward universal value function approximators uvfas allow generalisation value functions across different goals helps agent accomplish tasks never seen before. focus uvfas generalising between similar subtasks sharing representation different tasks. recently expanded upon hierarchical-dqn however goal-based approaches demonstrated domains different goals highly related. function approximation perspective goals share structure states. contrast approach focuses separating distinct subtasks partial independence subpolicies enforced structural constraints. particular expect separate q-functions background consider reinforcement learning problem want agent’s policy maximises expected disγtrt]. discount parameter controls importance immediate rewards relative distant rewards future. reward scalar value emitted state policy selects performs action response state transitions st+. transition states modelled markov decision process state sufﬁcient statistic entire history transition time need depend action at−. full introduction. q-learning algorithm solves reinforcement learning problem approximating optimal state-action value q-function deﬁned expected discounted reward starting state taking initial action henceforth following optimal policy approximate q-function function approximator parameters learning done adjusting parameters reduce inconsistency left right hand sides bellman equation. optimal policy derived simply choosing action maximises time step. parameters associated separate target network updated every steps. target network increases stability learning. parameters updated mini-batch stochastic gradient descent following gradient loss function. another successful training dqns experience replay updating parameters stochastic gradient descent squared loss function implies i.i.d. assumption valid online reinforcement learning problem. experience replay stores samples past transitions pool. training samples drawn uniformly pool. helps break temporal correlation samples also allows updates reuse samples several times. setting games different states unlikely many features common would assume hierarchical reinforcement learning setting subtasks problem occur. addition augmented policy network indexes option heads also introduce supervisory network learns mapping state option; allows option head focus subset state space. networks full policy written deterministic mapping option heads option heads consist fully connected layers branch topmost shared convolution layer. ﬁnal layer head outputs q-value discrete action available hence limited using domain knowledge task hand desired options. training oracle used choose option head evaluated time step action picked \u0001-greedy strategy head shared heads. experience samples tuples stored separate experience replay buffers head. evaluation oracle replaced decisions supervisory network. double deep q-networks follow learning algorithm hasselt lower overestimation q-values update rule. modiﬁes original target equation following options framework orginal deﬁnition options consists three components policy termination condition initiation illustrate role components following interpretation daniel consider stochastic policy distribution actions given state time step auxiliary variable dependent dependent ot−. variable controls selection action conditional dependence interpreted policy markov option termination condition thought speciﬁc constraint conditional form imposed transition option follows δotot− otherwise. initiation speciﬁes domain available consider fully observable assumed sufﬁcient statistic history including ot−. therefore model conditionally indepedent given deﬁne supervisory policy termination condition initiation absorbed supervisory policy. full policy decomposed into form policy seen combination option policies weighted supervisory policy. show next section decompose separate option policies alongside supervisory policy. deep q-networks option heads augment standard several distinct sets outputs; concretely architecture bootstrapped mcdqn mcdqn domain knowledge choose number heads priori knowledge train option head separately. comparison standard option heads pictured figure noted even augmentation fail multitask setting different policies interfering lower levels network highlights need study. along work assume convolutional layers learn general representation state space whilst fully connected layers network encode actual policy. multitask atari well investigating effects different kinds transfer also look effects varying capacity network—speciﬁcally experiments neurons fully connected hidden layer standard dqn. correspodingly half neurons option heads network also neurons. seen figure lack capacity significant effect performance. capacity increases differences three networks diminishes. besides capacity architecture appear signiﬁcant impact positive transfer subtasks. however negative transfer subtasks option heads able make signiﬁcantly quicker progress standard dqn. given enough capacity control head capacity experiment—the half dqn—also converges policy terms performance larger sample complexity. suggests incorporating domain knowledge form structural constraints beneﬁcial even whilst keeping model capacity same—in particular quicker learning suggests knowledge effectively utilised reduce number samples needed deep reinforcement learning. qualitatively convolutional ﬁlters learned dqns highly similar. reinforces intuition structural constraint imposed upon option heads allows low-level feature knowledge falling balls moving paddle learned shared convolution layers whilst policies catching avoiding balls represented explicitly head. discussion show simple architectural adjustment possible successfully impose prior domain knowledge subtasks algorithm. demonstrate idea game catch task catching avoiding falling balls depending colour decomposed intuitively subtask catching grey balls another subtask avoiding white balls—subtasks incur negative transfer. show learning subtasks separately different option heads allows learn lower sample complexity. shared convolutional layers learn generally useful features whilst heads learn specialise. comparison standard presumably suffers subtask interference single q-function. additionally structural constraint hinder performance learning subtasks positive transfer. results contrary reported mcdqn trained eight atari games simultaneously outperformed standard standard cording parisotto mcdqn tend focus performing well subset games expense others. posit strategy works well whilst explicitly conreceives entire state. hidden layers constructed using domain knowledge e.g. convolutional layers visual domains. output layer softmax layer outputs distribution options given state trained standard cross-entropy loss function. training targets given oracle. experiments experiments reimplemented game catch task control paddle bottom screen catch falling balls input greyscale pixel grid action space consists discrete actions move left move right noop. receives stack current plus previous frames. episode pixel ball falls randomly screen agent’s -pixel-wide paddle must move horizontally catch original reward given catching white ball; additional grey ball introduce subtasks environment. simple environment allows meaningfully evaluate effects architecture subtasks positive negative transfer. positive transfer case subtasks same—catching either ball results reward negative transfer case grey ball still gives reward catching white ball results reward subtask optimal agent must learn catch grey balls avoid perceptually-similar white balls; suboptimal solutions would include avoiding catching types balls. setups type ball used switched every episode. baseline standard dqn. order provide fair comparison impose condition architecture policy network condition training. ﬁrst condition divide number neurons hidden layer option head number option heads thereby keeping number parameters same. second condition alternate heads performing qlearning update keeping number training samples same. also construct half contains half parameters standard fully connected hidden layer; uses standard architecture option heads. tests whether sample complexity option heads either result fewer parameters tune head result imposed structural constraint. details model architectures training hyperparameters given appendix. figure average score episode standard half option heads. average standard deviation calculated runs score averaged episodes. epoch corresponds training steps. number neurons corresponds number neurons fully connected hidden layer standard dqn. positive transfer setting important factor capacity architecture. capacity increases difference performance networks diminishes. negative transfer setting effect capacity strong capacity otherwise option heads demonstrates superior sample complexity baselines. best viewed colour. structed controller heads mcdqn receive consistent training signals cause parameter gradients interfere shared convolutional layers less concern hierarchical reinforcement learning setting assume coherent state space across subtasks. regime subtasks rather multiple tasks option heads generalisable goal-based approaches however methods currently require hand-crafted representation goals input networks—a sensible approach goal representation reasonably appended state space. contrast oracle mapping states options used constructing representations goals inputs straightforward task. suggests future work removing oracle focusing option discovery agent must learn options without supervisory signal.", "year": 2016}