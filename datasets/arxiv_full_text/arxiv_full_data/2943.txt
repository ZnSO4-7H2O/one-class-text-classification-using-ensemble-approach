{"title": "Fooling OCR Systems with Adversarial Text Images", "tag": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "abstract": "We demonstrate that state-of-the-art optical character recognition (OCR) based on deep learning is vulnerable to adversarial images. Minor modifications to images of printed text, which do not change the meaning of the text to a human reader, cause the OCR system to \"recognize\" a different text where certain words chosen by the adversary are replaced by their semantic opposites. This completely changes the meaning of the output produced by the OCR system and by the NLP applications that use OCR for preprocessing their inputs.", "text": "demonstrate state-of-the-art optical character recognition based deep learning vulnerable adversarial images. minor modiﬁcations images printed text change meaning text human reader cause system recognize different text certain words chosen adversary replaced semantic opposites. completely changes meaning output produced system applications preprocessing inputs. machine learning techniques based deep neural networks major advances state many image analysis tasks including object classiﬁcation face recognition modern models however vulnerable adversarial examples minor modiﬁcation input image often imperceptible human change output model applied image e.g. produce incorrect classiﬁcation cause model segment image incorrectly optical character recognition another image analysis task deep learning great improvements quality models. different image classiﬁcation several essential ways. first modern models based classifying individual characters. instead assign sequences discrete labels variable-sized inputs. consequently recognize text line line opposed character character. presents challenge adversary because show attacks simply paste adversarial images individual characters input image ineffective state-of-the-art models. figure adversarial example ocr-based license plate recognition system. model takes variable-sized image input outputs sequence characters occur image. task different image classiﬁcation. natural-language text arbitrary sequences characters context small perturbations input image typically cause model reject input else produce meaningless output. search adversarial examples guided linguistic information—in case pairs words visually similar semantically opposite. aspects task favor adversary. goal recognize natural-language text incorrectly recognizing even single world impact overall meaning text. adversary effect small targeted change model’s output—for example replace well-chosen word antonym—can completely change human would understand resulting text. third systems often used components natural language processing pipelines. output applications document categorization summarization. ampliﬁes impact adversarial examples applications highly sensitive certain words input. show tiny changes input images dramatically change output models operating results applied images. further many models trained ocr-processed documents. training inputs replaced adversarial images adversary poison model. contributions. investigate power adversarial examples tesseract popular system based deep learning. chose tesseract because trained model publicly available also tesseract used many ocr-based applications. show generate adversarial images individual words cause tesseract misrecognize antonyms effectively ﬂipping meaning. extend word-level attacks entire documents. using corpus hillary clinton’s emails experiments show modify data including dates times numbers addresses change chosen words antonyms completely changing meaning text produced meaning text original document. evaluate attack applications rely extract text images. show adversarial text images fool semantic analysis model conﬁdently producing wrong predictions. document categorization model adversary generate text images that processed misclassiﬁed target class chosen adversary. also show adversarial images poison training data degrade performance sentiment analysis model. adversarial perturbations needed stage successful attacks tesseract affect tiny fraction pixels input image localized small subregion image corresponding words attacked. furthermore large documents present many opportunities attack adversary many choices words modify order change meaning resulting text. limitations attack include transferability physical realizability. general perturbations needed make adversarial text images physically realizable resulting images rejected tesseract noisy. nevertheless demonstrate adversarial word image fools tesseract recognizing semantically opposite word even image printed onto paper scanned back digital form. feature space output space. classiﬁcation problems vector space discrete classes supervised training model aims best parameters using labeled training dataset loss metric lti) measures model’s prediction correct label sequence labeling complicated task assigns sequence discrete labels variable-sized sequential input data. example optical character recognition input image output sequence characters alphabet input image output text vary length alignment image regions corresponding text characters known priori. connectionist temporal classiﬁcation provides alignment-free method training end-toend neural network sequence labeling tasks. neural-network model outputs sequence probability vectors probability distribution characters position training model requires calculating likelihood necessarily hard directly measure model’s preinstead diction target sequence measured using valid alignment sequence γ∪{blank} valid alignment turned removing blanks sequential duplicate characters. example valid alignment resulting model given input produces obtain probable output sequence greedy algorithm select argmax position collapse alignment greedy method however account fact sequence many valid alignments. better method based beam search decoding keep ﬁxed number probable alignments position return output highest probabilities valid alignments top-alignment list. figure pipeline deep learning-based recognition model. system ﬁrst performs page layout analysis detect text image segments image sub-images containing line text each. line image scaled normalized match training data recognition model. normalized line images recognition model. finally system outputs combined text predictions. optical character recognition technology converts images handwritten printed text digital text. pipeline generally starts preprocessing images. common preprocessing techniques include page layout analysis localizing blocks texts image de-skewing image text aligned properly segmenting image extract blocks regions contain text. recognition model applied preprocessed images. characters produced model output pipeline. recognition models roughly categorized character-based recognition traditional approach recognizing text block text images. examples character-based include gocr legacy version tesseract character recognition model ﬁrst localizes characters image segments image sub-images contain character each. model extracts features sub-image feeds machine learning classiﬁer identify likely character. features usually hand-engineered include example lines closed loops line directions intersections etc. classiﬁer typically fairly simple model k-nearest neighbors. performance classiﬁer severely degrades single-character images produced segmentation bad. therefore overall performance character-based recognition models strongly depends segmentation method. end-to-end recognition segmentation-free technique aims recognize entire sequences characters variable-sized block text image. sequential models hidden markov models used purpose recent advances deep learning image analysis end-to-end recognition models based deep neural networks become increasingly popular. models utilize neural networks feature extractor thus require features manually engineered. sequential deep neural-network models also allow variable-sized input images thus avoid issues arise segmentation character-based models. applications. widely used many realworld applications including automated data entry license plate recognition also serve main preprocessing step natural language processing tasks text classiﬁcation document retrieval machine translation even cancer classiﬁcation applications critically depend correctness consequences mistakes serious—from wrong cars ﬁned violations incorrect medical diagnoses. chose tesseract investigation widely used open-source systems trained model available legacy version tesseract uses character-based recognition model focus latest version tesseract uses end-to-end deep learning-based recognition model pipeline latest version tesseract follows chart fig. tesseract’s recognition model takes line image input outputs sequence characters recognized line. line recognition task essentially sequence labeling problem input images vary width. tesseract adds small preprocessing step scales normalizes line images match input domain tesseract’s training data. overall architecture tesseract’s deep learning model given table inputs gray-scaled images size network starts convolution layer ﬁlter tanh activation function followed max-pooling layer network stacked long short-term memory layers with respectively hidden units. lstm layer several modes means forward/reverse pass indicates direction pass horizontal vertical indicates whether returns last step lstm outputs. finally output layer units corresponding number possible english characters. therefore network produces probability vectors size given vectors tesseract outputs probable sequence characters beam-search decoding. many machine learning models vulnerable adversarial examples instance object classiﬁcation tasks small perturbation—perhaps even imperceptible human eye—can cause model classify image containing object certain type different type high conﬁdence. formally given model maps input output prediction adversary perform either untargeted attack targeted attack. untargeted attack goal adversary generate adversarial example targeted attack adversary speciﬁc target output mind. goal construct adversarial example types attacks must threshold distance metric measures similarity inputs. constructing adversarial examples usually formulated optimization problem. machine learning task loss function measures error true target model’s prediction problem generating untargeted adversarial example valid optimization problem typically solved using standard gradient descent. given access parameters model gradient respect adversarial input calculated using back-propagation. previous literature considered adversarial examples standard image classiﬁcation tasks. section explain generating adversarial examples models difﬁcult. assume adversary complete access entire pipeline including preprocessing algorithms architecture parameters recognition model decoding algorithm and—when output used input applications—the machine learning models used latter. assumption holds tesseract well open-source systems. prior work also shown sometimes possible generate adversarial examples black-box scenario adversary know core recognition model parameters steps pipeline including preprocessing standardized many systems thus easy adversary reconstruct. untargeted attack case cause model predict sequence characters match ground truth however output intended human-understandable natural text untargeted attack produce sequence gibberish characters form valid text. characters valid text whose semantic meaning different ground truth. given input image ground truth sequence target sequence standard formulation optimization problem generate adversarial example lctc loss function sequential labeling detailed equation ||x− l-norm distance clean perturbed images constant balances terms loss function. constraint number pixels ensures adversarial example valid input tesseract xminxmax respectively. using change variables method reformulate minimization problem formulation adds variable satisﬁes constraint automatically optimization. differences attacking attacking classiﬁcation. generating targeted adversarial examples sequence labeling models differs several respects attacking standard image classiﬁcation models subject much prior research first output model varied-length sequence instead single label. successful targeted attack thus needs ensure output sequence matches target sequence exactly terms length label sequence. harder attacking standard image classiﬁcation task requires transforming single label produced model. second successful attack label given sequence work different context. example suppose adversarial example causes include misrecognized exclude changed letters letters perturbation letters work applied words e.g. cause internally misrecognized externally. nature recurrent neural networks used recognition model. internal feature representations depend context perturbation transfer contexts. best knowledge adversarial examples sequence labeling models previocr widely used tasks processing scanned documents data entry output intended understandable humans. targeted attack section easily modify data dates addresses numbers etc. serious impact documents invoices contracts. interesting attack takes advantage fact produces natural-language text. attack transforms meaning output text causing model misrecognize words. choosing target word pairs. languages english pairs words meaning visually close enough small adversarial perturbation sufﬁcient fool system recognizing image word word. simple attack help transform meaning text replace word antonym. create list word pairs experiments collected pairs antonyms wordnet dictionary distance words pair threshold. experiments threshold adaptively according number characters word. also make sure replaced word part speech original word. ensures attack introduce grammatic errors text output model. semantic ﬁltering. although replacing word antonym cause syntactic errors still cause semantic awkwardness transformed text. several reasons this. first english word multiple meanings thus simply replacing word antonyms many context. example changing they carelessly ﬁred barn. they carelessly hired barn. turns sentence nonsense. issue potentially addressed using language modeling. adversary check linguistic likelihood transformed text apply attack. example phrase hired barn score rare—although entirely absent—in english-language corpuses. document contains multiple sentences replacement word context whole document even meaning indeed opposite word replaced. modiﬁed sentence follow logic surrounding sentences. example attack change glad however context around sentence suggests positive feeling replacement look awkward. changing entire context feasible requires careful paraphrasing rather simply replacing individual words. checking semantic smoothness transformed document non-trivial task. suggested possible crowd-sourcing decide transformed document makes sense not. generating adversarial text images. given original text document ﬁrst render clean image. words text appear list antonym pairs locate lines clean image containing words transformed transform them keep transformations produce valid words pass semantic ﬁltering generate adversarial examples line images replace images corresponding lines document image. model recognize lines image correctly except modiﬁed lines. modiﬁed lines model output correct text words replaced antonyms. systems often used component bigger pipeline passes output applications operating natural-language text pipelines perfect target adversarial-image attacks output intended read checked human. therefore adversary need worry syntactic semantic correctness output long output desired effect application operates generating adversarial text models. provide simple greedy algorithm automatically generate given original text target text want system produce output. target text serve input classiﬁer class predicted different correct prediction simplicity assume binary classiﬁer score likely input classiﬁed correct class e.g. probability logistic regression model distance hyperplane svm. ﬁrst optimal replacement word select candidate replacement words edit distance word threshold restriction edit distance allows smaller perturbations generating adversarial text images. compute sort optimal word replacements descending order changes cause score. goal identify words inﬂuential changing prediction model. greedily modify replacing inﬂuential words optimal replacements repeat procedure meets model failure criterion approach easily generalizes multi-class models whose output k-dimensional vector number classes. want model incorrectly predict class modify algorithm select word replacements maximize keep rest algorithm unchanged. generating adversarial text images. ﬁrst render clean image based original text algorithm obtain adversarial text locate lines clean image texts needs modiﬁed. basic attack generate adversarial examples lines replace original lines generated images. data poisoning attacks. often used preprocessing step collecting data train modassume adversary access text images modify subset images. ﬁrst trains benign model based output clean images. uses algorithm generate adversarial texts subset training texts based generates adversarial images accordingly uses replace clean images. adversarial images look benign texts extracted model different original texts. ﬁnal model trained adversarial texts thus performance degrade. used latest tesseract version alpha employs deep learning model described table recognition. downloaded parameters tesseract’s recognition model loaded tensorﬂow implementation recognition model. implemented attack described section adam optimizer generated adversarial examples using tensorﬂow implementation evaluated directly applying tesseract. selected pairs antonyms wordnet meet threshold requirement edit distance. threshold adaptively according number characters word examples pairs list presence/absence superiority/inferiority disabling/enabling defense/offense ascend/descend. render words common fonts antonyms target output. number iterations gradient descent step size optimization constant objective function resulting adversarial images shown fig. perturbation minor output tesseract opposite word appearing image. table results attacking single words rendered different fonts clean accuracy tesseract clean images. target accuracy accuracy tesseract predicting target word adversarial images. rejected percentage adversarial images rejected tesseract large perturbation. average distance clean adversarial images. nized antonyms. amount perturbation measured distance similar different fonts. much perturbation applied image tesseract rejects input output anything. illustrate attack works images whole documents using documents publicly available corpus hillary clinton’s emails. changing data text. ﬁrst show attack used change data document. chose document corpus contains information rendered clean image. target output according type information care must taken choosing target values order preserve semantic consistency. example ground-truth text tuesday target thursday semantically valid week actual date match. fig. shows example successful attack changes recognized date time name information small perturbation document image. shows potential risk systems used data entry scanned images output structured natural language discrete pieces information. changing semantic meaning text. also change meaning document using antonym pairs word-level attacks. although antonym list short words frequently used. example hillary clinton’s email corpus words antonym list occur emails email contains average words list. show example successful attack fig. text output tesseract conveys meaning opposite original email. original email expresses idea u.s. increase forces nato-led operation allies. render adversarial text image cause tesseract output increase instead decrease positions. text recognized tesseract expresses idea u.s. decrease commitment allies increase theirs exact opposite meaning original email. illustrates meaning relatively long document ﬂipped well-chosen change words. demonstrate attack signiﬁcantly affect applications operate results applied text images. sentiment analysis. sentiment analysis binary classiﬁcation task determine whether input text contains positive negative sentiment. chose rotten tomatoes imdb movie review datasets evaluation. dataset train logistic regression classiﬁer bag-of-word features. imdb dataset train convolutional neural network word embedding features given input text models output conﬁdence score polarity sentiment input text. logistic regression model achieves accuracy rt’s test data model achieves accuracy imdb’s test data. algorithm generate adversarial text. dataset construct list valid replacements word vocabulary setting edit-distance threshold first show control polarity conﬁdence sentiment prediction varying number words replaced text. chose ﬁrst correctly classiﬁed texts test datasets imdb targets attack. model failure criterion conﬁdence score lower imdb. average proportion words need replace text corresponding conﬁdence score model prediction shown fig. dataset change score original label value replacing text average. imdb dataset many fewer words need replaced changing text cause score original label drop evaluate whether successfully carry attack image domain. select successfully attacked examples datasets render clean images based original texts. generate adversarial examples texts. balance constant number iterations adversarial images achieve target accuracy. result accuracy sentiment clastesseract output madam secretaryhope note ﬁnds family well. i’ve following activities last several months. incredible sure surprise one. going annual gathering folks coming exchange. know probably long shot best dinner group adams thursday would tremendous honor would come words. doesn’t work town love stop minutes hello. please know possible. look forward seeing soon. best jerry tesseract output behalf nato warmly welcome president obama’s announcement approach commitment mission afghanistan. president obama’s decision substantially decrease numbers forces natoled operation proof resolve; overall approach laid broader political strategy success. united states’ contribution nato-led mission always substantial; even important.but mission alone america’s allies nato shared risks costs burdens mission beginning. decreases commitment conﬁdent allies well partners mission also make substantial increase contribution. taken together force contributions across alliance well approach agreed isaf countries help create momentum mission siﬁer drops applied output adversarial images average distance rt’s adversarial examples changed word. imdb adversarial images achieve target accuracy sentiment classiﬁer’s accuracy drops ocr-recognized text. note adversarial images need perfectly recognized targets adversary long output fools sentiment classiﬁer. average distance imdb’s adversarial examples changed word. document categorization. show attack generalize multi-class document categorization task. evaluate attack newsgroup dataset task categorize news documents topics. original dataset contains classes select subset classes experiment. train one-vs-all logistic regression classiﬁer bag-of-word features target model attack. model achieves accuracy test dataset. chose ﬁrst correctly classiﬁed texts test dataset attack. similar sentiment analysis experiment built list valid word replacement word vocabulary edit distance below text generated adversarial texts change model’s output original figure adversarial rendering movie review rotten tomatoes. sentiment analysis model predicts positive score original text tesseract’s output adversarial image review. class three classes. results attack shown table percentage modiﬁed texts classiﬁed adversary-chosen target high source classes target classes. proportion words need changed depends original target classes. example transforming class classes requires changing words transformations. next select successfully attacked examples generating adversarial images. render clean images based original texts generate adversarial examples setting balance constant number iterations resulting adversarial images cause tesseract output desired adversarial texts accuracy. texts classiﬁed adversary-speciﬁed target class. average distance adversarial examples changed word. data poisoning. described section model trained ocr-processed images adversarial images poison training dataset. evaluate attack rotten tomatoes dataset logistic retable class transformation accuracy newsgroup dataset. classes atheism religion graphics space respectively. entry column table means that average fraction words text needs changed texts class misclassiﬁed model class accuracy ﬁrst train benign model original training data. generate adversarial versions subset training texts. model failure criterion conﬁdence score generate texts conﬁdently misclassiﬁed retrain model poisoned dataset adversarial texts substituted corresponding original texts. results shown fig. different fractions training data replaced adversarial texts. select adversarial texts targets adversarial image rendering. balancing factor proportion words changed text much larger previous experiments. number iterations tesseract outputs desired adversarial texts accuracy. average distance adversarial examples changed word. furthermore generating transferable adversarial examples targeted attacks much harder untargeted attacks whether possible achieve transferability targeted adversarial images across models important topic future work. physical realizability. recent work demonstrated robust adversarial examples feasibly realized physical world digital images examples generated taking account physical-world conditions lighting scaling angle view etc. similar techniques help generate physically realizable adversarial images work tesseract. different levels scaling transformations adversarial image optimization also optimize loss small value model conﬁdent predicting target word. print resulting adversarial image paper scan back digital format submit scanned image tesseract. fig. shows successful example. distance example times larger digital adversarial example fig. indeed image much noisier visually. tesseract rejects input images perceives having quality. presents signiﬁcant obstacle physical realizability physical realizability requires large perturbations make scanned image noisy tesseract. fig. shows relationship amount perturbation tesseract’s rejection rate calculated images individual words. perturbations become signiﬁcant rejection rate increases. largely foils existing approaches generating physical adversarial examples although fig. shows examples succeed. recent research aimed increase robustness models adversarial inputs. best knowledge universal technique guaranteed defend image analysis models attacks figure proportion poisoned training texts classiﬁcation accuracy test data. baseline performance model trained clean training data poisoned performance model trained contaminated training data. consequently adversarial image word misrecognized tesseract particular document cannot simply pasted image another document. even word adversarial images must rendered separately document. reason adversarial images individual letters transfer word word. transferability across models. basic image classiﬁcation previous work demonstrated adversarial examples generated model also fool models. shows that principle adversarial examples work black-box setting. systems signiﬁcantly complex. employ multi-step image processing destroys modiﬁes many features input images. achieving transferability much harder setting. character-based models gocr legacy version tesseract adversarial examples transfer input processing pipeline different end-to-end models focus paper. particular segment inputs sequence characterlevel images machine learning model based recurrent neural network. adversarial examples transfer endto-end models either apply different preprocessing input images. example ocropus also uses recurrent neural networks core recognition model preprocessing includes binarizing pixel image truncates -bit values thus destroys almost entire adverfurthermore fig. shows attack target numbers dates data affect semantics overall text. attacks difﬁcult detect using language processing techniques. adversarial examples computer vision. recent research shown deep learning models vulnerable adversarial examples small change input image causes model produce different output. prior work focused mainly image classiﬁcation tasks input ﬁxed-size image output class label. carlini wagner demonstrated attack improves prior state terms amount perturbation success rate method generating adversarial examples designed classiﬁcation problems cannot directly applied models. houdini approach based family loss functions generate adversarial examples structured prediction problems including semantic segmentation sequence labeling. houdini tailored minimizing performance model opposed constructing targeted examples ideal targeted attacks trick model outputting speciﬁc text chosen adversary. adversarial examples demonstrated computer vision tasks semantic segmentation object detection visual question answering game playing approaches based model-speciﬁc task-speciﬁc loss functions generating adversarial examples cannot directly applied models. adversarial examples nlp. like computer vision models vulnerable adversarial perturbations image domain models vulnerable adversarial perturbations text domain. adversary substitute words input text fool many text classiﬁcation model approaches require careful word-level substitution limits performance power attack. generating adversarial natural text harder generating adversarial images. discrete nature text word carries much semantic meaning pixel image. difﬁcult design attack modiﬁes words would noticed human. models adversarial training training dataset augmented adversarial examples. contrast standard image classiﬁcation tasks whose purpose label images relatively small ﬁnite number classes recognize presence relatively small number object types potential outputs system include possible character strings complicating search comprehensive adversarial examples include training. assumptions adversarial examples common image classiﬁcation literature hold models. example often assumed images close must belong class natural-language context however visual similarity words imply anything semantic proximity. images word different pixel-wise similar images depict words semantically away other. furthermore models tesseract tolerate little noise inputs therefore small perturbation original image cause model reject else correctly output different sequence. investigated paper change individual words automatically guarantee semantic consistency output whole. adversary replaces certain words resulting text appear unnatural logically inconsistent. theory semantic checks output system help detect attacks relying humans perform check—determine output meaningful compare input image—would defeat purpose ocr. aware automated system check whether text produced makes sense. output resulting attacks gibberish. overall reads like normal english text occasional awkwardness inconsistency. therefore system checking semantic integrity text would need sensitive individual words appearing context. needed. still generate targeted attacks model cause output speciﬁc character strings many scenarios need ensure strings syntactically semantically correct long desired effect model consumes them. signiﬁcantly increases power adversarial examples. whereas prior work concerned classiﬁcation error model attack gives attacker full control model’s predictions conﬁdence. recent work also shown small modiﬁcations words adding random characters introducing typos degrade performance models tasks classiﬁcation machine translation modiﬁcations easily integrated attack core idea produce visually similar words human ignore. adversarial examples speech recognition. speech recognition similar sense aims assign sequence character labels input prior work shown generate mangled unintelligible even inaudible audio inputs nevertheless recognized commands speech speech recognition systems contrast generate incomprehensible inputs. goal generate images visual appearance human-understandable text recognized different attacker-speciﬁed text. recent results adversarial examples speech recognition include targeted attacks close clean audio inputs case attacker target desired output; case targets limited words somewhat visually similar original clean image. distinction speech recognition models optical recognition models former explicitly designed work noisy environments therefore speech-to-text models accept attempt transcribe sound recordings minor squeaks noises affect human perception contrast tesseract rejects inputs relatively minor perturbations—see example fig. greatly limits space feasible adversarial examples models. furthermore alphabet label sequences produced speech recognition models size corresponding english characters. alphabet tesseract’s outputs includes upperlower-case english characters numbers special symbols. larger alphabets make targeted attacks harder. demonstrated systems based deep learning vulnerable targeted adversarial examples. mimodiﬁcations images printed text cause system recognize word image semantic opposite chosen adversary. enables adversary craft adversarial documents whose meaning changes pass ocr. attack also signiﬁcant impact applications preprocessing step enabling adversary control output reported conﬁdence. best knowledge ﬁrst instance adversarial examples sequence labeling models image domain. adversarial examples paper developed latest version tesseract popular open-source system based deep learning. transfer legacy version tesseract employs character-based recognition. transferability adversarial images across different types models open problem. physical realizability i.e. whether possible create physical documents whose meaning changes scanned processed interesting topic future research. section demonstrated adversarial examples physically realizable. general however image perturbations sufﬁciently large survive scanning exceed amount noise tesseract tolerate input images. remains open question develop adversarial perturbations printed natural text small enough affect single word signiﬁcantly change appearance word human reader large enough preserved image scanned commodity scanner cause system output different word chosen adversary. acknowledgements. thanks ristenpart suggesting systems vulnerable adversarial examples jasmine kitahara experiments adversarial images digits. work partially supported grant schmidt sciences.", "year": 2018}