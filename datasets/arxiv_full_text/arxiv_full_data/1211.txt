{"title": "Visual7W: Grounded Question Answering in Images", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.", "text": "figure deep image understanding relies detailed knowledge different image parts. employ diverse questions acquire detailed information images ground objects mentioned text visual appearances provide multiple-choice setting evaluating visual question answering task textual visual answers. suggest tighter semantic link textual descriptions corresponding visual regions ingredient better models. fig. shows localization objects critical step understand images better solve image-related questions. providing image-text correspondences called grounding. inspired geman al.’s prototype visual turing test based image regions comprehensive data collection pairs coco images baidu fuse visual grounding order create dataset dense annotations ﬂexible evaluation environment. object-level grounding provides stronger link pairs images global image-level associations. furthermore allows resolve coreference ambiguity understand object distributions enables visually grounded answers consist object bounding boxes. motivated goal developing model visual based grounded regions paper introduces dataset extends previous approaches proposes attention-based model perform task. collected pairs coco images together human-generated multiple-choices object groundings categories. data collection inspired age-old idea seen great progress basic perceptual tasks object recognition detection. however models still fail match humans high-level vision tasks lack capacities deeper reasoning. recently task visual question answering proposed evaluate model’s capacity deep image understanding. previous works established loose global association sentences images. however many questions answers practice relate local regions images. establish semantic link between textual descriptions image regions object-level grounding. enables type visual answers addition textual answers used previous work. study visual tasks grounded setting large collection multiple-choice pairs. furthermore evaluate human performance several baseline models tasks. finally propose novel lstm model spatial attention tackle tasks. recent development deep learning technologies achieved successes many perceptual visual tasks object recognition image classiﬁcation pose estimation status computer vision still matching human capabilities especially comes understanding image details. recently visual question answering proposed proxy task evaluating vision system’s capacity deeper image understanding. several datasets released since last year. contributed valuable data training visual systems introduced various tasks picking correct multiple-choice answers ﬁlling blanks pioneer work image captioning sentence-based image retrieval visual shows promising results. works aimed establishing global association sentences images. however flickrk visual madlibs questions journalism describe complete story questions roughly correspond array standard vision tasks visualw dataset features richer questions longer answers addition provide complete grounding annotations link object mentions sentences bounding boxes images therefore introduce type image regions visually grounded answers. refer questions textual answers telling questions visual answers pointing questions provide detailed comparison data analysis sec. salient property dataset notable human performance state-of-the-art lstm models visual tasks. spatial attention mechanism lstm architecture tackling visually grounded tasks textual visual answers model aims capture intuition answers image-related questions usually correspond speciﬁc image regions. learns attend pertinent regions reads question tokens sequence. achieve state-of-the-art performance correlations model’s attention heat maps object groundings large performance human machine envision dataset visually grounded tasks contribute long-term joint effort several communities vision natural language processing knowledge close together. visualw dataset constitutes part visual genome project visual genome contains million pairs question types offers largest visual collection date training models. pairs visualw subset million pairs visual genome. moreover visualw includes extra annotations object groundings multiple choices human experiments making clean complete benchmark evaluation analysis. vision language. years effort connecting visual textual information joint learning image video captioning become popular task past year goal generate text snippets describe images regions instead predicting labels. visual question answering natural extension captioning tasks interactive stronger connection real-world applications text-based question answering. question answering well-established problem. successful applications seen voice assistants mobile devices search engines game shows traditional question answering system relies elaborate pipeline models involving natural language parsing knowledge base querying answer generation recent neural network models attempt learn end-to-end directly questions answers visual question answering. geman proposed restricted visual turing test evaluate visual understanding. daquar dataset ﬁrst toy-sized benchmark built upon indoor scene rgb-d images. datasets collected pairs microsoft coco images either generated automatically tools written human workers following datasets array models proposed tackle visual tasks. proposed models range probabilistic inference recurrent neural networks convolutional networks previous visual datasets evaluate textual answers images omitting links object mentions visual appearances. inspired geman establish link grounding objects images perform experiments grounded setting. elaborate details data collection conducted upon images coco leverage questions systematically examine model’s capability visual understanding append question category. extends existing visual setups accommodate visual answers. standardize visual tasks multi-modal answers multiple-choice format. question comes four answer candidates correct answer. addition ground objects mentioned pairs corresponding bounding boxes images. object-level groundings enable examining object distributions resolve coreference ambiguity data collection tasks conducted amazon mechanical turk online crowdsourcing platform. online workers asked write pairs question answer based image content. instruct workers concise unambiguous avoid wordy speculative questions. obtain clean high-quality pairs three workers label pair good independently. workers judge pair whether average person able tell answer seeing figure examples multiple-choice question categories. ﬁrst shows telling questions green answer ground-truth ones human-generated wrong answers. what questions often pertain recognition tasks spatial reasoning. where questions usually involve high-level common sense reasoning. second depicts pointing questions yellow correct answer boxes human-generated wrong answers. four answers form multiple-choice test question. relied human workers automatic methods generate pool candidate answers. human-generated answers produce best quality; contrary automatic methods prone introducing candidate answers paraphrasing ground-truth answers. telling questions human workers write three plausible answers question without seeing image. ensure uniqueness correct answers provide ground-truth answers workers instruct write answers different meanings. pointing questions workers draw three bounding boxes objects image ensuring boxes cannot taken correct answer. provide examples categories fig. collect object-level groundings linking object mentions pairs bounding boxes images. workers extract object mentions pairs draw boxes images. collect additional groundings multiple choices pointing questions. duplicate boxes removed based object names intersection-over-union threshold figure coreference ambiguity arises object mention multiple correspondences image textual context insufﬁcient tell apart. answer left question either gray yellow black depending meant. right example generic phrase refer buses image. thus algorithm might answer correctly even referring wrong bus. beneﬁts object-level groundings three-fold resolves coreference ambiguity problem sentences images; extends existing visual setups accommodate visual answers; offers means understand distribution objects shedding light essential knowledge acquired tackling tasks plausible answers test time thus complicating evaluation. online study shows that ambiguity occurs accepted questions accepted answers. illustrates drawback existing visual setups absence objectlevel groundings textual questions answers loosely coupled images. section analyze visualw dataset collected coco images present features provide comparisons dataset previous work. summarize important metrics existing visual datasets table advantages grounding unique feature visualw dataset grounding annotations textually mentioned objects total collected object groundings enables type visual answers form bounding boxes examining object distribution pairs sheds light focus questions essential knowledge acquired answering them. object groundings spread across categories thereby exhibiting long tail pattern categories fewer instances open-vocabulary annotations objects contrast traditional image datasets focusing predeﬁned categories salient objects provide broad coverage objects images. human-machine performance expect good benchmark exhibit sufﬁcient performance humans state-of-the-art models leaving room future research explore. additionally nearly perfect human performance desired certify quality questions. visualw conducted experiments measure human performance well examining percentage questions answered without images. results show strong human performance strong interdependency images pairs. provide report statistics dataset real images visual madlibs ﬁltered hard tasks. ﬁll-in-the-blank tasks visual madlibs answers sentence fragments differ tasks resulting distinct statistics. omit statistics baidu partial release. figure object distribution telling pointing rank object category based frequency rank referring frequent one. pointing pairs cover order magnitude objects telling pairs. object categories indicate object distribution’s bias towards persons daily-life objects natural entities. table compares visualw facebook babi reported model human performances. facebook babi textual dataset claiming humans potentially achieve accuracy without explicit experimental proof. numbers reported multiple-choice open-ended evaluation setups. visualw features largest performance desirable property challenging long-lasting evaluation task. time nearly perfect human performance proves high quality questions. diversity diversity pairs important feature good dataset reﬂects broad coverage image details introduces complexity potentially requires broad range skills solving questions. obtain diverse pairs decided rule binary questions contrasting geman al.’s proposal vqa’s approach hypothesize encourages workers write complex questions also prevents inﬂating answer baselines simple yes/no answers. lstm models achieved state-of-the-art results several sequence processing tasks also used tackle visual tasks models represent images global features lacking mechanism understand local image regions. spatial attention standard lstm model visual illustrated fig. consider two-stage process encoding stage model memorizes image question hidden state vector decoding stage model selects answer multiple choices based memory encoder structure visual tasks different decoders telling pointing tasks. given image question learn embeddings image word tokens follow transforms image pixel space -dimensional feature representation. extract activations last fully connected layer pre-trained model vgg- transforms word token one-hot representation indicator column vector single index token word vocabulary. matrix transforms -dimensional image features di-dimensional embedding space transforms one-hot vectors dw-dimensional embedding space value thus take image ﬁrst input token. embedding vectors v...m lstm model one. update rules lstm model deﬁned follow sigmoid function tanh function element-wise multiplication operator. attention mechanism introduced term weighted average convolutional features depends upon previous hidden state convolutional features. exact formulation follows figure diagram recurrent neural network model pointing encoding stage model reads image question tokens word word. word computes attention terms based previous hidden state convolutional feature deciding regions focus decoding stage computes log-likelihood answer product transformed visual feature last lstm hidden state. examining richness pairs length questions answers rough indicator amount information complexity contain. overall average question answer lengths words respectively. pointing questions longest average question length. telling questions exhibit long-tail distribution answers three words respectively. many answers questions phrases sentences average words. general dataset features long answers questions answers words contrast answers answers daquar answers coco-qa single word. also capture long-tail answers frequent answers account answers finally provide human created multiplechoices evaluation visual tasks visually grounded local image regions pertinent answering questions many cases. instance ﬁrst pointing example fig. regions window pillows reveal answer regions irrelevant question. capture intuition introducing spatial attention mechanism similar model image captioning model performances. test time input image natural language question followed four telling multiple choices multiple choices. written natural language; whereas pointing multiple choice corresponds image region. model correct question picks correct answer among candidates. accuracy used measure performance. alternative method evaluate telling model predict open-ended text outputs approach works well short answers; however performs poorly long answers many ways paraphrasing meaning. make training validation test splits pairs respectively. numbers reported test set. experiments human experiments evaluate human performances multiple-choice want measure experiments well humans perform visual task whether humans common sense answer questions without seeing images. conduct sets human experiments. ﬁrst experiment group workers asked guess best possible answers multiple choices without seeing images. second experiment different group workers answer questions given images. ﬁrst block table reports human performances experiments. measure mean accuracy pairs take majority votes among human responses. even without images humans manage guess plausible answers cases. human subjects achieve accuracy higher chance. human performance without images remarkably high questions indicating many questions encode fair amount common sense humans able infer without visual cue. however images important majority questions. human performance signiﬁcantly improved images provided. overall humans achieve high accuracy tasks. fig. shows plots response time human subjects telling human subjects spend double time respond images displayed. addition questions take longer average response time compared question types. human subjects spend average seconds pointing questions. however experiment conducted different user interface workers click answer boxes image. thus response time comparable telling tasks. interestingly longer response time imply higher performance. human subjects spend figure response time human subjects telling tasks. boxes ﬁrst quartile third quartile response time values. bars centers boxes indicate median response time category. returns -dimensional convolutional feature maps image fourth convolutional layer vgg- model attention term -dimensional unit vector deciding contribution convolutional feature t-th step. standard lstm model considered special case element uniformly. lstm model attention terms learnable parameters. learning inference model ﬁrst reads image question tokens reaching question mark training telling continue feed ground-truth answer tokens model. pointing compute log-likelihood candidate region product transformed visual feature last lstm hidden state crossentropy loss train model parameters backpropagation. testing select candidate answer largest log-likelihood. hyperparameters using validation set. dimensions lstm gates memory cells experiments. model trained adam update rule mini-batch size global learning rate evaluate human model performances tasks. report reasonably challenging performance delta leaving sufﬁcient room future research explore. experiment setups figure qualitative results human subjects state-of-the-art model multiple-choice qas. illustrate prediction results multiple-choice without images. green answer corresponds correct answer question rest three wrong answer candidates. take majority votes human subjects human predictions predictions model correct predictions indicated check marks. time questions lower accuracy. pearson correlation coefﬁcient average response time average accuracy indicating weak negative correlation response time human accuracy. examined human performance next question well state-of-the-art models perform task. evaluate automatic models tasks three sets experiments without images without questions images experiments without images zero image features. brieﬂy describe three models used experiments logistic regression logistic regression model predicts answer concatenation image feature question feature. questions represented dimensional averaged word embeddings pre-trained model telling take top- frequent answers class labels. test time select top-scoring answer candidate. pointing perform k-means cluster training regions features clusters used class labels. test time select answer candidate closest centroid predicted cluster. report results table baseline models surpass chance performance logistic regression baseline yields best performance question features provided. global image features hurts performance indicating importance understanding local image regions rather holistic representation. interestingly lstm performance signiﬁcantly outperforms human perforfigure object groundings attention heat maps. visualize attention heat maps images. brighter regions indicate larger attention terms i.e. model focuses. bounding boxes show object-level groundings objects mentioned answers. mance images present. human subjects trained answering questions; however lstm model manages learn priors answers training set. addition questions image content contribute better results. question image baseline shows large improvement overall accuracy ones either question image absent. finally attention-based lstm model outperforms baselines question types except category achieving best model performance show qualitative results human experiments lstm models telling task fig. human subjects fail tell sheep apart goat last example whereas lstm model gives correct answer. humans successfully answer fourth question seeing image lstm model fails cases. object groundings help analyzing behavior attention-based model. first examine model focuses visualizing attention terms attention terms vary model reads words one. perform pooling along time maximum attention weight image grid producing attention heat map. model attends mentioned objects. answer object boxes occupy average image area; peak attention heat resides answer object boxes time. indicates tendency model attend answer-related regions. visualize attenfigure impact object category frequency model accuracy pointing task. x-axis shows upper bound object category frequency bin. y-axis shows mean accuracy within bin. accuracy increases gradually model sees instances category. meanwhile model manages handle infrequent categories transferring knowledge larger categories. tion heat maps example pairs fig. examples show pairs answers containing object. peaks attention heat maps reside bounding boxes target objects. bottom examples show pairs answers containing object. attention heat maps scattered around image grid. instance model attends four corners borders image look carrots fig. furthermore object groundings examine model’s behavior pointing fig. shows impact object category frequency accuracy. divide object categories different bins based frequencies training set. compute mean accuracy test pairs within bin. observe increased accuracy categories object instances. however model able transfer knowledge common categories rare ones generating adequate performance object categories instances. paper propose leverage visually grounded questions facilitate deeper understanding images beyond recognizing objects. previous visual works lack tight semantic link textual descriptions image regions. link object mentions bounding boxes images. object grounding allows resolve coreference ambiguity understand object distributions evaluate type visually grounded propose attention-based lstm model achieve state-of-the-art performance tasks. future research directions include exploring ways utilizing common sense knowledge improve model’s performance tasks require complex reasoning. acknowledgements would like thank carsten rother dresden university technology establishing collaboration computer vision dresden stanford vision enabled oliver groth visit stanford contribute work. would also like thank olga russakovsky lamberto ballan justin johnson anonymous reviewers useful comments. research partially supported yahoo labs macro award muri award.", "year": 2015}