{"title": "Learning Optimal Policies from Observational Data", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Choosing optimal (or at least better) policies is an important problem in domains from medicine to education to finance and many others. One approach to this problem is through controlled experiments/trials - but controlled experiments are expensive. Hence it is important to choose the best policies on the basis of observational data. This presents two difficult challenges: (i) missing counterfactuals, and (ii) selection bias. This paper presents theoretical bounds on estimation errors of counterfactuals from observational data by making connections to domain adaptation theory. It also presents a principled way of choosing optimal policies using domain adversarial neural networks. We illustrate the effectiveness of domain adversarial training together with various features of our algorithm on a semi-synthetic breast cancer dataset and a supervised UCI dataset (Statlog).", "text": "choosing optimal policies important problem domains medicine education ﬁnance many others. approach problem controlled experiments/trials controlled experiments expensive. hence important choose best policies basis observational data. presents difﬁcult challenges missing counterfactuals selection bias. paper presents theoretical bounds estimation errors counterfactuals observational data making connections domain adaptation theory. also presents principled choosing optimal policies using domain adversarial neural networks. illustrate effectiveness domain adversarial training together various features algorithm semi-synthetic breast cancer dataset supervised dataset choice particular policy plan action involves consideration costs beneﬁts policy/plan consideration also alternative policies/plans might undertaken. examples abound; mention course treatment lead rapid recovery? mode advertisement lead orders? investment strategy lead greatest returns? obtaining information costs beneﬁts alternative plans might undertaken counterfactual exercise. possible estimate counterfactual information conducting controlled experiments. however controlled experiments expensive involve small samples frequently *equal contribution department electrical computer engineering university california angeles department economics university california angeles department engineering university oxford. correspondence onur atan <oatanucla.edu>. available. therefore important make decisions entirely basis observational data actions/decisions taken data selected existing logging policy. existing logging policy creates selection bias learning observational studies challenging problem. paper presents theoretical bounds estimation errors evaluation policy observational data principled algorithm learn optimal policy. methods algorithms develop widely applicable enormous range settings healthcare education recommender systems ﬁnance smart cities. examples.) noted algorithm applies many settings. medical context features information included electronic health records actions choices different treatments outcomes success treatment. ﬁnancial context features aspects macroeconomic environment actions choices different investment decisions outcomes revenues made investment decisions. recommender system context features information user actions choices items outcomes binary values indicating whether user purchased item not. theoretical results show true policy outcome least good policy outcome estimated observational data minus product number actions h-divergence observational randomized data. theoretical bounds different ones derived require propensity scores known. theory develop algorithm learn balanced representations instance indistinguishable randomized observational distribution also predictive decision problem hand. present experiments semi-synthetic breast cancer supervised statlog data show algorithm out-performs various methods explain why. policy optimization work ite’s aims estimate expected difference outcomes treated control patients given feature vector; work focuses settings actions notes approaches derived generalize well settings actions. work policy optimization aims policy maximizes expected outcome policy optimization objective somewhat easier objective sense turn action recommendations around. many applications much actions; interested learning good action rather learning outcomes action instance. work estimation closely related focuses learning balanced representations papers develop neural network algorithms minimize mean squared error predictions actual outcomes observational data also discrepancy representations factual counterfactual data. papers note principled approach extend treatments. recent works estimation include tree-based methods gaussian processes last perhaps successful computational complexity easy apply large observational studies. policy optimization literature work closely related develop counterfactual risk minimization principle. objective principle minimize estimated mean variance inverse propensity score instances; authors propose poem algorithm. work differs poem several ways poem minimizes objective class linear policies; allow arbitrary policies poem requires propensity scores available data; algorithm addresses selection bias without using propensity scores poem addresses selection bias re-weighting instance inverse propensities; algorithm addresses selection bias learning representations. another related paper policy optimization requires propensity scores known addresses selection bias rejection sampling. off-policy evaluation methods include estimator self normalizing estimator direct estimation doubly robust estimator matching based methods self-normalizing estimators address selection bias re-weighting instance inverse propensities. doubly robust estimation techniques combine direct methods generate robust counterfactual estimates. propensity score matching replaces missing counterfactual outcomes instance outcome instance closest propensity score. theoretical bounds strong connection domain adaptation bounds given particular show expected policy outcome bounded estimate policy outcome observational data minus product number actions h-divergence observational randomized data. algorithm based domain adaptation domain adaptation techniques include observational data denote actions s-dimensional space features space outcomes. assume outcome identiﬁed real number normalize outcomes interval cases outcome either cases outcome interpreted probability success failure. follow potential outcome model described rubin-neyman causal model instance k-potential outcomes corresponding different actions. fundamental problem setting outcome action actually performed recorded data work focus setting action assignment independent feature vector i.e. action assignments random. dependence modeled conditional distribution also known propensity score. section provide criterion learn policy maximizes outcome. handle selection bias dataset mapping features representations relevant policy outcomes less biased. denote representation function maps features representations. representation function induces distribution representations follows d-measurable. probability event according probability inverse image event according learning setting deﬁned choice representation function hypothesis class policies. connect problem domain adaptation. recall source distribution generated featureaction samples observational data. deﬁne target distribution note represents observational study actions randomized represents clinical study actions randomized. denote source target distributions induced representation function space respectively. denote marginal distribution representations write induced policy outcome ez∼dφ remainder theoretical analysis suppose representation function ﬁxed. missing counterfactual outcomes addressed importance sampling. denote expected policy outcome respect distributions respectively. given lemma shows true policy outcome least good policy outcome observational data minus product number actions times hdivergence observational randomized data. create monte carlo estimator policy outcome source data lower bound established best action recommendation policy. deﬁnition representation function monte-carlo estimator policy outcome source data given order provide uniform bounds monte-carlo estimator inﬁnitely large class recommendation functions need ﬁrst deﬁne complexity term class policy class integer growth function deﬁned z∈zn possible representations number cardinality smallest contained union \u0001-balls centered points metric induced following result provides inequality estimated true lemma then probability order provide data dependent bound estidata-dependent bounds h-divergence source target distributions. however aren’t given samples target data need generate tari= denote empirical distribution source data. empirical source distribution generate target data simply samobservational data. first we’ll connect provide theoretical bounds based distance source target distribution. proposition ﬁxed representation function. then create monte-carlo estimator since don’t samples target distribution samples source distribution. hence we’ll domain adaptation theory bound difference terms h-divergence. order that ﬁrst need introduce distance metric distributions. policy denote characteristic contains representation-action pairs mapped label function i.e. deﬁnition suppose probability distributions every characteristic measurable respect distributions. then -divergence distributions h-divergence measures difference behavior policies examples drawn plays important role theoretical bounds. next lemma establish bound difference based h-divergence tween source target. lemma representation function. theorem motivates general framework designing policy learning observational data bandit feedback. learning algorithm following criterion solves trade-off parameter empirical policy outcome source data empirical h-divergence source target distributions. optimization criterion seeks representation function source target domain indistinguishable. computing empirical h-divergence source target distributions known np-hard recent developments domain adversarial neural networks good approximation. testing stage convert probabilities action recommendations simply recommending action highest probability qia. domain loss standard cross entropy loss estimated domain probability actual domain probability standard classiﬁcation loss used literature given training procedure domain adverse training counterfactual policy training depicted algorithm neural network architecture depicted figure test instance covariates compute action recommendations following procedure ﬁrst compute representations compute action probabilities ﬁnally recommend action maxa∈a this need samples observed data sometimes referred source data unlabeled samples ideal dataset referred target data mentioned don’t samples ideal dataset. hence we’ll ﬁrst talk batch sampling source target dataset given batch size randomly sample domain variable indicating source data. then sample additional samples excluding samples source data randomly assign action according distribution multinomial); ﬁnally domain variable indicating target data. batch generation procedure depicted algorithm algorithm consists three blocks representation domain policy blocks. representation block seek combining objectives high predictive power outcomes predictive power domain. denote parametric function maps patient features representations parameter vector representation block. representations input survival policy blocks. denote mappings representation-action pair probabilities actions i.e. parameter vector policy block. instance features action element output policy block ˆqia probability recommending action subject estimated policy outcome source although theory applies deterministic policies allow stochastic policies order make optimization problem tractable. optimal; however we’ll show numerical results approach still able achieve signiﬁcant gains respect benchmark algorithms. mapping representation-action pair probability instances generated target i.e. parameters describe performance algorithm. note difﬁcult test validate algorithm real data missing counterfactual survival outcomes. paper provide results semi-synthetic breast cancer supervised dataset breast cancer dataset dataset includes records breast cancer patients participating national surgical adjuvant breast bowel project instance consists following information patient menopausal race estrogen receptor progesterone receptor human epidermal growth factor receptor tumor stage tumor grade positive axillary lymph node count score surgery type prior chemotherapy prior radiotherapy histology. treatment choice among chemotherapy regimens used cmf. outcomes regimens derived based references pubmed clinical queries. data contains feature vector derived outcomes treatment {yt}t∈t statlog dataset dataset includes multispectral values pixels satellite image. feature vector contains pixel values predict true description plot feature vector. follow procedure summarized treat label action outcome action matches label rest experimental setup generate artiﬁcially biased dataset following procedure ﬁrst draw random weights rs×k parameter used generate datasets different selection bias levels. generate actions data according logistic distribution testing provided evaluate algorithm. hyperparameter list used validation generate different datasets following procedure described report average conﬁdence levels. performance metric evaluate algorithm paper loss deﬁne accuracy; accuracy deﬁned fraction test instances recommended best action match. note evaluate accuracy metric since ground truth outcomes testing course ground truth outcomes used algorithm training validation test. experiments representation/domain/outcome fully-connected layers. neural network trained back propagation adam optimizer initial learning rate begin initial learning rate tradeoff parameter iterative adaptive parameters result; along decrease learning rate increases tradeoff parameter. standard procedure training domain adversarial neural networks implement dacpol tensorﬂow environment. poem deal selection bias data using propensity scores. note dacpol require propensity scores known order address selection bias. hence order make fair comparisons estimate propensity scores data estimates poem. table shows discriminative performance dacpol dacpol benchmark algorithms. compare algorithms breast cancer data statlog data seen table algorithm outperforms benchmark algorithms terms loss metric deﬁned above. empirical gain respect poem algorithm three sources dacpol need propensity scores dacpol optimizes policies linear policies dacpol trades hyperparameter controls domain loss training procedure. increases domain loss training dacpol increases; eventually source target become indistinguishable representations become balanced loss dacpol reaches minimum. increase beyond point algorithm classiﬁes source target target source representations become unbalanced loss dacpol increases again. figure illustrates effect breast cancer dataset. outcome access randomized data. however performance principle decreases additional irrelevant features inverse propensities irrelevant features become large hence variance estimator also become large. this example. begin relevant features generate additional irrelevant features create logging policy depends irrelevant features using logistic distribution. increases selection bias also increases. figure shows poem sensitive dacpol increase selection bias. subsection show effect selection bias performance algorithm varying parameter data generation process larger value creates biased data. figure shows important points selection bias increases loss dacpol increases selection bias increases domain adversarial training becomes efﬁcient hence improvement dacpol dacpol increases. subsection show advantage principle principle selection bias irrelevant features less important. happens representation optimization able remove effect irrelevant features outputted representations uses relevant features directly estimate policy conclusion paper presented estimation bounds error actual estimated policy outcomes observational data. theoretical results show estimation error observational data depends h-divergence between observational randomized data. result motivated development domain adversarial neural network learn optimal policy observational data. illustrated various features algorithm semisynthetic real data. future work includes multi-stage actions time-varying features etc. references alaa ahmed schaar mihaela. bayesian inference individualized treatment effects using multitask gaussian processes. advances neural information processing systems ben-david shai blitzer john crammer koby pereira fernando. analysis representations domain adapadvances neural information processing tation. systems beygelzimer alina langford john. offset tree learning partial labels. proceedings sigkdd international conference knowledge discovery data mining blitzer john crammer koby kulesza alex pereira fernando wortman jennifer. learning bounds domain adaptation. advances neural information processing systems bottou l´eon peters jonas candela joaquin quinonero charles denis xavier chickering portugaly elon dipankar simard patrice snelson counterfactual reasoning learning systems example computational advertising. journal machine learning research ganin yaroslav ustinova evgeniya ajakan hana germain pascal larochelle hugo laviolette franc¸ois marchand mario lempitsky victor. domainadversarial training neural networks. journal machine learning research hoiles william schaar mihaela. bounded off-policy evaluation missing data course recommendation curriculum design bounded off-policy evaluation missing data course recommendation curriculum design. international conference machine learning shalit johansson fredrik sontag david. estimating individual treatment effect generalization bounds algorithms. international conference machine learning swaminathan adith joachims thorsten. counterfactual risk minimization learning logged bandit feedback. proceedings international conference machine learning zhang sch¨olkopf bernhard muandet krikamol wang zhikun. domain adaptation target conditional shift. international conference machine learning", "year": 2018}