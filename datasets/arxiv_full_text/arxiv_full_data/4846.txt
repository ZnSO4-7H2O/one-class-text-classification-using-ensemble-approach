{"title": "e-Commerce product classification: our participation at cDiscount 2015  challenge", "tag": ["cs.LG", "cs.AI"], "abstract": "This report describes our participation in the cDiscount 2015 challenge where the goal was to classify product items in a predefined taxonomy of products. Our best submission yielded an accuracy score of 64.20\\% in the private part of the leaderboard and we were ranked 10th out of 175 participating teams. We followed a text classification approach employing mainly linear models. The final solution was a weighted voting system which combined a variety of trained models.", "text": "report describes participation cdiscount challenge goal classify product items predeﬁned taxonomy products. best submission yielded accuracy score private part leaderboard ranked participating teams. followed text classiﬁcation approach employing mainly linear models. ﬁnal solution weighted voting system combined variety trained models. report present participation classiﬁcacdicount organised tion challenge www.datascience.net platform. organisers provided large collection product items containing mainly textual description. followed text classiﬁcation approach using mainly linear models base models. ﬁnal solution consisted weighted voting system combined variety base models. section provide brief description cdiscount task data. section describes implementations results obtained. finally section concludes discussion lessons learnt. item categorization fundamental many aspects item life cycle e-commerce sites search recommendation catalog building etc. formulated supervised classiﬁcation problem categories target classes features words composing textual description items. context cdiscount challenge organisers provided descriptions e-commerce items goal develop system would perform automatic classiﬁcation products. table presents training instances. product descriptions training test instances classiﬁed test set. target categories organised hierarchy comprised levels level nodes middle lowest level goal challenge predict lowest level category test instance. noted classes represented training examples classes many examples. instance training instances belong common classes around classes contain product items. followed text classiﬁcation approach working textual information provided training data. context represents document vector space associated class label large number classes problem treated well scarcity data minority classes typical situation large-scale systems. example cite challenges line contrast available text much bigger like lshtc bioasq data pre-processing. since used textual information provided training/test instance organisers ﬁrst task clean data. pipeline cleaning data included removal non-ascii characters removal non-printable characters removal html tags accents removal punctuation removal lower-casing. also split words consisted text part numerical part distinct parts. instance would become addition perform stemming lemmatization stop-word removal fact text spans small operations would result loss information finally tokenized remaining text words using white space delimiter. vectorization. cleaned text generated vectors using one-hot-encoding approach. experimented binary one-hotencoded vectors term frequency vectors weighting scheme. early stages challenge found latter performed best experiments used exclusively. calculate −idf vectors smoothed applied sublinear scaling +log. finally vector normalized unit vector. classiﬁcation short documents beneﬁt successful feature selection n-grams. size vocabulary cleaned dataset combined large number training instances prohibitively large using n-grams hand selecting many features lead over-ﬁtting. result selecting representative part ngrams required careful tuning. apart description libelle ﬁelds concatenated also used marque ﬁeld. examined ways integrating information pipeline concatenating value already existing text description libelle generating binary ﬂags values ﬁeld seen training set. either compared feature generation using description libelle beneﬁted models. generating vectors applied α-power normalization vector transformed xpower normalisation main intuition reduces effect common words. transformation vector normalized unit vector. found taking root values features consistently beneﬁted performance. subsampling training dataset highly imbalanced learned models would biased towards classes. reason also order speed vectorization process well training systems randomly sampled data downsampling majority classes. procedure helped improve performance single systems also reduced training time base models. size vocabularies several sub-samples ranged around million unigrams around half data millions unigrams full dataset. tuning validation strategy tuning hyper-parameters models important aspect approach. performing k-fold cross validation million training instances thousands features prohibitive given resources. overcame problem performing hyper-parameter tuning locally subsample training set. subsample consisted instances classes randomly selected. approach allowed accelerate calculations. note also validated applicability tuning strategy using public part leaderboard; decisions improved accuracy models locally effect scores public leaderboard. trick found useful current best submission public board golden standard order validate trained models. helped avoid unnecessary submissions. relied linear models base learning choice efﬁciency high-dimensional tasks like text classiﬁcation. support vector machines well known achieving state-of-the-art performance tasks. learning base models used liblinear library support linear models learning high dimensional datasets tried main strategies classiﬁers ignore hierarchical structure among classes hierarchical top-down classiﬁcation. classiﬁcation followed oneversus-all approach complexity number classes. top-down classiﬁcation trained multi-class classiﬁer parent hierarchy products prediction started root selected best class according current multi-class classiﬁers. iteratively proceeded reaching leaf node. note top-down hierarchical classiﬁcation logarithmic complexity number classes accelerates training prediction processes signiﬁcantly. trying explore different learning methods would also help diversify ensemble experimented k-nearest neighbors classiﬁers rochio classiﬁers also known nearestcentroid classiﬁers online stochastic gradientdescent methods. although widely studied found methods give satisfactory results. instance -nearest neighbors runs using thousands unigram features feature representation achieved accuracy score public leaderboard. also experimented text embeddings using wordvec tool generating text representations dimensional space category prediction approach improved using representation still away performing competitively. mentioned reasons report results rest report svms. majority models used lregularized l-loss svms dual bias ﬁnal solutions based averaging models ensemble contained models. experimented simple voting well weighted voting schemes. simple voting always improving performance calculated fraction whole ensemble containing best performing models. simple approach order models according performance respect current best model leader board. simple majority voting applied around ordered classiﬁers. procedure would create homogeneous sub-ensemble would likely reduce variance. also employed weighted voting scheme weighting best single models. ﬁnal best submission weighting voting ensemble giving bigger weights best models. weighted voting consistently improved accuracy .%-.%. challenge used scikit-learn well scripts pre-process data. training models experimented liblinear scikitlearn vowpal wabbit. full access machine cores limited access shared machine cores ram. provide table subset submissions along public private scores obtained. comparing pairs submissions clear α-power transformation helps performance respect accuracy. submissions increasing number unigram features performance increases. however around thousand features improvements becomes negligible. pairs submissions demonstrate advantage adding bigrams apart unigrams feature set. going adding apart unigrams bigrams trigrams also improves performance indicated comparing submissions rest table. note cases features selected criterion frequency. instance submission selected common unigrams submission common features unigrams bigrams trigrams. tested also several hierarchical models using unigrams parent node hierarchy. best submission ﬁrst level pruned achieving private board fully hierarchical model note case pruned hierarchy remove step prediction thus reduce propagation errors. several hierarchical models used increase variability ensemble. table presents best single system trained approximately half data marque concatenated directly description. also present results best weighted voting systems using total base models. additionally report coverage system shows many classes detected prediction phase. data preparation. although short text classiﬁcation e-commerce product task cdiscount problems particularities huge number training instances vocabulary even after carefully cleaning dataset. found feature selection feature engineering performed studying statistics dataset predictions base models trying identify patterns classes played important role. example would like highlight marque ﬁeld. found late stages challenge using information models could beneﬁt signiﬁcantly. learning tools. early stages challenge decided svms known perform well problems. case however tools efﬁciently better since strategy works every setting. unfortunately perform exhaustive hyper-tuning vowpal wabbit on-line learning system potential provide high performing system. ensemble methods. base models svms obtain satisfactory performance classiﬁcation tasks. framework challenge however ensemble methods potential deciding winner. ensemble methods pool strong diversiﬁed models improve performance several units accuracy. relied weighted voting processes believe using sophisticated techniques would helped unigrams unigrams unigrams unigrams unigrams bigrams unigrams bigrams unigrams bigrams marque binary feature unigrams bigrams trigrams unigrams bigrams trigrams unigrams bigrams trigrams blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay. scikit-learn machine learning python. journal machine learning research shen jean-david ruvini badrul sarwar. large-scale item categorization e-commerce. proceedings international conference information knowledge management pages acm.", "year": 2016}