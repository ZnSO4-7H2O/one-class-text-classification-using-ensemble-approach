{"title": "Improving parameter learning of Bayesian nets from incomplete data", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper addresses the estimation of parameters of a Bayesian network from incomplete data. The task is usually tackled by running the Expectation-Maximization (EM) algorithm several times in order to obtain a high log-likelihood estimate. We argue that choosing the maximum log-likelihood estimate (as well as the maximum penalized log-likelihood and the maximum a posteriori estimate) has severe drawbacks, being affected both by overfitting and model uncertainty. Two ideas are discussed to overcome these issues: a maximum entropy approach and a Bayesian model averaging approach. Both ideas can be easily applied on top of EM, while the entropy idea can be also implemented in a more sophisticated way, through a dedicated non-linear solver. A vast set of experiments shows that these ideas produce significantly better estimates and inferences than the traditional and widely used maximum (penalized) log-likelihood and maximum a posteriori estimates. In particular, if EM is adopted as optimization engine, the model averaging approach is the best performing one; its performance is matched by the entropy approach when implemented using the non-linear solver. The results suggest that the applicability of these ideas is immediate (they are easy to implement and to integrate in currently available inference engines) and that they constitute a better way to learn Bayesian network parameters.", "text": "paper addresses estimation parameters bayesian network incomplete data. task usually tackled running expectation-maximization algorithm several times order obtain high log-likelihood estimate. argue choosing maximum log-likelihood estimate severe drawbacks affected overﬁtting model uncertainty. ideas discussed overcome issues maximum entropy approach bayesian model averaging approach. ideas easily applied entropy idea also implemented sophisticated dedicated non-linear solver. vast experiments shows ideas produce signiﬁcantly better estimates inferences traditional widely used maximum loglikelihood maximum posteriori estimates. particular adopted optimization engine model averaging approach best performing one; performance matched entropy approach implemented using non-linear solver. results suggest applicability ideas immediate constitute better learn bayesian network parameters. paper focuses learning parameters bayesian network known structure incomplete samples assumption missing data. setting missing data make log-likelihood function non-concave multimodal. common approach maximize presence missing data expectationmaximization algorithm generally converges local maximum function. easily modiﬁed maximize rather posterior probability data well penalized maximum likelihood ideas generally maximizing rather yields smoother estimates less prone overﬁtting following refer function maximized score. although focus learning ideas general shall apply also probabilistic graphical models share similar characteristics terms paramater learning. order reduce chance obtaining estimate score multi-start approach adopted started many diﬀerent initialization points eventually selecting estimate corresponding achieves highest score. argue however strategy drawbacks. firstly estimate maximizes score well overﬁtting. even estimation adopted ﬁxed structure amount data lead overﬁtting might fully represent distribution generated data. secondly estimates produced diﬀerent runs typically diﬀerent other achieve close scores choosing single estimate highest score implies model uncertainty estimates slightly worse score completely ignored. overall score alone seem powerful enough identify best estimate. note challenge presented involve model complexity competing estimates underlying structure thus approaches bayesian information criterion apply. propose compare approaches replace criterion selecting highest score estimate based well-known ideas already applied contexts. ﬁrst based principle maximum entropy second model averaging. maximum entropy criterion stated when make inferences incomplete information draw probability distribution maximum entropy permitted information have interpret principle ﬁrst discarding estimates score assume poor selecting entropic estimate among remaining ones. entropy-based criterion expected yield parameter estimates robust overﬁtting criterion maximum score. simplest version apply entropy principle multi-start sophisticated fashion implement using non-linear solver. similar idea explored continuous distributions partial knowledge moments features extracted data. model averaging idea instead inspired bayesian model averaging designed used multi-start technique speciﬁcally designed deal model uncertainty prescribes average predictions produced competing models assigning model weight proportional posterior probability. literature often used manage ensembles classiﬁers; attempts successful reviewed problem single model becomes usually much probable competitor thus little diﬀerence using single probable model approach apply locally conditional probability distribution deﬁnes allows instantiate single model whose parameters readily inspected instead dealing collection models. sections present vast amount experiments diﬀerent missingness processes. experiments performed grid computers would taken nine months single desktop computer. consistently show entropy bma-based approaches provide better estimates maximum score estimation better parameters also result better inferences resulting bns. adopt bayesian networks framework study even discussion paper relevant parameter learning probabilistic graphical models. therefore assume reader familiar basic concepts bayesian networks bayesian network triple directed acyclic graph nodes associated random variables discrete domains collection category ×x∈πj instantiation parents furthermore every variable conditionally independent nondescendants given parents. given independence assumptions joint agree uppercase letters used random variables lowercase letters corresponding categories. graph variables assumed known; θv|w used denote probability given data instances observed variables instance denote number instances agree conﬁguration goal learn usually done maximizing penalized likelihood common approach optimize function method completes data expected counts missing variable given observed variables variables completed weights missing value represents current estimate iteration idea equivalent weighting chance distribution given using weights together actual counts data suﬃcient statistics values computed every xjπj next estimate obtained data complete ˆθk+ ﬁrst step current estimate initial guess used. using score test convergence procedure achieves saddle point usually local optimum problem vary according initial guess view obtaining estimate high score common execute multiple runs distinct initial guesses take estimate maximum score among them. however often case many distinct estimates similar score simply selecting highest clearly over-simpliﬁed decision equal scores cannot used criterion best estimate. high score target order obtain good estimate. many estimates within tiny distance global maximum good better global one. simple alternative approach pick parameter estimate maximum entropy among maximum entropy used leads conservative distribution estimates achieve score good constraint bounds score simple reads fact necessary equations deﬁne score function force greater certain value wants non-linear optimization suite. hand simple implementation entropy done using many runs method. idea select among estimates returned diﬀerent runs achieve high enough score estimate maximum entropy. diﬀers usual maximum entropy inference ﬁrst checks high score estimates maximizes entropy among them. given usual great number parameters estimate bayesian network restricting estimates exactly equal score undesired usually scoring estimate left. related maximum entropy approach shown main diﬀerences focus continuous scenario compact parametrization e.g. using mean/variance similar features data implies estimates equally data force estimator likelihood precisely equal value maximum score note also extended idea so-called regularized version includes penalty entropy function. shown become similar estimation. work diﬀerently allowing variation score without extra penalization purpose consider estimates high score equally good couple variables number parameters estimate becomes quickly large small region parameter space estimates achieve global maximum value. however feasibility region deﬁned small percentage away maximum score enough produce whole region estimates indicating region high score estimates almost ﬂat. expected high-dimensional parameter space. bma-based approach also used overcome model uncertainty overﬁtting problems. applied alternative estimates returned various runs order obtain ﬁnal single estimate. rationale follows consider query variable given parents posterior probability distribution returned inference corresponds distribution speciﬁed conditional probability table answer query using estimates identiﬁed average returned inferences models identiﬁed using weights proportional score achieved them. repeat query combination eventually instantiate single setting ˆθxj bma-averaged inference. practice done simply averaging coeﬃcients conditional probability tables. thus locally applied estimate conditional probability distribution; denote estimate obtained way. note inferences returned match produced standard queries given parents general queries. however generally possible obtain summary estimate exactly matches inferences produced standard usage bma. fact standard seen ensemble bns; single estimate average joint distributions bns. would produce representation joint probability distribution guaranteed factorize original structure. moreover would demanding computational viewpoint. exception exists naive structures bma-based approach used estimate parameters order compare entropy approaches perform experiments using diﬀerent network structures sample sizes percentages missing data also include maximum score approach experiments serve baseline refer triple hnetwork structure identiﬁes setting. setting perform experiments experiment organized follows random draw parameters reference network sampling complete instances reference network; application mcar missingness process turns value instances actually requires average using weights posterior probability model obtained product prior probability marginal likelihood implies averaging parameters models. missing probability execution runs diﬀerent initializations using map-based score; choice estimate using entropy bma. evaluate quality estimates measure kl-divergence joint distribution represented reference network estimated networks measure quality inferences produced models obtained diﬀerent criteria necessary select queries interest. seen experiments entropy yield consistently better estimates chosen query could mitigate diﬀerences among methods order conservatively assess advantage entropy map. particular query marginal joint distribution leaf nodes without evidence network requires marginalizing non-leaf variables involves variables computation. that local mistakes estimates compensate other making harder assess diﬀerences among methods. desired characteristic wants understand strong diﬀerence among ideas. since kl-divergences normally distributed analyze results non-parametric friedman test signiﬁcance level prevent issues multiple comparisons performed post-hoc test tukeys honestly signiﬁcant diﬀerence. hence analysis yields rank methods setting metric. entropy choose maximum entropy estimate among whose score least high highest score. experiments uses structure asia network settings metrics friedman test returned rank bma; entropy; map. better understand quantitative diﬀerence among them report table relative medians divergence namely medians entropy certain task divided median obtained task. allows quantitative improvement map. improvement median ranges depending task entropy. moreover improvemenet consistent occurring settings. interestingly diﬀerence performance increase learning task challenging. instance advantage entropy entropy increases percentage missing data. conversely greater sample size easier learning task thus performance methods similar even though diﬀerences remain statistically signiﬁcant cases. result conservative design query diﬀerences among methods generally less apparent leaf table relative medians divergence asia network i.e. medians entropy divided median map. smaller numbers indicate better performance; particular values smaller indicate smaller median map. cell corresponds experiments. metric joint metric. overall experiments indicate best option entropy provide signiﬁcantly better performance either parameter estimates inferences. insight reason entropy outperform given figure clearly shows higher score necessarily imply better estimate; instead dealing estimates high score entropy discriminative score also stronger correlation divergence. figure relation divergence entropy score; darker points represent lower divergence true estimated joint distributions. ﬁgure refers thousand runs performed incomplete training samples. settings rank entropy; map. relative medians divergence reported table show large diﬀerences joint metric leaf one; slight diﬀerence exists entropy latter case advantage ideas clearer. joint metric best-performing approach. also case diﬀerence among ideas emphasized increases sample size decreases. before achieves best results provides better parameter estimates least good entropy inferences. entropy consistently outperform quality parameter estimates inferences. case randomly generated structures experimental procedure described section also includes generation random structure accomplished drawing parameters. given variables randomly included probability furthermore maximum number parents number categories variable ranges graph generated experiments follow joint metric always obtain rank entropy map. leaf metric obtain rank settings rank entropy; settings. thus consistent superiority entropy estimating parameters although always implies superiority leaf queries. entropy superior cases. summary experiments indicate best choice consistently yields best parameter estimates leaf queries either best idea ties entropy. overall entropy consistently better joint leaf metric. maximum entropy criterion applied selection parameter estimates maximizes entropy guaranteeing score exceed certain threshold. applied idea multi-start selects among estimates returned diﬀerent runs. approach identiﬁes parameter estimates better usually worse bma’s estimates. alternative implement idea maximum entropy solve directly non-linear optimization problem. requires maximize subject constraint score marginally smaller best score allowing ﬁne-grained select estimate looking solutions identiﬁed diﬀerent runs. approach referred following entropy. reason analyze situation runs tend return local optima score function. however maximum entropy estimate might well nonoptimum estimate terms score. that even increase number runs many initializations entropy idea still conﬁned saddle points score function approximation true maximum entropy idea. therefore perform experiments speciﬁc network structures understanding whether entropy idea improves using continuous optimization method instead ease expose focus joint metric metric entropy consistently inferior bma. figure shows structure ﬁrst experiments. variables uniform distributions always observed; variables binary value deﬁned logical relation variable always observed aﬀected missingness process. observed true. therefore either observed positive non-observed. missingness process given probability missing depend values. chosen conditions missing sampled instances. assume conditional probabilities known thus focusing diﬃculty learning probabilities nodes network also developed solver identiﬁes global maximum score; technically solver maps learning problem polynomial programming one; details given supplementary material. global solver diﬀerent ways compute entropy thus ensuring estimate score close global maximum compute global estimate referred global map. interesting compare global order assess impact local solver quality estimates. best knowledge previous analysis global exact solvers learning incomplete samples. experimental procedure section used considering sample sizes performing experiments setting. sample sizes friedman test joint metric returns following rank bma; entropy; entropy; map; global map. relative medians were respectively results commented several viewpoints. first entropy signiﬁcantly improves entropy almost reaching quality bma. second estimates found global solver worse map; fact penalized function oﬀers partial protection overﬁtting; indeed less prone log-likelihood overﬁtting still estimate maximizes score well aﬀected overﬁtting. value score achieved global solver slightly higher achieved local maximum identiﬁed multistart third entropy implementations outperform conﬁrming results previous experiments. network structure considered diﬀerent conﬁgurations number states node cases made randomly missing instances. thus despite simple structure network learning task interesting since many missing data moderately high number states. conﬁgurations requires estimate incomplete samples respectively parameters; numbers marginals however learned complete samples whose estimate thus identical methods. ﬁxed conﬁguration case performance entropy extremely good. obtained settings following rank friedman test entropy; bma; entropy; map. global number free unknowns continuous optimization problem high achieve global solution reasonable time. relative medians joint metric conﬁguration conﬁguration experiments conﬁrm that order entropy criterion much eﬀective dedicated solver applying multi-start here entropy even better implementation general settings analyses intended near future. paper suggests maximizing likelihood scores best choice learn parameters bayesian network. particular high score necessary suﬃcient order good estimate. improve estimation propose approach averages estimates learned diﬀerent runs maximum entropy criterion applied estimates high scores. entropy idea implemented using dedicated non-linear solver allows ﬁne-grained choice estimates. entropy promptly integrated implementation virtually cost terms implementation running time; instead non-linear solver entropy requires additional implementation eﬀort. thorough experiments show presented ideas signiﬁcantly improve quality estimates compared standard maximum penalized likelihood ideas. used optimization engine yields best results followed entropy map. dedicated non-linear solver used entropy performs good even better. moreover speciﬁc network developed global solver estimation. showed score slightly higher maximum identiﬁed runs yields worse estimates. corroborates results indicating usual scores suﬀer overﬁtting and/or model uncertainty.", "year": 2011}