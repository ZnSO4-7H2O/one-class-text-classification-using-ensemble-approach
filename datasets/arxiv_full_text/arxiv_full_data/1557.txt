{"title": "Sample-efficient Actor-Critic Reinforcement Learning with Supervised  Data for Dialogue Management", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation. However, they suffer from a poor performance in the early stages of learning. This is especially problematic for on-line learning with real users. Two approaches are introduced to tackle this problem. Firstly, to speed up the learning process, two sample-efficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER) are presented. For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes. For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence. Both models employ off-policy learning with experience replay to improve sample-efficiency. Secondly, to mitigate the cold start issue, a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning. Combining these two approaches, we demonstrate a practical approach to learn deep RL-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain.", "text": "deep reinforcement learning methods signiﬁcant potential dialogue policy optimisation. however suffer poor performance early stages learning. especially problematic on-line learning real users. approaches introduced tackle problem. firstly speed learning process sampleefﬁcient neural networks algorithms trust region actor-critic experience replay episodic natural actorcritic experience replay presented. tracer trust region helps control learning step size avoid catastrophic model changes. enacer natural gradient identiﬁes steepest ascent direction policy space speed convergence. models employ off-policy learning experience replay improve sampleefﬁciency. secondly mitigate cold start issue corpus demonstration data utilised pre-train models prior on-line reinforcement learning. combining approaches demonstrate practical approach learn deep rlbased dialogue policies demonstrate effectiveness task-oriented information seeking domain. task-oriented spoken dialogue systems assist users achieve speciﬁc goals speech hotel booking restaurant information accessing bus-schedules. systems typically designed according structured ontology deﬁnes domain system talk about. development robust traditionally requires substantial amount hand-crafted rules combined various statistical components. includes spoken language understanding module dialogue belief state tracker predict user intent track dialogue history dialogue policy determine dialogue natural language generator convert conceptual representations system responses. task-oriented teaching system respond appropriately situations nontrivial. traditionally dialogue management component designed manually using charts. recently formulated planning problem solved using reinforcement learning optimise dialogue policy interaction users framework system learns trial error process governed potentially delayed learning objective called reward. reward designed encapsulate desired behavioural features dialogue. typically provides positive reward success plus turn penalty encourage short dialogues allow system trained on-line bayesian sample-efﬁcient learning algorithms proposed learn policies minimal number dialogues. however even methods initial performance still relatively poor impact negatively supervised learning also used dialogue action selection. case policy trained produce appropriate response given dialogue state. wizard-of-oz methods widely used collecting domain-speciﬁc training corpora. recently emerging line research focused training neural networkbased dialogue models mostly text-based systems systems directly trained past dialogues without detailed speciﬁcation internal dialogue state. however limitations using sds. firstly effect selecting action future course dialogue considered result sub-optimal behaviour. secondly often large number dialogue states covered training data moreover reason suppose recorded dialogue participants acting optimally especially high noise levels. problems exacerbated larger domains multi-step planning needed. paper propose network-based approach policy learning combines best slrl-based dialogue management capitalises recent advances deep especially off-policy algorithms ﬁrst part focusses primarily increasing learning speed. tracer trust regions introduced standard actor-critic control step size thereby avoid catastrophic model changes. enacer natural gradient identiﬁes steepest ascent direction policy space ensure fast convergence. models exploit off-policy learning experience replay improve sample-efﬁciency. compared various state-of-the-art methods. second part aims mitigate cold start issue using demonstration data pre-train model. resembles training procedure adopted recent game playing applications feature framework single model trained using different training objectives without modifying architecture. combining above demonstrate practical approach learning deep rl-based dialogue policies domains achieve competitive performance without signiﬁcant detrimental impact users. rl-based approaches dialogue management actively studied time initially systems suffered slow training recent advances data efﬁcient methods gaussian processes enabled systems trained scratch on-line interaction real users provides estimate uncertainty underlying function built-in noise model. helps achieve highly sample-efﬁcient exploration robustness recognition/understanding errors. however since computation scales number points memorised sparse approximation methods kernel span algorithm must used limits ability scale large training sets. therefore questionable whether scale support commercial wide-domain sds. nevertheless provides good benchmark hence included evaluation below. addition increasing sample-efﬁciency learning algorithms reward shaping also investigated enrich reward function order speed dialogue policy learning. combining dialogue modelling new. henderson proposed hybrid sl/rl model that order ensure tractability policy optimisation performed exploration states dialogue corpus. policy deﬁned manually parts space found corpus. method initialising models using logistic regression also described gprl dialogue rather using linear kernel imposes heuristic data pair correlation preoptimised gaussian kernel learned using dialogue corpus proposed resulting kernel accurate data correlation achieved better performance however corpus help initialise better policy. better initialisation gprl studied context domain adaptation specifying prior re-using existing model pre-trained domain number authors proposed training standard neural-network policy stages asadi williams also explored off-policy methods dialogue policy learning. studies conducted simulation using error-free text-based input. similar approach also used conversational model contrast work introduces sample-efﬁcient actor-critic methods combines two-stage policy learning off-policy testing differing noise levels. proposed framework addresses dialogue management component modular sds. input model belief state encodes distribution possible user intents along dialogue history. model’s role select system action every turn lead maximum possible cumulative reward successful dialogue outcome. system action mapped system reply semantic level subsequently passed natural language generator output user. semantic reply consists three parts intent response slots talk value slot ensure tractability policy selects restricted action identiﬁes intent sometimes slot remaining information required complete reply extracted using heuristics tracked belief state. training reinforcement learning dialogue policy optimisation seen task learning select sequence responses turn maximises longterm objective deﬁned reward function. solved applying either value-based policy-based methods. cases goal optimal policy maximises γtrt dialogue turns reward taking action dialogue belief state turn discount factor. main difference categories policy-based methods stronger convergence characteristics value-based methods. latter often diverge using function approximation since optimise value space slight change value estimate lead large change policy space policy-based methods suffer sampleefﬁciency high variance often converge local optima since typically learn monte carlo estimation however preferred superior convergence properties. hence paper focus policy-based methods also include value-based method baseline. advantage actor-critic policy-based method training objective parametrised policy maximises expected reward possible dialogue trajectories given starting state. since form gradient potentially high variance baseline function typically introduced reduce variance whilst changing estimated gradient natural candidate ensure stability per-step policy change often limited setting small learning rate. however setting rate enough avoid occasional large destabilising updates conducive fast learning. here adopt modiﬁed trust region policy optimisation method introduced wang addition maximising cumulative reward optimisation also subject kullback-leibler divergence limit between updated policy average policy ensure safety. average policy represents running average past policies constrains updated policy deviate average weight advantage function. viewed special case actor-critic actor critic deﬁned parameter sets reduce number required parameters temporal difference errors used approximate advantage function left part figure shows architecture parameters resulting policy. on-policy methods update model samples collected current policy. sample-efﬁciency improved utilising experience replay minibatches dialogue experiences randomly sampled replay pool train model. increases learning efﬁciency re-using past samples multiple updates whilst ensuring stability reducing data correlation. since past experiences collected different policies compared current policy leads off-policy updates. training models \u0001-greedy action selection often used trade-off exploration exploitation whereby random action chosen probability otherwise top-ranking action selected. policy used generate training dialogues referred behaviour policy contrast policy optimised called target policy basic training algorithm described on-policy since assumed actions drawn policy target optimised off-policy learning since current policy updated samples generated behaviour policies importance sampling ratio used rescale sampled reward correct sampling bias time-step constraint satisﬁed change gradient respect otherwise update scaled along direction policy change rate lowered. direction also shown closely related natural gradient presented next section. enacer algorithm vanilla gradient descent algorithms guaranteed update model parameters steepest direction re-parametrisation widely used solution problem compatible function approximation advantage function equation ∇waw update update direction equation rewritten fisher information matrix. implies −∇θj called natural gradient. fisher matrix viewed correction term makes natural gradient independent parametrisation policy corresponds steepest ascent towards objective empirically natural gradient found signiﬁcantly speed convergence. based ideas natural actor-critic algorithm developed peters schaal episodic version fisher matrix need explicitly computed. instead gradient estimated least squares method given n-th episode consisting transition tuples enacer structured feed forward network output right figure updated natural gradient note using compatible function approximation value function need explicitly calculated. makes enacer practice policygradient method. user’s perspective performing scratch invariably result unacceptable performance early learning stages. problem mitigated off-line corpus demonstration data bootstrap policy. data come collection interactions users existing policy. used three ways pre-train model initialise supervised replay buffer psup combination two. model pre-training objective ‘mimic’ response behaviour corpus. phase essentially standard input model dialogue belief state training objective sample minimise action labels model predictions policy parametrised policy trained ﬁxed dataset generalise well. spoken dialogues noise levels vary across conditions thus signiﬁcantly affect performance. moreover policy trained using perform long-term planning conversation. nonetheless supervised pre-training offers good model starting point ﬁne-tuned using supervised replay initialisation demonstration data stored replay pool psup kept separate pool used never over-written. update iteration small portion demonstration data sampled supervised crossentropy loss computed data added learned parameters pre-trained model method might distribute differently optimal policy cause performance drop early stages learning policy model. alleviated using composite loss proposed method comparison three options included experimental evaluation. experiments utilised software tool-kit pydial provides platform modular sds. target application live telephone-based providing restaurant information cambridge area. task learn policy manages dialogue delivers requested information user. domain consists approximately venues slots used system constrain search system-informable properties available database entity found. input models full dialogue belief state size includes last system distributions user intention three requestable slots. output includes restricted dialogue actions determining system intent semantic level. combining dialogue belief states heuristic rules mapped spoken response using natural language generator. value-based methods shown comparison policy-based models described. these policy implicitly determined action-value function estimates expected total return choosing action given belief state time-step optimal policy q-function satisﬁes bellman equation eπ∗{rt maxa q∗)|bt at}. deep q-network variant q-learning algorithm whereby neural network used non-linearly approximate q-function. suggests sequential approximation equation minimising loss maxa target update parameters note evaluated target network updated less frequently network stabilise learning expectation tuples sampled experience replay pool described §... often suffers over-estimation qvalues operator used select action well evaluate double thus used de-couple action selection q-value estimation achieve better performance. gaussian processes gprl state-of-the-art value-based algorithm dialogue modelling. appealing since learn small number observations exploiting correlations deﬁned kernel function provides uncertainty measure estimates. gprl q-function modelled zero mean kernel qfunction updated calculating posterior given collected belief-action pairs corresponding rewards implicit knowledge distance data points observation space provided kernel greatly speeds learning since enables q-values unexplored space estimated. note gprl used fatemi compare deep uncertainty estimate used guide exploration result relatively poor performance. gprl uncertainty estimate used benchmark. model learns quickly relatively stable. enacer provides comparable performance. also showed high sample-efﬁciency high instability points. iterative improvement value space guarantee improvement policy space. although comparably slower learn difference on-policy clearly demonstrates sample-efﬁciency reusing past samples mini-batches. enhancements incorporated tracer algorithm make form learning competitive although still lags behind enacer gprl. learning demonstration data regardless choice model learning algorithm training policy scratch on-line always result poor user experience sufﬁcient interactions experienced allow acceptable behaviours learned. discussed off-line corpus demonstration data potentially mitigate problem. test this corpus real user spoken dialogues cambridge restaurant domain utilised. corpus split ratio training validation testing. contains interactions real users recruited amazon mechanical turk service wellbehaved described tracer three ways exploiting demonstration data explored. exploration parameter also annealed training dialogues. since tracer similar patterns ﬁrst explored impact demonstration data results since provides headroom identifying performance gains. figure shows different combinations demonstration data using noisefree conditions. supervised pre-trained model provides reasonable starting performance. model supervised pretraining improves after dialogues whilst suffering initially. hypothesise optimised pre-trained parameters distributed differently optimal parameters. also model replay shows clearly supervised replay buffer accelerate learning scratch. moreover pre-training combined replay total return dialogue dialogue length success indicator dialogue maximum dialogue length turns deep models contained hidden layers size adam optimiser used initial learning rate training \u0001-greedy policy used initially annealed training dialogues. linear kernel used. pool size minibatch size initial samples collected model updated every dialogues. note sample state transition whereas tracer enacer sample comprised whole dialogue state transitions. enacer natural gradient computed update model weights size tracer since ratio high variance occasionally extremely large clipped between maintain stable training. figure shows success rate learning curves on-policy tracer enacer. tested dialogues every training dialogues. reported previous studies benchmark achieved best result. note equation respectively. policy update demonstration data randomly sampled supervised replay pool psup number samples selected learning. similar patterns emerge utilising demonstration data improve early learning tracer enacer algorithms shown figure however case enacer less able exploit demonstration data since training method different standard actorcritics. hence supervised loss cannot directly incorporated objective equation could optimise model using separately every update. however experiments yield improvement. hence enacer learning pre-trained model reported here. compared enacer learning scratch enacer model started good performance learned slowly. again optimised pre-trained parameters distributed differently optimal enacer parameters sub-optimality. overall results suggest proposed sl+rl framework exploit demonstration data effective mitigating cold start problem tracer provides best solution terms avoiding poor initial performance rapid learning competitive fully trained performance. figure success rate tracer random policy policy trained corpus data improved respectively user simulation under various semantic error rates. user simulator different semantic error rates. random policy uniformly sampled action size regarded average initial performance learning system. generates robust model ﬁnetuned using wide range error rates. noted however drop-off performance high noise levels rapid might expected comparing gprl. believe deep architectures prone overﬁtting consequence handle well uncertainty user behaviour. plan investigate issue future work. overall outcomes validate beneﬁt proposed twophased approach system effectively pre-trained using corpus data reﬁned user interactions. paper presented compatible approaches tackling problem slow learning poor initial performance deep reinforcement learning algorithms. firstly trust region actor-critic experience replay episodic natural actor-critic experience replay presented shown sample-efﬁcient deep models broadly competitive gprl. secondly shown demonstration data utilised mitigate poor performance early stages learning. methods using off-line corpus data presented simple pre-training using using corpus data replay buffer. particularly effective used tracer provided best overall performance. also presented mismatched environments tracer demonstrated ability avoid poor initial performance trained demonstration corpus still improve substantially subsequent reinforcement learning. noted however performance still falls rather rapidly noise compared gprl uncertainty estimates handled well neural networks architectures. finally emphasised whilst paper focused early stages learning domain gprl provides benchmark hard beat potential deep readily scalability exploit on-line learning large user populations model size related experience replay buffer. pei-hao supported cambridge trust ministry education taiwan. paweł budzianowski supported epsrc council toshiba research europe cambridge research laboratory. authors would like thank members cambridge dialogue systems group valuable comments.", "year": 2017}