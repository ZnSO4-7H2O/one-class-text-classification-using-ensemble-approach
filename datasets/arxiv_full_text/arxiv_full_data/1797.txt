{"title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural  Networks", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.", "text": "network provides distribution sequences directly. popular neural network sequence models connectionist temporal classiﬁcation recurrent models sequence generation best knowledge end-to-end neural speech recognition systems employ recurrent neural networks least part processing pipeline. successful recurrent neural network architecture used context long short-term memory example model multiple layers bi-directional lstms pre-trained transducer networks obtained state-of-the-art timit dataset. after successes phoneme recognition similar systems proposed multiple layers rnns combined perform large vocabulary continuous speech recognition seems rnns become somewhat default method end-to-end models hybrid systems still tend rely feed-forward architectures. results rnn-based end-to-end systems impressive important downsides using rnns/lstms training speed slow iterative multiplications time input sequence long; training process sometimes tricky well-known problem gradient vanishing/exploding although various approaches proposed address issues data/model parallelization across multiple gpus careful initializations recurrent connections models still suffer computationally intensive otherwise demanding training procedures. inspired strengths cnns propose end-to-end speech framework combine cnns without intermediate recurrent layers. present experiments timit dataset show system able obtain results comparable obtained multiple layers lstms. previous attempt combine cnns know results state-of-the-art. straightforward incorporate end-to-end manner since task require model incorporate long-term dependencies. rnns learn kind dependencies combined reason known whether cnns able learn required temporal relationships. convolutional neural networks effective models reducing spectral variations modeling spectral correlations acoustic features automatic speech recognition hybrid speech recognition systems incorporating cnns hidden markov models/gaussian mixture models achieved state-of-the-art various benchmarks. meanwhile connectionist temporal classiﬁcation recurrent neural networks proposed labeling unsegmented sequences makes feasible train ‘end-to-end’ speech recognition system instead hybrid settings. however rnns computationally expensive sometimes difﬁcult train. paper inspired advantages cnns approach propose end-to-end speech framework sequence labeling combining hierarchical cnns directly without recurrent connections. evaluating approach timit phoneme recognition task show proposed model computationally efﬁcient also competitive existing baseline systems. moreover argue cnns capability model temporal correlations appropriate context information. index terms speech recognition convolutional neural networks connectionist temporal classiﬁcation recently convolutional neural networks achieved great success acoustic modeling context automatic speech recognition cnns usually combined hmms/gmms like regular deep neural networks results hybrid system typical hybrid system neural trained predict frame-level targets obtained forced alignment generated hmm/gmm system. temporal modeling decoding operations still handled posterior state predictions generated using neural network. hybrid approach problematic training different modules separately different criteria optimal solving ﬁnal task. consequence often requires additional hyperparameter tuning training stage laborious time consuming. furthermore issues motivated recent surge interests training ‘end-to-end’ systems end-to-end neural systems speech recognition typically replace neuzero padding along frame axis convolution; convolution stride chosen convolution operations model; limited weight sharing splits frequency bands groups limited bandwidths convolution done within group separately. instead perform convolution along frequency axis also along time axis results simple convolution commonly used computer vision. pre-activation feature maps passed nonlinear activation functions. introduce three activation functions following show functionalities convolutional layer example notice operations element-wise. rectiﬁer linear unit piece-wise linear activation function outputs zero input negative outputs input otherwise. formally given single feature relu function deﬁned follows another type activation function shown improve results task speech recognition maxout function following computational process take number piece-wise linear functions example. have dencies suitable context information. using small ﬁlter sizes along spectrogram frequency axis model able learn ﬁne-grained localized features multiple stacked convolutional layers help learn diverse features different time/frequency scales provide required non-linear modeling capabilities. unlike time windows applied systems temporal modeling deployed within convolutional layers perform convolution similar vision tasks multiple convolutional layers stacked provide relatively large context window output prediction highest layer. convolutional layers followed multiple fully connected layers ﬁnally added model. following suggestion perform pooling along frequency band ﬁrst convolutional layer. speciﬁcally evaluate model phoneme recognition timit dataset. models speech domain large ﬁlters limited weight sharing splits features limited frequency bands performing convolution separately convolution usually applied layers. section describe acoustic model whose architecture different above. complete includes stacked convolutional pooling layers multiple fully-connected layers. shown figure given sequence acoustic feature values rc×b×f number channels frequency bandwidth time length convolutional layer convolves ﬁlters {wi}k rc×m×n tensor width along frequency axis equal length along frame axis equal resulting preactivation feature maps consist tensor rk×bh×fh feature computed follows symbol denotes convolution operation bias parameter. three points worth mentioning sequence length convolution guaranteed equal input sequence length applying element-wise non-linearities features pass max-pooling layer outputs maximum unit adjacent units. pooling along frequency axis since helps reduce spectral variations within speaker different speakers pooling time shown less helpful speciﬁcally suppose feature pooling position computed step size pooling size r×s+jt values inside time index consequently feature maps pooling sequence lengths ones pooling. shown figure follow suggestions pooling performed ﬁrst convolutional layer. intuition pooling layers applied units higher layers would less discriminative respect variations input features. consider sequence sequence mapping task input sequence {z··· target sequence. case speech recognition acoustic signal sequence symbols. order train neural acoustic model must maximized input-output pair. provide distribution variable length output sequences given much longer input sequence introduce many-to-one mapping latent variable sequences {o··· shorter sequences serve ﬁnal predictions. probability sequence deﬁned probabilities latent sequences sequence. connectionist temporal classiﬁcation speciﬁes distribution latent sequences applying softmax function output network every time step provides probability emitting label alphabet output symbols time step extra blank output class introduced alphabet latent sequences represent probability outputting symbol particular time step. latent sequence sampled distribution transformed output sequence using many-to-one mapping function ﬁrst merges repetitions consecutive non-blank labels single label subsequently removes blank labels shown equation figure network structure phoneme recognition timit dataset. model consists convolutional layers followed fully-connected layers top. convolutional layers ﬁlter size max-pooling size ﬁrst convolutional layer. first second numbers correspond frequency time axes respectively. dynamic programming algorithm similar forward algorithm hmms used compute equation efﬁcient way. intermediate values dynamic programming also used compute gradient respect neural network outputs efﬁciently. generate predictions trained model using best path decoding algorithm. since model assumes latent symbols independent given network outputs framewise case latent sequence highest probability simply obtained emitting probable label time-step. predicted sequence given applying latent sequence prediction concatenation probable output formalized argmaxπp note necessarily output sequence highest probability. finding sequence generally tractable requires approximate search procedure like beam-search. evaluate models timit corpus standard -speaker training records removed. -speaker development used early stopping. evaluation performed core test audio transformed dimensional mel-ﬁlter-bank coefﬁcients deltas delta-deltas results dimensional best model consists convolutional layers fullyconnected hidden layers. unlike layers ﬁrst convolutional layer followed pooling layer described section pooling size means pool frequency axis. ﬁlter size across layers. model feature maps ﬁrst four convolutional layers feature maps remaining convolutional layers. fully-connected layer units. maxout piece-wise linear functions used activation function. architectures also evaluated comparison section details. optimize model adam learning rate stochastic gradient descent learning rate used ﬁne-tuning. batch size used training. initial weight values drawn uniformly interval dropout probability added across layers except input output layers norm coefﬁcient applied ﬁne-tuning stage. test time simple best path decoding used predicted sequences. model achieves phoneme error rate core test slightly better lstm baseline model transducer model explicit language model. details presented table notice model could take much less time train comparison lstm model keeping roughly number parameters. setup timit faster training speed using model without deliberately optimizing implementation. suppose gain computation efﬁciency might dramatic larger dataset. investigate different structural aspects model disentangle analysis three sub-experiments considering number convolutional layers ﬁlter sizes activation functions shown table turns model beneﬁt layers results nonlinearities larger input receptive ﬁelds units layers; reasonably large context windows help model capture spatial/temporal relations input sequences reasonable time-scales; maxout unit functional freedoms comparing relu parametric relu. results showed convolutional architectures cost achieve results comparable state-of-the-art adopting following methodology using signiﬁcantly deeper architecture results non-linear function also wider receptive ﬁelds along frequency temporal axes; using maxout non-linearities order make optimization easier; careful model regularization yields better generalization test time especially small table phoneme error rate timit. ’np’ number parameters. ’bilstm-l-h’ denotes model bidirectional lstm layers units direction. model ﬁlter size. results suggest deeper architecture larger ﬁlter sizes leads better performance. best performing model development test conjecture convolutional model might easier train phoneme-level sequences rather character-level. intuition local structures within phonemes robust easily captured model. additionally phoneme-level training might require modeling many long-term dependencies comparison character-level training. result convolutional model learning phonemes structure seems easier empirical research needs done investigate indeed case. finally important point favors convolutional recurrent architectures training speed. training time rendered virtually independent length input sequence parallel nature convolutional models highly optimized libraries available computations recurrent model sequential cannot easily parallelized. training time rnns increases least linearly length input sequence. work present cnn-based end-to-end speech recognition framework without recurrent neural networks widely used speech recognition tasks. show promising results timit dataset conclude model capacity learn temporal relations required integrated ctc. already observed gain computational efﬁciency timit dataset training model large vocabulary datasets integrate language model would part study. another interesting direction apply batch normalization current model. experiments conducted using theano blocks fuel authors would like acknowledge funding support samsung nserc calcul quebec compute canada canada research chairs cifar. authors would like thank dmitriy serdyuk dzmitry bahdanau arnaud bergeron pascal lamblin helpful comments. abdel-hamid a.-r. mohamed jiang penn applying convolutional neural networks concepts hybrid nn-hmm model speech recognition acoustics speech signal processing ieee international conference ieee sainath a.-r. mohamed kingsbury ramabhadran deep convolutional neural networks lvcsr acoustics speech signal processing ieee international conference sainath kingsbury a.-r. mohamed dahl saon soltau beran aravkin ramabhadran improvements deep convolutional neural networks lvcsr automatic speech recognition understanding ieee workshop hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deep speech scaling end-to-end speech recognition arxiv preprint arxiv. graves fern´andez gomez schmidhuber connectionist labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning. graves a.-r. mohamed hinton speech recognition deep recurrent neural networks acoustics speech signal processing ieee international conference zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision zhang trmal povey khudanpur improving deep neural network acoustic models using generalized maxout networks acoustics speech signal processing ieee international conference ieee graves supervised sequence labelling. springer garofolo lamel fisher fiscus pallett darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc nasa sti/recon technical report vol.", "year": 2017}