{"title": "Generative and Discriminative Text Classification with Recurrent Neural  Networks", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng & Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.", "text": "empirically characterize performance discriminative generative lstm models text classiﬁcation. although rnn-based generative models powerful bag-of-words ancestors higher asymptotic error rates discriminatively trained models. however also generative models approach asymptotic error rate rapidly discriminative counterparts—the pattern jordan proved holds linear classiﬁcation models make na¨ıve conditional independence assumptions. building ﬁnding hypothesize rnnbased generative classiﬁcation models robust shifts data distribution. hypothesis conﬁrmed series experiments zero-shot continual learning settings show generative models substantially outperform discriminative models. neural network models used natural language processing applications usually trained discriminatively. strategy succeeds many applications training data abundant data distribution stable. unfortunately neural networks require training data tend generalize poorly data distribution shifts paper explore using generative models obtain improvements sample complexity ability adapt shifting data distributions. neural networks traditionally used discriminative models ﬂexibility makes well suited estimating class priors class-conditional observation likelihoods. focus simple task—text classiﬁcation—using discriminative generative variant models based common neural network architecture models lstm process documents sequences words. generative model documents generated word word conditioned learned class embedding; discriminative model lstm reads document uses hidden representation model class posterior. contrast previous generative models text classiﬁcation model unbounded dependencies among words document. demonstrate empirically discriminative model obtains lower asymptotic error rate generative counterpart approaches rate slowly behavior precisely pattern jordan proved hold general generative discriminative linear models. finding pattern models somewhat surprising since generative models substantially powerful linear models analyzed work theoretical analysis relied heavily linearity. encouraged result turn learning problems good sample complexity crucial success explore whether generative models might preferable discriminative ones. ﬁrst consider single-task continual learning setting labels introduced sequentially learn newly introduced examples discriminative models known suffer catastrophic forgetting learning sequentially examples single class time specialized techniques actively developed minimize problem generative models hand natural kind setup since maximization training objective class decoupled classes easily order compare discriminative generative models fairly generative model shares many parameters across classes evaluate performance setting. finally compare performance discriminative generative lstm language models zero-shot learning construct semantic label space ﬁxed training based auxiliary task investigate whether learning documents onto semantic space learning generate points semantic space better. here substantial beneﬁts generative models. models inputs text classiﬁcation system document length words predict label compare discriminative generative text classiﬁcation models. discriminative models trained distinguish correct label among possible choices. given collection labeled documents models trained maximize conditional probability labels given documentsn generative models hand trained maximize joint probability labels documents following factorization predictions made bayes’ rule used compute models represent word d-dimensional embedding figure shows discriminative model uses lstm peephole connections encode document build classiﬁer encoder using average lstm hidden representations document representation. denotes vector concatenation. softmax layer lstm probability predicting label expvy re×|y| softmax parameters r|y| bias. simple average lstm hidden representations since preliminary experiments works better using last hidden state computationally much cheaper long documents attention-based models. importantly model trained discriminatively maximize conditional probability label given document generative model class-based language model shown figure here similarly compute hidden representation lstm. additionally also label embedding matrix re×|y|. chain rule factorize probability sequential prediction predict word concatenate lstm’s hidden representation label embedding softmax layer vocabulary parameters class-speciﬁc bias parameters byxt). designate model shared lstm since shares parameters across classes model’s novelty owes fact single conditional model shares parameters whose behavior modulated given label embedding whereas traditional generative classiﬁcation models label independent associated generative n-gram language classiﬁcation models peng schuurmans addition model also experiment class-based generative language model shared component among classes beneﬁt approach training parallelized across classes although resulting model larger number parameters. denote model independent lstms. note underlying lstm generative models similar discriminative model except trained maximize joint probability terms number parameters generative models extra parameters needed predict words prediction compute argmaxy∈ypp using empirical relative frequency estimate experiments datasets publicly available datasets zhang evaluate models standard text classiﬁcation datasets include news classiﬁcation sentiment analysis wikipedia article classiﬁcation questions answers categorization. table shows descriptive statistics datasets used experiments. dataset randomly hold examples original training used development set. kneser–ney bayes classiﬁer. sophisticated count-based language model uses tri| similar na¨ıve bayes classiﬁer construct language model class predict computing argmaxy∈ypp. na¨ıve bayes neural network. last also design na¨ıve bayes baseline extension na¨ıve bayes baseline replace class-conditional count-based unigram language model class-conditional vector-based unigram language model. experiments word embedding dimension lstm hidden dimension generative model dimension class embedding also train model using adagrad tune learning rate development sets. also development sets decide stop training based classiﬁcation accuracy evaluation metric. jordan theoretically empirically show generative linear models reach asymptotic error faster discriminative models difﬁcult derive theoretical properties expressive recurrent neural network models ours empirically evaluate performance models. table summarizes results full datasets along results previous work datasets. discriminative lstm model competitive discriminative models based logistic regression convolutional neural networks generative models lower classiﬁcation accuracies. results agree jordan discriminative models lower asymptotic errors generative models. comparing various generative models generative lstm models generally better baseline generative models stronger independence assumptions results suggest lstm effective method capture dependencies among words document. also compare generative lstm models shared lstm independent lstms. results roughly similar. next evaluate models varying training size. datasets randomly choose examples class. train models smaller datasets report results figure results show generative shared lstm model outperforms discriminative model almost cases small-data regime datasets except among generative models generative lstm model still achieves better classiﬁcation accuracies compared na¨ıve bayes kneser–ney bayes models even small-data regime. difﬁcult analyze theoretical sample complexity deep recurrent models collection results empirical support generative nonlinear models lower sample complexity discriminative counterparts. next experiments investigate properties discriminative generative lstm models adapt data distribution shifts. example data distribution shift classes introduced models. real-world setting able detect emergence class train examples class shows later time without retrain model entire dataset extremely attractive especially cases large dataset model. focus well models learn classes section discuss detection data distribution shifts details setup. consider setup models presented examples class sequentially here model learn information newly introduced examples able correctly classify documents class cannot train previously seen classes table summarizes results discuss details followings. discriminative lstm. ﬁrst investigate performance lstm discriminative model. discriminative models known suffer catastrophic forgetting—where models overtrain newly introduced class fail retain useful information previous classes—in setup. experiments every time examples class update parameters model information examples. however even extensive tuning learning rate freezing components network unable avoid catastrophic forgetting. observe since model trained discriminate among possible classes sees examples single class tens hundreds iterations adjusts parameters always predict class. course theory might oracle learning rate would prevent catastrophic forgetting still acquiring enough knowledge newly introduced classes update model. however practice difﬁcult discover learning rate values especially reasonably large lstm model takes high-dimensional input long news article. even learning rate value development performance varies widely across multiple training runs. note since model trained discriminatively example experiment ﬁxing word embedding matrix train components ﬁxing lstm language model component seeing ﬁrst class train softmax parameters. table continual learning results. shared ind.-gen generative shared independent models respectively also trivial make information unlabeled data pretrain components model except word embedding matrix. promising method prevent catastrophic forgetting discriminative models elastic weight consolidation however method requires computing fisher information matrix clear compute efﬁciently complex models lstm gpus. generative lstm. next consider generative independent lstms model. parameter estimations generative models na¨ıve bayes independent lstms naturally decoupled across classes. result models easily incorporate information newly introduced examples class. every time class introduced simply learn model update possible drawback approach size model grows number classes. last experiment generative shared lstm model. training procedure model follows. ﬁrst train lstm language model part large amount unlabeled data. training language model unlabeled data remove class-speciﬁc bias component class embedding random vector bounded norm pretrain shared components freeze parameters tune class embedding well class-speciﬁc softmax bias labeled training data. beneﬁt training— compared separate lstm model class—is faster train given class need learn vectors results table figure generative shared lstm model trained procedure approaches performance equivalent model examples classes performs competitively generative independent lstms model. zero-shot learning last experiments compare performance discriminative generative lstm language models zero-shot learning label embedding space ﬁxed based auxiliary task. humans acquire concepts learn relations among concepts external task knowledge effectively across multiple tasks. experiments datasets class labels semantically meaningful concepts setup. remove labels documents classes provide models knowledge classes external sources form class embeddings example labels words construct semantic space learning label embeddings using standard word embedding techniques order this pretrained glove word embedding vectors cases class labels consist words choose word labels test time examples classes evaluate precision recall hidden class well overall accuracy classes. discriminative lstm. model learns labeled data place documents semantic space embeddings documents close embeddings respective labels. practice softmax parameters learn embedding document maximize )vy) lstm hidden state word document. exexp. results show table zero shot learning results four datasets. hidden class indicates class included training data. show precision recall test data hidden class well accuracy examples classes. hidden class world sports business science tech sports ﬁnance entertainment automobile science tech society culture science math health education reference computers internet sports business ﬁnance entertainment music family relationships politics government company educational institution artist athlete ofﬁce holder mean transportation building natural place village animal plant album written work discriminative training expressive model lstm high dimensional text data produces reliable predictor seen classes resulting model overﬁts seen classes. generative lstm. model learns generate points label semantic space using labeled documents. model infer generate document politics without ever seen example training data. similar discriminative case case plays role class embeddings train parts model training data except examples hidden class observe development model able predict examples unseen class high precision recall design self-training algorithm predicted hidden class examples development training allow model train predicted examples. show results table hidden classes generative model achieves good performance. example news dataset model performs reasonably well hidden classes. difﬁcult dataset overall accuracy high yahoo precision hidden class lower result recall also suffers. nonetheless model still able achieve reasonable overall accuracy cases course include predicted hidden class examples training discriminative model also achieve good performance classes. however main point discriminative lstm model never predicts hidden classes without training data. two-class zero-shot learning. also perform experiments generative lstm model hide classes news dataset show results table case model perform well since precision predicting hidden classes drops signiﬁcantly introducing much noise training data. however model still able learn useful information since overall accuracy still higher discussion computational complexity. terms training inference time discriminative models much faster. example smallest dataset contains training examples takes approximately hours good generative models whereas training discriminative models takes approximately minutes main drawback generative models applications softmax computation since done entire vocabulary set. many cases experiments size vocabulary order hundreds thousands. approximate methods speed softmax computation hierarchical softmax noise contrastive estimation sampled softmax one-vs-each approximation however even approximations discriminative models still much faster. data likelihood. discriminative models obtaining would require separate language model training. explore whether indicator presence class. news corpus train generative lstm model examples labels compute documents test show results figure examples class training data tend lower marginal likelihoods examples classes observed training data. practice observation whether data distribution shifts need update parameters models. figure likelihood test data generative lstm model news dataset training data includes three classes. plot exclude training examples class whereas bottom plot exclude training examples class text details. conclusion compared discriminative generative lstm-based text classiﬁcation models terms sample complexity asymptotic error rates. showed generative models better discriminative counterparts small-data regime empirically extending results jordan linear nonlinear models. formal characterization generalization behavior complex neural networks difﬁcult ﬁndings convex problems failing account empirical facts generalization zhang such result remarkable domain generalization behavior simpler models transfers complex models. also investigated properties continual zero-shot settings. collection results showed generative models suitable settings able obtain comparable performance generative models trained full datasets standard setting. fernando chrisantha banarse dylan blundell charles zwols yori david rusu andrei pritzel alexander wierstra daan. pathnet evolution channels gradient descent super neural networks. arxiv preprint kirkpatrick james pascanu razvan rabinowitz neil veness joel desjardins guillaume rusu andrei milan kieran quan john ramalhoa tiago grabska-barwinska agnieszka hassabis demis clopath claudia kumaran dharshan hadsell raia. overcoming catastrophic forgetting neural networks. arxiv preprint hermann. probabilistic interpretation neural network classiﬁers discriminative training criteria. ieee transactions pattern analysis machine intelligence rusu andrei rabinowitz neil desjardins guillaume soyer hubert kirkpatrick james kavukcuoglu koray pascanu razvan hadsell raia. progressive neural networks. arxiv preprint", "year": 2017}