{"title": "An Empirical Evaluation of Four Algorithms for Multi-Class  Classification: Mart, ABC-Mart, Robust LogitBoost, and ABC-LogitBoost", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "This empirical study is mainly devoted to comparing four tree-based boosting algorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for multi-class classification on a variety of publicly available datasets. Some of those datasets have been thoroughly tested in prior studies using a broad range of classification algorithms including SVM, neural nets, and deep learning.  In terms of the empirical classification errors, our experiment results demonstrate:  1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably improves (robust) logitboost. 3. Robust) logitboost} considerably improves mart on most datasets. 4. Abc-logitboost considerably improves abc-mart on most datasets. 5. These four boosting algorithms (especially abc-logitboost) outperform SVM on many datasets. 6. Compared to the best deep learning methods, these four boosting algorithms (especially abc-logitboost) are competitive.", "text": "empirical study mainly devoted comparing four tree-based boosting algorithms mart abc-mart robust logitboost abc-logitboost multi-class classiﬁcation variety publicly available datasets. datasets thoroughly tested prior studies using broad range classiﬁcation algorithms including neural nets deep learning. abc-mart considerably improves mart. abc-logitboost considerably improves logitboost. logitboost considerably improves mart datasets. abc-logitboost considerably improves abc-mart datasets. four boosting algorithms outperform many datasets. compared best deep learning methods four boosting algorithms become successful machine learning. paper provide empirical evaluation four tree-based boosting algorithms multi-class classiﬁcation mart abc-mart robust logitboost abc-logitboost wide range datasets. abc-boost stands adaptive base class recent idea improving multi-class classiﬁcation. abc-mart abc-logitboost speciﬁc implementations abc-boost. although experiments reasonable consider thorough study necessary. datasets used small. datasets still popular machine learning research papers small practically meaningful. nowadays applications millions training samples uncommon example search engines. would also interesting compare four tree-based boosting algorithms popular learning methods support vector machines deep learning. recent study conducted thorough empirical comparison many learning algorithms including neural nets paper provide extensive experiment results using mart abc-mart robust logitboost abc-logitboost datasets used plus publicly available datasets. interesting dataset poker. private communications c.j. learn achieved classiﬁcation accuracy dataset. interestingly four boosting algorithms easily achieve accuracies. described alg. builds additive model greedy stage-wise procedure using second-order approximation requires knowing ﬁrst derivatives loss function respective function values fik. obtained derivatives derived assuming relations among however throughout paper provided alternative explanation. showed conditioning base class noticed resultant derivatives independent choice base. stage logitboost individual regression function separately class. analogous popular individualized regression approach multinomial logistic regression known result loss statistical efﬁciency compared full maximum likelihood approach. base class must identiﬁed boosting iteration training. suggested exhaustive procedure adaptively best base class minimize training loss iteration. mart paper recent discussion paper commented logitboost numerically unstable. fact logitboost paper suggested crucial implementation protections page note operations applied individual sample. goal ensure response |zik| large. hand hope larger |zik| better capture data variation. therefore thresholding operation occurs frequently expected part useful information lost. consider weights response values assumed ordered according sorted order corresponding feature values. tree-splitting procedure index weighted mean square error reduced split seek maximize alg. presents abc-logitboost using derivatives exhaustive search strategy abc-mart. again abc-logitboost differs abc-mart tree-splitting procedure number terminal nodes determines capacity base learner. suggested commented unlikely. experience large datasets often reasonable choice; also examples. number boosting iterations largely determined affordable computing time. commonly-regarded merit boosting that many datasets over-ﬁtting largely avoided reasonable dataset covertypek covertypek pokerk pokerk pokerk pokerk pokerkt pokerkt mnistk m-basic m-rotate m-image m-rand m-rotimg m-noise m-noise m-noise m-noise m-noise m-noise letterk letterk letterk poker dataset originally used samples training samples testing. since test large randomly divide equally parts pokerkt uses original training training part original test testing. pokerkt uses original training training part original test testing. pokerkt test pokerkt validation pokerkt test pokerkt validation. test sets still large treatment provide reliable results. since original training small compared size test enlarge training form pokerk pokerk pokerk pokerk. four enlarged training datasets test pokerekt training pokerk contains original training plus part original test set. similarly training pokerk pokerk pokerk contains original training plus k/k/k samples part original test set. original poker dataset provides features suit features rank features. ranks naturally ordinal appears reasonable treat suits nominal features. private communications cattral donor poker data suggested treat suits nominal. c.j. also kindly told performance affected whether suits treated nominal ordinal. experiments choose suits nominal feature; hence total number features becomes expanding suite feature binary features. created variety much difﬁcult datasets adding various background noise background images rotations original mnist dataset. shortened notations generated datasets m-basic m-rotate m-image m-rand m-rotimg m-noise m-noise m-noise. private communications erhan authors learn sizes training sets actually vary depending learning algorithms. methods retrained algorithms using training samples choosing best parameters; methods used samples training. experiments training samples m-basic m-rotate m-image m-rand m-rotimg; training samples m-noise m-noise. note datasets m-noise m-noise merely test samples each. private communications erhan understand mean compare statistical signiﬁcance test errors datasets. letter dataset total samples. experiments letterk last samples training rest testing. purpose demonstrate performance algorithms using small training sets. table summarizes test mis-classiﬁcation errors. datasets except pokerkt pokerkt report test errors tree size shrinkage pokerkt pokerkt report detailed experiment results sec. covertypek pokerk pokerk pokerk pokerk fairly large train boosting iterations. datasets always train iterations terminate training loss close machine accuracy. since notice obvious over-ﬁtting datasets simply report test errors last iterations. dataset covertypek covertypek pokerk pokerk pokerk pokerk pokerkt pokerkt mnistk m-basic m-rotate m-image m-rand m-rotimg m-noise m-noise m-noise m-noise m-noise m-noise letterk letterk letterk -values computed using binomial distributions normal approximations. recall random variable binomial probability parameter estimated variance estimated ˆp/n. -values computed using normal approximation binomial distributions. dataset covertypek covertypek pokerk pokerk pokerk pokerk pokerkt pokerkt mnistk m-basic m-rotate m-image m-rand m-rotimg m-noise m-noise m-noise m-noise m-noise m-noise letterk letterk letterk results demonstrate abc-logitboost abc-mart considerably outperform logitboost mart respectively. addition except pokerkt pokerkt observe abc-logitboost outperforms abc-mart logitboost outperforms mart. poker know could achieve error rate comparison four algorithms mart abc-mart logitboost abc-logitboost could achieve much smaller error rates pokerkt pokerkt. figure datasets m-noise m-noise. left panel error rates deep learning middle right panels errors rates four boosting algorithms. x-axis degree correlation high low; values correspond datasets m-noise m-noise. figure presents training loss i.e. covertypek pokerk boosting iterations. figures provide test mis-classiﬁcation errors covertype poker mnistk letter. ideally would like demonstrate that reasonable choice parameters abc-mart abc-logitboost always improve mart logitboost respectively. actually indeed case datasets experimented. section provide detailed experiment results mnistk pokerkt pokerkt letterk letterk. dataset experiment every combination train four boosting algorithms till training loss close machine accuracy exhaust capacity learner could provide reliable comparison iterations. experiment results illustrate performances four algorithms stable widerange base class tree sizes e.g. shrinkage parameter affect much test performance although smaller values result boosting iterations table presents test mis-classiﬁcation errors mnistkt. compared table misclassiﬁcation errors mnistkt roughly mis-classiﬁcation errors mnistk helps establish experiment results mnistk provide reliable comparison. table mnistkt. upper table test mis-classiﬁcation errors mart abc-mart bottom table test mis-classiﬁcation errors logitboost abc-logitboost mnistkt uses half test data mnistk. recall original poker dataset used samples training samples testing. provide reliable comparison form datasets pokerkt pokerkt equally dividing original test parts training set. pokerkt uses part original test testing pokerkt uses part testing. table table present test mis-classiﬁcation errors comparing tables corresponding entries close other veriﬁes four boosting algorithms provide reliable results dataset. four algorithms achieve error rates pokerkt pokerkt lowest test errors attained unlike mnistk test errors especially using mart logitboost slightly sensitive parameters. classiﬁcation fundamental task machine learning. paper presents extensive experiment results four tree-based boosting algorithms mart abc-mart logitboost) abc-logitboost multi-class classiﬁcation variety publicly available datasets. experiment results conclude following", "year": 2010}