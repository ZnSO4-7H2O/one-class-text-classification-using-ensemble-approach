{"title": "RAND-WALK: A Latent Variable Model Approach to Word Embeddings", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods.  This paper proposes a new generative model, a dynamic version of the log-linear topic model of~\\citet{mnih2007three}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by~\\citet{mikolov2013efficient} and many subsequent papers.  Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.", "text": "semantic word embeddings represent meaning word vector created diverse methods. many nonlinear operations co-occurrence statistics hand-tuned hyperparameters reweighting methods. paper proposes generative model dynamic version log-linear topic model mnih hinton methodological novelty prior compute closed form expressions word statistics. provides theoretical justiﬁcation nonlinear models like wordvec glove well hyperparameter choices. also helps explain low-dimensional semantic embeddings contain linear algebraic structure allows solution word analogies shown mikolov many subsequent papers. vector representations words capture relationships words distance angle many applications computational linguistics machine learning. constructed various models whose unifying philosophy meaning word deﬁned company keeps namely co-occurrence statistics. simplest methods word vectors explicitly represent co-occurrence statistics. reweighting heuristics known improve methods dimension reduction reweighting methods nonlinear include taking square root co-occurrence counts logarithm related pointwise mutual information collectively referred vector space models surveyed neural network language models propose another construct embeddings word vector simply neural network’s internal representation word. method nonlinear nonconvex. popularized wordvec family energy-based models followed matrix factorization approach called glove ﬁrst paper also showed solve analogies using linear algebra word embeddings. experiments theory used suggest newer methods related older pmi-based models hyperparameters and/or term reweighting methods note even method mysterious. simplest version considers symmetric matrix row/column indexed word. entry pmi) empirical probability words appearing within window certain size corpus marginal probability word vectors obtained low-rank matrix related matrix term reweightings. particular matrix found closely approximated rank matrix exist word vectors dimensions much smaller number words dictionary appears theoretical explanation empirical ﬁnding approximate rank matrix. current paper addresses this. speciﬁcally propose probabilistic model text generation augments log-linear topic model mnih hinton dynamics form random walk latent discourse space. chief methodological contribution using model priors analytically derive closed-form expression directly explains theorem section section builds insight give rigorous justiﬁcation models wordvec glove including hyperparameter choices latter. insight also leads mathematical explanation word embeddings allow analogies solved using linear algebra; section section shows good empirical model’s assumtions predictions including surprising word vectors pretty uniformly distributed space. latent variable probabilistic models language used word embeddings before including latent dirichlet allocation complicated variants neurally inspired nonlinear models fact evolved eﬀorts provide generative model explains success older vector space methods like latent semantic indexing however none earlier generative models linked models. scalar however skip-gram discriminative model generative. furthermore argument applies high-dimensional word embeddings thus address low-dimensional embeddings superior quality applications. suitable function model leads explanation contrast random walk involves latent discourse vector clearer semantic interpretation proven useful subsequent work e.g. understanding structure word embeddings polysemous words arora also work clariﬁes weighting bias terms training objectives previous methods also phenomenon discussed next paragraph. researchers tried understand vectors obtained highly nonlinear wordvec models exhibit linear structures speciﬁcally analogies like manwomanking?? queen happens word whose vector vqueen similar vector section surveys earlier attempts explain phenomenon shortcoming namely ignore large approximation error relationships like error appears larger diﬀerence best solution second best solution analogy solving error could principle lead complete failure analogy solving. explanation dimensionality word vectors plays role. also seen theoretical explanation observation dimension reduction improves quality word embeddings various tasks. intuitive explanation often given —that smaller models generalize better—turns fallacious since training method creating embeddings makes reference analogy solving. thus priori reason lowdimensional model parameters lead better performance analogy solving reason better unrelated task like predicting weather. addition giving form uniﬁcation existing methods generative model also brings intepretability word embeddings beyond traditional cosine similarity even analogy solving. example understanding diﬀerent senses polysemous word reside linear superposition within word embedding insight embeddings prove useful numerous settings neuroscience used. another explanatory feature model dimensionality word embeddings plays theoretical role —unlike previous papers model agnostic dimension embeddings superiority low-dimensional embeddings empirical ﬁnding speciﬁcally theoretical analysis makes assumption word vectors spatially isotropic means isotropy low-dimensional word vectors also plays role explanation relations=lines phenomenon isotropy puriﬁcation eﬀect mitigates eﬀect approximation error models. discourse vector slow random walk nearby words generated similar discourses. interested probabilities word pairs co-occur near other occasional jumps random walk allowed negligible eﬀect probabilities. similar log-linear model appears mnih hinton without random walk. linear chain collobert weston general. dynamic topic model blei laﬀerty utilizes topic dynamics linear word production model. belanger kakade proposed dynamic model text using kalman filters sequence words generated gaussian linear dynamical systems rather log-linear model case. novelty past works theoretical analysis method-of-moments tradition assuming prior random walk analytically integrate hidden random variables compute simple closed form expression approximately connects model parameters observable joint probabilities reminiscent analysis similar random walk models ﬁnance ﬁnding word probabilities satisfy power law. furthermore assume bulk word vectors distributed uniformly space earlier referred isotropy. quantiﬁed prior bayesian tradition. precisely ensemble word vectors consists i.i.d draws generated spherical gaussian distribution scalar random variable. assume constant. governs expected magnitude particularly important choose distribution expvw interesting. moreover dynamic range word probabilities roughly equal think absolute constant like details important realistic modeling important analysis. random walk uniform unit sphere denoted transition kernel random walk norm. form long step movement discourse vector still fast enough walk quickly space. equation close constant discourses seen plausible theoretical explanation phenomenon called self-normalization log-linear models ignoring partition function treating constant known often give good results. also studied concentration partition functions leads main theorem theorem gives simple closed form approximations probability word corpus probability words occur next other. theorem states result window size analysis works pairs appear small window size stated corollary recall pmi) log)/p))]. roughly positions discourse vector changes slowly. also consistent shift ﬁtting showed without dimension constraints solution skip-gram negative sampling satisﬁes claim must handled somewhat carefully since depend all. brieﬂy word vector reason holds follows since exp) discourse satisﬁes expvw expvw) exp) concentration inequalities show except small probability exp) exp) uniform sample since uniform distribution sphere random variable distribution pretty similar gaussian distribution vw/d) especially relatively large. observe closed form gaussian random variable ﬁrst claim using proper concentration measure tools shown variance relatively small compared mean thus concentrates around mean. note quite non-trivial random variable expvw neither bounded subgaussian/sub-exponential since tail approximately inverse poly-logarithmic instead inverse exponential. fact concentration phenomenon happen occurrence probability word necessarily concentrated norm vary model allows frequency words large dynamic range. proof theorem proof uses standard analysis linear regression. left singular values notational ease omit subscripts since relevant proof. since qς−p thus qς−p readers uncomfortable bayesian priors replace assumptions concrete properties word vectors empirically veriﬁable ﬁnal word vectors fact also word vectors computed using recent methods. constant singular values matrix word vectors satisfy properties similar random matrices formalized paragraph theorem bayesian prior word vectors happens imply conditions hold high probability. conditions hold even prior doesn’t hold. furthermore compatible sorts local structure among word vectors existence clusterings would absent truly random vectors drawn prior. training objective theorem reason follows. number times words co-occur within window corpus. probability co-occurrence particular time given successive samples random walk independent. random walk mixes fairly quickly computing approximate using fact error empirical true value pmi) driven smaller term larger terms this taylor series approximation error order ignoring theoretically justiﬁed follows. large value approaches expectation thus corresponding close thus ignoring well justiﬁed. terms signiﬁcant correspond small. empirically obey power distribution using shown terms contribute small fraction ﬁnal objective safely ignore errors. full details appear arxiv version paper expression seems mysterious since depends upon average word vector previous words. show theoretically justiﬁed. assume simpliﬁed version model small window vectors normalized suggests semantic relationships tested analogy characterized straight line referred earlier relations=lines. direction space word pair satisfying relation like plus noise vector. happens relations satisfying certain condition described below. empirical results supporting theory appear section linear structure leveraged slightly improve analogy solving. side product argument mathematical explanation empirically well-established superiority low-dimensional word embeddings high-dimensional ones setting mentioned earlier usual explanation smaller models generalize better fallacious. ﬁrst sketch missing prior attempts prove versions relations=lines ﬁrst principles. basic issue approximation error diﬀerence best solution best solution typically small whereas approximation error objective low-dimensional solutions larger. instance uses objective weighted average termwise error expression contains inner products. thus principle approximation error could lead failure method emergence linear relationship not. note interpretation disputed; e.g. argued levy goldberg understood using classical connection inner product word similarity using objective slightly improved diﬀerent objective called cosmul. however explanation still dogged issue large termwise error pinpointed here since inner product rough approximation word similarity. furthermore experiments section clearly support relations=lines interpretation. method least squares expression. shortcoming argument homomorphism assumption assumes linear relationships instead explaining basic principle. importantly empirical homomorphism nontrivial approximation error high enough imply desired strong linear relationships. shift. also give argument suggesting relationship must present solution allowed high-dimensional. unfortunately argument extend low-dimensional embeddings. even issue termwise approximation error remains. explanation. current paper introduced generative model theoretically explain emergence relationship however noted theorem issue high approximation error away either theory empirical show isotropy word vectors implies even weak version enough imply emergence observed linear relationships low-dimensional embeddings. element-wise vector essence shows solution linear regression variables constraints noise. design matrix regression matrix word vectors model satisﬁes isotropy condition. makes random-like thus solving regression left-multiplying pseudo-inverse ought denoise eﬀectively. show does. model assumed word vectors satisﬁes bulk properties similar gaussian vectors. next theorem need following weaker properties. smallest non-zero singular value figure partition function ﬁgure shows histogram random vectors appropriate norm deﬁned text. x-axis normalized mean values. values diﬀerent concentrate around mean mostly concentration phenomenon predicted analysis. larger constant times quadratic mean singular values namely empirically holds; section left singular vectors behave like random vectors respect constant norm pre-processed standard approach leaving billion tokens. words appeared less times corpus ignored resulting vocabulary co-occurrence computed using windows tokens side focus word. training method. embedding vectors trained optimizing objective using adagrad initial learning rate iterations. objective derived also used. average term-wise error observed vectors typically model better better performance explained larger errors implied theorem report results comparison glove variants wordvec vectors trained. glove’s vectors trained co-occurrence default parameter values. wordvec vectors trained using window size parameters default values. figure linear relationship squared norms word vectors logarithms word frequencies. plot corresponds word x-axis natural logarithm word frequency y-axis squared norm word vector. pearson correlation coeﬃcient indicating signiﬁcant linear relationship strongly supports mathematical prediction equation theorem experiments test modeling assumptions. first tested counter-intuitive properties concentration partition function diﬀerent discourse vectors random-like behavior matrix word embeddings terms singular values comparison also tested properties wordvec glove vectors though trained diﬀerent objectives. finally tested linear relation squared norms word vectors logarithm word frequencies implied theorem expvw) random discourse vector veriﬁed empirically picking uniformly random direction norm average norm word vectors. figure shows histogram randomly chosen vectors. values concentrated mostly range times mean. concentration also observed types vectors especially glove cbow. isotropy respect singular values. theoretical explanation relations=lines assumes matrix word vectors behaves like random matrix respect properties singular values. embeddings quadratic mean singular values minimum non-zero singular value word vectors therefore ratio small constant consistent model. ratios glove cbow skip-gram respectively also small constants. squared norms v.s. word frequencies. figure shows scatter plot squared norms vectors logarithms word frequencies. linear relationship observed thus supporting theorem correlation stronger high frequency words possibly corresponding terms higher weights training objective. correlation much weaker types word embeddings. possibly free parameters imbue embeddings properties. also cause diﬀerence concentration partition function methods. compare performance word vectors analogy tasks speciﬁcally testbeds google former contains semantic questions manwomanking?? syntactic ones runrunswalk latter syntactic questions adjectives nouns verbs. performance diﬀerent methods presented table vectors achieve performance comparable state semantic analogies syntactic tasks achieve accuracy lower glove skip-gram cbow typically outperforms others. reason probably model ignores local word order whereas models capture extent. example word aﬀect context determine next word thinks rather think incorporating linguistic features model left future work. theory section predicts existence direction relation whereas earlier levy goldberg questioned phenomenon real. experiment uses analogy testbed inner products around second singular vector. relations average projection ﬁrst singular vector average second singular vector example table shows mean similarities standard deviations ﬁrst second singular vectors relations. similar results also obtained word embedings glove wordvec. therefore ﬁrst singular vector taken direction associated relation components like random noise line model. table veriﬁcation relation directions semantic syntactic relations google testbed. relations include cap-com capital-common-countries; cap-wor capital-world; adj-adv gramcheating solver analogy testbeds. linear structure suggests better solve analogy task. uses fact semantic relationship tested many times testbed. relation represented direction cheating algorithm learn direction seeing examples relationship. word pairs presented among analogy questions k-means clustering them; estimate relation direction taking ﬁrst singular vector cluster substitute values e.g. using vectors leads accuracy. thus future designers analogy testbeds remember test relationship many times still leaves ways cheat learning directions interesting semantic relations collections analogies. non-cheating solver analogy testbeds. show even relationship tested testbed structure. given abc?? solver ﬁrst ﬁnds nearest neighbors ﬁnds among neighbors pairs cosine similarities largest. finally solver uses pairs estimate relation direction substitute estimate simple generative model introduced explain classical based word embedding models well recent variants involving energy-based models matrix factorization. model yields optimization objective essentially knobs turn embeddings lead good performance analogy tasks predictions generative model. model fewer knobs turn seen better scientiﬁc explanation certainly makes embeddings interpretable. spatial isotropy word vectors assumption model also empirical ﬁnding paper. feel help development language models. important explaining success solving analogies dimensional vectors also implies semantic relationships among words manifest special directions among word embeddings lead cheater algorithm solving analogy testbeds. model tailored capturing semantic similarity akin log-linear dynamic topic model. particular local word order unimportant. designing similar generative models linguistic features left future work. thank editors tacl granting special relaxation page limit paper. thank yann lecun christopher manning sham kakade helpful discussions various stages work. work supported part grants ccf- dms- simons investigator award simons collaboration grant onr-n---. tengyu supported addition simons award theoretical computer science fellowship.", "year": 2015}