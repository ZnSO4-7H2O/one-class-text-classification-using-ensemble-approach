{"title": "Domain Adaptation: Learning Bounds and Algorithms", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data. Building on previous work by Ben-David et al. (2007), we introduce a novel distance between distributions, discrepancy distance, that is tailored to adaptation problems with arbitrary loss functions. We give Rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions. Using this distance, we derive novel generalization bounds for domain adaptation for a wide family of loss functions. We also present a series of novel adaptation bounds for large classes of regularization-based algorithms, including support vector machines and kernel ridge regression based on the empirical discrepancy. This motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give novel algorithms. We report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation.", "text": "paper addresses general problem domain adaptation arises variety applications distribution labeled sample available somewhat differs test data. building previous work ben-david introduce novel distance between distributions discrepancy distance tailored adaptation problems arbitrary loss functions. give rademacher complexity bounds estimating discrepancy distance ﬁnite samples different loss functions. using distance derive novel generalization bounds domain adaptation wide family loss functions. also present series novel adaptation bounds large classes regularization-based algorithms including support vector machines kernel ridge regression based empirical discrepancy. motivates analysis problem minimizing empirical discrepancy various loss functions also give novel algorithms. report results preliminary experiments demonstrate beneﬁts discrepancy minimization algorithms domain adaptation. standard model theoretical models learning training test instances assumed drawn distribution. natural assumption since training test distributions substantially differ hope generalization. however practice several crucial scenarios distributions similar learning effective. scenario domain adaptation main topic analysis. many areas. quite often little labeled data available target domain labeled data source domain somewhat similar target well large amounts unlabeled data target domain one’s disposal. domain adaptation problem consists leveraging source labeled target unlabeled data derive hypothesis performing well target domain. number different adaptation techniques introduced past publications mentioned similar work context speciﬁc applications. example standard technique used statistical language modeling generative models part-ofspeech tagging parsing based maximum posteriori adaptation uses source data prior knowledge estimate model parameters similar techniques reﬁned ones used training maximum entropy models language modeling conditional models ﬁrst theoretical analysis domain adaptation problem presented ben-david gave vc-dimension-based generalization bounds adaptation classiﬁcation tasks. perhaps signiﬁcant contribution work deﬁnition application distance distributions distance particularly relevant problem domain adaptation estimated ﬁnite samples ﬁnite dimension previously shown kifer work later extended blitzer also gave bound error rate hypothesis derived weighted combination source data sets speciﬁc case empirical risk minimization. theoretical study domain adaptation presented mansour analysis deals related distinct case adaptation multiple sources target mixture source distributions. paper presents novel theoretical algorithmic analysis problem domain adaptation. builds work ben-david extends several ways. introduce novel distance discrepancy distance tailored comparing distributions adaptation. distance coincides distance classiﬁcation used compare distributions general tasks including regression loss functions. already pointed crucial advantage assume learner provided unlabeled sample drawn i.i.d. according target distribution denote loss function deﬁned pairs labels expected loss functions distribution ex∼q g)]. domain adaptation problem consists selecting hypothesis hypothesis small expected loss according target distribution rademacher complexity rademacher complexity measures ability class functions noise. empirical rademacher complexity added advantage data-dependent measured ﬁnite samples. lead tighter bounds based measures complexity vc-dimension pothesis expectation sample drawn according distribution considered. following version rademacher complexity bounds koltchinskii panchenko bartlett mendelson completeness full proof given appendix. theorem class functions mapping ﬁnite sample drawn i.i.d. according distribution then probability least samples size following inequality holds distance estimated ﬁnite samples regions used ﬁnite vc-dimension. prove holds discrepancy distance fact give data-dependent versions statement sharper bounds based rademacher complexity. give generalization bounds domain adaptation point beneﬁts comparing previous bounds. combine properties discrepancy distance derive data-dependent rademacher complexity learning bounds. also present series novel results large classes regularizationbased algorithms including support vector machines kernel ridge regression compare pointwise loss hypothesis returned algorithms trained sample drawn target domain distribution versus hypothesis selected algorithms training sample drawn source distribution. show difference pointwise losses bounded term depends directly empirical discrepancy distance source target distributions. learning bounds motivate idea replacing empirical source distribution another distribution support smallest discrepancy respect target empirical distribution viewed reweighting loss labeled point. analyze problem determining distribution minimizing discrepancy classiﬁcation square loss regression. show problem cast linear program loss derive speciﬁc efﬁcient combinatorial algorithm solve dimension one. also give polynomial-time algorithm solving problem case square loss proving cast semi-deﬁnite program finally report results preliminary experiments showing beneﬁts analysis discrepancy minimization algorithms. section describe learning set-up domain adaptation introduce notation rademacher complexity concepts needed presentation results. section introduces discrepancy distance analyzes properties. section presents generalization bounds theoretical guarantees regularization-based algorithms. section describes analyzes discrepancy minimization algorithms. section reports results preliminary experiments. consider familiar supervised learning setting learning algorithm receives sample labeled points input space label classiﬁcation measurable subset regression. domain adaptation problem training sample drawn according source distribution test points drawn according target distribution somewhat differ denote target labeling function. shall also discuss cases clearly generalization possible distribution must dissimilar thus measure similarity distributions critical derivation generalization bounds design algorithms. section discusses question introduces discrepancy distance relevant context adaptation. provides ﬁrst adaptation bound suggesting small values distance source target distributions average loss hypothesis tested target domain close average loss source domain. however general bound informative since distance large even favorable adaptation situations. instead distance distributions better suited learning task. consider example case classiﬁcation loss. denote support observe natural distance distributions context thus based supremum right-hand side regions since target hypothesis known region taken support leads following deﬁnition distance originally introduced devroye name generalized kolmogorov-smirnov distance later kifer distance introduced applied analysis adaptation classiﬁcation ben-david blitzer discussed classiﬁcation natural choice {|h′ introduce distance distributions discrepancy distance used compare distributions general tasks e.g. regression. choice terminology partly motivated relationship notion discrepancy problems arising combinatorial contexts discrepancy distance clearly symmetric hard verify veriﬁes triangle inequality regardless loss function used. general however deﬁne distance discl even non-trivial hypothesis sets bounded linear functions standard continuous loss functions. note classiﬁcation loss discrepancy distance coincides distance h∆h. discrepancy distance helps compare distributions losses y′|q general. shown kifer important advantage distance estimated ﬁnite samples ﬁnite vc-dimension. prove holds discl distance fact give datadependent versions statement sharper bounds based rademacher complexity. following theorem shows bounded loss function discrepancy distance discl distribution empirical distribution bounded terms empirical rademacher complexity class functions particular ﬁnite pseudo-dimension implies another natural question given speciﬁc learning algorithm much deviate hypothesis returned algorithm trained sample drawn would returned training sample drawn true target distribution generalization bounds minimizer note minimizers unique. adaptation succeed natural bestassume average loss three terms involving target function. extreme case single hypothesis single target function case theorem gives bound disc bound supplied disc larger factor fact reduced corollaries using favorable constant contraction lemma. following corollary shows discrepancy distance estimated ﬁnite samples. corollary hypothesis bounded loss function responding empirical distribution sample distribution sample then probability least samples size drawn according samples size drawn according figure example gray regions assumed zero support target distribution thus exist consistent hypotheses linear separator displayed. however source distribution linear separation possible. section ﬁrst assume hypothesis includes target function note imply even restrictions supp supp labeling function source problem could non-realizable. figure illustrates situation. ﬁxed loss function denote empirical error hypothesis respect empirical function deﬁned hypothesis assume convex subset vector space loss function convex respect arguments. regularization-based algorithms minimize objective form trade-off parameter. family algorithms includes support vector machines support vector regression kernel ridge regression algorithms based relative entropy regularization disc even bound might become vacuous moderate values clearly extreme case error factor arise realistic situations especially distance target function hypothesis class signiﬁcant. general bounds incomparable worthwhile compare using relatively plausible assumptions. assume discrepancy distance small average loss natural assumptions adaptween tation possible. then theorem indicates essentially bounded regret assumption holds hinge loss loss hypothesis output labels bounded x|h| y|y| theorem positive-deﬁnite symmetric kernel reproducing kernel hilbert space associated assume loss function σ-admissible. hypothesis returned regularization algorithm based empirical distribution theorem provides guarantee pointwise difference loss probability course stronger bound difference expected losses probabilistic statement. result well proof also suggests discrepancy distance right measure difference distributions context. theorem applies variety algorithms particular svms combined arbitrary kernels kernel ridge regression. viewed condition proximity labeling functions discrepancy distance relates distributions input space following result generalizes theorem setting case square loss. regularization-based algorithm better guarantee would obtained difference pointwise loss since ﬁxed sufﬁciently smaller discrepancy would actually lead hypothesis pointwise loss closer trol support search distribution minimal empirical discrepancy distance denotes distributions support supp. note using instead training viewed distribution used emphasize points de-emphasize others reduce empirical discrepancy distance. bears similarity reweighting importance weighting ideas used statistics machine learning sample bias correction techniques purposes. course objective optimized based discrepancy distance distinct previous reweighting techniques. min-max problems problem natural game theoretical interpretation. however here general cannot permute operators since convexity-type assumptions minimax theorems hold. nevertheless since max-min value always lower bound min-max provides lower bound value game minimal discrepancy identiﬁed {|h′ regions support element h∆h. problem similar min-max resource allocation problem arises task optimization rewritten following linear program thus sufﬁces keep canonical member equivalence class. necessary number constraints considered proportional πh∆h shattering coefﬁcient order hypothesis ni/n minimizes empirical discrepcrepancy bq′. case complement interval interval. discrepancy deﬁnition thatpj pi′′ maximal point less minimal point larger +pj− figure illustration discrepancy minimization algorithm dimension one. sequence labeled unlabeled points. weight assigned labeled point weights consecutive blue points right. class h∆h. sauer’s lemma bounded terms vc-dimension class πh∆h bounded since hard cases test efﬁciently whether exists consistent hypothesis e.g. half-spaces generate time consistent labeling sample points computing discrepancy consider case derive simple algorithm minimizing discrepancy loss. class preﬁxes sufﬁxes class includes intervals complements start general lower bound discrepancy. denote unlabeled regions regions thus max-min inequality following lower bound holds minimum discrepancy one-dimensional case give simple linear-time algorithm require show lower bound reached. thus case operators commute minimal discrepancy distance figure example application discrepancy minimization algorithm dimensions one. source target distributions classiﬁcation accuracy empirical results plotted function number training points unweighted case show application derived determining distribution minimizing empirical discrepancy ridge regression. figure distributions gaussians centered covariance matrix target function thus optimal linear prediction derived negative slope optimal prediction respect target distribution fact positive slope. figure shows performance ridge regression example extended -dimensions minimizing discrepancy. higher-dimension setting even several thousand points using problem could solved using single processor ram. algorithm yields distribution weights decrease discrepancy assist ridge regression selecting appropriate hypothesis target distribution. convex optimization problem since maximum eigenvalue matrix convex function matrix afﬁne function since belongs simplex. problem equivalent following semideﬁnite programming problem problems solved polynomial time using general interior point methods thus using general expression complexity interior point methods sdps following result holds. worth noting unconstrained version problem close problems seem studied number optimization publications suggests possibly efﬁcient speciﬁc algorithms general interior point methods solving problem constrained case well. observe also matrices speciﬁc structure case rank-one matrices many applications quite sparse could exploited improve efﬁciency. section reports results preliminary experiments showing beneﬁts discrepancy minimization algorithms. results conﬁrm algorithm effective practice produces distribution reduces empirical discrepancy distance allows train sample closer target distribution respect metric. also demonstrate accuracy beneﬁts algorithm respect target domain. algorithm described proposition case source target distributions shifted gaussians source distribution gaussian centered target distribution gaussian centered standard deviation hypothesis used half-spaces target function selected interval thus since positive deﬁnite exists diagonal matrix rp×p b/b/. thus write φb/. characteristic polynomial modulo multiplication thus since gram matrix kernel sample characteristic polynomial modulo multiplication conclusion presented extensive theoretical algorithmic analysis domain adaptation. analysis algorithms widely applicable beneﬁt variety adaptation tasks. efﬁcient versions algorithms instances efﬁcient approximations extend applicability techniques large-scale adaptation problems. rademacher classiﬁcation bound theorem family functions mapping denote loss. distribution then probability least following inequality holds samples size drawn according φaφ⊤ non-zero eigenvalues k/ak/. thus problem equivalent", "year": 2009}