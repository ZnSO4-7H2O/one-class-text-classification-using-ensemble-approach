{"title": "Efficient exploration with Double Uncertain Value Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action. We identify two sources of uncertainty that are relevant for exploration. The first originates from limited data (parametric uncertainty), while the second originates from the distribution of the returns (return uncertainty). We identify methods to learn these distributions with deep neural networks, where we estimate parametric uncertainty with Bayesian drop-out, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. Then, we identify that both can be jointly estimated in one network, which we call the Double Uncertain Value Network. The policy is directly derived from the learned distributions based on Thompson sampling. Experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge.", "text": "paper studies directed exploration reinforcement learning agents tracking uncertainty value available action. identify sources uncertainty relevant exploration. ﬁrst originates limited data second originates distribution returns identify methods learn distributions deep neural networks estimate parametric uncertainty bayesian drop-out return uncertainty propagated bellman equation gaussian distribution. then identify jointly estimated network call double uncertain value network. policy directly derived learned distributions based thompson sampling. experimental results show types uncertainty vastly improve learning domains strong exploration challenge. reinforcement learning dominant class algorithms learn sequential decision-making data. start zero prior knowledge need actively collect data. therefore settle policy early instead trying actions properly explored yet. however neither want continue exploring sub-optimal actions already know best. challenge known exploration/exploitation trade-off. state-of-the-art reinforcement learning implementations undirected forms exploration \u0001-greedy boltzmann exploration. methods point estimates mean actionvalue usually applying random perturbation avoid selecting currently optimal action. however undirected methods known highly inefﬁcient tracking point estimates mean state-action value algorithms lack information example discriminate action never tried action tried extensively deemed sub-optimal natural solution problem originates tracking uncertainties/distributions. intuition limited data large uncertainty reason explore narrow distributions naturally transfer exploitation work identify types uncertainties/distributions interesting exploration return uncertainty distribution returns state-action pair given policy. work focus deterministic domains makes return distribution entirely induced stochastic policy. argue deterministic environments explore acting probabilistically optimal respect distributions identify neural network methods estimate separately subsequently show combined network call double uncertain value network best knowledge ﬁrst distinguish uncertainty limited data uncertainty return distribution propagate bellman equation track neural networks improve exploration. remainder paper organized follows. section provide general introduction bayesian deep learning distributional reinforcement learning. section discuss parametric return uncertainty identify potential exploration. section discusses implementations policy evaluation neural networks also discusses derive policy learned distributions based thompson sampling. sections show experimental results discuss future work draw conclusions respectively. bayesian neural networks represent uncertainty model posterior distributions model parameters. assume observe random variables interested conditional distribution introduced neural network parameters estimate conditional distribution. bayesian setting treat model parameters random variables themselves. given observed dataset posterior distribution model parameters obtain posterior predictive distribution observed datapoint non-linear neural networks practical interest posterior distribution analytically intractable. ghahramani showed wellknown empirical procedure drop-out actually produces monte-carlo approximation providing samples posterior predictive distribution simply retaining drop-out test time technique paper discuss alternative methods bayesian inference neural networks future work section. reinforcement learning agents studied interact unknown environment goal optimize long-term performance measure. framework adopts markov decision process given tuple {sat every time-step observe state pick action {...na} available discrete actions. follows transition dynamics returns rewards work assume discrete action space deterministic transition reward functions. according stochastic policy i.e. discounted return state-action pair random process given note equality sign represents distributional equality ready deﬁne action-value function. denote expectation traces induced policy applying operator deﬁnes state-action value eπ]. applying operator gives known bellman equation papers actually start-off present current introduction emphasize mean action value quantity estimate sampling underlying return distribution approximate action-value deep neural network. write network predicting action-value network approximating entire return distribution. learn state-action value algorithms follow variants scheme known generalized policy iteration iterates policy evaluation calculate estimates state-action value based sample data policy improvement estimate improve policy argue probabilistic perspective value functions exploration. distributions might useful exploration point view statistical parametric uncertainty mean action value distribution return. parametric uncertainty mean given policy state-action value scalar number deﬁnition expectation possible future traces. however statistical point view makes sense treat estimate random variable need approximate ﬁnite number samples. call parametric uncertainty. parametric exploration i.e. acting optimistic respect uncertainty mean action-value successful bandit setting. however sparsely applied believe fundamental complication regarding uncertainties identiﬁed dearden before. bandits one-step decision problems pay-offs originating stationary distribution makes value approximation ordinary supervised learning problem. however target distribution highly non-stationary. standard target like falsely assumes known actually uncertain itself. therefore repeatedly visiting state-action pair makes certain value still uncertain next. words state-action value certainty depends future policy certainty. standard parametric uncertainty cannot account problem somehow need propagate uncertainty future state-action pairs’ value back bellman equation. illustration seen fig. right uncertainty inﬂuences current future value estimates timesteps already applies learn mean action-values). empirically observe shape return distribution strongly differs domains. matters shape return distribution also inﬂuences easily estimate expectation quantity like upper conﬁdence bound samples. example long thin right tail return distribution frequently case ‘good’ traces give mean estimate high variance appendix visualize return distributions well-known domains also introduce initial return entropy measure task exploration difﬁculty. figure three types neural networks different uncertanties/probabilitiy distributions. circles probabilistic nodes. left parametric uncertainty mean action-value. middle propagating distributions point estimate parameters. right parametric uncertainty propagating distribution illustration propagating distributions. subscripts identify unique state-action pairs. initialize state-action pairs prior parametric uncertainty prior output distribution then observed transition want update estimates current state action pair propagating distribution next node bellman operator work consider quantities propagate return distribution next node parametric uncertain return distribution next return distribution standard also parametric uncertainty introduced above usually deal mean action-value however exploration point view makes sense learn full return distribution note still focus deterministic environments. therefore distribution returns solely induced policy. modify policy makes sense optimistically respect return distribution observe. illustration consider state-action pair particular mean action value estimate really matters whether average originates highly varying return consistently return. matters policy inﬂuence shape distribution i.e. highly varying returns actively transform distribution towards good returns. words really care deterministic domains best return upper return distribution. turns challenges focus around propagating either parametric uncertainty and/or return distributions bellman equation overall idea memorize propagating global uncertainties neural network makes locally available action selection time. thereby avoid need forward planning approach entirely model-free. discuss three probabilistic policy evaluation approaches incorporate uncertainties introduced previous section parametric uncertainty only return distribution only combined. respective network structures illustrated fig. implementation details provided appendix parametric uncertainty estimate parametric uncertainty type bayesian inference method suitable neural networks. paper consider bayesian dropout simple practical implementation gives sample posterior predictive distribution mean action value return distribution next consider problem learning return distributions instead mean action-values. work assume return distribution approximated gaussian. therefore modify neural network output distribution parameters clearly note network parameters point estimates now. associated network structure visualized figure middle. policy evaluation need estimate distributional targets instead point estimate targets. construct bootstrap estimators based distributional bellman equation derivation mean well-known standard focus propagating return standard deviation distributional bellman equation assume next state distributions independent ignore covariances. standard deviation linear combination standard deviations reweighted policy probabilities shrunken approximate policy probabilities π|s) sampling policy network trained move current predictions closer targets example squared loss usual bootstrap predictions i.e. training gradients w.r.t. blocked there. approach seen form analytic approximate return propagation distributional loss similar ideas approximate return propagation recently explored discrete network output distributions also accommodate propagating multimodality. second simple propagation method also experimented sampling-based propagation. setting sample values push bellman operator construct train network collection samples with e.g. maximum likelihood loss. require samples less accurate also work complicated network output distributions analytic propagation projection infeasible. results approach shown comparable results approximate return propagation shown section parametric uncertainty return distributions ﬁnish observation ideas actually naturally combined function approximator note propagate return distribution parametric uncertainty next timestep i.e. effectively propagating uncertain return distributions starting sampled transition want propagate return distribution weighted parametric uncertainty next timestep besides that distributional bellman propagating machinery applies above. refer general mechanism uncertainty propagation bellman uncertainty. appearance network uncertainty network parameters output distribution track propagating return distributions makes refer random variables scalar constants have cov. sample next time-step make network predictions bellman propagation. repeated sampling monte carlo integration numerical integration like dearden infeasible neural network setting. double uncertain value network intuition early learning mostly propagating uncertainty converging distributions eventually start propagating true return distributions. summary identiﬁed three types probabilistic policy evaluation algorithms describe distributions naturally balance exploration versus exploitation based thompson sampling generalize notation introduce random variable distribution capture three policy evaluation distributions introduced previous section. write joint action-value distribution state assume posterior distributions action independent. thompson sampling selects action probability equal θa=a notational convention θ)∀a words choose action probability equal probability speciﬁc action optimal averaging uncertainty joint distribution practical implementation thompson sampling simple sample every argmax values consider parametric uncertainty ignore ﬁrst sampling step current parameter point estimates. consider bellman uncertainty replace second sampling step deterministic prediction thompson sampling possible choice make decisions uncertainty shown good empirical performance bandit literature naturally performs policy improvement gradually starts prefer better actions distributions start narrowing/converging. thereby hope improve instability greedy policy improvement undirected exploration. ideally uncertain return distribution would gradually narrow deterministic environment eventually converge dirac distribution optimal value function. evaluate different types probabilistic policy evaluation combination thompson sampling exploration. refer thompson sampling three types discussed policy evaluation parametric exploration return exploration uncertain return exploration. experimental details provided appendix ﬁrst consider chain domain domain consists chain states length available actions state. trace giving positive non-zero figure learning curves chain domain thompson sampling parametric uncertainty return distribution uncertain return distribution versus \u0001-greedy exploration plots progress row-wise increased depth chain i.e. increased exploration difﬁculty. note correct action state chain initialized random results averaged repetitions. reward select ‘correct’ action every step. correct action step determined domain initialization sampling uniform bernoulli. domain strong exploration challenge grows exponentially length chain learning curves chain domain shown fig. different lengths chain. first note \u0001-greedy strategy learn domain three probabilistic approaches explore best performance uncertain return exploration. longest chain length probabilistic exploration methods also trouble solving domain. however rewards makes hypothesize could issue stabilization appendix results correct action always same original variants problem next test method tasks openai repository exploration methods manage learn domains. achieved policies reﬂect good policies problem. \u0001-greedy exploration unstable domains generally performs reasonable well. note uncertainty exploration methods completely different exploration mechanism compared \u0001-greedy exploration never really perform worse domains. hypothesize domains much structure challenging enough show exploration difference seen chain domain. future work address challenging exploration problems. also want stress probabilistic exploration always outperform undirected methods especially domains relatively simple exploration. uncertainty methods generally create cautious agent ﬁrst wants properly verify parts domain. contrast undirected exploration agents exploit sooner beneﬁcial domains non-deep exploration types bayesian inference neural networks hypothesize bayesian drop-out unstable tedious tune sometimes observed experiments well. potentially different methods approximate posterior network parameters improve estimation parametric uncertainty. expressive output distributions work experimented gaussian distributions propagation. recently bellemare studied return distribution propagation categorical distributions naturally accommodate multi-modality another extension could involve expressive continuous network distributions e.g. based conditional variational inference figure learning curves parametric exploration return exploration uncertain return exploration different openai environments. results averaged repetitions. continuous action-spaces current implementations focussed discrete action spaces thompson sampling easily applied maintaining distribution action enumerating actions action selection. extension continuous setting would require either directly propagating policy uncertainty learning parametric policy whose distribution mimics uncertainty value function. paper entirely focussed domains deterministic reward transition functions makes return distribution induced stochastic policy. stochastic domains return distribution additional noise want expectation prevent continuing optimistically respect something can’t inﬂuence. finally want stress algorithms paper entirely model-free. uncertainty theme also appears model-based useful/necessary track uncertainty estimated transition and/or reward function parametric model uncertainty different parametric value/policy uncertainty studied work ideas extended model-based setting well paper introduced double uncertain value networks which best knowledge ﬁrst algorithm distinguishes uncertainty limited data uncertainty return distribution propagates bellman equation tracks neural networks uses improve exploration. implemented duvn algorithm bayesian dropout parametric uncertainty gaussian distribution bellman uncertainty propagation. main appeal implementation simplicity deep q-network implementation easily extended work adding drop-out neural network layers specifying gaussian output distribution instead mean-only prediction. take lines code automatic differentiation software packages. showed that even vanilla implementation least match improve undirected exploration performance variety problems drastically improve performance exploration heavy domain believe improvements distributional approach e.g. expressive network output distributions capture multi-modality promising direction exploration research. bellemare srinivasan ostrovski schaul saxton munos unifying countbased exploration intrinsic motivation. advances neural information processing systems pages dearden friedman andre model based bayesian exploration. proceedings fifteenth conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. engel mannor meir bayes meets bellman gaussian process approach temporal difference learning. proceedings international conference machine learning pages houthooft chen duan schulman turck abbeel vime variational information maximizing exploration. advances neural information processing systems pages exploration widely studied topic reinforcement learning. discuss work based parametric uncertainty return distributions/uncertainty context exploration approaches uncertainty methods research direction uses uncertainty mean action value either frequentist bayesian direction direct exploration. exploration usually based ’optimism uncertainty’. parametric uncertainty extensively studied bandit setting environments single state multiple actions unknown stochastic rewards. succesful approaches auer acts upper conﬁdence bound frequentist conﬁdence interval thompson sampling thompson also studied paper. extensions ideas rl/mdp setting. ﬁrst occurrence parametric uncertainty seems bayesian q-learning dearden authors tabular q-learning normal distributions conjugate updating. also ones explicitly identify necessity propagate parametric uncertainty future states. exploration based either thompson sampling also consider myopic value perfect information another exploration strategy. osband extended ideas linear function approximation setting randomized least-squares value iteration neural network context parametric uncertainty based variational inference studied bandits blundell ghahramani studied dropout uncertainty parametric value uncertainty similar work consider propagation distribution returns. osband also studied parametric exploration neural networks using non-parametric bootstrap concurrently present paper o’donoghue also identiﬁed need propagate parametric uncertainty timesteps. approach based variance estimate similar role gaussian uncertainty propagation. neural network implementation derives local parametric uncertainty estimates linearity last network layer frequentist uncertainty estimates known linear regression. contrasts bayesian approach parametric uncertainty. moreover still propagate uncertainty mean action value consider returns paper. work track uncertainty policy evaluation policy improvement exploration. focussed gaussian process regression rasmussen uses gaussian processes track parametric uncertainty value second model uncertainty transition model. however paper still uses greedy policy improvement. gaussian process approach also extended continuous action spaces. actor-critic policy search algorithms track uncertainty gradients stabalize update direct exploration. idea exploration based parametric uncertainty also connects difference action space parameter space episode-based exploration exploration methods like \u0001-greedy boltzmann inject exploration noise action space level. however beneﬁcial inject noise parameter level instead usually allows retain particular noise setting multiple steps risk action-space exploration noise agent redecide every timestep therefore cannot stick exploration decision. effect might jittering behaviour exploration exploitation steps. also identiﬁed challenge ensuring ‘deep’ exploration osband considered problem paper could example implemented ﬁxing dropout mask entire episode. also want note exact exploration problem occurs classical search. succesful monte carlo tree search algorithms like upper conﬁdence bounds trees kocsis szepesvári upper conﬁdence bound frequentist conﬁdence interval value state-action pair. overlap reinforcement learning search identiﬁed long sutton barto assume a-priori known environment model usually includes parametric function approximator represent value/policy search stores tree structure besides that exploration themes appear ﬁelds. distributional bellman equation certainly sobel white nearly research focussed mean action-value. papers study underlying return distribution study ’variance return’. engel learned distribution return gaussian processes exploration. tamar studied variance return linear function approximation. mannor tsitsiklis theoretically studies policies bound variance return. variance return need used ’optimism uncertainty’ actually frequently considered risk-sensitive several scenarios want avoid incidental large negative pay-offs e.g. desastrous real-world robot ﬁnancial portfolio. morimura studied parametric return distribution propagation well. risk-sensitive exploration softmax exploration quantile q-functions ﬁnancial management literature). distribution losses based kl-divergences could better distributional loss heuristic loss however implementations remain tabular setting. recently bellemare theoretically studied distributional bellman operator. authors show operator still contraction policy evaluation setting contraction distribution metric control setting. hypothesize ‘inherent instability greedy updates’ bellman optimality operator. algorithm uses categorical distribution propagate returns distributions easily accommodate multimodality compared gaussian distribution used work. backs-up complete bellman distributions exploration. methods nevertheless improves previous deep q-networks atari games. count-based exploration uses slightly different incentive exploration focussing rewarding regions state-space visited ideas extensively studied tabula rasa setting e.g. r-max brafman tennenholtz explicit-explore exploit kearns singh guez explicitly plans ahead using monte carlo tree search uncertain transition dynamics models. applications high-dimensional domains include stadie bellemare intrinsic motivation generalizes notion novelty internal reward domain-independent characteristics i.e. next domain-dependent external reward function. example rewarding actions decreases parametric uncertainty transition model alltogether class exploration methods usually depends ability learn good transition models problem trivial theoretical problem count-based intrinsic motivation approaches change objective itself. example bonuses novelty might make agent continue visit region state-space value functions already certain states frequently visited nevertheless intrinsic motivation-based approaches hold state-of-the-art challenging exploration problems like montezuma’s revenge work paper considered model-free however similar issues uncertainty appear learning transition and/or reward function known ‘model-based rl’. dearden ﬁrst address problem tabular setting. mannor studied environment sources variance inﬂuence return distribution ‘internal variance’ stochastic environment ‘parametric variance’ bias environment model. neither considered work added. ideas studied neural networks depeweg instead uses terms empistemic uncertainty model bias aleatoric uncertainty inherent environment noise/stochasticity. approach infers distributions neural network parameters capture model bias uses expressive output distributions capture true environment stochasticity actually similar structure double uncertain value network course transition model learning involve uncertainty propagation finally used bayesian drop-out considered work parametric value uncertainty track parametric model uncertainty. initial return entropy measure initial exploration difﬁculty deﬁne initial return distribution pinit distribution trace returns sampling initial state sinit initial state distribution following uniform random policy undirected exploration uniform policy best policy specify start encountering varying returns. deﬁne initial return entropy entropy distribution propose interesting measure domain exploration difﬁculty lower values indicate higher exploration challenge. figure shows various domains openai repository. quite large differences shape distributions. importantly domains hard exploration example atari game montezuma’s revenge show spiked therefore ire. challenge domains nearly initial traces give return makes hard ever ﬁrst indication many domains nearly traces give reward example mountain shows traces giving reward entropy return distribution course robust reward function translations making stable measure initial exploration difﬁculty. propose measure domain exploration difﬁculty. example wellknown exploration challenge choosing small suboptimal pay-off exploring obtain potential higher reward. type exploration challenge accurately reﬂected simple early rewards spread initial return distribution falsely suggest domain easy. course many dimensions inﬂuence task difﬁculty like state action space cardinality nicely illustrates low-dimensional task like chain actually quite challenging. figure return distributions initial state different environments uniform random policy. histogram produced traces maximum steps. ﬁrst domains directly taken openai gym. chain domain introduced appendix orange line kernel density estimate vertical dashed line empirical mean uniform random policy. top-right display initial return entropy estimate domain. quickly elaborate difference undirected exploration methods like \u0001-greedy boltzmann exploration directed methods like thompson sampling theoretical example. consider available actions which given observed data posterior actionvalue distribution figure shows scenario’s difference scenario’s uncertainty value action second scenario much uncertain true value. compare \u0001-greedy boltzmann thompson sampling scenario’s. main point undirected methods cannot leverage uncertainty information. \u0001-greedy exploration uses distribution means scenarios preferring action selecting action probability boltzmann exploration usually seen subtle gradually preferring actions higher pay-off. boltzmann consider numerical scale action means including difference however still acts scenario boltzmann approximates distribution actions still considering means. although softmax returns probability distribution actions interpreted uncertainty. another problem boltzmann action selection non-robust translation reward function making tedious tune. therefore many undirected implementations still prefer \u0001-greedy exploration. thompson sampling directed exploration method uses example normal random variables analytically calculate deﬁne still normal applying example gives scenario note thompson sampling naturally assigns extra probability mass action scenario much uncertain potential value. similar phenomenon happens softmax cross-entropy loss classiﬁcation tasks. outputs softmax also frequently falsely interpreted measure uncertainty classes however extrapolate away observed data classes usually gets high probability. thereby appears certain since observed data region input space actually uncertain. illustrates point estimates discrete cannot transformed uncertainties study chain domain example illustrates difﬁculty exploration sparse rewards. domain also empirically studied results section paper. consists chain states time step agent available actions every step actions ‘correct’ deterministically moves agent step chain. wrong action terminates episode. states zero reward except ﬁnal chain state variants problem studied frequently ‘ordered’ implementation correct action always optimal policy always walk right. variant illustrated fig. well. example denote number episodes ﬁrst reach state clearly reach ﬁrst time seen reward information undirected exploration follow uniform random policy. probability trace reaching state uniform policy therefore number episodes ﬁrst reach follows negative binomial distribution success probability i.e. follows example shows that undirected exploration point estimates required number exploratory episodes scales exponentially exploration depth although clearly simpliﬁed domain important note setting actually representative exploration problem sparse reward domains. well visible fig. chain domain similar initial return distributions example montezuma’s revenge game notorious challenging exploration. experiments section paper discusses unordered chain correct action every step randomized. compare ‘ordered’ chain correct action every step always although standard implementation literature believe systematic bias domain makes really exponentially challenging exploration. problem optimal policy network predicting every step. happen easily function approximator natural tendency generalize. show results ordered chain fig. first compared results fig. exploration indeed much easier ordered problem. example \u0001-greedy also solves problem something would expect exponential exploration time discussed above. nevertheless probabilistic exploration methods still outperform \u0001-greedy especially length chain increases. network architecture consists layer network discrete action nodes hidden layer relu activations. parametric uncertainty hidden layer drop-out applied output pkeep probability keep node. separate subnetworks action explicitly separate uncertainty. larger problems initial representation layers shared. learning rates ﬁxed experiments. optimization performed stochastic gradient descent using adam updates tensorﬂow. experiments parametric exploration train standard squared loss target predicted mean action value i.e. ﬁrst half target network replay database replay times prioritized domains taken openai repository available https//github.com/openai/gym. \u0001-greedy experiments ﬁxed throughout learning. drop-out rates either pkeep pkeep experiments used one-step sarsa except mountaincar experiments note ideas eligibility traces cutting traces equally apply propagation distributions i.e. allow quicker propagation multiple timesteps thompson sampling uses policy exploration evaluation. sense proper uncertainty policies somewhat blur line onoff-policy reasonable probabilistic policy incorporating uncertainty. nevertheless could consider thompson sampling exploration evaluating policy mean value again. believe uncertainty-based policies like thompson sampling also beneﬁt work cutting traces. trace cutting usually based importance sampling ratios exploratory policy target policy. thompson sampling provide realistic probabilities exploratory actions allow natural trace cutting. example \u0001-greedy always strongly cuts trace every exploratory step matter whether exploratory action close best known bad. contrast probabilistic policies traces possible actions state much uncertainty left indeed stop speed back-ups. challenge neural network implementation naturally samples next action associated probability action directly available. course small discrete action space could approximate repeatedly sampling policy.", "year": 2017}