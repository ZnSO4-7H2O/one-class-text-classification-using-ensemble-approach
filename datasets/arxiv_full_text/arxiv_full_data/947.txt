{"title": "Semi-Supervised Learning with the Deep Rendering Mixture Model", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Semi-supervised learning algorithms reduce the high cost of acquiring labeled training data by using both labeled and unlabeled data during learning. Deep Convolutional Networks (DCNs) have achieved great success in supervised tasks and as such have been widely employed in the semi-supervised learning. In this paper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a probabilistic generative model that models latent nuisance variation, and whose inference algorithm yields DCNs. We develop an EM algorithm for the DRMM to learn from both labeled and unlabeled data. Guided by the theory of the DRMM, we introduce a novel non-negativity constraint and a variational inference term. We report state-of-the-art performance on MNIST and SVHN and competitive results on CIFAR10. We also probe deeper into how a DRMM trained in a semi-supervised setting represents latent nuisance variation using synthetically rendered images. Taken together, our work provides a unified framework for supervised, unsupervised, and semi-supervised learning.", "text": "semi-supervised learning algorithms reduce high cost acquiring labeled training data using labeled unlabeled data learning. deep convolutional networks achieved great success supervised tasks widely employed semi-supervised learning. paper leverage recently developed deep rendering mixture model probabilistic generative model models latent nuisance variation whose inference algorithm yields dcns. develop algorithm drmm learn labeled unlabeled data. guided theory drmm introduce novel nonnegativity constraint variational inference term. report state-of-the-art performance mnist svhn competitive results cifar. also probe deeper drmm trained semi-supervised setting represents latent nuisance variation using synthetically rendered images. taken together work provides uniﬁed framework supervised unsupervised semisupervised learning. humans able learn labeled unlabeled data. young infants acquire knowledge world distinguish objects different classes provided labels. mathematically poverty input implies data distribution contains information useful inferring category posterior ability extract useful hidden knowledge data order leverage labeled unlabeled examples inference learning i.e. semi-supervised learning long-sought objective computer vision machine learning computational neuroscience. last years deep convolutional networks emerged powerful supervised learning models achieve near-human super-human performance various visual inference tasks object recognition image segmentation. however dcns still behind humans semi-supervised learning tasks labels available. main difﬁculty semi-supervised learning dcns that recently mathematical framework deep learning architectures. result clear dcns encode data distribution making combining supervised unsupervised learning challenging. recently deep rendering mixture model developed probabilistic graphical model underlying dcns. drmm hierarchical generative model image rendered multiple levels abstraction. shown bottominference drmm corresponds feedforward propagation dcns. drmm enables perform semi-supervised learning dcns. preliminary results semi-supervised learning drmm provided results promising work needed evaluate algorithms across many tasks. paper systematically develop semisupervised learning algorithm non-negative drmm drmm intermediate rendered templates non-negative. algorithm contains bottom-up inference pass infer nuisance variables data top-down pass performs reconstruction. also employ variational inference non-negative nature nn-drmm derive penalty terms training objective function. overview algorithm given figure validate methods showing state-of-the-art semi-supervised learning results mnist svhn well comparable results state-of-the-art methods cifar. finally analyze trained model using synthetically rendered dataset mimics cifar ground-truth labels nuisance variables including orientation location object image. show trained nn-drmm encodes nusiance variations across layers show comparison traditional dcns. figure semi-supervised learning deep rendering mixture model computation semi-supervised drmm loss function components. dashed arrows indicate parameter update. deep rendering mixture model dependence pixel location suppressed clarity. drmm generative model single super pixel level renders image patch level whose location speciﬁed shows transformation level hierarchy abstraction level focus review semi-supervised methods employ neural network structures divide different types. autoencoder-based architectures many early works semi-supevised learning neural networks built upon autoencoders autoencoders images ﬁrst projected onto low-dimensional manifold encoding neural network reconstructed using decoding neural network. model learned minimizing reconstruction error. method able learn unlabeled data combined traditional neural networks perform semi-supervised learning. line work contractive autoencoder manifold tangent classiﬁer pseudo-label denoising auto encoder winner-take-all autoencoders stacked what-where autoencoder architectures perform well enough labels dataset fail number labels reduced since data distribution taken account. recently ladder network developed overcome shortcoming. ladder network approximates deep factor analyzer layer model factor analyzer. deep neural networks used approximate bottom-up top-down inference. deep generative models another line work semisupervised learning neural networks estimate parameters probabilistic graphical model. approach applied inference graphical model hard derive exact inference computationally intractable. deep generative model family line work complementary semibenchmarks. supervised learning drmm. however method different approaches drmm graphical model underlying dcns theoretically derive semi-supervised learning algorithm proper probabilistic inference graphical model. generative adversarial networks last years variety gans achieved promising results semi-supervised learning different benchmarks including mnist svhn cifar. models also generate good-looking images natural objects. gans neural networks play minimax game. generates images classiﬁes images. objective function game’s nash equilibrium different standard object functions probabilistic modeling. would exciting promising extend drmm objective minimax game gans leave future work. deep rendering mixture model recently developed probabilistic generative model whose bottom-up inference non-negativity assumption equivalent feedforward propagation shown inference process drmm efﬁcient hierarchical structure model. particular latent variations data captured across multiple levels drmm. factorized structure results exponential reduction free parameters model enables efﬁcient learning inference. drmm potentially used semisupervised learning tasks layer object category latent variables layer parameter dictionaries contain templates layer image generated adding isotropic gaussian noise multiscale rendered template µcg. represents sequence local nuisance transformations partially render ﬁner-scale details move abstract concrete. note factorized structure drmm results exponential reduction number free parameters. enables efﬁcient inference learning better generalization. proven inference nn-drmm dynamic programming algorithm leads feedforward propagation dcn. paper develops semi-supevised learning algorithm nn-drmm. brevity throughout rest paper drop sum-over-paths formulation drmm drmm reformulated expanding matrix multiplications generation process scalar products. intensity active paths leading pixel product weights along path. sparsity controls number fraction active paths. figure depicts sum-over-paths formulation graphically. semi-supervised learning algorithm drmm analogous hard expectation-maximization algorithm gmms e-step perform bottom-up inference estimate likely joint conﬁguration latent variables object category given input image. bottom-up pass followed top-down inference uses reconstruct image µˆcˆg compute reconstruction error lrc. known applying hard algorithm gmms reconstruction error averaged dataset proportional expected complete-data log-likelihood. labeled images also compute cross-entropy predicted object classes given labels regular supervised learning tasks. order improve performance introduce kullback-leibler divergence penalty predicted object class non-negativity penalty intermediate rendered templates layer training cost objective function. motivation derivation terms figure sum-over-paths formulation drmm. rendering path contributes active exponentially many possible rendering paths exist small fraction active. rendering layer drmm. inference nonnegative drmm leads processing identical dcn. training instead closed-form step algorithm gmms gradient-based optimization methods stochastic gradient descent optimize objective function. variational inference drmm compute likely latent conﬁguration given image therefore alinference using varialows exact tional inference would like approximate posterior close true posteq minimizing divershown discussed section below. ﬁnal objective function semi-supervised learning drmm given αhlh +αrclrc +αkllkl weights cross-entropy loss reconstruction loss variational inference loss non-negativity penalty loss respectively. losses deﬁned follows here approximation true posterior context drmm softmax activations class prior labeled images otherwise. max{·} operator applied elementwise equivalent relu activation function used dcns. mnist dataset contains training images test images handwritten digits image size -by-. evaluating semi-supervised learning randomly choose images labels training amounts labeled training images class balanced. remaining training images provided without labels. -layer drmm feedforward conﬁguration similar conv small network apply batch normalization inputs stochastic gradient descent exponentially-decayed learning rate train model. table shows test error experiment. penalties help improve semi-supervised learning performance across setups. particular penalty alone reduces test error using penalties test error reduced drmm achieves state-of-the-art results experiments. also analyze value penalties learning. table reports reductions test errors using penalty only penalty only penalties training. individually penalty leads signiﬁcant improvements test errors likely since helps disentangle latent variations data. fact model continuous latent variables experimentally shown exists optimal value penalty latent variations data almost optimally disentangled results provided appendix. like mnist svhn dataset used validating semi-supervised learning methods. svhn contains color images street-view house number digits. training -layer drmm feedforward propagation similar conv large network training details mnist. train model show state-of-the-art results table results improved permutation invariant mnist task drmm performance regular mnist task. since drmm contains local latent variables level suitable tasks permutation invariant mnist divergence eqns. results loss semi-supervised learning objective function drmm similarly expected reconstruction error corresponds loss objective function. note expected reconstruction error exactly computed drmm since ﬁnite number conﬁgurations class number object classes large imagenet classes sampling techniques used approximate experiments notice semisupervised learning tasks mnist svhn cifar using likely predicted bottom-up inference compute reconstruction error yields best classiﬁcation accuracies. non-negativity constraint optimization order derive dcns drmm intermediate rendered templates must non-negative. necessary order apply max-product algorithm wherein push right efﬁcient message passing algorithm. enforce condition top-down inference drmm troducing non-negativity constrains optimization various well-developed methods solve optimization problems non-negativity constraints. employ simple useful approach adds extra non-negativity penalty objective function. yields unconstrained optimization solved gradient-based methods stochastic gradient descent. cross-validate penalty weight evaluate methods mnist svhn cifar datasets. experiments perform semisupervised learning using drmm training objective including cross-entropy cost reconstruction cost kl-distance non-negativity penalty discussed section train model provided training examples different numbers labels report state-of-the-art test errors mnist svhn. results cifar comparable state-of-the-art methods. order focus better understand impact penalties semi-supervised learning drmm don’t regularization techniques dropout noise injection experiments. also simple stochastic gradient descent optimization exponentially-decayed learning rates train model. applying regularization using better optimization methods like adam help cifar test semi-supervised learning performance drmm natural images. cifar training -layer drmm svhn. stochastic gradient descent exponentiallydecayed learning rate still used train model. table presents semi-supervised learning results -layer drmm images. even though simple algorithm train model drmm achieves comparable results state-of-the-art methods semi-supervised learning tasks cifar improved best classiﬁcation error however unlike ladder networks drmm gan-based architectures entirely different objective function approximating nash equilibrium two-layer minimax game therefore directly comparable model. order better understand drmm learns training latent variations encoded drmm train drmms synthetic dataset labels important latent variations data analyze trained model using linear decoding analysis. show drmm disentangles latent variations multiple layers compare results traditional dcns. dataset training drmm captures latent variations data given drmm yields good semi-supervised learning performance classiﬁcation tasks would like gain insight trained drmm stores knowledge latent variations data. analysis requires labels latent variations data. however popular benchmarks mnist svhn cifar include information. order overcome difﬁculty developed python blender open-source computer graphics rendering software allows generate images also access values latent variables used generate image. dataset generate linear decoding analysis section mimics cifar. dataset contains gray-scale images classes natural objects. image size -by- classes cifar. image also labels slant tilt x-location y-location depth object image. sample images dataset given figure training split dataset training test contains images respectively. perform semi-supervised learning labeled images images withlabels using -layer drmm conﬁguration experiments cifar. train equivalent dataset supervised setup using number labeled images. test errors reported table linear decoding analysis applied linear decoding analysis drmms dcns trained synthetic dataset using particularly given image activations layer latents variables ﬁrst quantizing values latent variables bins classifying activations using ﬁrst principle components activations. show classiﬁcation errors figure like dcns drmms disentangle latent variations data. however drmms keeps information latent variations across layers model drop information making decision class labels. behavior drmms semi-supervised learning addition object classiﬁcation tasks drmms also need minimize reconstruction error knowledge latent variation input images needed second task. paper proposed approach semisupervised learning dcns. algorithm builds upon drmm recently developed probabilistic generative model underlying dcns. employed algorithm develop bottom-up top-down inference drmm. also apply variational inference utilize non-negativity constraint drmm derive penalty terms penalties training objective function. method achieves state-of-the-art results semi-supervised learning tasks mnist svhn yields comparable results stateof-the-art methods cifar. analyzed trained drmm using synthetic dataset showed latent variations disentangled across layers drmm. taken together semi-supervised learning algorithm drmm promising wide range applications labels hard obtain well future research.", "year": 2016}