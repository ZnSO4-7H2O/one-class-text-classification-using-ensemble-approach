{"title": "Deep API Programmer: Learning to Program with APIs", "tag": ["cs.AI", "cs.LG"], "abstract": "We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domain-specific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.", "text": "present dapip programming-by-example system learns program apis perform data transformation tasks. design domainspeciﬁc language allows arbitrary concatenations outputs constant strings. consists three family apis regular expression-based apis lookup apis transformation apis. present novel neural synthesis algorithm search programs consistent given examples. search algorithm uses recently introduced neural architectures encode input-output examples model program search dsl. show synthesis algorithm outperforms baseline methods synthesizing programs synthetic real-world benchmarks. introduction ability discover program consistent given user intent considered central problems artiﬁcial intelligence signiﬁcant progress made synthesizing programs different domains current synthesis techniques scale larger complex programs. moreover state-of-the-art synthesis techniques require great deal domain expertise manually designed heuristics rules develop efﬁcient search procedure. paper present dapip deep programmer system aims overcome shortcomings automatically learning synthesis algorithm domain data transformation tasks. process transforming data data usable formats problem faced data scientists data analysis task. studies reported data wrangling process sometimes take total data analysis time recently programmingby-example techniques flashfill blinkfill developed help users perform data transformation tasks using examples instead write complex programs. techniques encode space programs using domain-speciﬁc language develop algorithms based version-space algebra efﬁciently search space programs. shortcomings approaches. first limited certain lowlevel syntactic regular expression-based operators allow efﬁcient structuring search space. limits expressiveness systems; example allow semantic data transformations using arbitrary transformation functions obtaining month names date abbreviating state name input address. second building efﬁcient synthesizer using requires large engineering effort manually designed heuristic rules. tackle ﬁrst shortcoming designing dapip’s function apis core element allows composition apis constant strings. consists three kinds apis regular expression-based apis lookup apis transformation apis. regular expression-based apis perform regular expression-based transformation input strings needed syntactic data transformations. lookup apis search particular string input data based dictionary strings transformation apis perform transformation lookup operation based predeﬁned mapping sets strings. lookup transformation apis allow semantic data transformations. second shortcoming handled learning synthesis algorithm dapip automatically data using recently introduced neural modules ﬁrst module called cross-correlational encoder computes ﬁxed-dimension vector representation inputoutput examples using tensor representations obtained running bi-directional lstms input output strings computing cross correlation. second module recursive-reverse-recursive neural network encodes partial derivation given example encoding vector returns distribution space possible expansions partial derivation. incrementally builds program consistent input-output examples. input-output encoder modules trained end-to-end using thousands programs corresponding input-output examples automatically sampled dsl. evaluate dapip synthetic realworld flashfill benchmarks. experiments indicate deep learning based approach able effectively model predict presence different types apis. able solve flashfill benchmarks signiﬁcantly outperforms enumerative search based baseline. automatically learn synthesis algorithm synthesizing programs using neural architectures. evaluate system dapip real-world flashfill benchmarks thousands synthetic benchmarks. motivating examples present real-world examples motivate dsl. example excel user wanted transform names ﬁrst initial followed last name shown figure since input examples optional middle names user struggling macro perform desired task. task concat conststr) getlastword). learned program uses getfirstchar getlastword apis belong class regex apis extract substrings input string based regular expressions. example excel user list addresses wanted extract city state values shown figure example common task performed systems flashfill. since data many different formats consistent regular expression used extract city names. moreover obtain state name system needs transform getstatefromcity. dapip learns following programconcat conststr getstatefromcity)). present overview end-to-end system learns synthesize programs consistent examples. training phase system shown figure test phase shown figure ﬁrst design allows composition nested calls constant strings. designed studying large family real-world string transformation tasks expressive enough encode tasks. training phase program sampler uniformly sample large number programs dsl. program rule-based approach construct input strings program prerequisites program met. obtain output strings executing program input strings. training sampled program together corresponding input-output examples used train model neural architecture learns distributions expansions conditioned examples. examples encoded using second neural architecture called cross-correlational encoder produces ﬁxed-dimensional vector. system takes input input-output conditioning vector training program trained predict conditional distribution expansions. next expansion sampled conditional distribution leading partial tree procedure repeats; observe potential order nodes growing respective ﬁgures. trained model used synthesize programs given examples. trained model takes input-output conditioning vector input generates distribution expansions likely expansions required construct desired program. distribution sampled derive programs order expansions speciﬁed distribution shown respective ﬁgure system returns ﬁrst program consistent input-output examples. figure training network learn distributions expansions conditioned input-output examples; expansion performed particular order dictated conditional distribution. domain-speciﬁc language syntax domain-speciﬁc language api-based string transformations shown figure top-level construct language concat function returns concatenation argument substrings substring expression either constant string input string result function argument. concat operator allows composition calls constant strings. consists types apis regex apis lookup apis transformation apis regex regex apis search certain regular expression-based patterns input string return matched string. examples regex apis getfirstnum getbetfirstandsecondcommas etc. consists regex apis. lookup lookup apis look presence certain strings input string return lookup string. lookup consists dictionary ﬁnite collection strings used searching input substrings. examples lookup apis getcitygetstate getstocksymbol etc. example getstate contains dictionary state names whereas getcity contains dictionary cities. consists lookup apis. transformation transformation apis consists dictionary maps ﬁnite collection strings another ﬁnite collection strings apis search string input string return corresponding output string examples apis include getstatefromcity getfirstnameinitial etc. example transformation getstatefromcity consists dictionary mapping collection cities corresponding states. consists functions. neural architecture search neural search programs conditioned input-output examples performed using model outlined first input-output examples encoded ﬁxed length feature vector aims capturing shared patterns input output strings. example representation passed neural tree-based generative model program trees called generate desired hidden program. provide high level overview architectures. neural input-output encoder ﬁxeddimensional vector representation input-output examples. intuitively encoder needs capture three information parts output strings likely constant strings parts output strings computed input strings characteristics example strings help program generator module identify useful apis given task. simplify assume ﬁxed universe possible constant strings focus training encoder produce likely apis. encoder ﬁrst runs bidirectional lstm networks separately input output strings example pair produces matrices size lstm hidden dimension maximum length string. encoder slides output matrix input matrix time step computes outer product respective matrix columns. total alignments slide matrices obtain vectors total product. finally encoder concatenates values overlapping time steps obtain ×-dimensional vector encoding example pair. tree-structured generation model tree generation model incrementally constructs program tree starting start symbol grammar expanding tree derivation time obtaining tree consists non-terminal nodes. network assigns posterior probabilities every valid expansion partial tree guide search algorithm. words given partial program tree network decides non-terminal node expand tree expansion rule grammar. deﬁned following parameters m-dimensional representation every symbol grammar m-dimensional representation grammar rule iii) deep neural network grammar rule takes input vector rq·m outputs vector deep neural network takes input vector rq·m outputs vector rq·m given partial program tree ﬁrst assigns representation leaf node denotes grammar symbol node performs standard recursive pass tree bottom-to-top recursively applying every non-leaf node node representations compute representation denotes rule associated node pass continues reach root node. represents information tree nodes encode notion node positions tree. solve issue performs reverse-recursive pass starting root node compute updated representations child nodes using reverse deep network performing reverserecursive pass leaf node assigned distributed representation intuitively captures global information every node tree. scores expansion obtained global leaf representations expansion type leaf node applied expansion score expansion calculated using probability expansion obtained exponentiated normalized scores evaluation present results major sets experiments analyze model detail goal assessing expressiveness. demonstrate model capable learning synthesize simple programs provided library functions. also show model capable strong generalization generalize across different examples given program also across unseen programs. experimental setup training details synthetic benchmarks real-world flashfill benchmarks evaluation. synthetic benchmarks obtained sampling programs uniformly using rule-based approach generate corresponding input-output examples. example sample program consisting getthirdnum getstate apis rule-based approach would ensure input strings example consist least three numbers state strings. benchmark sample input strings corresponding output strings obtained executing sampled program input strings. several examples training data shown appendix ﬁrst train consisting family apis evaluate effectiveness learning individual family. call models trained regex apis models call corresponding regex-only dsl. train apis evaluate effectiveness learning programs consisting different apis composition constant strings; call full dsl. models trained full called ff++ models. since flashfill benchmarks solved using regex apis constant strings also evaluate model flashfill benchmarks. train cross-correlation encoder jointly principle maximum likelihood; model produces posterior probabilities possible expansions backpropagate error signal based ground truth programs. adam optimizer initial learning rate clipping gradients modules. found small learning rates crucial prevent unstable learning. every epoch consists training batches instances instance contains ground truth program input-output pairs. evaluation synthetic data performed programs seen training. report results evaluating -best inference stochastic search resample program conditioned input-output examples multiple times. allow model small errors ﬁnal posterior probabilities selecting expansion. learning types three classes functions much interpretable still pose nontrivial challenges model learn compose. lookup functions contain large dictionaries model must learn call apis given input-output examples. example difference names cities seem trivial human practitioners model must learn disambiguate entities. transformation functions pose additional challenge; programs require types calls model need learn encoding hidden dictionary output string contain obvious matching substring input string because nature function. result simple string matching algorithm inputs outputs work solve problem input-output encoder must learn useful representations pairs them expressive enough capture implicit string transformation. lastly regex functions encode dictionaries represent syntactic substring operations model must learn recognize functions call based regex apis table report training validation accuracies different models trained regexdsl length column denotes maximum length programs model trained length model trained programs length length length programs. validation select randomly chosen held-out programs generate examples test generalization power trained model. particular note performance programs length length generate programs nesting composition concatenation constant string; represents possible constructs dsl. lookup transform apis experiment maximum size programs training validation size include lookup transform apis dsl. results shown table restricted apis trained models achieve high accuracy able identify composition apis high precision. performance ff++ models shown table observe training validation accuracies decreased compared models expected since increased apis also include complex apis encoding large dictionaries. however length model still able accuracy. network accredited speciﬁc nature encoder designed detect patterns substrings input output examples. interestingly lookup apis harder learn transformation apis attributed fact encode larger dictionaries compared dictionaries transform apis. flashfill using compositions present results best ff++ models flashfill benchmarks obtained authors flashfill benchmarks correspond real-world string transformation tasks excel benchmark comprises input-output string examples. models baseline performance uniform search ﬁrst present results obtain baseline uniform search model flashfill benchmarks table baseline model performs uniform search expansions biased towards small programs. also present stochastic sampling results fair comparison performance models. uniform search surprisingly well considering large space possible programs designed apis allows many benchmarks solved single call e.g. getfirstword uniform search sampler biased towards shorter programs. case observe samples lengthsur model able solve benchmarks. passes performance neural flashfill achieves accuracy samples samples. inspection benchmarks benchmarks solved programs length dsl. normalize across this solve solvable benchmarks. indicates model capable learning synthesize realistic programs. ff++ model baseline performance uniform search ﬁrst present baseline results uniform search. since expanded uniform search performs slightly worse achieve accuracy samples. flashfill benchmark performance results evaluating ff++ model flashfill benchmarks shown table length models still remarkably solve benchmarks even extended dsl. programming example string manipulations much recent work designing version space algebra-based systems performing data transformation extraction. flashfill system performs regular expression based string transformations using examples. given inputoutput example string flashfill ﬁrst searches possible ways decompose output string represent sub-programs concisely using data structure. vsa-based approach extended also build systems number transformations table joins data extraction data reshaping methods interpretable tractable unscalable additions functionality. dapip unlike vsa-based systems trained automatically using network sampling several thousands programs arbitrary dsls. neural program induction synthesis plethora recent work neural program induction neural program synthesis. goal neural program induction teach neural networks functional behavior program augmenting neural networks additional computational modules neural neural turing machine stacks-augmented rnns limitation architectures although able learn functional behavior expose interpretable program back user. addition need trained task separately representing lack strong generality. recent work terpret neural-ram seek mitigate interpretability issue need trained individual benchmark problem prohibitively expensive. recent approach proposed rnn-based neural architectures synthesize programs similar flashfill employ architecture different consisting apis core level expressions. apis allows program depth shallower programs primitives investigate make task automatically learning search strategy easier rnn. argue imposing higher-order functions much extensible akin human-like programming. function embeddings rely input-output encoder implicitly encode semantics function we’ve shown number experiments tree model capable impressive right order improve performance further extend model support explicit continuous representations function. achieved number ways simplest involves encoding function randomly initialized vector allowing model attend functions relevant input-output examples. freeze embeddings elect backpropagate errors attention mechanism embeddings jointly learn representations. represents principled approach adding functions method easy extend additional functions choose add. divide conquer function embeddings allow perform better existing problems giving model information choices make generating tree. however resolve issue scalability. even function embeddings inputs outputs grow size complexity scalable method performing inference programs synthesize. however instead viewing problem whole break problem smaller pieces solve subpiece concatenate answers together. divide-and-conquer approach allows treat larger problems conglomerations number smaller problems. procedure requires general mechanisms module need predict split output string smaller meaningful chunks second module consume input-output piece synthesize correct program piece eventually concatenated together. especially convenient problem setting flashfill language focused concatenations lose generality able solve problem. extending interesting extension multi-argument function calls. could yield general functions getnthobj could replace functions like getfirstword getsecondnumber. addition also multi-argument concat functions; idea goes neatly divide-and-conquer approach used help scale model synthesize larger programs. batching trees divide-and-conquer approach algorithmic improvement speed process training model also take advantage model incorporate faster batching proocols. using tree-based generative models allows batch operations together occur depth tree operation indepenedent siblings. moreover also batch multiple trees together increased performance. conclusion paper presented dapip system tries automatically learn synthesis algorithm given dsl. particular designed consisting apis ﬁrst class constructs allows system perform richer tasks using small sized programs. used recently introduced neural architecture automatically learn synthesis algorithm dsl. preliminary results suggest system able efﬁciently learn programs size accuracy real-world benchmarks. believe direction using neural architectures automatically develop synthesis algorithms systems lead advancements program synthesis techniques make generally applicable many domains. references rajeev alur rastislav bodik garvit juniwal milo martin mukund raghothaman sanjit seshia rishabh singh armando solar-lezama emina torlak abhishek udupa. syntax-guided synthesis. formal methods computer-aided design pages ieee daniel barowy sumit gulwani hart benjamin zorn. flashrelate extracting relational data semi-structured spreadsheets using examples. pldi pages marc brockschmidt rishabh singh nate kushman pushmeet kohli jonathan taylor daniel tarlow. terpret probabilistic programming language program induction. arxiv preprint arxiv. paepcke joseph hellerstein jeffrey heer. wrangler interactive visual speciﬁcation data transformation scripts. proceedings international conference human factors computing systems vancouver canada pages emilio parisotto abdel-rahman mohamed rishabh singh lihong dengyong zhou pushmeet kohli. neuro-symbolic program synthesis. arxiv preprint arxiv. thislibraryandtheunderlyinglearningalgorithmswillnotneedmodiﬁcations. ofanyoftheabovefunctionstothemodel;themodelimplicitlylearnslatentrepresentationsofeachfunction.thereforeanynumberoffunctionscanbeaddedto astheinputandproduceasingleoutputstringwhichcanthenbeconcatenatedornestedwithotherfunctioncalls.recallthatweprovidenofunctionsemantics figurefulllistoffunctionsusedbyourmodel;therearelookupfunctionstransformfunctionsandregexfunctions.allfunctionstaketheinputstring getfirstchar getstarttoendoffirstnumber getstringbetweenlastfirstandsecondquotegetsecondtolastcapsword getstringbetweenlastcolontoend getstarttosecondcolon getstarttofirstcolon getstarttoparan getwordbetweencommaspaceandend getallnumbers getallletters getfifthtolastpropercaseword getfourthtolastpropercaseword getthirdtolastpropercaseword getsecondtolastpropercaseword getlastpropercaseword getallpropercasewords getfifthpropercaseword getfourthpropercaseword getthirdpropercaseword getsecondpropercaseword getfirstpropercaseword getfifthtolastcapsword getlastcapsword getfifthcapsword getfourthcapsword getthirdcapsword getsecondcapsword getfirstcapsword getfirstfivedigit getfirstfourdigit getfirstthreedigit getfirsttwodigit getfirstdigit getfirstfivechar getfirstfourchar getfirstthreechar getfirsttwochar getwordbetweensecondandthirdcomma getwordbetweenfirstandsecondcomma getstarttofirstcomma getlastdashtoend getfirstdashtoseconddash getstarttodash getlastspacetoend getstarttolastspace getfirstspacetoend getstarttofirstspace getwordbetweendotandend getwordbetweenstartanddot getwordbetweenatandend getwordbetweenstartandat topropercase touppercase tolowercase replacespaceswithunderscoresgetlastcommatoend replacespaceswithcommas replacespaceswithdashes getidentity trimleadingzeros trimspaces getfifthtolastws getfourthtolastws getthirdtolastws getsecondtolastws regex getdate getyear getmonth getweekday getstocksymbol getceo getcompany getsuﬃx gettitle getlastname getfirstname getzipcode getstateabbr getstatename getcityname getaptnum getstreetname getstreetnum lookup getyearfromdate getweekdayfromdate getmonthfromdate getordinalfromdate getcompanyfromstocksymbol getceofromcompany getstocksymbolfromceo getlastinitial getfirstinitial getstatefromstateabbr getstateabbrfromstate getcityfromzipcode getstatefromcity transform getfourthws getthirdws getsecondws getfirstws getfifthtolastalpha getfourthtolastalpha getthirdtolastalpha getsecondtolastalpha getlastalpha getfifthalpha getfourthalpha getthirdalpha getsecondalpha getfirstalpha getfifthtolastnumber getfourthtolastnumber getthirdtolastnumber getlastws getsecondtolastnumbergetfifthws getlastnumber getfifthnumber getfourthnumber getthirdnumber getsecondnumber getfirstnumber getfifthtolastword getfourthtolastword getthirdtolastword getsecondtolastword getlastword getfifthword getfourthword getthirdword getsecondword getfirstword regex dapip prediction inputs elza foot locker illinois bo.com mollett sound mist nevada harpin utah reali laurinda borden connecticut belt mortimer danita tennessee throat dapip prediction inputs eldora john thain marotta marya clover sundar pichai drawer gregory wasson kristian inc. rinaldo quicksand james gorman richard johnson barbie gasaway figure selected samples correct model predictions flashfill++ synthetic test set. samples also provide samples nature programmatically generated training data ff++ models. note programs rely semantic lookup transform apis provided reference flashfill program would program could solve synthetic benchmarks. figure selected samples correct model predictions flashﬁll test set. additionally provide program flashfill inferred given input-output pairs contrast dapip’s prediction. note dapip programs much higher level expressivity interpretability. flashfill program inputs mickey minnie donald daﬀy bill jerry meyer rahul shahrukh aamir salman amitabh ajay kobe lebron dwayne chris kevin earth fire wind water outputs tommickeyminniedonald.and.daﬀy benbilljerrymeyer.and.rahul shahrukhaamirsalmanamitabh.and.ajay kobelebrondwaynechris.and.kevin earthfirewindwater.and.sun figure selected samples flashfill benchmarks could solved model; benchmarks constitute flashfill benchmarks model cannot solve. present maximum length programs dapip produce particular benchmarks would require much longer programs. however batching trees done efﬁciently system trained longer programs conceivable benchmarks solved.", "year": 2017}