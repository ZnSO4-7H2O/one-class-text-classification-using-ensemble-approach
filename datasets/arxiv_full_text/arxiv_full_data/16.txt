{"title": "Learning Features by Watching Objects Move", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.", "text": "figure low-level appearance cues lead incorrect grouping motion helps correctly group pixels move together identify group single object unsupervised motion-based grouping train convnet segment objects static images show network learns strong features transfer well tasks. input predicting pixels next frame video stream metric learning object track endpoints temporally ordering shufﬂed frames video spatially ordering patches static image challenge line research lies cleverly designing pretext task causes convnet learn high-level features. paper take different approach motivated human vision studies. infants newly sighted congenitally blind people tend oversegment static objects group things properly move rely gestalt principle common fate pixels move together tend belong together. ability parse static scenes improves time suggesting motion-based grouping appears early static grouping acquired later possibly bootstrapped motion cues. moreover experiments show shortly gaining sight human subjects better able name objects tend seen paper presents novel intuitive approach unsupervised feature learning. inspired human visual system explore whether low-level motion-based grouping cues used learn effective visual representation. speciﬁcally unsupervised motion-based segmentation videos obtain segments ‘pseudo ground truth’ train convolutional network segment objects single frame. given extensive evidence motion plays role development human visual system hope straightforward approach unsupervised learning effective cleverly designed ‘pretext’ tasks studied literature. indeed extensive experiments show case. used transfer learning object detection representation signiﬁcantly outperforms previous unsupervised approaches across multiple settings especially training data target task scarce. convnet-based image representations extremely versatile showing good performance variety recognition tasks typically representations trained using supervised learning large-scale image classiﬁcation datasets imagenet contrast animal visual systems require careful manual annotation learn instead take advantage nearly inﬁnite amount unlabeled data surrounding environments. developing models learn challenging conditions fundamental scientiﬁc problem ﬂurry recent work proposing methods learn visual representations without manual annotation. recurring theme works idea ‘pretext task’ task direct interest used obtain good visual representation byproduct training. example pretext tasks include reconstructinspired human vision studies propose train convnets well-established task object foreground background segmentation using unsupervised motion segmentation provide ‘pseudo ground truth’. concretely prepare training data optical group foreground pixels move together single object. resulting segmentation masks automatically generated targets task convnet predicting masks single static frames without motion information pixels different colors low-level image statistics still move together form single object convnet cannot solve task using low-level representation. instead recognize objects tend move identify shape pose. thus conjecture task forces convnet learn high-level representation. evaluate proposal settings. first test convnet learn good feature representation learning segment high-quality manually labeled segmentations coco without using class labels. indeed show resulting feature representation effective transferred pascal object detection. achieves state-of-the-art performance representations trained without semantic category labels performing within points imagenet pretrained model points higher best unsupervised methods. justiﬁes proposed task showing given good ground truth segmentations convnet trained segment objects learn effective feature representation. goal however learn features without manual supervision. thus second setting train automatically generated ‘pseudo ground truth’ obtained unsupervised motion segmentation uncurated videos yahoo flickr creative commons million dataset. transferred object detection representation retains good performance even convnet parameters frozen significantly outperforming previous unsupervised learning approaches. also allows much better transfer learning training data target task scarce. representation quality tends increase logarithmically amount data suggesting possibility outperforming imagenet pretraining given countless videos web. figure overview approach. motion cues segment objects videos without supervision. train convnet predict segmentations static frames i.e. without motion cues. transfer learned representation recognition tasks. tempt learn feature representations original image decoded error. alternative reconstruction-based objectives train generative models images using generative adversarial networks models extended produce good feature representations training jointly image encoders however generate realistic images models must signiﬁcant attention low-level details potentially ignoring higher-level semantics. self-supervision pretext tasks. instead producing images several recent studies focused providing alternate forms supervision require manual labeling algorithmically produced. instance doersch task convnet predicting relative location cropped image patches. noroozi favaro extend asking network arrange shufﬂed patches cropped grid. pathak train network perform image inpainting task. pretext tasks include predicting color channels luminance vice versa predicting sounds video frames assumption works perform tasks network need recognize high-level concepts objects order succeed. compare approach pretext tasks show proposed natural task object segmentation leads quantitatively better feature representation many cases. learning motion action. human visual system receive static images; receives continuous video stream. idea deﬁning auxiliary pretext tasks used unsupervised learning videos too. wang gupta train convnet distinguish between pairs tracked patches single video pairs patches different videos. misra network arrange shufﬂed frames video temporally correct order. another pretext task make predictions next frames goroshin predict pixels future frames walker predict dense future trajectories. however since nearby frames video tend visually similar approaches might learn low-level image statistics instead semantic features. alternatively motion boundary detection bootstrap convnet-based contour detector lead good feature representations. intuitions similar approach produces semantically strong representations. animals robots also sense motion possible task predict signal visual input alone cues undoubtedly useful show strong representations learned even cues unavailable. measure quality learned feature representation need evaluation reﬂects real-world constraints yield useful conclusions. prior work unsupervised learning evaluated representations using initializations ﬁne-tuning convnet particular isolated task object detection intuition good representations serve good starting point task-speciﬁc ﬁne-tuning. ﬁne-tuning task good solution also impractical. example mobile might want handle multiple tasks device image classiﬁcation object detection segmentation. download size execution time grow linearly number tasks unless computation shared. cases desirable general representation shared tasks task-speciﬁc lightweight classiﬁer ‘heads’. another practical concern arises amount labeled training data limited ﬁne-tuning. again scenario desirable ﬁxed general representation trained task-speciﬁc ‘head’ avoid overﬁtting. rather emphasizing cases paper broader understanding evaluating learned representations variety conditions shared layers ﬁne-tune pretrained convnet weights different extents ranging fully connected layers ﬁne-tuning everything similar evaluation imagenet). core intuition behind paper training convnet group pixels static images objects withclass labels cause learn strong highlevel feature representation. grouping difﬁcult low-level cues alone objects typically made multiple colors textures occluded might even consist spatially disjoint regions. therefore effectively grouping implicitly recognize object understand location shape even cannot named. thus train convnet task expect learn representation aids recognition. test hypothesis series experiments using high-quality manual annotations static images coco although supervised experiments help evaluate well method might work ideal conditions performance impacted segments lower quality much data needed. describe experiments detail. training convnet segment objects frame task follows given image patch containing single object want convnet segment object i.e. assign pixel label lies object otherwise. since image contains multiple objects task ambiguous feed convnet entire image. instead sample object image crop around ground truth segment. however given precise bounding easy convnet cheat blob center would yield loss. prevent degenerate solutions jitter position scale. note similar training setup used recent segmentation proposal methods straightforward convnet architecture takes input image outputs mask. network ends fully connected layer outputs followed element-wise sigmoid. resulting dimensional vector reshaped mask. also downsample ground truth mask cross entropy losses locations train network. experiments enable comparisons prior work unsupervised learning alexnet convnet architecture. images annotations trainval coco dataset discarding class labels using segmentations. training segmentation yield good features? following recent work unsupervised learning perform experiments task object detection pascal using fast r-cnn multifigure representation trained manually-annotated segments coco compared imagenet pretraining context prediction evaluated object detection pascal ‘>cx’ layers convx ﬁne-tuned; ‘all’ entire ﬁne-tuned. scale training testing keeping motivation described section measure performance convnet layers frozen different extents. compare representation convnet trained image classiﬁcation imagenet representation trained doersch latter competitive state-of-the-art. results shown figure supervised representation outperforms unsupervised context prediction model across scenarios large margin expected. notably though model maintains fairly small imagenet pretraining. result state-of-the-art model trained without semantic category labels. thus given highquality segments proposed method learn strong representation validates hypothesis. figure also shows model trained context prediction degrades rapidly layers frozen. drop indicates higher layers model become overly speciﬁc pretext task capture high-level concepts needed object recognition. contrast stable performance imagenet trained model even network frozen suggesting utility higher layers recognition tasks. trend also true representation retains good performance even convnet frozen indicating indeed learned high-level semantics higher layers. convnet learn noisy masks? next quality learned representation impacted quality ground truth important since segmentations obtained unsupervised motion-based grouping imperfect. simulate noisy segments train representation degraded masks coco. consider ways creating noisy segments introducing noise boundary truncating mask. figure degrade ground truth masks measure impact segmentation quality learned representation. left right original mask dilated eroded masks truncated mask figure object detection accuracy using supervised convnet noise introduced mask boundaries masks truncated amount data reduced. surprisingly representation maintains quality even large degradation. noise segment boundary simulates foreground leaking background vice-versa. introduce noise training cropped ground truth mask randomly either erode dilate mask using kernel ﬁxed size boundaries become noisier kernel size increases. truncation simulates case miss part object part object moves. specifically ground truth mask zero strip pixels corresponding ﬁxed percentage bounding area four sides evaluate representation trained noisy ground truth segments object detection using fast rcnn layers including conv frozen learned representation surprisingly resilient kinds degradation. even large systematic truncation large errors boundaries representation maintains quality. much data need? vary amount data available training evaluate resulting representation object detection using fast-rcnn conv layers frozen. results shown third plot figure performance drops signiﬁcantly amount training data reduced suggesting good representations need large amounts data. summary results suggest training segmentation leads strong features even imprecise object masks. however building good representation requires signiﬁcant amounts training data. observations strengthen case learning features unsupervised manner large unlabeled datasets. figure left right video frame output unlc train convnet output convnet. unlc able highlight moving object even potentially cluttered scenes often noisy sometimes fails nevertheless convnet still learn noisy data produce signiﬁcantly better smoother segmentations. idea behind motion segmentation single object moving respect background entire video pixels object move differently pixels background. analyzing optical therefore provide hints pixels belong foreground. however since part object might move frame information needs aggregated across multiple frames. adopt approach faktor irani unsupervised respect video segmentation utilizes edge detector trained labeled edge images order purely unsupervised method replace trained edge detector unsupervised superpixels. avoid confusion call implementation unlc. first unlc computes per-frame saliency based motion looking either pixels move mostly static frame frame contains signiﬁcant motion pixels move direction different dominant one. per-pixel saliency averaged superpixels next nearest neighbor graph computed superpixels video using location appearance features. finally uses nearest neighbor voting scheme propagate saliency across frames. figure examples segmentations produced convnet held images. convnet able identify motile object segment single frame. masks perfect capture general object shape. unlc often fails videos wild. sometimes assumption single moving object video satisﬁed especially long videos made multiple shots showing different objects. publicly available appearance-based shot detection method divide video shots unlc separately shot. videos wild also often resolution compression artifacts degrade resulting segmentations. experiments using strong supervision know approach robust noise. nevertheless since large video dataset comprises massive collection frames simply discard badly segmented frames based heuristics. speciﬁcally discard frames many pixels marked foreground; frames many pixels within frame border marked foreground. preliminary tests found results sensitive precise thresholds used. unlc videos yfccm contains videos. pruning ended videos. sampled frames shot video create dataset images slightly frames images imagenet. however note frames come fewer videos therefore correlated images imagenet. stress approach generating dataset completely unsupervised form supervised learning part pipeline. code segmentation pruning together automatically generated dataset frames segments made publicly available soon. motion segmentation approach state-ofthe-art seen noisy segments shown figure nevertheless representation quite resilient noise such improve particulars motion segmentation. before feed convnet cropped images jittered scale translation predict motile foreground object. since motion segmentation output noisy trust absolute foreground probabilities provides. instead convert trimap representation pixels probability marked negative samples probability marked positives remaining pixels marked don’t cares convnet trained logistic loss positive negative pixels; don’t care pixels ignored. similar techniques successfully explored earlier segmentation despite steps take good segments unlc output still noisy often grossly incorrect seen second column figure however systematic errors motion-based segments seen perturbations true latent segmentation. convnet ﬁnite capacity able noise perfectly might instead learn something closer underlying correct segmentation. positive evidence seen output trained convnet training images convnet correctly identiﬁes motile object rough shape leading smoother correct segmentation original motion segmentation. convnet also able generalize unseen images. figure shows output convnet frames davis fbms datasets used training. again able identify moving object rough shape single frame. evaluated human annotated segments datasets convnet’s output signiﬁcantly better unlc segmentation output shown below results conﬁrm earlier ﬁnding convnet able learn well even noisy often incorrect ground truth. however goal paper segmentation representation learning. evaluate learned representation next section. ﬁrst evaluate representation task object detection using fast r-cnn. crossvalidation pick appropriate learning rate method values finally train train test exactly once. multi-scale training testing discard difﬁcult objects training. present results convnet parameters frozen different extents. discussed section good representation work well initialization ﬁnetuning also convnet frozen. compare approach convnet representations produced recent prior work unsupervised learning publicly available models methods shown. like convnet representation models alexnet architecture differ minor details presence batch normalization layers presence grouped convolutions results shown figure table representation learned unsupervised motion segmentation performs better prior work unsupervised learning across scenarios. section contrast imagenet supervised representations representations learned previous unsupervised approaches show large decay performance layers frozen owing representation becoming highly speciﬁc pretext task. similar supervised approach trained segmentations coco unsupervised approach trained motion segmentation also shows stable performance layers frozen. thus unlike prior work unsupervised learning upper layers representation learn highlevel abstract concepts useful recognition. possible differences method prior work training data different domains control this retrained model frames video dataset variants perform similarly mean trained yfcc conv frozen compared imagenet version. conﬁrms different image sources explain gains. unsupervised jigsaw‡ kmeans egomotion inpainting tracking-gray sounds bigan colorization split-brain auto context context-videos† motion masks table object detection pascal using fast r-cnn various pretrained convnets. models trained train tested using consistent fast r-cnn settings. means training didn’t converge insufﬁcient data. approach achieves best performance majority settings. †doersch trained original context model using imagenet images. context-videos model obtained retraining approach video frames yfcc. experiment controls effect distribution training images shows image domain used training signiﬁcantly impact performance. ‡noroozi computationally intensive convnet architecture ﬁner stride conv preventing apples-to-apples comparisons. nevertheless model works signiﬁcantly worse representation either layers frozen case limited data comparable network ﬁnetuned full training data. figure results object detection using fast r-cnn. object detection results convnet representation frozen different extents. compare unsupervised supervised approaches. left using full training set. right using training images variation representation quality number training frames. methods also shown. context-videos representation doersch retrained video frames. note methods table imagenet train set. table show compare unsupervised supervised approaches task object detection training images. observe scenario actually hurts ﬁnetune entire network best setup leave layers frozen. approach provides best overall among representations recent unsupervised learning methods large margin. performance low-shot settings presented figure note spite strong performance relative prior unsupervised approaches representation learned without supervision video trails strongly supervised mask imagenet versions signiﬁcant margin. discuss following subsection. impact amount training data figure results image classiﬁcation single-image action classiﬁcation stanford actions semantic segmentation results shown convnet layers frozen different extents frames used. frames already better prior state-of-the-art trained million imagenet images figure full dataset accuracy increases substantially. logarithmic growth continues representation trained imagenet frames note frames video correlated. expect number could reduced algorithmic improvements. discussed section good representation generalize across tasks. show experiments tasks image classiﬁcation semantic image segmentation. image classiﬁcation test object action classiﬁcation. image classiﬁcation. experimented image classiﬁcation pascal stanford actions allow comparisons prior work used random crops training averaged scores crops testing details). minimally tuned hyperparameters validation used settings stanford actions. datasets trained different amounts ﬁne-tuning before. results ﬁrst plots figure semantic segmentation. fully convolutional networks semantic segmentation default hyperparameters pretrained convnet models ﬁnetuned union images train additional train released hariharan test removing overlapping images train. last plot figure shows performance different methods number layers ﬁnetuned varied. analysis. like object detection tasks require semantic knowledge. however object detection convnet given tight crop around target object input image classiﬁcation tasks entire image semantic segmentation involves running convnet sliding window locations. difference appears play major role. representation trained object crops similar setup object detection quite different setups figure mismatch negatively impact performance representation version trained motion segmentation strongly supervised version. mismatch also explain performance representation trained wang semantic segmentation. nevertheless convnet progressively frozen approach strong performer. layers until conv frozen representation better approaches action classiﬁcation second colorization image classiﬁcation semantic segmentation higher performance action classiﬁcation might fact video dataset many people various actions. presented simple intuitive approach unsupervised learning using segments low-level motion-based grouping train convnets. experiments show approach enables effective transfer especially computational data constraints limit amount task-speciﬁc tuning scaling larger video datasets allow improvements. noted figure network learns reﬁne noisy input segments. good example scenario convnets learn extract signal large amounts noisy data. combining reﬁned single-frame output convnet noisy motion cues extracted video lead better pseudo ground truth used convnet bootstrap itself. leave direction future work. figure results object detection pascal using fast r-cnn varying number images available ﬁnetuning. plot shows comparison different unsupervised learning methods number layers ﬁnetuned varied. different plots depict variation different amounts data available ﬁnetuning fast r-cnn data ﬁnetuning decreases actually better freeze layers. method works well across settings scales amount data decreases. layers frozen data limited method signiﬁcantly outperforms methods. suggests features learned higher layers model good recognition.", "year": 2016}