{"title": "EM Algorithms for Weighted-Data Clustering with Application to  Audio-Visual Scene Analysis", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Data clustering has received a lot of attention and numerous methods, algorithms and software packages are available. Among these techniques, parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization (EM). In this paper we propose a new mixture model that associates a weight with each observed point. We introduce the weighted-data Gaussian mixture and we derive two EM algorithms. The first one considers a fixed weight for each observation. The second one treats each weight as a random variable following a gamma distribution. We propose a model selection method based on a minimum message length criterion, provide a weight initialization strategy, and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques. We also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data, namely audio-visual scene analysis.", "text": "mean covariance gamma distribution univariate positive variable parameterized case mixtures t-distributions mixing coefﬁcients latent variable also introduced. distribution mixture gamma distributions accounts component-dependent clustering usually performed associating positive variable distributed observed point distributions depend observed data drawn i.i.d. variables distributed according t-mixture variants paper propose ﬁnite mixture model variable used weight account reliability observed independently assigned cluster. distribution gamma mixture anymore depend allow data point potentially treated differently. contrast mixtures t-distributions follows observed data independent identically distributed. introduce weighted-data gaussian mixture model distinguish cases namely weights known priori hence ﬁxed weights modeled variables hence iteratively updated given initial estimates. show case ﬁxed weights parameters estimated extension standard referred ﬁxed weighted-data algorithm consider general case weights treated random variables. model variables gamma distributions formally derive closed-form algorithm referred weighted-data algorithm m-step latter similar m-step fwd-em e-step considerably different posterior probabilities parameters posterior gamma distributions updated responsibilities computed using pearson type distribution recent discussion regarding distribution) also called arellano-valle bolfarine generalized tdistribution parameters posterior gamma distributions computed prior gamma parameters mahalanobis distance data mixture means. note weights play different role responsibilities. unlike responsibilities probabilities weights random variables take arbitrary positive values. posterior means used absolute measure relevance data. abstract—data clustering received attention numerous methods algorithms software packages available. among techniques parametric ﬁnite-mixture models play central role interesting mathematical properties existence maximum-likelihood estimators based expectation-maximization paper propose mixture model associates weight observed point. introduce weighted-data gaussian mixture derive algorithms. ﬁrst considers ﬁxed weight observation. second treats weight random variable following gamma distribution. propose model selection method based minimum message length criterion provide weight initialization strategy validate proposed algorithms comparing several state parametric non-parametric clustering techniques. also demonstrate effectiveness robustness proposed clustering technique presence heterogeneous data namely audio-visual scene analysis. expectation-maximization weighted-data clustering robust clustering outlier detection model selection minimum message length audio-visual fusion speaker localization. finding signiﬁcant groups data points central problem many ﬁelds. consequently clustering received attention many methods algorithms software packages available today. among techniques parametric ﬁnite mixture models play paramount role interesting mathematical properties well existence maximum likelihood estimators based expectation-maximization algorithms. ﬁnite gaussian mixture model choice extremely sensitive presence outliers. alternative robust models proposed statistical literature mixtures t-distributions numerous variants e.g. contrast gaussian case closed-form solution exists tdistribution tractability maintained gaussian scale mixture representation observed vector multivariate gaussian distribution typically outlying data point cluster center small weight still assigned signiﬁcant responsibility value closest cluster. responsibilities indicate cluster center closest close all. idea weighted-data clustering already proposed framework non-parametric clustering methods k-means spectral clustering e.g. methods generally propose incorporate prior information clustering process order prohibit atypical data contaminate clusters. idea modeling data weights random variables estimate proposed particular framework markovian brain image segmentation. shown speciﬁc expert knowledge needed dataweight distribution guide model towards satisfactory segmentation. variational proposed formulation closed form. paper build idea that instead relying prior information atypical data e.g. devise novel algorithm updates weight distributions. proposed method belongs robust clustering category mixture models observed data away cluster centers little inﬂuence estimation means covariances. important feature mixture based clustering methods perform model selection premise number components mixture corresponds number clusters data. traditionally model selection performed obtaining candidate models range values number components selected minimizing model selection criteria bayesian inference criterion minimum message length akaike’s information criteria cite disadvantage methods twofold. firstly whole candidates obtained problems associated running many times emerge. secondly provide number components optimally approximate density true number clusters present data. recently seems consensus among mixture model practitioners wellfounded computationally efﬁcient model selection strategy start large number components merge proposes practical algorithm starts large number components starts overestimated number components using merges hierarchically according entropy criterion. recently proposes similar method merges components based measuring pair-wise overlap. another trend handling issue ﬁnding proper number components consider bayesian non-parametric mixture models. allows implementation mixture models inﬁnite number components dirichlet process mixture models. inﬁnite gaussian mixture presented computationally intensive markov chain monte carlo implementation. ﬁrst glance igmm appear similar fwd-em. however algorithms quite different. igmm fully bayesian proposed fwd-em sense priors assumed parameters typically means covariance matrices. igmm implies student predictive distributions fwd-em involves gaussian distributions. recently ﬂexibility cluster shapes allowed considering inﬁnite mixture inﬁnite gaussian mixtures ﬂexibility however limited cluster composed sub-clusters identical shapes orientations alter performance approach. altogether igmm igmm designed handle outliers illustrated section viii figs. inﬁnite student mixture models also considered inference requires variational bayes approximation generates additional computational complexity. bayesian non-parametrics although promising techniques require fully bayesian setting. latter however induces additional complexity handling priors hyper-priors especially multi-variate context. contrast latent variable approach allows exact model selection therefore propose extend method weighted-data mixtures. formally derive criterion weighted-data mixture model plug criterion efﬁcient algorithm which starting large number components simultaneously estimates model parameters posterior probabilities weights optimal number components. also propose apply proposed weighted-data robust clustering method problem fusing auditory visual information. problem arises task e.g. detect person seen heard active speaker. single-modality signals vision-only audio-only often either weak ambiguous useful combine information different sensors e.g. cameras microphones. several difﬁculties associated audio-visual fusion data clustering perspective sensorial modalities live different spaces contaminated different types noise different distributions different spatiotemporal distributions perturbed different physical phenomena e.g. acoustic reverberations lighting conditions etc. example speaker face camera he/she silent emit speech he/she turns his/her face away camera. speech signals sparse spectro-temporal structure mixed sound sources music background noise. speaker faces totally partially occluded case face detection localization extremely unreliable. show proposed method well suited audio-visual clusters discriminate speaking silent people. simplest case weight values provided algorithm initialization either using prior knowledge estimated observations kept ﬁxed alternating expectation maximization steps. case expected complete-data log-likelihood denotes expectation respect distribution iteration consists steps namely evaluation posterior distribution given current model parameters weights maximization respect denotes equality constant depend canceling derivatives respect model parameters obtain following update formulae mixture proportions means covariances matrices variables introduced section wd-em described detail section section details deal unknown number clusters section addresses issue algorithm initialization. section viii proposed algorithms tested compared several parametric non-parametric clustering methods. section addresses clustering audio-visual data. section concludes paper. additional results videos available online. section present intuition formal deﬁnition proposed weighted-data model. random vector following multivariate gaussian distribution mean covariance rd×d namely notation weight indicating relevance observation intuitively higher weight stronger impact weight therefore incorporated model observing times. terms likelihood function equivalent raise power i.e. however latter probability distribution since integrate one. straightforward notice therefore plays role precision different datum subsequently write mixture parameters mixture coefﬁcients satisfying parameters k-th component number components. weighted-data refer model gaussian mixture model observed data weights associated assume independently drawn observed-data log-likelihood well known direct maximization log-likelihood function problematic case mixtures expected complete-data log-likelihood must considered instead. hence introduce hidden variables associated observed variables generated k-th component mixture. following ﬁrst consider ﬁxed number mixture components extend model unknown thus estimating number components data. posterior distribution namely gamma distribution conjugate prior precision gaussian distribution. therefore need compute parameters posterior gamma distribution estimating weights needed algorithm useful evaluate order fully characterize observations discriminate inliers outliers. first notice marginal posterior distribution mixture gamma distributions weights play role precisions. notable difference standard ﬁnite mixture models proposed model different weight hence different precision associated observation within bayesian formalism weights treated random variables rather ﬁxed advance previous case. since gaussian convenient choice prior conjugate prior precision known mean i.e. gamma distribution. ensures weight posteriors gamma distributions well. summarizing have section derive wd-em algorithm associated model weights treated random variables following gamma distribution assumed parameterized βi}. within framework expectation complete-data log-likelihood computed assignment weight variables used notation φn}. notice posterior distribution factorizes quantities right-hand side equation closed-form expressions. computation expressions leads sequential steps e-wstep e-z-step expectation step proposed algorithm. assumed number mixture components provided advance. assumption unrealistic real-world applications. section propose extend method algorithm proposed weighted-data clustering model. interesting feature model selection method require parameter estimation many different values would case bayesian information criterion instead algorithm starts large number components iteratively deletes components become irrelevant. starting large number components additional advantage making algorithm robust initialization. formally parameter estimation problem cast transmission encoding problem criterion minimize expected length message transmitted context observations parameters quantized ﬁnite precision transmission. quantization sets trade terms previous equation. indeed truncating high precision length long length short since parameters well data. conversely quantization coarse length short length long. optimal quantization step found means taylor approximation case optimization problem corresponding minimum message length criterion expected fisher −e{d information matrix denotes dimensionality model namely dimension parameter vector since minimization depend weight parameters omitted simplicity. particular case general case mixtures fisher information matrix cannot obtained analytically. indeed direct optimization log-likelihood lead closed-form solutions. nevertheless noticed complete upper bounds expected complete-data log-likelihood lower bounds loglikelihood. allows write following equivalent optimization problem inspection easily seen value decreases distance cluster centers observation increases. importantly evaluation enables outlier detection. indeed outlier expected clusters therefore small leading small value worth noticing possible using responsibilities since normalized deﬁnition therefore value absolute measure datum’s relevance relative measure parameter updates obtained canceling derivatives expected complete-data log-likelihood standard gaussian mixtures updates closedform expressions worth noticing m-step wd-em algorithm similar m-step fwd-em algorithm indeed iterative formulas identical formulas except ﬁxed weights replaced posterior means random weights algorithm described section iii). indeed remark minimization equivalent using symmetric improper dirichlet prior proportions exponent −m/. moreover since optimization function parameters gaussian components estimation formulas still hold. therefore need modify estimation mixture proportions namely operator veriﬁes whether k-th component supported data. components becomes weak i.e. required minimum support canobtained data component annihilated. words parameters estimated since need transmitting them. careful context since starting large value lead several empty components. order avoid singular situation adopt component-wise procedure proposed well. intuitively steps component moving next component. precisely running steps component parameters updated otherwise component annihilated rationale behind procedure that component annihilated probability mass immediately redistributed among remaining components. summarizing updates components whereas classical simultaneously updates components. proposed algorithm outlined algorithm practice upper lower number components khigh klow provided. iteration algorithm consists component-wise steps. needed components annihilated parameters updated accordingly relative length difference mml| case threshold |∆len lower current optimum message length i.e. parameters weights length saved θmin wmin lenmin respectively. order explore full range less populated component artiﬁcially annihilated again. complexity algorithm similar complexity algorithm exception step. however computationally intensive part step already achieved step. algorithms proposed section section section require proper initialization weights model parameters. kmeans algorithm used initial clustering values model parameters computed. section concentrate onto issue weight initialization. distributed model cannot considered classical mixture model. reason algorithm proposed cannot applied directly model. indeed proposed wd-gmm setting complete-fim fisher −e{d information matrix i-th observation respect parameter vector k-th component deﬁned fisher information matrix multinomial distribution namely evaluate |ic| diagonal matrix diag importantly main advantages methodology proposed complete freedom choose prior distribution parameters case inspired select following prior distributions parameters notice make sense πk’s allowed null. however current length coding framework point transmitting parameters empty component. therefore focus non-empty components namely components denote index non-empty components |k+| cardinality. rewrite interesting feature method constraint weights must positive. initial values depend expert prior knowledge experimentgoal-dependent. model ﬂexibility allows incorporation prior knowledge. absence prior information/knowledge propose data-driven initialization scheme make assumption densely sampled regions important sparsely sampled ones. note similar strategy could used wants reduce importance dense data give importance small groups data sparse data. euclidean distance denotes containing nearest neighbors positive scalar. experiments used simulated datasets real datasets. cases used case fwd-em algorithm weights thus initialized remain unchanged. however case wd-em algorithm weights modeled latent random variables drawn gamma distribution hence needs initial values parameters distribution namely using choose initialize parameters mean variance prior distribution respectively. proposed algorithms tested evaluated using eight datasets four simulated datasets four publicly available datasets widely used benchmarking clustering methods. main characteristics datasets summarized table simulated datasets designed evaluate robustness proposed method respect outliers. simulated inliers drawn gaussian mixtures simulated outliers drawn uniform distribution e.g. fig. datasets different cluster conﬁgurations terms separability shape compactness. eight datasets used following cluster scatter x∈ck number samples cluster cluster center value index means clusters respect scatter therefore discriminative power higher. since algorithms randomly initialized repeat experiment times compute mean standard deviation index experiment. table summarizes results obtained mnist letter recognition datasets. proposed wd-em method yields best results data igmm method yields best results mnist data. non-parametric methods k-means ncut yield best second best results letter recognition data. completeness also provide micro scores obtained mnist letter recognition datasets table iii. based classiﬁcation score proposed wd-em method yields best results data igmm yields best results letter recognition data igmm method yields best results mnist data. comparison also shows igmm gmm+u yield similar scores. letter recognition contains single-letter images generated randomly distorting images uppercase letters different commercial fonts letter/image described features. dataset available machine learning repository. k-means standard k-means algorithm; kk-means kernel k-means algorithm ncut spectral clustering algorithm hierarchical agglomerative clustering algoinitialization. mixture-based algorithms wd-em fwd-em gmm+u fm-umst igmm igmm start proportions means covariances estimated clusters provided k-means. latter randomly initialized several times good initialization. furthermore algorithms wd-em fwd-em gmm+u table results obtained mnist letter recognition datasets. clustering scores correspond davies-bouldin index. best results shown underlined bold second best results shown bold. proposed method yields best results datasets igmm yields best results mnist dataset. interestingly non-parametric methods yield excellent results letter recognition. table micro scores obtained real data sets number parenthesis indicates standard deviation repetitions. based classiﬁcation score igmm yields best result. interesting feature proposed weighted-data clustering algorithms robustness ﬁnding good clusters presence outliers. illustrate ability large number experiments adding outliers drawn uniform distribution four simulated datasets e.g. table fig. comparison wd-em fwd-em state-of-art clustering techniques mentioned above different percentages outliers provided. easily observed tables gmm+u performs extremely well presence outliers surprising since simulated outliers drawn uniform distribution. overall proposed wd-em method second best performing method. notice good performance ncut method sim-overlapped data. among methods gmm+u wd-em offer possibility characterize outliers using different strategies.the gmm+u model simply pulls outlier class based posterior probabilities. wd-em algorithm iteratively updates posterior probabilities weights ﬁnal posteriors allow implement simple outlier detection mechanism. another important remark wd-em systematically outperforms fwd-em fully justiﬁes proposed weighted-data model. fig. shows results ﬁtting mixture models sim-mixed data drawn gaussian mixture contaminated outliers drawn uniform distribution. plots show igmm igmm components corresponding data clusters also component onto outliers roughly centered data set. section illustrate effectiveness method deal audio-visual data belong heterogenous type data i.e. gathered different sensors different noise statistics different sources errors. challenges clustering audio-visual data enumerated section prior clustering needs represent audio visual observations euclidean space e.g. fig. without loss generality adopt sound-source localization method performs direction arrival estimation followed mapping estimated sound-source direction onto image plane estimate therefore corresponds pixel location image plane. visual features upper-body detector provides approximate localization human heads followed localization using facial landmark detection rationale combining upper-body detection facial landmark localization that altogether yields detection localization algorithm much robust head pose vast majority face detection methods. denote auditory visual observations respectively. initialize weight variables following way. auditory sample given high initial weight many visual samples neighbors vj∈v exp/σ). visual weights initialized aj∈a exp/σ). fig. audio-visual data acquisition alignment. leftright-microphone signals. temporal segment outlined red. middle binaural spectrogram corresponds outlined segment. spectrogram composed binaural vectors associated audio frame bottom video frames associated segment. sound-source direction arrival extracted binaural vector mapped onto image plane hence green image plane corresponds doa. cocktail party sequence e.g. ﬁfth sixth rows fig. consists four persons engaged informal dialog. persons wander around turn heads towards active speaker; occasionally persons speak simultaneously. moreover speakers always face camera hence face detection/localization unreliable. visual data gathered single camera auditory data gathered microphones plugged ears acoustic dummy head referred binaural audition. visual data recorded video frames second auditory data gathered processed following way. first short-time fourier transform applied leftright-microphone signals sampled khz. second left right spectrograms thus obtained combined yield binaural spectrogram sound-source estimated. spectrogram composed frequency bins obtained applying stft sliding window width shifted along signal hops. audio frame frequency bins associated window hence audio frames second visual audio frames grouped temporal segments width hence visual frames audio frames segment. already mentioned follow method extract sound-source audio frame. order increase robustness audio localization voice activity detector ﬁrst applied frame frames estimates associated them. average audio observations segment. sequence contains segments sequence contains segments sequence contains segments. left hand sides fig. show central frame segment visual features auditory features available within segment. tested proposed wd-em algorithm audiovisual data well gmm+u fm-umst algorithms. chose compare method methods following reasons. firstly three methods based ﬁnite mixtures hence model selection criterion estimate number components mixture best approximates clusters data. important since number persons active speakers among persons known advance. secondly demonstrated previous section three methods yield robust clustering presence outliers. wd-em uses criterion model selection described section implemented model selection criterion based optimally select number components gmm+u fm-umst. algorithm yields optimal number components audio-visual segment contain sufﬁcient number audio visual observations component associated active speaker. therefore apply simple two-step strategy ﬁrstly decide whether component audio-visual audio-only visual-only secondly select best audio-visual components. total number visual audio observations segment. start assigning observation component number audio visual observations associated component min{nk measure audio-visual relevance component. component corresponds active speaker ﬁxed threshold. fig. shows examples applying wd-em gmm+u fm-umst algorithms three sequences. notice that visual observations accurate form small lumps around moving lips speaker audio observations noisy different statistics; presence reverberations sound sources computer fans. ground-truth active speaker shown yellow frame. data clusters obtained three methods shown ellipses. blue disk around cluster center designates audio-visual cluster. altogether notice proposed method outperforms methods. interesting feature wd-em weights give importance accurate visual data hence audiovisual cluster centers pulled towards visual data quantify performance three methods carefully annotated data. segment identiﬁed active speaker precisely located speaker’s lips. ground-truth location. assign component computing maximum responsibility assigned audio-visual cluster active speaker said correctly detected posterior probability equal greater number components. table summarizes results obtained three methods. presented weighted-data gaussian mixture model. derived maximum-likelihood formulation devised algorithms uses ﬁxed weights another weights modeled random varifig. results obtained fake speaker moving speaker cocktail party sequences. ﬁrst column shows audio visual observations well yellow bounding shows ground-truth active speaker. second third fourth columns show mixture components obtained wd-em gmm+u fmumst methods respectively. blue disks mark components correspond correct detections active speakers namely whenever overlap component ground-truth bounding box. ables ﬁrst algorithm appears straightforward generalization standard gaussian mixtures second complex structure. showed expectation maximization steps proposed wd-em admit closed-form solutions hence algorithm extremely efﬁcient. moreover wd-em performs much better fwd-em fully justiﬁes proposed generative probabilistic model weights. extended mml-based model selection criterion proposed weighted-data gaussian mixture model proposed algorithm ﬁnds optimal number components data. interestingly wd-em algorithm compares favorably several state-of-the-art parametric nonparametric clustering methods performs particularly well presence large number outliers e.g. outliers. hence proposed formulation belongs robust category clustering methods. also applied wd-em problem clustering heterogenous/multimodal data sets audio-visual data. brieﬂy described audio-visual fusion problem cast challenging audio-visual clustering problem e.g. associate human faces speech signals detect localize active speakers complex audio-visual scenes. showed proposed algorithm yields better audio-visual clustering results ﬁnite-mixture models reasons robust noise outliers allows crossmodal weighting scheme. although implemented paper proposed model many interesting features dealing multimodal data enables balance importance modalities emphasize modality prior information might available example giving high weight priors visual data corresponding face/lip localization. forbes wraith family multivariate heavy-tailed distributions variable marginal amounts tailweight application robust clustering statistics computing vol. november feldman schulman data reduction weighted outlierresistant clustering proceedings twenty-third annual acmsiam symposium discrete algorithms. siam forbes doyle garcia-lorenzo barillot dojat weighted multi-sequence markov model brain lesion segmentation proceedings international conference artiﬁcial intelligence statistics vol. sardinia italy street wolberg mangasarian nuclear feature extraction breast tumor diagnosis is&t/spie’s symposium electronic imaging science technology. international society optics photonics frey slate letter recognition using holland-style adaptive classiﬁers machine learning vol. bishop pattern recognition machine learning. springer dhillon guan kulis kernel k-means spectral clustering normalized cuts proceedings tenth international conference knowledge discovery data mining. zhao karypis evaluation hierarchical clustering algorithms document datasets proceedings eleventh international conference information knowledge management. deleforge horaud schechner girin co-localization audio sources images using binaural features locally-linear regression ieee transactions audio speech language processing vol. apr. ferrari m.-j. marin-jimenez zisserman progressive search space reduction human pose estimation proceedings ieee conference computer vision pattern recognition radu horaud received b.sc. degree electrical engineering m.sc. degree control engineering ph.d. degree computer science institut national polytechnique grenoble france. currently holds position director research inria grenoble montbonnot saintmartin france founder head perception team. research interests include computer vision machine learning audio signal processing audiovisual analysis robotics. area editor elsevier computer vision image understanding member advisory board sage international journal robotics research associate editor kluwer international journal computer vision. radu horaud awarded advanced grant project vision hearing action israel dejene gebru received b.sc. degree computer engineering addis ababa university ethiopia m.sc. degree telecommunication engineering university trento italy worked cooperation engineer. currently candidate universit´e joseph fourier works perception team inria grenoble rhˆone-alpes france. research interests include development statistical methods machine learning algorithms multimodal signal analysis speaker tracking computer vision problems. particularly interested joint processing audio visual data applications human-robot interaction. xavier alameda-pineda received m.sc. degree mathematics telecommunications engineering universitat polit`ecnica catalunya barcelonatech respectively m.sc. degree computer science universit´e joseph fourier grenoble ph.d. degree mathematics/computer science universit´e joseph fourier worked towards ph.d. degree perception team inria grenoble rhˆone-alpes. currently holds postdoctoral position multimodal human understanding group university trento. research interests machine learning signal processing scene understanding speaker diaritzation tracking sound source separation behavior analysis. florence forbes received b.sc. m.sc. degrees computer science applied mathematics ecole nationale sup´erieure d’informatique math´ematiques appliqu´ees grenoble france degree applied probabilities university joseph fourier grenoble france. since research scientist inria currently holds position director research. founded mistis team team head since research activities include bayesian", "year": 2015}