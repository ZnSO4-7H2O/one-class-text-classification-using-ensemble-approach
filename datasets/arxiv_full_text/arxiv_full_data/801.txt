{"title": "Surprisal-Driven Zoneout", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.", "text": "proven general solution problem lowest kolmogorov complexity code short possible. terms neural networks could measure complexity solution counting number active neurons. according redundancy-reducing hypothesis neurons within brain code messages using different number impulses. indicates probable events assigned codes fewer impulses order minimize energy expenditure words frequently occuring patterns lower level neurons trigger sparser activations higher level ones. keeping mind focused problem adaptive regularization i.e. minimization number neurons activated depending novelty current input. zoneout recently proposed regularizer recurrent neural networks shown success variety benchmark datasets zoneout regularizes zoning activations freezing state time step ﬁxed probability. mitigates unstable behavior standard dropout applied recurrent connections. however since zoneout rate ﬁxed beforehand decide priori prefer faster convergence higher stochasticity propose novel method regularization recurrent neural networks called suprisal-driven zoneout. method states zoneout suprisal small. thus regularization adaptive input-driven per-neuron basis. demonstrate effectiveness idea achieving state-of-the-art bits character hutter prize wikipedia dataset signiﬁcantly reducing best known highly-engineered compression methods. important part learning beyond simple memorization general dependencies data possible. sequences information means looking concise representation things change time. common modeling recurrent neural networks whose parameters thought transition operator markov chain. training process learning transition operator. generally speaking temporal dynamics different timescales intuitively challenge keep track long-term dependencies accurately modeling short-term processes well. long-short term memory architecture type proven exceptionally well suited learning longterm dependencies widely used model sequence data. learned parameterized gating mechanisms control retrieved stored lstm’s state timestep multiplicative interactions lstm’s state. many approaches capturing temporal dynamics different timescales e.g. neural networks kalman ﬁlters clockwork rnns narx cases ﬁrst corpus used training next validation last reporting test accuracy. iteration sequences length randomly selected. learning algorithm used adadelta learning rate weights initialized using so-called xavier initialization sequence length bptt batch size experiments layer lstm cells used. states carried entire sequence emulating full bptt. forget bias initially parameters zero. algorithm written cuda titan weeks. remark surprisal-driven feedback sometimes wrongly characterized ’dynamic evaluation’ method. incorrect following reasons never actually sees test data training. adapt weights testing. evaluation procedure exactly using standard inputs. therefore fair compare ’static’ methods. whereas would like able memory cell according learning phase i.e. lower initially higher later prevent memorization/unnecessary activation. decided surpisal-driven feedback since gives measurement current progress learning. provided feedback loop enables change zoneout rate online within scope given cell allowing zoneout rate adapt current information. learning progresses activations cell become less frequent time iterations skip memorization thus proposed mechanism fact enables different memory cells operate different time scales. idea illustrated fig. main contribution paper introduction surprisal-driven adaptive zoneout neuron encouraged active rarely possible preferred state operation. motivation behind idea complexity codes provide better generalization. observed substantial improvements enwik linux datasets. hypothesis presence memorizable tags nestedness dataset ideal learning suprisaldriven zoneout. patterns timestamp long periods spaces represented single impulse approach zoning entirely pattern. without adaptive zoneout would controlled entirely learned gates suprisal allows quick adaptation pattern. shows side side comparison version without adaptive zoneout demonstrating fact dynamic span memory cells greater adaptive zoneout used. furthermore show activations using adaptive zoneout fact sparser without supports intuition inner workings network. especially interesting observation fact adaptive zoneout seems help separate instructions appear mixed otherwise similar approach problem called hierarchical multiscale recurrent neural networks main difference design explicit hierarchy levels instead allowing neuron operate arbitrary timescale depending zoneout rate. syntactic patterns enwik linux datasets highly nested. example believe order learn complex structure need distributed representations every neuron operating arbitrary time scale independent another. hardcoded hierarchical architecture problems solving problem. proposed surprisal-driven zoneout appeared ﬂexible mechanism control activation given cell. empirically method performs extremely well enwik linux datasets. would like explore variations suprisal-driven zoneout state cell. another interesting direction pursue connection sparse coding using suprisal-driven zoneout lstm’s cell contents sparsely revealed time potentially resulting information used effectively. chung g¨ulc¸ehre bengio. gated feedback recurrent neural networks. corr abs/. †best known compressor http//mattmahoney.net/dc/text.html ‡our implementation glorot bengio. understanding difﬁculty protraining deep feedforward neural networks. ceedings international conference artiﬁcial intelligence statistics society artiﬁcial intelligence statistics krueger maharaj kram´ar pezeshki ballas goyal bengio larochelle courville pal. zoneout regularizing rnns randomly preserving hidden activations. corr abs/. http//arxiv.org/ abs/.. sutskever martens hinton. generating text recurrent neural networks. getoor scheffer editors proceedings international conference machine learning icml pages york june acm. isbn ----.", "year": 2016}