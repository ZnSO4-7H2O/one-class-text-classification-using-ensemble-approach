{"title": "An Empirical Comparison of Algorithms for Aggregating Expert Predictions", "tag": ["cs.AI", "cs.LG"], "abstract": "Predicting the outcomes of future events is a challenging problem for which a variety of solution methods have been explored and attempted. We present an empirical comparison of a variety of online and offline adaptive algorithms for aggregating experts' predictions of the outcomes of five years of US National Football League games (1319 games) using expert probability elicitations obtained from an Internet contest called ProbabilitySports. We find that it is difficult to improve over simple averaging of the predictions in terms of prediction accuracy, but that there is room for improvement in quadratic loss. Somewhat surprisingly, a Bayesian estimation algorithm which estimates the variance of each expert's prediction exhibits the most consistent superior performance over simple averaging among our collection of algorithms.", "text": "predicting outcomes future events challenging problem variety solution methods explored attempted. present empirical comparison variety online oine adaptive algorithms aggregating experts’ predictions outcomes years national football league games using expert probability elicitations obtained internet contest called probabilitysports. dicult improve simple averaging predictions terms prediction accuracy room improvement quadratic loss. somewhat surprisingly bayesian estimation algorithm estimates variance expert’s prediction exhibits consistent superior performance simple averaging among collection algorithms. consider problem predicting outcomes future events forcasting weather stock markets political races sports games. prediction problems often access extra information form prediction probabilities various possible outcomes group \\experts\". task aggregate information eectively predict future outcomes. number dierent methods studied attempted prediction tasks including information markets polling prediction methods based machine learning belief aggregation methods main predicting professional football games using actual expert predictions online contest called probability sports present comparison number algorithms task ranging baseline simple averaging expert predictions sophisticated machine learning algorithms including experts algorithms novel algocesa-bianchi rithm refer \\variance\" algorithm. variance algorithm bayesian estimation algorithm models expert predictions gaussians centered around actual outcome’s probability expert’s prediction dierent variance algorithm attemps estimate variances data seen far. probabilitiysports challening domain. example observe variety algorithms experimented beat baseline simple averaging experts’ probabilities terms prediction accuracy experiments provide evidence room improvement average better predicting binary outcomes thus problem purely predicting better probabilities. further observe simple average algorithm competitive variety adaptive algorithms experimented quadratic loss criterion. however cross-validation experiments statistical signicance tests years worth data indicate algorithms superior average quadratic loss. particular variance algorithm appears best consistently beating average experiments. prediction problems often dicult small statistically signicant improvements predictions make signicant dierence long run. paper organized follows. section dene problem describe data. section describes methods section presents results together analyses discussions section discusses related work. prediction task forecast outcomes sequence events. events binary represented total number time steps particular time point. predictions experts event represented probability according expert. measure quality predictions made algorithm expert shall loss function. algorithm supposed perform better loss function loss function event depends outcome predicted probability denoted total loss function losses yt). common loss functions include absolute loss quadratic loss loss depending loss function expert’s strategy vary. important property quadratic loss functions experts’ best strategy reveal true beliefs shall focus quadratic loss function assessing quality probability predictions. data worked collected probability sports website game site follows national football league games leagues year. season participants make predictions outcomes games. round based outcomes actual game receive score according quadratic scoring rule. cumulative score season scores individual games. incentive season participants receive prizes. become fairly popular site; competition participants. website uses quadratic scoring rule given predicted probability outcome. scoring rule scaled inverse quadratic loss function described earlier. thus experts trying maximize scores best strategy give true beliefs. makes data close ideal testbed experiments. treated predictions participants game expert advice prediction algorithms. additional information like previous records teams participating game. data available football games onward. total number games seasons season. figure shows ranked nal) scores experts. median scores years respectively averages ---- respectively. basic premise experts algorithm cesabianchi predict according weighted average experts’ advice. weights associated experts changed dynamically based ongoing performance. briey state algorithm completeness. fact even necessary prediction function every round. guarantees experts algorithm hold long predictions made algorithm always within range specied bounds. weights updated according multiplicative update rule; weight expert weight whose prediction true value update function used provided satises bounds weights measure relative credibility experts. experimented number variants algorithm based prediction update functions used method handling data. tried three dierent prediction functions vovk’s function lnr+r)+lnr)+r) piece wise linear function satises bounds specied analysis experts algorithm analysis seems work reasonably well nevertheless). algorithm specied assumes every expert provides advice every round play. dataset hand contained missing data experts provide predictions rounds. handle problem treat missing prediction prediction note prediction minimizes loss averaged binary outcomes. another possibility advice) take experts advice consideration making prediction. updating weights modify weights experts didn’t participate round relative weight stays same. experts’ predictions samples gaussian distribution centered around true probability event mean experts’ predictions converge true probability increase number experts. however experts behave fashion. example experts aggressive others informed others etc. capture notion assuming gaussian expert variance event associated \\true\" probability outcome drawn. expert’s prediction event assumed drawn gaussian distribution centered around variance since wants minimize quadratic loss function predictor’s best strategy predict true probability case knew variance experts true probability computed maximizes likelihood observed experts predictions. know start equal variances experts equation true probabilities. given true probabilities events compute variances using equation repeat procedure. reduce time complexity algorithm event initialize variances ones calculated previous event. thus starting procedure priori estimates experts’ variances alternate computations better better estimates variances true probabilities. experts algorithm viewed online thus single pass machine learning algorithm adapts experts weights \\touching\" instance also experimented variety \\batch\" learning algorithms necessarily single pass multi-pass versions perceptron winnow algorithms attempt minimize objective function error linear nonlinear support vector machines well algorithms decision trees ensemble methods evaluated methods prediction accuracy performance quadratic loss performance online setting cross validation setting unfortunately none standard classication methods performed better simple averaging either accuracy quadratic loss here describe batch learning algorithm exponentiated gradient based exponential updating technique performed competitive experts algorithms appears beat simple averaging cross-validation experiments. gradient designed minimize quadratic loss every pass training data algorithm updates expert weights every instance using following weight update formula number variations basic algorithm e.g. whether randomize orderings instances whether continue passes error longer lowered whether stop number passes choice parameter values experimented variations seasons found following settings perform satisfactory number passes tting). learning rate ideally parameters could dynamically extensive crossvalidation. instances visited chronological order. pass error current weights computed best predictor across dierent passes selected predict probability next game. randomizing initial weights experts order instances visited found insignicant eects result inferior performance. interestingly forming committee training bootstrap samples training data adjust possible overtting appear help. also experimented variations additive rather exponential updates resulted inferior performance possibly expert weights becoming negative standard additive updates search space unmodied additive update algorithms allows negative weights appropriate. explain inferior performance svms well note variants additive updates ensure expert weights remain non-negative prediction markets betting markets shown eectively aggregate opinions traders provide accurate forecasts. implement simulated information market agents market correspond experts. agent prior belief equal prediction given expert logarithmic utility money. game agents sell security paying team wins. agents reach competitive equilibrium supply high-belief agents meets demand low-belief agents. moreover simulate agents learning market setting posterior belief equilibrium average prior belief equilibrium price agents gain lose money round; agents accurate tend gain money. additional money reected back utility function agents money tend risk money thus higher weight future. aggregate prediction taken equilibrium price market case logarithmic utility happens form wealth-weighted average since accurate agents tend accrue wealth market simulation thought variant expert algorithm. note however market simulation satisfy worst-case bounds expert algorithms. agent belief wealth incorrect eliminated future rounds. clearly adversary would eliminate expert round make expert informative predictor round even though market worst-case guarantee still make reasonable expert algorithm terms average case behavior. approach obtaining better probability outputs seek learning algorithms well accuracy objective i.e. minimizing misclassication rate extract probabilities algorithms example subsampling much work obtaining robust classication algorithms attractive approach. however online crossvalidation experiments observed evidence machine learning algorithms tested well probability prediction algorithms reported paper consistently better zero-one error performance simple linear averaging figure presents zero-one errors four years several methods. linear svms numbers correspond cross-validation used best parameter settings observed held-out data note zero-one errors average best expert would whether cross-validation measure online variant simple averaging expert ignore \\training data\". standard deviation erent held-out data samples) simple averaging validation experiments revealed many learning algorithms cantly altered) inferior task either accuracy quadratic loss. case even given fair amount training instances crossvalidation experiments report performance online experiments except gradient algorithm. furthermore experiments suggested terms accuracy much gained beyond simple averaging it’s even clear expert decidedly beats average quadratic scoring criterion beats average error hand cross-validation experiments also suggested algorithms beat simple averaging quadratic scoring criterion. average obtains score around season idea room improvement note example even method error almost perfect outputting probabilities score game season could reach could obtain close points games predicts accurately outputing condent probability roughly many games remaining could output probability close also note conservative method would output probability according roughly error rate average i.e. event would output probability whenever probability given average otherwise would underperform obtain score roughly period. fig. shows scores obtained various algorithms score expert. algorithms except average adaptive i.e. training games played adjust experts’ weights. average variance algorithms require parameters. remaining algorithms used years selection good parameter values number \\average refers variant time point scoring experts point used give prediciton next game. similarly variance figure scores data dierent years. expert refers missing data variant experts algorithm. average competitive adaptive adaptive algorithms variance algorithm consistently outperforms scores ncaa data. foremost observation expert beats algorihms score much greater scores obtained various strategies. also worth noting years algorithms come within positions season. secondly average performs considerably well compared adaptive algorithms considering simplicity. furthermore sees season adaptive algorithms beat average fairly signicantly goes subsequent years even though number participating experts increases every year. perhaps implies experts population getting better competitive whole average median scores experts negative question arises average relatively well. reason poor individual scores good average scores lies fact experts well-calibrated i.e. often predict well expert gives extreme probabilties suciently many games looses many points poor score. however even simple average experts ended score season yields scores years respectively. positive scores rank fairly high corresponding seasons implies average every game \\good\" predictions outweigh \\bad\" ones. thus averaging procedure eective smoothing extreme probabilities turning individual negative scores competitive overall positive population scores. within variants experts algorithm update function appeared superior others. prediction functions vovk’s function piecewise linear seem comparable performance vovk’s function. signicant advantage obtained using missing data variant experts algorithm relative weight expert changed rounds participate. however none experts algorithm consistently better average every season. many cases attributed fact many aggressive experts receive high weights initial games following experts future rounds results losses gains. variance algorithm seems good performance around. also conclude variance algorithm consistently outperforms average. conducted sign test years follows every game recorded whether variance gets higher score average found number wins variance higher losses signicance level years. thus believe that algorithms tested domain variance consistent best performing algorithm sinsince number experts considerably total number games algorithms enough data train hence algorithms multi-year periods consider experts played years period. along increasing number events side-eect number experts reduced around figure shows results experiments periods variance average algorithms beat scoring expert three cases conrming intuition. participants game tradesports newsfutures online information market sites people trade securities events. widespread compelling empirical evidence prices securities markets good estimators probabilities events represent season markets introduced participants probability sports study servanschrieber prices securities games markets entered predictions game. markets nished close coming sixth eighth places scores respectively. variance algorithm’s score competitive markets results additional years needed better comparison. also applied algorithms ncaa basketball data available site. data includes playo games thus number games much less case variance algorithm also gives best results variance beats average experiments variance algorithm motivated obervation simple averaging expert predictions competitive algorithms based core assumption every expert’s prediction sample distribution centered around \\true probability\" event. made number additional simplifying assumptions derivation example experts distributions gaussians change time experts independent. lifting assumptions lead superior performance variance algorithm. perhaps peculiar aspect variance algorithm take outcomes previous games experts’ performances account variance successful algorithm tested domain. variance algorithm require parameters others experts algorithms exp. gradient require number choices made. however experts algorithms often carry worst-case guarantees fairly versatile. obviously success variance depends heavily accuracy core assumption. possible experts algorithm several baseline aggregation algorithms lead improved reliable performance. opinion pool mathematical aggregation function combining expert beliefs example weighted algebraic geometric average. opinion pools usually justied axiomatic basis computational expert algorithms hand typically evaluated according worst-case performance. studies belief aggregation empirical components. abramson simulate distribution experts’ beliefs guassian around \\true\" probability test several opinion pools concluding weighted average works best. three recent papers leverage probabilitysports data. servan-schreiber compare accuracy real-money play-money prediction markets additional \"contestants\" probabilitysports contest. wolfers zitzewitz derive number models prediction market prices interpreted functions expert beliefs show probabilitysports data model quite well. chen take approach similar ours comparing dierent belief aggregation methods data. focus compare prediction markets market data study limited data. tested weighted algebraic geometric averages various types; examined wider variety machine learning methods. conclusion simple average accurate prediction market prices. paper evidence that although hard beat average algorithm consistently outperforms average. yond achievable baseline simple averaging dicult task. however experiments provide evidence variance algorithm yields consistent superior performance compared algorithms. future directions include extensions variance algorithm example estimating experts’ biases addition variances dropping independence assumption weighting recent history more well developing better understanding reasons superior performance validity core assumption. would also like experiment algorithmic ideas particular algorithms directly attempt minimize quadratic loss training data using appropriate regularization constraints. taking contextual information account division identity teams playing game also boost performance. probabilitysports realistic challenging domain evaluation prediction algorithms particular comparison algorithms aggregating expert predictions. data accumulated upcoming years value collection. plan make current data available repository", "year": 2012}