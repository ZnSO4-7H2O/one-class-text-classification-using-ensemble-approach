{"title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence  Labelling", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.", "text": "existing sequence labelling models rely ﬁxed decomposition target sequence sequence basic units. methods suffer major drawbacks basic units ﬁxed words characters phonemes speech recognition decomposition target sequences ﬁxed. drawbacks usually result sub-optimal performance modeling sequences. paper extend popular loss criterion alleviate limitations propose loss function called gram-ctc. preserving advantages gram-ctc automatically learns best basic units well suitable decomposition target sequences. unlike gram-ctc allows model output variable number characters time step enables model capture longer term dependency improves computational efﬁciency. demonstrate proposed gram-ctc improves terms performance efﬁciency large vocabulary speech recognition task multiple scales data gram-ctc outperform state-of-the-art standard speech benchmark. introduction recent years explosion interest sequence labelling tasks. connectionist temporal classiﬁcation loss sequenceto-sequence models present powerful approaches multiple applications automatic speech recognition machine translation parsing methods based ﬁxed carefully chosen basic units words phonemes characters ﬁxed pre-determined decomposition target sequences basic units. preconditions greatly simplify problems especially training processes also strict unnecessary constraints usually lead suboptimal solutions. models especially harmed ﬁxed basic units target space build independence assumption successive outputs space assumption often violated practice. problem ﬁxed basic units obvious really hard impossible determine optimal basic units beforehand. example english words basic units need deal large vocabulary-sized softmax well rare words data sparsity problem. hand characters basic units model forced learn complex rules english spelling pronunciation. example \"oh\" sound spelled following ways depending word occurs ough easily model commonly co-occuring grams together impossible give roughly equal probability many possible spellings transcribing unseen words. speech recognition systems model phonemes sub-phoneme units senones around problems. similarly state-of-the-art neural machine translation systems pre-segmented word pieces aiming best worlds. reality groups characters typically cohesive units many tasks. task words decomposed groups characters associated sound machine translation task values decomposing words root words extensions since information already available training data perhaps better model ﬁgure itself. time raises another import question decompose target sequence basic units? coupled problem automatic selection basic units thus also better model determine. recently interesting attempts directions seqseq framework. example chan proposed latent sequence decomposition decompose target sequences variable length units function input sequence output sequence. work propose gram-ctc strictly general version automatically seek best basic units training data called grams automatically decompose target sequences sequences grams. sequence prediction cross entropy training seen special case loss ﬁxed alignment seen special case gram-ctc ﬁxed decomposition target sequences. since loss function applied many seqseq tasks enable automatic selection grams decomposition target sequences without modifying underlying networks. extensive experiments multiple scales data validate gram-ctc improve terms performance efﬁciency using gram-ctc models outperform state-of-the-arts standard speech benchmarks. related work basic text units previous works utilized text prediction tasks generally divided categories handcrafted ones learning-based ones. hand-crafted basic units. fixed sets characters word-pieces words phonemes widely used basic units text prediction drawbacks. using ﬁxed deterministic decompositions text sequences deﬁnes prior necessarily optimal end-to-end learning. word-segmented models remove component learning spell thus enable direct optimization towards reducing word error rate however models suffer handle large vocabulary out-of-vocabulary words data sparsity problems using characters results much smaller vocabularies requires much longer contexts compared using words word-pieces poses challenge composing characters words word-pieces middle-ground words characters providing good trade-off vocabulary size context size performance using word pieces sensitive choice word-piece decomposition. task phonemes popular past decades eases acoustic modeling good results reported phonemic models however introduces uncertainties mapping phonemes words decoding becomes less robust especially accented speech data. learning-based basic units. recently attempts made learn basic unit sets automatically. proposed hybrid wordcharacter model translates mostly word level consults character components rare words. chan proposed latent sequence decompositions framework decomposes target sequences variable length-ed basic units function input sequence output sequence. exist earlier works unit discovery task standard problem solutions task degenerate solutions i.e. predicting full corpus probability start. often bayesian priors minimum description length constraints used remedy this. since alignment information marginalizes possible alignments. tries maximize input represent valid alignment. example size input output ‘hi’ whose length three possible alignments ‘-hi’ ‘h-i’ ‘hi-’ represents blank. details please refer original paper gram-ctc basic units ﬁxed desirable applications. generalize considering sequence basic units called gram whole usually reasonable many applications. figure illustration states forward-backward transitions label ‘cat’. uni-grams bi-grams english alphabet. valid states label ‘cat’ listed left. states transitions common vanilla gram-ctc black unique gram-ctc orange. general extension collapses back valid transition example transition termine transitions states adjacent time steps figure many-to-one mapping denote note rules adopted general idea presented paper depend speciﬁc rules. target sequence represents paths mapped then equation allows training sequence labeling models without alignment information using loss because marginalizes possible alignments training. gram-ctc uses effect enable model marginalize alignments also decompositions target sequence. note target sequence paths ctc. times valid states time step state valid transition states previous time step. original method thus special case gram-ctc quadratic increase complexity algorithm trivial assert trivial increase overall training time typical neural networks computation time dominated neural networks themselves. additionally algorithm extends generally arbitrary need possible n-grams length n-grams basic units target sequence length longest gram gram-ctc network softmax output layer |g|+ units probability grams additional symbol blank. simplify problem also assume input sequence length sequence network outputs denote probability k-th gram time index grams {blank} case refer elements paths denote represents possible alignment input output. difference word target sequence decomposed different sequences grams. example word ‘hello’ decomposed sequence also decomposed sequence ‘he’ ‘ll’ this valid decompositions target sequences since gram-ctc ﬁgure ideal decomposition target sequences grams training condition guarantees least valid decomposition every target sequence. states problem solve future states reusing solutions earlier states. case state must contain information required identify valid extensions incomplete path collapsing function eventually collapse complete back gram-ctc done collapsing last element path therefore state tuple ﬁrst item collapsed path representing preﬁx target label sequence length last gram used making preﬁx. valid means blank used. denote gram state readability further shorten state corresponding gram denoted positions ﬁrst character last character denoted respectively. dynamic programming dealing sequence states state sequence corresponding gram sequences unique denoted figure illustrates partially dynamic programming process target sequence ‘cat’. suppose contains possible uni-grams bi-grams. thus character ‘cat’ three possible states associated current character bi-gram ending current character blank current character. also blank beginning. total states. supposing maximum length grams ﬁrst scan possible states |l|} corresponding target sequence deﬁne forward variable total probability valid paths preﬁxes state time figure compares training curves auto-reﬁnement grams. look similar although number grams greatly reduced reﬁnement makes training faster potentially robust less gram sparsity. figure training curve model without joint-training. model corresponding orange training curve jointly trained together vanilla models often stable training. figure typical joint-training model architecture vanilla loss best applied levels lower gram-ctc loss. although gram-ctc automatically select useful grams challenging train large total number possible grams usually huge. example english characters total number bi-grams total number tri-grams grows exponentially quickly becomes intractable. however unnecessary consider many grams ‘aaaa’ obviously useless. experiments ﬁrst eliminate useless grams statistics huge corpus count frequency gram corpus drop grams rare frequencies. then train model gram-ctc remaining grams. applying trained model large speech dataset real statistics gram’s usage. ultimately choose high frequency grams together uni-grams ﬁnal gram table shows impact iterative gram selection figure shows corresponding training curve. details please refer section gram-ctc needs solve decomposition alignment tasks harder task model learn ctc. often manifested unstable training curves forcing lower learning rate turn results models converging worse optima. overcome difﬁculty found beneﬁcial train model gram-ctc well vanilla loss joint training multiple objectives sequence labelling also explored previous works typical joint-training model looks like figure corresponding training curve shown figure effect joint-training shown table table experiments. test gram-ctc loss task introduced gram-ctc generic techniques sequence labelling tasks. experiments model speciﬁcation training procedure model recurrent neural network two-dimensional convolutional input layers followed forward bidirectional gated recurrent layers cells each fully connected layer softmax layer. short hand model written conv gru’. network trained end-to-end gramctc weighted combination both. combination described earlier section. experiments audio data sampled khz. linear features extracted size window size normalized input feature zero mean unit variance. network inputs thus spectral magnitude maps ranging -khz features frame. epoch utterances randomly selected background noise optimization method stochastic gradient descent nesterov momentum. learning hyperparameters vary across different datasets tuned model optimizing hold-out set. typical values learning rate momentum wall street journal corpora consists primarily read speech texts drawn machinereadable corpus wall street journal news text contains hours speech data. used standard conﬁguration train dataset training validation eval testing. relatively ‘clean’ task often used model prototyping fisher-switchboard. commonly used english conversational telephone speech corpora contains hours data. following previous works evaluation carried nist test comprises switchboard callhome subsets. speech dataset. conduct large scale experiments noisy internal dataset hours. dataset contains speech collected various scenarios different background noises far-ﬁeld different accents inherent complexities challenging task thus validate effectiveness proposed method real-world application. employ dataset demonstrating different strategies selecting grams gram-ctc since widely used dataset also small enough rapid idea veriﬁcation. however small cannot large grams data sparsity problem. thus auto-reﬁned gram optimal larger datasets larger grams could effectively used procedure reﬁnement them. uni-grams bi-grams total grams) decoding obtained model another speech dataset statistics usage grams. bi-grams together uni-grams used second round training. comparison also present result best handpicked grams well results uni-grams. results shown table interesting observations found table first performance auto-reﬁned grams slightly better combination uni-grams bigrams. probably small gram learning suffers data sparsity problem auto-reﬁned gram contains small subset bi-grams thus robust. also bi-grams including higher-order grams. second performance best handpicked grams worse auto-reﬁned grams. desirable. time-consuming handpick grams especially consider high-order grams. method iterative gram selection fast usually better. third performance gram-ctc auto-reﬁned grams slightly better uni-grams. gram-ctc inherently difﬁcult train since needs learn decomposition alignment. small provide enough data train gram-ctc. sequence labelling large stride using large time stride sequence labelling rnns greatly boost overall computation efﬁciency since effectively reduces time steps recurrent computation thus speeds process forward inference backward propagation. however largest stride used limited gram use. work high time resolution order enough number frames output every character. inefﬁcient know acoustic feature could correspond several grams different lengths larger grams larger stride potentially able use. employed non-overlapping bigram outputs allow larger stride. imposes artiﬁcial constraint forcing model learn spelling word also split words bigrams. example part split word figure max-decoding results gram-ctc utterances switchboard dataset. predicted characters grams timestep separated \"|\". gram-ctc model trained doubled stride model place grams doubled width characters better viewing. represents blank. apart forced decomposed gramctc removes constraint allowing model decompose words larger units convenient sensible decomposition. comparison results show change enables gram-ctc work much better bigram table table compare performance trained model training efﬁciency strides gramctc auto-reﬁned gram previous section. expected using stride almost cuts training time epoch half compared stride stride stride performance uni-gram drops quickly. small grams inherently need higher time resolutions. gram-ctc stride stride performance decreases little experiments datasets gram-ctc constantly works better stride possible explanation small gram-ctc learn large grams well. contrast performance bi-gram good gram-ctc either stride. figure illustrates max-decoding results gram-ctc nine utterances. label characters label gramctc auto-reﬁned gram containing uni-grams high-frequency high-order grams. gramfigure that gram-ctc automatically many intuitive meaningful grams ‘the’ ‘ng’ ‘are’. also decomposes sentences segments meaningful term pronunciation. decomposition resembles phonetic decomposition larger granularity arguably natural. since gram-ctc predicts chunk characters time prediction utilizes larger context characters predicted chunk dependent thus potentially robust. example word ‘will’ last sentence figure since output network probability grams decoding process almost still end-toend. makes decomposition superior phonetic decomposition. summary gram-ctc combines advantages characters phonemes. model used gram-ctc loss. results shown table models trained language model greatly improve performances term wer. though dataset contains limited amount text data learning gram selection decomposition gramuse sample switchboard- portion nist dataset tuning language model hyper-parameters. evaluation done nist set. conﬁguration forms standard benchmark evaluating models. results table compare model best published results in-domain data. results often improved using out-of-domain data training language model sometimes acoustic model well. together techniques allow reach swbd set. finally experiment large noisy dataset collected ourself building large-vocabulary continuous speech recognition systems. dataset contains hours speech diversity scenarios farﬁeld background noises accents. cases model change loss function. conv’ refers look ahead convolution layer seen works together forward-only rnns deployment purpose. gram-ctc trained mush faster vanilla itself. result shown table gram-ctc performs better ctc. joint-training vanilla alignment information loss performance boosted veriﬁes joint-training helps training. fact small additional cost time effectively reduces paper proposed gram-ctc loss enable automatic decomposition target sequences learned grams. also present techniques train gram-ctc clean stable way. extensive experiments demonstrate proposed gram-ctc enables models efﬁciently vanilla using larger stride obtaining better performance sequence labelling. comparison experiments multiplescale datasets show proposed gram-ctc obtains stateof-the-art results various tasks. interesting observation learning gramctc implicitly avoids degenerated solution occurring traditional unit discovery task without involving bayesian priors minimum description length constraint. using small gram contains short well highfrequency grams explain success here. references alex graves santiago fernández faustino gomez jürgen schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning pages kyunghyun bart merriënboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. william chan navdeep jaitly quoc oriol vinyals. listen attend spell neural network large vocabulary conversational speech recognition. ieee international conference acoustics speech signal processing pages ieee awni hannun carl case jared casper bryan catanzaro greg diamos erich elsen ryan prenger sanjeev satheesh shubho sengupta adam coates andrew deep speech scaling end-to-end speech recognition. corr abs/. dzmitry bahdanau chorowski dmitriy serdyuk yoshua bengio end-to-end attention-based large vocabulary speech recognition. ieee international conference acoustics speech signal processing pages ieee oriol vinyals łukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. grammar foreign language. advances neural information processing systems pages chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun yoshua bengio. attention-based models speech recognition. advances neural information processing systems pages yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals gregory corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. corr abs/. dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. large-vocabulary speakerindependent continuous speech recognition using hmm. acoustics speech signal processing icassp-. international conference pages ieee wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig. achieving human parity conversational speech recognition. arxiv preprint arxiv. mathew magimai doss todd stephenson hervé bourlard samy bengio. phoneme-grapheme based speech recognition system. automatic speech recognition understanding asru’. ieee workshop pages ieee sharon goldwater thomas grifﬁths mark johnson. contextual dependencies unsupervised word segmentation. proceedings international conference computational linguistics annual meeting association computational linguistics pages association computational linguistics yajie miao mohammad gowayyed florian metze. eesen end-to-end speech recognition using deep models wfst-based decoding. automatic speech recognition understanding ieee workshop pages ieee daniel povey vijayaditya peddinti daniel galvez pegah ghahrmani vimal manohar xingyu yiming wang sanjeev khudanpur. purely sequence-trained neural", "year": 2017}