{"title": "Evolution in Groups: A deeper look at synaptic cluster driven evolution  of deep neural networks", "tag": ["cs.NE", "cs.AI", "cs.CV", "stat.ML"], "abstract": "A promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence, which mimics biological evolution processes to progressively synthesize more efficient networks. A crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks. In this study, we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks. Utilizing a synaptic cluster-driven genetic encoding, the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter-synaptic relationships within a deep neural network. This process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as GPUs and deep neural network accelerator chips. Comprehensive experimental results using four well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and DetectNet) on two different tasks (object categorization and object detection) demonstrate the efficiency of the proposed method. Cluster-driven genetic encoding scheme synthesizes networks that can achieve state-of-the-art performance with significantly smaller number of synapses than that of the original ancestor network. ($\\sim$125-fold decrease in synapses for MNIST). Furthermore, the improved cluster efficiency in the generated offspring networks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold decrease in clusters for KITTI) is particularly useful for accelerated performance on parallel computing hardware architectures such as those in GPUs and deep neural network accelerator chips.", "text": "promising paradigm achieving highly efﬁcient deep neural networks idea evolutionary deep intelligence mimics biological evolution processes progressively synthesize efﬁcient networks. crucial design factor evolutionary deep intelligence genetic encoding scheme used simulate heredity determine architectures offspring networks. study take deeper look notion synaptic cluster-driven evolution deep neural networks guides evolution process towards formation highly sparse synaptic clusters offspring networks. utilizing synaptic cluster-driven genetic encoding probabilistic encoding synaptic traits considers individual synaptic properties also inter-synaptic relationships within deep neural network. process results highly sparse offspring networks particularly tailored parallel computational devices gpus deep neural network accelerator chips. comprehensive experimental results using four well-known deep neural network architectures different tasks demonstrate efﬁciency proposed method. cluster-driven genetic encoding scheme synthesizes networks achieve state-of-the-art performance signiﬁcantly smaller number synapses original ancestor network. furthermore improved cluster efﬁciency generated offspring networks particularly useful accelerated performance parallel computing hardware architectures gpus deep neural network accelerator chips. introduction factors revitalizing deep neural networks tremendous success signiﬁcant growth computational power. proliferation massively parallel computing devices graphics processing units distributed computing revolutionized training inference deep neural networks highly parallelizable nature. incredible rise computational power enables researchers design build increasingly larger deeper neural networks boost modeling accuracy. on-going growth architectural complexity however become bottleneck widespread adoption deep neural networks many operational scenarios. example many applications self-driving cars smartphone applications surveillance cameras computational resources limited low-power embedded gpus cpus deep learning accelerator chips strong constraints memory usage. furthermore many situations cloud computing intractable transmission cost bandwidth issues well privacy concerns. considering obstacles associated architectural complexity deep neural networks witnessed growing attention towards learning highly efﬁcient deep neural network architectures able provide strong modeling power operational scenarios limited memory computational resources available. ﬁrst approaches area optimal brain damage method synapses pruned based strength. gong proposed network compression framework vector quantization used shrink storage requirements deep neural networks. utilized huffman coding addition pruning vector quantization reduce memory requirements. hashing another related trick employed chen network compression. low-rank approximation sparsity learning strategies used build smaller efﬁcient deep neural networks. recently shaﬁee introduced evolutionary synthesis framework progressively learn efﬁcient deep neural networks along successive generations. proposed evolutionary deep intelligence approach mimics biological evolution mechanisms random mutation natural selection heredity within probabilistic graphical modeling paradigm successively synthesize efﬁcient offspring network architectures. crucial design factor evolutionary deep intelligence genetic encoding scheme used simulate heredity affects architectural traits passed offspring networks signiﬁcant way. therefore effective genetic encoding scheme enable better transfer genetic information ancestor network offspring networks build efﬁcient powerful future generation. introduced genetic encoding scheme merely considers individual synaptic properties sense probability synthesizing synapse within network independent rest synapses thus ignores dependence different synapses. however neurobiological evidences support increasing probability co-activation correlated synapses encode similar information locate close dendrite—synaptic clustering inspired observation incorporating synaptic clustering genetic encoding scheme evolutionary deep models potentially fruitful direction investigate. moreover synthesizing offspring networks based synaptic clusters increase efﬁciency offspring deep neural networks running parallel computing devices gpus deep neural network accelerator chips. study take deeper look notion synaptic cluster-driven evolution deep neural networks guides evolution process towards formation highly sparse synaptic clusters offspring networks. process results highly sparse offspring networks particularly tailored parallel computing devices gpus deep neural network accelerator chips. introduce multi-factor genetic encoding scheme synaptic probability considers probability synthesis cluster synapses includes particular synapse probability synthesis particular synapse within cluster. genetic encoding scheme effectively promotes formation synaptic clusters successive generations evolution process supporting formation highly efﬁcient deep neural networks. inspired neurobiological evidences propose synaptic cluster-driven evolution framework deep neural networks ancestor network guided towards formation highly sparse synaptic clusters offspring networks. idea design genetic encoding scheme considers inter-synaptic relationships well individual properties synapse. formally synthesis probability distribution synapse product formation likelihood corresponding synaptic cluster synthesis probability particular synapse within cluster. evolutionary deep intelligence. prior describing notion synaptic cluster-driven evolutionary synthesis correspondingly proposed genetic encoding scheme ﬁrst provide overview evolutionary deep intelligence framework introduced shaﬁee synthesizing progressively efﬁcient deep neural networks successive generations within probabilistic graphical modeling paradigm. evolutionary deep intelligence framework this framework fundamentally different past attempts utilizing evolutionary methods training neural networks genetic algorithm used create neural networks high modeling capabilities direct highly computationally expensive manner. architectural traits ancestor deep neural networks encoded probabilistic ‘dna’ sequences. offspring networks possessing diverse network architectures synthesized stochastically based ‘dna’ ancestor networks probabilistic computational environmental factor models thus mimicking random mutation heredity natural selection. offspring networks trained much like would train newborn efﬁcient diverse network architectures achieving powerful modeling capabilities. crucial design factor evolutionary deep intelligence genetic encoding scheme used mimic heredity signiﬁcant impact architecture evolved offspring networks. study exploring effective genetic encoding schemes guide synthesis process modeling capabilities ancestor network faithfully captured efﬁcient offspring networks along successive generations. cluster-driven genetic encoding. network architecture deep neural network expressed denoting possible neurons possible synapses network. neuron connected synapses neuron synaptic connectivity associated denoting strength. given network architecture previous generation i.e. architectural traits deep neural network generation encoded conditional probability treated probabilistic ‘dna’ sequence deep neural network. assume synaptic connectivity characteristics ancestor network desirable traits inherited descendant networks. therefore genetic information deep neural network encoded synaptic probability wkg− synaptic strength synapse proposed genetic encoding scheme synaptic cluster-driven evolution incorporating neurobiological phenomenon synaptic clustering probability synaptic co-activation increases correlated synapses encode similar information close together dendrite. promote formation synaptic clusters successive generations efﬁcient offspring networks introduce multi-factor synaptic probability model deﬁned follows ﬁrst factor models synthesis probability particular cluster synapses ¯sgc second factor models synthesis probability particular synapse within synaptic cluster speciﬁcally probability represents likelihood particular synaptic cluster ¯sgc synthesized part network architecture generation given synaptic strength generation example deep convolutional neural network synaptic cluster subset synapses kernel kernels within deep neural network. probability represents likelihood existence synapse within cluster generation given synaptic strength generation such proposed synaptic probability model promotes persistence strong synaptic connectivity offspring deep neural networks successive generations also promotes persistence strong synaptic clusters offspring deep neural networks successive generations. cluster-driven evolutionary synthesis. seminal paper evolutionary deep intelligence shaﬁee synthesis probability composed synaptic probability mimic heredity environmental factor model mimic natural selection introducing quantitative environmental conditions offspring networks must adapt study reformulated general enable incorporation different quantitative environmental factors synthesis synaptic clusters well individual synapse thus facilitating synaptic cluster-driven evolution deep neural networks realization cluster-driven genetic encoding. demonstrate beneﬁts proposed cluster-driven genetic encoding scheme simple realization scheme presented study. here since wish promote persistence strong synaptic clusters offspring deep neural networks successive generations synthesis probability particular cluster synapses modeled model reduces inﬂuence weak synapses within synaptic cluster genetic encoding process. probability particular synapse within synaptic cluster denoted expressed layer-wise normalization constant. incorporating aforementioned probabilities proposed scheme relationships amongst synapses well individual synaptic strengths taken consideration genetic encoding process. study efﬁcacy synaptic cluster-driven evolution synthesizing highly efﬁcient deep neural networks evolutionary synthesis deep neural networks across several generations performed using proposed genetic encoding scheme across four well-known deep neural network architectures different tasks mnist stl- experiments object categorization lenet- architecture selected network architecture original ﬁrst generation ancestor network. cifar experiment object categorization different network architectures explored ﬁrst generation ancestor network. first alexnet architecture utilized ancestor networks cifar ﬁrst layer modiﬁed utilize kernels instead kernels given smaller image size cifar. second resnet- architecture utilized another ancestor network cifar allows study behaviour proposed scheme different deep neural network architectures. kitti experiment object detection detectnet architecture derived googlenet tuned object detection selected network architecture original ﬁrst generation ancestor network. environmental factor model imposed different generations study designed form deep neural networks progressively efﬁcient network architectures ancestor networks maintaining modeling accuracy. speciﬁcally formulated study offspring deep neural network total number synapses direct ancestor network. furthermore study kernel deep neural network considered synaptic cluster synapse probability model. words probability synthesis particular synaptic cluster modeled truncated summation weights within kernel. architectural efﬁciency generations. study offspring deep neural networks synthesized successive generations accuracy offspring network exceeded better study changes architectural efﬁciency descendant networks multiple generations. table table show architectural efﬁciency versus modeling accuracy several generations three datasets based three different network architectures observed table architectural efﬁciency test accuracy different generations synthesized networks mnist stl- using lenet-. gen. acc. denote generation architectural efﬁciency accuracy respectively. table architectural efﬁciency test accuracy different generations synthesized networks cifar- using alexnet resnet-. gen. acc. denote generation architectural efﬁciency accuracy respectively. table descendant network generation mnist based lenet- architecture staggering ∼-fold efﬁcient original ﬁrst-generation ancestor network without exhibiting signiﬁcant drop test accuracy trend consistent observed stl- results based lenet- architecture descendant network generation ∼-fold efﬁcient original ﬁrst-generation ancestor network without signiﬁcant drop test accuracy also worth noting since training dataset stl- dataset relatively small descendant networks generations actually achieved higher test accuracies compared original ﬁrst-generation ancestor network illustrates generalizability descendant networks compared original ancestor network descendant networks fewer parameters train. case cifar based alexnet architecture descendant network generation network ∼.-fold efﬁcient original ancestor network drop test accuracy. case cifar based resnet- architecture descendant network generation network ∼-fold efﬁcient original ancestor network drop test accuracy. finally table shows architectural efﬁciency versus modeling accuracy several generations kitti dataset based detectnet architecture. case descendant network generation ∼-fold efﬁcient compared original ﬁrst-generation ancestor network achieving increase precision increase recall. results demonstrate efﬁcacy proposed scheme also illustrate applicability proposed scheme variety different network architectures. cluster efﬁciency. table shows cluster efﬁciency synthesized deep neural networks last generations cluster efﬁciency deﬁned study total number kernels layer original ﬁrst-generation ancestor network divided current synthesized network. observed mnist cluster efﬁciency last-generation descendant network result near .-fold potential speed-up running time embedded gpus deep neural network accelerator chips reducing number arithmetic operations ∼.-fold compared ﬁrst-generation ancestor network though computational overhead layers relu lead reduction actual speed-up. potential speed-up last-generation descendant network stl- lower compared mnist dataset reported cluster efﬁciency last-generation descendant network table architectural efﬁciency precision recall different generations synthesized networks kitti using detectnet. gen. denote generation architectural efﬁciency respectively. table cluster efﬁciency test accuracy ﬁrst last reported generations synthesized networks mnist stl- using lenet-. gen. acc. denote generation cluster efﬁciency accuracy respectively. cluster efﬁciency last generation descendant networks cifar using alexnet resnet- respectively. finally table demonstrates cluster efﬁciency synthesized deep neural networks last generations kitti dataset using detectnet. seen cluster efﬁciency last generation descendent network achieving increase precision increase recall compared original ﬁrst-generation ancestor network. results demonstrate proposed genetic encoding scheme promotes synthesis deep neural networks highly efﬁcient maintains modeling accuracy also promotes formation highly sparse synaptic clusters make highly tailored devices designed highly parallel computations gpus deep neural network accelerator chips. study took deeper look synaptic cluster-driven strategy evolution deep neural networks synaptic probability within deep neural network driven towards formation highly sparse synaptic clusters. experimental results tasks object categorization object detection demonstrated synthesized ‘evolved’ offspring networks using evolution strategy synaptic cluster-driven genetic encoding scheme achieve state-of-the-art performance network architectures signiﬁcantly efﬁcient compared original ancestor network also highly tailored operational machine learning applications scenarios using devices designed highly parallel computations gpus deep neural network accelerator chips. future work involves investigating alternative realizations genetic encoding scheme beyond simple realization presented study study network architecture evolutions deep neural networks synthesized realizations successive generations better understand effectiveness efﬁciency. research supported natural sciences engineering research council canada canada research chairs program. authors also thank nvidia hardware used study nvidia hardware grant program.", "year": 2017}