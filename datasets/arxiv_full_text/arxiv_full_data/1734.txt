{"title": "Analysis of Stopping Active Learning based on Stabilizing Predictions", "tag": ["cs.LG", "cs.CL", "stat.ML", "I.5.1; I.5.4; G.3; I.2.7; I.2.6"], "abstract": "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen's Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where $T>0$), then the difference in F-measure performance between those models is bounded above by $\\frac{4(1-T)}{T}$ in all cases. If precision of the positive conjunction of the models is assumed to be $p$, then the bound can be tightened to $\\frac{4(1-T)}{(p+1)T}$.", "text": "dagan baldridge osborne bloodgood vijay-shanker bloodgood callison-burch hachey haertel haffari sarkar lewis gale sassano settles craven shen thompson tomanek hovy within community active learning widely investigated applied order alleviate annotation bottleneck faced developers systems technologies. main idea judiciously selecting examples labeled annotation effort focused helpful examples less annotation effort required achieve given levels performance passive learning policy used. historically problem developing methods detecting stop tabled future work research literature focused select examples labeled analyzing selection methods however realize savings annotation effort enables must method knowing stop annotation process. challenge stop early useful generalizations still made wind model performs poorly stop late useful generalizations made human annotation effort wasted beneﬁts using active learning lost. within natural language processing community active learning widely investigated applied order alleviate annotation bottleneck faced developers systems technologies. paper presents ﬁrst theoretical analysis stopping active learning based stabilizing predictions analysis revealed three elements central success method bounds cohen’s kappa agreement successively trained models impose bounds differences f-measure performance models; since stop labeled made large practice helping guarantee results transfer previously unseen streams examples test/application time; good sample estimates kappa successive models obtained. proofs relationships level kappa agreement difference performance consecutive models presented. speciﬁcally kappa agreement models exceeds threshold difference f-measure performance models bounded cases. precision positive conjunction models assumed bound tightened active learning also called query learning selective sampling approach reduce costs creating training data received considerable interest association computational linguistics heuristics based estimates model conﬁdence error stability. although heuristic methods appealing intuitions experimental success small handful tasks datasets methods widely usable practice community’s understanding stopping methods remains coarse inexact. pushing forward understanding mechanics stopping exact level therefore crucial achieving design widely usable effective stopping criteria. bloodgood vijay-shanker introduce terminology aggressive conservative describe behavior stopping methods conduct empirical evaluation different published stopping methods several datasets. stopping methods tend behave conservatively stopping based stabilizing predictions computed inter-model kappa agreement shown consistently aggressive without losing performance several published empirical tests. method stops kappa agreement between consecutively learned models exceeds threshold three consecutive iterations although intuitive heuristic performed well published experimental results theoretical analysis method. current paper presents ﬁrst theoretical analysis stopping based stabilizing predictions. analysis helps explain deeper exact level method works does. results analysis help characterize classes problems method expected work well expected work well. theory suggestive modiﬁcations improve robustness stopping method certain classes problems. perhaps important approach analysis provides enabling framework precise analysis stopping criteria possibly parts active learning decision space. aggressive methods stop sooner aggressively trying reduce unnecessary annotations conservative methods careful risk losing model performance even means annotating many examples necessary. rest paper f-measure denote f-measure balanced harmonic mean precision recall standard metric used evaluate systems. useful works consider switching between different active learning strategies operating regions knowing switch strategies example similar stopping problem another setting detailed understanding variance stabilization estimates link performance ramiﬁcations useful. exact understanding mechanics stopping also useful applications co-training agreement-based co-training particular. finally proofs theorems regarding relationships cohen’s kappa statistic f-measure broader works consider interannotator agreement ramiﬁcations performance appraisals topic longstanding interest computational linguistics intuition behind method models learned applied large representative unlabeled data called stop consecutively learned models high agreement predictions classifying examples stop indicates time stop active learning stopping strategy explicitly examined calculate cohen’s kappa agreement statistic consecutive rounds active learning stop three consecutive calculations. since kappa statistic important aspect method discuss background regarding measuring agreement general cohen’s kappa particular. measurement agreement human annotators received signiﬁcant attention context drawbacks using percentage agreement recognized alternative metrics proposed take chance agreement account. artstein poesio survey several agreement metrics. observed agreement agreement expected chance. different metrics differ compute instances usage agreement metric article categories coders. categories coders consecutive models agreement measured. cohen’s kappa statistic measures agreement expected chance modeling coder separate distribution governing likelihood assigning particular category. formally kappa deﬁned equation computed follows section analyzes stopping method. section analyzes variance estimator kappa uses particular relationship variance speciﬁc aspects operationalization stop size. section analyzes relationships kappa agreement models difference f-measure models. variance kappa estimator bases decision stop information contained contingency tables classiﬁcations models learned consecutive iterations determining whether stop iteration classiﬁcations current model compared classiﬁcations previous model mt−. table shows population parameters models where population probability {+−} probability example placed category model category model population probability {+−} probability example placed category model population probability {+−} probability example placed category model mt−. actual probability agreement π−−. indicated equation kappa models probability agreement expected chance assuming classiﬁcations made independently. hence probability agreement expected chance terms population probabilities π+.π.++π−.π.−. deﬁnition kappa kappa parameter terms population probabilities given practical applications know true population probabilities resort using sample estimates. method uses stop size deriving estimates. table shows contingency table counts classiﬁcations models sample size population probabilities estimated relative frequencies where a/n; b/n; c/n; d/n; observed proportion agreement p+.p.+ p−.p.− proportion agreement expected chance assume make classiﬁcations independently. kappa measure agreement estimated bloodgood vijay-shanker used stop size datasets. although worked well results reported believe ﬁxed size work well tasks datasets method could used. table shows variances computed using equation points stopped datasets variances indicate size typically sufﬁcient tight estimates kappa helping illuminate empirical success method datasets. generally method augmented variance check variance estimated kappa potential stopping point exceeds desired looking equation again note relatively close variance expected quite large. situations users expect larger stop sizes extreme conditions advisable method use. heretofore published literature contained informal explanations stabilizing predictions expected work well stopping method remainder section describe mathematical foundations stopping methods based stabilizing predictions. particular prove even worst possible case kappa agreement subsequently learned models greater threshold must case change performance models bounded prove additional theorems tighten bound assumptions made model precision. lemma suppose f-measure kappa computed contingency table counts given table suppose proof deﬁnition terms contingency table counts task-dataset ner-dna ner-celltype ner-protein reuters newsgroups webkb student webkb project webkb faculty webkb course tc-spamassassin tc-trec-spam average table estimates variance dataset estimate variance computed contingency table point stopped average variances displayed. last contains macro-average average variances datasets. theorem model learned iteration active learning model learned iteration estimate kappa agreement classiﬁcations examples stop set. f-measure classiﬁcations truth stop set. ˜ft− fmeasure classiﬁcations truth stop set. ˜ft−. suppose |∆ft| proof suppose ˜ft− deﬁned stated statement theorem f-measure classiﬁcations examples stop set. table show contingency table counts versus examples stop set. then deﬁ+ nitions a+b+c. exist true labels exft amples stop don’t know since stop unlabeled nonetheless must exist. truth stop split table subtables counts table examples truly positive table examples truly negative. table itive truly negative. similar explanations hold counts. also tables equalities hold. contingency tables versus truth versus truth derived tables convenience table shows contingency table versus truth table shows contingency table versus truth. suppose implies lemma implies note deriving inequality used previously derived inequality also proof theorem assumes worst possible case sense examples classiﬁcations differ assumed truth values serve maximize model’s f-measure minimize model’s f-measure maximize |∆ft| much possible. resulting limitation bound loose many cases. possible derive tighter bounds perhaps easing expected case instead worst case and/or making additional assumptions. taking possibility prove tighter bounds assumptions precision models made. consider proof theorem transitioning equality inequality used fact sees h+da+db zero. pathological case. many practically important classes cases consider strictly less often subh+da+db stantially less following theorems prove tighter bounds |∆ft| theorem utilizing insight. theorem suppose ˜ft− deﬁned stated statement theorem contingency tables deﬁned proof theorem ositiveconjunction model classiﬁes example positive models classify example positive. suppose ositiveconjunction perfect precision stop words every single example stop classify positive truthfully positive |∆ft| proof proof theorem holds exactly equality using additional assumption theorem suppose ˜ft− deﬁned stated statement theorem contingency tables deﬁned proof theorem ositiveconjunction model classiﬁes example positive models classify example positive. suppose ositiveconjunction precision stop set. |∆ft| proof proof theorem holds exactly equality ositiveconjunction precision stop solving terms therefore applying lemma theorem shows precision conjunctive model affects bound. theorem scaling factor implicitly order handle pathological case positive conjunctive model precision theorem positive conjunctive model precision examples stop scaling factor theorem generalizes scaling factor function precision positive conjunctive model. convenience table shows scaling factor values different precision values. bounds theorems bound difference performance stop consecutively learned models mt−. issue consider connected difference performance stop difference performance stream application examples generated according population probabilities. taking issue consider proof theorems would hold used sample proportions instead sample counts since stop unbiased approaches inﬁnity sample proportions approach population probabilities difference difference performance stop stream application examples generated according population probabilities approach zero. datasets. methods widely usable practice community’s understanding stopping methods remains inexact. pushing forward understanding mechanics stopping exact level therefore crucial achieving design widely usable effective stopping criteria. paper presented ﬁrst theoretical analysis stopping based stabilizing predictions. analysis revealed three elements central method’s success sample estimates kappa variance; kappa tight connections differences f-measure; since stop doesn’t labeled arbitrarily large helping guarantee results transfer previously unseen streams examples test/application time. presented proofs relationships level kappa agreement difference performance consecutive models. specifically kappa agreement models least difference f-measure performance models bounded precision positive conjunction models assumed bound tightened setup methodology proofs serve launching many investigations including analyses stopping; works consider switching different active learning strategies operating regions; works consider stopping co-training especially agreement-based co-training. finally relationships exposed kappa statistic f-measure broader works consider inter-annotator agreement interplay system evaluation topic long-standing interest.", "year": 2015}