{"title": "Deep Action Sequence Learning for Causal Shape Transformation", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep learning became the method of choice in recent year for solving a wide variety of predictive analytics tasks. For sequence prediction, recurrent neural networks (RNN) are often the go-to architecture for exploiting sequential information where the output is dependent on previous computation. However, the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step-wise transformation sequence that is dependent on the previous computation only in the visible domain. We propose that a hybrid architecture of convolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support. While such a framework can be useful in a variety of problems such as robotic path planning, sequential decision-making in games, and identifying material processing pathways to achieve desired microstructures, the application of the framework is exemplified by the control of fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars. Learning of a multistep topological transform has significant implications for rapid advances in material science and biomedical applications.", "text": "deep learning became method choice recent year solving wide variety predictive analytics tasks. sequence prediction recurrent neural networks often go-to architecture exploiting sequential information output dependent previous computation. however dependencies computation latent domain suitable certain applications involving prediction step-wise transformation sequence dependent previous computation visible domain. propose hybrid architecture convolution neural networks stacked autoencoders sufﬁcient learn sequence actions nonlinearly transforms input shape distribution target shape distribution support. framework useful variety problems robotic path planning sequential decision-making games identifying material processing pathways achieve desired microstructures application framework exempliﬁed control ﬂuid deformations microﬂuidic channel deliberately placing sequence pillars. learning multistep topological transform signiﬁcant implications rapid advances material science biomedical applications. hierarchical feature extraction using deep neural networks successful accomplishing various tasks object recognition speech recognition deep vision enhancement multi-modal sensor fusion prognostics engineering design policy reward learning relating variants diseases paper proposes application deep learning fusing multiple architectures solving design engineering problems potentially accelerates derecurrent neural networks often go-to architecture exploiting sequential information output dependent previous computation. however dependencies computation latent domain suitable certain applications involving prediction step-wise transformation sequence dependent previous computation visible domain paper propose deep learning architecture simultaneously predicts intermediate shape images learns sequence causal actions contributing desired shape transformation visible domain. topological transformation framework multiple implications. architecture easily implemented applications learning transform belief space robotic path planning sequential decision making games learning material processing pathways obtain desired microstructures starting initial microstructure learning sequence manufacturing steps additive manufacturing fast-design main advantage. contributions paper outlined next formulation learning causal shape transformations predict sequence transformation actions presented setting initial shape desired target shape provided. integrated hierachical feature extraction approach using stacked autoencoders convolutional neural networks proposed capture transformation features generate associated sequence resulting transformation. proposed approach tested validated numerical simulations engineering design problem results showing competitive prediction accuracy previously explored methods. figure difference hybrid approach. models take input vector time pass hidden layer produces latent output latent outputs mapped visible outputs function dependencies latent layer. hybrid approach considered problem visible output used input next without dependencies latent layer. section describe problem setup provide brief background previous approach. illustrate previous current architectures implemented focus attention microﬂuidic sculpting application. inertial ﬂuid sculpting micropillar sequences recently developed method ﬂuid control wealth applications microﬂuidics community technique utilizes individual deformations imposed ﬂuid ﬂowing past single obstacle conﬁned create overall deformation rationally designed sequence pillars. pillars spaced enough apart individual deformations become independent thus pre-computed building blocks highly efﬁcient forward model prediction sculpted given input pillar sequence since debut sculpting micropillar sequence design used fundamental applications novel particle fabrication solution transfer however practical applications require solving inverse problem generate sequence pillars given user-deﬁned shape. without intelligent computer algorithms tasks require time-consuming trial error design iterations. automated determination pillar sequences yields custom shape impactful advance. although researchers tried frame inverse problem unconstrained optimization problem invariable time-consuming. many methods used solve forward problem limited amount effort done solving inverse problem practical time-efﬁcient applications deep learning methods explored user-deﬁned shapes corresponding pillars sequence. additionally application serves impactful advance towards learning topological transformations. framework utilized concrete biomedical applications require design microﬂuidic devices. possible applications include designing device move ﬂuid surrounding cells wall microﬂuidic channel collected high purity cells maintained channel center. high purity allows reuse valuable reagents staining cells diagnosis wrapping ﬂuid around microchannel surface characterize binding anti-p antibody immobilized microchannel surface. flow sculpting enhance reaction abundance proteins improve diagnostic limits detection various diseases. hence study promise application areas machine learning community related thermoﬂuid sciences design engineering. approach inverse problem class indices assigned pillars different speciﬁcations. instance pillar located position diameter index whereas another pillar position diameter assigned index diameter position values represented ratios respect channel size locations dimensionless quantities help enable scalability ﬂuid channel. index learning transform belief space robotic path planning. robot chasing mobile target maintaining posterior corresponding location target. would like speciﬁcally maximize posterior certain region corresponding problem would what best course action taken robot succession?. learning material processing pathways obtain desired microstructures. materials processing processing materials altering properties succession alter morphology material. equivalent inverse problem would want material achieve speciﬁc morphology processing steps taken?. cnn-smc capable predicting large number different sequences total time seconds drawback sequence length constrained model needs trained generate sequence different lengths. furthermore sequence deforms target shape predicted joint manner provide sufﬁcient insight interplay pillars causing deformation. assignment performed ﬁnite combination pillar positions diameters obtained discretizing design space. study possible indices describe diameter position single pillar. simultaneous multi-class classiﬁcation method used solve similar problem. instead solving single classiﬁcation problem model solves sub-problem pillar using parameters learned cnn. formulation requires slight modiﬁcation loss function pillar sequence length loss function minimized data negative loglikelihood deﬁned denotes training model parameters weights biases predicted pillar index whereas provided shape image. total loss computed summing individual losses pillar. cnn-smc framework without drawbacks. pillars predicted joint manner pillars sequence inferred simultaneously. produce satisfactory sequences regenerates desired shape unclear successive pillars interact another. section propose deep learning architecture predicts intermediate shape images learns sequence words sentence. application pillar independently added channel shape resulted previous pillar stabilizes i.e. longer changes. guarantee interactions among pillars however pillars contribute change overall shape context outputs e.g. case next-word prediction oftentimes output vectors independentone word high probability next word certain word. context choice next pillar depend choice previous pillar. rather look making given generated shape regardless history. related concept spatial transformer network localization network outputs geometrical transformation parameters; however desire exact class attributed arbitrary transformation function. work predict pillar sequence pillar time without disregarding causality produced sequences deform resembles closely desired shape. predict sequence pillars results desired shape chapter introduces notion transformation learning. fig. shows learning approach supplying juxtaposed shapes deformation after extracts relevant features predicts class pillar causing deformation. training data generation procedure input comprised three parts pre-deformed shape produced randomly generated sequence varying length values number classes single pillar; padding prevent convolutional kernels pick interfering features juxtaposed images; post-deformed shape produced adding random pillar index previous sequence. newly-added pillar index become label train cnn. essentially action prediction network predicts index pillar given prepost-deformed shape. actually inferencing right portion input image replaced ﬁnal target shape. input image obtain pillar index added initially empty pillar sequence. forward model computationally expensive takes merely milliseconds pillar sequence used regenerate current shape replaces left portion input right portion remains unchanged. input enters obtain second pillar index subsequently added previously obtained pillar sequence. point sequence length sequence used regenerate current shape. process repeated current shape matches target shape user-deﬁned stopping criterion met. alternative feed prepost-deformed shapes separately isolated channels merging together fully connected layer architecture referred apn-c words channel different sets ﬁlters relationship sets fuse together fully connected layer. verge ﬁnal desired shape. furthermore training data covers vanishingly small fraction design space coverage shrinking exponentially sequence length increases necessary learn transformations meaningful way. help alleviate issue introduce intermediate transformation network next subsection. parameters input image comprised shape image padding between resulting ﬁnal dimension apn-c inputs treated separate channels without introducing padding. model contains convolutional layers kernels respectively followed maxpooling layer. fully connected layer hidden units. training done training examples validation examples miniitn attempts construct shape bridges purely undeformed shape ﬁnal desired shape nonlinear transformation path used deep autoencoder extract hierarchical features desired ﬁnal shape outputs approximated bridging shape. generate training data network input image generated random pillar sequence varying length. corresponding bridging shape generated truncating pillar sequence half thus shape lies middle transformation pathway formally sequence length truncated even. pair images used train deep autoencoder mean squared error model outputs desired bridging shape backpropagated ﬁnetune weights biases model. extended obtain several waypoints instead midpoint number waypoints dictated complexity target shape allow smoother transformation pathway require changes training data generation procedure. call recursive leave future work. comment somewhat similar rnn. note problem requires transforming hidden states physical domain using complex forward function making next prediction. unlike rnns itns solely latent output directly input next time step. forward function need constantly transform output back physical domain re-evaluate every step. also argue somewhat similar terms memory argument difference domain key. latent domain used every step ﬁnal sequence output necessarily reﬂect true transformation physical domain. parameters autoencoder layers hidden units accepts shape images training done training examples validation examples minibatch size learning rate training also done using earlystopping algorithm. roles clearly described integrate networks form integrated pipeline. schematic shown fig. beginning automatically guesses candidate bridging shape considered temporary target shape concatenated undeformed shape placed left padding. concatenated input supplied predict ﬁrst pillar causing deformation added sequence initially empty hence resulting sequence length current shape regenerated updated -pillar sequence replaces left portion input image next iteration obtain second pillar index. process repeated ‘stage current shape sufﬁciently similar bridging shape iteration limit reached. then right portion input image replaced ﬁnal desired shape target. process continued ‘stage undergoes process ‘stage except target shape ﬁnal desired shape. iteration predicted pillar index added sequence reconstructed shape matches desired shape stopping criterion achieved. resulting sequence vary length different desired shapes. perimeter shape area shape. measure utilized determine number intermediate shapes predicted itn. higher shape complexity necessary bridging shapes. fig. shows complexity various shapes. quantify effectiveness cnn-smc versus proposed approach evaluated pixel match rate target shapes computed appropriate statistics. deﬁned computed follows transformation essential aspect problem formulation structural similarity index used supplementary metric compare structurally similar regenerated shape images target shape. ssim explores change image structure incorporates pixel interdependencies. ssim expressed shows four example target shapes performance using apn+itn cases apn-only formulation produces pillar sequences resulting shapes resemble target shape close using apn+itn. clearly seen cases fig. hand using bridging shape temporary target prediction performance great improvements. addition shapes able converge bridging shape well target apn+itn formulation. realistic applications sequence stored memory post-processed remove redundant pillars thus producing shorter sequence increase ﬁnancial savings manufacturing process. figure four examples sequence prediction using apn-only without bridging apn+itn bridging. predicting bridging shape resulting predicted sequence able reconstruct shapes similar target shape. frame shows deformation shape additional predicted pillar added sequence. figure test shapes reconstructed shapes generated sequences predicted using different methods. column target shape reconstruction cnn+smc apn-c apn+itn. table ssim regenerated shapes using different enhancement methods. numbers reported format bolded numbers correspond method highest ssim. asterisk denotes architecture presented paper. target flow shapes test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape test shape average shapes predicted sequences shown fig. fig. fig. ssim apn-c apn+itn clearly higher cnn+smc approach. observe target shapes predicted sequence cnn+smc result entirely dissimilar shape cases fared better hybrid apn+itn model. however apn+itn consistently superior terms ssim apn-only architecture shows bridging shape generally helps producing sequences generate complex shapes. inferior performance cnn+smc model constrained always generate sequence pillars thus resultant shapes often become overcomplicated. additionally treating input separate channels results lower performance model parameters kept equal. suggests single ﬁlter learns features input images simultaneously clear advantage using together model need retrained variable sequence lengths unlike cnn-smc model number pillars output sequence constrained. method highly scalable enormous room extension sculpting highly complex shapes. paper proposes deep learning based approach solve complex design exploration problems speciﬁcally design microﬂuidic channels sculpting. deep architecture proposed learn sequence actions carries desired transformation input great implications innovation manufacturing processes material sciences biomedical applications decision planning many more. creative integration based tools tackle inverse ﬂuid problem achieve required design accuracy expediting design process. current efforts primarily focusing optimizing tool-chain well tailoring speciﬁc application areas manufacturing biology.", "year": 2016}