{"title": "Algorithms for multi-armed bandit problems", "tag": ["cs.AI", "cs.LG"], "abstract": "Although many algorithms for the multi-armed bandit problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as epsilon-greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. Thirdly, the algorithms' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of application of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50% more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies.", "text": "stochastic multi-armed bandit problem important model studying explorationexploitation tradeoﬀ reinforcement learning. although many algorithms problem well-understood theoretically empirical conﬁrmation eﬀectiveness generally scarce. paper presents thorough empirical study popular multi-armed bandit algorithms. three important observations made results. firstly simple heuristics \u0001-greedy boltzmann exploration outperform theoretically sound algorithms settings signiﬁcant margin. secondly performance algorithms varies dramatically parameters bandit problem. study identiﬁes algorithm settings performs well settings performs poorly. properties described current theory even though exploited practice design heuristics. thirdly algorithms’ performance relative aﬀected number bandit arms variance rewards. ﬁnding guide design subsequent empirical evaluations. second part paper turn attention important area application bandit algorithms clinical trials. although design clinical trials principal practical problems motivating research multi-armed bandits bandit algorithms never evaluated potential treatment allocation strategies. using data real study simulate outcome clinical trial would bandit algorithms used allocate patients treatments. adaptive trial would successfully treated least patients signiﬁcantly reducing number adverse eﬀects increasing patient retention. trial best treatment could still identiﬁed high level statistical conﬁdence. ﬁndings demonstrate bandit algorithms attractive alternatives current adaptive treatment allocation strategies. multi-armed bandit problems introduced robbins since used extensively model trade-oﬀs faced automated agent aims gain knowledge exploring environment exploit current reliable knowledge. problems arise frequently practice example context clinical trials on-line advertising. multi-armed bandit problem oﬀers clean simple theoretical formulation analyzing trade-oﬀs exploration exploitation. comprehensive overview bandit problems statistical perspective given berry fristedt distributions generally interpreted corresponding arms slot machine; player viewed gambler whose goal collect much money possible pulling arms many turns. turn player selects hand ﬁnding distribution highest expected value; hand gaining much rewards possible playing. bandit algorithms specify strategy player choose turn. many theoretical bounds established regret diﬀerent bandit algorithms recent years however theoretical analyses available algorithms existing bounds generally loose accurately measure strategies’ performance. empirical evidence regarding performance many algorithms unfortunately also limited. algorithms often evaluated within larger body mostly theoretical work. evaluation done context often performed small number bandit problem instances generalize settings. moreover diﬀerent authors evaluate algorithms diﬀerent settings complicates comparison algorithms. extensive empirical study compares multiple algorithms done vermorel mohri paper include evaluation family algorithms recently become popular well important strategies pursuit reinforcement comparison moreover paper investigate eﬀect variance rewards algorithm performance. experiments eﬀect turned signiﬁcant. paper also attempt tune algorithms optimally; algorithms therefore underperforming. finally paper describe full detail criteria used evaluate performance making diﬃcult interpret results. paper address bandit literature providing extensive empirical evaluation popular bandit strategies. conduct thorough analysis identify aspects bandit problem aﬀect performance algorithms relative other. surprisingly relevant characteristics turn number arms reward distributions’ variance. proceed measure algorithm performance diﬀerent combinations parameters. remarkably simplest heuristics outperform sophisticated theoretically sound algorithms settings. although similar observation made vermorel mohri experiments ﬁrst strongly suggest occurs practically every bandit problem instance. also observe performance algorithm varies dramatically bandit problem instance other. identify algorithm settings performs well settings performs poorly. properties described current theory even though exploited within heuristics solving real-world problems. important side result study precisely identiﬁes aspects bandit problem must considered experiment. number arms reward variance. hope observation taken account design subsequent empirical studies. theoretical viewpoint ﬁndings indicate need formal analysis simple heuristics generally development theoretically sound algorithms perform well simpler heuristics practice. second half paper turn attention important application bandit algorithms clinical trials. design clinical trials main practical problems motivates research multi-armed bandits several seminar papers ﬁeld describe such. indeed clinical trial perfectly captures problem balancing exploration exploitation looking traditional clinical trial patients randomized equal-sized groups. best treatment usually identiﬁed high level conﬁdence half patients beneﬁt adaptive trials dynamically allocate patients better treatment long advocated ethical reasons even decades theoretical discussion remains limited modern adaptive clinical trials classiﬁed several families. group sequential designs trials stopped prematurely based interim results performance particular treatment. sample size re-estimation designs allow patient population size readjusted course trial. drop-the-losers designs hand allow certain treatments dropped added. naturally trials drop less promising treatments ﬁrst. types adaptive trials include adaptive dose ﬁnding designs adaptive treatment-switching designs well multiple adaptive designs combine features families described above. thorough discussion literature adaptive clinical trials survey paper chow chang context interesting family adaptive trials so-called adaptive randomization designs. designs adjust patients’ treatment assignment probabilities favor successful treatments course trial. popular adaptive randomization strategies play-the-winner operates similarly pursuit family bandit algorithms. simplicity adaptive randomization strategies form arguably popular family adaptive trials. however randomization strategies often based ad-hoc heuristics oﬀer guarantees patient welfare. terms formal guarantees bandit algorithms advantage strategies like playthe-winner. even though extensive literature adaptive clinical trials knowledge study evaluates bandit algorithms treatment allocation strategies. particularly surprising given fact many bandit algorithms speciﬁcally designed purpose. moreover clinical trial simulations seldom based real clinical data conducted simulation studies aware based data actual clinical trial. demonstrate play-the-winner adaptive randomization strategy used trial drug zidovudine signiﬁcantly patients could successfully treated eﬀectiveness drug could still established high conﬁdence. work fold. spirit want determine whether bandit algorithms constitute feasible eﬀective adaptive trial strategies generally wish produce evaluation eﬀectiveness adaptive clinical trials based real-world data. answer questions simulate using real data would happened clinical trial opioid addiction treatments used adaptive bandit strategies instead simple randomization. measure eﬀectiveness approach variety criteria including number patients successfully treated patient retention number adverse eﬀects others. bandit-based treatments would allowed least patients successfully treated signiﬁcantly reducing number adverse eﬀects increasing patient retention. trial best treatment could still identiﬁed high level statistical conﬁdence. ﬁndings demonstrate bandit algorithms attractive alternatives current treatment allocation strategies. paper organized follows. section brieﬂy review studied algorithms. section describes setup used experiments. section presents selection representative results. section discuss three main conclusions drawn results well implications subsequent theoretical empirical work. section brieﬂy resummarize motivation considering clinical trials context multi-armed bandit algorithms. section present trial base simulation section describe detail simulation performed. section present results section discuss three questions simulation intending answer light results obtained. conclude section ﬁrst four algorithms \u0001-greedy boltzmann exploration pursuit reinforcement comparison. heuristics captures distinct ideas handling exploration/exploitation tradeoﬀ example boltzmann exploration based principle frequency plays proportional average reward; idea pursuit maintain explicit probability distribution arms search directly space probability distributions. even though idea represents important approach handling exploration/exploitation tradeoﬀ none heuristics well understood theoretically. even aware empirical study systematically evaluates pursuit reinforcement comparison. expected regret. fact algorithm solves multi-armed bandit problem optimally constant factor made precise below. former intuitive heuristics compare sophisticated algorithms important question answer clearly known. order deﬁne algorithms following notation. strategies maintain empirical reward means updated every turn denote empirical mean turns. probability picking time denoted held constant linear bound expected regret achieved. cesabianchi fisher proved poly-logarithmic bounds variants algorithm decreases time. earlier empirical study vermorel mohri practical advantage using methods. therefore experiments consider ﬁxed values softmax methods based luce’s axiom choice pick probability proportional average reward. arms greater empirical means therefore picked higher probability. experiments study boltzmann exploration softmax method selects using boltzmann distribution. given initial empirical means temperature parameter controlling randomness choice. boltzmann exploration acts like pure greedy. tends inﬁnity algorithms picks arms uniformly random. case \u0001-greedy polylogarithmic regret bounds exist cesa-bianchi fischer decreasing schedules. however empirical evidence suggests schedules oﬀer practical advantage vermorel mohri ﬁxed values experiments. arms whose updates informed empirical means performed separately. simple version pursuit algorithm described sutton barto algorithms starts uniform probabilities assigned turn probabilities re-computed follows used reinforcement learning sequential decision problems. rajaraman sastry provide pac-style convergence rates diﬀerent forms pursuit algorithm context learning automata. reinforcement comparison methods sutton barto similar pursuit methods maintain distribution actions computed directly empirical means. methods also maintain average expected reward probability selecting computed comparing empirical mean probability increased average decreased otherwise. intuitively scheme designed account cases arms similar value. family algorithms proposed auer cesa-bianchi fisher simpler elegant implementation idea optimism face uncertainty proposed robbins extension ucb-style algorithms sequential tree-based planning developed kocsis szepesvari proven successful playing programs. authors also propose another algorithm ucb-tuned claim performs better practice comes without theoretical guarantees. main feature ucbtuned takes account variance empirical mean. speciﬁcally turn algorithm picks estimate variance computed usual maintaining empirical squares reward addition empirical mean. audibert munos szepesvari provide expected regret bounds regret concentration results variance-based algorithms similar ucb-tuned. instance bandit problem fully characterized number arms arms’ reward distributions. however every aspect distributions aﬀects relative performance algorithms. clearly number arms reward variance aﬀect performance. quite surprisingly turn characteristics bandit need considered. moments higher variance little importance comparing algorithms. goals experimental setup thus twofold identify characteristics bandit problem aﬀect algorithm performance evaluate exactly aﬀect performance. achieve that vary isolation number arms variance well type reward distribution distribution expected values arms algorithms admit parameters tune algorithm optimally experiment. ﬁrst criterion summarizes performance algorithm number second illustrates detail algorithm handled problem. third criterion relevant situations minimizing number suboptimal plays important every experiment repeated times every repetition expected values reward distributions chosen uniformly random results averaged independent runs. evaluate algorithms settings number arms equals value included special case practical applications. values larger provide tasks diﬃcult algorithms. however relative behavior consistent case even higher values. values represent good benchmarks settings small medium numbers arms respectively. unless indicated otherwise rewards sampled normal distribution. evaluate algorithms settings variance parameter equals every arm. values correspond standard deviations interval containing expected values. obviously smaller variance leads easier problem arms well separated. evaluate eﬀects higher moments experimented several types reward distributions case parameters distribution chosen obtain identical expected values variances. results similar distributions normal distribution main experiments. means arms randomly chosen interval repetitions. experimented choosing means according uniform distri variance uniform distribution). results similar terms ranking behavior algorithms main experiments report results uniform distribution. algorithms require initial values empirical means always optimistic initialization start initial values found choice always results best performance. every experiment algorithms tuned maximum performance according ﬁrst criterion almost always leads best possible performance second criterion good performance third. optimized parameter settings included legends next graphs. section ﬁrst present main results obtained evaluating algorithms twelve multi-armed bandit instances. instance characterized diﬀerent combination number arms reward variance. second half section present results demonstrating aspects bandit important purpose evaluating algorithm performance. finally tuning algorithms observed parameters dramatically aﬀect performance include material illustrates that. main results presented figure figure figure figure case report every value variance total regret achieved algorithm. also present graphs regret percentage optimal plays respect time. striking observation simplest algorithms \u0001-greedy boltzmann exploration outperform competitors almost tasks. heuristics perform similarly softmax usually slightly better. particular softmax outperforms algorithms terms total regret tasks except high-variance setting small medium numbers arms settings softmax comes second behind ucb-tuned algorithm. sense fact ucbtuned speciﬁcally designed sensitive variance arms justiﬁes fact superior high values variance. relatively little note performance algorithms. pursuit methods perform overall worst plateau sub-optimal solutions time steps. converges solution much slowly algorithms although ﬁnal solution appears good consistent results reported auer reinforcement comparison generates good average regret turn towards turns small numbers arms starts trailing algorithms larger values total regret relatively high slower beginning overall results suggest advantage using pursuit reinforcement comparison practice. second important observation algorithms aﬀected diﬀerently variations characteristics bandit. methods example handle bandits small numbers arms high reward variances well performance deteriorates much quickly algorithms becomes large. expected values reward distribution sampled normal distribution algorithms performed slightly better uniform setting. surprising best better separated others expected values normally distributed. however observe signiﬁcant diﬀerences relative ranking algorithms therefore omit detailed results. example observed present figure graphs setting quite surprisingly type reward distribution noticeable eﬀect performance algorithms. figure example. rather counter-intuitive would expect example would harder identify best reward distribution skewed left. observed algorithm parameters signiﬁcantly aﬀect performance. average increase total regret incorrectly tuned algorithm roughly although several cases increase regret much larger. surprisingly parameter value optimal bandit instance could suddenly become worst increased reward variance notch. illustrate behavior show table total regret achieved boltzmann exploration given diﬀerent values measurements carried tune every algorithm every combination similar behavior observed every case. initially tuned algorithms value used parameters values results obtained somewhat diﬀerent reported above. suspect many empirical studies also fail properly tune every algorithm many bandit strategies underperforming. results admittedly similar observation made vermorel mohri experiments ﬁrst strongly suggest behavior occurs practically every bandit problem instance. results also illustrate magnitude advantage simple heuristics oﬀer. boltzmann exploration generally outperforms closest competitor anywhere advantage therefore appears quite substantial. results also indicate need formal analysis simple heuristics generally development theoretically sound algorithms perform well simpler heuristics practice. even though auer derive polylogarithmic bounds \u0001-greedy boltzmann exploration remains open problem determine whether simple heuristics optimal sense achieving regret. generally current theoretical bounds need improved capture least rich behavior observed experiments. since problem balancing exploration exploitation appears throughout reinforcement learning numerous algorithms employ bandit strategies subroutines. example well known algorithm leverages bandit strategies solving mdps kocsis szepesvari algorithm many others bandit strategy ucb’s theoretical properties often exploited analysis original algorithm. however results indicate ucb-based algorithms always considered along ones based \u0001-greedy softmax since switching heuristics yield substantial performance improvements practice. every algorithm settings performs well compared strategies settings performs poorly. sense algorithms possess speciﬁc strengths weaknesses. performance family example excellent bandits small number arms high reward variances degrades rapidly number arms increases. important algorithm properties described theoretical result even though result would valuable choosing algorithms speciﬁc problems. absence theoretical results empirical measurements used guide design heuristics speciﬁc bandit problems. example consider multi-stage clinical trial number type treatments varies signiﬁcantly stage stage. treatments’ eﬀectiveness unknown would like identify best treatment maximize number successfully treated patients. good banditbased heuristic assigning patients treatments based diﬀerent algorithm every stage experiments suggest ones choose. finally would like point extreme variability algorithm performance makes necessary evaluate algorithms wide range settings past empirical studies seldom done. relative performance algorithms appears aﬀected number arms reward variance. interesting implications type regret bounds expect obtain. recent theoretical work focused obtaining improved regret bounds considering reward variance results suggest considering higher moments reward distribution fruitful since type reward distribution little impact algorithm performance. importantly experiments precisely identiﬁed aspects bandit problem must considered accurately evaluate algorithm made apparent need ﬁnely tune algorithms every bandit setting. experimental setup thus forms good example needed accurately evaluate algorithms. similar methodology adopted subsequent studies algorithms evaluated accurately become easy compare studies. fact suggest measurements used comparison point bandit strategies. accurate systematic measurements algorithm performance would useful practice could help direct research eﬀort towards promising algorithms. turn attention important application bandit algorithms design adaptive clinical trials. study whether bandit algorithms well suited allocating patients treatments clinical trial. answer questions simulate using real data would happened clinical trial opioid addiction treatments used adaptive bandit strategies instead simple randomization. measure eﬀectiveness approach variety criteria including number patients successfully treated patient retention number adverse eﬀects others. trial conducted compare eﬀectiveness buprenorphinenaloxone clonidine treating opioid addiction among in-patients out-patients within community treatment programs. primary goal conﬁrm earlier results established bupnal superior treatment; therefore stage trial. adaptive strategy would particularly suited context since identiﬁcation best treatment already done large extent. patients randomized ﬁxed ratio majority assigned bupnal. initially in-patients out-patients expected enrolled study. however trial terminated earlier expected in-patients out-patients participated. patients arrived period weeks admitted trial wednesday. received treatment days tested opioids using urine test. patient providing opioid-free urine sample considered successfully treated opioid-positive result failure provide urine sample considered treatment failure. among in-patients individuals assigned bupnal achieved treatment success compared clon patients. among out-patients bupnal individuals achieved success criterion compared assigned clonidine. patients’ condition also measured course treatment. every adverse eﬀects recorded several tests designed measure patients’ level well-being administered. simulation data tests arsw vas. arsw consists series observations performed doctor recorded numerical value scale example doctor would estimate irritated patient watery eyes appear. summary arsw result deﬁned observations. test consists asking patient place mark line indicate strength craving opiates indicating craving indicating extreme craving. data trial publicly available internet. unfortunately lacking detailed documentation unclear interpreted obtain results reported link therefore make certain minor assumptions regarding data since algorithms aﬀect validity results. present assumptions appendix simulation implemented python source code program available upon request. treatment strategy class patients simulations trial performed. results presented report form average simulations. simulation proceeds follows. total patients assigned arrival dates random period weeks treatment strategy picks treatment patient order arrival study. decisions based information available arrival results back algorithm weeks later. words algorithm observe outcome assigning patient bupnal week week outcome patient available processing patients coming weeks treatment strategy assigns patient either bupnal clon determine outcome participation period craving levels adverse eﬀects results patient sampling random replacement chosen treatment’s population. thus making bootstrap estimates patients’ true results. preserve natural relationships among attributes example take account fact patient negative responder also likely high craving ratings. rewards bandit algorithms patient positive response negative response. thus algorithms essentially playing multi-armed bandit bernoulli arms. algorithms admit initial empirical means values performance bandit algorithms similar therefore present results \u0001-greedy softmax ucb-tuned. results in-patients out-patients also exhibited similar features therefore include in-patient results. out-patient material available appendix out-patient setting fact harder in-patient setting success rate clon among out-patients minimal number clon successes required obtain good conﬁdence level best treatment. figure display every algorithm average number patients treated turn average taken repetitions simulated trial. figure directly corresponds instantaneous regret plots ﬁrst half paper. adaptive clinical trial must maximize number treated patients importantly must also identify best treatment high level statistical conﬁdence. obtaining rigorous statistical guarantees performance best treatment adaptive trial generally considered diﬃcult often cited main causes limited adoption study however take approach compute pvalues null hypothesis treatments equal probabilites success using simple test. test independence common clinical trials literature although give weaker guarantees advanced statistical techniques p-values produces hold even adaptive trial setting. important clinical trial retain large fraction patients treatment. patients adequately treated importantly clinical data accurately processed using statistical methods. treatment retention represented using kaplan-meier curves show percentage patients remaining treatment day. figure shows kaplan-meier curves algorithm. bandit algorithms’ curves nearly indistinguishable other hence bother annotate them. addition number treated patients looking level well-being patients still treatment. measure patient well-being number adverse eﬀects occur. figure shows average number adverse eﬀects patient treatment. value computed dividing total number adverse eﬀects given number patients still treatment. again bandit strategies nearly indistinguishable other. practical constraints unknown arrival times patient dropout pose problem context simulation. since dropout interpreted treatment failure required missing treatment outcomes. treatment patient dropout inspired methodology initial study link used many clinical trials. .×−. identifying best treatment out-patient setting much diﬃcult success rate clonidine test requires minimum number successes every treatment. expected bandit algorithms diﬃculty obtaining good p-value still performed well. worst p-value returned epsilon greedy. although much weaker simple randomization returned even conﬁdence level would generally suﬃcient establish superiority bupnal. treatment strategies achieved much better p-values. therefore conclude bandit algorithms reliably identify best arm. clearly adaptive bandit-based clinical trials result large increase patient welfare. in-patient out-patient cases least patients successfully treated. interesting observe almost algorithms able best treatment processing patients point administered better treatment. addition percentage patients still trial days increased almost much fewer adverse eﬀects observed. out-patient setting almost adverse eﬀects occurred patients’ levels well-being clearly better bandit-based trials. arsw scores almost lower adaptive trials in-patient case. results strongly suggest average patient adaptive trial able overcome addiction much easily. paper presented empirical study popular algorithms multi-armed bandit problem. current theoretical guarantees accurately represent real-world performance bandit algorithms. empirical results hand usually apply small number strategies limited number settings fail consider every important aspect bandit problem notably reward variance. proceeded evaluate performance algorithm twelve values parameters. twelve settings meant cover types bandits appear practice. surprisingly simple heuristics consistently outperformed advanced algorithms strong theoretical guarantees. almost every bandit problem instance softmax algorithm generated least less regret ucb-tuned best algorithm theoretical guarantees exist. ﬁnding clearly shows need theoretical analysis simple heuristics. addition observed algorithm performance varies signiﬁcantly across bandit instances. identiﬁed algorithm instances performs well instances performs poorly. information exploited practice designing heuristics real-world instances bandit problem. finally study precisely identiﬁed aspects bandit problem must considered experiment demonstrated need tune every algorithm every instance problem. experimental setup forms good reference future experiments data obtained serve point comparison algorithms. addition including algorithms considering diﬀerent variances numbers study could improved considering settings reward variances identical. certain algorithms ucb-tuned speciﬁcally designed take account variance arms therefore advantage settings. second half paper turned attention important application bandit problem clinical trials. although clinical trials motivated theoretical research multi-armed bandits since robbins’ original paper bandit algorithms never evaluated treatment allocation strategies clinical trial. study simulated using real data would happened clinical trial used bandit strategies instead simple randomization. found banditbased strategies successfully treated least patients resulted fewer adverse eﬀects fewer cravings greater patient retention. trial best treatment could still identiﬁed high degree statistical conﬁdence. results oﬀer compelling reasons adaptive clinical trials particular ones based multi-armed bandit algorithms establishing eﬃcacy medical treatments. study could improved considering larger dataset richer clinical setting multi-stage trial. indeed experimental setup simple particular unable identify signiﬁcant diﬀerence performance among algorithms. generally bandit algorithms applied problems ﬁelds online advertising network routing. although online advertising datasets diﬃcult obtain applying bandit algorithms ﬁeld would particularly interesting since advertising problem great practical signiﬁcance existing empirical results rather limited. overall surprising problem broadly applicable bandit problem scarcely studied applications point view. hope empirical study encourage authors apply bandit algorithms interesting real-world problems.", "year": 2014}