{"title": "An Exploration of Softmax Alternatives Belonging to the Spherical Loss  Family", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In a multi-class classification problem, it is standard to model the output of a neural network as a categorical distribution conditioned on the inputs. The output must therefore be positive and sum to one, which is traditionally enforced by a softmax. This probabilistic mapping allows to use the maximum likelihood principle, which leads to the well-known log-softmax loss. However the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions. It is thus unclear why the log-softmax loss would perform better than other loss alternatives. In particular Vincent et al. (2015) recently introduced a class of loss functions, called the spherical family, for which there exists an efficient algorithm to compute the updates of the output weights irrespective of the output size. In this paper, we explore several loss functions from this family as possible alternatives to the traditional log-softmax. In particular, we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and the log-Taylor Softmax that we introduce. Although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks, they surprisingly outperform it in our experiments on MNIST and CIFAR-10, suggesting that they might be relevant in a broad range of applications.", "text": "alexandre br´ebisson pascal vincent∗ mila d´epartement d’informatique recherche op´erationnelle university montr´eal alexandre.de.brebissonumontreal.ca vincentpiro.umontreal.ca multi-class classiﬁcation problem standard model output neural network categorical distribution conditioned inputs. output must therefore positive traditionally enforced softmax. probabilistic mapping allows maximum likelihood principle leads well-known log-softmax loss. however choice softmax function seems somehow arbitrary many possible normalizing functions. thus unclear log-softmax loss would perform better loss alternatives. particular vincent recently introduced class loss functions called spherical family exists efﬁcient algorithm compute updates output weights irrespective output size. paper explore several loss functions family possible alternatives traditional log-softmax. particular focus investigation spherical bounds log-softmax loss spherical log-likelihood losses namely log-spherical softmax suggested vincent log-taylor softmax introduce. although alternatives yield good results log-softmax loss language modeling tasks surprisingly outperform experiments mnist cifar suggesting might relevant broad range applications. classiﬁcation problems high dimensional outputs particularly common many language applications target word predicted large vocabulary. standard application backpropagation take advantage sparsity categorical targets result computations update weights output layer prohibitively expensive. popular workarounds based approximations divided main approaches. ﬁrst sampling methods approximations compute tiny fraction output’s dimensions mnih kavukcuoglu mikolov shrivastava second hierarchical softmax modiﬁes original architecture replacing large output softmax heuristically deﬁned hierarchical tree mikolov vincent recently proposed algorithm compute exact updates output weights efﬁcient fashion provided loss belongs particular class functions call spherical family includes alternative softmax named spherical softmax ollivier rest paper call losses spherical losses. denote dimension last hidden layer dimension high dimensional output layer showed spherical loss possible compute exact updates output weights instead naive implementation. however remains unclear spherical losses compare traditional log-softmax loss context classiﬁcation. precisely investigate paper. ﬁrst describe precisely spherical family extract spherical bounds log-softmax loss identify particular normalizing activation functions namely spherical softmax taylor softmax lead log-likelihoods belong spherical family suitable train neural network classiﬁers. finally evaluate different losses empirically training models several tasks mnist cifar/ language models penntree bank billion word dataset. linear outputs neural network dimension represents dimensional output last hidden layer. sparse target indices non-zero elements spherical family described vincent composed vincent showed loss functions possible compute exact gradient updates without even computing output instead naive implementation. focus classiﬁcation problems assume rest paper contains single target class index corresponding single non-zero element resulting family rewritten follows loss choice regression problems. also sometimes used classiﬁcation problems even though log-softmax loss nowadays considerably popular. contrary log-softmax loss using square error classiﬁcation correspond conditional likelihood categorical distribution. nevertheless like log-softmax loss likelihood losses mean square error desirable property minimum conditional expectation. bound clearly belongs spherical family. tried approaches determine optimal either considering ﬁxed hyperparameter optimizing every example yield tightest bound. minimizing bound hope minimize indirectly negative log-softmax. classiﬁcation problems standard model output categorical posterior distribution hence computed output must consist positive values generally enforced softmax function applied linear output however property holds general class normalizing functions output represents categorical distribution corresponding network trained maximizing likelihood training dataset i.e. minimizing negative log-likelihood however despite widely used remains unclear softmax compares normalizing functions. particular normalizing activation functions following form lead log-likelihoods belong spherical family ﬁrst spherical alternative softmax function consider spherical softmax minor modiﬁcation non-linearity investigated ollivier small constant added numerical stability reasons interesting property spherical softmax invariant global rescaling pre-activations contrasts translation invariance softmax unclear desirable property. also notice that contrary softmax function spherical softmax even i.e. ignores sign pre-activation softmax function taken absolute value pre-activations. second spherical alternative softmax comes second-order taylor expansion exponential around zero leads following function call taylor softmax numerator taylor softmax assured strictly positive greater minimum value. gradients well-behaved well risk numerical instability. therefore contrary spherical softmax need extra hyperparameter furthermore unlike spherical softmax taylor softmax small asymmetry around zero. section compare log-softmax different spherical alternatives several tasks mnist cifar/ language modeling task penntree bank billion word datasets. goal reach state task compare inﬂuence loss. therefore restricted reasonably sized standard architectures little regularization ensembling data augmentation apart cifar. experiments used hidden layers rectiﬁers whose weights initialized standard deviation suggested output layers initial weights zero. language experiments bias values initial network outputs matched prior frequencies classes. experiments train neural language models softmax outputs minimizing spherical upper bounds given section results disappointing. optimizing bound actually degraded initial perplexity means minimum bound worse simple initialization. subsections below thus provide detailed results promising spherical losses outlined section ﬁrst compared effectiveness different loss functions training mnist digit classiﬁers used architecture different losses convolutional neural network composed conv-pooling layers followed fully connected layer neurons output layer. used rectiﬁers hidden neurons initialized weights scheme networks trained minibatches size nesterov momentum decaying learning rate initial learning rate hyperparameter tuned individually loss. used early stopping validation dataset stopping criterion. table test performances convolutional network trained mnist different loss functions. loss results averaged runs standard deviation parenthesis. loss column reports training loss evaluated test set. negll refers negative loglikelihood. softmax corresponds log-softmax loss except softmax applied absolute value pre-activations. log-taylor softmax outperforms log-softmax especially respect negative log-likelihood. table test performances convolutional network trained mnist different loss functions trained evaluated ofﬁcial training testing sets mnist loss results averaged runs different initial random parameters standard deviation parenthesis. loss column reports training loss evaluated test set. negll refers negative log-likelihood. results signiﬁcantly better reported table suggesting ofﬁcial mnist particularly advantageous. log-taylor softmax still outperforms log-softmax. order obtain results reliably reﬂect effect individual loss repeated training times different random splits training/validation/testing datasets different initial random weight values. results reported table averaged scores obtained test runs. standard deviations reported parenthesis. note results computed averaged random splits training/valid/test datasets order reliable. also trained best models original dataset split mnist results reported table signiﬁcantly better random splits suggesting ofﬁcial testing simpler classify randomly extracted one. cifar dataset composed images size output categories. experiments used large convnet architecture layers ﬁlters size pooling windows size used weight decay batch normalization random horizontal ﬂips. softmax taylor softmax averaged testing scores runs different splits training/validation/testing datasets different initial weight values. tuned initial learning rate loss function. table reports performances test different losses. table test performances convolutional network trained cifar different loss functions. log-softmax log-taylor softmax results averaged experiments order reliable standard deviation parenthesis. log-spherical softmax time single experiment. log-taylor softmax outperforms log-softmax. table performances test convolutional network trained cifar different loss functions. loss results averaged experiments order reliable standard deviation parenthesis. log-softmax outperforms spherical losses. vocabulary words. trained neural language model vanilla stochastic gradient descent mini-batches size using input context words. models embedding size hidden activation functions rectiﬁers. loss function hyper-optimized learning rate number layers number neurons layer. model computed perplexity test exponential mean negative log-likelihood. also computed simlex- score measures quality word embeddings based similarity words evaluated humans. table reports results obtained best models different losses. table comparison different losses used train neural language model penntree bank dataset. loss hyperparameters controlling model architecture tuned individually yield best perplexity validation set. error rate measures proportion time target word among predicted words. hidden layers neurons each. three hidden layers neurons each. three hidden layers neurons each. architecture except softmax applied absolute value pre-activations. also trained word-level neural language models billion word dataset composed billion words belonging vocabulary million words. experiments chose restrict vocabulary words. training dataset almost times bigger penntree bank dataset even full epoch thus constantly regime online learning training example seen before. result almost impossible overﬁt training dataset reasonable size models. bigger models tend always perform better chose restrict architecture sizes compared different losses those rather exhaustive architecture search case experiments penntree bank dataset. mnist cifar spherical losses work surprisingly well ﬁxed architectures used even outperform log-softmax. suggests log-softmax necessarily best loss function classiﬁcation alternatives categorical log-losses spherical family might preferred broad range applications. hand experiments higher output dimensions i.e. cifar penntree bank billion word dataset found softmax yields better results log-spherical softmax log-taylor softmax. reasons apparent qualitative shift number output categories increases remain unclear venture hypothetical leads. ﬁrst exponential non-linearity softmax boosts large pre-activations relatively smaller ones squaring operation. thus yields stronger competition pre-activations discriminative behavior. possible resulting ability precisely single winning classes becomes increasingly crucial number categories grows. second lead relies fact exponential linear combination features product exponentials weighted feature. exponential factors represent probability density resulting product table comparison different losses used train neural language model billion word dataset. log-softmax outperforms spherical losses even though adding layers reduces gap. log-softmax adding hidden layers degrades simlex score improves spherical losses. increase ﬂexibility networks spherical losses tried increase non-linearity network prior loss adding layers replacing rectiﬁers stronger nonlinearities. particular tried softmax activation function last hidden layer. also tried directly exponential activation function hidden layers. case avoid excessive values exponential used truncated version also used batch normalization obtain reasonable ranges pre-activations prevent exponential exploding. although able train models perform better simple rectiﬁer layers. among approaches explored upper-bounding negative softmax spherical loss unsuccessful attempt. spherical losses tried promising particular log-taylor softmax seems appropriate classiﬁcation. contrary spherical softmax require extra hyperparameter make spherical softmax quite unstable difﬁcult train. also small asymmetry desirable property. experiments showed several dimensional problems log-softmax surprisingly outperformed certain losses spherical family particular log-taylor softmax. hand higher dimensional problems log-softmax yields better results. reasons qualitative shift remain unclear research carried understand chelba ciprian mikolov tomas schuster mike brants thorsten koehn phillipp robinson tony. billion word benchmark measuring progress statistical language modeling. interspeech gutmann hyvarinen noise-contrastive estimation estimation principle unnormalized statistical models. proceedings thirteenth international conference artiﬁcial intelligence statistics kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. ieee international conference computer vision ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. blei david bach francis proceedings international conference machine learning jmlr workshop conference proceedings mnih andriy kavukcuoglu koray. learning word embeddings efﬁciently noisecontrastive estimation. burges c.j.c. bottou welling ghahramani weinberger k.q. advances neural information processing systems curran associates inc. maximum inner product search lawrence n.d. advances tion processing systems curran associates http//papers.nips.cc/paper/-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner", "year": 2015}