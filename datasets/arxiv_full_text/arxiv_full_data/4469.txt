{"title": "Generative Adversarial Networks using Adaptive Convolution", "tag": ["cs.CV", "stat.ML"], "abstract": "Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator. We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances. We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem. We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator. Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin. Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.", "text": "existing gans architectures generate images transposed convolution resize-convolution upsampling algorithm lower higher resolution feature maps generator. argue kind ﬁxed operation problematic gans model objects different visual appearances. propose novel adaptive convolution method learns upsampling algorithm based local context location address problem. modify baseline gans architecture replacing normal convolutions adaptive convolutions generator. experiments cifar- dataset show modiﬁed models improve baseline model large margin. furthermore models achieve state-of-the-art performance cifar- stl- datasets unsupervised setting. generative adversarial networks unsupervised learning method able generate realistic looking images noise. gans employs minimax game generator network learns generate synthesized data random noise conjunction discriminator network learns distinguish real generated data. theoretically training processes networks intertwine iterate networks reach nash equilibrium real synthesized data indistinguishable. however practice gans notoriously hard train. learning generator happen effectively hyper-parameters well architectures generator discriminator must chosen carefully. nevertheless datasets visually similar images bedroom scenes faces gans produced good results success however translate datasets diverse visual categories. gans models trained imagenet able learn basic image statistics shapes learn object recent works address problem incorporating additional high-level information guide generator training discriminator semi-supervised manner adding second training objective direct generator toward similar samples training using artiﬁcial class labels clustering representation space take different approach tackle problem. hypothesize rigidity normal convolution operator partially responsible difﬁculty gans learn diverse visual datasets. gans generators upsample resolution feature maps toward higher resolution using ﬁxed convolutions resize-convolution operations unintuitive because pixel locations different local contexts belong diverse object categories. consequently different pixel locations different upsampling schemes. shortcoming normal convolution especially problematic early upsampling layers higher level information usually associates object shapes context images. propose novel adaptive convolution architecture called adaptive convolution block replaces normal convolutions address aforementioned shortcoming gans generators. instead learning ﬁxed convolution upsampling pixels lower higher resolution feature adaconvblock learns generate convolution weights biases upsampling operation adaptively based local feature pixel location. adaconvblock helps generator learn generate upsampling algorithms take account local context. believe kind adaptive convolution intuitive akin process human draws something thought process used whole drawing style stroke vary depend local context around pixel position. conduct experiments compare adaptive convolution method normal convolution unsupervised setting. progressively replace convolutions gans generator adaconvblocks lowest resolution highest resolution. experiments cifar- dataset show modiﬁed adaptive convolution models superior qualitative quantitative performance baseline architecture replacing convolution upsampling lowest resolution feature adaptive convolution signiﬁcant impacts baseline model. furthermore best models achieve state-of-the-art unsupervised performance cifar- stl- datasets. code models released. generative adversarial networks framework generator tries mimic real data target distribution pitted discriminator tries distinguish generated samples target distribution. trained increase chance generated samples classiﬁed real data trained minimize training processes alternate formulated minimax game qdata real data distribution commonly used distribution random noise variable drawn generator function maps random noise variable real data space function outputs probability data point belonging target real data distribution. training process gans takes turns update discriminator number times updating generator once. ideally discriminator trained convergence updating generator. however computationally infeasible causes overﬁt datasets ﬁnite data. framework gans differentiable functions. image generation usually formulated convolutional neural networks. generator usually consist fully connected layer project random variable small volume followed upsampling layers using convolutions progressively reﬁne volume desired spatial dimensions discriminator usually constructed reverse generator using strided convolutions downsample feature maps. difﬁculties training gans well known. example balance strength generator discriminator essential successful training. strong close zero would almost gradient could learn generate data. hand weak able provide good feedback improve. another prominent issue mode collapse learns majority random distribution regions real data space resulting near duplicate images. overall training gans unstable sensitive hyper-parameters. several works done address difﬁculties training gans. wgan pioneered conditioning lipschitz function using weight clipping. danihelka proposed improved version called wgan-gp enforces conditioning penalizing gradient straight lines real generated samples. spectral normalization gans recent works category. spectral normalization controls lipschitz constant discriminator dividing weight matrix layer spectral norm equal largest singular value. largest singular value matrix efﬁciently computed approximation algorithm spectral normalization shown make gans robust hyper-parameters choices without incurring signiﬁcant overhead. reasons spectral normalization train architectures. although progress made improving training procedure datasets visually diverse images still pose challenge gans. gans fail produce good-looking samples even dimension datasets like cifar- stl-. paper propose novel adaptive convolution block replacement normal convolutions gans generator tackle issue. adaconvblock thought increase capacity generator making easier learn sophisticated multimodal distributions underlying visually diverse datasets. details network architectures adaconvblocks shown section note kind adaconvblock describe paper replace normal convolution. case transposed convolution adaptive transposed convolution block implemented simply rearranging input volume ﬁrst apply adaconvblock rearranged volume. implementation difﬁculties rearrangement operation tensorﬂow deep learning framework experiment adaptive convolution blocks paper. adaptive convolution block consider convolution operation ﬁlter size ilter ilter number output channels cout feature input channels. every location convolution requires weight matrix size ilter ilter cout ﬁlters convolved local feature spatial dimension size ilter ilter followed adding bias matrix size cout channels previous convolution. normal convolution spatial locations input feature weight wnormal bias bnormal. shared weight bias matrices serve learned variables normal convolution. adaptive convolution however spatial locations share weight bias variables. rather share variables used generate weight bias pixel location based local information. pixel adaptive convolution consists normal convolutions regress adaptive weight adaptive bias location followed local convolution local feature addition previous local convolution. case learnable variables adaptive convolution weights bias matrices convolutions used generate naive adaconvblock learns four matrices size kadaptive kadaptive cadaptive cadaptive kadaptive kadaptive cout cout serial order. weight bias matrices convolution regress adaptive weight weight bias matrices convolution regress adaptive bias pixel location. kadaptive ﬁlter size convolution input feature regress from. cadaptive ilter ilter cout number output channels convolution regress equal size matrix wnormal normal convolution. note kadaptive controls amount local information used adaconvblock different regressed ﬁlter size ilter. denote input feature fout output feature naive adaconvblock exact formulation fout wadaptive badaptive volumes consisting adaptive convolution weights biases note wadaptive contains weights ﬂattened vectors. adaptive denotes wadaptive adaptive weight matrices reshaped back appropriate shape convolution. relu denotes relu activation function. ∗local denotes local convolution operator. drawback naive adaconvblock however extremely expensive operation computing adaptive convolution weights input volume amount memory computation used operation grow proportionally kadaptive kadaptive cadaptive cout. depthwise separable convolution place normal convolution reduce computation cost well memory usage operation. depthwise separable convolution replaces normal convolution convolutions convolution acts separately channel followed immediately convolution mixes output previous convolution number output channels ﬁrst depthwise convolution memory computation costs proportional adaptive cdepthwise second pointwise convolution memory compuk cout depth multiplier cdepthwise tation costs proportional cdepthwise number output channels input channel depthwise convolution. adaconvblocks architectures cost pointwise convolution dominates cost depthwise convolution. choosing cdepthwise separation convolution smaller convolutions cuts amount memory computation cost models roughly first adaptive convolution weights biases regressed necessarily input volume. additional transformations applied input volume regressing weights biases. tried transformations found cripple performance network. example dilated convolutions used exponentially increase receptive ﬁeld regression weights biases. increase receptive ﬁeld make object shapes coherent. however practice found using multiple dilated convolutions made training unstable. effect achieved increasing kadaptive adaptive convolution without downside. another idea tried convolutions regression increase nonlinearity adaconvblock. however experiments showed detrimental generator hammered model’s performance. next considered choice activation functions lack batch normalization adaconvblock. regress convolution weights biases apply batch normalization reasons regressed weights biases follow probability distribution. applied non-linearity convolution regress weights. empirically found relu activation made adaconvblock work better activation functions including identity activation regress biases apply activation function results unwanted effects limiting output adaconvblock range. lastly described section reduce memory computation cost used depthwise separable convolutions depth multiplier equal place normal convolutions regressing adaptive convolution weights. depthwise separable convolutions also another beneﬁt made memory computation cost almost insensitive parameter kadaptive allowed increase receptive ﬁeld regression almost cost. choice depth multiplier came experiments. empirically found increasing depth multiplier increased memory computation cost also slowed training progress. overall improve model’s performance. several works seek improve gans performance datasets high visual variability. salimans proposed semi-supervised training procedure gans. instead learning distinguish real fake samples discriminator also learns classify class real data points belong method turns discriminator -way classiﬁer classes real data class fake sample. empirical results show kind formulation surprisingly improves quality generated images. based ﬁndings work warde-farley bengio trained denoising auto-encoder feature discriminator penultimate layer. generated sample squared difference discriminator feature reconstructed feature denoising auto-encoder penultimate layer minimized. additional training objective effect guiding generated samples toward regions feature space correspond higher probability conﬁgurations. procedure referred authors denoising feature matching. grinblat employed simple successful artiﬁcial class augmentation method training gans dividing samples using k-means clustering representation learned last hidden layer. cluster treated artiﬁcial class. networks trained acgan using artiﬁcial class labels. generator uses random noise variable artiﬁcial class label generate fake samples discriminator tries classify whether sample real fake also construct probability distribution artiﬁcial class labels. discriminator starts cluster unsupervised case. training progresses cluster split samples threshold. labels cluster replaced ones whole dataset. step training resumed clusters. aforementioned methods successful varifying degrees. however common theme among make additional high level information whether directly training labels indirectly discriminator augment training objectives direct generator toward better sample generation. approach different better generator output improving architecture. method complementary existing methods combination potential yield better results. method inspired work niklaus applies adaptive convolution video frame interpolation. authors trained encoder-decoder network extract features neareast-neighbor resize. stride= output channels conv. batchnorm. relu neareast-neighbor resize. stride= output channels conv. batchnorm. relu neareast-neighbor resize. stride= output channels conv. batchnorm. relu large image patches video frames. features four subnetworks estimate four kernels used interpolation. although base idea using adaptive convolution similar differences work originates differences problems. video interpolation task regress small number outputs pixel location size model well outputs grow cubically size base channels. constraint makes efﬁcient memory important work. secondly ﬁlters video frame interpolation task limited range case gans convolution ﬁlters. therefore design paradigms architectures different. perform experiments cifar- stl- datasets purely unsupervised setting. labels additional training objectives used training process. spectral normalization applied discriminator stabilize training experiments. zero padding used convolutions. weights initialized using truncated normal distribution mean zero standard deviation biases initialized zero. following miyato adam optimizer beta batch size experiments. number discriminator updates generator update also ﬁxed one. aligning previous works compute mean standard deviations inception score groups randomly generated images. metrics reported every training iterations ﬁnally model highest mean score selected architecture. baseline architecture based spectral norm architecture. replace transposed convolution generator network resize-convolution upsampling algorithm. generator consists layers. ﬁrst layer gaussian noise generator followed immediately fully connected layer project noise vector volume spatial shape square side depends dataset depth base channels equal reduce base channels baseline generator reason architectures using adaconvblocks memory base channels. table show architecture baseline generator. note baseline generator discriminator work balanced leads relatively inception score. progressively replace convolutions third last layer baseline generator table adaconvblocks. note convolution last layer part upsampling step. however experiments replacing convolution also improves performance model slightly. adaconvblocks replace normal convolutions must keep ﬁlter size ilter output channels cout intact leaving parameter left neareast-neighbor resize. ilter cout adaconvblock. batchnorm. relu neareast-neighbor resize. ilter cout adaconvblock. batchnorm. relu neareast-neighbor resize. ilter cout adaconvblock. batchnorm. relu vary size local window regress adaptive weights biases kadaptive. generator base channel architectures adaconvblocks memory. memory computation cost adaconvblock grows cubically number input channels adaconvblocks determined base channels. therefore reduce number base channels architecture. consequently reduce base channels baseline generator well. name architectures based number adaconvblocks used replace normal convolution baseline model. example adagan- model convolution third layer replaced adaconvblock adagan- model convolutions third fourth layers replaced adaconvblocks adagan- model convolutions replaced except last layer. additionally name adagan model convolutions replaced adaconvblocks. table shows architecture adagan model. adagan- adagan- adagan- architectures derived easily table table choice kadaptive important factor performance architectures. ideally kadaptive chosen separately layer. however simplicity kadaptive adaconvblocks architecture. append kadaptive kadaptive name every architecture. example adagan--x adagan- architecture kadaptive three adagan-x adagan architecture kadaptive ﬁve. show effectiveness adaconvblocks compare performance baseline generator architectures cifar- dataset. kadaptive adaconvblock experiment. train models iterations. table shows inception score baseline generator versus architectures. experimental results show inception score increases number adaconvblocks used place normal convolutions. replacing convolution ﬁrst upsampling layer adaconvblock highest impact improving mean inception score points difference. adaconvblock upsampling layer helps increase generator capacity signiﬁcantly allowing generator counterbalance discriminator strength thus leads much better training results. beneﬁts adaconvblocks weaken gracefully subsequent layers become negligible last layer. adagan-x architecture base channels beats spectral norm normal convolutions large margin even though latter uses generator base channels arguably better balance. therefore increases inception scores models compared baseline model cannot attributed effect balancing generator discriminator alone ﬂexibility induced adaconvblocks must played major role. conﬁrms hypothesis using normal convolution upsampling layers limits performance generator adaptive convolution used alleviate problem. test effects kadaptive additionally train another adagan-x model leads small increase mean inception score adagan-x model. adagan models achieve state-of-the-art performance cifar- dataset. table second column shows unsupervised inception scores adagan models compared methods cifar-. figure appendix show samples generated adagan models. stl- stl- experiments train unlabeled downsample images following warde-farley bengio stl- bigger image size cifar larger kadaptive maybe helpful. thus train adagan-x model dataset well. architectures converge much slower stl- therefore train models iterations. adagan-x adagan-x models achieve state-of-the-art performance adagan-x model behind work grinblat table third column shows unsupervised inception scores models methods. figure appendix show samples generated models. demonstrated using adaptive convolutions replace normal convolutions gans generator improve performance weak baseline model signiﬁcantly visually diverse datasets. adagan models able beat state-of-the-art methods without using augmented training objectives. samples generated models show seem able learn global context pretty well able learn rough shapes objects cases sample quality quite reasonable cifar- dataset. furthermore much visible convolution artifacts generated images. success models suggests non-trivial performance improvement gained modifying architectures gans. approach take different methods inject high level information discriminator. existing methods adagan complement other. experiments need done believe architectures beneﬁt augmented training objectives existing methods. method without downside. even though used depthwise separable convolution reduce cost amount memory computation still extremely high. tricks could applied alleviate issue. example similar manner niklaus work local convolutions convolution regress adaptive weights local convolutions adaconvblock approximated separate convolutions. reduce cost another idea exploit locality. expect adaptive convolution weights biases pixel location quite similar neighbors interpolated certain way. address issue future work. references mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard tensorﬂow system largescale machine learning. chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. proceedings fourteenth international conference artiﬁcial intelligence statistics danihelka balaji lakshminarayanan benigno uria daan wierstra peter dayan. comparison maximum likelihood gan-based training real nvps. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems fisher seff yinda zhang shuran song thomas funkhouser jianxiong xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv.", "year": 2018}