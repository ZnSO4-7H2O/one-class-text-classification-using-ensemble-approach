{"title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large  Vocabulary Speech Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.", "text": "present results show possible build competitive greatly simpliﬁed large vocabulary continuous speech recognition system whole words acoustic units. model output vocabulary words directly using deep bi-directional lstm rnns loss. model trained hours semi-supervised acoustic training data enables alleviate data sparsity problem word models. show word models work well end-to-end all-neural speech recognition model without traditional context-dependent sub-word phone units require pronunciation lexicon without language model removing need decode. demonstrate word models perform better strong complex state-of-the-art baseline sub-word units. end-to-end speech recognition neural networks goal machine learning speech processing communities past best speech recognition systems used many complex modeling techniques improving accuracy example hand-crafted feature representations speaker environment adaptation feature afﬁne transformations context-dependent phonetic models decision tree clustering name few. automatic speech recognition goal minimize word error rate. therefore natural choice words units acoustic modeling estimate word probabilities. attempts made model words directly particular isolated word recognition limited vocabularies dominant approach model clustered sub-word units instead. hand clustered units necessity data limited sub-optimal choice. recently amount user-uploaded captions public youtube videos grown dramatically. using powerful neural network models large amounts training data allow directly model words greatly simplify automatic speech recognition system. previously found combination lstm model’s memorization capacity ability loss learn alignment acoustic input label sequences allows neural network recognize whole words trained although training data sparsity issue bi-directional lstm models large output vocabulary could trained i.e. words obtained respectable accuracy without speech decoding however still sub-word phone-based recognizer. paper show techniques coupled larger amount acoustic training data enable build neural speech recognizer trained end-to-end recognize words directly without needing decode. many different approaches end-to-end neural network models speech recognition. encoder-decoder model conditional probability full word output sequence given input sequence. however limited capacity extend dynamic attention scheme makes attention vector function internal state encoder-decoder added encoder rnn. compare explicitly increasing capacity rnn. cases still performance conventional hybrid neural network phone-based recognizer. instead word outputs letters generated directly grapheme-based neural network recognizer good results obtained large vocabulary task quite comparable phone-based baseline system. here describe techniques used building single neural network model capable accurate speech recognition search decoding involved. model deep lstm architecture built stacking multiple lstm layers. since bidirectional models better accuracy application ofﬂine speech recognition lstm layers depth—one operating forward another operating backward direction time input sequence. layers connected previous forward backward layers. train model loss criterion sequence alignment/labeling technique softmax output layer additional unit blank label used represent outputting label given time. output label probabilities network deﬁne probability distribution possible labelings input sequences including blank labels. network trained optimize total probability correct labelings training data estimated using network outputs forward-backward algorithm. correct labelings input sequence deﬁned possible labelings input target labels correct sequence order possibly repetitions blank labels permitted labels. loss efﬁciently easily computed using ﬁnite state transducers described input sequence acoustic frames input label sequence lattice encoding possible alignments allows label repetitions possibly interleaved blank labels. probability correct labelings computed using forward-backward algorithm. gradient loss function w.r.t. input activations softmax activation label time step represents lattice states aligned label time αxzl forward variable representing summed probability paths lattice starting initial state time ending state time backward variable starting state lattice time going ﬁnal state. model ﬁnal softmax predicting word posteriors number outputs equaling vocabulary size. modeling words directly problematic data sparsity large amount acoustic training data alleviate experiment written spoken vocabulary. vocabulary obtained training data transcripts mapped spoken forms reduce data sparsity limit label ambiguity spoken vocabulary experiments. written-to-spoken domain mapping verbalization model used example converted hundred four four. given possible verbalizations entity aligns best acoustic training data chosen. model essentially all-neural network speech recognizer require beam search type decoding. network takes input mel-spaced ﬁlterbank features. word posterior probabilities output model simply used recognized word sequence. since word sequence spoken domain spoken vocabulary model written forms also create simple lattice enumerating alternate words blank label time step rescore lattice written-domain word language model composition composing verbalizer fst. written vocabulary model directly compose lattice language model assess importance language model rescoring accuracy. train models distributed manner using asynchronous stochastic gradient descent large number machines found word acoustic models performed better initialized using parameters hidden states phone models—the output layer weights randomly initialized weights initial networks randomly initialized uniform distribution. training stability clip activations memory cells gradients range. implemented optimized native tensorflow kernel multi-layer lstm forward pass gradient calculations. multi_lstm_op allows parallelize computations across lstm layers using pipelining resulting speed-up decreases parameter staleness asynchronous updates improves accuracy. youtube video sharing website billion users. improve accessibility google functionality caption youtube videos using automatic speech recognition technology generated caption quality vary generally better human created ones produced scale. whole users found helpful google received technology breakthrough award national association deaf automatic captioning youtube. work evaluate models videos sampled google preferred channels youtube test comprised videos categories video averaging minutes length. total test duration roughly hours words bulk training data supervised important question valuable type data training acoustic models. experiments keep language model constant -gram model n-grams vocabulary words. training large accurate neural network models speech recognition requires abundant data. others used read speech corpora unsupervised methods gather thousands even tens thousands hours labeled training data apply approach ﬁrst described scaled build training hours. islands conﬁdence ﬁltering allows user-uploaded captions labels selecting audio segments video user uploaded caption matches transcript produced system constrained likely produce n-grams found uploaded caption. approximately hours video available english captions quarter remained ﬁltering. initial acoustic model trained hours supervised training data comes youtube google videos broadcast news described acoustic model -state triphone states. system gave word error rate google preferred test shown table training sequence-level state-mbr criterion using two-pass adapted decoding setup best could hour training set. contrast adding semi-supervised training data hours reduced error rate model size. since data available models capture longer temporal context show results single-state phone units gives relative improvement -state triphone models. type model improves amount training data little difference training criteria. figure word posterior probabilities predicted model time-frame segment music video ‘stressed out’ twenty pilots. plot word highest posterior missing words correct transcription‘sometimes certain smell take back young come never able identify it’s coming from’. entire acoustic training corpus billions words vocabulary million words. neural speech recognizer experimented spoken written output vocabularies loss. spoken vocabulary decided model words seen times. resulted vocabulary words rate written vocabulary chose words seen times resulting words rate comparison full test vocabulary baseline words rate evaluated impact reduced vocabulary best phone models observed increase table compare phone word models. units trained models bidirectional lstm layers. output layer word models substantially larger total number parameters word models larger phone models number size lstm layers. also want note tried increase number parameters phone models reported results table best results could obtain phone models. show table increasing number phones yield reduction error rate. deep decision trees tend work mostly scenarios phonetic contexts well matched train test data. open domains youtube videos typically don’t gain large number context dependent models particular temporal context already covered deep bidirectional lstm models. difference performance phone models comparison word models. part earlier experiments trained hours training model performed poorly error rate training loss performed substantially better unexpected predicting longer units frame frame basis makes prediction task substantially harder. overall table shows word models outperform phone models even handicap higher rate word models. mentioned earlier word model directly without decoding language model recognition output becomes output layer essentially making word model end-to-end all-neural speech recognition model. entire speech recognizer becomes single neural network. figure shows word posterior probabilities predicted model music video. even though trained music videos model quite robust accurate transcribing songs. results shown last columns table word models. without language model decoding spoken word model error rate written word model wer. written word model better conventional phone model obtained decoding language model. shows bi-directional lstm word models capable accurate speech recognition language model decoding involved. sanity check pruned language model heavily de-weighted uni-gram model used phone models. expected error rate increases drastically showing language model important conventional models less important whole word models. spoken word model improves word lattices obtained model rescored language model. improvements mostly conversion spoken word forms written forms since scoring done written domain. written word model improves word lattices rescored showing relatively small impact accuracy system. error rate calculation disadvantages spoken word model references written domain output model spoken domain creating artiﬁcial errors like \"three\" case conventional phone baseline written word model words modeled written domain. evaluate error rate spoken domain automatically converted test data force aligning utterances graph built project context transducer lexicon transducer spoken-to-written transducer written transcript. project maps input symbols output symbols thereby output symbols entire graph spoken domain. using approach convert written language model spoken form calculating project using spoken build decoding graph. word error rates spoken domain shown table models previous table word models without language model decoding performs slightly better phone model uses lvcsr decoder incorporates -gram language model. also separate effect language model spoken-to-written text normalization. adding language model spoken word model improves error rate showing spoken word models perform well even without language model. presented neural speech recognizer end-to-end all-neural large vocabulary continuous speech recognizer forgoes pronunciation lexicon decoder. mining hours training data using public captions allows train large powerful bi-directional lstm model speech recognition loss predicts words. neural speech recognizer model written vocabulary words including numeric entities. unlike many end-to-end systems compromise accuracy system simplicity ﬁnal system performs better well-trained conventional context-dependent phone-based system achieving word error rate difﬁcult youtube video transcription task. ying zhang mohammad pezeshki philémon brakel saizheng zhang césar laurent yoshua bengio aaron courville. towards end-to-end speech recognition deep convolutional neural networks. proc. interspeech l.r. bahl p.v. souza p.s. gopalakrishnan nahamoo m.a. picheny. context dependent modelling phones continuous speech using decision trees. proc. darpa speech natural language processing workshop alex graves santiago fernández faustino gomez jürgen schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proc. icml ha¸sim andrew senior kanishka ozan irsoy alex graves françoise beaufays johan schalkwyk. learning acoustic frame labeling speech recognition recurrent neural networks. proc. icassp jeffrey dean greg corrado rajat monga chen matthieu devin quoc mark marc’aurelio ranzato andrew senior paul tucker yang andrew large scale distributed deep networks. nips", "year": 2016}