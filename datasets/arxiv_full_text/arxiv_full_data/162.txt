{"title": "Automated Problem Identification: Regression vs Classification via  Evolutionary Deep Networks", "tag": ["cs.NE", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Regression or classification? This is perhaps the most basic question faced when tackling a new supervised learning problem. We present an Evolutionary Deep Learning (EDL) algorithm that automatically solves this by identifying the question type with high accuracy, along with a proposed deep architecture. Typically, a significant amount of human insight and preparation is required prior to executing machine learning algorithms. For example, when creating deep neural networks, the number of parameters must be selected in advance and furthermore, a lot of these choices are made based upon pre-existing knowledge of the data such as the use of a categorical cross entropy loss function. Humans are able to study a dataset and decide whether it represents a classification or a regression problem, and consequently make decisions which will be applied to the execution of the neural network. We propose the Automated Problem Identification (API) algorithm, which uses an evolutionary algorithm interface to TensorFlow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem. We test API on 16 different classification, regression and sentiment analysis datasets with up to 10,000 features and up to 17,000 unique target values. API achieves an average accuracy of $96.3\\%$ in identifying the problem type without hardcoding any insights about the general characteristics of regression or classification problems. For example, API successfully identifies classification problems even with 1000 target values. Furthermore, the algorithm recommends which loss function to use and also recommends a neural network architecture. Our work is therefore a step towards fully automated machine learning.", "text": "scientist process understanding data using bayesian model selection. real propose evolutionary algorithm optimising image classification neural networks requires human intervention creating networks. similarly zoph recurrent neural networks along reinforcement learning order achieve similar goal. clear research efforts trend continue driven potential industrial profits compensate shortages expensive data scientists general goal artificial general intelligence nevertheless current machine learning algorithms considerable amount human intervention must performed prior final execution algorithm. example setting number parameters preprocessing data deciding loss function interpreting results name few. another example perhaps first steps data science process problem identification \"does supervised data correspond classification regression problem?\" understanding type problems given dataset represents step direction automated machine learning research subject study. classification problems typically represent problems whereby goal create predictive model discriminate various known classes. cifar- mnist examples classification datasets goals identify correct label image. regression problems predictive output continuous example regression dataset boston housing price regression dataset goal predict median value houses. context deep learning presented dataset typically verify whether data represents classification regression problem decide loss function network layers accordingly. cifar- image dataset might consider using convolutional dropout fully connected layers; boston housing price dataset might fully connected dropout layers. furthermore decision made regards loss function use. cifar- might categorical cross entropy mean squared error loss function boston housing case. researchers machine learning cases decisions made relative ease. machine hand decision non-trivial current machine learning algorithms automatically decide given dataset abstract regression classification? perhaps basic question faced tackling supervised learning problem. present evolutionary deep learning algorithm automatically solves identifying question type high accuracy along proposed deep architecture. typically significant amount human insight preparation required prior executing machine learning algorithms. example creating deep neural networks number parameters must selected advance furthermore choices made based upon pre-existing knowledge data categorical cross entropy loss function. humans able study dataset decide whether represents classification regression problem consequently make decisions applied execution neural network. propose automated problem identification algorithm uses evolutionary algorithm interface tensorflow manipulate deep neural network decide dataset represents classification regression problem. test different classification regression sentiment analysis datasets features unique target values. achieves average accuracy identifying problem type without hardcoding insights general characteristics regression classification problems. example successfully identifies classification problems even target values. furthermore algorithm recommends loss function also recommends neural network architecture. work therefore step towards fully automated machine learning. introduction performance machine learning algorithms skyrocketed recent years often unspoken relationship human data scientist machines evolved significantly. great deal work stateof-the-art methods researchers constantly optimising various aspects machine learning algorithms. efforts include proposing algorithms optimising hyperparameters network architectures latest trends show increasing emphasis algorithms require less human intervention. consider automatic statistician project aims removing data https//www.automaticstatistician.com/index/ figure chromosome contains four genes gene represents network architecture. figure illustrates example network architecture generated chromosome input dataset cifar- image classification dataset. chromosome recommended last layer units sigmoid activation function. furthermore chromosome recommended using categorical cross entropy loss function consequently correctly determined dataset classification problem. study genetic algorithm harnessed dynamic flexible deep learning framework proposed automated identification problems. call automated problem identification algorithm show successfully determine dataset classification regression one; furthermore recommend whether categorical cross entropy mean-squared error. additionally recommend layers known either final architecture input optimisation. figure illustrates example network produced chromosome cifar- dataset input api. resulting architecture similar human might problem. paper organized follows section describes gas. section describes chromosome used determine dataset classification regression optimisation problem. section provides details proposed algorithm. experimental setup presented section section discusses results. conclude section discus future work. genetic algorithm genetic algorithm biologically inspired evolutionary algorithm mimic species fight survival reproduce nature. makes population chromosomes solve optimisation problem. chromosome encodes potential solution problem. time chromosomes undergo many modifications known genetic operators order traverse search space. fitness function used determines good chromosome solving optimisation problem. generation parent chromosomes selected genetic operators applied parents create offspring constitute chromosome parent population. population evaluated fitness process repeated; illustrated algorithm generation generation select parents. perform genetic operators. replace current population offspring created step evaluate current population. proposed chromosome section following subsections describe chromosome along description genes within chromosome. study word layer refers layers deep neural network architectures. chromosome made four genes namely neural network loss function number units last layer neural network activation function used last layer configuration layers chromosome thus encodes entire deep neural network architecture associated loss function. figure illustrates example chromosome encodes neural network architecture following layers fully connected dropout fully connected layers. furthermore chromosome apply mean squared error loss function last layer unit activation function rectified linear unit. number units last layer second gene denotes number units last layer network. possible values gene denotes number unique values formally {yi}i∈{ number samples. example assume dataset since unique values targets. last layer function gene takes four possible values denotes activation function used last layer network. possible values {linear relu sigmoid softmax}. ‘relu’ refers rectified linear units. given input layer equations activation functions presented equations respectively. configuration layers chromosome gene corresponds architecture network define configuration. configuration represents exact sequence network layers stored list. first element configuration represents first layer last element represents last layer. four possible values element configuration take namely convolution fully connected dropout pooling here convolution refers two-dimensional convolution. dropout list possible configuration values even though dropout layer. size configurations randomly selected configurations initialised randomly initial population generation modified mutation operator; explained sections respectively. layers mapped integer value i.e. convolution mapped fully connected dropout pooling chromosome exactly configuration. chose since number genes easily modified order encode additional complexity easily handle discrete nature parameters chosen since searches space network architectures addition parameters. increase complexity chromosomes including parameters discuss section figure example chromosome encodes mean squared error loss function unit last layer network relu activation function. architecture network denoted represents fully connected layer followed dropout fully connected layers. configurations explained section loss function gene represents loss function used training network takes possible values mean squared error categorical cross entropy loss. denote target label sample denote model’s predicted output sample denote number training samples. mean-squared error used study presented equation example assume network predicts msen similarly assume another network predicts msen network preferred since using objective optimisation algorithm minimise value reduce distance correct values model’s predicted values. using loss function objective maximise make network predictions similar labels possible. case target labels represented vector network predictions also vector length. example assume network predicts sample similarly assume another network predicts chromosome fitness evaluation make fitness function evaluate good chromosome solving optimisation problem. case designed fitness function discriminate classification regression problems. proposed system commences splits dataset subsets features labels labels converted corresponding one-hot encoded values. example label value unique values one-hot encoded value system retains one-hot encoded values. dataset split data training remaining validation set. chromosome evaluated follows. chromosome’s loss function used train neural network training data. chromosome’s loss function categorical cross entropy one-hot encoded values used training. however loss function mean squared error values used training. validation loss recorded optimisation neural network across epochs. validation loss denotes total number epochs denote validation loss epoch define change validation follows δval averaдe finally define ratio validation drop δval thus chromosome optimisation neural network taken place compute implies network done learning since validation loss increased. furthermore network managed learn anything since validation loss remained constant epochs. finally conclude given drop validation loss network managed learn. model predicts output values validation data. predictions validation target values compared using mean squared error. loss obtained validation data using categorical cross entropy different loss computed using mean squared error. chose mean squared error consistent comparisons. case whereby network learnt anything penalise chromosome fitness infinity. however case whereby network learnt i.e. assign computed validation mean squared error fitness chromosome. objective algorithm thus minimise fitness chromosome rewarding networks learn small mean squared error validation set. lower fitness value better chromosome performed. figure illustrates plot explains fitness chromosome. plot separated plot observed fitness large value. value fitness corresponds mean squared error whereby smaller value better. sake example straight line drawn illustrate smaller mean squared error results better chromosome fitness. figure training neural network validation data record validation loss. determine whether network learnt. network learnt penalise chromosome large fitness. however network able learn assign mean squared error fitness value. since chromosomes randomly generated possible represent invalid networks particular features labels given dataset. example assume that chromosome number units last layer categorical cross entropy loss function used. given previous description subsection one-hot encoded values used training. however example number outputs thus one-hot encoded vector cannot compared single value. illustrate another example consider chromosome tries convolutional layers feature based regression dataset course feasible. invalid chromosomes penalised fitness infinity. section describes chromosome makes discrimination regression classification. algorithm following subsections explain aspect adapted determine given dataset classification regression problem. furthermore algorithm recommends following upon termination loss function used order enable training neural network number units type activation function last layer finally simple network architecture also recommended. algorithm performs optimisation phases namely optimising population since chromosome represents neural network optimisation performed training networks. initial population generation initial population size value userdefined population size. suppose population size chromosomes created initial population generation. chromosome fixed length genes creation chromosome gene randomly created based values gene take pseudocode creating chromosome presented algorithm initial fitness chromosome infinity. initialise empty chromosome. loss function either categorical cross entropy mean squared error. number units last layer either activation function last layer either linear sigmoid softmax relu. create random configuration. parent selection parent selection methods used obtain parents current population chromosomes. parents used genetic operators create offspring. single parent obtained parent selection method executed. chromosome chosen parent selection method select particular chromosome again. three common parent selection methods fitness-proportionate rank tournament selection study tournament selection used given shown successful method zhong algorithm presents pseudocode tournament selection. selection method user-defined parameter namely tournament size. tournament size. tournament selection randomly selects chromosomes current population compares fitness chromosomes. chromosome lowest fitness returned parent chromosome. occurs random chromosome selected break tie. genetic operators genetic operators applied parents exchange genetic material parent chromosomes consequently create novel offspring. common genetic operators mutation crossover. implementation details study described below. gene randomly selected value gene created. user-defined parameter associated mutation operator namely mutation application rate. figure illustrates application mutation operator parent chromosome resulting offspring illustrated. example forth gene selected mutation thus forth gene within parent changed configuration offspring. netic material parent chromosomes parent parent consequently creates offspring offspring offspring. several variations crossover genetic operator uniform one-point two-point crossover. crossover method implement randomly selects position range denotes length chromo— within parent chromosomes; position must selected within parents. offspring created genes except position copied across corresponding offspring without modification. genes position swapped i.e. gene position parent inserted position offspring similarly gene position parent inserted position offspring. example application crossover operator presented figure figure shows parent chromosomes. crossover point third gene parent i.e. last activation function swapped parents. offspring show result crossover. algorithm termination final decision generational loop best chromosome output. loss function best chromosome used decide dataset classification regression problem. loss function categorical cross entropy problem labelled classification. however loss function mean squared error problem labelled regression. table datasets used study. used datasets four problem domains various characteristics denoted follows represents data classification regression ‘ic’ image classification ‘sa’ sentiment analysis. sentiment analysis datasets considered classification problems. unique targets refers unique number outputs target values dataset. example cifar- unique target classes whereas relative slice unique target values. crowdflower imdb used bags words approach order generate word embeddings. results discussion results obtained presented discussed section. aloi dataset included experiments could hypothesise dataset large number targets regression dataset. reason included aloi much larger number classes comparison classification datasets. accuracy results achieved datasets across runs presented figure discriminating regression classification problems obtained average accuracy lowest accuracy obtained datasets highest accuracy achieved datasets. figure example crossover operator applied parent chromosomes. third gene parents selected crossover. result last activation functions swapped parents. experimental setup section describes experimental used evaluate performance api. algorithm programmed python tensorflow algorithm evaluated machine intel core ram. datasets table presents datasets used study along characteristics type. datasets obtained machine learning repository except cifar- cifar- obtained mnist crowdflower aloi imdb obtained externally. study crowdflower represents ‘emotion text’ dataset. assumptions missing values dataset categorical features converted corresponding numerical features using one-hot encoding approach. course would possible implement imputation method overcome datasets missing values however part scope study. algorithm standardises feature. possible used samples training validation. boston housing example many samples. case simply split dataset equally sets. distinguish data image classification problems former data typically resented one-dimensional vectors; whereas image classification datasets commonly represented three-dimensional arrays. adapt various input shapes without human intervention. experimental parameters neural network parameters used study presented tables respectively. parameters obtained preliminary runs algorithm. purpose study evolve chromosomes could determine whether given dataset classification regression addition several outputs. certain variables remain fixed order evolve chromosomes. parameter table fixed value. https//www.crowdflower.com/data-for-everyone/ https//www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/multiclass.html https//keras.io/datasets/ table table presents number runs algorithm incorrectly classified dataset. objective discriminate regression classification datasets. dataset performed runs. perfect accuracy achieved datasets. types represents data classification regression ‘ic’ image classification ‘sa’ sentiment analysis. chromosomes runs cifar- evolved similar architectures always case. example particular evolved architecture case architecture primarily made dropout convolutional layers fully connected layer. certain runs evolved architectures made deep networks containing fully connected layers. example appendix consider chromosome figure accuracy results obtained various datasets. dataset runs algorithm executed. lowest accuracy achieved accuracy datasets. average accuracy across datasets table presents number times runs incorrectly classified dataset. datasets incorrectly classified runs represents accuracy dataset performance across runs less case misclassifications cifar- dataset fitness chromosomes mean squared error categorical cross entropy loss function close. happened that particular former slightly lower fitness. second case population rapidly dominated chromosomes mean squared error loss function generational loop progressed. similar observation made incorrectly classified runs. possible ways overcoming issue would re-introduce genetic diversity population randomly initialising number chromosomes across generations. would thus allow chromosomes containing types loss functions present population. alternatively increasing tournament size could allow weaker chromosomes remain population could turn preserve balance chromosomes containing loss functions. appendix presents dataset example chromosome evolved. chromosomes randomly selected runs. networks varied size layers however cases networks deep. architecture generated image classification problems complex ones generated problems. particular evolved chromosome cifar- dataset interest configuration resembles architecture human might generated creating deep neural network image classification. instance consider alexnet made series convolutional pooling layers towards start network ends three fully connected layers. similar chromosome’s architecture presented appendix similar structure convolutions pooling layers followed fully connected dropout layers. number epochs used throughout optimisation neural networks small. would thus interest extend study order investigate architectures would generated using larger number epochs. drawback computational effort required obtain results. would interest decrease population size determine extent reduced whilst retaining current accuracy discriminating classification regression problems. conclusion study present automated problem identification algorithm genetic algorithm coupled deep networks automatically determining whether dataset represents regression classification problem. great effort improving proposing machine learning algorithms typically practitioner must still decide loss function neural network architecture number units layer select appropriate activation functions prior execution neural network. propose goal moving towards general artificial intelligence automated machine learning requires little human intervention. applied times different datasets drawn varied problem domains data characteristics. find correctly identified problem type average accuracy running single cpu. furthermore able recommend whether mean squared error categorical cross entropy suitable number units last layer together activation function furthermore recommend network architecture. despite primary focus study proposed algorithm generated interesting relevant deep architectures. already begun working next phase research develop algorithm optimise entire pipeline creating deep neural networks; whereby goal simply provide algorithm dataset return deep neural network produce competitive results. would completely remove human pipeline. would interest determine evolved networks could outperform created humans. clear efforts various researchers machine learning community steer towards algorithms completely automated requiring human intervention. examples chromosomes illustrate examples chromosomes evolved various datasets. dataset name provided along problem type. last activation function ‘mse’ denotes mean squared error ‘cce’ denotes categorical cross entropy. configurations convolution mapped fully connected dropout pooling example chromosome able correctly classify dataset. acknowledgments financial assistance national research foundation towards research hereby acknowledged. opinions expressed conclusions arrived author necessarily attributed nrf. authors would like thank ethan roberts shankar agarwal arun aniyan feedback comments. references esteban real sherry moore andrew selle saurabh saxena yutaka leon suematsu quoc alex kurakin. large-scale evolution image classifiers. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overfitting. mach. learn. res. january tobias blickle lothar thiele. comparison selection schemes used evolutionary algorithms. evolutionary computation december jinghui zhong xiaomin zhang comparison performance different selection strategies simple genetic algorithms. international conference computational intelligence modelling control automation international conference intelligent agents technologies internet commerce volume pages martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems software available tensorflow.org. lichman. machine learning repository http//archive.ics.uci.edu/ml. alex krizhevsky. learning multiple layers features tiny images. lecun bottou bengio haffner. gradient-based learning applied document recognition. proceedings ieee patrick mcknight katherine mcknight souraya sidani aurelio jose", "year": 2017}