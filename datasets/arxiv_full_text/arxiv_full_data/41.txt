{"title": "De-identification of Patient Notes with Recurrent Neural Networks", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.  Materials and Methods: We introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.  Results: Our ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall 99.25 and a precision of 99.06.  Conclusion: Our findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no feature engineering.", "text": "objective patient notes electronic health records contain critical information medical investigations. however vast majority medical investigators access de-identiﬁed notes order protect conﬁdentiality patients. united states health insurance portability accountability deﬁnes types protected health information needs removed de-identify patient notes. manual de-identiﬁcation impractical given size databases limited number researchers access non-de-identiﬁed notes frequent mistakes human annotators. reliable automated de-identiﬁcation system would consequently high value. materials methods introduce ﬁrst de-identiﬁcation system based artiﬁcial neural networks requires handcrafted features rules unlike existing systems. compare performance system state-of-the-art systems datasets de-identiﬁcation challenge dataset largest publicly available de-identiﬁcation dataset mimic de-identiﬁcation dataset assembled twice large dataset. results model outperforms state-of-the-art systems. yields f-score dataset recall precision f-score mimic de-identiﬁcation dataset recall precision conclusion ﬁndings support anns de-identiﬁcation patient notes show better performance previously published systems requiring feature engineering. many countries united states medical professionals strongly encouraged adopt electronic health records face ﬁnancial penalties fail centers medicare medicaid services paid billion incentive payments hospitals providers attested meaningful march medical investigations greatly beneﬁt resulting increasingly large datasets. components ehrs patient notes information contain critical medical investigation much information present texts cannot found elements ehr. however patient notes shared medical investigators types information referred protected health information must removed order preserve patient conﬁdentiality. united states health insurance portability accountability deﬁnes different types ranging patient names phone numbers. table presents exhaustive list types deﬁned hipaa. cannot identiﬁed removed. deidentiﬁcation either manual automated. manual de-identiﬁcation means labeled human annotators. three main shortcomings approach. first restricted individuals allowed access identiﬁed patient notes thus task cannot crowdsourced. second humans prone mistakes. asked clinicians detect approximately patient notes results manual de-identiﬁcation varied clinician clinician recall ranging reported annotators paid hour read words hour best. matter comparison mimic dataset contains data intensive care unit stays consists million words. would require hours annotation would cost rate. given annotators’ spotty performance patient note would annotated least different annotators would cost least de-identify notes mimic dataset. automated de-identiﬁcation systems classiﬁed categories rule-based systems machine-learning-based systems. rule-based systems typically rely patterns expressed regular expressions gazetteers deﬁned tuned humans. require labeled data easy implement interpret maintain improve explains large presence industry however need meticulously ﬁne-tuned dataset robust language changes cannot easily take account context rule-based systems described pervised machine learning algorithms de-identify patient notes training classiﬁer label word sometimes distinguishing different types. common statistical methods include decision trees log-linear models support vector machines conditional random ﬁelds latter employed state-of-the-art systems. thorough review existing systems methods share downsides require decent sized labeled dataset much feature engineering. rules quality features challenging time-consuming develop. recent approaches natural language processing based artiﬁcial neural networks require handcrafted rules features automatically learn effective features performing composition tokens represented vectors often called token embeddings. token embeddings jointly learned parameters ann. initialized randomly pre-trained using large unlabeled datasets typically based token cooccurrences latter often performs better since pre-trained token embeddings explicitly encode many linguistic regularities patterns. result methods based anns shown promising results various tasks natural language processing language modeling text classiﬁcation question answering machine translation well named entity recognition methods also vector representations characters inputs order either replace augment token embeddings types ages ages telephone numbers electronic mail addresses urls addresses* dates year holidays week social security numbers medical record numbers account numbers certiﬁcate license numbers vehicle device identiﬁers biometric identiﬁers full face photographic images* addresses components smaller state state country employers hospital name ward name names patients family members provider name profession like machine learning based systems anns require manually-curated features based regular expressions gazetteers. show anns achieve state-of-the-art results de-identiﬁcation different datasets patient notes challenge dataset mimic dataset. ﬁrst present de-identiﬁer developed based conditional random ﬁeld model section de-identiﬁer yields state-of-the-art results dataset reference dataset comparing de-identiﬁcation systems. system used challenging baseline model present section model outperforms model outlined section model patient note tokenized features extracted token. training phase crf’s parameters optimized maximize likelihood gold standard labels. test phase predicts labels. performance model depends mostly quality features. used combination n-gram morphological orthographic gazetteer features. similar features used best-performing crfbased competitors challenge bias vectors used input gate memory cell output gate calculations respectively. symbols tanh refer element-wise sigmoid hyperbolic tangent functions element-wise multiplication. bidirectional lstm consists forward lstm backward lstm for− ward lstm calculates forward hidden states backward lstm calcu feeding input sequence backward order depending application lstm might need output sequence corresponding element sequence single output summarizes whole sequence. former case output sequence lstm obtained concatenating hidden states forward backward lstms element i.e. latter case output obtained concatenating last hidden states forward backward lstms i.e. character-enhanced token embedding layer takes token input outputs vector representation. latter results concatenation different types embeddings ﬁrst directly maps token vector second comes output character-level token encoder. direct mapping token vector often called token embedding pre-trained large unlabeled datasets using programs wordvec glove learned jointly rest model. token embeddings often learned sampling token cooccurrence distributions desirable properties locating semantically similar words closely vector space hence leading state-of-the-art performance various tasks. character-enhanced token embedding layer maps token vector representation. sequence vector representations corresponding sequence tokens input label prediction layer outputs sequence vectors containing probability label corresponding token. lastly sequence optimization layer outputs likely sequence predicted labels based sequence probability vectors previous layer. layers learned jointly. figure shows architecture. following denote scalars italic lowercase vectors bold lowercase matrices italic uppercase symbols. colon notations denote sequence scalars vectors respectively. bidirectional lstm neural network architecture designed handle input sequences variable sizes fails model long term dependencies. lstm type mitigates issue keeping memory cell serves summary preceding elements input sequence. speciﬁcally given sequence vectors step lstm takes input produces hidden state memory cell based following formulas figure architecture artiﬁcial neural network model. stands recurrent neural network. type used model long short term memory number tokens token. mapping tokens token embeddings. number characters character token. mapping characters character embeddings. character-enhanced token embeddings token. output lstm label prediction layer probability vector labels predicted label token. encoder generates character-based token embedding ﬁrst mapping character vector called character embedding mapping sequence passed bidirectional lstm outputs character-based token embedding result ﬁnal output characterenhanced token embedding layer token concatenation token embedding character-based token embedding summary character-enhanced token embedding layer receives sequence tokens input output sequence token embeddings tics tokens degree still suffer data sparsity. example cannot account out-of-vocabulary tokens misspellings different noun forms verb endings. solution remediate issues would lemmatize tokens training approach fail retain useful information distinction verb noun forms. address issue using character-based token embeddings incorporate individual character token generate vector representation. approach enables model learn sub-token patterns morphemes roots thereby capturing outof-vocabulary tokens different surface forms information contained token embeddings. experiments results datasets evaluate models datasets mimic de-identiﬁcation datasets. dataset released part ib/uthealth shared task track largest publicly available dataset de-identiﬁcation. teams participated shared task systems submitted. result used dataset compare models state-of-the-art systems. mimic de-identiﬁcation dataset created work follows. mimic-iii dataset contains data stays hospital admissions patients including million patient notes. order make notes publicly available rulebased de-identiﬁcation system written speciﬁc purpose de-identifying patient notes mimic leveraging dataset-speciﬁc information list patient names addresses. system favors recall precision virtually false negatives numerous false positives. create gold standard mimic de-identiﬁcation dataset selected discharge summaries belonging different patient containing total instances. annotated instances detected rule-based system true positives false positives. found instances detected rule-based system false positives. label prediction layer contains bidirectional lstm takes input sequence gener←→ ates corresponding output sequence lstm given feed-forward output neural network hidden layer outputs corresponding probability vector label sequence optimization layer takes sequence probability vectors label prediction layer input outputs sequence labels label assigned token simplest strategy select label would choose label highest probability i.e. argmaxk however greedy approach fails take account dependencies subsequent labels. example likely token type state followed token type type. even though label prediction layer capacity capture dependencies certain degree preferable allow model directly learn dependencies last layer model. model dependencies incorporate matrix contains transition probabilities subsequent labels. probability token label followed token label score label sequence deﬁned probabilities individual labels transition probabilities scores turned probabilities label sequences taking softmax function possible label sequences. training phase objective maximize probability gold label sequence. testing phase given input sequence tokens corresponding sequence predicted labels chosen maximizes score. evaluation metrics assess performance models computed precision recall f-score. number true positives number false positives number false negatives. precision recall f-score deﬁned follows precision f-score ∗precision∗recall intuitively preciprecision+recall sion proportion predicted labels gold labels recall proportion gold labels correctly predicted f-score harmonic mean precision recall. training hyperparameters model trained using stochastic gradient descent updating parameters i.e. token embeddings character embeddings parameters bidirectional lstms transition probabilities gradient step. regularization dropout applied character-enhanced token embeddings label prediction layer. choices hyperparameters token embeddings optimized using subset training token embedding dimension label prediction lstm dimension dropout probability tried pre-training token embeddings dataset mimic dataset using wordvec glove. wordvec glove trained using window size minimum vocabulary count iterations. additional parameters wordvec negative sampling model type skipgram respectively. also experimented publicly available token embeddings glove trained wikipedia gigaword results quite robust choice pre-trained token embeddings. glove embeddings trained wikipedia articles yielded slightly better results chose rest work. results computed using ofﬁcial evaluation script de-identiﬁcation challenge. table presents main results based binary token-based precision recall f-score hipaa-deﬁned only. types important since required removed law. dataset model higher f-score recall model well best system de-identiﬁcation challenge nottingham system freely available off-the-shelf program de-identiﬁcation called mitre identiﬁcation scrubber toolkit performed poorly. combining outputs models considering token identiﬁed either model further increases performance terms f-score recall. noted nottingham system speciﬁcally ﬁne-tuned dataset well evaluation script. example nottingham system post-processes detected terms order match offset gold tokens modifying mwfs mimic dataset model also higher f-score recall model. interestingly combining outputs models increase f-score precision negatively impacted. however recall beneﬁt combining models. mist much competitive dataset. calculated statistical signiﬁcance differences precision recall f-score between models using approximate randomization shufﬂes. signiﬁcance levels differences precision recall fscore dataset mimic dataset respectively. table performance deﬁned hipaa. evaluated systems based detection token versus non-phi token best performance metric dataset highlighted bold. nottingham best performing system ib/uthealth shared task track mist mitre identiﬁcation scrubber toolkit freely available de-identiﬁcation program. model based conditional random field model based artiﬁcial neural network crf+ann result obtained combining outputs model model. nottingham system could mimic dataset publicly available. figure binary token-based f-scores category. evaluation based types deﬁned hipaa well additional types speciﬁc dataset. category corresponding types deﬁned table profession category exists dataset removed graph avoid distorting y-axis f-scores crf+ann respectively. reason category mimic removed f-scores crf+ann respectively. another interesting difference results profession category signiﬁcantly outperforms crf. reason behind result embeddings tokens represent profession tend close token embedding space allows model generalize well. tried assembling various gazetteers profession category performing signiﬁcantly worse model. model failed predict conversely. illustrates model efﬁciently copes diversity contexts tokens appear whereas model address contexts manually encoded features. words model’s intrinsic ﬂexibility allows better capture variance human languages model. example would challenging time-consuming engineer features possible contexts stroke quit smoking month friend epstein. model also robust variations surface forms misspellings tokenizations different phrases referring semantable examples correctly detected instances models dataset. examples column predicted model predicted model conversely. typographical errors original text. meaning furthermore model able detect many instances despite explicit gazetteers examples location profession categories illustrate. conjecture character-enhanced token embeddings contain rich enough information effectively function gazetteers tokens similar semantics closely located vector representation gazetteers example token christmas occurs test unless context gives strong indication model cannot detect whereas model could long included gazetteers. effect training size figure shows impact training size performance models mimic dataset. training size limited performs slightly better model since model leverage handcrafted features without much training data. order quantify importance various elements model tried variations model eliminating different elements time. figure presents results ablation tests. removing either label sequence optimization layer pre-trained token embeddings token embeddings slightly decreased performance. surprisingly performed pretty well character embeddings without token embeddings eliminating character embeddings detrimental eliminating token embeddings. suggests character-based token embeddings capturing sub-token level features also semantics tokens themselves. proposed ﬁrst system based patient note de-identiﬁcation. outperforms stateof-the-art systems based datasets requiring handcrafted features. utilizing token character embeddings system automatically learn effective features data ﬁne-tuning parameters. jointly learns parameters embeddings bidirectional lstms well label sequence optimization make token embeddings pre-trained large unlabeled datasets. quantitative qualitative analysis models indicates model better incorporates context ﬂexible variations inherent human languages model. viewpoint deploying off-theshelf de-identiﬁcation system results table demonstrate recall mimic discharge summaries quite encouraging. figure however shows f-score name category probably sensitive type falls model. anticipate adding gazetteer features based local institution’s patient staff census improve result explored future work. figure ablation test performance based binary hipaa token-based evaluation. model based artiﬁcial neural network. model without label sequence optimization layer. pre-train model token embeddings initialized random values instead pre-trained embeddings. token model using character-based token embeddings without token embeddings. character model using token embeddings without character-based token embeddings. training size increases model starts signiﬁcantly outperform model since parameters including embeddings automatically ﬁne-tuned data therefore features learned model become increasingly reﬁned manually handcrafted features. result combining outputs models increases f-score model small training size yields less competitive f-score model bigger training size.", "year": 2016}