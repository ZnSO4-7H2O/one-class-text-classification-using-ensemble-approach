{"title": "Robust Feature Selection by Mutual Information Distributions", "tag": ["cs.AI", "cs.LG", "I.2"], "abstract": "Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean and an analytical approximation of the variance are reported. Asymptotic approximations of the distribution are proposed. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. Finally, a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way.", "text": "mutual information widely used artiﬁcial intelligence descriptive measure stochastic dependence discrete random variables. order address questions reliability empirical value must consider sample-to-population inferential approaches. paper deals distribution mutual information obtained bayesian framework second-order dirichlet prior distribution. exact analytical expression mean analytical approximation variance reported. asymptotic approximations distribution proposed. results applied problem selecting features incremental learning classiﬁcation naive bayes classiﬁer. fast newly deﬁned method shown outperform traditional approach based empirical mutual information number real data sets. finally theoretical development reported allows eﬃciently extend methods incomplete samples easy eﬀective way. robust feature selection naive bayes classiﬁer mutual information cross entropy dirichlet distribution second order distribution expectation variance mutual information. mutual information computed joint chances random variables known. usual procedure common case unknown chances empirical probabilities ˆπij precisely known chances. always appropriate. furthermore empirical mutual information carry information reliability estimate. bayesian framework address questions using prior distribution takes account uncertainty prior likelihood compute posterior distribution mutual information principle obtained. variance. reliable quickly computable expressions following dirichlet prior assumed results allow obtain analytical approximations distribution introduce asymptotic approximations distribution section graphically showing good also small sample sizes. mutual information widely used informationtheoretic measure stochastic dependency discrete random variables used instance learning bayesian nets stochastically dependent nodes shall connected; used induce classiﬁcation trees also used select features classiﬁcation problems i.e. select subset variables predict class variable. done context ﬁlter approach discards irrelevant features basis values mutual distribution mutual information applied feature selection. section proposes ﬁlters credible intervals robustly estimate mutual information. ﬁlters empirically tested turn coupling naive bayes classiﬁer incrementally learn classify data. real data sets used proposed ﬁlters outperforms traditional ﬁlter almost always selects fewer attributes traditional always leading equal signiﬁcantly better prediction accuracy classiﬁer ﬁlter order computational complexity ﬁlter based empirical mutual information appears signiﬁcant immany non-informative priors lead dirichlet posteinterpretaij tion number samples comprises prior information principle allows posterior density mutual information computed. results derived result mean also reported theorem aware similar analytical approximations variance. express exact variance inﬁnite allow straightforward systematic approximation obtained. used heuristic numerical methods estimate mean variance. however heuristic estimates proved importance distribution mutual information extend mentioned analytical work towards even eﬀective applicable methods. section proposes improved analytical approximations tails distribution often critical point asymptotic approximations. section allows distribution mutual information computed also incomplete samples. closed-form formulas developed case feature selection. consider discrete random variables taking values respectively i.i.d. random process samples drawn joint chances important measure stochastic dependence mutual information sample size. leads empirical estimate mutual information. unfortunately point estimation carries information accuracy. bayesian approach problem assumes prior probability density unknown chances probability simplex. comi denotes mutual information speciﬁc chances whereas context non-negative real number. also denote mutual information random variable expectation variance var. expectations always w.r.t. posterior distribution figure distribution mutual information binary random variables three groups curves diﬀerent choices counts upper group related vector intermediate vector lower group group shows exact distribution three approximating curves based gaussian gamma beta distributions. classiﬁcation important techniques knowledge discovery databases classiﬁer algorithm allocates objects ﬁnite previously deﬁned groups basis observations several characteristics objects called attributes features. classiﬁers learnt data alone making explicit knowledge hidden data using knowledge make predictions data. feature selection basic step process building classiﬁers fact even theoretically features provide better prediction accuracy real cases observed many times case depends limited availability data real problems successful models seem good balance model complexity available information. facts feature selection tends produce models simpler clearer computationally less expensive moreover providing often better prediction accuracy. major approaches feature selection commonly used ﬁlter wrapper models. ﬁlter approach preprocessing step classiﬁcation consider points. first complexity compute expressions order empirical mutual information quantities needed compute mean variance involve double sums only function pre-tabled. consider approximating overall distribution mutual information based formulas mean variance given section fitting normal distribution obvious possible choice central limit theorem ensures converges gaussian distribution mean variance var. since non-negative also worth considering approximation gamma even better normalized order upper bounded beta distribution seems another natural candidate deﬁned variables real interval. course gamma beta asymptotically correct too. report graphical comparison diﬀerent approximations focusing special case binary random variables three possible vectors counts. figure compares exact distribution mutual information computed monte carlo simulation approximating curves. ﬁgure clearly shows approximations rather good slight preference beta approximation. curves tend worse smaller sample sizes—as expected—. higher moments computed used improve accuracy. method speciﬁcally improve tail approximation given section focus attention ﬁlter approach. consider well-known ﬁlter computes empirical mutual information features class discards low-valued features easy eﬀective approach gained popularity time. cheng reports particularly well suited jointly work bayesian network classiﬁers approach international knowledge discovery competition weka data mining package implements standard system tool problem ﬁlter variability empirical mutual information sample. allow wrong judgments relevance made features selected keeping mutual information exceeds ﬁxed threshold order selection robust must guarantee actual value mutual information. deﬁne ﬁlters. backward ﬁlter discards attribute value mutual information class less equal given probability forward ﬁlter includes attribute mutual information greater given probability conservative ﬁlter discard features observing substantial evidence supporting irrelevance. instead tend fewer features i.e. substantial evidence useful predicting class. following experiments naive bayes classiﬁer good classiﬁcation model— despite simplifying assumptions often competes successfully state-ofthe-art classiﬁers machine learning ﬁeld experiments focus incremental naive bayes classiﬁer natural learning process data available sequentially data read instance instance; time chosen ﬁlter selects subset attributes naive bayes uses classify instance; naive bayes updates knowledge taking consideration instance actual class. incremental approach allows better highlight diﬀerent behaviors empirical ﬁlter based credible intervals mutual information fact increasing sizes learning ﬁlters converge behavior. ﬁlter interested experimentally evaluating quantities instance data average number correct predictions naive bayes classiﬁer instance; average number attributes used. quantities compare ﬁlters judge eﬀectiveness. implementation details following experiments include using beta approximation distribution mutual information; using uniform prior naive bayes classiﬁer ﬁlters; using natural logarithms everywhere; setting level posterior probability concerned cannot zero probability variables independent zero according inferential bayesian approach. interpret parameter degree dependency strength attributes deemed irrelevant. attempt discarding attributes negligible impact predictions. threshold nevertheless bring discard many attributes. table lists data sets used experiments. real data sets number diﬀerent domains. example shuttle-small reports data diagnosing failures space shuttle; lymphography hypothyroid medical data sets; spam body e-mails spam non-spam; etc. data sets presenting non-nominal features pre-discretized mlc++ default options. step remove attributes judging irrelevant number features table refers data sets possible discretization. instances missing values discarded third column table refers data sets without missing values. finally instances randomly sorted starting experiments. table data sets used experiments together number features instances relative frequency majority class. spam data sets available repository machine learning data sets spam data described available androutsopoulos’s page. remaining cases described means following ﬁgures. figure shows allowed naive bayes signiﬁcantly better predictions greatest part chess data set. maximum diﬀerence prediction accuracy obtained instance accuracies cases respectively. figure report case signiﬁcant diﬀerence curve. good performance obtained using third attributes figure compares accuracies spam data set. diﬀerence cases signiﬁcant range instances maximum instance accuracies respectively. signiﬁcantly worse instance end. excellent performance even valuable considered number attributes selected classiﬁcation. spam case attributes binary table average number attributes selected ﬁlters entire data reported last three columns. second column left reports original number features. case selected fewer features sometimes much fewer; usually selected much fewer features conservative. boldface names refer data sets prediction accuracies signiﬁcantly different. correspond presence absence words e-mail goal decide whether email spam. words found body e-mails initially considered. shows average relevant words needed make good predictions. worse predictions made using select average words respectively. figure shows average number excluded features three ﬁlters spam data set. suddenly discards features keeps number selected features almost constant process. remaining ﬁlters tend number diﬀerent speeds initially including many features summary experimental evidence supports strategy using features reliably judged carrying useful information predict class provided judgment updated soon observations collected. almost always selects fewer features leading prediction accuracy least good leads comparison analogous appears best ﬁlter worst. however conservative nature might turn successful data available groups making sequential updating viable. case seem safe take strong decisions exclusion maintained number instances unless substantial evidence relevance attribute. following generalize setup include case missing data often occurs practice. instance observed instances often consist several features plus class label features observed i.e. feature class label pair observed. extend contingency table include counts number instances class observed instances). shown using partially observed instances improve classiﬁcation accuracy make common assumption missing-data mechanism ignorable i.e. probability distribution class labels instances missing feature assumed coincide marginal π+j. expansion around mean poor estimate extreme values imax better tail approximations. scaling behavior determined following small describes near independent random variables suggests reparameterization ˜πi+ ˜π+j integral small lead small hence small expand expression correctly taking account constraints scaling argument shows similarly scaling behavior around imax min{log written entropy. without loss generality prior converges zero suﬃciently rapid gives dominant contribution imax. scalpaper presented ongoing research distribution mutual information application important issue feature selection. former case provide fast analytical formulations shown approximate distribution well also small sample sizes. extensions presented that side allow improved approximations tails distribution obtained other allow distribution eﬃciently approximated also common case incomplete samples. feature selection concerned empirically showed newly deﬁned ﬁlter based distribution mutual information outperforms popular ﬁlter based empirical mutual information. result obtained jointly naive bayes classiﬁer. broadly speaking presented results important since reliable estimates mutual information signiﬁcantly improve quality applications case feature selection reported here. signiﬁcance results also enforced many important models based mutual information. results could applied instance robustly infer classiﬁcation trees. bayesian networks inferred using credible intervals mutual information proposed well-known chow liu’s approach inference treenetworks might extended credible intervals", "year": 2002}