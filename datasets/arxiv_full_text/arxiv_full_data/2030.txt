{"title": "Uncovering the Riffled Independence Structure of Rankings", "tag": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "abstract": "Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of $n$ objects scales factorially in $n$. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called \\emph{riffled independence}, encompassing a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity. In riffled independence, one draws two permutations independently, then performs the \\emph{riffle shuffle}, common in card games, to combine the two permutations to form a single permutation. Within the context of ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. In this paper, we provide a formal introduction to riffled independence and present algorithms for using riffled independence within Fourier-theoretic frameworks which have been explored by a number of recent papers. Additionally, we propose an automated method for discovering sets of items which are riffle independent from a training set of rankings. We show that our clustering-like algorithms can be used to discover meaningful latent coalitions from real preference ranking datasets and to learn the structure of hierarchically decomposable models based on riffled independence.", "text": "representing distributions permutations daunting task fact number permutations objects scales factorially recent used reduce storage complexity exploit probabilistic independence argue full independence assumptions impose strong sparsity constraints distributions unsuitable modeling rankings. identify novel class independence structures called riﬄed independence encompassing expressive family distributions retaining many properties necessary performing eﬃcient inference reducing sample complexity. rifﬂed independence draws permutations independently performs riﬄe shuﬄe common card games combine permutations form single permutation. within context ranking riﬄed independence corresponds ranking disjoint sets objects independently interleaving rankings. paper provide formal introduction riﬄed independence present algorithms using riﬄed independence within fouriertheoretic frameworks explored number recent papers. additionally propose automated method discovering sets items riﬄe independent training rankings. show clustering-like algorithms used discover meaningful latent coalitions real preference ranking datasets learn structure hierarchically decomposable models based riﬄed independence. introduction. ranked data appears ubiquitously various statistics machine learning application domains. rankings useful example reasoning preference lists surveys search results information retrieval applications ballots certain elections even ordering topics paragraphs within document problem building statistical models rankings thus important research topic learning community. many challenging learning problems must contend intractably large state space dealing rankings since ways rank objects. building statistical model popular highly successful approach achieving simplicity distributions involving large collections interdependent variables exploit conditional independence structures ranking problems however independence-based relations typically trickier exploit so-called mutual exclusivity constraints constrain items diﬀerent ranks given ranking. paper present novel relaxed notion independence called riﬄed independence ranks disjoint subsets items independently interleaves subset rankings form joint ranking item set. example ranks food items containing fruits vegetables preference might ﬁrst rank vegetable fruit sets separately interleave rankings form ranking full item set. riﬄed independence appears naturally many ranked datasets show political coalitions elections voting mechanism typically lead pronounced riﬄed independence constraints vote histograms. chaining interleaving operations recursively leads simple interpretable class models rankings unlike graphical models. present methods learning parameters models estimating structure. section gives broad overview several approaches modeling probability distributions permutations. particular summarize results huang studied probabilistic independence relations distributions permutations. section introduce main contribution intuitive novel generalization notion independence permutations riﬄed independence based interleaving independent rankings subsets items. show riﬄed independence appropriate notion independence ranked data exhibit evidence riﬄe independence relations approximately hold real ranked datasets. also discuss ideas exploiting riﬄed independence relations distribution reduce sample complexity perform eﬃcient paper extended presentation previous papers ﬁrst introduction riﬄed independence studied hierarchical models based riﬄe independent decompositions. within section introduce novel family distributions interleavings item sets called biased riﬄe shuﬄes useful context riﬄed independence. propose eﬃcient recursive procedure computing fourier transform biased riﬄe shuﬄe distributions section discuss problem estimating model parameters riﬄe independent model ranking data computing various statistics model parameters. perform computations scalable develop algorithms used fouriertheoretic framework kondor howard jebara huang huang guestrin guibas joining riﬄe independent factors teasing apart riﬄe independent factors joint provide theoretical empirical evidence algorithms perform well. section deﬁne family simple interpretable ﬂexible distributions rankings called hierarchical riﬄe independent models subsets items iteratively interleaved larger larger subsets recursive stagewise fashion. sections tackle problem structure learning riﬄe independent models. section propose method ﬁnding partitioning item subsets partition close riﬄe independent possible. particular propose novel objective quantifying degree subsets riﬄe independent other. section apply partitioning algorithm perform model selection training data polynomial time without exhaustively search exponentially large space hierarchical structures. finally section apply algorithms number datasets simulated real order validate methods assumptions. show methods indeed eﬀective apply particular various voting preference ranking datasets. paper concerned distributions rankings. ranking oneto-one association items ranks means item assigned rank convention think ranked items preferred higher ranked items permutations symmetric group. rankings similar permutations mappings itself subtle diﬀerence rankings diﬀerent sets size paper notation permutations rankings permutations refer functions rearrange ordering item ranks. permutation ranks given ranking rearrange ranks left-composing thus ranking maps item rank hand permutation item rearrange item right-composing thus item relabeled item returns rank item respect original item ordering. finally note composition permutations permutation collection permutations forms group commonly known symmetric group distribution deﬁned rankings permutations viewed joint distribution variables subject mutual exclusivity constraints stipulate objects cannot simultaneously rank alternatively ranks cannot simultaneously occupied object whenever election data. vote distribution percentage votes possible rankings mode distribution matrix ﬁrst order marginals entry reﬂects number voters ranked candidate rank. election dataset ﬁrst used diaconis since analyzed number ranking studies. dataset collection ballots presidential election american psychological association members rank ordered candidates favorite least favorite. names candidates year william bevan iscoe charles kiesler siegle logan wright since candidates possible rankings. figure plot proportion votes ranking received. interestingly instead concentrating small rankings vote distribution dataset fairly diﬀuse every ranking receiving number votes. mode vote distribution occurs rankinterpretability also visualize matrix ﬁrst-order marginals entry represents number voters assigned rank candidate figure represents ﬁrst-order matrix using grayscale levels represent numbers voters. seen overall candidate received highest number votes rank vote distribution gives story goes deeper simply telling winner however. diaconis example noticed candidate also signiﬁcant hate vote good number voters placed last rank. throughout paper story unfold series examples based dataset. dealing factorial possibilities. fact factorially many possible rankings poses number signiﬁcant challenges learning inference. first tractably represent arbitrary distributions rankings large storing array doubles example requires roughly gigabytes storage beyond capacity typical modern second naive algorithmic complexity common probabilistic operations also intractable distributions. computing marginal probability item preferred item example requires summation elements. finally even storage computation issues resolved would still sample complexity issues contend nontrivial impractical hope possible rankings would appear even training rankings. existing datasets every possible ranking realized fact dataset dataset aware quest exploitable problem structure researchers machine learning related ﬁelds consider number possibilities including distribution sparsity exponential family parameterizations algebraic/fourier structure probabilistic independence brieﬂy summarize several approaches following. parametric models. able justice sheer volume previous work parametric ranking models. parametric probabilistic models space rankings rich tradition statistics researchers continue expand upon body work. example well known mallows model often thought analogy normal distribution permutations parameterizes distribution mean permutation precision/spread parameter. models proposed paper generalize classical models statistical ranking literature allowing expressive distributions captured. time methods form conceptual bridge popular models used machine learning which rather relying prespeciﬁed parametric form simply work within family distributions consistent condisparse methods. sparse methods summarizing distributions range older ad-hoc approaches maintaining k-best hypotheses updated compressed sensing inspired approaches discussed approaches assume permutations probability mass scales either sublinearly degree polynomial sparse distributions successfully applied certain tracking domains argue often less suitable ranking problems might necessary model indiﬀerence large subset objects. approximately indiﬀerent among subset objects least rankings nonzero probability mass. example vote distribution clearly sparse distribution ranking received nonzero number votes. fourier-based methods. another recent thread research centered around fourier-based methods maintain low-order summary statistics ﬁrst-order summary example stores marginal probability form every pair thus requires storing matrix numbers. fruits/vegetables example might store probability figs ranked ﬁrst probability peas ranked last. following matrix record ﬁrst order matrix computed histogram votes election example dividing number total number votes would yield matrix ﬁrst order marginal probabilities. situations particularly interested primarily accurately capturing loss payoﬀ function instead ranking probabilities suﬃce sparse proxy distribution even true underlying distribution sparse. example details. low-order marginals turn intimately related generalized form fourier analysis. generalized fourier transforms functions permutations studied several decades primarily persi diaconis collaborators low-order marginals correspond certain sense low-frequency fourier coeﬃcients distribution permutations. example ﬁrst-order matrix reconstructed exactly lowest frequency fourier coeﬃcients second-order matrix lowest frequency fourier coeﬃcients. fourier theoretic perspective sees order marginals reasonable summarizing distribution actually viewed principled frequency approximation thereof. contrast sparse methods fourier-based methods handle diffuse distributions well easily scalable without making aggressive independence assumptions since general requires coeﬃcients exactly reconstruct sth-order marginals quickly becomes intractable moderately large demonstrated that exploiting probabilistic independence could dramatically improve scalability fourier-based methods e.g. tracking problems since confusion data association occurs small independent subgroups objects many problems. probabilistic independence assumptions symmetric group simply stated follows. consider distribution deﬁned p-subset complement size independent storing parameters distribution requires keeping probabilities instead much larger size required general distributions. course still quite large. typically decomposes distribution recursively stores factors exactly small enough factors compresses factors using fourier coeﬃcients order exploit probabilistic independence example ﬁrst-order matrices fully independent black means case -subset constrained probability one. notice that respect rearranging rows independence imposes block-diagonal structure ﬁrst-order matrices. approximating vote distribution factored distribution candidate independent candidates thick gray true distribution dotted purple approximate distribution. notice factored distribution assigns zero probability permutations. matrix ﬁrst order marginals approximating distribution. despite utility many tracking problems however argue independence assumption permutations implies rather restrictive constraint distributions rendering independence highly unrealistic ranking applications. particular using mutual exclusivity property shown that independent allowed ranks. ﬁxed p-subset permutation elements permutation complement probability known independent. vegetables occupy ﬁrst second ranks probability fruits occupy ranks probability reﬂecting vegetables always preferred fruits according distribution. huang refer restrictive constraint ﬁrst-order condition block structure imposed upon ﬁrst-order marginals sports tracking permutations represent mapping identities players positions ﬁeld settings ﬁrst-order condition might quite reasonably potential identity confusion within tracks team within tracks blue team confusion teams. ranking example however ﬁrst-order condition forces probability vegetable third place zero even though vegetables will general nonzero marginal probability second place seems quite unrealistic. example consider approximating vote distribution factorized distribution figure plot factored distribution closest true distribution respect total variation distance. approximation candidate constrained independent remaining four candidates maps rank probability capturing fact winner election candidate fully factored distribution seen poor approximation assigning zero probability permutations even permutations received positive number votes. since support true distribution contained within support approximation divergence inﬁnite. riﬄed independence deﬁnitions examples. riﬄe shuﬄe perhaps commonly used method card shuﬄing cuts deck cards piles size respectively successively drops cards photograph riﬄe shuﬄe executed standard deck cards; pictorial example -interleaving distribution cards denoting vegetables blue cards denoting fruits. piles become interleaved single deck again. inspired riﬄe shuﬄe present novel relaxation full independence assumption call riﬄed independence. rankings riﬄe independent formed independently selecting rankings disjoint subsets objects interleaving rankings using riﬄe shuﬄe form ﬁnal ranking objects. intuitively riﬄed independence models complex relationships within allowing correlations sets modeled constrained form shuﬄing. example consider generating ranking vegetables fruits. might ﬁrst ‘cut deck’ piles pile vegetables pile fruits ﬁrst stage independently decide rank pile. example within vegetables might decide peas preferred convolution based deﬁnition riﬄed independence. ways deﬁne riﬄed independence ﬁrst provide deﬁnition using convolutions view inspired card shuﬄing intuitions. mathematically shuﬄes modeled random walks symmetric group. besides riﬄe shuﬄe number diﬀerent shuﬄing strategies pairwise shuﬄe example simply selects cards random swaps them. question then interleaving shuﬄing distributions correspond riﬄe shuﬄes? answer question distinguishing property riﬄe shuﬄe that cutting deck piles size must preserve relative ranking relations within pile. thus card appears card piles shuﬄing card remains card. example relative rank preservation says peas preferred corn prior shuﬄing continue preferred corn shuﬄing. allowable riﬄe shuﬄing distribution must therefore assign zero probability permutations preserve relative ranking relations. turns permutations preserve relations simple description. possible riﬄe shuﬄing distribution might example assign uniform probability permutation zero probability everything else reﬂecting indiﬀerence vegetables fruits. figure graphical example interleaving distribution. definition subsets said riﬄe independent gb)) respect interleaving distribution distributions respectively. notate riﬄed independence relation refer relative ranking factors. starts piles cards stacked together deck. fruits/vegetables setting always prefer vegetables fruits vegetables occupy positions fruits occupy positions ﬁrst step rankings pile drawn independent. example might rankings constituting draw fully independent model described section second stage deck cards interleaved independently selected element example ative example ranking relative ranks vegetables corn. similarly relative ranks fruits lemons grapes oranges. ruit ruit ruit ruit. ranking notation therefore assume note supported subgroup whenever ranking. need simple claim consider ranking element subgroup τab. discussion. presented ways thinking riﬄed independence. ﬁrst formulation terms convolution motivated connections riﬄed independence card shuﬄing theory. show section convolution based view also crucial working fourier coeﬃcients riﬄe independent distributions analyzing theoretical properties riﬄed independence. second formulation hand shows concept riﬄed independence remarkably simple probability single ranking computed without summing rankings fact obvious deﬁnition finally interested readers concept riﬄed independence also simple natural group theoretic description. fully factorized distribution refer distribution supported subgroup factors along dimensions. discussed sparse distributions appropriate ranking applications would like work distributions capable placing nonzero probability mass rankings. case symmetric group however third missing dimension coset space sn/. thus natural extension full independence randomize coset representatives referred discussion interleavings. draws independent ordinary sense item sets riﬄe independent. special cases. number special case distributions captured riﬄed independence model useful honing intuition. discuss extreme cases following list. setting interleaving distribution relative ranking factors uniform distributions yields uniform distribution full rankings. similarly setting distributions delta distributions always yields delta distribution. interesting note always fully independent delta distribution never independent uniform distribution. however uniform delta distributions factor riﬄe independently respect partitioning item set. thus riﬄe independent fact riﬄe independent complement. setting interleaving distribution uniform discuss detail later reﬂects complete indiﬀerence sets even encode complex preferences within alone. setting relative ranking factors uniform distributions means respect joint distribution items completely interchangeable amongst setting interleaving distribution delta distribution -interleavings recovers deﬁnition ordinary probabilistic independence thus riﬄed independence strict generalization thereof full independence regime distributions approximating vote distribution riﬄe independent distributions. approximate distribution candidate riﬄe independent remaining candidates; approximate distribution candidate riﬄe independent remaining candidates; corresponding ﬁrst order marginals approximate distribution. marginal distributions absolute rankings riﬄed independence regime thought marginal distributions relative rankings item sets hand relative ranking factors delta distribution distributions uniform resulting riﬄe independent distribution thought indicator function rankings consistent particular incomplete ranking distributions useful practice input data comes form incomplete rankings rather full rankings. example like independence assumptions commonly used naive bayes models would rarely expect riﬄed independence exactly hold real data. instead appropriate view riﬄed independence assumptions form model bias ensures learnability small sample sizes indicated almost always case distributions rankings. ever expect riﬄed independence manifested real dataset? figure plot riﬄe independent approximation true vote distribution optimal respect kl-divergence approximation figure obtained assuming candidate riﬄe independent seen quite accurate compared truth figure exhibits ﬁrst order marginals approximating distribution also visually seen faithful approximation discuss interpretation result section comparison also display result approximating true distribution candidate winner riﬄe independent remaining candidate. resulting approximation inferior lesson learned example ﬁnding correct/optimal partitioning item important practice. remark however approximation obtained factoring candidate terrible approximation approximations accurate fully independent approximation showed earlier figure divergence true distribution factored distribution interleaving distributions. general case signiﬁcant increase storage required riﬄed independence full independence. addition storage required distributions first-order matrices deck cards riﬄe independent various settings compare matrices fully independent ﬁrst order marginal matrices figure note here nonzero blocks allowed ‘bleed’ zero regions. setting however recovers fully independent case subset objects preferred probability one. distribution mpq. introduce family useful riﬄe shuﬄing distributions described using handful parameters. simplest riﬄe shuﬄing distribution uniform riﬄe shuﬄe munif assigns uniform probability -interleavings zero probability elements used context riﬄed independence munif models potentially complex relations within captures simplest possible correlations across subsets. might example complex preference relations amongst vegetables amongst fruits completely indiﬀerent respect subsets vegetables fruits whole. starting deck cards left pile right pile pick piles probability proportional size drop bottommost card thus mapping either card card rank recurse remaining undropped cards drawing -interleaving right pile picked -interleaving left pile picked. algorithm model bias using simple one-parameter family distributions cards left right piles drop probability proportional respectively instead refer bias parameter family distributions parameterized biased riﬄe shuﬄes. context rankings biased riﬄe shuﬄes provide simple model expressing groupwise preferences entire subset vice-versa. bias parameter thought knob controlling preference subset other might reﬂect example preference fruits vegetables perhaps indiﬀerence subsets. setting recovers full independence assumption preferring objects objects probability setting recovers uniform riﬄe shuﬄe finally number straightforward generalizations biased riﬄe shuﬄe realize richer distributions. example might depend number cards dropped pile section discuss number basic properties riﬄed independence show certain probabilistic inference operations accomplished operating single factor rather entire joint distribution. upon knowing riﬄe independent immediate consequence show full independence regime conditioning operations certain observations assignment problems decompose according riﬄed independence structure. following properties straightforward derive using factorization deﬁnition proposition consider prior likelihood functions hprior hlike subsets riﬄe independent parameters respectively. denote pointwise product operation functions. also riﬄe independent respect posterior distribution bayes rule interleaving distribution recurrence alg. appeared various forms literature ﬁrst recurrence fourier transform consider biased versions. biased riﬄe shuﬄes fulman similar biased riﬄe shuﬄes. uncovering riffled independence structure rankings mlikemprior relative ranking factors flikefprior glikegprior respectively. riﬄe independent subsets. conranked datasets come form pairwise comparisons records form object preferred object corollary proposition argue conditioning pairwise ranking likelihood functions decomposes along riﬄed independence structures. pairwise ranking model objects deﬁned reﬂects fact object preferred object objects belong sets factor requires update using bayes rule. vegetables fruits riﬄe independent example less computation would required compare vegetable vegetable compare fruit vegetable. example observation corn preferred peas aﬀects distribution vegetables. formally state intuitive corollary follows corollary consider conditioning pairwise ranking model suppose riﬄe independent flike subsets respect prior distribution hprior parameters riﬄe independent respect posterior distribution whose parameters identical prior except relative ranking factor corresponding fpost fprior compare result corollary possible fully factored distribution. fully independent conditioning distribution involved items would require updating factor associated item example ﬁrst-order observations form item rank eﬃciently conditioned fully independent scenario. riﬄed independence general possible condition ﬁrst-order observations without modifying parameters. however corollary shows pairwise comparisons involving performed exactly updating either without touch probabilities. algorithms ﬁxed partitioning item set. thus covered number intuitive examples properties riﬄed independence. given rankings drawn distribution interested estimating number statistical quantities parameters riﬄe independent model. section assume known structure given partitioning item interested problem estimating parameters inverse problem computing probabilities given parameters riﬄesplit would like estimate various statistics relative ranking interleaving distributions riﬄe independent distribution given i.i.d. training examples might example want estimate probability interleaving general interested estimating general statistics since variables discrete computing maximum likelihood parameter estimates consists forming counts number training examples consistent given interleaving relative ranking. thus parameters problem simply given following formulas riﬄejoin. estimated parameters riﬄe independent distribution would like compute various statistics data itself. simplest case interested estimating joint probability single ranking evaluated simply plugging parameter estimates second deﬁnition riﬄed independence generally however interested knowing low-order statistics data related statistics probability object preferred object typically low-order statistics must compute rankings. example compute probability item ranked position must rankings equation feasible small quickly grows intractable larger main observations remainder section however loworder marginal probabilities joint distribution always computed directly low-order marginal probabilities relative ranking interleaving distributions without explicitly computing intractable sums. fourier theoretic algorithms riﬄed independence. present algorithms working riﬄed independence fourier theoretic framework huang guestrin guibas huang kondor howard jebara fourier theoretic perspective riﬄed independence presented valuable allow work directly low-order statistics instead form necessary probabilities ﬁrst. note readers primarily interested structure learning jump directly section begin brief introduction fourier theoretic inference permutations kondor detailed exposition). unlike analog real line fourier transform function takes form collection fourier coeﬃcient matrices ordered respect frequency. discussing analog frequency bution simply index fourier coeﬃcient matrices ordered respect measure increasing complexity. useh denote complete collection fourier coeﬃcient matrices. rough understand complexity mentioned section fact low-frequency fourier coeﬃcient matrices distribution used reconstruct low-order marginals. example ﬁrst-order matrix marginals always reconstructed matrices real line many familiar properties fourier transform continue hold. following several basic properties used paper number papers recent years huang kondor howard jebara considered approximating distributions permutations using truncated fourier coeﬃcients proposed inference algorithms operate fourier coeﬃcient matrices. example perform generic marginalization markov chain prediction conditioning operations using fourier coeﬃcients without ever perform inverse fourier transform. section provide generalizations algorithms huang tackle riﬄejoin riﬄesplit problems. assume without loss generality although begin following discussions fourier coeﬃcients provided especially interested algorithms work well cases truncated fourier coeﬃcients present approximately riﬄe independent. conversely given distribution split computes marginalizing respectively. example split returns overload join/split names refer ordinary fourier theoretic formulations procedures. riﬄejoin fourier domain. given fourier coeﬃcients compute fourier coeﬃcients using deﬁnition applying join algorithm huang convolution theorem tells fourier transform convolution written pointwise product fourier transforms. compute fourier theoretic forgeneral intractable fourier transform riﬄe shuﬄing distribution mpq. however cases computed. example computed directly training examples simply compute desired fourier coeﬃcients using deﬁnition fourier transform given huang guestrin guibas tractable long samples tractably stored employing recurrence relation algorithm particular algorithm expresses biased riﬄe shuﬄe linear combination biased riﬄe shuﬄes sn−. invoking linearity fourier transform dynamic programming approach quite reminiscent clausen’s algorithm clausen baum describe algorithm detail appendix best knowledge ﬁrst compute fourier transform riﬄe shuﬄing distributions. riﬄesplit fourier domain. given fourier coeﬃcients riﬄe independent distribution would like tease apart factors. following show recover relative ranking distributions defer problem recovering interleaving distribution appendix mpq]i· g]i. ﬁrst solution splitting problem might occur perform deconvolution multiplying eachhi term inverse ·hi) call split algorithm huang matrix mpq]i mpq] result. unfortunately matrix mpq]i general noninvertible. instead riﬄesplit algorithm left-multiplies eachhi term munif shown equivalent convolving distribution ‘dual shuﬄe’ deﬁned munif convolving produce distribution factors independently split algorithm huang still shown recover fourier transforms maximum likelihood parameter estimates distribution training examples computed summing examples relative ranking consistent equivalently marginalizing interleavings relative rankings thus have second step notice outer summation equation exactly type marginalization already done fourier domain split algorithm huang thus split) function deﬁned rewritten hence could compute fourier transform function could apply ordinary split algorithm recover fourier transform next standard fact fourier transforms given function deﬁned fourier coeﬃcient matrices related transpose. hence every frequency level applying convolution theorem fourier coeﬃcients dual shuﬄe empirical distribution establishes ﬁnal part theorem. notice compute relative ranking factors fourier domain necessary know interleaving distribution. necessary however compute fourier coeﬃcients uniform interleaving distribution discuss appendix also necessary normalize output split fortunately normalizing function performed fourier domain simply dividing marginal preservation guarantees. performing fourier domain algorithms complete fourier coeﬃcients intractable performing computations naively. typically fourier setting hopes instead work low-order terms. example case riﬄejoin might receive second order marginals parameter distributions input. natural question then approximation quality output given bandlimited input? state result below shows algorithms perform called truncated fourier coeﬃcients. theorem given enough fourier terms reconstruct kth-order marginals riﬄejoin returns enough fourier terms exactly reconstruct kth-order marginals likewise given enough fourier terms reconstruct kth-order marginals riﬄesplit returns enough fourier terms exactly reconstruct kth-order marginals proof. result simple consequence well-known convolution theorem theorems huang theorem huang states that given sth-order marginals factors join algorithm reconstruct sth-order marginals joint distribution exactly. since riﬄe independent joint distribution convolution operations pointwise fourier domain given enough fourier terms reconstruct sth-order marginals function also reconstruct sth-order marginals riﬄe independent joint output riﬄesplit. fourier coeﬃcient matrix frequency level joint distribution running time complexity join/split algorithms huang worst cubic dimension interleaving fourier coeﬃcients precomputed ahead time complexity riﬄejoin/riﬄesplit also must fourier transform interleaving distribution. riﬄejoin fourier transform empirical distribution directly deﬁnition algorithms presented appendix case biased riﬄe shuﬄes running time worst case riﬄesplit must compute fourier transform uniform interleaving distribution which shown section also takes form biased riﬄe shuﬄe therefore also computed time. section plot experimental running times. section explore natural model simpliﬁcation comes simple observation that since relative ranking distributions distributions rankings sets decomposed riﬄe independent subsets. call models hierarchical riﬄe independent decompositions. continuing running example imagine fruits partitioned sets consisting citrus fruits lemons oranges) consisting mediterranean fruits figs grapes). generate full ranking ﬁrst draws rankings citrus mediterranean fruits independently andg example). secondly sets interleaved form ranking fruits finally ranking vegetables drawn interleaved fruit rankings form full joint rankingp notationally express hiernatural question used diﬀerent hierarchy leaf sets would capture distributions? example distribution decomposes according tree figure also decompose according tree figure answer general fact distinct hierarchies impose diﬀerent sets independence assumptions result diﬀerent structures well badly suited modeling given dataset. consequently important correct structure possible. interesting note however structures figures encode distinct families distributions possible identify independence assumptions common structures. particular since structures leaf sets distributions consistent either hierarchies must also consistent call -way decomposition. deﬁne dway decomposition distribution single level hierarchy instead partitioning entire item subsets partitions subsets interleaves relative rankings subsets together form joint ranking items. distribution consistent either figure must consequently also consistent structure figure generally have proof. proceed induction. suppose result holds want establish result also holds factors according hierarchical riﬄe independent model written interleaving distribution factor hierarchical riﬄe independent distributions with leaf sets respectively hypothesis since |a||b| factor d-way decompositions respectively. therefore write last line follows legitimate interleaving sets also legitimate interleaving sets since φai) φai. shows distribution factors d-way decomposition concludes proof. general knowing hierarchical decomposition model desirable knowing d-way decomposition require many parameters example nway decomposition requires parameters captures every distribution permutations. thin chain models. class particularly simple hierarchical models refer k-thin chain models. k-thin chain model refer hierarchical structure size smaller split hierarchy ﬁxed constant therefore expressed figure example -thin chain. view thin chains somewhat analogous thin junction tree models cliques never allowed variables. example number model parameters scales polynomially draw rankings thin chain model sequentially inserts items independently group size time full ranking. research psychologists) fell opposite ends political spectrum candidate somewhat independent. diaconis conjectured voters choose group other choose within. able verify diaconis’ conjecture using riﬄed independence framework. removing candidate distribution perform search within candidates nearly riﬄe independent subsets. nearly riﬄe independent thus able verify candidate sets indeed grouped riﬄe independent sense data. remark later work marden identiﬁed candidate belonging third group psychologists called community psychologists. hierarchical structure best describes data shown figure kl-divergence true distribution hierarchical model finally main opposing groups within riﬄe shufﬂing distribution sets well approximated biased riﬄe shuﬄe. instead since coalitions mixture biased riﬄe shuﬄes data found bias parameters mixture components indicating components oppose structure discovery objective functions. since diﬀerent hierarchies impose diﬀerent independence assumptions would like structure best suited modeling given ranking dataset. datasets natural hierarchy might available example familiar typical politics elections possible guess optimal hierarchy. however general ranked data always obvious kind groupings riﬄed independence lead particularly large fruits really riﬄe independent vegetables? green foods riﬄe independent foods? covering hierarchical riﬄe independent structures training data. among observations fact item ranks cannot independent mutual exclusivity relative ranks sets items subject constraints. simply ‘clustering’ algorithm however procedure thought structure learning algorithm like graphical models literature koller friedman optimal independence decomposition distribution. base problem address current section best structure level partitioning leaf sets alternatively want topmost partitioning tree. section base case part top-down approach learning full hierarchy. problem statement. given then training rankings drawn i.i.d. distribution subset items riﬄe independent complement problem address section automatically determining sets exactly factor riﬄe independently would like riﬄe independent approximation closest sense. formally would like solve problem empirical distribution training examples kullback-leibler divergence measure. equation seemingly reasonable objective since also interpreted maximizing likelihood training data. limit inﬁnite data equation shown gibbs inequality attain minimum zero subsets sets truly riﬄe independent other. small problems actually solve problem using single computer evaluating approximation quality subset taking minimum approach taken example however larger problems runs time sample complexity problems since optimizing globally deﬁned objective function requires relearning model parameters exponentially many subsets fact large sets rare would enough samples estimate relative ranking parameters without already discovered hierarchical riﬄe independent decompositions next propose locally deﬁned objective function reminiscent clustering instead equation proposed objective function. approach take minimize diﬀerent measure exploits observation absolute ranks items fully independent relative ranks items vice versa vegetables fruits example knowing figs ranked ﬁrst among items give information whether corn preferred peas formally given subset recall denotes vector ranks assigned items σ))). propose minimize alternative objective function denotes mutual information dkl||p function likelihood interpretation objective function equation however thought composite likelihood models relative rankings independent absolute rankings relative rankings independent absolute rankings respect distributions satisfy models minimizing equivalent maximizing likelihood data. furthermore show guaranteed detect riﬄed independence proof. suppose riﬄe independent. ﬁrst claim independent. this observe absolute ranks determined relative rankings interleaving τab. assumption riﬄe independent know relative rankings interleaving independent establishing claim. argument independent similar thus establishing direction proposition. converse observation above note absolute ranks determine relative ranks well interleaving τab. similarly determines τab. thus τab) φb). follows equation optimizing still intractable large however motivates natural proxy replace mutual informations deﬁned variables mutual informations deﬁned three variables time. viewed order version involving mutual information computations triplets variables time instead n-tuples. mutual information ii;jk example reﬂects much rank vegetable tells fruits compare. riﬄe independent know ii;jk given fruits vegetables riﬄe independent sets knowing grapes preferred figs give information absolute examples shows graphical depiction problem ﬁnding riﬄe independent subsets. triangle vertices represents term ii;jk since ii;jk invariant respect permutation indices triangles directed therefore double bars represent nodes term ii;jk. note tripletwise terms instead replaced edgewise terms problem would simply standard clustering problem; shows matrix tripletwise mutual informations computed dataset rank corn therefore icorn;grapesf zero. note tripletwise independence assertions bear resemblance assumptions sometimes made social choice theory commonly referred independence irrelevant alternatives addition third element assumed aﬀect whether prefers element objective somewhat reminiscent typical graphcut clustering objectives. instead partitioning nodes based sums pairwise similarities partition based sums tripletwise aﬃnities. show graphical depiction problem figure cross triplets note word directed signiﬁcant because unlike typical clustering problems triplets symmetric ij;ik) resulting nonstandard poorly understood optimization problem. highlighted corresponds candidate mutual information terms close zero. tripletwise mutual information terms tell story consistent conclusion example showed candidate approximately riﬄe independent remaining candidates. finally also interesting examine entry. largest mutual information matrix fact surprising since candidates politically aligned thus knowing example candidate ranked ﬁrst strong indication candidate preferred candidate practice like minimum objective graphs tripletwise objective equation tendency prefer small partitions balanced partitions fact unbalanced partitions fewer triplets cross simplest avoid bias optimize objective function subsets ﬁxed size discuss next section optimizing ﬁxed useful building thin hierarchical riﬄe independent models. alternatively modiﬁed objective function encourages balanced partitions. example found following normalized inspired variation objective useful detecting riﬄed independence size unknown intuitively denominator equation penalizes subsets whose interiors small weight. note exist many variations objective function encourage balance balanced used experiments. low-order detectability assumptions.. detect riﬄed independence? diﬃcult example necessary condition riﬄed independence since implies ia;bb have riﬄe independent sets pairwise independence assumptions commonly used randomized algorithms imply full independence sets variables exist distributions look riﬄe independent tripletwise marginals factor upon examining higher-order terms. nonetheless practical scenarios expect imply riﬄed independence. quadrupletwise objective functions riﬄed independence. natural variation method base objective function following quantities deﬁned quadruplets items instead triplets intuitively iij;kl measures much knowing that peas preferred corn tells whether grapes preferred oranges. again fruits vegetables riﬄe independent mutual information zero. summing terms cross obtain riﬄe independent mutual information iij;kl zero. unlike tripletwise counterparts however iijkl arise global measure necessary suﬃcient detecting riﬄed independence. particular insuﬃcient guarantee riﬄed independence. example interleaving depends relative rankings riﬄed independence satisﬁed quad moreover clear would detect riﬄe independent subsets consisting single element using quadrupletwise measure. such focused tripletwise measures experiments. nonetheless quadrupletwise measures potentially useful practice signiﬁcant advantage iij;kl estimated fewer samples using almost imaginable form partially ranked data. estimating objective samples. argued reasonable function ﬁnding riﬄe independent subsets. however since access samples rather true distribution itself possible compute approximation objective particular every triplet items must compute estimate mutual information ii;jk i.i.d. samples drawn pairwise independent family random variables members marginally independent. subsets larger members necessarily factor independently however. following denote estimated value ii;jk ˆii;jk. triplet regularized procedure höﬀgen estimate mutual information. adapt sample complexity bound problem below. want show that exists unique partition riﬄe independent sets given enough training examples approximation uniquely singles correct partition minimum high probability. class riﬄe independent distributions uniqueness requirement satisﬁed consists distributions strongly connected according following deﬁnition. definition subset called \u0001-third-order strongly connected every triplet distinct ii;jk riﬄe independent sets third order strongly connected ensure riﬄed independence detectable third-order terms partition unique. following probabilistic guarantee. appendix details. finally remark strong connectivity assumptions used theorem stronger necessary respect certain interleaving distributions even case estimated objective function singles correct partition internal triplets belonging zero mutual information. moreover cases multiple valid partitionings item set. example uniform distribution distribution every subset riﬄe independent complement. cases multiple solutions equally good evaluated sample approximation structure discovery algorithms. designed function tractable estimate perspectives computational sample complexity turn problem learning hierarchical riﬄe independence structure distribution training examples. instead directly optimizing objective space possible hierarchies take simple top-down approach item sets recursively partitioned optimizing stopping criterion small therefore exhaustive optimization learn structure k-thin chain models polynomial time. structure learning problem thin chains discover items partitioned groups group inserted ﬁrst group inserted second learn structure thin chain exhaustive optimization learn topmost partitioning item recursively learn thin chain model items larger subset. handling arbitrary partitions using anchors. large even unknown cannot optimized using exhaustive methods. instead propose simple algorithm ﬁnding based following observation. oracle could identify elements advance quantity ix;aa indicates whether item belongs since ix;aa nonzero ﬁrst case zero second case. ﬁnite training sets known approximately sort {ix;aa known take items closest zero since compare items refer ﬁxed items anchors. course known advance ﬁxing arbitrary item repeat method settings produce collection candidate partitions. partition scored using approximate objective ﬁnal optimal partition selected minimum candidates. algorithm cases known priori evaluate partitions possible settings using since anchors method require searching subsets signiﬁcantly faster exhaustive optimization moreover assuming \u0001-third order strong connectivity previous section similar arguments derive sample complexity bounds. corollary \u0001-third order strongly connected riﬄe independent sets suppose given i.i.d. samples output anchors algorithm exactly probability particular anchors estimator consistent. remark however practical diﬀerences times make anchors method somewhat less robust exhaustive search. conceptually anchoring works well exists elements strongly connected elements used anchor elements exhaustive search work well weaker conditions items strongly connected longer paths. show experiments anchors method nonetheless quite eﬀective learning hierarchies. running time. consider running time structure learning procedures. cases necessary precompute mutual information quantities ii;jk triplets samples. triplet compute ii;jk linear time respect sample size. triplets therefore computed time. exhaustive method ﬁnding k-subset minimizes recomplexity evaluating particular partition need precomputed mutual informations number triangles cross bound number triangles thus require optimization time leading bound total time. anchors method requires precompute mutual informations. seeming bottleneck last step must evaluate objective function partitions. reality larger held ﬁxed arbitrary element must optimize partitions. case sets trivially riﬄe independent showed previous paragraph evaluating requires time thus optimization using anchors method total time. since much smaller drop big-o notation time complexity showing anchors method dominated time required precompute cache mutual informations. structure discovery quantifying stability. given hierarchy estimated data discuss might practically quantify conﬁdent hypothesized structure. might like know amount data used estimating structure adequate support learned structure data looked slightly diﬀerent would hypothesis change? bootstrapping oﬀers simple approach repeatedly resample data replacement estimate hierarchical structure resampling. diﬀerence setting typical bootstrapping settings however structures large show distribution structures estimated bootstrapped samples data plots fraction bootstrapped trees sample size agree exactly hierarchy given figure summarize boostrap distribution largest sample sizes. discrete set. thus unlike continuous parameters whose conﬁdence often summarize intervals ellipses clear might compactly summarize collection many hierarchical clusterings items. simplest summarize collection hierarchies obtained bootstrap measure fraction estimated structures identical structure estimated original unperturbed dataset. small sets resampled data estimated hierarchy consistently identical obtained original data conﬁdent data supports hypothesis. show following example that structure learned dataset smaller dataset would suﬃced. example ﬁnal related example show results bootstrap resampling figure generate plots resampled dataset replacement times varying sample sizes anchors algorithm resulting sample. figure plots fraction bootstrapped trees sample size agree exactly hierarchy given figure given forced sets partitioned items possible hierarchical structures dataset. interesting hierarchies returned algorithm surprisingly stable even given fewer samples bootstrapped trees agreeing optimal hierarchy. samples altrees agree optimal hierarchy. figure show table bootstrap distribution largest sample sizes leaf sets original dataset necessarily correct hierarchy. proposition correctly discovering leaf partitioning probabilistically meaningful corresponds correctly identifying d-way decomposition corresponding distribution failing identifying speciﬁc hierarchy. remark sometimes unique structure corresponding distribution. uniform distribution example consistent hierarchical riﬄe independent structure bootstrapped hierarchies concentrate particular structure even substructure. moreover even true unique structure corresponding generating distribution case simpler structures perform better much available training data. related work. work draws several literatures card shufﬂing research primarily persi diaconis collaborators papers fourier theoretic probabilistic inference permutations machine learning community well graphical model structure learning research. card shuﬄing theory. bayer diaconis provided convergence analysis repeated riﬄe shuﬄes. novelty lies combination shuﬄing theory independence ﬁrst exploited huang scaling inference operations large problems. finally remark fulman introduced class shuﬄes known biased riﬄe shuﬄes biased riﬄe shuﬄes discussed paper. fact uniform riﬄe shuﬄing realized dropping card probability proportional number cards remaining hand observed number papers ﬁrst formalize form fourier analysis permutations. dynamic programming approach bears similarities algorithm proposed clausen baum particular relies branching rule recursions clausen requires time since biased riﬄe shuﬄes parameterized single recurrence compute low-frequency fourier terms polynomial time. learning structured representations. insights structure learning problems inspired recent approaches machine learning literature learning structure thin junction trees particular idea using order proxy objective graph-cut like optimization algorithm similar idea recently introduced shahaf chechetka guestrin determines optimally thin separators respect bethe free energy approximation rather typical log-likelihood objective. sample analysis based mutual information sample complexity bounds derived höﬀgen also used chechetka guestrin developing structure learning algorithm thin junction trees provably polynomial sample complexity. finally bootstrap methods employed experiments verifying robustness bear much resemblance common bootstrapping methods used bioinformatics analyzing phylogenetic trees section present series experiments validate models methods. experiments implemented matlab except fourier theoretic routines written c++. tested machines quadcore opteron .ghz processors memory. already analyzed data extensively throughout paper. here demonstrate algorithms simulated data well real datasets namely sushi preference data irish election data. simulated data. begin discussion simulated data experiments. ﬁrst consider approximation quality timing issues single binary partition item set. binary partitioning item set. understand behavior riﬄesplit approximately riﬄe independent situations drew sample sets varying sizes riﬄe independent distribution riﬄesplit estimate relative ranking factors interleaving distribution empirical distribution. figure plot kl-divergence true distribution obtained applying riﬄejoin estimated riﬄe factors. small sample sizes able recover accurate approximations despite fact empirical distributions exactly riﬄe independent. comparison experiment using split algorithm huang recover parameters. perhaps surprisingly show split algorithm huang also unbiased consistent estimator riﬄe factors return maximum likelihood parameter estimates eﬀectively ignores rankings contained subgroup consequently riﬄesplit algorithm converges correct parameters fewer samples. next show fourier domain algorithms capable handling sizeable item sets working low-order terms. figure fourier domain riﬄejoin algorithm various simulated distributions. plot running times riﬄejoin function worst case) scaling learning hierarchy items. next applied methods synthetic data show that given enough samples algorithms eﬀectively recover optimal hierarchical structures generated original datasets. various settings simulated data drawn jointly k-thin chain model random parameter setting structure applied exact method learning thin chains sampled dataset. first investigated eﬀect varying sample size proportion trials algorithms able recover full underlying tree structure exactly recover topmost partition correctly recover leaf sets correctly figure shows result itemset size figure shows function number samples required experiments exactly recover full underlying structure recover correct leaf sets least trials. observe plots that given enough samples reliable structure recovery indeed possible. also interesting note recovery correct leaf sets done much fewer samples required recovering full hierarchical structure model. evaluated log-likelihood model test examples drawn true distributions. figure compare log-likelihood performance true structure given k-thin chain learned known random generated -chain structure. expected knowing true structure results best performance -chain overconstrained. however structure learning algorithm eventually able catch performance true structure given enough samples. also interesting note jump performance halfway point plot coincides jump success rate discovering leaf sets correctly conjecture performance sometimes less sensitive actual hierarchy used long leaf sets correctly discovered. test anchors algorithm simulation using algorithm data drawn hierarchical models ﬁxed generated roughly balanced structures meaning item sets recursively partitioned equally sized subsets level hierarchy. figure anchors algorithm also discover true structure given enough samples. interestingly diﬀerence sample complexity discovering leaf sets versus discovering full tree nearly pronounced figure believe fact balanced trees less depth thin chains leading fewer opportunities greedy top-down approach commit errors. data analysis sushi preference data. turn analyzing real datasets. ﬁrst analysis examine sushi preference ranking dataset consisting full rankings types sushi. items enumerated figure note that compared election data sushi dataset twice many items fewer examples. begin studying methods case single binary partitioning item set. unlike dataset obvious naturally partition types sushi sets ﬁrst experiments arbitrarily divided item divided data training test sets estimated true distribution three ways directly samples using riﬄe independent distribution optimal shuﬄing distribution biased riﬄe shuﬄe figure plots testset log-likelihood function training size riﬄe independence assumptions help signiﬁcantly lower sample complexity learning. biased riﬄe shuﬄes also seen useful learning bias small samples. illustration behavior biased riﬄe shuﬄes figure shows approximate ﬁrst-order marginals rankings biased riﬄe approximation. marginals interesting many people like thus providing high rankings many people also hate providing rankings. ﬁrst-order marginal estimates signiﬁcant variance sample sizes biased riﬄe approximation achieve reasonable approximation distribution even samples cost somewhat oversmoothed. structure learning sushi dataset. figure shows hierarchical structure learn using entire sushi dataset. since sushi prepartitioned distinct coalitions somewhat diﬃcult with data interpret whether estimated structure makes sense. however parts tree certainly seem like reasonable groupings. example tuna related sushi types clustered together. tamago kappa-maki safer typically boring choices sake daring. anago estimated hierarchy understand behavior algorithm smaller sample sizes looked features tree figure remained stable even learning smaller sample sizes. figure summarizes results bootstrap analysis sushi dataset resample original training times diﬀerent sample sizes plot proportion learned hierarchies which recover ‘sea eel’ topmost partition recover leaf sets correctly recover entire tree correctly recover tuna-related sushi leaf recover {tamago kappa-maki} leaf recover {uni sake} leaf set. data analysis irish election data. next applied algorithms larger irish house parliament election dataset meath constituency ireland dáil éireann uses single transferable vote election system voters rank subset candidates. meath constituency candidates election running allotted seats. candidates identiﬁed major rival political parties fianna fáil fine gael well number smaller parties gormley murphy election details well alternative analysis. experiments used subset roughly fully ranked ballots election. summarize dataset figure shows matrix ﬁrst-order marginals estimated dataset. candidates form major party candidates belonging either fianna fáil fine gael shown ﬁgure fared much better election seven minor party candidates. notably candidates received average lowest ranks election. diﬀerences candidates however signiﬁcant portion electorate also ranked sinn féin candidate high. though necessarily clear might partition candidates natural idea might assume major party candidates riﬄe independent minor party candidates figure show ﬁrst-order marginals corresponding approximation assumed riﬄe independent. visually approximate ﬁrst-order marginals seen roughly similar exact ﬁrstorder marginals however signiﬁcant features matrix captured approximation example columns belonging candidates well approximated. figure plot principled approximation corresponding learned hierarchy discuss next. seen ﬁrst-order marginals obtained structure discovery irish election data. data exhaustive optimization anchors algorithm returned tree running times seconds seconds respectively resulting tree candidates enumerated alphabetically shown figure expected candidates belonging major parties fianna fáil fine gael neatly partitioned leaf sets. topmost leaf sinn fein candidate indicating voters tended insert ranking independently candidates. understand behavior algorithm smaller sample sizes looked features tree figure remained stable even learning smaller sample sizes. figure resampled original training times diﬀerent sample sizes plot proportion learned hierarchies which recover sinn fein candidate topmost leaf partition major parties leaf sets agree original tree leaf sets recover entire tree. note dataset insuﬃcient support entire tree structure even training examples candidates belonging major parties consistently grouped together indicating strong party inﬂuence voting behavior. compared results learning general hierarchy learning -thin chain model irish data. figure shows log-likelihoods achieved models held-out test training size increases. training size subsampled irish dataset times produce conﬁdence intervals. again even small sample sizes hierarchy outperforms -chain continually improves training data. might think hierarchical models parameters prone overﬁtting practice models learned algorithm devote extra parameters towards modeling correlations among major parties. results suggest intraparty ranking correlations crucial achieving good modeling performance. finally structure learning algorithm similar smaller election datasets constituencies election supported electronic voting dublin north west constituencies. figure shows resulting hierarchies learned dataset. meath constituency fianna fáil fine gael consistently grouped together leaf sets dublin datasets. interestingly sinn féin socialist parties also consistently grouped dublin datasets potentially indicating latent similarities parties. conclusions. exploiting independence structure eﬃcient inference sample complexity simple powerful idea pervasive throughout machine learning literature showing form bayesian networks markov random ﬁelds more. rankings independence problematic mutual exlusivity constraints began paper indicating need useful generalization independence. main contribution paper deﬁnition generalized notion namely riﬄed independence. number natural questions immediately follow deﬁnition uncovering riffled independence structure rankings generalization retain computational advantages evidence generalized independence relations hold subsets items ranking dataset indeed satisfy generalized independence assumption approximately could algorithmically determine subsets samples? shown riﬄed independence answer questions lies aﬃrmative. next explored hierarchical riﬄe independent decompositions. model riﬄe independent subsets recursively chained together leads simple interpretable model whose structure estimate data successfully applied learning algorithms several real datasets. currently success structure learning methods depends existence fairly sizeable dataset full rankings. however ranking datasets typically composed partial incomplete rankings often easier elicit multitude users. example top-k type rankings even rating data common. extending parameter structure learning algorithms handling partially ranked data would valuable practical extension work. structure learning tripletwise mutual information measures already potentially estimated within top-k ranking setting. would interesting also develop methods estimating mutual information measures forms partial rankings. additionally eﬀect using partial rankings structure learning sample complexity understood ﬁeld would beneﬁt careful analysis. many possible extensions possible. paper developed algorithms estimating maximum likelihood parameters. small training sizes bayesian approach would appropriate prior placed parameter space. however prior distribution ties parameters together structure learning problem considerably complicated since would able simply identify independence relations. riﬄed independence tool analyzing ranked data shown potential give insights ranking datasets. strongly believe crucial developing fast eﬃcient inference learning procedures ranking data perhaps forms permutation data. work supported part muri young investigator program grant n---. thank khalid el-arini feedback initial drafts brendan murphy claire gormley providing irish voting datasets. discussions marina meila provided valuable initial ideas upon work based. proof lemma proof. item since number deﬁnition interleaving returns largest rank thus φa-th largest rank simply absolute rank item therefore conclude similarly conclude φb]. examine divergence objective introduced section standard fact minimizing equation equivalent structure maximizes log-likelihood training data. estimated using counts training data. equivalence signiﬁcant justiﬁes structure learning data necessarily generated distribution factors riﬄe independent components. using similar manipulation rewrite objective function composite likelihood functions. evaluating data log-likelihood ﬁrst model absolute ranks items independent relative ranks items secondly model absolute ranks items independent relative ranks items again estimated using counts training data. distributions factor along inputs optimizing objective function equivalent optimizing likelihood riﬄe independent model. thus data already riﬄe independent structure learning objective indeed interpreted maximizing log-likelihood data otherwise seem clear equivalence objective functions. testing independence relative ranks insuﬃcient. check relative ranks independent relative ranks another natural objective function detecting riﬄe independent subsets equation certainly necessary condition subsets riﬄe independent would suﬃcient? easy construct counterexample simply distribution interleaving depends either relative rankings. proof. proof induction base case obvious since algorithm return single permutation. next assume sake induction algorithm returns uniformly distributed interleaving want show also case fourier transforming biased riﬄe shuﬄe. describe recurrence satisﬁed munif distribution terms munif distributions given function deﬁne embedded function otherwise. algorithm rephrased recurrence relation follows. writing recursion form equation provides construction uniform riﬄe shuﬄe sequence operations smaller distribution performed completely respect fourier coeﬃcients. particular given fourier coeﬃcients function construct fourier coeﬃcients embedding applying branching rule sagan details). using linearity property convolution theorem fact embeddings performed fourier domain arrive equivalent fourier-theoretic recurrence frequency level information algorithm bears much resemblance pascal’s triangle computing binomial coeﬃcients. arrows diagram indicate fourier transforms must precomputed computing fourier transform larger irreducible representations). implementing recurrence code naively result exponential time algorithm careful. necessary dynamic programming sure recompute things already computed. algorithm present pseudocode dynamic programming approach builds ‘pascal’s triangle’ similar might constructed compute table binomial coeﬃcients. pseudocode assumes existence fourier domain algorithms convolving distributions embedding distribution figure graphical illustration algorithm. uncovering riffled independence structure rankings proof. ﬁxed triplet hoﬀgen’s result implies estimated accuracy probability least using i.i.d. samples since variable arity setting estimating mutual information triplet therefore requires sample complexity expansion ii;jk simple union bound bound probability collection mutual informations triplets estimated within accuracy. deﬁne ∆ijk ii;jk ˆii;jk. goal proof strong connectivity assumptions lower bound ˜f). particular strong connectivity triplet inside ωcross must contribute least objective function ˜f). therefore suﬃces lower bound number triplets cross internal either deﬁne note straightforward check that |ωcross |ωcross theorem k-subset riﬄe independent complement \u0001-third order strongly connected given i.i.d. samples minimum achieved exactly subsets probability least c.-t. lebanon visualizing diﬀerences search algorithms using expected weighted hoeﬀding distance. proceedings international world wide conference mallows non-null ranking models. biometrika marden analyzing modeling rank data. chapman hall. maslen eﬃcient computation fourier transforms symmetric", "year": 2010}