{"title": "Interactive Robot Learning of Gestures, Language and Affordances", "tag": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "abstract": "A growing field in robotics and Artificial Intelligence (AI) research is human-robot collaboration, whose target is to enable effective teamwork between humans and robots. However, in many situations human teams are still superior to human-robot teams, primarily because human teams can easily agree on a common goal with language, and the individual members observe each other effectively, leveraging their shared motor repertoire and sensorimotor resources. This paper shows that for cognitive robots it is possible, and indeed fruitful, to combine knowledge acquired from interacting with elements of the environment (affordance exploration) with the probabilistic observation of another agent's actions.  We propose a model that unites (i) learning robot affordances and word descriptions with (ii) statistical recognition of human gestures with vision sensors. We discuss theoretical motivations, possible implementations, and we show initial results which highlight that, after having acquired knowledge of its surrounding environment, a humanoid robot can generalize this knowledge to the case when it observes another agent (human partner) performing the same motor actions previously executed during training.", "text": "figure experimental setup consisting icub humanoid robot human user performing manipulation gesture shared table different objects top. depth sensor top-left corner used extract human hand coordinates gesture recognition. depending gesture target object resulting effect differ. ical actions towards realization ﬁnal target. human team coordination mutual understanding effective capacity adapt unforeseen events environment re-plan one’s actions real time necessary common motor repertoire action model permits understand partner’s physical actions manifested intentions neuroscience research visuomotor neurons subject ample study mirror neurons class neurons responds action object interaction agent acts observes action performed others hence name mirror. work takes inspiration theory mirror neurons contributes towards using humanoid cognitive robots. show robot ﬁrst acquire knowledge sensing self-exploring surrounding environment result robot capable generalizing acquired knowledge observing another agent performs similar physical actions ones executed prior robot training. fig. shows experimental setup. growing ﬁeld robotics artiﬁcial intelligence research human–robot collaboration whose target enable effective teamwork humans robots. however many situations human teams still superior human–robot teams primarily human teams easily agree common goal language individual members observe effectively leveraging shared motor repertoire sensorimotor resources. paper shows cognitive robots possible indeed fruitful combine knowledge acquired interacting elements environment probabilistic observation another agent’s actions. propose model unites learning robot affordances word descriptions statistical recognition human gestures vision sensors. discuss theoretical motivations possible implementations show initial results highlight that acquired knowledge surrounding environment humanoid robot generalize knowledge case observes another agent performing motor actions previously executed training. index terms cognitive robotics gesture recognition object affordances robotics progressing fast steady systematic shift industrial domain domestic public leisure environments application areas particularly relevant researched scientiﬁc community include robots people’s health active aging mobility advanced manufacturing short domains require direct effective human–robot interaction communication however robots reached level performance would enable work humans routine activities ﬂexible adaptive example presence sensor noise unexpected events previously seen during training learning phase. reasons explain performance human–human teamwork human–robot teamwork collaboration aspect whether members team understand another. humans ability working successfully groups. agree common goals work towards execution goals coordinated understand other’s physlarge growing body research directed towards having robots learn cognitive skills improving capabilities interacting autonomously surrounding environment. particular robots operating unstructured scenario understand available opportunities conditioned body perception sensorimotor experiences intersection elements gives rise object affordances called psychology usefulness affordances cognitive robotics fact capture essential properties environment objects terms actions robot able perform authors suggested alternative computational model called object–action complexes links low-level sensorimotor knowledge high-level symbolic reasoning hierarchically autonomous robots. addition several works demonstrated combining robot affordance learning language grounding provide cognitive robots useful skills learning association spoken words sensorimotor experience sensorimotor representations learning tool capabilities carrying complex manipulation tasks expressed natural language instructions require planning reasoning joint model proposed learn robot affordances together word meanings. data contains robot manipulation experiments associated number alternative verbal descriptions uttered speakers total recordings. framework assumes robot action known priori training phase resulting model used testing make inferences environment including estimating likely action based evidence pieces information. several neuroscience psychology studies build upon theory mirror neurons brought introduction. studies indicate perceptual input linked human action system predicting future outcomes actions effect actions particularly person possesses concrete personal experience actions observed others also exploited deep learning paradigm using multiple timescales recurrent neural network artiﬁcial simulated agent infer human intention joint information object affordances human actions. difference line research real noisy data acquired robots sensors test models rather virtual simulations. paper combine robot affordance model associates verbal descriptions physical interactions agent environment gesture recognition system infers type action human user movements. consider three manipulative gestures corresponding physical actions performed agent onto objects table grasp touch. reason effects actions onto objects world co-occurring verbal description complete framework experiments. figure abstract representation probabilistic dependencies model. shaded nodes observable measurable present study edges indicate bayesian dependency. bayesian networks probabilistic model represents random variables conditional dependencies graph fig. advantages using expressive power allows marginalization variables given variables. main contribution extending relaxing assumption action known learning phase. assumption acceptable robot learns self-exploration interaction environment must relaxed robot needs generalize acquired knowledge observation another agent. estimate action performed human user human–robot collaborative task employing statistical inference methods hidden markov models provides advantages. first infer executed action training. secondly testing time merge action information obtained gesture recognition information affordances. following method adopted bayesian probabilistic framework allow robot ground basic world behavior verbal descriptions associated world behavior deﬁned random variables describing actions deﬁned {ai} object properties {fi} effects {ei}. denote state world experienced robot. verbal descriptions denoted words {wi}. consequently relationships words concepts expressed joint probability distribution actions object features effects words spoken utterance. symbolic variables discrete values listed table addition symbolic variables model also includes word variables describing figure structure hmms used human gesture recognition adapted work consider three independent multiple-state hmms trained recognize considered manipulation gestures. joint probability distribution illustrated part fig. enclosed dashed estimated robot ego-centric interaction environment consequence learning robot knows action performing certainty variable assumes deterministic value. assumption relaxed present study extending model observation external agents explained below. gesture recognition hmms models previously trained spotting manipulationrelated gestures consideration. input features coordinates tracked human hand coordinates obtained commodity depth sensor transformed centered person torso normalized account variability amplitude gesture recognition models represented fig. correspond gesture hmms block fig. gesture deﬁned discrete states model temporal phases comprising dynamic execution gesture parameters {aij} transition probability matrix transition probability state time state time {fi} observation probability functions continuous mixtures gaussian values initial probability distribution states. study wish generalize model observing external agents shown fig. reason full model extended perception module capable inferring action agent visual inputs. corresponds gesture hmms block fig. affordance–words bayesian network model gestures hmms combined different ways gesture hmms provide hard decision action performed human gesture hmms provide posterior distribution task infer action posterior gesture hmms combined follows assuming provide independent information present preliminary examples types results predictions effects actions onto environment objects predictions associated word descriptions presence absence action prior. section assume gesture hmms provide discrete value recognized action performed human agent combined model words affordances observed actions report inferred posterior value object velocity effect given prior information action cases observes human agent performing familiar actions shared human–robot environment. shown promising preliminary results indicate robot’s ability predict future beneﬁt incorporate knowledge partner’s action facilitating scene interpretation result teamwork. terms future work several avenues explore. main ones implementation fully probabilistic fusion affordance gesture components quantitative tests larger corpora human–robot data; explicitly address correspondence problem actions between agents operating world objects research partly supported chist-era project iglu project uid/eea//. thank konstantinos theoﬁlis software help permitting acquisition human hand coordinates human–robot interaction scenarios icub robot. saponaro salvi bernardino robot anticipation human intentions continuous gesture recognition international conference collaboration technologies systems ser. international workshop collaborative robots human–robot interaction jamone ugur cangelosi fadiga bernardino piater santos-victor affordances psychology neuroscience robotics survey ieee transactions cognitive developmental systems variation word occurrence probabilities figure {size=big shape=sphere} {objvel=fast}. variation corresponds difference word probability action evidence initial evidence object features effects. omitted words signiﬁcant variation observed. gesture hmms) also object features fig. shows computed predictions cases. fig. shows anticipated object velocity human user performs tapping action onto small spherical object whereas fig. displays target object box. indeed given observed action prior expected movement different depending physical properties target object. experiment compare associated verbal description obtained bayesian network absence action prior ones obtained presence one. particular compare probability word occurrence following situations fig. shows variation word occurrence probabilities cases omitted words signiﬁcant variation observed case. interpret difference predictions follows expected probabilities words related tapping pushing increase tapping action evidence gestures hmms introduced; conversely probabilities action words decreases; interestingly probability word rolling also increases tapping action evidence entered. even though initial evidence case already included effect information krüger geib piater petrick steedman wörgötter asfour kraft omrˇcen agostini dillmann object–action complexes grounded abstractions sensory–motor processes robotics autonomous systems vol. salvi montesano bernardino santos-victor language bootstrapping learning word meanings perception–action association ieee transactions systems cybernetics part cybernetics vol. stramandinoli tikhanoff pattacini nori grounding speech utterances robotics affordances embodied statistical language model ieee international conference developmental learning epigenetic robotics gonçalves saponaro jamone bernardino learning visual affordances objects tools autonomous robot exploration ieee international conference autonomous robot systems competitions gonçalves abrantes saponaro jamone bernardino learning intermediate object affordances towards development tool concept ieee international conference developmental learning epigenetic robotics antunes jamone saponaro bernardino ventura from human instructions robot actions formulation goals affordances probabilistic planning ieee international conference robotics automation", "year": 2017}