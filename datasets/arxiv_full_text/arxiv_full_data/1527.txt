{"title": "A Sequential Model for Multi-Class Classification", "tag": ["cs.AI", "cs.CL", "cs.LG", "I.2.6;I.2.67"], "abstract": "Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging.", "text": "many classiﬁcation problems require decisions among large number competing classes. tasks however handled well general purpose learning methods usually addressed ad-hoc fashion. suggest general approach sequential learning model utilizes classiﬁers sequentially restrict number competing classes maintaining high probability presence true outcome candidates set. theoretical computational properties model discussed argue important nlp-like domains. advantages model illustrated experiment partof-speech tagging. introduction large number important natural language inferences viewed problems resolving ambiguity either semantic syntactic based properties surrounding context. these turn viewed classiﬁcation problems goal select class label among collection candidates. examples include part-of speech tagging word-sense disambiguation accent restoration word choice selection machine translation context-sensitive spelling correction word selection speech recognition identifying discourse markers. machine learning methods become popular technique variety classiﬁcation problems sort shown signiﬁcant success. list consists bayesian classiﬁers decision lists bayesian hybrids hmms inductive logic methods memorymany classiﬁcation problems significant source difﬁculty fact number candidates large words words selection problems possible tags tagging problems etc. since general purpose learning algorithms handle multi-class classiﬁcation problems well studies address whole problem; rather small candidates ﬁrst selected classiﬁer trained choose among these. approach important allows research community develop better learning methods evaluate range applications important realize important stage missing. could signiﬁcant classiﬁcation methods embedded part higher level tasks machine translation information extraction small candidates classiﬁer handle ﬁxed could hard determine. work develop general approach study multi-class classiﬁers. suggest sequential learning model utilizes general purpose classiﬁers sequentially restrict number competing classes maintaining high probability presence true outcome candidate set. paradigm sought classiﬁer choose single class label among large labels. works sequentially applying simpler classiﬁers outputs probability distribution candidate labels. distributions multiplied thresholded resulting classiﬁer sequence needs deal smaller number candidate labels previous classiﬁer. classiﬁers sequence selected simple sense typically work part feature space decomposition feature space done achieve statistical independence. simple classiﬁer used since likely accurate; chosen that high probability sided error therefore presence true label candidate maintained. order sequence determined maximize rate decreasing size candidate labels set. beyond increased accuracy multi-class classiﬁcation problems scheme improves computation time problems several orders magnitude relative standard schemes. work describe approach discuss experiment done context part-of-speech tagging provide theoretical justiﬁcations approach. sec. provides background approaches multi-class classiﬁcation machine learning nlp. sec. describe sequential model proposed sec. describe experiment exhibits advantages. theoretical justiﬁcations outlined sec. several works within machine learning community attempted develop general approaches multi-class classiﬁcation. promising approaches error correcting output codes however approach able handle well large number classes large scale applications therefore questionable. statistician studied several schemes learning single classiﬁer class labels learning discriminator pair class labels discussed relative merits. although argued latter provide better results others experimental results mixed cases involved schemes e.g. learning classiﬁer three class labels shown perform better moreover none methods seem computationally plausible large scale problems since number classiﬁers needs train least quadratic number class labels. within several learning works already addressed problem multi-class classiﬁcation. methods pairs used learn phrase annotations shallow parsing. different classiﬁers used task making infeasible general solution. cases know taken account properties domain fact several works viewed instantiations sequential model formalize here albeit done ad-hoc fashion. speech recognition sequential model used process speech signal. abstracting away details ﬁrst classiﬁer used speech signal analyzer; assigns positive probability words somewhat sophisticated techniques words assigned probabilities using different contextual classiﬁer e.g. language model then additional sentence level classiﬁer uses outcome word classiﬁers word lattice choose likely sentence. several word prediction tasks make decisions sequential well. spell correction confusion sets created using classiﬁer takes input word transcription outputs positive probability potential words. conventional spellers output classiﬁer given user selects intended word. context sensitive spelling correction additional classiﬁer utilized predict among words supported ﬁrst classiﬁer using contextual lexical information surrounding words. studies done however ﬁrst classiﬁer confusion sets constructed manually researchers. word predictions tasks also constructed manually list confusion sets justiﬁcations given reasonable construct present similar task confusion sets generation automated. study also quantiﬁed experimentally advantage using early classiﬁers restrict size confusion set. sequential model study problem learning multi-class classiﬁer typically large order address problem using sequential model simpler classiﬁers sequentially used ﬁlter subsets consideration. efﬁcient make decisions cases several different constrains play role advantageous additive models. fact thresholding step model viewed selective poe. thresholding ensures following monotonicity property evaluate classiﬁers sequentially smaller equal confusion sets considered. desirable design goal that w.h.p. classiﬁers sided error true target would like rest paper presents concrete instantiation provides theoretical analysis properties work address question acquiring i.e. learning {ǫi} example tagging section describes part experiment tagging compare identical conditions classiﬁcation models single classiﬁer. provided input features difference model structure. ﬁrst part comparison done context assigning tags unknown words words presented training therefore learner baseline knowledge possible take. experiment emphasizes advantage using evaluation terms accuracy. second part done context tagging known words. compares processing time well accuracy assigning tags known words part exhibits large reduction training time using common one-vs-all method accuracy methods almost identical. types features lexical features contextual features used learning words pos. contextual features capture information surrounding context word lemma lexical features capture morphology unknown word. several issues make tagging problem natural problem study within relatively large number classes natural decomposition feature space contextual lexical features. lexical knowledge word lemma provide w.h.p sided error tagger classiﬁers domain experiment deﬁned using following features computed relative target word contextual features tags word preceding target word respectively. figure tagging unknown word using contextual lexical features sequential model. input capitalized classiﬁer values therefore ways create confusion sets. different inputs sufﬁx classiﬁer therefore sufﬁx emit confusion sets. classiﬁers sequential model well single classiﬁer snow learning architecture winnow update rule. snow multi-class classiﬁer speciﬁcally tailored learning domains potential number features taking part decisions large decisions actually depend small number features. snow works learning sparse network linear functions pre-deﬁned incrementally learned feature space. snow already used successfully several tasks natural language processing functions case target nodes used reside feature space thought autonomous functions given example treated autonomously target subnetwork; example labeled considered positive example function learned negative example rest functions network sparse target node need connected nodes input layer. example connected input nodes never active sentence. although snow used different targets utilizes determining confusion dynamically. evaluation maximum taken currently applicable confusion set. moreover training given example used train target networks currently applicable confusion set. example positive target viewed positive target negative targets confusion set. targets example. case tagging known words handled similar way. case possible tags known. training record word tags tagged training corpus. evaluation whenever word occurs tagged tags. evaluation confusion consists tags observed target word training maximum taken these. always case using single classiﬁer. training though sake experiment treat differently depending whether trained single classiﬁer. trained single classiﬁer uses t-tagged example positive example negative example tags. hand classiﬁer trained t-tagged example word using positive example negative example effective confusion set. tags observed tags training corpus. experimental results data experiments extracted penn treebank brown corpora. training corpus consists words. test corpus consists words unknown words included among unknown words). tagging unknown words table summarizes results experiments single classiﬁer uses contextual features. notice adding baseline signiﬁcantly improves results much gained baseline. reason baseline feature almost perfect training data. reason next experiments baseline since could hide phenomenon addressed. table tagging unknown words using contextual lexical features based contextual features based contextual lexical features. denotes follows sequential model. table summarizes results main experiment part. exhibits advantage using single classiﬁer makes features cases features used. classiﬁer trained input consists features chooses label among class labels. features used input different classiﬁers used sequentially using part feature space restricting possible outcomes available next classiﬁer sequence chooses among left candidates. interesting note improvement achieved shown right column. given last stage identical single classiﬁer shows contribution ﬁltering done ﬁrst stages using addition result shows input spaces classiﬁers need disjoint. tagging known words essentially everyone learning tagger known words makes sequential model assumption evaluation restricting candidates discussed focus experiment thus investigate advantage training. case single classiﬁer trains tags classiﬁer trains effective confusion table compares performance classiﬁer trained using one-vs-all method classiﬁer trained way. results known words results brill’s tagger presented comparison. table tagging known words using contextual features one-vs-all denotes training example serves positive example true negative example tags. smtrain denotes training example serves positive example true negative example restricted tags based previous classiﬁer here simple baseline restriction. while principle better one-vs-all classiﬁer believe case performance advantages since classiﬁers work high dimensional feature space allows one-vs-all classiﬁer separating hyperplane separates positive examples many different kinds negative examples however advantage case signiﬁcant decrease computation time training evaluation. table shows tagging task training using times faster one-vs-all method faster brill’s learner. addition evaluation table processing time tagging known words using contextual features train training time sentences. brill’s learner interrupted days training test average number seconds evaluate single sentence. runs done machine. expressivity viewed generate expressive classiﬁer building number simpler ones. argue generating expressive classiﬁer advantages ways decision tree. addition several signiﬁcant computational advantages training test since needs consider subset candidate class labels. discuss issues detail here. addition decomposing domain significant advantages learning theory point view learning domains lower dimensionality implies better generalization bounds equivalently accurate classiﬁers ﬁxed size training set. decomposing range attempts reduce size candidates set. justify considering cases test argue prediction among smaller classes advantages predicting among large classes; training argue advantageous ignore irrelevant examples. decomposing range test following discussion formalizes intuition smaller confusion preferred. true target function probability assigned ﬁnal classiﬁer class given example assuming prediction done naturally choosing likely class label expected error using confusion size decomposing domain decomposing domain essential part possible classiﬁers used actually domain. shown below though decomposition possible advantageous shown possible decompose domain subsets conditionally independent given class label classiﬁers deﬁned subsets accurate optimal single classiﬁer. note although conditional independence assumption strong reasonable assumption many applications; particular cross modality information used assumption typically holds decomposition done across modalities. example tagging lexical information often conditionally independent contextual information given true expressivity decision process conceptually similar decision tree processes especially allows general classiﬁers decision tree nodes. section show express compact decision tree even makes used expressive internal nodes next theorem shows ﬁxed functions input features binary decision tree represented extending proof beyond binary decision trees straight-forward. claim shows reducing size confusion help; holds assumption true class label eliminated consideration stream classiﬁers one-sided error assumption. moreover easy proof claim allows relax sided error assumption assume instead previous classiﬁers probability smaller than decomposing range training assume suggested previous discussion evaluation stage smallest possible candidates considered classiﬁer. based assumption claim shows training advantageous. utilizing training yields better classiﬁer. claim class labels examples class assume sequential model class compete class whenever ﬁlters ﬁnal classiﬁer considers then error hypothesis produced algorithm trained examples larger error produced hypothesis produces trained examples note given also relatively easy construct decision tree produces decisions ﬁnal classiﬁer however simple construction results decision tree exponentially larger original theorem shows difference expressivity inherent. theorem number classiﬁers sequential model number internal nodes decision tree classes output also maximum degree internal nodes denote number functions representable respectively. then exponentially larger proof proof follows counting number functions represented using decision tree internal nodes number functions represented using sequential model intermediate classiﬁer. given exponential follows need exponentially large decision trees represent equivalent predictor size conclusion wide range large number classiﬁcation tasks used order perform high level natural language inference speech recognition machine translation question answering. although instantiation real conﬂict could choose among small candidates original candidates could large; deriving small candidates relevant task hand immediate. sequentially restricts candidate classes small driven data observed. described method provided justiﬁcations advantages especially nlp-like domains. preliminary experiments also show promise. several issues still missing work. experimental study decomposition feature space done manually; would nice develop methods automatically. better understanding methods thresholding probability distributions classiﬁers output well principled ways order also among future directions research.", "year": 2001}