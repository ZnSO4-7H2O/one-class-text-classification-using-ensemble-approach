{"title": "OptNet: Differentiable Optimization as a Layer in Neural Networks", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.", "text": "reduce overall depth network preserving richness representation. speciﬁcally build framework output layer network solution constrained optimization problem based upon previous layers. framework naturally encompasses wide variety inference problems expressed within neural network allowing potential much richer end-to-end training complex tasks require inference procedures. concretely paper speciﬁcally consider task solving small quadratic programs individual layers. optimization problems well-suited capturing interesting behavior efﬁciently solved gpus. speciﬁcally consider layers form optimization variable parameters optimization problem. notation suggests parameters depend differentiable previous layer eventually optimized like weights neural network. layers learned taking gradients loss function respect parameters. paper derive gradients taking matrix differentials conditions optimization problem solution. order make approach practical larger networks develop custom solver simultaneously solve multiple small batch form. developing custom primal-dual interior point method tailored speciﬁcally dense batch operations gpu. total solver solve batches quadratic programs times faster existing highly tuned quadratic programming solvers gurobi cplex. crucial algorithmic insight solver using speciﬁc factorization primal-dual interior point update obtain backward pass optimization layer virtually free together innovations enable parameterized optimization problems inserted within architecpaper presents optnet network architecture integrates optimization problems individual layers larger end-to-end trainable deep networks. layers encode constraints complex dependencies hidden states traditional convolutional fully-connected layers often cannot capture. paper explore foundations architecture show techniques sensitivity analysis bilevel optimization implicit differentiation used exactly differentiate layers respect layer parameters; develop highly efﬁcient solver layers exploits fast gpubased batch solves within primal-dual interior point method provides backpropagation gradients virtually additional cost solve; highlight application approaches several problems. notable example show method capable learning play mini-sudoku given input output games priori information rules game; highlights ability architecture learn hard constraints better neural architectures. paper consider treat exact constrained optimization individual layer within deep learning architecture. unlike traditional feedforward networks output layer relatively simple function previous layer optimization framework allows individual layers capture much richer behavior expressing complex operations toschool computer science carnegie mellon university. pittsburgh usa. correspondence brandon amos <bamoscs.cmu.edu> zico kolter <zkoltercs.cmu.edu>. begin highlighting background related work present optimization layer itself. using matrix differentials derive rules computing necessary backpropagation updates. detail speciﬁc solver quadratic programs based upon state-ofthe-art primal-dual interior point method highlight novel elements apply formulation aforementioned fact compute backpropagation little additional cost. provide experimental results demonstrate capabilities architecture highlighting potential tasks architectures solve illustrating improvements upon existing approaches. optimization plays role modeling complex phenomena providing concrete decision making processes sophisticated environments. full treatment optimization applications beyond scope methods bound applicability control frameworks numerous statistical mathematical formalisms physical simulation problems like rigid body dynamics generally speaking work step towards learning optimization problems behind real-world processes data learned end-to-end rather requiring human speciﬁcation intervention. machine learning setting wide array applications consider optimization means perform inference learning. among many applications architectures well-studied generic classiﬁcation structured prediction tasks vision tasks denoising metz uses unrolled optimization within network stabilize convergence generative adversarial networks indeed general idea solving restricted classes optimization problem using neural networks goes back many decades seen number advances recent years. models often trained following four methods. energy-based learning methods methods used tasks like prediction training method shapes energy function around observed data manifold high elsewhere recent years strong push further incorporate structured prediction methods like conditional random ﬁelds last layer deep network architecture well deeper energy-based architectures learning context requires observed data isn’t present contexts consider paper also suffer instability issues combined deep energy-based architectures observed belanger mccallum belanger amos analytically analytic solution argmin found unconstrained quadratic minimization gradients often also computed analytically. done tappen schmidt roth cannot methods constrained optimization problems consider paper known analytic solutions. unrolling argmin operation unconstrained objective approximated ﬁrst-order gradientbased method unrolled. architectures typically introduce optimization procedure gradient descent inference procedure. done domke amos belanger metz goodfellow stoyanov brakel optimization procedure unrolled automatically manually obtain derivatives training incorporate effects in-the-loop optimization procedures. however unrolling computation method like gradient descent typically requires substantially larger network adds substantially computational complexity network. existing cases optimization problem unconstrained unrolling gradient descent often easy constraints added optimization problem iterative algorithms often projection operator difﬁcult unroll through. paper unroll optimization procedure instead argmin differentiation described next section. argmin differentiation closely related work several papers propose form differentiation argmin operators. techniques also come bilevel optimization sensitivity analysis case gould authors describe general techniques differentiation optimization problems describe case exact equality constraints rather equality inequality constraints amos network setting optimal solution optimization problems becomes output layer denoted problem data depend value previous layer forward pass optnet architecture thus involves simply setting ﬁnding solution optimization problem. training deep architectures however requires forward pass network also backward pass. requires compute derivative solution respect input parameters general topic topic discussed previously. obtain derivatives differentiate conditions solution problem using techniques matrix differential calculus analysis extended general convex optimization problems. dual variables equality constraints dual variables inequality constraints. conditions stationarity primal feasibility complementary slackness considers argmin differentiation within context speciﬁc optimization problem consider general setting. johnson performs implicit differentiation convex objectives coordinate subspace constraints don’t consider inequality constraints don’t consider detail general linear equality constraints. optimization problem ﬁnal layer variational inference network propose insert optimization problems anywhere network. therefore special case optnet layers natural interpretation terms gaussian inference gaussian graphical models provide tools making computation efﬁcient interpreting constraining structure. similarly older work mairal considered argmin differentiation lasso problem deriving speciﬁc rules case presenting efﬁcient algorithm based upon ability solve lasso problem efﬁciently. paper implicit differentiation techniques matrix differential calculus derive gradients matrix problem interested notable different work within aware analytically differentiate inequality well equality constraints differentiating complementarity conditions; differs e.g. gould instead approximately convert problem unconstrained barrier method. also developed methods make approach practical reasonably scalable within context deep architectures. optimization variable rn×n rm×n rp×n problem data leaving dependence previous layer showed notational convenience. wellknown problems solved polynomial time using variety methods; desires exact solutions problems primal-dual interior point methods later section current state solution methods. neufollowing method mattingley boyd solver introduces slack variables inequality constraints iteratively minimizes residuals conditions primal variable slack variable dual variables associated equality constraints associated inequality constraints. iteration computes afﬁne scaling directions solving duality deﬁned mattingley boyd variable updated ∆vaﬀ ∆vcc using appropriate step size. actually solve symmetrized version conditions obtained scaling second block analytically decompose systems smaller symmetric systems pre-factorize portions don’t change iterations). implemented batched version method pytorch library released open source library https//github.com/locuslab/ qpth. uses custom cublas extension provides interface solve multiple matrix factorizations solves parallel provides necessary backpropagation gradients end-to-end learning system. point particular form primal-dual interior point method employ possible compute backward pass gradients free solving original without additional matrix factorization solve. speciﬁcally iteration primal-dual interior point computing decomposition backpropagation algorithm however never want explicitly form actual jacobian matrices rather want form left matrix-vector product previous backward pass vector efﬁciently noting solution involves multiplying inverse lefthand-side matrix right hand side. thus multiply backward pass vector transpose differential matrix standard backpropagation terms size parameter matrices. note parameters depend previous layer gradients respect previous layer obtained chain rule. next section solution interior point method fact already provides factorization compute gradient efﬁciently. deep networks typically trained mini-batches take advantage efﬁcient data-parallel operations. withmini-batching many modern deep learning architectures become intractable practical purposes. however today’s state-of-the-art solvers like gurobi cplex capability solving multiple optimization problems parallel across entire minibatch. makes larger optnet layers become quickly intractable compared fully-connected layer number parameters. overcome performance bottleneck quadratic program layers implemented gpu-based primal-dual interior point method based mattingley boyd solves batch quadratic programs provides necessary gradients needed train end-to-end fashion. performance experiments section shows solver signifmate arbitrary elementwise piecewise-linear functions among things represent relu layer. theorem elementwise piecewise linear function linear regions. function represented optnet layer using parameters. additionally layer max{w rn×m represented optnet layer parameters. finally show converse hold function representable optnet layer cannot represented exactly two-layer relu layer take exponentially many units approximate simple example layer three linear functions max{at theorem scalar-valued function speciﬁed optnet layer parameters. conz output two-layer relu network. exist functions relu network cannot represent exactly require parameters approximate ﬁnite region. although show shortly optnet layer several strong points also want highlight potential drawbacks approach. first although efﬁcient batch solver integrating optnet layer existing deep learning architectures potentially practical note solving optimization problems exactly cubic complexity number variables and/or constraints. contrasts quadratic complexity standard feedforward layers. means ultimately limited settings number hidden variables optnet layer large secondly many improvements optnet layers still possible. solver instance uses fully dense matrix operations makes solves efﬁcient solutions also makes sense general setting coefﬁcients quadratic problem learned. however setting many realworld optimization problems often substantial structure data matrices exploited efﬁciency. course prohibition incorporating sparse matrix methods fast custom solver would require substantial added complexity especially regarding efforts like ﬁnding minimum orderings matrix ksym. matrix essentially symmetrized version matrix needed computing backpropagated gradients similarly compute dzλν terms solving linear system d)dλ deﬁned thus backward pass gradients computed using factored matrix solution. crucially bottleneck solving linear system computing factorization matrix additional time requirements computing necessary gradients backward pass virtually nonexistent compared time computing solution. best knowledge ﬁrst time fact exploited context learning end-to-end systems. section brieﬂy highlight mathematical properties optnet layers. proofs straightforward mostly based upon well-known results convex analysis deferred appendix. ﬁrst result simply highlights optnet layers subdifferentiable everywhere differentiable measure-zero points. theorem output optnet layer assuming full rank subdifferentiable everywhere denotes clarke generalized subdifferential single unique element measure zero points next results show representational power optnet layer speciﬁcally optnet layer compares common linear layer followed relu activation. ﬁrst theorem shows optnet layer approxiwe actually perform decomposition certain subset matrix formed eliminating variables create matrix needs factor iteration primal-dual algorithm matrix start primal-dual algorithm though omit detail here. also decomposition routine provided batch form cublas could potentially cholesky factorization appropriate functionality added cublas). lastly note optnet layers trained neural network layer since creation since manifolds parameter space effect resulting solution admittedly tuning required work. situation common developing neural network architectures also reported similar architecture schmidt roth hope techniques overcoming challenges learning layers continue developed future work. section present several experimental results highlight capabilities optnet layer. specifically look computational efﬁciency exiting solvers; ability improve upon existing convex problems used signal denoising; integrating architecture generic deep learning architectures; performance approach problem challenging current approaches. particular want emphasize results system learning game mini-sudoku well-known logical puzzle; layer able directly learn necessary constraints using gradient information priori knowledge rules sudoku. code data experiments open sourced icml branch https//github.com/locuslab/optnet batched solver available library https //github.com/locuslab/qpth. optnet layers much computationally expensive linear convolutional layer natural question performance difference experiment comparing linear layer optnet layer mini-batch size cuda randomly generated input vectors sized layer maps input output dimension; linear layer batched matrix-vector multiplication optnet layer taking argmin random number inequality constraints dimensionality problem. figure shows proﬁling results variables inequality constraints gurobi serialized batched versions solver qpth vary batch size. figure shows means standard deviations running trial times showing batched solver outperforms gurobi highly tuned solver reasonable batch sizes. minibatch size solve problems average seconds whereas gurobi tasks average seconds. context training deep architecture type speed difference single minibatch make difference practical completely unusable solution. experimental details sample entries matrix random uniform distribution sample random normal entries selecting generating random normal random uniform setting choice guarantees problem feasible. tialize. accuracy substantially lower even fully connected case largely result learning over-regularized solution indeed point addressed future work point want highlight optnet layer seems learning something interpretable understandable. speciﬁcally figure shows matrix solution learning interesting picture learned matrix typically captures exactly intuition matrix used total variation denoising mainly sparse matrix entries alternating sign next other. implies data have total variation denoising indeed right think denoising resulting signal noise process generate data learn process instead. attain lower actual error method ﬁxing learned sparsity matrix tuning. ﬁnally highlight ability optnet methods improve upon results convex program speciﬁcally tailoring data. here optnet architecture previous subsection initialize differencing matrix total variation solution. shown table procedure able improve training testing solution speciﬁcally improving upon test section shows convergence ﬁne-tuning. currently convex optimization basis. speciﬁcally goal case denoise noisy signal given training data consistency noisy clean signals generated distribution. problems often addressed convex optimization procedures total variation denoising particularly common simple approach. speciﬁcally total variation denoising approach attempts smooth noisy observed signal solving optimization problem ﬁrst-order differencing operation expressed matrix form matrix rows penalizing norm signal difference encourages difference sparse i.e. number changepoints signal small approximating piecewise constant function. test approach competing ones denoising task generate piecewise constant signals corrupt independent gaussian noise table shows error rate four approaches. establish baseline denoising performance total variation optimization problem varying values procedure performs best choice achieves minimum test task alternative approach denoising learning data. function parameterized used predict original signal. optimal learned using mean squared error true predicted signals. denoising typically difﬁcult function learn table shows fully-connected neural network perform substantially worse denoising task convex optimization problem. section shows convergence fully-connected network. feedforward neural network approach convex total variation optimization could instead generic optnet layers effectively allowed solve using denoising matrix randomly inione compelling case optnet layer learn constraints dependencies output latent space model. simple example illustrate optnet layers included existing architectures gradients efﬁciently propagated layer show performance fully-connected feedforward network without optnet layer section supplemental material. finally present main illustrative example representational power approach task learning game sudoku. sudoku popular logical puzzle grid points must arranged given initial point column grid points must contain number consider simpler case sudoku puzzles numbers shown figure sudoku fundamentally constraint satisfaction problem trivial computers solve told rules game. however know rules game presented examples unsolved corresponding solved puzzle challenging task. consider interesting benchmark task algorithms seek capture complex strict relationships input output variables. input algorithm consists grid desired output tensor one-hot encoding solution. problem traditional neural networks difﬁculties learning necessary hard constraints. baseline inspired models https//github. com/kyubyong/sudoku implemented multilayer feedforward network attempt solve sudoku problems. speciﬁcally report results network convolutional layers ﬁlters each tried architectures well. optnet layer task completely generic standard form positivity inequality constraints arbitrary constraint matrix small make sure problem strictly feasible linear term simply input one-hot encoding sudoku problem. know sudoku approximated well linear program minimize dataset created consisting training puzzles tested models different heldpuzzles. error rate percentage puzzles solved correctly cells assigned whichever index largest prediction. figure shows convolutional able learn necessary logic task ends over-ﬁtting training data. contrast performance optnet network learns correct hard constraints within ﬁrst three epochs able generalize much better unseen examples. presented optnet neural network architecture optimization problems single layer network. derived algorithmic formulation differentiating layers allowing backpropagating end-to-end architectures. also developed efﬁcient batch solver optimizations based upon primal-dual interior point method developed method attaining necessary gradient information free approach. experiments highlight potential power networks showing solve problems existing networks poorly suited learning sudoku problems purely data. many future directions research approaches feel another important primitive toolbox neural network practitioners. supported national science foundation graduate research fellowship program grant dge. would like thank developers pytorch helping core features particularly soumith chintala adam paszke. also thank goodfellow lekan ogunmolu silva po-wei wang eric wong invaluable comments well rocky duan helped improve feedforward network baseline mini-sudoku. belanger david yang bishan mccallum andrew. end-to-end learning structured prediction energy networks. proceedings international conference machine learning chen liang-chieh schwing alexander yuille alan urtasun raquel. learning deep structured models. proceedings international conference machine learning duchi john shalev-shwartz shai singer yoram chandra tushar. efﬁcient projections onto -ball learning high dimensions. proceedings international conference machine learning goodfellow mirza mehdi courville aaron bengio yoshua. multi-prediction deep boltzmann machines. advances neural information processing systems goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems gould stephen fernando basura cherian anoop anderson peter santa cruz rodrigo edison. differentiating parameterized argmin argmax problems application bi-level optimization. arxiv preprint arxiv. johnson matthew duvenaud david wiltschko alex adams ryan datta sandeep composing graphical models neural networks structured representations fast inference. advances neural information processing systems lillo walter heng stefen stanislaw solving constrained optimization problems neural networks penalty method approach. ieee transactions neural networks l¨otstedt per. numerical simulation time-dependent contact friction problems rigid body mechanics. siam journal scientiﬁc statistical computing stoyanov veselin ropson alexander eisner jason. empirical risk minimization graphical model parameters given approximate inference decoding model structure. aistats tappen marshall adelson edward freeman william learning gaussian conditional random ﬁelds low-level vision. computer vision pattern recognition cvpr’. ieee conference ieee zheng shuai jayasumana sadeep romera-paredes bernardino vineet vibhav zhizhong dalong huang chang torr philip conditional random proceedings ﬁelds recurrent neural networks. ieee international conference computer vision section consider integration optnet layers traditional fully connected network mnist problem. results show marginal improvement fully connected layer main point comparison simply illustrate include layers within existing network architectures efﬁciently propagate gradients layer. speciﬁcally fc-fc-fc-softmax fully connected network compare fc-fcoptnet-softmax network numbers layer indicate layer size. optnet layer case includes inequality constraints previous layer used linear objective term keep cholesky factorization directly learn also directly learn ensure feasible solution always exists select learnable figure shows results similar networks slightly lower error less variance optnet network. section contains proofs results highlight section mentioned before proofs quite straightforward follow well-known properties include completeness. proof. fact optnet layer subdifferentiable strictly convex follows directly well-known result solution strictly convex continuous proof essentially boils showing fact clearly objective minimized small possible meaning must either bound optimal solution objective function. obtain multivariate elementwise function simply apply function coordinate input proof. ﬁnal theorem simply states two-layer relu network often require exponentially many units approximate function speciﬁed optnet layer. consider single-output relu network much like previous section deﬁned multi-variate inputs. zero) still exists solution system right hand side always range also zero rows. case longer unique solution corresponding subdifferentiable differentiable case. though explicitly showing unique solution jacobian equations presented earlier except measure zero set. measure zero consists degenerate solutions points inequality constraints hold equality also zero-valued dual variables. simplicity assume full rank relaxed. non-singular gives standard system nonsingular invertible full column rank must hold condition fact must less total tight constraints solution. also note d−h)ii term non-zero entire second block matrix. thus want solve system seem possible represent closed form simple network closed form solution projection operator requires sorting ﬁnding particular median term data feasible single layer form network aware simplicity stated theorem using relu networks straightforward example works even dimensions. relu capable representing function exactly even note sum-of-max function nature term max{at stated must creases span entire input space; contrast terms creases partially span space. illustrated figure apparent therefore two-layer relu cannot exactly approximate three maximum term function captured simple optnet layer fact relu network universal function approximator means able approximate three-max term means require dense covering points input space choose equal number relu terms choose coefﬁcients approximate underlying function points; however large enough radius require exponential size covering approximate underlying function arbitrarily closely. although example proof quite simple number functions unable compact representation. example projection point simplex easily written optnet layer", "year": 2017}