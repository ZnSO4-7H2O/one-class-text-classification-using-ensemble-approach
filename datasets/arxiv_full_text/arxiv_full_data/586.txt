{"title": "Backward and Forward Language Modeling for Constrained Sentence  Generation", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN's hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a specific word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality.", "text": "recent language models especially based recurrent neural networks make possible generate natural language learned probability. language generation wide applications including machine translation summarization question answering conversation systems etc. existing methods typically learn joint probability words conditioned additional information rnn’s hidden layer. many applications likely impose hard constraints generated texts i.e. particular word must appear sentence. unfortunately existing approaches could solve problem. paper propose novel backward forward language model. provided speciﬁc word rnns generate previous words future words either simultaneously asynchronously resulting model variants. given word could appear position sentence. experimental results show generated texts comparable sequential quality. language modeling aimed minimizing joint probability corpus. long core natural language processing inspired variety models e.g. n-gram model smoothing techniques well various neural networks particular renewed prosperity neural models made groundbreaking improvement many tasks including language modeling part-of-speech tagging named entity recognition semantic role labeling etc. recurrent neural network prevailing class language models; suitable modeling time-series data iterative nature. usually keeps hidden layers; time slot reads word changes state accordingly. compared traditional n-gram models rnns capable learning long range features—especially long short term memory units gated recurrent units —and hence better capturing nature sentences. basis even possible generate sentence language model wide applications including machine translation abstractive summarization question answering conversation systems sentence generation process many scenarios however likely impose constraints generated sentences. example question answering system involve analyzing question querying existing knowledge base point which candidate answer hand. natural language generator supposed generate sentence coherent semantics containing candidate answer. unfortunately using existing language models generate sentence given word non-trivial adding additional information word guarantee wanted word appear; generic probabilistic samplers hardly applies language models; setting arbitrary word wanted word damages ﬂuency sentence; imposing constraint ﬁrst word restricts form generated sentences. paper propose novel backward forward language model tackle problem constrained natural language generation. rather generate sentence ﬁrst word last sequence traditional models rnns generate previous subsequent words conditioned given word. forward backward generation accomplished either simultaneously asynchronously resulting variants syn-b/f asyn-b/f. model complete theory generating sentence wanted word appear arbitrary position sentence. rest paper organized follows. section reviews existing language models natural language generators. section describes proposed language models detail. section presents experimental results. finally conclusion section bengio propose feed-forward neural networks estimate probability equation model word ﬁrst mapped small dimensional vector known embedding; feed-forward neural network propagates information softmax output layer estimates probability next word. keeps hidden state vector dependent previous state current input vector word embedding current word. output layer estimates probability word occurs time slot. following listed formulas vanilla rnn. indicated equations provides means direct parametrization equation hence ability capture long term dependency compared ngram models. practice vanilla diﬃcult train gradient vanishing exploding problem; long short term units gated recurrent units proposed better balance previous state current input. using rnns model joint probability language makes feasible generate sentences. early attempt generates texts character-level language model recently rnn-based language generation made breakthroughs several real applications. sequence sequence machine translation model uses encode source sentence ﬁxed-size vectors; another decodes vector target sentence. network viewed language model conditioned source sentence. time step predicts likely word output; embedding word input layer next step. process continues generates special symbol <eos> indicating sequence. beam search sampling methods used improve quality diversity generated texts. source sentence long ﬁxed-size vectors attention mechanism used dynamically focus diﬀerent parts source sentence during target generation. studies generate sentence based abstract representations semantics; feed one-hot vector additional information rnn’s hidden layer question answering system leverage soft logistic switcher either generate word vocabulary copy candidate answer parametrize equation propose model variants. ﬁrst approach generate previous backward models simultaneously call syn-b/f language model concretely equation takes form factor refers conditional probability current time step generates ws−t ws+t forward backward sequences respectively given middle part sentence ws−t+ ws+t−. part generated <eos> special syn-b/f model design single generate chains hope aware other’s state. besides also propose asynchronous version denoted asyn-b/f idea ﬁrst generate backward sequence feed obtained result another forward generate future words. detailed formulas repeated. important notice asyn-b/f’s backward sequence generation different generic backward latter presumed model sentence last word ﬁrst whereas backward fact half starting training criteria. assume always given training criterion shall crossentropy loss words chains except alternatively penalize split word addition make possible generate entire sentence without provided. deem criteria diﬀer signiﬁcantly adopt latter experiments. labeled unlabeled datasets suﬃce train language model. sentence annotated specially interesting word natural split word. unlabeled dataset randomly choose word notice equation gives joint probability sentence particular split word compute probability sentence shall marginalize diﬀerent split words i.e. evaluate prefer vertical domain corpus interesting application nuggets instead using generic texts like wikipedia. particular chose build language model upon scientiﬁc paper titles arxiv. building language model paper titles help researchers preparing drafts. provided topic constrained natural language generation could also acts brainstorming. grouped single token <unk> removed non-english titles three <unk>’s. notice <unk>’s appear frequently large number refer acronyms thus mostly consistent semantics. currently samples training validation another testing; vocabulary size asyn-b/f hidden layer units; synb/f makes fair comparison syn-b/f simultaneously learn implicit forward backward completely diﬀerent. models embeddings dimensional initialized randomly. train model used standard backpropagation element-wise gradient clipping. following applied rmsprop optimization suitable training rnns na¨ıve stochastic gradient descent less sensitive hyperparameters compared momentum randomly choosing split word increases uncertainly. also noticed that model perplexity reﬂects probability sentence speciﬁc split word whereas perplexity sequential assesses probability sentence itself. asyn-b/f deep tional networks segmentation object tracking visual recognition optimal control systems type approach method based line counting titles scientiﬁc papers example oftentimes follow templates begin <unk> approach <unk> based approach. therefore sequential yields perplexity generating word particular position information smoothed split word chosen randomly. long term behavior similar sequential rule impact choosing random words. syn-b/f particular result indicates feeding words’ embeddings hidden layer confusion. table reduces perplexity less showing well make information word appear generated text. further syn-b/f better na¨ıve sep-b/f; asyn-b/f capable integrating information backward forward sequences. generate paper titles learned language model speciﬁc word given thought application particular interest research topics. table illustrates examples generated models baselines. words common beginning paper title—like adjective convolutional gerund tracking—sequential generate reasonable results. plural nouns like systems models titles generated sequential somewhat inﬂuent basically comply grammar rules. words unlikely initial word sequential fails generate grammatically correct sentences. contrast ability generate correct sentences. sep-b/f model greedy chain. generating short general texts known issue neural network-based sep-b/f hardly generate sentence containing much substance. syn-b/f better asyn-b/f able generate sentences whose quality comparable sequential lms. paper proposed backward forward language model constrained natural language generation. given particular word model generate previous words future words either synchronously asynchronously. experiments show similar perplexity sequential disregard perplexity introduced random splitting. case study demonstrates asynchronous generate sentences contain given word comparable sequential quality.", "year": 2015}