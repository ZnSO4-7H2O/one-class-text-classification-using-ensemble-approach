{"title": "Event Representations for Automated Story Generation with Deep Neural  Nets", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.", "text": "recurrent encoder-decoder neural networks open story generation. encoder-decoder trained predict next token sequence given input tokens. network architecture weights comprise generative model capturing generalizing patters observed training data. open story generation must train network dataset encompasses many story topics possible. work corpus movie plot summaries extracted wikipedia premise movies plots wikipedia covers range topics people want tell stories about. narratological terms event unit story featuring world state change textual story corpora including wikipedia movie plot corpora comprised unstructured textual sentences. beneﬁt dealing movie plots clarity events occur although often expense creative language. even characterword-level analysis sentences would fail capture interplay words make meaning behind sentence. characterwordlevel recurrent neural networks learn create grammatically correct sentences often fail produce coherent narratives beyond couple sentences. hand sentence-level events would unique real relationship them. even large corpus stories would likely sequences sentences would ever seen once. example ranch-hand frank osorio travels patagonia buenos aires bring news daughter’s demise granddaughter alina. occurs corpus ever seen example likely occur event sparsity likely poor predictive ability. order help maintain coherent story provide event representation expressive enough preserve semantic meaning sentences story corpus also reducing sparsity events paper developed event representation aids process automated open story generation. insight automated story generation problem automatically selecting sequence events actions words told story. seek develop system generate stories learning everything needs know textual story corpora. date recurrent neural networks learn language models character word sentence levels little success generating coherent stories. explore question event representations provide midlevel abstraction words sentences order retain semantic information original data minimizing event sparsity. present technique preprocessing textual story data event sequences. present technique automated story generation whereby decompose problem generation successive events generation natural language sentences events give empirical results comparing different event representations effects event successor generation translation events natural language. automated story generation problem automatically selecting sequence events actions words told story. date story generation systems used symbolic planning case-based reasoning automated story generation systems able produce impressive results rely human-knowledge engineer provide symbolic domain models indicated legal characters actions knowledge character actions cannot performed; systems limited telling stories topics covered domain knowledge. consequently difﬁcult determine whether quality stories produced systems result algorithm good knowledge engineering. open story generation problem automatically generating story topic withpriori manual domain knowledge engineering. open story generation requires intelligent system either learn domain model available data reuse data knowledge available corpus extract basic semantic information sentences preexisting stories learn skeletons good stories supposed like. then using templates system able generate novel sequences events would resemble decent story. ﬁrst contribution paper thus event representation proposed recurrent encoder-decoder neural network story generation called eventevent. evaluate event representation naive baseline sentence representation number alternative representations. eventevent textual story corpus preprocessed— sentences translated event representation extracting core semantic information sentence. event preprocessing linear-time algorithm using number natural language processing techniques. processed text used train neural network. however event preprocessing lossy process resultant events human-readable. address this present story generation pipeline second neural network eventsentence translates abstract events back natural language sentences. eventsentence network encoder-decoder network trained missing details necessary abstract events human-readable. second contribution overall story generation pipeline subsequent events story generated eventevent network translated natural language using eventsentence network. present evaluation eventsentence different event representations draw conclusions effect representations ability produce readable stories. remainder paper organized follows. first discuss related work automated story generation followed introduction event representation. introduce eventevent network provide evaluation event representation context story generation. show event representation used eventsentence generate human-readable sentences events. discussion future work conclusions experiments event representation event-to-sentence model ﬁnal system. reasoning identify relevant existing story content online blogs. scheherazade system uses crowdsourced corpus example stories learn domain model generate novel stories. recurrent neural networks theoretically learn predict probability next character word sentence story. roemmele gordon long short-term memory network generate stories. skip-thought vectors encode sentences technique similar wordvec embedded entire sentences -dimensional space. trained network bookcorpus dataset. khalifa argue stories better generated using recurrent neural networks trained highly specialized textual corpora body works single proliﬁc author. however technique capable open story generation. based theory script learning chambers jurafsky learn causal chains revolve around protagonist. developed representation took note event/verb occurred type dependency connected event protagonist pichotta mooney developed -tuple event representation verb preposition nouns representing subject direction object prepositional object respectively. representation inspired work although slightly different representation. paper script learning need convert event representations back natural language. related automated story generation story cloze test task choosing given endings story. story cloze test transforms story generation classiﬁcation problem -sentence story given along alternative sentences sentence. state-of-the story cloze test techniques combination word embeddings sentiment analysis stylistic features automated story generation research problem interest since nearly inception artiﬁcial intelligence. early attempts relied symbolic planning case-based reasoning using ontologies techniques could generate stories predetermined well-deﬁned domains characters places actions. creativity systems conﬂated robustness manually-engineered knowledge algorithm suitability. recently machine learning used attempt learn domain model stories created identify segments story content available existing repository assemble stories. sayanthing system uses textual case-based automated story generation formalized follows given sequence events sample probability distribution successor events. simple automated story generation expressed process whereby next event computed sampling maximizing parameters generative domain model event time indicates size sliding window context history. work probability distribution produced recurrent encoder-decoder network parameters section consider level abstraction inputs network produces best predictive power retaining semantic knowledge. event sparsity results situation event successors probability occurrence potentially within margin error. situation story generation devolves random generation process. following pichotta mooney developed -tuple event representation verb subject verb object verb modiﬁer wildcard propositional object indirect object causal complement dependency unclassiﬁable stanford’s dependency parser. words stemmed. events created ﬁrst extracting dependencies stanford’s corenlp locating appropriate dependencies mentioned above. object modiﬁer cannot identiﬁed insert placeholder emptyp arameter refer paper. event translation process either extract single event sentence multiple events sentence. extract multiple events verbal sentential conjunctions sentence. consider sentence john mary went store algorithm would extract events john store∅ mary store∅. average number events sentence experiments used corpus movie plots wikipedia cleaned remove extraneous wikipedia syntax links actors played characters. corpus contains stories average number sentences story simplest form event representation achieved extracting verb subject object modiﬁer term sentence. however variations event representation increase level abstraction help encoder-decoder network predict successor events. enumerate possible variations below. generalized. element event tuple undergoes abstraction. named entities identiﬁed person names replaced <ne>n indicates n-th named entity sentence. named entities labeled category rest nouns replaced wordnet synset levels inherited hypernym hierarchy giving general category avoiding labeling generally verbs replaced version frames adding genre information. topic modeling entire corpus using python’s latent dirichlet analysis discovering different categories. took categorization type emergent genre classiﬁcation. clusters clear pattern e.g. company work money business. others less clear. cluster given unique genre number added event representation create -tuple deﬁned genre cluster number. note event representations exist including representations incorporate information experiments next section show different representations affect ability recurrent neural network predict story continuations. eventevent network recurrent multi-layer encoderdecoder network based unless otherwise stated experiments below eventevent network trained input either n-th event output experiments described seek determine different event representations affected eventevent predictions successor event story. evaluated event representation using metrics. perplexity measure surprised model training set. gain sense well probabilistic model trained predict data. speciﬁcally built model using n-gram length second metric bleu score compares similarity generated output ground truth looking n-gram precision. neural network architecture initially envisioned machine translation purposes bleu common evaluation metric. speciﬁcally n-gram length score takes account n-gram overlaps generated expected output varies experimental setup experiment trained sequence-to-sequence recurrent neural using tensorﬂow network trained parameters varying input/output bucket size number epochs vocabulary. neural nets trained decrease overall loss less epoch. took between epochs experiments. data split training validation test data. reported results evaluated using held-out test data. original words <ne>s. identical previous experiment except entity names classiﬁed person substituted <ne>n. generalized. -word event structure except named entities replaced words generalized wordnet verbnet following procedure described earlier. avoid overwhelming number experiments next experiments used winner ﬁrst experiments. subsequent experiments used variations generalized event representation showed drastically lower perplexity scores. generalized continued <ne>s. experiment mirrors previous exception number <ne>s. previous experiment numbers restarted every event. here numbers continue across input output. event mentioned kendall event mentioned kendall would number character. generalized genre. event structure experiment exception additional parameter event genre. genre number used training eventevent removed inputs outputs testing; artiﬁcially inﬂated bleu scores because easy network guess genre number genre number weighted equally words. generalized bigram. experiment tests whether history aids predicting next event. modiﬁed eventevent give event bigram predict en+. believe experiment could generalize cases en−k history. generalized bigram genre. combination ideas experiments generalized events event bigrams genre added. following three experiments investigate extracting event sentence story corpus possible; prior experiments ﬁrst event sentence original corpus. generalized multiple sequential. sentence yields senone event event created sentence tence train neural network event occurs sequence i.e. etc. last event sentence predicts ﬁrst event sentence generalized multiple all. experiment took events produced single sentence together input events produced following sentence together output. example sentence produced events follow would sentence produced events train neural network input output original word events similar perplexity original sentences. parallels similar observations made pichotta mooney deleting words little improve predictive ability eventevent network. however perplexity improved significantly character names replaced generalized <ne>tags followed generalizing words verbs. overall generalized events much better perplexity scores making bigrams—incorporating history—improved bleu scores nearly original word events. adding genre information improved perplexity. although events original words performed better terms bleu score belief bleu appropriate metric event generation emphasizes recreation input. overall bleu scores experiments attesting inappropriateness metric. perplexity appropriate metric event generation correlates ability model predict entire test dataset. borrowing heavily ﬁeld language modeling recurrent neural network approach story generation prediction problem. intuition generalized events would perform better generating successive events bears data. however greater generalization makes harder return events natural language sentences. also bleu scores bigram experiments generally higher others. shows history matters additional context provided increases number n-gram overlaps generated expected outputs. movie plots corpus contains numerous sentences interpreted describing multiple events. naive implementation multiple events hurt perplexity implicit order events generated sentence; necessarily sequential. allow multiple events sentences followed events subsequent sentence perplexity improves. unfortunately events human-readable must converted natural language sentences. since conversion sentences events eventevent linear lossy process translation events back sentences non-trivial requires adding details back example event relative.n. characterize-. male.n. feeling.n. could hypothetically come sentence brother praised empathy. actuality event came sentence experiment original sentences original words baseline original words <ne>s generalized baseline generalized continued generalized genre generalized bigram generalized bigram cont. generalized bigram genre generalized multiple sequential generalized multiple order generalized multiple eventevent encoderdecoder network guaranteed produce event ever seen training story corpus. furthermore experiments event representations eventevent indicate greater generalization lends better story generation. however greater generalization harder translate event back natural language sentence. section introduce eventsentence neural network designed translate event natural language. eventevent network takes input event samples distribution possible successor events mn+. before recurrent encoder-decoder network based eventsentence network trained parallel corpora sentences story corpus corresponding events. sense eventsentence attempting learn reverse lossy event creation process. envision eventevent eventsentence working together illustrated figure first sentence— provided human—is turned events. eventevent network generates successive events. eventsentence network translates events back natural language presents human reader. dashed lines boxes represent future work ﬁlling story speciﬁcs. continue story generation eventn+ back eventevent; sentence generation purely human consumption. experimental setup setup experiments almost identical eventevent experiments main difference used pytorch easily lent implementing beam search. lstm networks experiments beam search instead greedy search ﬁnding optimal solution decoding. input experiments events particular representation output newly-generated sentence based input event. models experiments trained events paired sentences eventiﬁed from. complete story generation system output eventevent network feeds eventsentence network. examples seen table however tested eventsentence network events extracted original sentences used eventevent order conduct controlled experiments compute perplexity bleu scores. test eventsentence event representation used original words relatively straight forward. experimenting translating generalized events natural language sentences challenging since would forcing neural guess character names nouns verbs. devised alternative approach generalized eventsentence whereby sentences ﬁrst partially eventiﬁed. trained eventsentence generalized sentences person named entities replaced <ne>tags named entities replaced category remaining nouns replaced wordnet synsets. verbs left alone since often consistent across sentences within story. intuition character names particulars objects places highly mutable affect overall story long remain consistent. below show example sentence partially remaining activity.n. launches happening.n. droid organization property.n. near rear skilled worker.n. uses instrumentality.n. happening.n. chemical.n. dislodge device.n. also looked whether eventsentence performance would improved used multiple events sentence instead default single event sentence. alternatively automatically split prune sentences; removing prepositional phrases splitting sentential phrases conjunctions start pronoun splitting original sentence removing ﬁrst word. would allow evaluate sentences would fewer events extracted each. example experiment original words event original sentence generalized event generalized sentence generalized events gen. sentence original words event sentence generalized event gen. sentence generalized events gen. sent. although splitting pruning sentences bring sentences single event isn’t always case. thus eventsentence experiment extracted events sentences. results discussion results eventsentence experiments shown table although generalizing sentences improves perplexity drastically splitting pruning sentences yields better bleu scores original words kept. case eventsentence bleu scores make sense metric since task translation task. perplexity experiments appears correspond vocabulary size. generalized events full-length generalized sentences better bleu scores original words used. however work sentences pattern ﬂips. believe word generalizing methods reduce sparsity events combined much information lost neural network struggles distinguishing patterns. table shows examples entire pipeline currently exists sentence next sentence without slot ﬁlling full sense generalized sentences would read imagine adding character names details completing mad-libs game. question remains determine exactly character names noun details place <ne>s wordnet placeholders. figure propose addition working memory long-term memory modules. working memory module would retain character names nouns lookup table removed during eventiﬁcation process. partially generalized sentence produced eventsentence system working memory lookup table character names nouns back placeholders. intuition event next many details—especially character names—are likely reused. stories common form turn-taking between characters. example events john hits andrew andrew runs away john followed john chases andrew illustrate turn-taking pattern. john always used ﬁrst named entity meaning example would signiﬁcantly altered. continuous generated next event monster.n. amuse-. sarge monster.n. amuse-. monster.n. realize conjecture-.- male.n. conduit.n. entity.n. male.n. free- penal institution.n. male.n. spatial conﬁguration-. adopt come know truth times working memory able named entity wordnet synset placeholder slots recent event bigram contain element necessary reuse. long-term memory maintains history named entities nouns ever used story information long last used. cognitively-plausible event-based memory used compute salience entities story. underlying assumption stories likely reuse existing entities concepts introduce entities concepts. model automated story generation prediction successor events simplistic; assumes stories generated language model captures generalized patterns event co-occurrence. story generation also formalized planning problem taking account communicative goals. storytelling communicative goal tell story particular topic include theme story particular way. future work plan replace eventevent network reinforcement learning process perform lookahead analyze whether potential successor events likely lead communicative intent met. grams signiﬁcantly harm generative model likely help coherence incorporate history process although story coherence difﬁcult measure evaluated experiments. although generalization events away natural language appears help event successor generation poses problem making story content unreadable. introduced second neural network eventsentence learns translate events generalized original words back natural language. important possible eventevent generate events never occurred story training corpus. maintain able recover human-readable sentences generalized events valuable since eventevent experiments show preferred necessary able speciﬁcs later dynamic storytelling. present proposed pipeline architecture ﬁlling missing details automatically generated partially generalized sentences. pursuit automated story generation nearly ﬁeld artiﬁcial intelligence itself. whereas prior efforts success hand-authored domain knowledge machine learning techniques neural networks provide path forward toward vision open story generation ability computational system create stories conceivable topic without human intervention providing comprehensive corpus story texts. automated story generation event representation matters. hypothesize using intuitions storytelling select representation story events maintains semantic meaning textual story data reducing sparsity events. sparsity events particular results poor story generation performance. experiments different story representations eventevent generation back hypothesis event representation. found events abstract away natural language text improve generative ability recurrent neural network story generation process. event biwork supported darpa wnf--c-. views opinions and/or conclusions contained paper author interpreted representing ofﬁcial views policies either expressed implied darpa dod. authors would like thank murtaza dhuliawala animesh mehta yuval pinter technical contributions.", "year": 2017}