{"title": "Adversarial Neural Machine Translation", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "In this paper, we study a new learning paradigm for Neural Machine Translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English$\\rightarrow$French and German$\\rightarrow$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.", "text": "achieves similar even better translation results end-to-end framework. sentence level maximum likelihood principle gating units lstm/gru together attention mechanisms grant ability better translate long sentences. despite success translation quality latest systems still satisfaction remains large room improvement. example usually adopts maximum likelihood estimation principle training i.e. maximize probability target ground-truth sentence conditioned source sentence. objective guarantee translation results natural sufﬁcient accurate compared ground-truth translation human. previous works alleviate limitations maximum likelihood training adopting sequence level objectives reduce objective inconsistency training inference. somewhat improved objectives still cannot fully bridge translations ground-truth translations. paper adopt thoroughly different targeting directly minimizing difference human translation translation given model. achieve target inspired recent success generative adversarial networks design adversarial training protocol name paper study learning paradigm neural machine translation instead maximizing likelihood human translation previous works minimize distinction between human translation translation given model. achieve goal inspired recent success generative adversarial networks employ adversarial training architecture name adversarial-nmt. adversarialnmt training model assisted adversary elaborately designed convolutional neural network goal adversary differentiate translation result generated model human. goal model produce high quality translations cheat adversary. policy gradient method leveraged co-train model adversary. experimental results english→french german→english translation tasks show adversarial-nmt achieve signiﬁcantly better translation quality several strong baselines. adversarial-nmt. adversarial-nmt besides typical model adversary introduced distinguish translation generated human meanwhile model tries improve translation results successfully cheat adversary. modules adversarial-nmt cotrained performances mutually improved. particular discriminative power adversary improved learning training samples ability model cheating adversary improved taking output adversary reward. translation results professor forced close possible ground-truth translation. different previous gans assume existence generator continuous space proposed framework model fact typical generative model instead probabilistic transformation maps source language sentence target language sentence discrete space. differences make necessary design network architectures optimization methods make adversarial training possible nmt. therefore aspect leverage specially designed convolutional neural network model adversary takes sentence pair input; aspect turn policy gradient method named reinforce widely used reinforcement learning literature guarantee modules effectively optimized adversarial manner. conduct extensive experiments demonstrates adversarial-nmt achieve signiﬁcantly better translation results traditional models even much larger vocabulary size higher model complexity. research focus community. typical system built within based encoderdecoder framework. framework encoder sequentially processes words source language sentence ﬁxed length vectors inputs decoder decode translation sentence. typically adopts principle maximum likelihood estimation training i.e. maximizing per-word likelihood target sentence. training criteria minimum risk training based reinforcement learning translation reconstruction shown improve word-level principle since objectives take translation sentence whole. training principle propose based spirit generative adversarial networks generally adversarial training adversarial training discriminator generator compete other forcing generator produce high quality outputs able fool discriminator. adversarial training typically succeed image generation limited contribution natural language processing tasks mainly difﬁculty propagating error signals discriminator generator discretely generated natural language tokens. alleviates difﬁculty reinforcement learning approach sequence generation. however know limited efforts adversarial training sequenceto-sequence task conditional mapping between sequences involved work among ﬁrst endeavors explore potential acting especially neural machine translation decoding state decoder time recurrent unit long short term memory unit gated recurrent unit distinct source representation time calculated attention mechanism adversary model adversary used differentiate translation result ground-truth translation given source language sentence achieve that needs measure translative matching degree source-target sentence pair turn convolution neural network task since layerby-layer convolution pooling strategies able accurately capture hierarchical correspondence different abstraction levels. general structure shown figure speciﬁcally given sentence pair ﬁrst construct image-like representation simply concatenating embedding vectors words i-th word j-th word sentence following feature figure adversarial-nmt framework. ‘ref’ short ‘reference’ means groundtruth translation ‘hyp’ short ‘hypothesis’ denoting model translation sentence. yellow parts denote model maps source sentence translation sentence. parts adversary network predicts whether given target sentence ground-truth translation given source sentence combat other generating sampled translation train reward signals train policy gradient sentence. denote translation sentence system source sentence previously stated goal adversarial-nmt force ‘similar’ perfect case similar human translation even human cannot tell whether generated machine human. order achieve that introduce extra adversary network acts similarly discriminator adopted gans goal adversary differentiate human translation machine translation model tries produce target sentence similar human translation fool adversary. adopt recurrent neural network based encoder-decoder model seek target language translation given source sentence particular probabilistic mapping ﬁrstly learnt translation result sampled speciﬁc given source sentence previously generated words could layers convolution max-pooling aiming capturing correspondence different levels abstraction. extracted features multi-layer perceptron sigmoid activation last layer give probability ground-truth data i.e. optimization target adversary minimize cross entropy loss binary classiﬁcation ground-truth data positive instance sampled data negative one. eqn. reveals straightforward train adversary keeping providing ground-truth sentence pair sampled translation pair respectively positive negative training data. however turns model non-trivial design training process given discretely sampled makes difﬁcult directly back-propagate error signals making nondifferentiable w.r.t. model parameters tackle challenge leverage reinforce algorithm monte-carlo policy gradient method reinforcement learning literature optimize note objective training ﬁxed source language sentence minimize following loss item using language reinforcement learning eqn. model conditional policy faced term log)) provided adversary acts monte-carlo estimation reward. intuitively speaking eqn. implies likely note fact sample log)) trajectory estimate terminal reward given acting brings high variance reduce variance moving average historical reward values reward baseline sample multiple trajectories decoding step regarding roll-out policy reduce estimation variance immediate reward however empirically approach intolerably time-consuming task given decoding space typically extremely large worth comparing adversarial training existing methods directly maximize sequence level measure bleu training models using similar approaches based reinforcement learning ours. argue adversarial-nmt makes optimization easier compared methods. firstly reward learned adversary provides rich global information evaluate translation goes beyond bleu’s simple low-level n-gram matching criteria. acting provides much smoother objective compared bleu since latter highly sensitive slight translation difference word phrase level. model adversary adversarialnmt co-evolves. dynamics adversary makes model grows adaptive rather controlled ﬁxed evaluation metric bleu. given reasons adversarialnmt makes optimization process towards sequence level objectives much robust better controlled veriﬁed superior performances aforementioned methods reported next section dataset en→fr translation sake fair comparison previous works dataset dataset composed subset training corpus training combination news-test news-test news-test test respectively contains roughly sentence pairs. maximal sentence length frequent english french words replace words ‘unk’ token. de→en translation following previous works dataset iwslt evaluation campaign consisting training/dev/test corpus approximately bilingual sentence pairs respectively. maximal sentence length also dictionary english german corpus respectively include frequent words words replaced special token ‘unk’. adversarial-nmt structure model rnnsearch model based encoding-decoding framework attention mechanism. single layer grus encoder decoder. en→fr translation dimensions word embedding hidden state respectively de→en translation adversary consists convolution+pooling layers layer softmax layer convolution window size pooling window size feature size hidden layer size. training model similar commonly done previous works warm start well-trained rnnsearch model optimize using vanilla mini-batch size en→fr translation de→en translation. gradient clipping used clipping value en→fr de→en. initial learning rate chosen cross-validation halve every iterations. rnnsearch replace rnnsearch vocabs replace lstm layers vocabs lstm layers vocabs posunk rnnsearch +minimum risk training objective rnnsearch +monolingual data rnnsearch+ monolingual data dual objective table different systems’ performances en→fr translation. default setting single layer vocabs training objective trained monolingual data i.e. rnnsearch model proposed bahdanau signiﬁcantly better shen important factor successfully training combination adversarial objective mle. force randomly chosen mini-batch data trained adversarialnmt apply principle minibatches. acting signiﬁcantly improves stability model training also reported tasks language model neural dialogue generation conjecture reason acts regularizer guarantee smooth model update alleviating negative effects brought high gradient estimation variance one-step montecarlo sample reinforce. adversary network initially pre-trained using sampled data sampled rnnsearch model ground-truth translation that joint training adversarial-nmt adversary optimized using nesterov batch size initial learning rate en→fr de→en chosen validation set. dimension word embedding word embeddings training. batch normalization observed signiﬁcantly improve performance. considergenerating model translation evaluation beam width en→fr de→en respectively according bleu set. translation quality measured tokenized casesensitive bleu score result en→fr translation table provide en→fr translation result adversarial-nmt together several strong baselines well representative attention-based model rnnsearch addition make comparison comprehensive would like cover several well acknowledged techniques whose effectiveness veriﬁed improve en→fr translation previously published works including leverage using large vocabulary handle rare words different training objectives minimum risk training directly optimize evaluation learning rates; different learning rates. figure bleus en→fr adversarial-nmt training process learning rates different learning rates left learning rates different learning rates right measure dual learning enhance primal dual tasks improved inference process beam search optimization postprocessing leveraging additional monolingual data table clearly observe adversarial-nmt obtains satisfactory translation quality baseline systems. particular even surpasses performances models much larger vocabularies deeper layers much larger monolingual training corpus goal directly maximizing bleu fact know adversarialnmt achieves state-of-the-art result en→fr translation single-layer sequenceto-sequence models trained supervised bilingual corpus news-test test set. human evaluation apart comparison based objective bleu scores better appraise performance model also involve human judgements subjective measure. speciﬁc generate translation results randomly selected english sentences en→fr news-test dataset using table human evaluations adversarial-nmt english→french translation. means evaluator made decision translations generated adversarial-nmt better mrt. adversarialnmt. chosen since well representative previous methods maximize sequence level objectives achieving satisfactory results among single layer models afterwards three human labelers choose better versions translated sentences. evaluation process conducted amazon mechanical turk workers native english french speakers. adversarial training slow fast subsection analyze pace training model adversary make speciﬁcally en→fr combatting effectively. translation inspect bleu varies along adversarial training process different initial learning rates conditioned ﬁxed. ﬁgures show adversarial-nmt much robust regard pace making progress since three curves grow similar pattern curves drastically differ other. conjecture reason adversarial-nmt based powerful classiﬁcation tasks especially warm started sampled data rnnsearch. comparison translation model relatively weak providing qualiﬁed translations. therefore training needs carefully conﬁgurations learning rate small value leads slower convergence large value brings un-stability proper learning rate induces make fast meanwhile stable progress along training. result de→en translation table provide de→en translation result adversarial-nmt compared strong baselines rnnsearch effect adversarial training better visualize understand advantages adversarial training brought adversarial-nmt show several translation cases table concretely speaking give german→english translation examples including source language sentence ground-truth translation sentence model translation sentences respectively rnnsearch adversarial-nmt emphasized different parts bold fonts lead different translation quality. model translation also list i.e. probability adversary regards ground-truth third column sentence level bleu score last column. since rnnsearch model acts warm start training adversarial-nmt translation could viewed result adversarial-nmt initial phase. therefore table observe correspondingly translation quality growth makes adversary deteriorated shown recognition rnnsearch translated model whereas makes mistakes classifying adversarial-nmt ground-truth paper propose novel intuitive training objective force translation results similar ground-truth translations generated human. objective achieved adversarial training framework called adversarial-nmt complements original model adversary based cnn. adversarial-nmt adopts network architectures reﬂect mapping within sentence efﬁcient policy gradient algorithm tackle optimization difﬁculty brought discrete nature machine translation. experiments english→french german→english translation tasks clearly demonstrate effectiveness adversarial training method nmt. future works hope achieving state-of-the-art performance system plan fully exploit potential adversarialnmt combining powerful methods listed subsection training large vocabulary minimum-risk principle deep structures. additionally would like emphasize explore feasibility adversarial training table different systems’ performances de→en translation. default setting single layer encoder-decoder model training objective i.e. rnnsearch model proposed bahdanau signiﬁcantly better shen weiß dass k¨onnen soweit mich betrifft etwas welt jetzt braucht know &apos;m concerned &apos;s something world needs right know &apos;s time world needs know something world needs m¨ussen verhindern dass menschen kenntnis erlangen dingen allem dann wenn wahr sind prevent people ﬁnding things especially true table cases-studies demonstrate translation quality improvement brought adversarial-nmt. provide de→en translation examples source german sentence ground-truth english sentence translation results respectively provided rnnsearch adversarial-nmt. probability model translation ground-truth translation calculated adversary bleu per-sentence translation bleu score translated sentence.", "year": 2017}