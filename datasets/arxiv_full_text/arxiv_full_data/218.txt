{"title": "Deciding How to Decide: Dynamic Routing in Artificial Neural Networks", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.", "text": "milner faces behaviorally-relevant stimuli ellicit responses anatomically distinct specialized regions however state-of-the-art artiﬁcial neural networks visual inference routed statically every input triggers identical sequence operations. mind propose mechanism introducing cascaded evaluation arbitrary feedforward anns focusing task object recognition proof concept. instead classifying images ﬁnal layer every layer network attempt classify images lowambiguity regions input space passing ambiguous images forward subsequent layers consideration propose three approaches training networks test small image datasets synthesized mnist cifar- quantify accuracy/efﬁciency trade-off occurs network parameters tuned yield aggressive early classiﬁcation policies. additionally propose evaluate methods appropriating regularization optimization techniques developed statically-routed networks. propose systematically evaluate three strategies training dynamically-routed artiﬁcial neural networks graphs learned transformations different input signals take different paths. though approaches advantages others resulting networks often qualitatively similar. that dynamically-routed networks trained classify images layers branches become specialized process distinct categories images. additionally given ﬁxed computational budget dynamically-routed networks tend perform better comparable staticallyrouted networks. decisions easier make others—for example large unoccluded objects easier recognize. additionally different difﬁcult decisions require different expertise—an avid birder know little identifying cars. hypothesize complex decision-making tasks like visual classiﬁcation meaningfully divided specialized subtasks system designed perform complex task ﬁrst attempt identify subtask presented information select suitable algorithm solution. approach—dynamically routing signals inference system based content—has already incorporated machine vision pipelines methods boosting coarse-to-ﬁne cascades random decision forests dynamic routing also performed primate visual system spatial information processed somewhat separately object identity information recently kontschieder performed joint optimization decision tree parameters bulo kontschieder used randomized multi-layer networks compute decision tree split functions. knowledge family inference systems discuss ﬁrst described denoyer gallinari additionally bengio explored dynamically skipping layers neural networks ioannou explored dynamic routing networks equallength paths. recently-developed visual detection systems perform cascaded evaluation convolutional neural network layers though highly specialized task visual detection modiﬁcations radically improve efﬁciency. approaches lend evidence dynamic routing effective either ignore cost computation represent explicitly instead opaque heuristics trade accuracy efﬁciency. build foundation deriving training procedures arbitrary application-provided costs error computation comparing actor-style critic-style strategies considering regularization optimization context dynamically-routed networks. statically-routed feedforward artiﬁcial neural network every layer transforms single input feature vector single output feature vector. output feature vector used input following layer exists ouptut network whole not. consider networks layers sink. network every n-way junction signal reaches network must make decision {..n} signal propagate sink compute argmax score vector learned function last feature vector computed reaching we’ll refer rule generating inference routing policy. convolutional network layers compute collections local descriptions input signal. unreasonable expect kind feature vector explicitly encode global information relevant deciding route entire signal local descriptors bottom global descriptors top. every junction score vector computed small routing network operating last-computed global descriptor. multipath architecture illustrated fig. nops number multiply-accumulate operations performed kcpt scalar hyperparameter. deﬁnition assumes timeenergy-constrained system—every operation consumes roughly amount time energy every operation equally expensive. ccpt deﬁned differently constraints propose three approaches training dynamicallyrouted networks along complementary approaches regularization optimization method adapting changes cost computation. figure multiscale convolutional architecture. column evaluated network decides whether classify image evaluate subsequent columns. deeper columns operate coarser scales compute higher-dimensional representations location. convolutions kernels downsampling achieved pooling routing layers channels. kure scalar hyperparameter subnetwork consisting child descendants. since want learn policy indirectly treated constant respect optimization. improve stability loss potentially accelerate training adjust routing utility function that every junction independent routing parameters downstream instead predicting cost making routing decisions given current downstream routing policy predict cost making routing decisions given optimal downstream routing policy. optimistic variant critic method since discrete cinf cannot minimized gradient-based methods. however replaced stochastic approximation training engineer gradient nonzero. learn routing parameters classiﬁcation parameters simultaneously minimizing loss want single network perform well situations various degrees computational resource scarcity make network’s routing behavior responsive dynamic ccpt concatenate ccpt’s known many regularization techniques involve adding modelcomplexity term cmod loss function inﬂuence learning effectively imposing soft constraints upon network parameters however term affects layers independent amount signal routed them either underconstrain frequently-used layers overconstrain infrequently-used layers. support frequentlyinfrequently-used layers regularize subnetworks activated instead regularizing entire network directly. weights associated layers activated scalar hyperparameter. actor networks apply extra term control magnitude therefore extent explores subpotimal paths training techniques attempt minimize expected cost performing inference network training routing policy. setup constant learning rate every layer network layers policy routes examples frequently receive larger parameter updates since contribute expected cost. allow every layer learn quickly possible scale learning rate layer dynamically factor elementwise variance loss gradient respect parameters independent amount probability density routed derive consider alternative routing policy routes signals though routes subseparameters—in case {kcpt}—to input every routing subnetwork allow modulate routing policy. match scale image features facilitate optimization express kcpt units cost tenmillion operations. experiments mini-batch size training iterations. perform stochastic gradient descent initial learning rate ./nex momentum learning rate decays continuously half-life iterations. networks classify images small-image dataset synthesized mnist cifar dataset includes classes mnist airplane automobile deer horse frog cifar- images mnist resized match scale images cifar- linear interpolation colormodulated make difﬁcult trivially distinguish cifar- images weights ﬁnal layers routing networks zero-initialized initialize weights using xavier initialization method biases zero-initialized. perform batch normalization every rectiﬁcation lected values exploring hyperparameter space logarithmically powers training evaluating hybrid mnist/cifar- dataset coarse level values locally optimal—multiplying dividing improve performance. augment data using approach popular cifar- augment image applying vertical horizontal shifts sampled uniformly range image cifar- ﬂipping horizontally probability blank pixels introduced shifts mean color image compare approaches dynamic routing training networks classify small images varying policy-learning strategy regularization strategy optimization strategy architecture cost computation details task. results experiments reported fig. code available github. figure sample images hybrid mnist/cifar- dataset. recolor images mnist following procedure select random colors least units away space; black pixels ﬁrst color white pixels second color linearly interpolate between. given computational budget dynamically-routed networks achieve higher accuracy rates architecturematched statically-routed baselines additionally dynamically-routed networks tend avoid routing data along deep paths beginning training possibly because error surfaces deeper networks complicated deeper paths less stable—changing parameters component layer better classify images routed along other overlapping paths decrease performance. whatever mechanism tendency initially simpler solutions seems prevent overﬁtting occurs -layer statically-routed networks. figure hybrid dataset performance. every point along statically-routed nets curve corresponds network composed ﬁrst columns architecture illustrated fig. points along actor dynamic kcpt curve correspond single network evaluated various values kcpt described section points along curves correspond distinct networks trained different values kcpt. kcpt figure dataﬂow actor networks trained classify images hybrid mnist/cifar- dataset. every node-link diagram corresponding network trained different kcpt. circle indicates area fraction examples classiﬁed corresponding layer. circles colored indicate accuracy layer kinds images classiﬁed layer compared dynamically-routed networks optimistic critic networks perform poorly possibly optimal routers poor approximation small lowcapacity router networks. actor networks perform better critic networks possibly critic networks forced learn potentially-intractable auxilliary task actor networks also consistently achieve higher peak accuracy rates comparable statically-routed networks across experiments. figure dataﬂow course training. heatmaps illustrate fraction validation images classiﬁed every terminal node bottom four networks fig. course training. although actor networks performant critic networks ﬂexible. since critic networks don’t require differentiable function trained sampling saving memory support wider selection training routing policies cinf deﬁnitions. addition training standard critic networks train networks using variant pragmatic critic training policy replace cross-entropy error cure term classiﬁcation error. although networks perform well original pragmatic critic networks still outperform comparable statically-routed networks. based experiments hybrid dataset regularizing described section discourages networks routing data along deep paths reducing peak accuracy. additionally mechanism encouraging exploration appears necessary train effective actor networks. throughput-adjusting learning rates described section improves hybrid dataset performance actor critic networks computationalresource-abundant high-accuracy contexts. given computational budget architectures -way junctions higher capacity subtrees -way junctions. hybrid dataset under tight computational constraints trees higher degrees branching achieve higher accuracy rates. unconstrained however prone overﬁtting. dynamically-routed networks early classiﬁcation layers tend high accuracy rates pushing difﬁcult decisions downstream. even without energy contraints terminal layers specialize detecting instances certain classes images. classes usually related networks -way junctions branches specialize even greater extent. train single actor network classify images hybrid datset various levels computational constraints using approach described section sampling kcpt randomly mentioned fig. training example. network performs comparably collection actor nets trained various static values kcpt signiﬁcant central region accuracy/efﬁciency curve -fold reduction memory consumption training time. works train networks classify images cifar adjusting classiﬁcation task vary frequency difﬁcult decisions call variants cifar-—labelling images horse other— cifar-—labelling images deer horse other. experiment compare actor networks architecture-matched statically-routed networks. test whether dynamic routing advantageous highercapacity settings train actor networks architecturematched statically-routed networks classify images cifar- varying width networks increasing model capacity either increases affect relative advantage dynamically-routed networks suggesting approach applicable complicated tasks. experiments suggest dynamically-routed networks trained mild computational constraints operate times efﬁciently comparable staticallyrouted networks without sacriﬁcing performance. additionally despite higher capacity dynamically-routed networks less prone overﬁtting. designing multipath architecture suggest supporting early decision-making wherever possible since cheap simple routing networks seem work well. convolutional architectures pyramidal layers appear reasonable sites branching. actor strategy described section generally effective learn routing policy. however pragmatic critic strategy described section better suited large networks networks designed applications nonsmooth cost-of-inference functions—e.g. kcpt units errors/operation. adjusting learning rates compensate throughput variations described section improve performance deep networks. cost computation dynamic single network trained procedure described section still sufﬁcient. test approach tasks degree difﬁculty variation possible dynamic routing even advantageous performing complex tasks. example video annotation require specialized modules recognize locations objects faces human actions scene components attributes having every module constantly operating extremely inefﬁcient. dynamic routing policy could fuse modules allowing share common components activate specialized components necessary. another interesting topic future research growing shrinking dynamically-routed networks training. network necessary specify architecture. network instead take shape course training computational contraints memory contraints data dictate. figure performance effects task difﬁculty distribution described section statically-routed nets actor nets curves drawn analogously counterparts fig. figure performance effects model capacity training testing cifar-. networks architecture illustrated fig. networks otherwise identical presented left panel number output channels every convolutional layer multiplied kcpt divided networks otherwise identical presented left panel number output channels every convolutional layer multiplied kcpt divided dynamic routing beneﬁcial task involves many low-difﬁculty decisions allowing network route data along shorter paths. dynamic routing offers slight advantage cifar- dynamically-routed networks achieve higher peak accuracy rate cifar- statically-routed networks third computational cost. bulo samuel kontschieder peter. neural decision forests semantic image labelling. proceedings ieee conference computer vision pattern recognition zhaowei saberian mohammad vasconcelos nuno. learning complexity-aware cascades deep proceedings ieee inpedestrian detection. ternational conference computer vision dosovitskiy alexey fischer philipp eddy hausser philip hazirbas caner golkov vladimir smagt patrick cremers daniel brox thomas. flownet learning optical convolutional networks. proceedings ieee international conference computer vision ioannou yani robertson duncan zikic darko kontschieder peter shotton jamie brown matthew criminisi antonio. decision forests convolutional networks models in-between. arxiv preprint arxiv. ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv. kontschieder peter fiterau madalina criminisi antonio rota bulo samuel. deep neural decision forests. proceedings ieee international conference computer vision haoxiang shen xiaohui brandt jonathan gang. convolutional neural network cascade face detection. proceedings ieee conference computer vision pattern recognition kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition shaoqing kaiming girshick ross jian. faster r-cnn towards real-time object detection region proposal networks. advances neural information processing systems zhou erjin haoqiang zhimin jiang yuning extensive facial landmark localization procoarse-to-ﬁne convolutional network cascade. ceedings ieee international conference computer vision workshops", "year": 2017}