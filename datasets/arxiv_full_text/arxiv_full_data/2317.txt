{"title": "Imitation Learning with Concurrent Actions in 3D Games", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this work we describe a novel deep reinforcement learning neural network architecture that allows multiple actions to be selected at every time-step. Multi-action policies allows complex behaviors to be learnt that are otherwise hard to achieve when using single action selection techniques. This work describes an algorithm that uses both imitation learning (IL) and temporal difference (TD) reinforcement learning (RL) to provide a 4x improvement in training time and 2.5x improvement in performance over single action selection TD RL. We demonstrate the capabilities of this network using a complex in-house 3D game. Mimicking the behavior of the expert teacher significantly improves world state exploration and allows the agents vision system to be trained more rapidly than TD RL alone. This initial training technique kick-starts TD learning and the agent quickly learns to surpass the capabilities of the expert.", "text": "improve exploration breaking action value function components models value state models per-action advantage also modeling state value function distribution unlike previous off-policy techniques mnih propose algorithm moves away experience replay based training regime. describe architecture carries updates using data large number simultaneously running agents. show training using multiple agents version environment decorrelates updates similar manner memory sampling added beneﬁt improved exploration training speed stability. despite advances algorithms based around computationally expensive take signiﬁcant amount time train. training using effective target value grounded reality current estimate. condition satisﬁed accumulation reward states otherwise training simply updates guess towards another. thus training using particularly problematic reward sparse environments might require many speciﬁc consecutive actions receive reward. consider racing game reward scaled inversely time received completing lap. task extremely difﬁcult solve using receive reward thus perform useful update agent would select correct action many thousands steps stage agent understanding world. technique known reward shaping alleviate problems reward sparse environments. using reward shaping reward function changed expert understands objective task order encourage behaviours help agent solve task. care taken though always trivial tweak rewards without signiﬁcantly altering nature task hand. further often difﬁcult break complicated task number smaller sub-tasks amenable shaping. problems large action spaces also difﬁcult train using probability selecting correct action order receive reward decreases size action space increases. credit assignment also becomes difﬁcult abstract—in work describe novel deep reinforcement learning neural network architecture allows multiple actions selected every time-step. multi-action policies allows complex behaviours learnt otherwise hard achieve using single action selection techniques. work describes algorithm uses imitation learning temporal difference reinforcement learning provide improvement training time improvement performance single action selection demonstrate capabilities network using complex in-house game. mimicking behavior expert teacher signiﬁcantly improves world state exploration allows agents vision system trained rapidly alone. initial training technique kick-starts learning agent quickly learns surpass capabilities expert. barto undergone renascence transformative powers deep neural network architectures. strength architectures ability used arbitrary function approximators. ability allowed neural network based model free techniques solve number challenging tasks previously intractable. mnih describe algorithm employs neural network estimate value high dimensional input states. algorithm uses bootstrapping technique train neural network. simply algorithm minimizes difference networks estimate value current state target value. target value simply networks predicted value next state plus rewards received in-between states. target value partly made rewards gained agents interactions environment grounded reality initial guess. time updates allow network learn value function. mnih demonstrate capability algorithm training agents play atari video games using pixel values input. number improvements algorithm suggested literature including limited modiﬁcation reduces bias value function estimate technique improving data efﬁciency algorithm adding type prioritisation experience replay memory sampling scheme adding noise speciﬁc layers expert break tasks rewards ﬁne-grained rewards expert demonstrate desired behaviour. then network determine change policy order match expert behaviour. technique known imitation learning imitation learning provides agent prior knowledge effective strategies behaving world. combining allows agent learn it’s experiences helps avoid situations skill agent limited skill teacher. training using deﬁnition off-policy typically limited off-policy training techniques carried pre-training step used pre-training step policy often prone collapse limited state-space coverage expert data; models tend over-ﬁt data instead learning general solution. effects mitigated using large amount expert training data work silver however time effort cost associated collecting data often limiting factor effective deployment techniques. training agents interact complex environments large action spaces behaviour associated single action time step policy often undesirable. example running forward whilst straﬁng shooting video game effective strategy impossible achieve using saps architectures. solving problems multiple action time step required networks architectures rely modeling possible combinations actions separate distinct actions however using large actions spaces dramatic increase number possible action combinations severely limits applicability techniques. example typical modern video game controller might around buttons modeling possible combinations inputs would require output policy size entangled action representations large action space make much harder agent learn value true actions. network ﬁrst learn disentangled version action space order understand similarities certain actions. reasons algorithm allows multiple output actions time step improve performance agent. video game algorithms typically rely access hidden world-state information different rendered image human player observes. result agents controlled using algorithms often manner breaks player immersion inhuman like behavior. further rigid nature scripted algorithms range behaviours display also often quite limited. immersion entertainment describe technique training agent play style game. comparison games atari platform games particularly challenging problem mainly factors described previously also partially observed nature games challenges related exploring large state spaces. work present algorithm combines supervised imitation learning temporal difference throughout training; using small amount expert data. describe neural network architecture outputs multiple discrete actions time step describe loss function allows policy trained. combining multi-action time step imitation learning manner allows higher quality expert data used circumvents difﬁculties associated recording expert data expert limited single action time step interactions environment. call resulting model multi-action time step imitation learning describe techniques used teach agent play challenging fully ﬁrst person shooter style video game important milestone training neural networks play modern games. discounted return roll-out policy bootstrapped value estimate. given action sampled policy policy update negative probability action multiplied advantage. advantage measure much reward agent received network initially estimated. training using loss increases likelihood actions lead positive advantage decreases likelihood actions don’t. probability taking action positive advantage large small update performed. whereas probability small large update occurs since negative probability large. encourage exploration state space entropy policy also added loss however omitted clarity. since accurate estimate value function critical accurate advantage estimation also trained. accomplished minimizing norm value function discounted target value highly inefﬁcient training agents perform tasks complex environments sparse reward and/or high dimensional action spaces. powerful simple technique improving pure learning train network imitate behaviour expert domain another algorithm human expert. silver describe effective technique manage train neural network controlled agents play game superhuman performance levels. perform imitation learning pre-training step sample large repository expert human data data train deep neural network maximise likelihood selecting expert action given input. major problems associated pre-training imitation learning over-ﬁtting expert data. network remembers exactly actions perform speciﬁc input image expert training data instead learning robust general solution problem. then states encountered learning agent incapable selecting action intelligently. problem becomes severe partially observed environments. silver work around training using large expert data helped fully observed nature task. difﬁculties involved collecting large amount expert data take different approach. instead applying imitation learning pre-training step apply time batch update comprised expert live agent data. every update step network predicts action expert sample expert data whilst learning policy maximises discounted future reward live agent stream. training network allows network maintain valid learning compatible state throughout training. encourage generalisation gaussian noise expert data inputs apply dropout every layer except outputs. dropout used expert data. prevent ﬁnal performance agent limited quality expert data loss weighting factor linearly decayed start training. found training value function using expert data reduced performance stability agent. trained value function using pure alone. ﬁnal policy loss mail network described policy output layer based actor-critic architectures typically softmax action space. choosing action step simply requires sampling policy. normalization term softmax ensures updates increase likelihood action reduce likelihood actions. importance highlighted later. propose modiﬁcation neural network architecture allows multiple actions selected ﬁrst replace softmax activation function policy output layer sigmoid action policy output layer outputs probability individual action selected. might tempted perform training simply setting loss equal negative probability sampled actions given policy. however leads signiﬁcantly longer training times using saps policy. further generated policies tend biased towards performing large number actions step. likely reason step update process carried softmax saps policy neglected. architecture longer reduces likelihood actions weren’t selected automatically performed softmax saps case. simulate effect need increase probability actions selected advantage scaled gradient cross-entropy increases likelihood sampled actions decreases likelihood actions weren’t sampled. scale relative updates controlled using weighting factors mail experiments training batch consisted approximately on-policy live agent data expert data. salient information expert data provided table expert data generated prior training recording episodes human play. time-step following information stored memory buffer input observation expert action vector reward terminal state game features vector. game features vector contained agents health ammo simulate on-screen text human player read. in-house developed video game used training environment. game rewards received eliminating enemies collecting health ammo ﬁnding occupying region-of-interest map. location health ammo boxes region-of-interest change regular intervals throughout episode random location. enemies spawn waves navigate towards agent; attacking within range. figure provides visual overview environment demonstrates features game. time step agent observes pixel image agents ﬁrst-person view. small radar visible bottom left corner agents input image. agent also provided game features vector contains information related agents health ammo. experiments indicated using image observations improved agents performance relative observations high visual ﬁdelity environment. used base network architecture shown table experiments. high level input features concatenated output linear layer prior lstm inputs normalised maximum possible value. training parameters global experiments shown table iii. experiments gaussian noise added input observations high level features vector dropout applied hidden convolutional layers. used dropout values convolution hidden layers respectively. dropout applied processing live agent data. dropout chosen weight regularisation reduce risk image size input features size batch-size roll-out length gradient norm clipping optimiser initial learning rate final learning rate training steps network ﬁnding non-optimal local minima instead general solutions larger weights. experiments using decay expert prediction loss factor linearly decayed number decay steps experiment. fig. example input observations. left agent health buildings main view. agent also number enemies blue region-of-interest marker radar view. centre example green ammo box. right agent reached region interest indicated blue lighting ﬂoor around agent. using alone maps difﬁcult train saps difﬁculties associated credit assignment training using multiple actions main problem training using saps agent however policy imposes hard limit maximum capability agent. capability lower optimal maps agent saps policies subset maps policies. indeed best case scenario simple environment advantage associate carrying multiple actions simultaneously capability best match maps agent. however relatively high update frequency agent offsets problems associated single action time step updates game. running forward whilst straﬁng limited extent approximated selecting forward action frame strafe action next. early stages training maps agent trains rapidly saps agent. saps agent case ﬁring limits opportunity move turn adversely affects ability pick boxes regionof-interest. maps case ﬁring effect locomotion allows agent enemy targets agent quickly learns ﬁring generally positive action. however initial advantage disappears halfway training point saps agent learns beneﬁts interleaving actions locomotion actions. performance saps agent eventually surpasses maps agent since less affected credit assignment issues. environment absolute magnitude theoretical performance difference maps saps difﬁcult determine performance saps maps agents never gets high enough behaviour considered optimal difﬁculties training using mail signiﬁcantly outperforms saps maps reaching ﬁnal score higher saps higher maps allows effective policy learnt fewer steps using alone exceeding ﬁnal score saps steps reduction training time speed-up pronounced early stages training reward sparsity severely limits effectiveness learning updates; imitation learning provides useful feedback every training step start training. supervised learning allows vision system trained much rapidly further mimicking behaviour expert signiﬁcantly improves exploration state-space comparison unguided random actions early stages mail agent quickly learns collect boxes whilst heading towards region-of-interest; behaviour seen less hour training point view agent rapid increase agent capability signiﬁcantly reduces reward sparsity kick-starts next phase training temporal difference learning dominates. ﬁnal phase agent learns surpass capabilities expert. mean score expert human player signiﬁcantly lower ﬁnal score mail agent signiﬁcantly higher algorithms. trained mail agent takes full advantage maps architecture typically performs actions once learning behaviors running forward whilst simultaneously moving sideways turning shooting. mail agent performs similar number actions step expert teacher taking full advantage ability better understand affect ﬁnal mail agents capability also trained network without using maps il-only network achieved ﬁnal score signiﬁcantly lower training runs. score achieved steps improvement remaining training steps. results show that combined positive contribution early stages training; steps performance mail surpasses pure mail. steps mail agents score twice pure agent. appears forcing network learn solution maximises future reward also helps agent general solution allows extract useful information expert data; however testing hypothesis left future work. asses whether expert data eventually starts limit performance agent compare performance mail agent using different decay rates expert data loss using data higher decay rate reaches higher ﬁnal score suggesting eventually holds back performance agent. results also seem indicate learning reduces variance agent performance across games seen figure interestingly behaviour trained mail agent distinctly modal nature. behaviour changes signiﬁcantly depending upon agents current state. certain triggers agent running ammo cause agent drastically alter style play. behavior arises naturally without deliberately partitioning network facilitate behaviour i.e. using concepts manager networks. even efﬁcient training techniques deeper networks simple architectures might capable much higher level reasoning currently observed. examples observed behaviours agent include searching waypoint searching ammo/health patrolling region-of-interest attacking enemies ﬂeeing enemies health/ammo rapidly turning around face enemies immediately ﬁnding ammo human future work enhance capabilities mail architecture adding continuous actions rotations. provide number beneﬁts combined current mail architecture. provide agent grained motor control reduce size action space also allow much higher quality expert data recorded allowing data acquired using mouse keyboard analogue inputs game controller. improvements allow mail architecture used train agents play modern games. fortunato azar piot menick osband graves mnih munos hassabis pietquin blundell legg. noisy networks exploration. http//arxiv.org/abs/.. huazhe levine darrell. reinforcement learning imperfect demonstrations. http//arxiv.org/abs/.. hester vecerik pietquin lanctot schaul piot horgan quan sendonaris osband dulac-arnold agapiou leibo gruslys. deep q-learning demonstrations. https//arxiv.org/pdf/..pdf. hester vecerik pietquin lanctot schaul piot sendonaris dulac-arnold osband agapiou leibo gruslys. learning demonstrations real world reinforcement learning. ing. http//arxiv.org/abs/.. mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis. human-level control deep reinforcement learning. nature issn ./nature. http //dx.doi.org/./nature. mnih badia mirza graves lillicrap harley silver kavukcuoglu. asynchronous methods deep reinforcement learning. http//arxiv.org/abs/.. harada russell. policy invariance reward transformations theory application reward shaping. sixteenth international conference machine learning issn ..... silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis. mastering game deep neural networks tree search. nature issn ./nature. http//www.nature. com/doiﬁnder/./nature.", "year": 2018}