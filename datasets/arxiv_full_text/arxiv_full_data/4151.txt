{"title": "Visual-Inertial-Semantic Scene Representation for 3-D Object Detection", "tag": ["cs.CV", "cs.AI"], "abstract": "We describe a system to detect objects in three-dimensional space using video and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile platforms from phones to drones. Inertials afford the ability to impose class-specific scale priors for objects, and provide a global orientation reference. A minimal sufficient representation, the posterior of semantic (identity) and syntactic (pose) attributes of objects in space, can be decomposed into a geometric term, which can be maintained by a localization-and-mapping filter, and a likelihood function, which can be approximated by a discriminatively-trained convolutional neural network. The resulting system can process the video stream causally in real time, and provides a representation of objects in the scene that is persistent: Confidence in the presence of objects grows with evidence, and objects previously seen are kept in memory even when temporarily occluded, with their return into view automatically predicted to prime re-detection.", "text": "call object detectors traditionally refers algorithms process single image return decision presence objects certain class said image missing several critical elements above. nevertheless algorithms modiﬁed produce decisions evidence presence objects processed time integrated geometric topological structure scene yield object detector desired characteristics. scene context encompasses identity co-occurrence objects also spatial arrangement three-dimensional space design object detector based premises above formalize explicit model posterior probability object attributes semantic syntactic natively scene maintains updates posterior processing image causally time posterior distribution form short-term memory predict visibility occlusion relations exploit availability cheap inertial sensors almost every mobile computing platform impose class-speciﬁc priors size objects insight formalization optimal representation objects scene factored components geometric computed recursively localization system likelihood term evaluated instantaneously discriminatively-trained convolutional neural network operating single image. consequences insight discussed sect. practice means implement system using off-the-shelf components ﬁne-tuning pretrained least rudimentary modeling assumptions system operates real-time generating object-scene representations frames second. sect. report results representative sample describe system detect objects threedimensional space using video inertial sensors ubiquitous modern mobile platforms phones drones. inertials afford ability impose class-speciﬁc scale priors objects provide global orientation reference. minimal sufﬁcient representation posterior semantic syntactic attributes objects space decomposed geometric term maintained localization-and-mapping ﬁlter likelihood function approximated discriminatively-trained convolutional neural network. resulting system process video stream causally real time provides representation objects scene persistent conﬁdence presence objects grows evidence objects previously seen kept memory even temporarily occluded return view automatically predicted prime re-detection. deem object detector system takes input images produces output decisions presence objects scene. design based following premises objects exist scene image; persist conﬁdence presence grow evidence accrued multiple images; seen system aware presence even temporarily visible; awareness allow predict return view based scene geometry topology; objects characteristic shape size vestibular sensors provide global scale orientation reference system leverage jects’ shape parallelepiped bounding step forward bounding boxes image still rudimentary model objects based visibility computation rather crude. performed several tests dense reconstruction well models matching visibility computation based level accuracy efﬁciency enable real-time computation. third limitation full joint syntactic-semantic prior enforced. ideally would like predict objects likely become visible based context also appear relative other still computationally prohibitive scale. sect. start deﬁning object representation sufﬁcient invariant detection show main factor updated recursively integral measure represents syntactic context computed slam system factor computed cnn. update straightforward top-down initialization requires deﬁning prior object identity pose. bottom-up mode putative detection used initialize object hypotheses several heuristics place genetic phenomena work nature relates vast body literature scene understanding computer vision robotics dating back decades recently advent cheap consumer range sensors wealth activity area rgb-d cameras unfortunately restricts domain applicability mostly indoors close range whereas target mobility applications camera typically inertial sensor strapped range sensor used indoor outdoors. expect that indoor sequences method would underperform structured light rgb-d source subject future investigation. also work focuses scene understanding visual sensors speciﬁcally video although none integrates inertial data despite resurgent interest sensor fusion additional related work includes figure illustration system detect objects-in-scenes. state system reconstructed scene representation currently tracked points viewer trajectory previous loop current pose cars detected shown point-estimates green including previously-seen side streets middle visualization implicit measurement process objects state projected onto current image based mean vehicle pose estimate likelihood score computed cars different streets known visible visualized dashed boxes score discarded. bottom view state entire kitti- sequence system ﬁrst exploit inertial sensors provide scale discrimination global orientation visual recognition -object detectors assume images gravity-aligned safe photographic images robots drones. system also ﬁrst integrate cnn-based detectors recursive bayesian inference scheme implement overall system real-time formalization problem object detection general real-time implementation several limitations. first returns joint geometric semantic description static objects. moving objects detected image geometry shape pose estimating would require sophisticated class-speciﬁc deformation priors inferred. second models obscene populated number objects geometric semantic attributes lj}. measurements current time tured sensor pose semantic representation scene joint posterior j-th objects seen time sensor pose nuisances marginalized. joint posterior decomposed ﬁrst factor ideally updated asynchronously time object becomes manifest starting prior second factor updated time measurement becomes available starting given representation scene support localization tasks posterior sensor pose sparse attributed point cloud given measurements current time. conditioning semantics geometry write second factor measure approximated widesense using extended kalman filter customary simultaneous localization mapping diffusion around mean/mode ˆgt|t ˆxt|t; covariance pt|t small approximated given object pose position orientation world frame. inertials pose reduced position rotation around gravity. sensor pose full degree-of-freedom position orientation. semantic scene understanding single image also area research references therein). instead interested agents embedded physical space restriction single image limiting. also vast literature scene segmentation references therein) mostly using range sensors. popular pipeline dense semantic segmentation adopted depth maps obtained either rgb-d stereo fused; semantic labeling transferred smoothed fully-connected also related methods joint semantic segmentation reconstruction also work recognition inertial measurements motion. focus real-time operation operate off-line none datasets commonly used works provide inertial reference except kitti. terms object detection kitti authors focus image-based detection place objects scene others focus object proposal generation veriﬁcation using network trains voxel pattern based detector infer object attributes demonstrates ability accurately localize cars kitti. subsequent work trains classify dvps. different representations object proposals also exploited cuboids deformable wireframes various priors also considered exploits geo-tagged images; geometric priors objects incorporated various optimization frameworks estimate object attributes algorithms report good performance detection none reports scores semantic-syntactic state objects except since latter dominated former take paragon comparison sect. aforementioned object recognition methods based detection without temporal consistency. therefore comparison somewhat unfair single-image based detectors cannot reliably detect objects space main motivation proposed approach. details comparison methodology sect. multiple views output point-estimate instead posterior. also optimization rerun datum available. recent work data association aims directly infer association computationally prohibitive scale needed real-time system. therefore resort heuristics described sect. speciﬁcally implementation leverage existing visual-inertial ﬁlters single image-trained likelihood term approximated convolutional neural network shown sect. posterior updated bayesian ﬁlter approximated bank ekfs leaves ﬁrst factor posterior encodes context. could approximate recurrent network would beyond scope here; even forgo using co-occurrence prior amounts matrix multiplication rebalances classes following since limited number classes context priors experimented with makes little difference. approximating likelihood appears daunting because purported need generate future data given object class shape pose normalize respect possible images object. fortunately latter needed since product right-hand side needs normalized anyway done easily particle/mixturebased representation posterior dividing weights components. generating actual images similarly needed. needed mechanism that given image allows quantifying likelihood object class shape being present portion image projects vantage point sect. show discriminatively-trained leveraged end. measurement process instant image processed probing functions designed trained invariant nuisance variability. slam system processes past image measurements current inertial measurements collectively refer collection sparse contrast-invariant feature descriptors computed image visible regions scene produces joint posterior distribution poses sparse geometric representation scene assumed uni-modal approximated gaussian ∪jsj i.e. scene assumed composed union objects including default class background localization pipeline borrowed agnostic organization scene objects identity. also restricts subset scene rigid co-visible sufﬁciently long interval time located surfaces that locally exhibit lambertian reﬂection. compute marginal likelihood class leverage trained discriminatively classify given image region classes including background class. architecture soft-max layer preceded nodes class trained using cross-entropy loss providing class image bounding discard soft-max layer forgo class-normalization. activations nodes penultimate layer resulting network provide mechanism given image quantifying likelihood object class present bounding interpret likelihoods class present given bounding process induces likelihood object classes present visible portion scene regions corresponding vantage points projection. since inertials directly measured gaussian noise have inertial biases noise covariance; object attributes labels geometry thus given image possible object pose shape vantage point test presence least instance class within. note visibility function implicit object visible likelihood given image constant/uniform. note depends global layscene since must take account occlusions objects cannot considered independently. dependencies co-visibility computing likelihood object present scene requires ascertaining whether visible image turn depends objects scene modeled holistically rather independent collection objects. addition presence certain objects conﬁguration affects probability objects visible present. capture dependencies note geometric representation used provide joint distribution position objects cameras yields co-visibility information specifically probability point visible instance seeing keyboard monitor desk affects probability mouse scene even cannot present. relative pose also informs vantage point would reduce uncertainty presence mouse. visual odometry baseline robust slam implemented acquire sparse point clouds camera pose occurs frame. quantitative evaluation kitti underlying localization pipeline. real-time system yolo baseline method compute object likelihoods whereas off-line system subcnn either case result given window positive score class read penultimate layer. used compute likelihood generate proposals initialization discussed later. filter organization object represented ﬁlter class labels class-conditional ekfs class thus object represented mixture ekfs pruned describe later. maintains posterior estimate position scale orientation relative gravity. state predicts projection object onto image plane evaluates likelihood. object classes shape prior enforced pseudo-measurement uncertainty manually tuned expected class-variability. instance people parallelepipeds expected volume anisotropic covariance along coordinate axes range decimeters whereas couches signiﬁcantly uncertainty. data association avoid running baseline multiple times overlapping regions query sequentially prediction. instead once threshold obtain large number regions. efﬁcient create data association problem must attribute image regions object hypotheses multiple possible class labels avoid explicit data association opting simple heuristics instead ﬁrst generate predictions ﬁlter; occluded objects excluded likelihood evaluation. others generate four-tuple coordinates bounding -dimensional gaussian given projection current state. sloppy prediction image parallelepiped general axis-aligned rectangle image. nevertheless scoring likelihood produced predicted class. threshold used decide bounding used update object. bounding boxes lower likelihood given small weights ﬁlter update. requires accurate initialization describe below. silver lining inter-frame motion usually camera however determining visibility objects since contains topological information know space points empty occupied object void salient photometric features.to enable visibility computation point cloud together images compute dense shape objects maximum-likelihood sense using generic regularizers. done level accuracy efﬁciency needed live operation. alternative approximate shape objects parametric family instance cuboids ellipsoids compute visibility accordingly also leveraging co-visibility graph computed corollary slam system priors size aspect ratios objects. approximate implemented renditions program operating real-time demonstrated live june operating off-line used experiments reported sect. fig. sketches system chart. cases taken shortcuts improve efﬁciency approximation likelihood function implemented cnn. also semantic ﬁlter needs initialization data association requires heuristics computationally viable. describe heuristics order. table quantitative evaluation kitti comparison subcnn number true positives positional error angular error less threshold shown along precision recall. scores aggregated across ground-truth labeled frames dataset annotated objects. last rows discard orientation error. small data association proceeds smoothly unless multiple instances object class present nearby partially occlude other. initialization putative detections associated object used proposals initialization. object positioned weighted centroid sparse points whose projections within detection region. weight center largest decreases exponentially outwards. orientation initialized azimuth subcnn rotated according camera pose gravity. given position orientation scale optimized minimizing reprojection error. merge objects assumed simply-connected compact objects cannot occupy space. projected bounding boxes overlap. multiple instances object detected initialized propagated eventually merge overlap space sufﬁciently large. objects class allowed merge different classes appear co-located intersecting sloppy parallelepipedal shape model e.g. chair table. termination object maintains probability classes associated class-conditional ﬁlter. classes becomes dominant ﬁlters eliminated save computational cost. objects converge classes within iterations. objects disappear view retained state seen sufﬁciently long time stored long-term memory seen again. figure qualitative comparison subcnn. images back-projected objects method subcnn bottom top-view corresponding portion scene. ground truth shown blue. explained sec. choose subcnn paragon even though based single image because performer recognition kitti among non-anonymous reproducible ones particular dominates single-image based subcnn returns different results frame therefore naturally disadvantage. make comparison fair would average integrate detections object across frames visible. however subcnn provide data association making direct comparison challenging. make comparison fair possible without developing alternate aggregation method subcnn compare algorithm frame-by-frame basis. speciﬁcally frame transfer ground truth camera frame remove occluded objects. compare detections subcnn point estimate computed causally ﬁlter current time. call method ours-inst. hand beneﬁt aggregating temporal information long possible also report results based point-estimate ﬁlter state last time instant object seen. estimate mapped back current frame call ours-fnl. best knowledge known methods recognition causally update posterior estimates object identity/presence geometric attributes even naive temporal averaging method like straightforward abfigure evolution state ground-truth annotation ﬁrst seen cars estimated side-by-side; frames however fall place appears ﬂank ‘b’. time goes falls place cars appear ‘f.’ error pose relative ground truth appreciated qualitatively. quantitative results shown table many datasets image-based object detection provide ground truth. also object detection datasets using extra sensor data e.g. depth structured-light sensor. none provide inertial measurements except kitti whose object detection benchmark contains images exclude frames used subcnn training leaving validation frames. videos cover validation set. removing moving objects objects observed times instants order magnitude validation set. figure class-speciﬁc scale prior. real detected system unlike despite scoring high likelihood therefore detected image-based system time goes conﬁdence real increases online video kitti provides ground-truth object tracklets deﬁne true positives miss detections false alarms. true positive nearest detection ground truth object within speciﬁed error threshold position orientation miss occurs detection within threshold. false alarm occurs object detected despite true object within threshold distance orientation. precision fraction true positives detections recall percentage detected instances among true objects. objects characteristic scales lost perspective projection inferable inertial sensor. impose class-dependent prior size shape fig. detected image-based detector rejected system inconsistent scale prior fig. shows background cars ﬁeld whose images smaller detected correctly whereas rejected. table shows result kitti dataset averaged sequences. average ours-inst already outperforms subcnn even initialization rather inaccurate. note method requires evidence accumulated time claiming existence object scene ours-inst penalized heavily ﬁrst frames object spotted. ours-fnl improves results large margin. fig. shows method reﬁnes state time. visual comparison shown fig. ground truth ours-fnl subcnn system represents objects state even visible detected image-based detector. allows predicting re-appearance objects future frames resume update evidences appear. fig. shows chair ﬁrst detected occluded monitor later reappearing. system predicts chair completely occluded therefore image update chair resumes reappears time known chair previously seen sect. show phenomenon large-scale driving sequence. figure occlusion management short-term memory. chair detected later becomes occluded monitor projection onto image shown dashed lines indicating occlusion. model allows prediction dis-occlusion allows resuming update chair comes back view. online video fig. online video show results .km-long sequence kitti. contains hundreds cars along route. recognized replace bounding model similar aligned pose estimate ﬁlter manner similar however uses rgb-d data. sequence also cars different streets through walls previously detected help navigation. indoor sequences tested system live public demo operating real time cluttered environments people chairs tables monitors like. representative examples shown simpler scenes illustrative purposes fig. models objects rendered detected system produce exact orientation estimates seen fig. plenty room improvement. inertial sensors every modern phone tablet even many toys devices embedded physical space occasionally need interact makes sense exploit inertials along visual sensors help detecting objects exist physical space characteristic shape size addition appearance. recorded tremendous progress object detection recent years object means group pixels image. leverage progress design detector follows prescriptions indicated introduction. start deﬁning representation minimal sufﬁcient invariant statistic object attributes line marginalize camera euclidean pose allows enforce priors class-speciﬁc scale objects update measure bayesian ﬁlter note minimal sufﬁcient invariant localization attributed point cloud therefore need deploy machineries deep learning determine camera pose instead extended kalman filter conditioned update object attributes performed mixture-of-kalman ﬁlter. result system whereby objects ﬂicker inand-out existence conﬁdence presence grows accrued evidence know presence even temporarily occluded predict seen enforce known scale priors reject spurious hypotheses bottom-up proposal mechanism. made stringent admittedly restrictive assumptions order keep model viable real-time inference. could certainly relax assumptions obtain general models forgo ability operate real time. main limitation system restriction static objects. theory framework general geometry moving deforming objects represented therefore attributes remain limited inferred image. also representation objects’ shape rather rudimentary result visibility computation rather fragile. areas prime future development.", "year": 2016}