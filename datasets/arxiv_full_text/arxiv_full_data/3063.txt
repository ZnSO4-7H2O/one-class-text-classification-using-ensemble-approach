{"title": "Combination of Hyperband and Bayesian Optimization for Hyperparameter  Optimization in Deep Learning", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Deep learning has achieved impressive results on many problems. However, it requires high degree of expertise or a lot of experience to tune well the hyperparameters, and such manual tuning process is likely to be biased. Moreover, it is not practical to try out as many different hyperparameter configurations in deep learning as in other machine learning scenarios, because evaluating each single hyperparameter configuration in deep learning would mean training a deep neural network, which usually takes quite long time. Hyperband algorithm achieves state-of-the-art performance on various hyperparameter optimization problems in the field of deep learning. However, Hyperband algorithm does not utilize history information of previous explored hyperparameter configurations, thus the solution found is suboptimal. We propose to combine Hyperband algorithm with Bayesian optimization (which does not ignore history when sampling next trial configuration). Experimental results show that our combination approach is superior to other hyperparameter optimization approaches including Hyperband algorithm.", "text": "deep learning achieved impressive results many problems. however requires high degree expertise experience tune well hyperparameters manual tuning process likely biased. moreover practical many different hyperparameter conﬁgurations deep learning machine learning scenarios evaluating single hyperparameter conﬁguration deep learning would mean training deep neural network usually takes quite long time. hyperband algorithm achieves state-of-the-art performance various hyperparameter optimization problems ﬁeld deep learning. however hyperband algorithm utilize history information previous explored hyperparameter conﬁgurations thus solution found suboptimal. propose combine hyperband algorithm bayesian optimization experimental results show combination approach superior hyperparameter optimization approaches including hyperband algorithm. witnessing breakthrough brought deep learning various research topics computer vision natural language processing speech recognition; signiﬁcant impact many industries augmented reality self driving cars smart home devices name few. however design deep neural networks usually requires huge amount efforts even experienced deep learning researchers practitioners high complexity involved nature deep neural networks makes adoption application deep learning based solution even complex feasible hyperparameter conﬁgurations form huge space need choose optimal case evaluating even single hyperparameter conﬁguration point space often time consuming hyperparameters deep learning roughly divided types. type associated learning algorithms. example suppose chosen off-the-shelf deep neural network alexnet resnet designed network speciﬁcally problem; comes training network usually clear advance training dataset hand learning rate appropriate many epochs decrease learning much value need weight decay setting hyperparameters correctly often critical reaching full potential deep neural network chosen designed. type hyperparameters deep learning related design deep neural networks. example important design questions include many layers need network many ﬁlters given convolutional layer needs layer downsample size feature maps choosing different values hyperparameters would result different network architectures consequently would inﬂuence quality produced results them. deep learning community choosing appropriate values types hyperparameters usually accomplished manually largely depends one’s experience intuition hard reproduce results similar quality others would like apply different datasets problems. moreover manual tuning makes difﬁcult understand real ability deep learning model overestimate underestimate model happens choose good hyperparameter conﬁguration chance. systematic approach tune hyperparameters usually adopted machine learning community grid search. however applied certain hyperparameter optimization problems deep learning time-consuming. example suppose decided design network convolutional layers hyperparameters number ﬁlters assigned convolutional layers number ﬁlters restricted certain range e.g. would points total hyperparameter conﬁguration space; even grid length still points evaluated. would take unacceptable long time remember evaluating single point means training network till convergence. random search proposes randomly sample points hyperparameter conﬁguration space. although approach looks simple shown able comparable hyperparameter conﬁguration grid search deep learning using small fraction computation time used grid search. granted computational budget random search better conﬁgurations. state-of-the-art hyperband algorithm makes hyperparameter optimization much efﬁcient observing usually necessary allocate computational budget resources uniformly every hyperparameter conﬁguration; smarter allocate budget promising hyperparameter conﬁgurations eliminating less promising ones quickly. applied deep learning algorithm viewed early-stopping strategy often adapts tuning hyperparameters neural networks training learning curve look quite promising even poor would likely abandon current hyperparameter conﬁguration. more early-stopping principled would brieﬂy reviewed methodology section. major limitation hyperband algorithm treats hyperparameter conﬁguration points independent other. hyperparameter conﬁguration space usually smooth means knowing quality certain points might help infer quality nearby points. therefore would efﬁcient sample trial points independently according known actually bayesian optimization utilizes smoothness assumption. sampling next trial point balances exploitation exploration. exploitation means samples regions good quality trial points found nearby before exploration means also examines regions rarely visited before. notice unlike hyperband algorithm bayesian optimization still allocates computational budget uniformly causes efﬁciency issues. better hyperparameter conﬁgurations efﬁciently possible propose combine hyperband algorithm bayesian optimization. motivated fact methods complementary nature other. combining them able fully utilize learned history previous trial points time focus attention really promising ones avoid wasting time less promising ones. nutshell spirit combination approach quite straightforward simply follow entire process hyperband algorithm critical difference instead sampling trial points independently sample sequentially according bayesian optimization criterion balancing exploitation exploration. experimental results show combination approach outperforms state-of-the-art hyperband algorithm several deep learning problems hyperparameter optimization scenarios. results section present core algorithm. ﬁrst brieﬂy review critical ideas helpful understanding combination approach respectively hyperband algorithm bayesian optimization. illustrate details combine together. essential idea hyperband allocate resources promising hyperparameter conﬁgurations. first initializes trial points then uniformly allocates budget trial point evaluates performance given budget. proportion trial points worse performance deemed less promising thus removed procedure repeats several rounds single trial point remains set. idea based assumption trial point’s performance obtained earlier rounds good indicator ﬁnal performance. apparently resources trial point allocated reliable indicator given total ﬁnite budget resources roughly trial point allocated resources. large used trial point would allocated less resources thus earlier rounds performance reﬂect well ﬁnal performance. contrary small used although trial point likely allocated sufﬁcient resources reﬂect ﬁnal performance algorithm would suffer pool less candidates. alleviate dilemma hyperband actually tries different values returns best hyperparameter conﬁguration among them. hyperband algorithm presented algorithm details). function get_hyperparameter_conf iguration returns i.i.d. trial points predeﬁned conﬁguration space. function run_then_return_obj_val ﬁrst trains trial point amount resources returns objective function value evaluated training. function top_k ﬁrst ranks input trials according objective function value obj_vals returns performing trial points. main idea bayesian optimization follows. since evaluating objective function trial point expensive approximates using probabilistic surrogate model much cheaper evaluate iterative process algorithm samples next trial point according current surrogate model. evaluates updates surrogate model based data point goes back step bayesian optimization samples trial points sequentially trial point sampled utilizing information history namely sampled next actually determined sampled previously. bayesian optimization presented algorithm line next trial point sampled place optimizing acquisition function expected improvement equation usually used acquisition function literature trial point best objective function value found ﬁrst steps. note expected improvement acquisition function favors points high variance high mean value. namely balances exploration well explored regions exploitation visited regions conﬁguration space. particular types probabilistic models proposed surrogate compute expectation equation discriminative models like gaussian process directly models p|x) generative models like tree-structured parzen estimator models reported achieves better results although hyperband bayesian optimization powerful strength weakness. observe approaches actually complementary other namely weakness strength other vice versa illustrated next. therefore propose combine together obtain enhanced hyperparameter optimization algorithm. speciﬁcally hand trial points hyperband sampled independently. however trial point sampled objective function trial point evaluated data point tells something objective function behaves conﬁguration space. intuitively avoid sample trial points near subsequently; otherwise consider sampling trial points near note reasonable take actions condition objective function smooth conﬁguration space usually weak assumption wide spectrum hyperparameter optimization problems encountered deep learning. trial point hyperband sampled independently trial points sampled previously thus fails utilize lessons learned history result wasting precious time repeating errors already happened history. hand bayesian optimization trial point allocated sufﬁcient resources obtain ﬁnal objective function value deep learning allocating sufﬁcient resources single trial point usually means training deep neural network convergence take hours days even weeks using modern gpu. case cannot afford allocating whatever amount resources required single trial point obtain ﬁnal objective function value without knowing advance value likely best among trial points. analysis weakness approach actually perfectly complementary other. trial points hyperband sampled sequentially trial point sampled according experience obtained previously sampled evaluated trial points exactly bayesian optimization works. early stopping like mechanism used hyperband adopted bayesian optimization trials points already worse trial points early rounds abandoned since ﬁnal performance also less likely best among all. propose combine hyperband bayesian optimization straightforward manner follows. generally follow process hyperband algorithm. however difference that instead sampling trial points independently hyperband combination approach sample trial points using bayesian optimization. speciﬁcally ﬁrst round hyperband trial point sampled intermediate performance given current allocated resources evaluated. data point contributes updating surrogate probabilistic model bayesian optimization based next trial point sampled. illustrated algorithm bulk combination algorithm similar hyperband algorithm major difference line line basically bayesian optimization algorithm section empirically evaluate performance combination algorithm. conduct major sets experiments. experiments ﬁrst related deep learning problems mentioned hyperband paper. second experiment whose target decomposed neural network architecture balancing well accuracy speed object detection. experiments conducted caffe framework. combination algorithm choose bayesian optimization part supreme performance discriminative counterpart experiments compare performance combination algorithm named hyperband_tpe three baselines random search hyperband. experiment section train lenet mnist dataset using mini-batch sgd. four hyperparameters learning rate batch size number ﬁlters convolutional layers lenet. range feasible values hyperparameter could take follows hyperband paper. hyperband combination algorithm hyperband paper; empirically number training images corresponding unit mentioned hyperband paper. settings result equivalent number trial points either random search trial point consumes units hyperband paper make sure statistical signiﬁcance results runs algorithm times computes average result. save time algorithm times compute average. performance comparison four algorithms presented figure unit time axis multiple used hyperband paper make comparison hardware independent. reason curves hyperband combination start time takes time either algorithm abandon trial points algorithm produces output hyperparameter conﬁguration. note since problem easy namely many different hyperparameter conﬁgurations produce trained lenet pretty high accuracy mnist four approaches quickly converges high level learn simple problems good hyperparameter conﬁgurations reside massively searching space sophisticated hyperparameter optimization approach needed. however show later experiments problems complicated sophisticated approaches clearly advantages less sophisticated ones. three experiments section train alexnet respectively three datasets cifar rotated mnist background images street view house number eight hyperparameters initial learning rate penalty respectively three convolutional layers fully connected layer learning rate reduction scale power local response normalization. range feasible values hyperparameter hyperband paper. clearly harder problems previous terms complexity neural network hyperparameter searching space datasets. cifar mrbi svhn three experiments. numbers hyperband paper. still number training images corresponding unit empirically. settings result equivalent number trial points either random search trial point consumes units algorithm times compute average accuracy. performance comparison four algorithms presented figure still unit time axis multiple used. unlike lenet mnist three problems many hyperparameter conﬁgurations produce high accuracy reﬂected separation curves. curves respectively random search intertwine other curve hyperband much higher previous two. although hyperband forms strong baseline combination algorithm outperforms slightly. also problem becomes harder harder svhn cifar mrbi combination algorithm hyperband also become larger. note although might possible given sufﬁcient even inﬁnite resources hyperparameter optimization algorithms ﬁnally converge similar high accuracy target game better solution quickly possible. results validate idea combining strength hyperband bayesian optimization also show combination algorithm bigger advantage approaches problems become difﬁcult. section target problem rank decomposed approximation neural network single shot detector balance well accuracy speed. fully convolutional neural network convolutional layers objects detection. achieves real time detection speed high like titan competitive accuracy. however like amazon instances real time detection cannot achieved ssd. therefore needed speed computation probably cost sacriﬁcing certain accuracy. rank regularization technique proposed decomposes shape weights existing trained convolutional layer rank approximation removing redundant information contained tensor formed ﬁlters convolutional layer svd. produced rank approximation usually quicker original convolutional layer. speciﬁcally suppose convolutional layer ﬁlters channels size spatially decomposed convolutional layers first ﬁlters channels size spatially; second ﬁlters channels size spatially. hyperparameter controls degree information compression smaller value aggressively layer compressed thus quicker less accurate resulted approximation apply rank regularization technique since convolutional layers hyperparameter conﬁguration space dimension i.e. dimension corresponds value convolutional layers whose range original number ﬁlters layer. experiment objective function single metric previous experiments combination metrics accuracy speed parameter balancing metrics. test accuracy network corresponding hyperparameter conﬁguration pascal dataset instead directly reporting accuracy decomposed approximation network actually ﬁnetune small number epochs allow chance adapt better report accuracy ﬁnetuning. hyperband combination algorithm number images corresponding unit accordingly match epochs ﬁnetuning. settings result equivalent number trial points either random search trial point consumes units algorithm experiment consideration total running time. performance comparison four algorithms presented figure still unit time axis multiple used. time facing deeper network higher dimensionality hyperparameter conﬁguration space larger complex dataset hard imagine conﬁgurations good performance become even rarer again combination algorithm shows better performance approaches. proposed novel approach hyperparameter optimization deep learning. approach combines strength hyperband algorithm bayesian optimization. sampled trial points balancing exploitation exploration allocating computational budget promising trial points. experimental results validated combination approach could better hyperparameter conﬁguration quickly approaches random search bayesian optimization hyperband. more results also indicated approach outperforms approaches larger margin hyperparameter optimization problem become complex difﬁcult.", "year": 2018}