{"title": "Nonparametric Relational Topic Models through Dependent Gamma Processes", "tag": ["stat.ML", "cs.CL", "cs.IR", "cs.LG"], "abstract": "Traditional Relational Topic Models provide a way to discover the hidden topics from a document network. Many theoretical and practical tasks, such as dimensional reduction, document clustering, link prediction, benefit from this revealed knowledge. However, existing relational topic models are based on an assumption that the number of hidden topics is known in advance, and this is impractical in many real-world applications. Therefore, in order to relax this assumption, we propose a nonparametric relational topic model in this paper. Instead of using fixed-dimensional probability distributions in its generative model, we use stochastic processes. Specifically, a gamma process is assigned to each document, which represents the topic interest of this document. Although this method provides an elegant solution, it brings additional challenges when mathematically modeling the inherent network structure of typical document network, i.e., two spatially closer documents tend to have more similar topics. Furthermore, we require that the topics are shared by all the documents. In order to resolve these challenges, we use a subsampling strategy to assign each document a different gamma process from the global gamma process, and the subsampling probabilities of documents are assigned with a Markov Random Field constraint that inherits the document network structure. Through the designed posterior inference algorithm, we can discover the hidden topics and its number simultaneously. Experimental results on both synthetic and real-world network datasets demonstrate the capabilities of learning the hidden topics and, more importantly, the number of topics.", "text": "previously browsed webpages person. commonly accepted successful understand corpus discover hidden topics corpus revealed hidden topics could improve services ieee ability search browse visualize academic papers help organization understand resolve concerns employees; help internet browsers understand interests person provide accurate personalized services. furthermore normally links documents corpus. paper citation network example document network academic papers linked citation relations; email network document network emails linked reply relations; webpage network document network webpages linked hyperlinks. since links also express nature documents apparent hidden topic discovery consider links well. similar studies focusing hidden topics discovering document network using relational topic models already successfully developed. unlike traditional topic models focus mining hidden topics document corpus make discovered topics inherit document network structure. links documents considered constrains hidden topics. drawback existing rtms built ﬁxed-dimensional probability distributions dirichlet multinomial gamma possion distribution require dimensions ﬁxed use. hence number hidden topics must speciﬁed advance normally chosen using domain knowledge. difﬁcult unrealistic many real-world applications rtms fail number topics document network. order overcome drawback propose nonparametric relational topic model paper removes necessity ﬁxing topic number. instead probability distributions stochastic processes adopted proposed model. stochastic process simply considered ‘inﬁnite’ dimensional distributions. order express interest document abstract—traditional relational topic models provide discover hidden topics document network. many theoretical practical tasks dimensional reduction document clustering link prediction beneﬁt revealed knowledge. however existing relational topic models based assumption number hidden topics known advance impractical many real-world applications. therefore order relax assumption propose nonparametric relational topic model paper. instead using ﬁxed-dimensional probability distributions generative model stochastic processes. speciﬁcally gamma process assigned document represents topic interest document. although method provides elegant solution brings additional challenges mathematically modeling inherent network structure typical document network i.e. spatially closer documents tend similar topics. furthermore require topics shared documents. order resolve challenges subsampling strategy assign document different gamma process global gamma process subsampling probabilities documents assigned markov random field constraint inherits document network structure. designed posterior inference algorithm discover hidden topics number simultaneously. experimental results synthetic real-world network datasets demonstrate capabilities learning hidden topics importantly number topics. xuan centre quantum computation intelligent systems school software faculty engineering information technology university technology sydney australia school computer engineering science shanghai university china zhang centre quantum computation intelligent systems school software faculty engineering information technology university technology sydney australia ‘inﬁnite’ number topics assign document gamma process inﬁnite components. additional requirement gamma process assignment linked documents tendency share similar topics. common feature found many real-world applications many literatures exploited property work. order achieve requirement formally deﬁned properties relational topic model document network satisfy. first global gamma process represent base components shared documents. important users interested analyzing documents database without sharing common topics model achieves deﬁned properties through thinning global gamma process document-dependent probabilities; adding markov random field constraint thinning probabilities retain network structure. finally assign document gamma process inherits content document link structure. sampling algorithms designed learn proposed model different conditions. experiments document networks show efﬁciency learning hidden topics superior performance model’s ability learn number hidden topics. worth noting that although document networks examples throughout paper work applied networks node features. design sampling inference algorithms proposed model truncated version exact version. rest paper structured follows. section summarizes related work. proposed model presented section illustrated detailed derivations sampling inference section section presents experimental results synthetic real-world data. finally section concludes study discussion future directions. section brieﬂy review related work paper. ﬁrst part summarizes literature relational topic models. second part summarizes literatures nonparametric bayesian learning. work paper aims model data network structure constraint. since social network citation network explicit commonly-used networks data mining machine learning areas extensions traditional topic models adapt networks. social network authorrecipient-topic model proposed analyze categories roles social networks based relationships people network. similar task investigated social network structure inferred informal chat-room conversations utilizing topic model ‘noisy links’ ‘popularity bias’ social network addressed properly designed topic model important issue social network analysis communities extracted using social topic model mixed membership stochastic blockmodel another learn mixed membership vector node network structure consider content/features node. citation network relational topic model proposed infer topics discriminative topics hierarchical topics citation networks introducing link variable linked documents. unlike block adopted model link document considering physical meaning citation relations variable introduced indicate content citing paper inherited cited paper order keep document structure markov random field combined topic model communities citation network also investigated topic models inherited traditional topic models number topics needs ﬁxed. unrealistic many real-world situations number advance. work tries resolve issue nonparametric learning techniques reviewed following subsection. since ﬁnite relational topic models comparative model introduce relational topic model detail. corresponding graphical representation shown fig. generative process follows topic distribution document word distribution topic topic index word document observed word document variables original different signiﬁcant part variable denotes observed document link. model uses generalized linear model model generation document links. components used generate data. another inﬁnite mixture model inﬁnite gaussian mixture model normally gaussian mixture model used continuous variables dirichlet process mixture used discrete variables. example dirichlet process hierarchical topic model composed latent dirichlet allocation nested chinese restaurant process using nested chinese restaurant process prior number ﬁxed topics model also hierarchically organized. order learn dirichlet process mixture based models inﬁnite property inference methods properly designed. popular successful methods this markov chain monte carlo variational inference summarize nonparametric learning successfully used extending many models applied many real-world applications. however still work nonparametric extension relational topic models. paper uses gamma processes extend ﬁnite relational topic model inﬁnite one. iii. nonparametric relational topic model section present proposed nonparametric relational topic model detail. model handle issue number topics needs deﬁned. proposed model uses gamma process express interest document inﬁnite hidden topics. gamma process stochastic process base measure parameter concentration parameter. also corresponds complete random measure. random realization gamma process product space then indicator function satisﬁes improper gamma distribution parameters gamma using express document interest {θi)}∞ denotes inﬁnite number topics {πi)}∞ denotes weights inﬁnite number topics document. illustrated fig. idea assign document gamma process. assignment satisfy following properties property gamma processes documents link similar components/topics other. property gamma processes documents share components/topics. nonparametric bayesian learning approach learning number mixtures mixture model without predeﬁning number mixtures number supposed inferred data i.e. data speak. traditional elements probabilistic models ﬁxed-dimensional distributions gaussian distribution dirichlet distribution logistic normal distribution distributions need predeﬁne dimensions. order avoid this gaussian process dirichlet process used replace former ﬁxed-dimensional distributions inﬁnite properties. since data limited learned/used atoms also limited even ‘inﬁnite’ stochastic processes. dirichlet process seen distribution distributions. since sample dirichlet process deﬁnes bunch variables satisﬁes dirichlet distribution dirichlet process good alternative models dirichlet distribution prior. three different methods construct process black-macqueen schema chinese restaurant process stick breaking process although processes result dirichlet processes express different properties dirichlet process posterior distribution black-macqueen schema clustering chinese restaurant process formal sampling function stick breaking process. based constructive processes dirichlet process mixture proposed kind inﬁnite mixture models. inﬁnite mixture models extension finite mixture models ﬁnite number hidden order make linked documents similar gamma processes deﬁne subsampling markov random field constrain documents deﬁnition subsampling probabilities documents component/topic global gamma process following constraint subsampling constraint marginal distribution subsampling probability dependents values neighbors. therefore linked documents similar ensures proposed achieve property fig. illustration gamma process assignments document network. document assigned gamma process inﬁnite components fence denotes hidden topic examples shown ﬁgure. length fences denote weights different topics document. θk}∞ shared global components/topics documents. hope components gamma process document falls within components/topics global gamma process. dependent thinned gamma process achieve goal. deﬁnition follow give bernoulli prior apparently different realizations {ri} lead different gamma processes. furthermore dependence different realizations {ri} also lead dependence generated gamma processes. here gibbs sampling method samples posterior distribution truncation also adopt slice sampling technique develop exact sampling without truncation. difﬁcult perform posterior inference inﬁnite mixtures common work-around solution nonparametric bayesian learning truncation method. method widely accepted uses relatively maximum number topics. given conditional distribution efﬁcient sampling developed recently conditional distribution dec+− composed parts ﬁrst part easily sampled using beta distribution second part bounded function. sampling wdnk wdnk although truncated method commonly accepted literature maintaining large number components parameters time space consuming. elegant idea resolve problem introducing additional variables adaptively truncate/select inﬁnite components. note introduced additional variables. original model appearances sampling without help truncation level. whole slice sampling algorithm summarized algorithm section evaluate effectiveness proposed model learning hidden topics document networks. first small synthetic dataset demonstrate model’s ability recover number available topics dataset. show usefulness using real-world datasets. generated synthetic data explore nrts ability infer number hidden topics document network. chose ground truth numbers symbolised refer number topics documents keywords respectively. then generate global topics -dimensional dirichlet distribution parameterized next generate document interests topics k-dimensional dirichlet distribution parameterized topics document interests topics generate document follows document chosen number word document ﬁrstly draw topic document’s interest draw word selected topic. finally obtain matrix rows documents columns words entry matrix denotes frequency particular word particular document. next step generate relations documents. pairs documents compute inner product topic distributions. order sparsify relationships retain ones inner product greater here adjust values generate synthetic datasets. distributions learned topic numbers proposed algorithms shown fig. subﬁguers ﬁrst column truncated version algorithm subﬁguers second column slice version algorithm subﬁgure counts topic numbers iterations illustrated charts. despite rough initial guess recovered histogram appears similar ground truth value small variance. sampled plotted across gibbs iterations shows markov chain begin well around samples. real-world datasets used cora dataset cora dataset consists scientiﬁc publications. citation network consists links. publication dataset described /-valued word vector indicating absence/presence corresponding word dictionary. dictionary consists unique words. citeseer dataset citeseer dataset consists scientiﬁc publications. citation network consists links. publication dataset also described /-valued word vector indicating absence/presence corresponding word dictionary. dictionary consists unique words dataset -fold cross validation evaluate performance proposed model comparing relational topic model. whole dataset equally split parts. stage documents part chosen testing rest four parts used training. used implementation fig. results synthetic data. left sub-ﬁgures denote distribution active topic number slice version; right sub-ﬁgures denote distribution active topic number truncated version. sub-ﬁgure ground-truth topic number given bars represent frequencies possible active topic number. dtest number test documents dtrain number training documents number word document link otherwise. denotes learned topic distribution training document topic distribution word evaluated {θ}k learned topics. expresses interest word topics expresses interest training document topics inner product used evaluate probability link. consider normalization since inﬂuence order quantitatively compare proposed model evaluation metrics designed real-world datasets link prediction document prediction. link prediction used predict links test training documents using learned topics. despite success existing relational topic models discovering hidden topics document networks based unrealistic assumption many real-world applications number topics easily predeﬁned. order relax assumption presented nonparametric relational topic model. proposed model stochastic processes adopted replace ﬁxed-dimensional probability distributions used existing relational topic models lead necessity pre-deﬁning number topics. time introducing stochastic processes leads difﬁculty model inference therefore also presented truncated gibbs slice sampling algorithms proposed model. experiments synthetic dataset real-world dataset demonstrated method’s ability inference hidden topics number. future interested making sampling algorithm scalable large networks using network constrain methods instead mrfs. current mrf-based methods make inference efﬁcient enough. believe network constraint methods avoid issue. research work reported paper partly supported australian research council discovery grant china scholarship council. work jointly supported national science foundation china grant no.. blei jordan latent dirichlet allocation journal machine learning research vol. blei probabilistic topic models communications vol. apr. available http//doi.acm.org/./. mccallum wang corrada-emmanuel topic role discovery social networks experiments enron academic email. journal artiﬁcial intelligence research vol. word prediction basic idea test document similar interest topics linked training documents words generated according interest. evaluation equation number neighbors document has. results cora dataset shown fig. results citeseer dataset shown fig. compared several settings. clarity denote rtmnum. example means note slice version algorithm used implementation nrt. reason slice version efﬁcient truncated version slice version need keep large number hidden topics memory notice algorithm mixed better settings generally compatible rest. shown left subﬁgures group data likelihood model generally larger various settings. means proposed model explains data better rtm. synthetic case also plot distribution compared method terms link word prediction. terms word prediction algorithm consistently outperform every category. terms link prediction nrt’s performance universally better noticed less accurate results settings. trend link prediction respective topic number rtm. trend comes evaluation equation smaller topic number tends bigger provability observed links also observed extreme situation reaches best performance link prediction. problem choose hidden topic number rtm. take cora dataset example. candidates possible topic number least within however proposed model active topic number automatically learned data without prior domain knowledge topic number achieve relatively good results link prediction considering large range terms overall result argue absence accurate domain knowledge value algorithm allowed achieving better robust performance compared current state-of-the-art methods. fig. results different setting ﬁrst -fold cora dataset. ﬁrst subﬁgure shows log-likelihood along iterations; second subﬁgure learned distribution active topic number; third subﬁgure comparison link prediction task; fourth subﬁgure comparison word prediction task. social-network analysis using topic models proceedings international sigir conference research development information retrieval ser. sigir york c.-c. hsieh incorporating popularity topic models social network analysis proceedings international sigir conference research development information retrieval ser. sigir york airoldi blei fienberg xing mixed membership stochastic blockmodels journal machine learning research vol. jun. available http//dl.acm.org/citation.cfm?id=. nallapati ahmed xing cohen joint latent topic models text citations proceedings sigkdd international conference knowledge discovery data mining ser. york chen mitra giles detecting topic evolution scientiﬁc literature citations help? proceedings conference information knowledge management ser. cikm york niculescu-mizil gryc topic-link joint models topic author community proceedings annual international conference machine learning ser. icml york inﬁnite gaussian mixture model advances information processing systems solla leen m¨uller eds. press http//papers.nips.cc/paper/ -the-inﬁnite-gaussian-mixture-model.pdf topic models nested chinese restaurant process advances neural information processing systems vol. neal markov chain sampling methods dirichlet process mixture models journal computational graphical statistics vol. maddison tarlow minka sampling advances information processing systems ghahramani welling cortes lawrence weinberger eds. curran associates inc. available http//papers.nips.cc/paper/-a-sampling.pdf namata bilgic getoor gallagher eliassi-rad collective classiﬁcation network data magazine vol. junyu xuan received bachelor’s degree china university geosciences beijing. currently working toward dualdoctoral degree shanghai university university technology sydney. main research interests include machine learning complex network mining. full professor associate dean faculty engineering information technology university technology sydney. research interests area decision support systems uncertain information processing. published research books papers australian research council discovery grants grants. received university research excellent medal serves editor-in-chief knowledge-based systems editor-in-chief international journal computational intelligence systems editor book series intelligent information systems guest editor special issues international journals well delivered keynote speeches international conferences. guangquan zhang associate professor faculty engineering information technology university technology sydney australia. applied mathematics curtin university technology australia. department mathematics hebei university china lecturer associate professor professor. main research interests area multi-objective bilevel group decision making decision support system tools fuzzy measure fuzzy optimization uncertain information processing. published four monographs four reference books papers refereed journals conference proceedings book chapters. four australian research council discovery grants many research grants. richard received b.eng. degree computer engineering university south wales sydney australia ph.d. degree computer sciences university technology sydney sydney australia currently senior lecturer school computing communications uts. current research interests include machine learning computer vision statistical data mining. xiangfeng professor school computers shanghai university china. currently visiting professor purdue university. received master’s degrees hefei university technology respectively. postdoctoral researcher china knowledge grid research group institute computing technology chinese academy sciences main research interests include wisdom cognitive informatics text understanding. authored co-authored publications publications appeared ieee trans. automation science engineering ieee trans. systems cybernetics-part ieee trans. learning technology concurrency computation practice experience generation computing etc. served guest editor transactions intelligent systems technology. also served committees number conferences/workshops including program co-chair icwl wism ctuw members conferences workshops.", "year": 2015}