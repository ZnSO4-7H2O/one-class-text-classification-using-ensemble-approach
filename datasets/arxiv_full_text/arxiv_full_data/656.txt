{"title": "Adapting Deep Network Features to Capture Psychological Representations", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments.", "text": "deep neural networks become increasingly successful solving classic perception problems object recognition semantic segmentation scene understanding often reaching surpassing human-level accuracy. success part ability dnns learn useful representations high-dimensional inputs problem humans must also solve. examine relationship representations learned networks human psychological representations recovered similarity judgments. deep features learned service object classiﬁcation account signiﬁcant amount variance human similarity judgments animal images. however features capture qualitative distinctions part human representations. remedy this develop method adapting deep features align human similarity judgments resulting image representations potentially used extend scope psychological experiments. keywords deep learning; neural networks; psychological representations; similarity resurgence neural networks form deep learning continued dominate object recognition benchmarks ﬁeld computer vision often attaining near human-level accuracy variety perceptual tasks notably recent advances classifying thousands objects within natural images part success models ability learn effective feature representations high-dimensional inputs challenge human perception must also confront result cognitive scientists started explore representations learned networks used models human behavior perceptual tasks predicting memorability objects images predicting judgments category typicality deep learning models continue mimic growing list human-like abilities number core questions remain unanswered relevance models actual human cognition perception. instance features input learned using networks excel predicting certain human judgments feature representations related human psychological representations? ﬁrst glance would seem ability representations predict typicality judgments stimulus memorability would constitute robust evidence relevance people however recent work shown neural networks understanding relationship representations found deep learning humans important question cognitive science could potentially beneﬁt artiﬁcial intelligence. however independent question simply good approximation people represent images would allow cognitive scientists test psychological theories using complex realistic stimuli. indeed tasks creating stimulus sets uniformly span psychological space trivial. paper address question directly examining well features extracted state-of-the-art deep neural networks predict human similarity judgments. initial evaluation shows features account signiﬁcant amount variance human judgments fail capture qualitative distinctions human representations. develop method adapting deep network features better predict human similarity judgments show approach reproduce qualitative distinctions. results suggest features produced deep learning suitable modeling cognition modiﬁed bring close alignment human representations. general deep neural networks neural networks depth terms number hidden layers between input output past years training networks understand aspects large complex data sets number advances vision language applications computer vision majority progress driven particular called convolutional neural network cnns name convolutional layers learn image ﬁlters produce feature maps spatially-organized inputs like images. allows drastic decrease number parameters network must learn would otherwise explode exponentially fully connected network high-dimensional inputs. typical architecture includes series hidden convolutional layers followed smaller number fully connected layers ﬁnally layer generates ﬁnal output classiﬁcation. cnns initially developed decades came mainstream popularity -layer architecture named alexnet imagenet large scale visual recognition challenge reducing previous winner’s error rate uncommonly large margin. since then deeper contest every year currently dominated microsoft’s -layer network obtained best-of-top error rate surpassing accuracy non-expert humans interestingly cnns produce much outputs also return feature representations layer network. deep representations learned networks proven useful predicting human behavior. dubey peterson khosla yang ghanem used representations extracted last fully-connected layer predict intrinsic memorability objects. objects humans jointly likely remember forget large complex natural scene database. correlation estimates memorability original memorability scores object matched human consistency similarly lake zaremba fergus gureckis able reliably predict human typicality ratings eight object categories using network features called cognitive scientists attention deep learning since categorization foundational problem ﬁeld. deep representations also beginning interest neuroscience community. example activations used predict monkey cortex activity well lowhigh-level activity human visual areas delving deeper khaligh-razavi kriegeskorte found best explained cortex representations well-known models computer vision neuroscience ﬁelds although model completely explained variance unsupervised models worst them. although representations currently best predicting neural activity measured blood oxygenation level dependent response guarantee explain psychological representations result. fact partly successful predicting human similarity judgments cortex representations. however categorical distinctions human representations well predicted human cortex representations similar monkey cortex representations human psychological representations. remainder paper similar approach evaluate well deep network features align human psychological representations explore correspondence increased. ﬁrst step evaluate potential correspondence deep network features psychological representations. unlike neural representations psychological representations cannot measured directly. however spatial hierarchical psychological representations objects recovered given matrix similarity judgments using methods multidimensional scaling hierarchical clustering thus reduce problem capturing human similarity judgments subjecting human judgments model predictions different methods extracting representations. approach problem taking inner-product deep feature representations pair images compute correlation pairwise vector similarities human similarity judgments stimulus pairs gives measure correspondence want evaluate. stimuli. stimulus consisted color photographs animals images cropped pixels resulting close-ups either animal’s face body. constructed include interintraspecies variation. behavioral experiment. collected pairwise similarity ratings animal stimulus amazon mechanical turk. participants instructed rate similarity four pairs animal images scale paid workers four comparisons. task eight examples shown help prevent bias early judgments. amazon workers could repeat task animal pairs many times wanted. possible image comparisons rated unique participants total ratings different participants. result similarity matrix averaging judgments. feature extraction. extracted features image data using three different popular off-the-shelf cnns varying complexity pretrained caffe speciﬁcally used caffenet googlenet layer depths respectively. googlenet achieve roughly half error rates alexnet. network already trained classify object categories previous ilsvrc competitions. feedforward pass ﬂattened image vector network yields feature responses layer. analysis extracted last layer network classiﬁcation layer. caffenet -dimensional fully-connected layer last quantifying discrepancy deep human representations attempt bring closer alignment. first consider ﬁnal hidden layer feature representation neural network thought input ﬁnal linear classiﬁcation layer problem solved ﬁnal weight matrix linear transformation thought rescaling ﬁnal stimulus representation solve categorization problem. suggests think features extracted network static representation ingredients transformation solves problem. thinking terms show easily solve linear transformation better captures human similarity judgments. formulation similar employed additive clustering models wherein represents binary feature identity matrix model similarity). used continuous features approach akin factor analysis. given existing feature-by-object matrix diagonal solved using linear regression predictors similarity product values feature objects identity matrix reduces model evaluated previous section. result convex optimization problem solved straightforwardly allowing transformation deep features closer correspondence human similarity judgments. layer googlenet -dimensional average pooling layer. lastly also extracted histograms oriented gradients scale-invariant feature transform representations comparison since features represent generic representations choice tasks computer vision prior popularity deep learning. results table gives performance model. representations three networks show medium high correlations human data. general deeper networks better imagenet classiﬁcation accuracy like googlenet better caffenet considerbly shallow. hog+sift baseline surprisingly poorly explaining little variance compared deep representations suggesting features useful many computer vision tasks differ large part representations humans employ judging animal similarity. although representation explained fair amount variance analyses revealed crucial structural aspects human representations preserved. ﬁrst second panels figure show multidimensional scaling solutions original human data predictions unaltered deep representations. structure solutions predicted judgments looks reasonable major categorical divisions preserved. hierarchical clusterings actual predicted human judgments show similar pattern results human judgments exhibit several major categorical divisions whereas much structure lost predicted data. figure multidimensional scaling solutions similarity matrices obtained human judgements non-transformed deep representations transformed deep representations figure hierarchical clustering human judgements deep representations transformed representations human judgments resulted nine interpretable clusters grouped color semantic category label panel. leaves deep transformed representation clusterings color-coded relative human judgments. analysis. large number predictors regularization critical avoid overﬁtting. used ridge regression performed grid search crossvalidated generalization performance best regularization parameter. predicted upper triangle similarity matrix since matrix symmetric. model evaluated generalization performance fold cross-validation. feature vectors extracted layer network. additional control overﬁtting compared model performance several baselines. baseline shufﬂed rows feature matrix feature representation image replaced different randomly chosen image. baseline columns feature matrix randomly permuted separately. lastly baseline simply combined shufﬂing schemes ﬁrst baselines. three cases randomized feature matrices subjected analyses true features allowing check spurious correlations. results table shows performance network using adjustment representations. values reported average values across folds crossvalidation. models performed considerably well showing improvement original non-weighted models. notably performed best accounting variance. training using estimated regularization parameter entire dataset yielded contrast three baseline models explained essentially variance suggesting results spurious correlations resulting large sets predictors. crucially solution improved predictions almost identical original human spatial representation. improvements found hierarchical clusterings actual predicted similarity matrices time largely form top-level parent nodes. feature analysis. higher layers cnns tend produce generic high-level features domain transfer across image applications choice feature depth ultimately dependent task implies layer responses different depths explain different types human similarity judgments examined model’s performance predicting similarity judgments function feature depth using caffenet given straightforward architecture manageably-sized layers. speciﬁcally compared performance across last three convolutional layers last fully-connected layers. results shown figure performance appear correspond strongly layer depth although fully connected layers perform much better convolutional layers suggesting human similarity judgments explained well simpler image features. reweighted classiﬁcation. investigated effect ﬁne-tuned representations separate animal classiﬁer using animal data consisting images animal classes used multinomial logistic regression -fold cross-validation classify animals using ﬁne-tuned representations predictors. ﬁne-tuned representations pairwise multiplying original representations square-root weights obtained prediction human similarity data. however weights original solution negative used elastic regression solve weights constrained positive. model using original unaltered representation serve baseline performance. original model performed well whereas ﬁne-tuned model performed consistently worse analysis constitutes ﬁrst formal comparison deep representations human psychological representations. initial results using currently high-performing convolutional neural networks show representations moderately correlated diverge terms crucial structural characteristics problem exhibited similar experiments method overcoming problem parsimonious adjustment feature representation inspired classic model similarity appears largely successful. indeed human representations almost completely reconstructed adjusted features. using features extracted deep convolutional neural networks provides opportunity estimate psychological representations real sensory inputs however potential limitation work generalizability transformation acquired broader stimulus contexts. testing question require replication transfer across several domains. extent established envision method standard tool studying cognitive science using natural stimulus sets modern artiﬁcial intelligence. beyond this potential interface between cognitive science artiﬁcial intelligence exploited beneﬁt each. attempt improve common categorization objective computer vision using human-tuned representations successful raise interesting distinctions computational problems solved humans cnns. full breadth human categorization behavior exhibits complex patterns overlapping class assignments likely well-represented learning objective deﬁned images objects characterized single label. further might poor categorization performance one-versus-all kind price paid ﬂexible system categorization respect complex objects partitioned using several good conﬁgurations depending context task hand. given possibility careful equate classiﬁcation performance human categorization abilities general.", "year": 2016}