{"title": "Agent-Agnostic Human-in-the-Loop Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.", "text": "providing reinforcement learning agents expert advice dramatically improve various aspects learning. prior work developed teaching protocols enable agents learn efﬁciently complex environments; many methods tailor teacher’s guidance agents particular representation underlying learning scheme offering effective specialized teaching procedures. work explore protocol programs agent-agnostic schema human-in-the-loop reinforcement learning. goal incorporate beneﬁcial properties human teacher reinforcement learning without making strong assumptions inner workings agent. show represent existing approaches action pruning reward shaping training simulation special cases schema conduct preliminary experiments simple domains. central goal reinforcement learning design agents learn fully autonomous way. engineer designs reward function input/output channels learning algorithm. then apart debugging engineer need intervene actual learning process. fully autonomous learning often infeasible complexity real-world problems difﬁculty specifying reward functions presence potentially dangerous outcomes constrain exploration. consider robot learning perform household chores. human engineers create curriculum moving agent simulation practice environments real house environments. time tweak reward functions heuristics sensors state action representations. intervene directly real-world training prevent robot damaging itself destroying valuable goods harming people interacts with. example humans design learning agent also loop agent’s learning process typical many learning systems. self-driving cars learn humans ready intervene dangerous situations. facebook’s algorithm recommending trending news stories humans ﬁltering inappropriate content examples agent’s environment complex non-stationary wide range damaging outcomes applied increasingly complex real-world problems interactive guidance critical success systems. prior literature investigated people help agents learn efﬁciently different methods interaction often human’s role pass along knowledge relevant quantities problem like q-values action optimality true reward particular state-action pair. person bias exploration prevent catastrophic outcomes accelerate learning. existing work develops agent-speciﬁc protocols human interaction. protocols human interaction advice designed speciﬁc algorithm instance grifﬁth investigate power policy advice bayesian q-learner. works assume states take particular representation action space discrete ﬁnite. making explicit assumptions agent’s learning process enable powerful teaching protocols leverage insights learning algorithm representation. goal develop framework human-agent interaction agent-agnostic capture wide range ways human help agent. setting informative structure general teaching paradigms relationship interplay pre-existing teaching methods suggestive teaching methodologies discuss section additionally approaching human-in-the-loop maximally general standpoint help illustrate relationship requisite power teacher teacher’s effectiveness learning. instance demonstrate sufﬁcient conditions teacher’s knowledge environment enable effective action pruning arbitrary agent. results form informative general structure teaching agents. make simplifying assumptions. first consider environments state fully observed; learning agent interacts markov decision process second note conducting experiments actual human loop creates huge amount work human slow training unacceptable degree. reason focus programatic instantiations humans-in-the-loop; person informed task question write program facilitate various teaching protocols. obvious disadvantages agent-agnostic protocols. agent specialized protocol unable human informative questions observation model faithfully represents process human uses generate advice likewise human cannot provide optimally informative advice agent don’t know agent’s prior knowledge exploration technique representation learning method. conversely agent-speciﬁc protocols perform well type algorithm environment poorly others. many cases without hand-engineering agent-speciﬁc protocols can’t adapted variety agent-types. researchers tackle challenging problems tend explore large space algorithms important structural differences model-based model-free approximate optimal policy others value function takes substantial effort adapt advice protocol algorithm. moreover advice protocols learning algorithms become complex greater modularity help limit design complexity. framework interaction person guiding learning process agent environment formalized protocol program. program controls channels agent environment based human input pictured figure gives teacher extensive control agent extreme case agent prevented interacting real environment entirely interact simulation. time require human interact agent learning protocol program—both agent environment black human. environment speciﬁed tuple state space action space denotes transition function probability distribution states given state action reward function discount factor. agent function human receive send advice information ﬂexible type xout treat human function xout. example might contain history actions states rewards proposed action xout might action well either equivalent different assume human knows general terms responses used making good-faith effort helpful. interaction environment agent human advisor sets mechanism design problem design interface orchestrates interaction components combined system maximizes expected γ-discounted rewards environment? words write protocol program take place given agent achieves higher rewards making efﬁcient information gained sub-calls formalizing existing techniques programs facilitate understanding comparison techniques within common framework. abstracting particular agents environments better understand mechanisms underlying effective teaching reinforcement learning developing portable modular teaching methods. naturally protocol programs cannot capture advice protocols. protocol depends prior knowledge agent’s learning algorithm representation priors hyperparameters ruled out. despite constraint framework capture range existing protocols human-in-the-loop guides agent. figure shows human manipulate actions sent environment agent’s observed states observed rewards points following combinatorial protocol families human manipulates components inﬂuence learning ﬁrst three elements correspond state manipulation action pruning reward shaping protocol families. remaining elements represent families teaching schemes modify multiple elements agent’s learning; protocols introduce powerful interplay different components hope future work explore. demonstrate simple ways protocol programs instantiate typical methods intervening agent’s learning process. figure many schemes human guidance algorithms expressed protocol programs. programs interface agent safer efﬁcient learners making human advice reward shaping section deﬁned reward function part however humans generally don’t design environment design reward functions. usually reward function handcoded prior learning must accurately assign reward values state agent might reach. alternative human generate rewards interactively human observes state action returns scalar agent. setup explored work tamer similar setup applied robotics daniel straightforward represent rewards generated interactively using protocol programs. turn protocols human manipulates rewards. protocols assume ﬁxed reward function part reward shaping protocols human engineer changes rewards given ﬁxed reward function order inﬂuence agent’s learning. introduced potential-based shaping shapes rewards without changing mdp’s optimal policy. particular reward received environment augmented shaping function main result dynamic shaping functions form also guarantee optimal policy invariance. similarly wiewiora extend potential shaping potential-based advice functions identiﬁes similar class shaping functions pairs. section show framework captures reward shaping consequently limited notion q-value initialization. common practice train agent simulation transfer real world performs well enough. algorithm shows represent process training simulation protocol program. represent real-world decision problem simulator included protocol program. initially protocol program agent interact human observes interaction. human decides agent ready protocol program interact instead. action pruning technique dynamically removing actions reduce branching factor search space. techniques shown accelerate learning planning time section apply action-pruning prevent catastrophic outcomes exploration problem explored lipton garcia fernandez hans moldovan abbeel protocol programs allow action pruning carried interactively. instead decide actions prune prior learning human wait observe states actually encountered agent valuable cases human limited knowledge environment agent’s learning ability. section exhibit agent-agnostic protocol interactively pruning actions preserves optimal policy removing actions. pruning protocol illustrated gridworld lava pits agent represented gray circle goal state provides reward cells lava pits reward white cells provide reward time step human checks whether agent moves lava pit. agent continues normal. human bypasses sending action true sends agent next state agent doesn’t actually fall lava human sends reward negative reward agent less likely action again. protocol program algorithm figure note agent receives explicit signal attempted catastrophic action blocked human. observe negative reward self-loop information whether human environment generated observation. agent’s state representation signiﬁcant inﬂuence learning. suppose states consist number features deﬁning state vector human engineer specify mapping agent always receives place vector mappings used specify high-level features state important learning dynamically ignore confusing features agent. transformation state vector normally ﬁxed learning. protocol program allow human provide processed states high-level features interactively. time human stops providing features agent might learned generate methods focused state abstraction functions decrease learning time preserve quality learned behavior using state abstraction function agents compress representations environments enabling deeper planning lower sample complexity. state aggregation function implemented protocol program perhaps dynamically induced interaction teacher. illustrate simple ways proposed agent-agnostic interaction scheme captures existing agent-agnostic protocols. following results concern tabular mdps intended offer intuition high-dimensional continuous environments well. first observe protocol programs precisely capture methods shaping reward functions. remark reward shaping function including potential-based shaping potential-based advice dynamic potential-based advice protocol produces rewards. construct protocol given simply reward output protocol take value time step. algorithm simply deﬁne show simple class protocol programs carry action pruning certain form. remark protocol pruning actions following sense state action pairs protocol ensures that pair action never executed state protocol described section shown algorithm premise this cases agent executes action pruned protocol gives agent reward forces agent self-loop. knowing actions prune challenging problem. often natural assume human guiding learner knows something environment interest know every detail problem. thus consider case human partial knowledge problem interest represented approximate q-function. next remark shows protocol based approximate knowledge properties never prunes optimal action limits magnitude agent’s worst mistake remark assuming protocol designer β-optimal function section applies action pruning protocols concrete problems. experiment action pruning used prevent agent trying catastrophic actions i.e. achieve safe exploration. experiment action pruning used accelerate learning. human-in-the-loop help prevent disastrous outcomes result ignorance environment’s dynamics reward function. goal experiment prevent agent taking catastrophic actions. real world actions costly want agent never take action. notion catastrophic action closely related ideas safe work signiﬁcant rare events section describes protocol program preventing catastrophes ﬁnite mdps using action pruning. important elements program protocol prevents catastrophic actions preserving optimal policy minimal side-effects agent’s learning. extend protocol environments high-dimensional state spaces. element remains same. must modiﬁed preventing future catastrophes requires generalization across catastrophic actions discuss setting appendix protocol preventing catastrophes intended real-world environment. provide preliminary test protocol simple video game. protocol treats agent black box. applied protocol opensource implementation state-of-the-art algorithm trust region policy optimization duan environment catcher simpliﬁed version pong non-visual state representation. since catastrophic actions catcher modiﬁed game give large negative reward paddle’s speed exceeds speed limit. compare performance agent assisted protocol blocked catastrophic actions performance normal agent figure shows agent’s mean performance course learning. agent protocol support performed much better overall. unsurprising blocked ever catastrophic action. mean performance large early diminishes pruned agent learns avoid high speeds. pruned close pruned mean performance total returns whole period around times worse. pruned agent observes incongruous state transitions blocked protocol figure suggests observations negative side effects learning. also conducted simple experiment taxi domain dietterich taxi problem complex version grid world problem instances consists taxi number passengers. agent directs taxi passenger picks passenger brings destination drops off. actual human loop. instead agent blocked protocol program checked whether action would exceed speed limit. essentially protocol outlined appendix classiﬁer trained ofﬂine recognize catastrophes. future work test similar protocols using actual humans. taxi evaluate effect action pruning protocol accelerating learning discrete mdps. natural procedure pruning suboptimal actions dramatically reduces size reachable state space taxi carrying passenger passenger’s destination prune dropoff action returning agent back current state reward. prevents agent exploring large portion state space thus accelerating learning. experiment accelerated learning taxi evaluated q-learning r-max without action pruning simple instance passenger. taxi starts passenger destination standard q-learning ε-greedy exploration r-max using planning horizon four. results displayed figure results suggest action pruning protocol simpliﬁes problem q-learner dramatically r-max. allotted number episodes pruning substantially improves overall cumulative reward achieved; case r-max agent able effectively solve problem small number episodes. further results suggests agentagnostic method pruning effective without internal access agent’s code. presented agent-agnostic method giving guidance reinforcement learning agents. protocol programs written framework apply possible agent sophisticated schemes human-agent interaction designed modular fashion without need adaptation different algorithms. presented simple theoretical results relate method existing schemes interactive illustrated power action pruning domains. promising avenue future work dynamic state manipulation protocols guide agent’s learning process incrementally obscuring confusing features highlighting relevant features simply reducing dimensionality representation. additionally future work might investigate whether certain types value initialization protocols captured protocol programs optimistic initialization arbitrary domains developed machado moreover full combinatoric space learning protocols suggestive teaching paradigms explored. hypothesize powerful teaching methods take advantage interplay state manipulation action pruning reward shaping. challenge extend formalism account interplay multiple agents competitive cooperative settings. additionally experiments protocols explicitly programmed advance. future we’d like experiment dynamic protocols human loop learning process. lastly alternate perspective framework centaur system joint human-ai decision maker view human trains queries dynamically cases human needs help. future we’d like establish investigate formalisms relevant centaur view framework. work supported future life institute grant future humanity institute thank shimon whiteson james macglashan ellis herskowitz helpful conversations. david abel david ellis hershkowitz gabriel barth-maron stephen brawner kevin o’farrell james macglashan stefanie tellex. goal-based action priors. icaps pages david abel ellis hershkowitz michael littman. near optimal behavior approximate state abstraction. proceedings international conference machine learning ronen brafman moshe tennenholtz. r-max-a general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research thomas dean robert givan sonia leach. model reduction techniques computing approximately optimal solutions markov decision processes. proceedings thirteenth conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. norman ferns pablo samuel castro doina precup prakash panangaden. methods computing state similarity markov decision processes. proceedings conference uncertainty artiﬁcial intelligence javier garcia fernando fernandez. safe reinforcement learning high-risk tasks policy improvement. ieee ssci symposium series computational intelligence adprl ieee symposium adaptive dynamic programming reinforcement learning pages shane grifﬁth kaushik subramanian scholz. policy shaping integrating human feedback reinforcement learning. advances neural information processing systems pages alexander hans daniel schneegaß anton maximilian schäfer steffen udluft. safe exploration reinforcement learning. proceedings european symposium artiﬁcial neural networks http//citeseerx.ist.psu.edu/ viewdoc/download?doi=....{&}rep=rep{&}type=pdf. bradley knox peter stone. interactively shaping agents human reinforcement tamer framework. proceedings ﬁfth international conference knowledge capture pages robert loftin peng james macglashan michael littman matthew taylor huang david roberts. learning something nothing leveraging implicit human feedback strategies. robot human interactive communication ro-man ieee international symposium pages ieee marlos machado sriram srinivasan michael bowling. domain-independent optimistic initialization reinforcement learning. aaai workshop learning general competency video games teodor mihai moldovan pieter abbeel. safe exploration markov decision processes. proceedings international conference machine learning http //arxiv.org/abs/.. supratik paul kamil ciosek michael osborne shimon whiteson. alternating optimisation quadrature robust reinforcement learning. arxiv preprint arxiv. peng james macglashan robert loftin michael littman david roberts matthew taylor. need speed adapting agent action speed improve task learning non-expert humans. proceedings international conference autonomous agents multiagent systems pages international foundation autonomous agents multiagent systems benjamin rosman subramanian ramamoorthy. good actions? accelerating learning using learned action priors. development learning epigenetic robotics ieee international conference pages ieee a.a. sherstov stone. improving action selection mdp’s knowledge transfer. proceedings national conference artiﬁcial intelligence pages aaai press andrea lockerd thomaz cynthia breazeal. reinforcement learning human teachers evidence feedback guidance implications learning performance. aaai lisa torrey matthew taylor. teaching budget agents advising agents reinforcement learning. proceedings international conference autonomous agents multi-agent systems pages international foundation autonomous agents multiagent systems lisa torrey matthew taylor. help agent student/teacher learning sequential decision tasks. proceedings adaptive learning agents workshop held conjunction international conference autonomous agents multiagent systems aamas pages thomas walsh daniel hewlett clayton morrison. blending autonomous exploration apprenticeship learning. advances neural information processing systems pages yusen zhan haitham ammar matthew taylor. theoretically-grounded policy advice multiple teachers reinforcement learning settings applications negative transfer. pages provide informal overview protocol program avoiding catastrophes. focus differences high-dimensional case ﬁnite case described section ﬁnite case pruned actions stored table. human satisﬁed catastrophic actions table human’s monitoring agent fully automated protocol program. human need loop agent attempted catastrophic action human retire. inﬁnite case replace look-up table supervised classiﬁcation algorithm. visited state-actions stored labeled based whether human decides block them. labeled large enough serve training human trains classiﬁer tests performance held-out instances. classiﬁer passes test human replaced classiﬁer. otherwise data-gathering process continues training large enough classiﬁer pass test. class catastrophic actions learnable classiﬁer protocol prevents catastrophes minimal side-effects agent’s learning. however limitations protocol subject future work human need monitor agent long time provide sufﬁcient training data. possible remedy human augment training adding synthetically generated states example human might noise genuine states without altering labels. alternatively extra training data could sampled accurate generative model. catastrophic outcomes local cause easy block. moves slowly avoid hitting obstacle braking last second. lots momentum cannot slowed quickly enough. cases human in-the-loop would recognize danger time actual catastrophe would take place. prevent catastrophes ever taking place classiﬁer needs correctly identify every catastrophic action. requires strong guarantees generalization performance classiﬁer. distribution state-action instances non-stationary", "year": 2017}