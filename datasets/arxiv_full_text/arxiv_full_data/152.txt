{"title": "Learning values across many orders of magnitude", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.", "text": "learning algorithms invariant scale function approximated. propose adaptively normalize targets used learning. useful value-based reinforcement learning magnitude appropriate value approximations change time update policy behavior. main motivation prior work learning play atari games rewards clipped predetermined range. clipping facilitates learning across many different games single learning algorithm clipped reward function result qualitatively different behavior. using adaptive normalization remove domain-speciﬁc heuristic without diminishing overall performance. main motivation work mnih q-learning combined deep convolutional neural network resulting deep network algorithm learned play varied atari games arcade learning environment proposed evaluation framework test general learning algorithms solving many different interesting tasks. proposed singular solution using single hyperparameters magnitudes frequencies rewards vary wildly different games. overcome hurdle rewards temporaldifference errors clipped instance pong rewards bounded pac-man eating single ghost yield reward clips latter well. satisfying solution reasons. first clipping introduces domain knowledge. games sparse non-zero rewards outside clipping results optimizing frequency rewards rather sum. good heuristic atari generalize domains. importantly clipping changes objective sometimes resulting qualitatively different policies behavior. propose method adaptively normalize targets used learning updates. targets guaranteed normalized much easier suitable hyperparameters. proposed technique speciﬁc generally applicable supervised learning reinforcement learning. several reasons normalization desirable. first sometimes desire single system able solve multiple different problems varying natural magnitudes atari domain. second multi-variate functions normalization used disentangle natural magnitude component relative importance loss function. particularly useful components different units predict signals sensors different modalities. finally adaptive normalization help deal non-stationary. instance reinforcement learning policy behavior change repeatedly learning thereby changing distribution magnitude values. many machine-learning algorithms rely a-priori access data properly tune relevant hyperparameters however much harder learn efﬁciently stream data know magnitude function seek approximate beforehand magnitudes change time instance typically case reinforcement learning policy behavior improves time. input normalization long recognized important efﬁciently learn non-linear approximations neural networks leading research achieve scale-invariance inputs output target normalization received much attention probably supervised learning data commonly available learning commences making straightforward determine appropriate normalizations tune hyper-parameters. however assumes data available priori true online settings. natural gradients invariant reparameterizations function approximation thereby avoiding many scaling issues computationally expensive functions many parameters deep neural networks. approximations regularly proposed typically trading accuracy computation sometimes focusing certain aspect input normalization algorithms fully invariant rescaling targets. atari domain several algorithmic variants improvements proposed well alternative solutions however none address clipping rewards explicitly discuss impacts clipping performance behavior. concretely consider learning stream data {}∞t= inputs targets real-valued tensors. update parameters function output close target according loss instance deﬁned squared difference yt). canonical update stochastic gradient descent sample update α∇θlt step size. magnitude update depends step size loss hard pick suitable step sizes nothing known magnitude loss. important special case neural network often trained form hyperparameters interact scale loss. especially deep neural networks large updates harm learning networks highly non-linear updates ‘bump’ parameters regions high error. scale shift parameters learned data. scale matrix dense diagonal deﬁned scalar σti. similarly shift vector contain separate components deﬁned scalar deﬁne loss normalized function normalized target ˜yt. unnormalized approximation input given normalized function unnormalized function. ﬁrst glance seem made little progress. learn using algorithm used parameters function problem become fundamentally different easier; would merely changed structure parameterized function slightly. conversely consider tuning scale shift hyperparameters tuning fundamentally easier tuning hyperparameters step size directly. fortunately alternative. propose update according separate objective normalizing updates thereby decompose problem learning appropriate normalization learning speciﬁc shape function. properties want simultaneously achieve discuss properties separately below. refer algorithms combine output-preserving updates adaptive rescaling pop-art algorithms acronym preserving outputs precisely adaptively rescaling targets. unless care taken repeated updates normalization might make learning harder rather easier normalized targets become non-stationary. importantly whenever adapt normalization based certain target would simultaneously change output unnormalized function inputs. little reason believe unnormalized outputs incorrect undesirable hurt performance practice illustrated section ﬁrst discuss prevent issues discuss update scale shift. avoid changing outputs unnormalized function whenever update scale shift changing normalized function simultaneously. goal preserve outputs change normalization inputs. prevents normalization affecting approximation appropriate objective solely make learning easier leave solving approximation optimization algorithm. without loss generality unnormalized function written parametrized function gθwb normalized function. uncommon deep neural networks linear layer output last layer non-linearities. alternatively always square linear layer non-linear function ensure constraint instance initialized following proposition shows update parameters fulﬁll second desideratum preserving outputs precisely change normalization. non-linear function matrix k-element vectors matrix. consider change scale shift parameters σnew µnew σnew non-singular. additionally change parameters wnew bnew deﬁned later propositions proven appendix. special case scalar scale shift updates become wnew bnew /σnew. updating scale shift update output normalized function gθwb toward normalized output using learning algorithm. importantly normalization updated ﬁrst thereby avoiding harmful large updates would otherwise occur. observation made precise proposition section algorithm example implementation pop-art squared loss. generalized easily loss changing deﬁnition notice updated twice ﬁrst adapt scale shift preserve outputs function sgd. order updates important allows normalization immediately subsequent update. natural choice normalize targets approximately zero mean unit variance. clarity conciseness consider scalar normalizations. straightforward extend diagonal dense matrices. data positive estimates second moment targets step size. initially always remain although avoid issues numerical precision full equivalence useful enforce lower bound explicitly requiring constant exponential moving averages placing weight recent data points appropriate non-stationary settings. constant additional beneﬁt never becoming negligibly small. consider ﬁrst time target observed much larger previously observed targets. small statistics would adapt slightly resulting update large enough harm learning. small normalization adapt large target updating potentially making learning robust. particular following proposition holds. proposition using updates adapt normalization parameters normalized targets bounded instance normalized target guaranteed note proposition rely assumptions distribution targets. important result implies bound potential normalized errors learning without prior knowledge actual targets observe. open question whether uniformly best normalize mean variance. appendix discuss normalization updates based percentiles mini-batches derive correspondences these. step back analyze effect magnitude errors gradients using regular sgd. analysis suggests different normalization algorithm interesting correspondence pop-art sgd. consider updates unnormalized multi-layer function form fθwb update weight matrix fθwb−yt gradient squared loss call unnormalized error. magnitude update depends linearly magnitude error appropriate inputs normalized ideal scale weights depends linearly magnitude targets. consider update parameters αjtwt−δt ∇gθm) jacobian magnitudes weights errors depend linearly magnitude targets. means magnitude update depends quadratically magnitude targets. compelling reason updates depend magnitudes weights layer already ensure appropriate scaling. words doubling magnitudes targets updates lower layers quadruple clear reason. analysis suggests algorithmic solution seems novel itself track magnitudes targets separate parameter multiply updates lower layers factor general version matrix scalings given algorithm prove interesting perhaps surprising connection pop-art algorithm. proposition consider functions deﬁned differentiable function cases functions initialized identically using initial consider updating ﬁrst function using algorithm second using algorithm then sequence non-singular scales {σt}∞t= shifts {µt}∞t= algorithms equivalent sense sequences {θt}∞t= identical outputs functions identical input. fig. gradient norms learning atari games actual unclipped rewards clipped rewards using popart instead clipping. shaded areas correspond games. algorithm updates layer normalized thereby allowing last linear layer adapt scale targets. contrast algorithms ﬂavor adaptive normalization rmsprop adagrad adam component gradient square root empirical second moment component. said methods complementary straightforward combine pop-art optimization algorithms sgd. ﬁrst analyze effect rare events online learning infrequently much larger target observed. events instance occur learning noisy sensors sometimes captures actual signal learning sparse non-zero reinforcements. empirically compare three variants without normalization normalization without preserving outputs precisely pop-art. inputs binary representations integers drawn uniformly randomly desired outputs corresponding integer values. every samples present binary representation input target approximating function fully connected neural network inputs hidden layers nodes layer tanh internal activation functions. simple setup allows extensive sweeps hyper-parameters avoid bias towards algorithm tune these. step sizes normalization tuned grid search figure shows root mean squared error samples updating function solid line median repetitions shaded region covers percentiles. plotted results correspond best hyper-parameters according overall rmse lines slightly smoothed averaging consecutive samples. favors relatively small step size avoid harmful large updates slows learning smaller updates; error curve almost spikes. adaptive normalization larger step size therefore learns faster high error spikes changing normalization also changes outputs smaller inputs increasing errors these. comparison pop-art performs much better. prefers step size pop-art exploit much faster rate statistics faster tracking statistics protects pop-art large spikes output preservation avoids invalidating important motivation work reinforcement learning non-linear function approximators neural networks goal predict optimize action values deﬁned expected future rewards. rewards differ arbitrarily domain next non-zero rewards sparse. result action values span varied wide range often unknown learning commences. mnih combined q-learning deep neural network algorithm called impressively learned play many games using single hyper-parameters. however discussed above handle different reward magnitudes single system rewards clipped interval harmless games pong reward ever higher lower satisfactory heuristic introduces speciﬁc domain knowledge optimizing reward frequencies approximately useful optimizing total score. however clipping makes algorithm blind differences certain actions difference reward eating ghost eating pellet pac-man. hypothesize overall performance decreases turn clipping possible tune step size works many games regain much lost performance pop-art. goal improve state-of-the-art performance remove domain-dependent heuristic induced clipping rewards thereby uncovering true rewards. double algorithm three versions without changes without clipping rewards temporal difference errors without clipping additionally using pop-art. targets cumulation reward discounted value next state estimated action value action state according current parameters stable periodic copy parameters form double q-learning roughly tuned main step size step size normalization straightforward tune unclipped version reasons become clear soon. figure shows norm gradient double learning function number training steps. left plot corresponds reward clipping middle clipping right using pop-art instead clipping. faint dashed lines corresponds median norms game. shaded areas correspond games. without clipping rewards pop-art produces much narrower band within gradients fall. across games median norms range less orders magnitude compared almost four orders magnitude clipped double orders magnitude unclipped double without pop-art. wide range latter shows impossible suitable step size neither clipping pop-art updates either small games large others. frames evaluated actual scores best performing agent game episodes minutes play normalized human random scores described mnih figure shows differences normalized scores double double pop-art. main eye-catching result distribution performance drastically changed. games observe dramatic improvements games substantial decrease. instance pac-man clipped double agent care ghosts pellets double pop-art learns actively hunt ghosts resulting higher scores. especially remarkable improved performance games like centipede gopher also notable game like frostbite went near-human performance level. scores found appendix. games fare worse unclipped rewards changes nature problem. instance time pilot pop-art agent learns quickly shoot mothership advance next level game obtaining many points process. clipped agent instead shoots anything moves ignoring mothership. however long game points scored safer homogeneous strategy clipped agent. reason disconnect seemingly qualitatively good behavior combined lower scores agents fairly myopic discount factor therefore optimize rewards happen within dozen seconds future. whole results show pop-art successfully remove clipping heuristic present prior variants retaining overall performance levels. double pop-art performs slightly better double clipped rewards games performance least good clipped double median mean differences positive. demonstrated pop-art used adapt different non-stationary target magnitudes. problem perhaps previously commonly appreciated potentially deep learning common tune normalize priori using existing data set. straightforward reinforcement learning policy corresponding values repeatedly change time. makes pop-art promising tool deep reinforcement learning although speciﬁc setting. pop-art successfully replace clipping rewards done handle various magnitudes targets used q-learning update. true problem exposed learning algorithm hope make progress instance improving exploration informed true unclipped rewards. liang machado talvitie bowling. state control atari games using shallow reinforcement learning. international conference autonomous agents multiagent systems martens grosse. optimizing neural networks kronecker-factored approximate curvature. proceedings international conference machine learning volume pages mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis. human-level control deep reinforcement learning. nature wang freitas schaul hessel hasselt lanctot. dueling network architectures deep reinforcement learning. international conference machine learning york appendix introduce analyze several extensions variations including normalizing based percentiles minibatches. additionally prove propositions main text appendix. experiments described section main paper closely followed setup described mnih hasselt particular double algorithm identical described hasselt shown results obtained running trained agent minutes simulated play repeated times diversity different runs ensured small probability exploration step well performing ‘no-op’ actions also used described mnih summary evaluation setup used mnih except allowed evaluation time game also used wang results figure obtained normalizing scores ﬁrst subtracting score random agent dividing absolute difference human random agents additional parameter instance useful desire fast tracking non-stationary problems. want large step size without risking overly large updates. figure left plot shows histograms normally distributed targets mean standard deviation normalized targets middle plot shows histograms except histogram replaced histogram right plot shows variance normalized targets function upper bound either change keeping ﬁxed change keeping ﬁxed parameter seem superﬂuous increasing normalization step size also reduces hard bounds normalized targets. however additionally inﬂuences distribution normalized targets. histograms left-most plot figure show happens limit magnitudes using histogram shows normalized targets unnormalized targets come normal distribution shown blue. normalized targets contained distribution non-normal even though actual targets normal. conversely histogram middle plot shows distribution remains approximately normal instead reduce magnitudes. right plot shows effect variance normalized targets either approach. change keeping ﬁxed variance normalized targets drop desired variance change keeping ﬁxed variance remains predictably approximately difference behavior resulting normalization demonstrates gives potentially useful additional degree freedom. sometimes simply roll additional scaling step size without loss generality decrease step size avoid overly large updates. however sometimes easier separate magnitude targets inﬂuenced magnitude updates instance using adaptive step-size algorithm. addition introduction explicit scaling allows make interesting connections normalization percentiles next section. example percentiles correspond respecticely. conversely corresponds fact applies targets normal. distributions forms normalization differ even terms objectives. guaranteed normalized targets fall could result overly conservative normalization sensitive outliers reduce overall magnitude updates far. words learning safe sense updates slow many updates small. general probably typically better ratio exact order statistics hard compute online would need store previous targets. obtain memory-efﬁcient online updates percentiles store values ymin ymax eventually property proportion values larger proportion values smaller ymin ymax step size small take long updates converge appropriate values. practice might better magnitude steps depend actual errors update takes form asymmetrical least-squares update proposition consider minibatches {{yt ytb}}∞t= size whose elements limit updates converge values holds fact connects online minibatch updates normalization percentiles. instance minibatch size would correspond roughly online percentile updates proposition normalization mean variance different normalizations strictly equivalent behave similarly practice. proposition quantiﬁes interesting correspondence minibatch updates normalizing percentiles. although fact stated holds uniform targets proportion normalized targets interval generally becomes larger increase minibatch size increase decrease potentially resulting better robustness outliers possible expense slower learning. using constant step sizes useful aware start learning trust data rather arbitrary initial values. done using step size deﬁned following fact. proposition consider recency-weighted running average updated stream data {zt}∞t= using ¯zt− βtzt deﬁned βt)− similar result derived remove effect initialization certain parameters kingma stochastic optimization algorithm called adam. work initial values assumed zero standard exponentially weighted average explicitly computed stored divided term analogous step size corrects initialization place without storing auxiliary variables rest method motivation similar. alternatively possible initialize normalization safely choosing scale relatively high initially. beneﬁcial ﬁrst targets relatively small noisy. would step size updates would treat initial observations important would approximating function noise. high initialization would instead reduce effect ﬁrst targets learning updates would instead appropriate normalization. ﬁnding normalization actual learning would commence. weight matrix last linear layer soft-max. actual outputs already normalized using soft-max outputs layer soft-max still beneﬁt normalization. determine targets normalized either back-propagate gradient loss soft-max invert function. generally consider applying normalization level hierarchical non-linear function. seems promising counteract undesirable characteristics back-propagating gradients vanishing exploding gradients addition normalizing gradients network provide straightforward combine gradients different sources complex network graphs standard feedforward multi-layer network. first normalization allows normalize gradient source separately merging gradients thereby avoiding source fully drown others allowing weight gradients actual relative importance rather implicitly relying current magnitude proxy this. second normalization prevent undesirably large gradients many gradients come together point graph normalizing merging gradients. non-linear function matrix k-element vectors matrix. consider change scale shift parameters non-singular. additionally change parameters deﬁned differentiable function cases functions initialized identically using initial consider updating ﬁrst function using algorithm second using algorithm then sequence non-singular scales {σt}∞t= shifts {µt}∞t= algorithms equivalent sense sequences {θt}∞t= identical outputs functions identical input. proof. denote parameters algorithms respectively. similarly parameters ﬁrst function parameters second function. enough show single updates algorithms starting points equivalent results. proof. normalized targets distributed according normal distribution targets normally distributed normalization afﬁne transformation. normal distribution mean zero variance values exactly standard deviations mean implying ratio data points detail weaker conditions refer reader extensive literature stochastic approximation proof update ymin proposition consider minibatches {{yt ytb}}∞t= size whose elements proof. conditions step size quantities ymin converge expected value minimum maximum i.i.d. random variables. cumulative distribution function maximum i.i.d. random variables since note deﬁned exactly removes contribution initial value time weight exponential moving average renormalizes remaining value dividing relative weights observed samples {zt}∞t= conserved. holds game alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar h.e.r.o. hockey james bond kangaroo krull kung-fu master montezuma’s revenge pacman name game phoenix pitfall pong private q*bert river raid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham venture video pinball wizard yars revenge zaxxon table scores random agent human tested double described hasselt double pop-art reward clipping minutes simulated play random human double scores taken wang", "year": 2016}