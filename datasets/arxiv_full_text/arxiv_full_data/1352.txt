{"title": "Rapid Exact Signal Scanning with Deep Convolutional Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "A rigorous formulation of the dynamics of a signal processing scheme aimed at dense signal scanning without any loss in accuracy is introduced and analyzed. Related methods proposed in the recent past lack a satisfactory analysis of whether they actually fulfill any exactness constraints. This is improved through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The proposed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors. This is demonstrated both theoretically in a computational complexity analysis and empirically on modern parallel processors.", "text": "extended preprint thom gritschneder rapid exact signal scanning deep convolutional neural networks ieee transactions signal processing vol. digital object identiﬁer ./tsp... pages only copyright ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. important architecture analyzing signals possess spatial structure images pixels arranged two-dimensional grid inspired ﬁndings dynamics mammalian visual cortex convolutional neural networks respect weight-sharing principle hence convolution trainable ﬁlters becomes actual workhorse data processing. principle greatly reduces network’s degrees freedom making less susceptible overﬁtting incorporates strong prior respect spatial layout input data. fact particular architecture proven highly successful image restoration tasks pattern recognition problems trained object categorization evaluated feasible image position possible assign class membership estimations pixels image yielding semantic segmentation scene representation much powerful gained strict conventional object detection approach solely outputs bounding boxes found object instances. instead facilitates applications automated biological medical image analysis dense vehicle environment perception computational complexity sophisticated classiﬁcation system used conjunction sliding window approach seem excessive ﬁrst glance weight-sharing principle exploited intermediate computation results shared among adjacent image patches resulting speedup several orders magnitude. although already realized cnns without pooling layers decades approaches also account pooling layers emerged recently approach giusti achieves fast scanning entire images introduction fragmentation data structure. here internal representations decomposed using spatial reordering operation pooling layer allowing evaluation convolutions contiguous signals times. intermediate signals however inhomogeneous respect dimensionality leaving possibility efﬁcient tensor convolution routines unclear. hand propose enlarging ﬁlter banks convolutional layers inserting vanishing entries regular locations. sparse ﬁlter banks require cumbersome re-engineering efﬁcient convolution implementations able achieve maximum throughput modern massively parallel processors. sermanet processing pipeline patches abstract—a rigorous formulation dynamics signal processing scheme aimed dense signal scanning without loss accuracy introduced analyzed. related methods proposed recent past lack satisfactory analysis whether actually fulﬁll exactness constraints. improved exact characterization requirements sound sliding window approach. tools developed paper especially beneﬁcial convolutional neural networks employed also used general framework validate related approaches signal scanning. proposed theory helps eliminate redundant computations renders special case treatment unnecessary resulting dramatic boost efﬁciency particularly massively parallel processors. demonstrated theoretically computational complexity analysis empirically modern parallel processors. achieved unprecedented complexity multitude basic commonality application translation-invariant function large signal sliding fashion facilitates dense computation interesting output values possible spatial location. consider ﬁlter-based signal denoising example here entry denoised output signal always depends ﬁxed computation rule applied limited number samples within input signal words subsignal input signal. computation rule completely agnostic regard actual position merely important input samples drawn accordingly input signal. course modern systems apply sophisticated techniques mere ﬁltering. however recently architecture essentially made simple ﬁltering building blocks displayed advantages approach wide variety practical applications. signiﬁcant advances design massively parallel processors availability huge annotated data sets deep artiﬁcial neural networks learn desired behavior adapting degrees freedom concrete sample data rather programmed explicitly become facto state-of-the-art domains signal restoration signal classiﬁcation. approaches common inherently clear actually compute result even desired one. instead rigorous mathematical proof correctness examples available illustrating implementation approaches. situation especially unsatisfactory instead pure convenience functions systems subject safety considerations realized precise statements rather empirical evaluation required. contributions paper development original theory subsignal compatible transformations exact characterization functions fulﬁll invariants required sound sliding window approach proposition method dense signal scanning provably without accuracy loss yields signiﬁcant speedups homogeneous data structures elimination redundant computations special case treatment demonstration cnns interconnect theory exactly transformed subsignal-based application signal-based application without necessary adjustments computationally demanding tensor convolution. authors’ best knowledge ﬁrst actually mathematically rigorous statements support claims correctness dense signal scanning cnns. generality results herein developed theoretical framework also serve basis analyzing related emerging methods signal processing based translationinvariant functions applied sliding fashion. remainder paper structured follows. section presents introduction structure ﬁxes notation introduces meant subsignals. section establishes basics theory subsignal compatible transformations shows building blocks cnns theory. following sect. theory extended functions applied strided fashion particularly important pooling operators evaluated non-overlapping blocks. section provides theoretical computational complexity analysis. practical considerations image processing results experiments real parallel processors discussed sect. paper concluded discussion results sect. vii. section begins introduction building blocks cnn. notation used throughout paper established. section concludes deﬁnition subsignal extraction operator statements properties. cnns organized number specialized layers layer receives input data predecessor processes sends result next layer. network’s output output ﬁnal layer. training process consists tuning network’s degrees freedom network produces desired output given concrete input different specialized layer types given follows. convolutional weight-sharing principle convolve input trainable ﬁlter bank trainable scalar bias form layer output. layers fall class subsignal compatible transformations detailed sect. mathematical analysis involved computations given sect. iii-c. fully-connected layers special case convolutional layers carry convolution unit spatial ﬁlter size. mathematical treatment layers hence superseded analysis convolutional layers. non-linearity layers independently pass sample signal scalar transfer function. prevents entire network forming purely linear system hence enhances network’s representational capacity. since operations agnostic respect spatial structure analysis straightforward handled sect. iii-c. eventually pooling layers strengthen network’s invariance small translations input data evaluation ﬁxed pooling kernel followed downsampling operation. brevity presentation functions applied non-overlapping blocks considered here. pooling requires extension plain theory subsignal compatible transformations provided sect. paper proves cnns transformed subsignal-based application signal-based application transforming strided function evaluation sliding function evaluation inserting special helper layers namely fragmentation defragmentation stufﬁng trimming. transformation completely lossless subsignal-based application signal-based application lead results. even transformation cnns ﬁne-tuned standard optimization methods. example process given sect. sake simplicity mathematical analysis restricted vector-shaped signals. generalization complex signals images straightforward application theory independent spatial dimensions images. brieﬂy discussed sect. represents positive natural numbers. denotes q-tuples entries elements called signals entries called samples. signal index list entries used element example equals real numbers hence r-dimensional euclidean space formal corresponds linear combination canonical basis vectors weighted selected coordinates signal dimm represents dimensionality need correspond exactly concept lemma subsignal dimensionalities. dimm subsignalc subsignalc. proof. subsignal indices left-hand side well within bounds. since dimm also holds right-hand side. section introduces concept subsignal compatible transformations. functions applied entire signal yield result applied subsignal independently. shown functions applied sliding fashion characterized subsignal compatible transformations composition subsignal compatible transformations subsignal compatible transformation. section cnns without pooling layers considered demonstrated satisfy requirements subsignal compatible transformations. consequence networks applied whole input signal without handle individual subsignals. cnns contain pooling layers require theoretical preparations discussed verbosely sect. deﬁnition sets positive natural number function. called subsignal compatible transformation dimensionality reduction constant properties hold dimn dimm subsignal dimensionalities holds subsignald−c+ dimm ﬁrst property guarantees reduces dimensionality argument always amount regardless concrete input. second property states applied individual subsignal applying entire signal afterwards extracting appropriate samples resulting signal. therefore subsignal-based application outcome feasible subsignals determined sufﬁces carry signal-based application entire input signal illustration subsignal extraction operator applied signal fig. samples extraction subsignals samples. left-hand side shows ﬁrst subsignal right-hand side shows ﬁnal subsignal maximum subsignal index dimensionality sense linear algebra. example categorical data features vector space theory presented paper requires algebraic structures vector spaces analytic structures real numbers certain examples. bulk results hold signals samples arbitrary sets. positive natural number q=cm written contains signals dimensionality greater equal samples example natural number note contains non-empty signals samples subsignal contiguous list samples contained larger signal. first concept extracting subsignals ﬁxed number samples given signal formalized deﬁnition arbitrary denote ﬁxed subsignal dimensionality. function straightforward verify subsignald welldeﬁned actually returns possible contiguous subsignals length given signal samples note application operator must always ensured requested subsignal index within bounds must hold address valid subsignal. operator applies sliding fashion subsignals length input signal stores result contiguous signal. sliding window always advanced exactly entry evaluation next result states functions applied sliding fashion essentially subsignal compatible transformations exchange property could weakened hold case dimensionality reduction constant equals subsignal dimensionality theorem sets function. following equivalent subsignal compatible transformation dimensionality reduction constant fulﬁlls dimensionality reduction property dimm holds unique function slidef proof. trivial since dimensionality reduction property fulﬁlled deﬁnition claimed condition special case exchange property showing existence deﬁne dimn dimensionality reduction property therefore well-deﬁned. deﬁne dimm clear dimn dimn precondition implies slidef hence slidef considering uniqueness suppose exist functions slidef slidef. arbitrary deﬁnition gives slidef slidef therefore suppose function slidef slidef inherently fulﬁlls dimensionality reduction property. arbitrary subsignal dimensionality signal. further dimm arbitrary subsignal index. remembering dimm using lemma gives therefore subsignal compatible transformation unique function generates transformation. yields succinct characterization helps deciding whether given transformation fulﬁlls dimensionality reduction property exchange property. clear fig. example subsignal compatible transformation. here quot non-linear operator computes quotient adjacent samples always reduces dimensionality input sample satisfying dimensionality reduction property. lower part shows result ﬁrst extracting subsignals samples input signal evaluating quot. yields processed subsignals quot samples each. exchange property guarantees processed subsignals also found quot exemplarily shown right-hand side graphics subsignal index note exchange property well-deﬁned dimensionality reduction property guarantees dimensionalities sides equation match. further subsignal index within bounds sides. trivial left-hand side seen right-hand side since dimm tions immediately follows theorem sets subsignal compatible transformations dimensionality reduction constant holds already proof. dimm applying precondition exchange property subsignal dimensionality yields hence samples transformed signals match thus domain quot operator introduced quotient samples evaluated sliding fashion. seems plausible example convolution also subsignal compatible transformation. proven rigorously sect. iii-c. ample transformation subsignal compatible example denote integers consider function dimz fulﬁlls dimensionality reduction property dimensionality reduction constant exchange property however satisﬁed yields subsignald−c+ since unless vanishes cannot subsignal compatible transformation. composition subsignal compatible transformations subsignal compatible transformation dimensionality reduction constant adjusted theorem sets suppose subsignal compatible transformation dimensionality reduction constant subsignal compatible transformation dimensionality reduction constant deﬁne subsignal compatible transformation dimensionality reduction constant proof. note ﬁrst since hence indeed arbitrary demonstrating well-deﬁned. yields hence well-deﬁned. further dimn dimm using dimensionality reduction property therefore thus well-deﬁned dimensionality reduction property implies dimp dimp dimn dimm dimm therefore fulﬁlls dimensionality reduction property. arbitrary dimm since satisfy exchange property follows subsignald−c+ hold respective applications exchange property. therefore also fulﬁlls exchange property. conclude section demonstration provided cnns without pooling layers theory developed far. since pooling layers require non-trivial extension theory detailed sect. ingredient cnns trainable degrees freedom facilitate adaptation network speciﬁc task located here. layers multi-channel input feature maps convolved channel-wise adjustable ﬁlter banks result accumulated adjustable bias added yield output feature map. first introduction indexing rules iterated structures account multi-channel nature occurring signals. positive natural numbers multi-channel signal. indices moreover indices rule extended naturally sets written explicitly products factors. therefore another number example indices rules become clearer multi-channel convolution operation considered. suppose samples members ring denotes number input channels number output channels equals number samples considered time convolution ﬁlter bank words receptive ﬁeld size convolutional layer. input signals feature maps samples form ﬁlter banks represented tensor m)c. must hold ﬁlter kernel smaller input signal. illustration pooling kernel determines maximum fig. adjacent samples. applied strided fashion non-overlapping subsignals input signal samples yielding output signal stridemax samples. since dimensionality halves therefore dimensionality reduction property violated subsignal compatible transformation. substituted step. multi-channel convolution operation deﬁned hence fact application fconv sliding fashion. therefore theorem guarantees subsignal compatible transformation dimensionality reduction constant since fully-connected layers merely special case convolutional layers need special treatment here. addition biases require knowledge spatial structure convolution’s result therefore trivial subsignal compatible transformation dimensionality reduction constant non-linearity layers nothing application scalar-valued function samples input signal. hence layers also form subsignal compatible transformations dimensionality reduction constant theorem furthermore compositions operations also understood subsignal compatible transformations corollary consequence exchange property facilitates application cnns without pooling layers entire signal instead subsignal independently without incurring accuracy loss. next section extend result cnns also feature pooling layers. shown convolutional layers non-linearity layers theoretical framework subsignal compatible transformations. section analyzes pooling layers apply pooling kernel nonoverlapping blocks input signal. equivalent function applied sliding fashion followed downsampling operation referred application function strided fashion. theory developed herein course also applied functions pooling kernels encountered ordinary cnns. example multi-channel convolution ﬁlter bank advanced receptive ﬁeld size essentially fconv sect. iii-c applied strided fashion. application convolution ﬁlter banks advanced section demonstrates functions turned efﬁciently computable subsignal compatible transformations using data structure recently introduced fragmentation giusti here proposed method generalized rigorously proven correct. added beneﬁt results dynamics entire signal processing chain also accurately described including possibility tracking position processed subsignal fragmentation data structure. circumstances fragment dimensionalities guaranteed always homogeneous analyzed. desirable property facilitates application subsequent operations signals number samples rendering cumbersome handling special cases obsolete thus resulting accelerated execution massively parallel processors. cnns means conventional tensor convolutions used without modiﬁcations whatsoever especially beneﬁcial highly-optimized implementation readily available. g+))·edimm operator applies strided fashion signals number samples multiple subsignal indices chosen non-overlapping subsignals starting ﬁrst valid subsignal. since dimm dimm strideg well-deﬁned. further dimm dimn domain strideg. since input dimensionality reduced division natural number rather subtraction dimensionality reduction property cannot fulﬁlled unless situation however particularly interesting since strideg slideg already handled sect. iii. channel pooling kernels commonly encountered cnns example assume goal process real-valued signals channels channel processed independently others adjacent samples compressed output sample. average pooling realized pooling determines channelkernel gavg wise empirical mean value samples. another example max-pooling maximum entry channel determined. achieved pooling fragmentation operator performs spatial reordering operation. precise analysis requires recap elementary number theory. numbers euclidean division guarantees unique numbers rem. small collection results operators reference proposition moreover fragmentation operator applied signal puts certain samples individual fragments grasped signals themselves. collection fragments fragmented further larger collection fragments results. total number samples however left unchanged operations. sake convenience matrices used concrete data structure fragmented signals columns correspond fragments rows correspond signal samples. first notation needs deﬁned. denotes matrices rows columns entries present context represents collection fragments signal samples. rdimm cdimm denote number rows columns respectively. furthermore entry i-th j-th column transpose written matrix another deﬁnition vectorization operator veca×b characterized veca×bj ξrem+ div+ indices matrices a×b. inverse given vectorization operator vec− a×bi ξa+i indices vec− vectors veriﬁed directly operators welldeﬁned permutations inversely related another. help fragmentation operator deﬁned deﬁnition arbitrary vector dimensionalities numbers input fragments function fragk kq×s q×ks here equals corresponding parameter application function strided fashion. fragk clearly well-deﬁned number output fragments next consider operator undoes ordering fragmentation operator deﬁnition denote vector dimensionality number output fragments. defragk q×ks kq×s note defragk well-deﬁned number input fragments must equal fragmentation defragmentation inversely related defragk fragk kq×s fragk defragk q×ks. illustration operations performed fragmentation defragmentation depicted fig. lemma suppose rdimm kq×s. rdimm cdimm cdimm further fragkµ ξdivks+ν− remks+ν− indices ks}. proof. dimensionality statements obvious definition fragk. prove identity ks}. yields lemma set. positive natural numbers q×ks arbitrary fragmented signal. rdimm rdimm cdimm defragkµ cdimm ξdivs+ν− ks)+ rems+ν− ks)+ indices proof. completely analogous lemma since data dependency fragments parallelization subsignal compatible transformation evaluation output samples straightforward. follows formal introduction processing chain concept captures generalizes dynamics notions application signal processing deﬁnition collection following objects called processing chain ﬁxed subsignal dimensionality number layers sequence sets subsignal compatible transformations dimensionality reduction constant functions called stride products cessing chain. implies operator evalstridej number represents extent region words entire network’s receptive ﬁeld size. size design parameter network depends concrete deﬁnitions layers. functions processing chain substituted appropriate layer types discussed earlier convolutions non-linearities compositions thereof. pooling kernels functions applied strided fashion nonoverlapping blocks plugged processing chain functions. recursive deﬁnitions evalstridej evalslidej represent alternating evaluation subsignal compatible transformation function applied strided fashion speciﬁed layer index rationale evalstride operator naive subsignal-based application here applied ordinary signals length equal network’s receptive ﬁeld size according application network using sliding window approach involves extraction feasible overlapping subsignals length feeding network independently other. evalslide operator differs evalstride corresponds signal-based application overlapping subsignals need processed separately here preventing redundant computations. instead complete input signal processed entirety sharing intermediate computation results among adjacent subsignals. using fig. illustration fragmentation defragmentation operators. left-hand side shows input signal samples single fragment fragmentation parameter yields signal samples fragments shown upper right graphics. second application fragmentation results fragments samples each depicted lower right. complete defragmentation parameter yields original input signal again samples single fragment remark fragk fragkk kkq×s. proof. claim follows entry-wise comparison between fragk fragkk using lemma follows immediately fragmentation commutative relationship fragmentation functions applied strided fashion subsignal compatible transformations background necessary analyzing theory functions applied strided fashion subsignal compatible transformations. outcome subsignal compatible transformation applied fragmented signal deﬁned naturally deﬁnition sets subsignal compatible transformation dimensionality reduction constant fragmented signal samples fragments holds. denote individual deﬁnition thus describes recipe transformed subsignal-based application signal-based application. concrete example discussed sect. first however theoretical justiﬁcation method indeed produces correct outcome circumstances presented. next result states application processing chain well-deﬁned proves result evalstride operator applied subsignal larger signal found within result evalslide applied entire signal. implies approaches deliver values hence verify evalslide involves accuracy loss whatsoever. lemma given processing chain notation deﬁnition ﬁrst assume divides dimnj evalstridej non-empty words application processing chain strided fashion well-deﬁned. signal dimensionality number subsignals length divisible ﬁnal stride product considered signal. application processing chain sliding fashion well-deﬁned additional statements hold dimmj abbreviation dimensionality intermediate representations layer evalstride cascade. note numbers actually independent subsignal index further cdimmj rdimmj deﬁned abbreviations number fragments fragmented signal dimensionality respectively layer using evalslide operator. following holds words number distinct subsignals samples fragment fragmented signals equals original number distinct subsignals divided corresponding number fragments. denotes substitution induction hypothesis. hence claimed expression follows since note indeed positive natural number divides dimnj evalstridej non-empty requirement. shown application processing chain sliding fashion well-deﬁned using induction follows evalslide trivially well-deﬁned deﬁnition equals claimed expressions since ﬁrst demonstrated divides rdimmj implies well-deﬁnedness since fragmentation operator indeed applied. follows proposition implies divides since shown hence processing chain applied j-th layer. lemma follows immediately yields claimed expression. since fragmentation inﬂuences number columns processing chain application follows lemma proving claimed identity. natural number number subsignals required divisible since proposition shows right-hand side equals ξdiv+µ rem+ ξi−+µ hence sides equal. turning arbitrary ﬁxed subsignal index write evalstridej−) abbreviation. left-hand side claim leads conclusion result evalstride operator applied arbitrary subsignal input signal emerges contiguously result evalslide operator processes signal entirety. concrete position fragmentation data structure determined lemma example depicted fig. shown later defragmentation used eventually restore expected order resulting samples. intuitively clear evalslide much efﬁcient evalstride since redundant computations avoided. analyzed rigorously sect. continuing theory short discussion notable special cases. first processing chain course required pooling layer following every subsignal compatible transformation even pooling layers all. consider ﬁxed layer index setting sees stridegj slidegj fragkj identity functions respective domains. words parameterization pooling layer causes like neutral bypass operation. hand would want pooling layers directly other completely analogous achieve neutral subsignal compatible transformation. special case convolution evaluated non-overlapping manner equivalently strided fashion. achieved plugging fconv sect. iii-c pooling kernel within processing chain. non-overlapping convolution however advantage computational complexity entire input signals processed using sliding window approach without accuracy loss lemma states strided fashion turned sliding fashion essentially carrying convolution conventionally advancing ﬁlter banks exactly sample evaluation. lemma requires length input signal satisfy certain divisibility constraints. extension statements signals arbitrary length requires additional operators deﬁnition natural number arbitrary dummy element stuﬀ concrete choice dummy element matter following considerations since output entries affected choice trimmed away end. possible state main theoretical result section theorem consider processing chain notation deﬁnition application strided fashion well-deﬁned lemma assume dimml) output entire processing chain applied strided fashion consists exactly sample. fig. example notions one-layered processing chain receptive ﬁeld size applied input signal samples. processing chain consists quot operator subsignal compatible transformation followed max-pooling stride example processing chain well-deﬁned divisibility requirements met. upper part shows result evalslide evaluated here max-pooling applied quot sliding fashion followed fragmentation producing fragments. lower part shows outcome evalstride applied different subsignals length max-pooling used strided fashion halves dimensionality. evident outcome evalstride found within slidemax although individual output signals interleaved reordering fragmentation corrects interleaving outcome always available contiguously. second layer processing chain ability process independent contiguous signals particularly effective real computing machines. function ﬁrst stuffs input signal many dummy samples fragmentation operation application processing chain sliding fashion comes even applies processing chain sliding fashion defragments outcome eventually removes superﬂuous entries emerged initial stufﬁng. subsignal compatible transformation dimensionality reduction constant furthermore evalstridel proof. note well-deﬁned since rem) well-deﬁned. lemma follows cdimml since dimml required lemma therefore defragk∗ well-deﬁned exactly output fragment number samples subsignals length stuffed input signal dimml holds thus trimr) well-deﬁned. since trimming reduces dimensionality follows dimm−dimml) ˜d−) therefore fulﬁlls dimensionality reduction property dimensionality reduction constant prove subsignal compatible hence sufﬁcient theorem show ﬁrst demonstrated evalstridel used prove weakened exchange property. sample since dimensionality reduction constant hence extraction ﬁrst sample result trimming operator equal extraction ﬁrst sample trimming operator’s argument. therefore scale entire processing chain layers evaluated subsequently. further shown theoretical speedup factorized simple expressions facilitating statements effect individual parameters. evalstridel evalstridel) hence subsignal compatible transformation theorem theorem ﬁnally implies slideevalstridel. concluded cnns turned efﬁciently computable subsignal compatible transformations using evalslide operator regardless input signal’s dimensionality. could suspect stufﬁng input signal dummy samples might negative effect efﬁciency. however number stuffed samples always less stride product ﬁnal layer hence small reasonably sized cnns. moreover stufﬁng guarantees fragments encountered evaluation homogeneous. enables tensors used sole data structure input data intermediate representations computation results. efﬁcient storing fragment individually especially massively parallel processors simple parallelized implementations achieve maximum throughput. section detailed theoretical analysis computational complexity processing chain evaluation carried out. introduced deﬁnition corresponds alternating application subsignal compatible transformations functions applied strided fashion. measuring computational complexity evalstride evalslide operators function evaluations required computing output layer counted. regarding subsignal compatible transformations unique functions generate transformations account theorem considered. shown evalslide requires number function evaluations evalstride layer. implies evalslide efﬁcient global assume situation lemma input signal well-deﬁned processing chain given. arbitrary layer index ﬁxed evalstridej analyzed arbitrary subsignal index proof lemma write evalstridej−) results evalstridej) stridegj theorem guarantees exactly function here evaluations necessary. since function evaluations carried possible subsignals total number function evaluations increases proportionately factor. redundant computations avoided evalslidej used instead. complexity processing individual fragment layer analyzed. overall complexity results multiplication number input analogous proof lemma fragments fragment index deﬁne determine resulting speedup redundant computations avoided ratio number function evaluations required naive approach evalstridej number needed input signal processed entirety using evalslidej evaluated. considering yields number subsignals number fragments included numerator denominator respectively. lemma lemma obtains d−b+ substituting minor algebraic manipulation sees since using lemma follows means evalslidej requires number applications evalstridej. merely special cases extent region layer equals length signal fragments dimensionality reduction constant subsignal compatible transformation speedup attains unity indicating approaches require number function evaluations. understood function dependent upon signal dimensionality quantity depends derived expression since constant independent seen lemma lemma requires divisible next larger feasible signal dimensionality subtracting evaluated signal dimensionality value extended signal dimensionality yields lemma implies therefore thus speedup increases signal dimensionality increased. limit case arbitrarily large input signals obtains limd→∞ speedup asymptotically attains ﬁnite value. evident greatest speedups achieved large regions interest small dimensionality reduction constants analysis strided function evaluation component turning function applied strided fashion incorporating number subsignals fragments evalstridej evalslidej respectively yields ratio simple expressions derived imply number function evaluations required evalslide always less equal evalstride layer. therefore also holds subsequent evaluation layers. further special cases identiﬁed computational complexity evalslide matches evalstride. moreover demonstrated speedup becomes signiﬁcant larger input signals although growth unbounded. analysis restricted amount necessary function evaluations neglected parallelization potential individual approaches. massively parallel processor throughput evalslide approach might substantially lower evalstride since coarsegrained parallelism subsignal level exploited facilitating load balancing thousands parallel computing cores. however experiments next section demonstrate that predicted theoretical analysis practice evalslide orders magnitude faster evalstride. section discusses practical considerations using theory proposed paper image processing tasks. further results experiments semantic segmentation runtime measurements real processors reported. context image processing two-dimensional signals referred images two-dimensional subsignals patches. generalization subsignal compatible transformation theory spatial dimensions straightforward shown exemplarily deﬁnition images represented matrices patch indices two-dimensional patch extraction forming submatrix adjacent entries. suppose transformation space sufﬁciently large images pixels images pixels fulﬁlls dimensionality reduction property dimensionality reduction constants rdimn rdimm cdimn cdimm −c+. exchange property generalizes condition patch× dimensions. here patch patch extraction operator subscripts specifying dimensionalities extracted patches patch dimensionalities rdimm cdimm patch indices. properties fulﬁlled called patch compatible transformation. remaining theory generalized analogously. practice sufﬁcient plain tensors data structure input data intermediate representations computation results. here dimension accounts feature index dimensions account spatial position within feature map. fourth dimension represents image index. although image processing fragmentation requires dimensions collapsed dimension linearizing two-dimensional fragment indices. since computations fragments carried independently deﬁnition essentially corresponds meaning image index dimension common tensor processing therefore fragmentation require modiﬁcations whatsoever computationally demanding routines tensor convolution. experimental evaluation cnns varying number convolutional layers created using following scheme. similar convolutional layer parameterized ﬁlter size pixels. output convolutional layer rectiﬁcation nonlinearity max-pooling layer inserted three pairs convolution rectiﬁcation layers unless pooling layer would ﬁnal layer network fig. depicts network architecture used evalstride operator transformed account image-based application using evalslide operator practicality approach proposed paper veriﬁed realizing semantic image segmentation evaluation classiﬁer feasible patches image images recorded wide angle camera attached windshield experimental vehicle manually labeled yield regions contain pixels four object categories road vehicle person background. classiﬁer convolutional layers described used. number output feature maps ﬁrst three convolutional layers subsequently doubled pooling layer. fully-connected layer four output feature maps appended project high-dimensional feature space label space followed ﬁnal softmax non-linearity ﬁrst trained patches extracted random positions images learning transformed image-based application thereafter ﬁne-tuned using entire images therefore fig. example network architecture targeted processing two-dimensional images. evalstride suitable patchbased application architecture consists four convolutional layers third convolutional layer max-pooling carried strided fashion. right-hand side shows architecture transformed evalslide approach suitable image-based application pooling applied sliding fashion followed fragmentation. input image stuffed guarantee fragmentation always comes even hence homogeneous tensors used data structure. effects stufﬁng fragmentation undone yielding output pixel input image. here denotes entire cnn’s receptive ﬁeld size huge number weight updates unbiased learning examples carried early phase training facilitating fast learning progress transformation learning entire images ensured feasible patches considered improving homogeneity reducing remaining misclassiﬁcation artifacts. note backpropagating gradients transformed straightforward gradients stufﬁng trimming trivial fragmentation defragmentation merely inversely related permutations. elementary calculus sufﬁcient determination gradient maxpooling sliding fashion. classiﬁcation decisions image test defragmentation carried shown fig. since three pooling layers involved accounting stride product fragments total. fragments represents low-resolution version ﬁnal output shifted corresponding number pixels either spatial dimension. defragmentation yields single output image fig. fig. visualization fragmented classiﬁcation decisions. fragment carries low-resolution information afterwards combined defragmentation operation fig. visualization ﬁnal output classiﬁer grayscale input image tinted defragmented classiﬁcation decisions. information obtained facilitates accurate representation scene front vehicle. classiﬁcation decisions available high resolution. output subsequently employed vehicle environment perception however efﬁcient additionally employ multi-scale analysis incorporates context information hence provides discriminant features technique considered paper space constraints. elaborate discussion theoretical background available technical report section analyzed theoretical speedup expected redundant computations eliminated means evalslide operator. conﬁrm whether speed signiﬁcantly increased real processors employed cnns applied entire two-dimensional images using patch-based image-based application. cnns parameterized described above number layers varied number output feature maps always renders computational complexity layer equal allowing undistorted evaluation overall speedup. degrees freedom cnns initialized random numbers random image data used input. restriction generality practicality results since focus analysis runtime measurements assessment achieved speedups. fig. speedup factors evalslide evalstride measured dependence number convolutional layers networks. second axis depicts resulting receptive ﬁeld size entire network either spatial dimension. note values non-linear since pooling layers inserted regular intervals networks. measurements indicate especially deep networks huge speedups obtained images processed entirety. cudnn software library. implementation employed intel core processor. here intel integrated performance primitives openmp libraries used increasing efﬁciency. images height pixels width pixels single feature used input networks. time required carrying operators measured ratio taken determine speedup. measurements repeated twenty times twenty distinct input images average resulting four hundred runs used evaluation. evalstride neither time required extracting feasible patches storing dedicated tensor time required assembling ﬁnal output tensor included measurements. memory constraints tensor patches broken batches processing gpu. batch size maximized respect available memory ensure maximum throughput graphics card. evalslide time required stufﬁng fragmentation defragmentation trimming included measurements. here splitting batches necessary since redundancies overlapping patches avoided memory demands therefore low. measured speedups hence biased favor evalstride computations overhead organizing data structures considered. achieved speedups evalslide evalstride dependence number convolutional layers depicted fig. although input images rather small notable speedup could determined even shallow networks. even signiﬁcant speedups could measured although theoretical number function evaluations equal operators. however still huge memory redundancy tensor storing patches necessary evalstride disadvantageous terms memory throughput. implementation achieved speedup factors beyond hundred cires¸an meier schmidhuber multi-column deep neural networks image classiﬁcation proceedings ieee conference computer vision pattern recognition szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition grangier bottou collobert deep convolutional networks scene parsing international conference machine learning workshop learning feature hierarchies giusti cires¸an masci gambardella schmidhuber fast image scanning deep max-pooling convolutional neural networks ieee international conference image processing thong kadoury pich´e convolutional networks kidney segmentation contrast-enhanced scans computer methods biomechanics biomedical engineering imaging visualization nuss thom danzer dietmayer fusion laser monocular camera data object grid maps vehicle environment perception proceedings international conference information fusion sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks proceedings international conference learning representations. chetlur woolley vandermersch cohen tran catanzaro shelhamer cudnn efﬁcient primitives deep learning advances neural information processing systems deep learning representation learning workshop. simonyan zisserman very deep convolutional networks large-scale image recognition proceedings international conference learning representations. sanger optimal unsupervised learning feedforward neural networks master’s thesis massachusetts institute technology bottou lecun large scale online learning advances neural information processing systems vol. thom gritschneder rapid exact signal scanning deep convolutional neural networks tech. rep. arxiv. taylor optimizing applications multi-core processors using implementation required deeper networks similar speedup. gpu’s superior parallelization capabilities facilitated overproportional throughput evalstride operator patches could processed parallel. peak speedup respectively noted. deeper networks used relative speedup decreased remained high level. reason large receptive ﬁeld size deep networks since patch size almost matched image dimensions relatively patches compared situation smaller receptive ﬁeld size. predicted theoretical results sect. degree redundancy evalstride decreases case resulting decreased relative speedup. finally execution times evalslide using implementation versus implementation compared. averaging yielded relative speedup implementation factor implementation demonstrating massive parallelization potential. paper introduced analyzed concept subsignal compatible transformations functions allow exchanging subsignal extraction function evaluation without effect outcome. demonstrated cnns applied efﬁciently without accuracy loss large signals using sliding window approach homogeneous data structures eliminating redundant computations special case treatment. theoretical analysis proven computational complexity processing input signal entirety inferior subsignal-based application subsequently veriﬁed numerical experiments. theoretical results proven rigorously mathematically demonstrating exactness proposed approach. theoretical framework developed paper facilitates further research gaining deeper insight related methods dense signal scanning. fukushima neocognitron self-organizing neural network model mechanism pattern recognition unaffected shift position biological cybernetics vol. lecun boser denker henderson howard hubbard jackel handwritten digit recognition backpropagation network advances neural information processing systems vol. left-hand side equation form integer right-hand side discrete interval seen substituting extreme values operator. hence equation satisﬁed sides vanish. yields claimed identities. proof remark omitted main part paper space constraints proof kkq×s fragmented signal. deﬁne fragk kq×ks fragk q×kks fragkk q×kks. since equal size enough show entry-wise equivalence. kks}. lemma follows using indices main part paper shown cnns efﬁciently evaluated entire images theory subsignal compatible transformations. functions take multiple spatial resolutions single signal input considered. since context local regions incorporated addition here approach proven highly effective classiﬁcation tasks assumed number samples considered time ﬁxed scale levels. facilitates design scale-invariant representations example using classiﬁer scales input analysis here however restricted situation classiﬁer used scales. instead analysis conducted directly different functions applied scale. signal downscaled application lowpass ﬁlter reduce aliasing artifacts followed downsampling operator returns subset equidistant samples. subsignal extracted downscaled input signal contain downscaled copy corresponding subsignal original input signal. requires boundary-handling input signal since example ﬁrst subsignal cannot extended allow larger context means original samples. following denote integers denote ceiling function rounds argument next larger natural number. first concepts boundary handling subsignal extraction subject boundary handling formalized deﬁnition denote subsignal dimensionality boundary size. function called boundary-handling function deﬁnition boundary-handling function leaves open concrete values returned access outside original signal allowing great ﬂexibility. example functions realize dirichlet neumann boundary conditions respectively. padded subsignal extraction operator strict generalization subsignal extraction operator second argument boundary-handling function fulﬁlls dimm denotes input signal dimensionality. therefore case holds ξi+ν− deﬁnition boundary-handling function hence subsignalpadϑ called multi-scale subsignal index transformation. suppose lowpass ﬁlter kernel size here hold avoid aliasing artifacts. further subsignal dimensionality boundary size boundary-handling function. function multiscalesubsignal downsampling operator well-deﬁned dimm holds dimm seen case-by-case analysis depending whether divides dimm not. deﬁned functions clearly well-deﬁned. requirements extraction downscaled subsignals makes sense. important correct determination boundary size deﬁnition multiscalesubsignal operator. chosen extracted subsignals scale level always centered exactly around corresponding subsignals original scale level. moreover beneﬁcial entire input signal downscaled entirety using operation output multiscalesubsignal operator equals simple extraction subsignals downscaled signal. however approach pursued subsignals original signal possess downscaled counterpart representation. multiscaleindex function alleviates problem computation appropriate subsignal index always guaranteed possess downscaled counterpart. although merely approximation assured correct subsignal index downscaled signal always less sample off. next result formalizes thoughts illustration statements presented fig. fig. lemma downsampling step size required moreover lowpass ﬁlter kernel size subsignal dimensionality suppose boundary-handling function. deﬁne samples index subsignals index words padded subsignals centered around original subsignals. subsignal index write multiscaleindexk result index transformation. index adjustment hence decreases subsignal indices samples respect original scale level. fig. illustration multi-scale analysis approach detailed lemma image processing example. original image padded pixels using neumann boundary conditions. downscaling padded image scale factor yields small image right-hand side. area original image region interest pixels either dimension extracted conventionally using subsignal operator. subsignalpad operator extracts extended region interest pixels either dimension provides context original region interest. extended region downscaled guaranteed choice resulting signal possesses samples either dimension. downscaled padded region equivalent application multiscalesubsignal operator. outcome also found downscaled padded image indices adjusted multiscaleindex function. figure best viewed color. step i+µ− used. here boundary handling function evaluates original sample input signal. hence samples middle subsignalpadϑ stem input signal subject boundary conditions. first note deﬁnition ceiling function marked following. therefore dimm dimm σ+σ+σ lemma upper part shows input signal samples padded samples either using dirichlet boundary conditions lowpass-ﬁltered downsampled using step size yields downscaled signal lower part illustrates process extracting padded subsignal subsignal index downscaling yielding multiscalesubsignal stated lemma padded subsignal centered around original subsignal illustrated colored pattern graphics. further located subsignal illustrated different colored pattern guaranteed lemma since ﬁxed point multiscaleindexk. note example fourth subsignal possess exact downscaled correspondence predicted theoretically. given claim. clearly since follows hand euclidean division yields analogously follows proves claimed inequalities. arbitrary subsignal index. ﬁrst shown right-hand side claimed identity indeed dimensionality this deﬁne multiscaleindexk multiscalesubsignal abbreviations. analogously expression deduced marked following obtains ultimate goal analyze functions applied different scale levels signal propose efﬁcient evaluation scheme. ﬁrst step already taken analyzing connection downscaled subsignal extraction subsignal extraction downscaled signal lemma complement downscaling course action repeat samples many times samples omitted downsampling. leads following deﬁnition deﬁnition function deﬁnition holds dimm indeed bijection index sets therefore operator well-deﬁned sample output copy certain sample input signal. statement samples upsampling directly follows lemma upkν ξdiv+ kq}. proof. deﬁnition exists upkν obtains here hence uniqueness euclidean division implies claim follows. main result appendix states circumstances function accepts inputs original scale downscaled version evaluated efﬁciently. indexing rules follows. suppose sets signal paired samples exists dimensionality since also express pair signals index individual sample. theorem sets constant subsignal dimensionality. further function accepts signals original scale downscaled version. assume factorized functions gorg gdown words applied subsignals certain multi-scale subsignals equals output samples gorg gdown applied sliding fashion signals derived gdown applied downscaled signal result upsampled superﬂuous trailing entries trimmed away. theorem directly provides algorithm efﬁcient multi-scale analysis. functional part gorg operating original input signal applied sliding fashion. function cast processing chain discussed main part paper theory proposed used efﬁcient evaluation. multi-scale subsignal index approximation proved lemma facilitates application functional part gdown operating downscaled subsignals sliding fashion well. therefore subsignal compatible transformation theory applied also. ﬁnally noted generalization statements theorem functions process arbitrary number different downscaled signals straightforward proper factorization provided. convolution trainable ﬁlter banks enables artiﬁcial neural networks solve important tasks domains signal restoration signal classiﬁcation. transposed convolution operation emerges computation gradient convolution since conventional convolution expressed matrix-vector product gradient involves transpose matrix. operator sometimes also called backwards convolution fractionally strided convolution deconvolution note term deconvolution mixed context process reversing effects convolution sense solving inverse problem. artiﬁcial neural networks transposed convolution facilitates upsampling internal representations network output example used obtain output values feasible subsignal larger input signal appendix analyzes transposed convolution greater detail. exact deﬁnition given proved well-deﬁned. then analyzed upsampling zero-order hold realized transposed convolution. afterwards operator called dense upsampling convolution originally proposed introduced. eventually shown operator equivalent certain parameterization transposed convolution. remark boundary handling functions holds cropp padϑ proof. arbitrary boundary handling function. further arbitrary signal dimm denote length. padϑ deﬁnition cropp applied padded signal producing output signal deﬁnition follows fig. spreading operator inserts zeros samples input signal distance original samples equals stride parameter. number zeros pair samples therefore stride minus one. depicted example input signal samples spreaded using stride stride yielding output signals samples respectively. proof. first note dimm always follows therefore output coordinates deﬁnition spreadk distinct. consider case divides clearly div. deﬁne dimm clearly positive integer dimm follows spreadki ξdiv+. suppose divide cannot exist index dimm since would imply divides therefore spreadki vanishes. deﬁnition ring ﬁlter bank spatial extent mapping input channels output channels. stride denote minimum input signal length. padding size. max{ transposedconvolution called transposed convolution operator. ﬁrst spreads input signal using stride then full convolution ﬁlter bank carried ﬁrst padding spreaded input signal respecting dirichlet boundary conditions performing conventional valid convolution. eventually samples removed ends result convolution. note names stride padding parameters deﬁnition actually refer convolution transposed convolution computes gradient. transposed convolution conventional unitstride convolution carried input signal spreaded using stride parameter. moreover padding parameter speciﬁes number samples removed convolution operation. minimum input signal length depends spatial extent used ﬁlter bank stride padding size. next result shows requirement correct deduces output signal length transposed convolution lemma situation deﬁnition transposed convolution operator well-deﬁned. output signal’s spatial extent dimrn) domain operator. proof. signal domain transposedconvolution. clearly spreadk padϑst showing transposed convolution well-deﬁned ﬁnally demonstrated length implies least deﬁnition first consider case guaranteed dimrm follows hence well-deﬁnedness follows case. hand lemma evident transposed convolution capable producing signals output dimensionality strictly greater input dimensionality. therefore cannot subsignal compatible transformation. instead possible upsample signals operation. shown explicitly certain parameterization ﬁlter bank lead upsampling zero-order hold introduced deﬁnition ality upsampling even integer factor remark suppose denotes even upsampling factor. ring ﬁlter bank number input channels output channels spatial extent denote stride padding size respectively. dimrn) dimrm proof. quantities clearly positive natural numbers. also applies since therefore required even positive. minimum input signal length transposed convolution using parameterization dimrm. dimrn) statements regarding properties sample indices emerge analysis upsampling transposed convolution formulated. ﬁrst result used later show special case convolution spreaded signal reduces multiplication scalars since relevant values spreaded signal vanish lemma assume proof. consider function rem. arbitrary deﬁne idempotence operator. therefore surjective. mapping ﬁnite sets cardinality hence bijection. claim follows since next statement used prove special case boundary handling necessary since transposedconvolution words transposed convolution wzoh carries upsampling zero-order hold. proof. remark follows parameterization claim performs upsampling factor non-empty input signals. therefore remains shown output samples match. input signal write dimrm further arbitrary index respect spatial output dimension denote index arbitrary output channel. ﬁlter bank wzoh theorem sparse binary. first interconnection distinct channels channel processed independently others. second non-vanishing entries located middle spatial dimension. therefore output sample depends single input sample spreading input signal. sophisticated upsampling methods linear interpolation realized ﬁlter banks entire spatial dimension populated non-vanishing entries. number output channels. used example classiﬁcation tasks achieve projection downscaled internal representation label space increased spatial resolution here employed ﬁlter bank adapted concrete sample data intention recover detailed information otherwise lost conventional interpolation used. however heuristic lead result exact dense signal scanning. deﬁnition upsampling factor ring positive natural numbers. furthermore suppose ﬁlter bank unit spatial extent used mapping input channels output channels. arbitrary positive natural numbers rb×a helper function transforming multi-channel representation fragmented representation. function clearly bijection inverse given bj)i rb×a ducw called dense upsampling convolution operator. convolves input signal ﬁlter bank provides increased number output channels followed defragmentation restore intended number output channels increased spatial resolution. well-deﬁnedness output dimensionalities lemma situation deﬁnition operator well-deﬁned. increases spatial extent output signal factor dimrn) dimrm proof. input signal dimrm length. welldeﬁned produces signal rd×un hence defragmentation applied leads signal defragu) rud×n. eventually holds ducw ud)) implies dimrn) dense upsampling convolution equivalent special case transposed convolution theorem ring. suppose denote number input output channels respectively. further denote upsampling factor ﬁlter bank. function )λ)ν +)λ)rem+ realizes reordering ﬁlter bank shape compatible dense upsampling convolution operator. ducψ transposedconvolution here transposed convolution operator used spatial ﬁlter size equals stride furthermore equivalent upsampling factor padding used since padding size vanishes. proof. suppose input signal. ducψ lemma minimum spatial input signal length transposed convolution unity. further transposedconvolution lemma signals sides claimed identity hence compared sample level. analysis begun dense upsampling convolution operator. arbitrary indices. theorem guarantees dense upsampling convolution expressed special case transposed convolution without accuracy loss. practice either implementation using conventional convolution defragmentation alternative implementation using transposed convolution efﬁcient. depends concrete parameterization employed processor implementation required routines. appendix studies alternative evalslide operator proposed main part paper. original work proposed modifying convolution pooling carried transform subsignal-based application signal-based application. d-regularly sparse kernels enlarge receptive ﬁeld size operators insertion vanishing entries regular intervals. proven rigorously method suffer accuracy loss. moreover similar equivalent concepts reported recently different names namely ﬁlter rarefactions strided kernels dilated convolutions atrous convolutions claimed methods equivalent fragmentation-based approach advocated paper. appendix subsignal compatible transformation theory used analyze modiﬁed operators. notational convenience hereafter referred application functions dilated fashion. proved eventually direct connection naive subsignal-based application using evalstride operator dilated signal processing scheme. consequence shown approach involve accuracy loss. further computational complexity analysis shows dilated function application application functions sliding fashion fragmented signals share number required function applications. practice however fragmentation-based approach beneﬁcial since facilitates direct usage readily available routines computationally demanding tensor convolution. also noted ﬁrst implemented atrous convolution using dilated function application scheme switching fragmentation. illustration operator shown fig. clear deﬁnition i-th dilated subsignal starts i-th sample original input signal samples input signal always skipped extraction individual samples. minimum input signal length ensures feasible dilated subsignal indices always non-empty hence always contains unity. maximum dilated subsignal index guarantees maximum sample index used accessing input signal corresponds signal length therefore accesses within bounds operator well-deﬁned. eventually operator becomes strict generalization subsignal extraction operator since follows dilatedsubsignal subsignald. words operator extracts feasible dilated subsignals input signal using dimensionality function speciﬁed stride applies stores outcome contiguously fig. extraction dilated subsignals signal samples extracted dilated subsignal samples stride within original signal. left-hand side ﬁrst feasible dilated subsignal shown. right-hand side depicts ﬁnal dilated subsignal maximum index signal. minimum length input signal chosen least dilated subsignal exists. dimensionality result dimm matches exactly number dilated subsignals therefore dilate well-deﬁned. moreover generalization application function sliding fashion since dilate slidef. straightforward verify application function dilated fashion direct connection application sliding fashion non-trivial fragmentation defragmentation involved remark sets function divides dimm proof. dilate well-deﬁned choice fragk well-deﬁned since dimm required divisible since slidef alter number fragments defragk applied result here. write dimm left-hand side samples. since fragk follows slidef therefore result defragmentation exactly fragment samples. dimensionalities match sides remains shown samples equivalent. fig. upper part depicts illustration operator applied dilated fashion input signal samples. here accepts input samples applied dilated subsignals using stride lower portion illustrates statement remark input ﬁrst fragmented using parameter followed application sliding fashion defragmentation operation outcome application dilated fashion. combination corresponding pair input samples resulting output sample highlighted colored patterns graphics illustrating dilate access strided data output sample whereas slide able process contiguous samples fragmentation carried beforehand. words layer evaldilate cascade applies functions dilated fashion output previous layer. stride function application chosen equal previous layer’s stride product. analogous main part paper notion analyzed rigorously lemma consider processing chain deﬁnition suppose holds divides dimnj evalstridej non-empty evalstride well-deﬁned. input signal samples. contrast lemma divisibility constraints imposed facilitates processing input signals arbitrary length greater equal processing chain’s receptive ﬁeld size prevents results evalslide operator used here. lemma dimmj denote dimensionalities encountered evalstride evaluation independent concrete subsignal index dimmj denote dimensionalities intermediate representations layer application processing chain dilated fashion. following holds therefore evaldilate well-deﬁned. similar evalslide intermediate representations evaldilate operator contain complete output evalstride operator applied feasible subsignals input signal relevant layer. remark shown connection dilated function application fragmentation. statement used establish connection evaldilate evalslide independent lemma remark also possible prove identity lemma using results main part paper. since evalslide operator required well-deﬁned ﬁnal stride product must divide lemma following holds situation lemma defragk∗ evaldilatej defragk∗ evalstridej) dilatedsubsignal evalstridej) dilatedsubsignal remark follows follows directly subsequent preparation main result appendix formulated. states application processing chain dilated fashion leads outcome naive subsignalbased application theorem consider processing chain using notation deﬁnition evaldilate operator deﬁnition evalstride operator. suppose divides dimnj evalstridej non-empty evalstride well-deﬁned. further assume dimml) words processing chain outputs signals unit spatial size using subsignal-based application. evaldilatel slideevalstridel therefore evaldilatel subsignal compatible transformation dimensionality reduction constant proof. subsignal index. dimml) evaldilate operator produces correct outcome circumstances computational complexity analyzed. analogous analysis evalstride evalslide operators required function evaluations counted compared. ultimately shown computational complexity evaldilate measured means exactly equals evalslide. therefore theoretical results obtained complexity evalslide respect asymptotic behavior comparison evalstride operator hold evaldilate well. comparison evalslide requires input signal length satisfy certain divisibility constraint since guarantees evalslide well-deﬁned. first consider simple relationship turn helpful later remark situation lemma assume well-deﬁned lemma proof. obtains substituted input signal number samples chosen evalslide well-deﬁned. further ﬁxed layer index evaldilatej− abbreviation. output j-th layer using evaldilate operator conclusion computational complexity terms function evaluations dilated function application fragmentation-based approach. evaldilate necessary functions convolution generalized support strided input data permutation realized fragmentation evalslide approach facilitates usage routines operating contiguous data. therefore cnns readily available highly-optimized implementations tensor convolutions employed without modiﬁcations whatsoever. focus paper study efﬁcient signal processing schemes carry dense signal scanning without accuracy loss. appendix analyzes method processing chain application naively uses processing pipeline signal-based application subsignal-based application. theory developed paper suggests approach cannot produce result exact dense signal scanning hence must involve relaxations kind. here negative effects output signal quality relaxed processing chain application exactly characterized. analysis sheds light approaches employ scheme central part computations helps identifying possible limitations optimization potential. further relationship relaxed application fragmentation-based approach investigated computational complexity analysis conducted. note deﬁnition evalrelax essentially evalstride except domain allows arbitrarily large input signals samples. further evalrelax welldeﬁned unless constraints input signal length fulﬁlled. constraints fundamental dynamics operator detailed below lemma suppose processing chain deﬁnition given. assume divides dimnj evalstridej non-empty input signal dimensionality. assume stride product ﬁnal layer divides difference d−b. input signal dimmj denote length intermediate representations processing chain applied relaxed fashion lemma dimmj denote dimensionalities encountered application processing chain strided fashion actually independent concrete subsignal index note numbers necessary divisibility requirements number subsignals lemma fulﬁlled. following holds application processing chain relaxed fashion layer well-deﬁned. holds index represents feasible subsignals evalrelaxj therefore identity provides complete characterization information available intermediate representations application processing chain relaxed fashion. hand left-hand side describes result evalstridej operator applied subsignal input signal words application processing chain relaxed fashion results loss spatial resolution non-trivial stride products. well-deﬁnedness evalrelaxj must shown component applicable. slidefj always applicable non-empty signals. therefore enough show stridegj applicable outcome slidefj overall result j-th layer non-empty signal. deﬁnition required dimensionality input stridegj divisible requirement input signal length natural number since evalstridej well-deﬁned requirement another natural number dimnj step relations divisibility requirements substituted. since positive dimensionality divisible hence stridegj applicable. resulting signal length µ-th sample left-hand side claim. therefore identity holds. lemma shown output evalrelax operator consists result evalstride operator applied subsignals input signal. example illustrated fig. example application processing chain strided fashion subset feasible subsignals input signal relaxed fashion entire input signal processing chain layer consists quot operator followed max-pooling stride resulting receptive ﬁeld size evalstride well-deﬁned computes output signals sample. input signal samples total. since implies divides statements lemma hold. particular result evalrelax evaluated samples. contains result evalstride evaluated subsignals indices fig. next result analyzes greater detail shows divisibility constraints lemma satisﬁed arbitrary input signals without incurring output signal quality loss already involved application processing chains relaxed fashion. theorem suppose processing chain deﬁnition given evalstride well-deﬁned. assume processing chain applied strided fashion outputs signals exactly sample dimml) further assume stride product l-th layer non-trivial applies processing chain relaxed fashion input signal ﬁnal samples slideevalstridel result equals exactly result trimmed away. downk∗ conventional dense signal scanning downsampled ﬁnal stride product proof. consider input signal dimm denote dimensionality. must veriﬁed ﬁrst trimming actually permits evalrelaxl applied. showing this deﬁne deﬁnition deﬁnition dimm idempotence operator. therefore lemma guarantees evalrelaxl applied trimrevalrelaxl hence well-deﬁned. conclusion processing appropriately trimmed signals evalrelax obtains result dense signal scanning followed downsampling. words evalrelax involves precision loss loss spatial resolution since output samples available l-th input subsignal. illustrated fig. exemplary processing chain. hence amount pooling layers signiﬁcant inﬂuence output signal quality processing chain applied relaxed fashion rather using exact notions discussed earlier. degradation ameliorated example using skip architecture includes forward connections intermediate pooling results ﬁnal processing chain output restore limited amount spatial information. nevertheless method yield result exact dense signal scanning. although evalrelax reduces spatial output signal resolution factor equal ﬁnal layer’s stride product straightforward perform multiple passes operator using shifted versions input signal afterwards combine result obtain full resolution output. also noted exploited similar method increase spatial resolution. exact shift-and-stitch approach analyzed greater detail context fragmentation-based approach theorem consider processing chain deﬁnition evalstride well-deﬁned. input signal dimensionality input signal. denotes output signal dimensionality evalslidel applied signal length lemma here left-hand side equals γ-th fragment result processing chain applied sliding fashion right-hand side outcome processing chain applied relaxed fashion subsignal starting γ-th sample length chosen evalrelaxl applicable. words since evalrelax known produce downsampled version result exact dense signal scanning theorem considering shifted versions input signal used restore original resolution. proof. left-hand side claim well-deﬁned since prerequisites lemma satisﬁed accesses within bounds lemma considering right-hand side ﬁrst note actually entries trimmed away evalrelax operator divides revalrelaxl fig. detailed illustration processing chain application applied since relaxed fashion. yields output signal three samples shown theorem corresponds exactly result conventional dense signal scanning downsampled ﬁnal stride product fig. although evalrelax operator computes output values regular subset feasible subsignals input signal repeated evaluation shifted versions leads dense output values feasible subsignals. illustrated lower part graphics input signal samples processing chain layer consisting quot operator max-pooling stride resulting ﬁnal stride product shown theorem evalrelax applied input signal outcome shifted evaluation corresponds exactly output fragment evalslide operator processes entirety particular ﬁrst output fragment matches result evalrelax applied trimmed version input signal implied theorem veriﬁed signal input evalrelaxl operator least samples. requirement positive natural number moreover proposition implies rem. since number vanishes requirement input signal length divisibility constraint lemma fulﬁlled right-hand side well-deﬁned. furthermore dimensionality right-hand side here output signal length evalrelaxl applied signal length lemma hence number samples sides claim equal contents output signals compared. comparison carried subsignal level rather sample level allows ﬁxed fragment index direct application lemma this shifting offset. number subsignals length lemma concrete deﬁnition claimed identity natural number requirement. arbitrary subsignal index deﬁne div. abbreviation input πevalrelaxj evalrelaxj−) stridegj deﬁnition since difference non-negative ¯kj− larger input signal dimensionality. further obtains limd→∞ sevalrelax maximally achievable speedup greatest early layers. here numerator corresponds evaluations evalrelaxj. denominator corresponds fragmentation-based approach main part paper. using remark lemma yields computational complexity analysis proved signiﬁcant amount redundant computations involved evalrelax used obtain full resolution output using shift-and-stitch approach. case method avoids redundant computations discussed earlier paper preferred. however certain case downsampled output signals sufﬁcient evalrelax operator used achieve reasonable computational complexity instead evaluations evalrelax carried computational complexity analysis repeated simultaneously omitting factor numerators ratios number function evaluations ratios hence demonstrates plain evalrelax requires fraction operations hence lower computational complexity course expense yielding low-resolution output signals compared exact approaches discussed earlier. ﬁnally noted maximum speedup achieved layer intermediate stride products. necessarily equal ﬁnal stride product corresponds actual downsampling factor. conclusion overall speedup inferior sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks proceedings international conference learning representations. arxiv. long shelhamer darrell fully convolutional networks semantic segmentation proceedings ieee illustration effect mixed processing chain application spatial resolution output signal using three-layered fig. processing chain strides example. purely sliding evaluation using evalslide operator analyzed main part paper involves loss spatial resolution whatsoever extreme applying layer relaxed fashion evalrelax operator results decrease spatial output resolution factor equal ﬁnal stride product application mixed fashion using evalrelaxslide operator introduced appendix offers compromise extremes. here ﬁrst layers applied relaxed fashion accounting resolution loss factor then remaining layers applied sliding fashion preserving spatial resolution. example ﬁrst layers applied relaxed fashion third layer applied sliding fashion. results moderate resolution loss factor application processing chain relaxed fashion discussed previous appendix results downscaled output signal. precisely output values obtained l-th input subsignal denotes ﬁnal stride product processing chain. appendix investigates application processing chain mixed fashion ﬁrst layers applied relaxed fashion followed application remaining layers sliding fashion fragmentation-based approach studied main part paper. shown approach facilitates direct control spatial output resolution choosing many processing chain’s layers applied relaxed fashion. example illustrated fig. besides degradation output resolution precision loss involved method. somewhat related approach stride application dilated convolution pooling systematically manipulated obtain output signals reduced spatial resolution however greater output resolution purely relaxed approach. unclear ﬁrst approach would lead desired results satisﬁes correctness statements sense proven rigorously combination relaxed application fragmentation-based approach provides efﬁcient trade-off full output resolution completely reduced resolution. developed results show divisibility requirements fulﬁlled prior actual processing chain application enabling homogeneous data structures times therefore optimizing throughput massively parallel processors. mixed fashion deﬁnition consider processing chain deﬁnition ﬁxed layer unique functions fulﬁll slidefj index. theorem operator evalrelaxslide deﬁned operator parameterized layer index ﬁrst layers processing chain applied relaxed fashion analogous deﬁnition remaining layers applied sliding fashion using fragmentation-based approach deﬁnition therefore extreme cases would correspond purely sliding fashion purely relaxed fashion respectively excluded deﬁnition already analyzed. following result elaborates divisibility requirements well-deﬁnedness. moreover shown application processing chain mixed fashion results loss spatial resolution loss precision. amount output signal degradation explicitly controlled parameter. lemma consider processing chain deﬁnition suppose evalstride well-deﬁned dimmj denote lengths intermediate representations operator applied arbitrary signal least minimum input signal length evalrelaxslide required deﬁnition suppose denote intermediate dimensionalities relaxed fashion part evalrelaxslide. numbers equal exactly lemma symbol. further denote dimensionalities encountered application fragmentation-based part evalrelaxslide. following holds intermediate stride product words evalrelaxslide equals result evalstridel applied proof. proof presented uses lemma relaxed fashion parts explicit arguments sliding fashion parts. alternative somewhat obscure proof could proceeded evalrelaxj therefore lemma guarantees evalrelaxslide operator applied layer. claimed dimensionality follows directly lemma abbreviation input considered layer. since layers completely agnostic fragmentation indeed holds rdimm) cdimm) k+u+ lemma step substituted. divides fragk+ applied slideg++)). yields non-empty signal positive hence evalrelaxslide well-deﬁned. number output fragments samples. claimed identity immediately follows result. eventually ﬁnished induction proved analyzed operator well-deﬁned claimed identities hold. lemma shown application evalrelaxslide operator well-deﬁned provided ﬁnal stride product operator computes exactly evalstridel operator satisﬁed. further evalrelaxslide applied subsignal samples input signal. therefore tuning number layers applied relaxed fashion degree loss spatial output resolution becomes controllable factors resolution loss desired equivalent factor conventional fragmentation-based approach used. extreme full resolution loss factor achieved layers applied relaxed fashion done evaluation evalrelax operator. lemma requires length input signal satisﬁes certain divisibility requirements. here approach developed allows processing input signals arbitrary number samples. first relationship remainders respect numbers divides other established lemma divides integer proof. since divides positive natural number preparations algorithm application processing chain mixed fashion signal arbitrary dimensionality proposed proved correct. algorithm summarized fig. consists operating modes input signal ﬁrst trimmed dummy samples ﬁrst stuffed input signal. modes used eventually depends concrete input signal dimensionality. theorem suppose processing chain deﬁnition given. suppose application processing chain strided fashion well-deﬁned outputs signals trivial spatial extent dimml) fig. flowchart algorithm application processing chain mixed fashion signals arbitrary dimensionality. depending concrete input signal length either trimming operating mode stufﬁng operating mode carried out. guaranteed compute result conventional dense signal scanning followed downsampling using factor equal intermediate stride product denote operator implements trimming operating mode. here input signal ﬁrst trimmed satisfy divisibility constraints. then processing chain applied mixed fashion. finally output signal defragmented obtain overall result. implements stufﬁng operating mode. first divisibility requirements fulﬁlled appending certain number dummy samples input signal. processing chain applied mixed fashion output defragmented. superﬂuous samples emerged stufﬁng ﬁnally removed. decides operating modes used based input signal’s number samples returns result. slideevalstridel. words yields result conventional dense downk∗ signal scanning followed downsampling intermediate stride product proof. input signal write dimm. based concrete value proof divided cases ttrim tstuﬀ respectively. first statements required analyzing cases shown dimml downk∗ tstuﬀ downk∗ combination ﬁnally implies downk∗ input signal dimensionality proof common statements. first statements downsampling commonly required cases shown obtains follows difference positive. therefore conclu trims sion tstuﬀ well-deﬁned. dimensionality equals difference analyzed deﬁnition requirement simpliﬁed using lemma summing claim holds regardless whether since shown dimensionality signals matches remains shown individual samples equal. this dimml)} sample index. revalrelaxslide situation non-trivial defragk∗ trimming stufﬁng input signal necessary processing chain application mixed fashion. note conditions exactly requirements lemma underlining theorem non-trivial generalization result. conclusion method efﬁciently scanning signals controllable trade-off spatial output signal resolution required computational complexity proved always produce correct outcome. since approach relies relaxed fragmentation-based processing beneﬁts efﬁcient homogeneous data structures absence necessary modiﬁcations computationally demanding functions conserved.", "year": 2015}