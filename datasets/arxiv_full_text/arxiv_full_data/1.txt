{"title": "Dual Recurrent Attention Units for Visual Question Answering", "tag": ["cs.AI", "cs.CL", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA dataset.", "text": "paper propose rnn-based joint representation generate visual textual attention. argue embedding joint representation helps model process information sequential manner determine relevant solve task. refer combination embedding attention recurrent textual attention unit recurrent visual attention unit respective purpose. furthermore employ units fairly simple network referred dual recurrent attention units network show improved results several baselines. finally enhance state-of-the-art models replacing model’s default attention mechanism rvau. main contributions following introduce novel approach generate soft attention. best knowledge ﬁrst attempt generate attention maps using recurrent neural networks. provide quantitative qualitative results showing performance improvements depropose architecture utilizes recurrent layers generate visual textual attention. memory characteristic proposed recurrent attention units offers rich joint embedding visual textual features enables model reason relations several parts image question. single model outperforms ﬁrst place winner dataset performs within margin current state-of-the-art ensemble model. also experiment replacing attention mechanisms state-of-the-art models implementation show increased accuracy. cases recurrent attention mechanism improves performance tasks requiring sequential relational reasoning dataset. although convolutional neural networks recurrent neural networks successfully applied various image natural language processing tasks breakthroughs slowly translate multimodal tasks visual question answering model needs create joint understanding image question. multimodal tasks require joint visual textual representations. since global features hardly answer questions certain local parts input attention mechanisms extensively used recently attempts make model predict based spatial lingual context. however attention mechanisms used models rather simple consisting convolutional layers followed softmax generate attention weights summed image features. shallow attention mechanisms fail select relevant information joint representation question image. creating attention complex questions particularly sequential relational reasoning questions requires processing information sequen attention modules modular thus substitute existing attention mechanisms models fairly easily. show state-of-the-art models rvau plugged-in perform consistently better vanilla counterparts. bilinear representations fukui compact bilinear pooling attend image features combine language representation. basic concept behind compact bilinear pooling approximating outer product randomly projecting embeddings higher dimensional space using count sketch projection exploiting fast fourier transforms compute efﬁcient convolution. ensemble model using ﬁrst place challenge. argues compact bilinear pooling still expensive compute shows replaced element-wise product linear mapping gives lower dimensional representation also improves model accuracy. recently ben-younes proposed using tucker decomposition low-rank matrix constraint bilinear representation. propose fusion scheme architecture refer mutan writing current state-of-the-art dataset. attention-based closely related work ﬁrst feature co-attention mechanism applies attention question image. dual attention network employs attention text visual features iteratively predict result. goal behind allow image question attentions iteratively guide synergistic manner. rnns using recurrent neural networks explored past. xiong build upon dynamic memory network kumar varaiya proposes dmn+. dmn+ uses episodic modules contain attention-based gated recurrent units note propose; xiong generate soft attention using convolutional layers uses substitute update gate gru. contrast approach uses recurrent layers generate attention. propose recurrent answering units unit complete module answer question image. notable mentions kazemi elqursh show simple model state-of-the-art results proper training parameters. construct textual representation semantic content image merges textual information sourced knowledge base. introduce task identifying relevant questions vqa. apply residual learning techniques propose novel attention image attention visualization method using backpropagation. propose method section. figure illustrates information drau model. given image question create input representations next features combined convolutions separate branches. then branches passed rtau rvau. finally branches combined using fusion operation ﬁnal classiﬁer. full architecture network depicted figure input representation image representation -layer resnet pretrained extract image features. similar resize images extract last layer ﬁnal pooling layer size finally normalization dimensions. recently anderson shown object-level features provide signiﬁcant performance uplift compared global-level features pretrained cnns. therefore experiment replacing resnet features frcnn features ﬁxed number proposals image question representation fairly similar representation short question tokenized encoded using embedding layer followed tanh activation. also exploit pretrained glove vectors concatenate output embedding layer. concatenated vector two-layer unidirectional lstm contains hidden states each. contrast fukui hidden states lstms rather concatenating ﬁnal states represent ﬁnal question representation. convolution prelu input representation attan attention applied input. finally attention maps fully-connected layer followed prelu activation. figure illustrates structure rau. fusion operation used merge textual visual branches. drau experiment using elementwise multiplication result fusion given many-class classiﬁer using frequent answers. single-layer softmax cross-entropy loss. written image question representations early layers. important especially image representation since originally trained different task. second used generate common representation size. obtain joint representation apply convolutions followed prelu activations image question representations. empirical evidence prelu activations found reduce training time signiﬁcantly improve performance compared relu tanh activations. provide results section hidden state time generate attention weights feed hidden states previous lstm convolution layer followed softmax function. convolution layer could interpreted number glimpses model sees. model used question representation described passed output softmax -way classiﬁcation layer. goal architecture assess extent language bias present vqa. second baseline simple joint representation image features language representation. representations combined using compact bilinear pooling chose method speciﬁcally because shown effective fukui main objective model measure robust pooling method multimodal features would perform without deep architecture attention. refer model simple mcb. last baseline substituted compact bilinear pooling simple lstm consisting hidden states equal image size. convolutional layer followed tanh activation used image features prior lstm question representation replicated common embedding size representations model referred joint lstm. begin testing baseline models validation set. shown table language-only baseline model managed overall. impressively scored yes/no questions. simple model improves overall performance although little improvement gained binary yes/no tasks. replacing basic joint lstm embedding improves performance across board. modiﬁcations joint lstm model test several variations joint lstm baseline highlighted table using prelu activations helped ways. first reduced time convergence iterations second overall accuracy improved especially category. next modiﬁcations inspired results experimented appending positional features described coordinates pixel depth/feature dimension image representation. unnormalized respect features worsened results signiﬁcantly dropping overall accuracy points. ms-coco dataset generate questions labels using amazon’s mechanical turk compared adds imagequestion pairs balance language prior present dataset. ground truth answers dataset evaluated using human consensus. evaluate results validation test-dev teststd splits dataset. models evaluated validation train visual genome training. splits include validation training data. however models using frcnn features data augmentation visual genome. train model adam optimization initial learning rate ﬁnal model trained small batch size iterations. fully explore tuning batch size explains relatively high number training iterations. dropout applied lstm fusion operation. weights initialized described except lstm layers uniform weight distribution. pretrained resnet ﬁxed training massive computational overhead ﬁne-tuning network task. datasets provide answers image-question pair sample answer randomly training iteration. early experiments dataset released. thus baselines early models evaluated dataset. building ﬁnal model several parameters changed mainly learning rate activation functions dropout value modiﬁcations discuss section. normalizing positional features enough noticeable improvement warrant effectiveness. next dropout values increased deteriorated network’s accuracy particularly number categories. ﬁnal modiﬁcation inserting fully connected layer hidden units classiﬁer surprisingly dropped accuracy massively. release shifted empirical evaluation towards newer dataset. first retrain retest best performing model joint lstm well several improvements modiﬁcations. since built reduce language prior bias inherent accuracy joint lstm drops signiﬁcantly shown table note models trained explicit visual textual attention implemented. ﬁrst network explicit visual attention rvau shows accuracy jump almost points compared joint lstm model. result highlights importance attention good performance vqa. training rvau network multilabel task i.e. using available annotations training iteration drops accuracy horribly. biggest drop performance far. might caused variety annotations question makes task optimizing answers much harder. drau evaluation addition rtau marks creation drau network. drau model shows favorable improvements rvau model. adding textual attention improves overall accuracy points. substituting prelu activations relu massively drops performance. training might helped model improve prelu offers much faster convergence. increasing value dropout layer fusion operation improves performance points contrast results joint lstm model note tests changed values layers apply dropout change last fusion operation. totally removing dropout layer worsens accuracy. suggests optimal dropout value tuned per-layer. test variations drau test-dev set. observe beneﬁts training data; drau network performs better thanks additional data. literature resize original resnet features test effect scaling train drau variant original resnet size reducing image feature size adversely affects accuracy shown table adding glimpses signiﬁcantly reduces model’s accuracy cause performance drop could related fact lstms process input onedimensional fashion thus decide input either relevant non-relevant. might explain attention maps drau separate objects background glimpses mention section grid lstms might help remove limitation. removing extra data visual genome hurts model’s accuracy. supports fact diverse extra data helps model perform better. finally substituting hadamard product ﬁnal fusion operation boosts network’s accuracy signiﬁcantly points drau versus state-of-the-art table shows comparison drau state-of-the-art models. excluding model ensembles drau performs favorably models. best knowledge best single model performance test-std split close best model small modiﬁcations hyperparameter tuning could push model further. finally frcnn image features boosts model’s performance close state-of-the-art ensemble model. model draumcb fusion landed place test-standard task.. currently reported submissions outperform single model model ensembles. using frcnn features boosted model’s performance outperform ensemble models ﬁrst place submission reports using ensemble models. report best single model uses frcnn features achieves test-standard split outperformed best single model draufrcnn features. remove layers ﬁrst operation ﬁrst output replace rvau. memory constraints reduced size hidden unit rvau’s lstm setting rvau signiﬁcantly helps improve original model’s accuracy shown table noticeable performance boost seen number category supports hypothesis recurrent layers suited sequential reasoning. furthermore test rvau mutan model authors multimodal vector dimension size joint representations. coherence change usual dimension size rvau time writing authors released results using single model rather model ensemble. therefore train single-model mutan using authors’ implementation. story change here rvau improves model’s overall accuracy. strength raus notable tasks require sequentially processing image relational/multi-step reasoning. setting drau outperforms counting questions. validated subset validation split questions dataset shown figure figure shows qualitative results drau mcb. fair comparison compare ﬁrst attention second attention model. authors visualize ﬁrst work. furthermore ﬁrst glimpse model seems complement second attention i.e. model separates background target object separate attention maps. tested visual effect glimpses model. figure clear recurrence helps model attend multiple targets apparent difference attention maps models. drau seems also know count right object. right example figure illustrates drau easily fooled counting whatever object present image rather object needed answer question. model neural-vqa-attention crcv vqateam vqamachine postech upmc-lip dlait hdu-usyd-uncc adelaide-teney acrv drauhadamard fusion draumcb fusion draufrcnn features property also translates questions require relational reasoning. second column figure demonstrates drau attend location required answer question based textual visual attention maps. tion since network reason relations several parts image question. provided quantitative qualitative results indicating usefulness recurrent attention mechanism. drau model showed improved performance tasks requiring sequential/complex reasoning counting relational reasoning winners challenge. achieved near state-of-the-art results single model performance drau network adding frcnn features gets model within margin state-of-the-art -model ensemble mutan finally demonstrated substituting visual attention mechanism networks mutan consistently improves performance. future work investigate implicit recurrent attention mechanism using recently proposed explanation methods", "year": 2018}