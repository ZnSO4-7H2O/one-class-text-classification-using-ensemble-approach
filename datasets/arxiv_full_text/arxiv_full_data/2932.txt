{"title": "Gradient-free Policy Architecture Search and Adaptation", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We develop a method for policy architecture search and adaptation via gradient-free optimization which can learn to perform autonomous driving tasks. By learning from both demonstration and environmental reward we develop a model that can learn with relatively few early catastrophic failures. We first learn an architecture of appropriate complexity to perceive aspects of world state relevant to the expert demonstration, and then mitigate the effect of domain-shift during deployment by adapting a policy demonstrated in a source domain to rewards obtained in a target environment. We show that our approach allows safer learning than baseline methods, offering a reduced cumulative crash metric over the agent's lifetime as it learns to drive in a realistic simulated environment.", "text": "abstract develop method policy architecture search adaptation gradient-free optimization learn perform autonomous driving tasks. learning demonstration environmental reward develop model learn relatively early catastrophic failures. ﬁrst learn architecture appropriate complexity perceive aspects world state relevant expert demonstration mitigate effect domainshift deployment adapting policy demonstrated source domain rewards obtained target environment. show approach allows safer learning baseline methods offering reduced cumulative crash metric agent’s lifetime learns drive realistic simulated environment. deep architectures become popular function approximators represent action-selection policies. common approaches learn parameters models include reinforcement learning and/or learning demonstration learn model parameters maximize expected reward mimic human behavior and/or achieve implicit goals. however design policy architectures especially deep learning paradigm remains relatively unexplored. architectures typically selected combination intuition and/or trial error. learning learn including learning learning architectures long-articulated goal many meta-learning lifelong learning schemes proposed offered seminal views; survey). recently renewed interest topic focused models explicitly search structure deep architectures including models fuse nonparametric bayesian inference deep learning select number channels visual recognition tasks models reinforcement learning directly optimize deep architectures recognition models gradient-free optimization method infer optimal network structure investigate policy architecture search using gradient-free optimization learn optimal policy structure autonomous driving tasks. propose model learns jointly demonstration optimization goal safe training minimizing amount damage vehicle incurs learn threshold level performance. base approach exploration-based schemes ability optimize model weights architecture hyperparameters leverage expert demonstrations adapt reward obtained domains. believe model initialize demonstration learn optimal policy foundation likely achieve higher performance maintaining constraint safe training compared models must randomly search action space initial learning learn reasonably safe demonstration cannot optimize performance based environmental reward. prior approaches combine demonstration reward-based learning mixed successes mainly poor generalization policy learned demonstrations. posit effective behavior cloning requires learning visual agent architecture sufﬁcient structure perceive state world deemed relevant expert providing demonstration. case existing off-the-shelf visual models. thus think wise optimize architectures parameters performing expert behavioral cloning. often deep models learn perform domain fail perform well deployed another setting differing weather lighting conditions. models learned demonstration also well known fail learned policy takes agent away region state space demonstration provided show method effectively safely adapt model demonstrated environment deployed visually different environment based reward signal latter domain even agent initialized initial demonstrations. approach leverages target domain reward makes assumptions domain alignment explicit implicit assumes demonstration supervision target domain. achieve goals present gradient-free optimization algorithm inspired modiﬁcation noise generation results estimating gradients efﬁciently accurately apply algorithm search variable length architectures next combine gradient-free policy search demonstrations learn better policy adapts environment receiving rewards feedback experimentally show architecture search model ﬁnds policy game environment outperforms previously published methods end-to-end steering prediction demonstrations efﬁciently adapted learn drive previously unseen scenarios model reduces number crashes incurred learning drive compared baselines based reward demonstration both compared previously proposed ﬁxed architectures optimized domain. architecture search investigated different frameworks including reinforcement learning evolutionary techniques recurrent neural network used generate ﬁxed-length architecture descriptions predeﬁned search space trained policy gradient methods. able close surpass state results cifar- penn treebank datasets respectively. meta-modeling algorithm proposed used q-learning sequentially search convolutional layers image classiﬁcation tasks. showed approach outperforms existing meta-models manuallydesigned architectures similar types layers. recently introduced budgeted super networks inspired reinforce algorithm objective function maximizes prediction quality computation cost simultaneously. various versions biologically-inspired methods neuroevolution strategies proposed architecture search ever since introduced based biological genetics algorithms ﬁtness function gets re-evaluated generation determine whether genotypes perturbed correct direction evolve appropriately i.e. initialize model evolve based performance. paradigm recently re-visited alternative reinforcement learning algorithms optimization performed gradient-free fashion algorithm shown highly parallelizable resulting signiﬁcant speedups playing mujoco atari games figure sample images used architecture search behavioral cloning task; sample images target domain seen architecture search behavioral cloning. policy search autonomous driving application largely focused demonstration-based optimization approaches without affordance measurements. dates back classic alvin model shallow architecture could pixels simple driving actions. several years after researchers demonstrated end-to-end deep learning models steering control small-scale cars recently nvidia followed path showed success predicting steering angle full-size vehicle pixels using convolutional network novel fcn-lstm architecture proposed large scale crowed-sourced data perform egomotion predictions conditioned previous temporal states used dashcam camera videos derive generic driving model predicted trajectory angle propose learning-to-learn model includes architecture optimization parameter learning representation adaptation different time scales. approach summarized following steps. given expert demonstration search architectures parameters policy best mimics performance monitoring obtained accuracy number parameters. learned demonstration adapt model reward provided target environment. steps essential derive function approximator optimizes objective function. gradient-free optimization algorithm maximizes parametrized reward function using gradient estimation perform architecture search policy learning gradient-free optimization algorithm objective function parametrized n-dimensional vector. reward environment provides agent executes policy parameters goal maximize expected reward perturbing policy parameters denoted moving particular directions. parameter estimate update performed using general stochastic form approximation objective function noise) ∇ˆθy gradient objective estimate approximated gradient estimator family ﬁnite difference methods. gradient estimated randomly chosen direction perturbing elements obtain measurements follows laplace distribution tends choose orthogonal directions long run. recent efforts utilized gaussian noise sample mirrored projections. figure shows comparison distributions. small positive number noise associ|{θ θ··· inspired used recurrent neural network sequentially generate description layers architecture given design space deﬁned user. acts controller generates architecture description deﬁned hyper-parameters chosen predeﬁned search space. authors used policy gradients train able produce ﬁxed-length convolutional recurrent architectures. given demonstrations child network deﬁned trained child network using supervised learning obtained accuracy metric given task held-out validation used accuracy reward signal train rnn. model uses demonstrations provide reward function train rnn. unlike backpropagation suffers gradient vanishing training rnns gradient-free algorithms issue controller speciﬁes three types layers convolutional fully connected max-pool inter-layer dropouts. reward signal negative value total loss function. last layer network regress three real-valued numbers mean-squared loss. total loss three losses. novel reward function decrease penalized adding parameters turns gain loss reduction. propose relu-based lagrange-multiplier reward function below negative minimum loss validation last epochs total number parameters child network. lagrange parameter deﬁned function ﬁrst sub-reward relu-based fashion reward function acts follows. keeps generating layers rewarded based obtained total loss produces child network achieves desired value loss validation set. reaches threshold penalized growing architecture loss decrease consequently. parameter deﬁnes respective threshold. e.g. choosing allows architecture growth figure illustration baseline learned architectures prior work small network large network; comparison random gaussian random laplace distribution terms reward number iterations. number parameters layer causes overall loss decrease thresholds adjustable based problem hand desired trade-off computational cost loss minimization. controller three-layer lstm network followed softmax layer. inputs hyper-parameters describe layer training starts randomly initializing hyper-parameters child network initially layer. uses reward function update parameter weights contributed obtained reward receive higher weighting factor update hence move hill-climbing direction eventually maximizes reward. algorithm described sec. generate architecture yields minimum loss. process generating layer terminates achieve convergence received reward. conﬁrmed experimentally below well known policy learned behavioral cloning perform poorly evaluated inputs domain shift relative demonstration supervision. overcome this gradient-free search algorithm described sec. adapt driving policy learned demonstration source domain based rewards target domain. experiment setting initial agent state and/or weather lighting conditions substantially different provided demonstration. compare baselines perform reward-based optimization using initial demonstrations instead randomly initialized policy makes reward function converge faster safely. driving scenario wish learn drive optimal near-optimal performance deﬁned reward target domain. speciﬁcally reward function used experiment composed factors crashes objects staying within lane lines available driving scene. used lane reward function accident detection function deﬁned source code. necessary information provided paths.xml model episode time interval agent successfully driven without crash. note deviations middle road necessarily result accident. case minor deviation receives reward continues driving makes mistake causes crash game restarts. distinct thresholds middle-lane deviation deﬁned different roads different vehicle types. implemented method described comprehensive experiments show efﬁciency applicability approach searching optimal driving policy minimum number catastrophic failures. details experiments along results provided following subsections. experiments executed game environment evaluation collected dataset expert policy playing collecting images size similar labels include steering angle brake throttle values. order learn diverse driving scenarios used data different locations weather lighting conditions adjusted using goal expose learning algorithm comprehensive demonstration aside speciﬁc scenes testing performance behavioral cloning task. sample images demonstration shown fig. include rainy overcast foggy sunny scenes thunderstorms particular scenes rain thunderstorm nighttime well snow daytime kept test test composed images. search space hyperparameters describe fully-convolutional architecture presented tab. activation function ﬁxed rectiﬁed linear unit. rnn-controller three lstm layers hidden units each softmax layer choose given search space. weights initialized random laplace distribution predicts layer’s description child network built trained batch size adam optimizer learning rate train child network different number epochs starting epochs compute reward function described sec. order ﬁnish optimizing layer track loss reduction validation training sets ﬁrst last epoch avoid overﬁtting. model capable generating architectures high costs architecture growth. order compare designed architecture control reward function never produces architecture parameters minimizing loss present architectures comparison prior work fig. corresponding performance comparison shown tab. smallest architecture built shown fig. obtains smaller total loss network appears suffer overﬁtting might explained absence dropout pooling layer. restricting architecture search algorithm bounded number parameters learn larger network parameters obtains minimum total loss training validation set. discussed above test models substantially different driving scenes never seen training. challenging test large network obtains minimum loss network initial driving policy next section improve reward-providing environment. reference implementation openly available suboptimal w.r.t. authors’ full access model parameters. also model used predict steering angle overall clear whether goal maximize performance model relatively parameters both explored full design space model. nonetheless closest model literature end-to-end steering angle prediction thus best available baseline. small model using distributions mean zero; variance chosen using grid search. fig. shows results reward convergence versus number iterations. distributions result convergence high reward values however laplace distribution tends less noisy reaches slightly higher reward values. next want learn driving policy target game domain. start initial model either using behaviorally cloned randomly initialized policy gradually improve receiving rewards environment. stated episode game starts random location weather condition game. initialize policy larger architecture learned ﬁrst step baseline). evaluate models without adapted demonstration forming four cases baseline network without demonstration behaviorally cloned initial weights larger architecture without demonstration behaviorally cloned initial weights. models environment receive reward described sec. reward received weights perturbed laplace random noise procedure repeated average reward episode game converges maximum value. results averaged across several runs presented tab. model optimized demonstration outperforms cases. particular model least number cumulative crash occurrence prior converging averaged reward table listed results test accuracies dataset taken target domain seen demonstrations. left behavioral cloning alone poor performance signiﬁcant domain shifts occur. adapted model performance loss minimization right adapted performance strong even without reward target domain indicating visual domain shift lesser issue off-demonstration; model adapt source domain still accurate target. best performance obtained adaptation reward source target. also shows improvement loss minimization learn rewards. worth noting judge driving behavior looking total loss comprehensive representative driving task. angle brake throttle converges separate loss among steering angle least brake largest loss values. shows learning steering angle easier demonstrations compared brake throttle time step depend multi previous frames. table shows model predictions relatively complex image chosen target domain pedestrian crossing street signal light green. behaviorally cloned models tend predict agent keep going whereas adapted models target domain rewards able predict correct decision despite green light presence image. adapted large network rewarded source target able make best prediction throttle brake fig. illustrates percentage averaged reward episode aforementioned four models convergence. designed architecture adapted demonstrations source domain starts less reward ﬁrst episode lasts seconds. reasonable considering fact episode game intentionally start completely random environment highly possible signiﬁcantly different domain policy seen point. highlights fact behaviorally cloned model high risks failure tested different domain. models keep learning rewards convergence. seen fig. designed architecture adapted demonstration reaches averaged reward hours last episode lasts minutes terminated user also shown suboptimal adapted demonstrations policy also converges maximum reward plateaus hours. unadapted policies also shown fig. converging averaged reward drastically different timescale conﬁrming positive effect using demonstrations policy learning. supplementary video results found https//saynaebrahimi.github.io/corl.html goal work learn policy autonomous driving task minimizing crashes safety violations training. propose algorithm learns generate optimal network architecture demonstration using reward function optimizes accuracy model size simultaneously. conﬁrm behavioral cloning alone perform poorly target domain differs source demonstrations. show method adapt model learned demonstration domain relying target environmental rewards. experimental evaluation shows model achieves higher accuracy fewer cumulative crashes higher target domain reward. believe results encouraging important steps towards ultimate goal learning complex driving policies zero cumulative crashes serious accidents either simulation real world. chen seff kornhauser xiao. deepdriving learning affordance direct perception autonomous driving. proceedings ieee international conference computer vision pages ingo rechenberg evolutionsstrategie optimierung technischer systeme nach prinzipien biologishen evolution. einem nachwort manfred eigen friedrich frommann verlag struttgart-bad cannstatt ross gordon bagnell. reduction imitation learning structured prediction no-regret online learning. international conference artiﬁcial intelligence statistics pages interactive task training mobile robot human gesture recognition. robotics automation proceedings. ieee international conference volume pages ieee schaffer whitley eshelman. combinations genetic algorithms neural networks survey state art. combinations genetic algorithms neural networks cogann-. international workshop pages ieee", "year": 2017}