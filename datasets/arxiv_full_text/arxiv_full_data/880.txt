{"title": "Robust Large Margin Deep Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "The generalization error of deep neural networks via their classification margin is studied in this work. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary non-linearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. This is a significant improvement over the current bounds in the literature, which imply that the generalization error grows with either the width or the depth of the network. Moreover, it shows that the recently proposed batch normalization and weight normalization re-parametrizations enjoy good generalization properties, and leads to a novel network regularizer based on the network's Jacobian matrix. The analysis is supported with experimental results on the MNIST, CIFAR-10, LaRED and ImageNet datasets.", "text": "boldface upper-case letters boldface lower-case letters italic letters calligraphic upper-case letters respectively. convex hull denoted conv. rn×n denotes identity matrix rm×n denotes zero matrix denotes vector ones. subscripts omitted dimensions clear context. denotes k-th basis vector standard basis denotes euclidean norm denotes spectral norm denotes frobenious norm i-th element vector denoted element i-th j-th column denoted covering number d-metric balls radius denoted consider classiﬁcation problem observe vector corresponding class label called input space called label space denotes number classes. samples space denoted element denoted assume samples drawn according probability distribution deﬁned training samples drawn denoted {si}m goal learning leverage training classiﬁer provides label note element training arbitrary element sample space therefore robust learning algorithm chooses classiﬁer losses partition close. following theorem provides bound robust algorithms. bound number partitions found covering number samples space covering number smallest number metric balls radius needed cover denoted denotes metric. space cartesian product continuous input space discrete label space write corresponds number classes. choice metric determines efﬁciently cover common choice euclidean metric classiﬁcation margin training sample radius largest metric ball centered contained decision region associated class label robustness large margin classiﬁers given following theorem. zero rate number training samples grows. also increases sub-linearly number classes finally depends complexity input space classiﬁcation margin covering number represents l-th layer parameters output l-th layer denoted i.e. rml; input layer corresponds output last layer denoted visualized fig. next deﬁne various layers used modern state-of-the-art dnns. associated corresponding class labels. decision boundary class class corresponds hyperplane softmax layer usually coupled categorical cross-entropy training objective. represents element-wise non-linearity applied element represents linear transformation layer input wlzl− rml×ml− weight matrix bias vector. typical non-linearities relu sigmoid hyperbolic tangent. listed table choice non-linearity usually pooling matrix. usual choices pooling down-sampling max-pooling i-th assume pooling average pooling. denote ﬁrst element regions case down-sampling pooling region case max-pooling maxj∈pi |j|; case average pooling j∈pi note properties chain rule computed product individual network layers evaluated appropriate values layer inputs zl−. establish relation pair vectors input space output space. note established corresponds linear operator maps vector vector implies maximum distance expansion network bounded maximum spectral norm network’s moreover corresponds also brieﬂy explore relationship jacobian matrix fisher information matrix. simplify derivations assume model parameter deterministic. fisher information measures much information parameter contained random variable represents dnn. particular case fisher information given recall deﬁnition classiﬁer note decision boundary class class feature space given hyperplane positive score indicates network output classes separated margin corresponds score. however bound based spectral norm tighter based frobenious norm example take orthonormal rows dimension constraint based spectral norm form constraint diag denotes diagonal part matrix. main motivation method faster training authors also show empirically networks achieve good generalization. note row-normalized weight matrices computation gradient regularizer layer requires computation gradients computation jacobian matrices computation gradient typical loss used training usually involves computation ˜gl− |x=xi random index. compared made simpliﬁcations. first assumed input layer ﬁxed. need compute output layer input. second choosing index training sample compute additional gradient training sample. signiﬁcantly reduces computational complexity. gradient ˜gl− euclidean distance. geodesic distance appropriate euclidean distance since natural metric manifold. moreover covering number manifold smaller covering based geodesic metric balls lead tighter bounds. promoting large euclidean distance points lead large geodesic distance points input space. moreover ratio upper bounded maximum value spectral norm network’s evaluated line result described section mnist dataset train dnns different number fully connected layers different sizes weight matrices last layer always softmax layer objective loss. networks trained using stochastic gradient descent momentum perspective constraint sets note case latter take account worst case spectral norm inputs conv. maximum value spectral norm testing indicates value increases network depth approximately images gesture subject. extracted depth images hands using masks provided resized images images ﬁrst subjects used create non-overlapping training testing sets. addition also constructed testing following architecture -conv -max-pool -conv -max-pool followed softmax layer -conv denotes convolutional layer ﬁlters size -max-pool denotes max-pooling pooling regions size training procedure follows described previous paragraphs. results reported table images dataset resized experiment without data augmentation data augmentation following includes random cropping images size original image color augmentation. classiﬁcation accuracies training shown diagonal elements bounded implies spectral norm matrix bounded therefore spectral norm upper bounded proof linear layer trivial. case softmax layer show proof statement straightforward. pooling regions non-overlapping straightforward verify rows deﬁned pooling operators orthonormal. therefore spectral norm equal", "year": 2016}