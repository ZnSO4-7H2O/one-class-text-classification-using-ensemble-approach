{"title": "Variational Recurrent Auto-Encoders", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.", "text": "paper propose model combines strengths rnns sgvb variational recurrent auto-encoder model used efﬁcient large scale unsupervised learning time series data mapping time series data latent vector representation. model generative data generated samples latent space. important contribution work model make unlabeled data order facilitate supervised training rnns initialising weights network state. recurrent neural networks exhibit dynamic temporal behaviour makes suitable capturing time dependencies temporal data. recently succesfully applied handwriting recognition music modelling another recent development introduced model structure consisting networks encoder decoder. encoder encodes input intermediate representation forms input decoder. resulting model able obtain state-of-the-art bleu score. propose model based variational bayes variational recurrent auto encoder model similar auto-encoder sense learns encoder learns mapping data latent representation decoder latent representation data. however variational bayesian approach maps data distribution latent variables. type network efﬁciently trained stochastic gradient variational bayes introduced last year iclr kingma welling resulting model similarities variational auto-encoder presented paper. combining rnns sgvb partly inspired work justin bayer ﬁrst results presented workshop nips vrae allows time sequences latent representation enables efﬁcient large scale unsupervised variational learning time sequences. also trained vrae gives sensible initialisation weights network state standard rnn. general network states initialised zero however pascanu shown network state large factor explaining exploding gradients problem. initializing standard weights network state obtained vrae likely make training efﬁcient possibly avoid exploding gradients problem enable better scores. stochastic gradient variational bayes independently developed kingma welling rezende train models assumed data generated using unobserved continuous random variable general marginal likelihood expensive even small datasets. sgvb solves approximating true posterior optimizing lower bound log-likelihood. similar nomenclature kingma’s paper call encoder decoder. want optimize lower bound gradient ascent need gradients respect parameters. obtaining gradients encoder relatively straightforward obtaining gradients decoder not. order solve kingma welling introduced reparametrization trick reparametrize random variable deterministic variable model latent variables univariate gaussians reparametrization modelling latent variables allows divergence integrated analytically resulting following estimator encoder contains recurrent connections state calculated based previous state data corresponding time step. distribution obtained last state hend that encht initialised zero vector. using reparametrization trick sampled encoding initial state decoding computed weights. hereafter state updated traditional experiments used midi ﬁles wellknown video game songs sampled upon inspection dimensions contained signiﬁcant amount notes dimensions removed. songs divided short parts part becomes data point. order equal number data points song ﬁrst data points song used. choice optimizer proved vital make vrae learn useful representation especially adaptive gradients momentum important. experiments used adam optimizer inspired rmsprop included momentum correction factor zero bias created kingma trained vrae two-dimensional latent space hidden units dataset described last section. songs divided non-overlapping sequences time steps each. adam parameters used instability learning rate decreased gradually learning. initial learning rate ﬁnal learning rate resulting lower bound training shown figure figure left lower bound log-likelihood datapoint time step training. ﬁrst epochs scale reasons. right organisation data points latent space. datapoint encoded visualized location resulting two-dimensional mean encoding. mario underworld mario mariokart occupy distinct regions. model two-dimensional latent space possible show position data point latent space. data points seconds long therefore capture characteristics song. nevertheless figure shows clustering certain songs occupy distinct regions latent space. two-dimensional latent space however suboptimal modelling underlying distribution data. therefore also trained model twenty latent variables. model used sequences time steps overlap start data point halfway previous data point. model learns individual data points also transitions them enables generating music arbitrary length. training ﬁrst model adam parameters used learning rate adjusted epochs. resulting lower bound shown figure similar organisation data latent space using model shown order visualize twenty-dimensional latent representations used t-sne given latent space vector decoding part trained models used generating data. ﬁrst model described chapter trained non-overlapping sequences time steps. therefore expected generating longer sequences yield data distribution training data. however since know data point latent representation dimensions inspect positions model interpolate parts different songs. resulting music lasts seconds clearly elements parts. model trained overlapping data points used generate music time steps various latent state vectors. possible figure left lower bound log-likelihood datapoint time step training. ﬁrst epochs scale reasons. right visualization organisation encoded data latent space. calculated -dimensional latent representation calculated data point. mean representation visualized dimensions using t-sne. color represents data points song. seen song parts song occupy part space parts songs clearly grouped together. course much parts song grouped together depends homogeneity song relative similarity different songs well much spatial information lost dimensionality reduction t-sne. obtain latent vectors encoding data point sample randomly latent space. creates might call medley songs used training. generated sample randomly chosen point latent space available youtube shown possible train rnns sgvb effective modeling time sequences. important difference earlier similar approaches model maps time sequences latent vector opposed latent state sequences. ﬁrst possible improvement current model dividing song many data points possible training instead data points overlap. another improvement reverse order input ﬁrst time steps strongly related latent space last time steps. likely improve upon length time dependency captured around time steps current approach. another train longer time sequences incorporate lstm framework direct applications approach include recognition denoising feature extraction. model combined models sequential data example improve current music genre tagging methods e.g. sigtia addition method could complement current methods supervised training rnns providing initial hidden states. boulanger-lewandowski nicolas bengio yoshua vincent pascal. modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. international conference machine learning kyunghyun merrienboer bart gulcehre caglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing graves alex liwicki marcus fern´andez santiago bertolami roman bunke horst schmidhuber j¨urgen. novel connectionist system unconstrained handwriting recognition. pattern analysis machine intelligence ieee transactions rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning sigtia siddharth benetos emmanouil cherla srikanth weyde tillman garcez artur davila dixon simon. rnn-based music language model improving automatic music transcription. international society music information retrieval conference", "year": 2014}