{"title": "Local Contrast Learning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Learning a deep model from small data is yet an opening and challenging problem. We focus on one-shot classification by deep learning approach based on a small quantity of training samples. We proposed a novel deep learning approach named Local Contrast Learning (LCL) based on the key insight about a human cognitive behavior that human recognizes the objects in a specific context by contrasting the objects in the context or in her/his memory. LCL is used to train a deep model that can contrast the recognizing sample with a couple of contrastive samples randomly drawn and shuffled. On one-shot classification task on Omniglot, the deep model based LCL with 122 layers and 1.94 millions of parameters, which was trained on a tiny dataset with only 60 classes and 20 samples per class, achieved the accuracy 97.99% that outperforms human and state-of-the-art established by Bayesian Program Learning (BPL) trained on 964 classes. LCL is a fundamental idea which can be applied to alleviate parametric model's overfitting resulted by lack of training samples.", "text": "learning deep model small data opening challenging problem. focus one-shot classiﬁcation deep learning approach based small quantity training samples. proposed novel deep learning approach named local contrast learning based insight human cognitive behavior human recognizes objects speciﬁc context contrasting objects context her/his memory. used train deep model contrast recognizing sample couple contrastive samples randomly drawn shufﬂed. one-shot classiﬁcation task omniglot deep model based layers millions parameters trained tiny dataset classes samples class achieved accuracy outperforms human state-of-the-art established bayesian program learning trained classes. fundamental idea applied alleviate parametric model’s overﬁtting resulted lack training samples. deep learning achieved large successes many domains science business government many years provide kind end-to-end approach machine learning requires little engineering hand however deep model requires large amount annotated data tuning millions parameters. building large training deep learning sometimes extremely expensive acceptable hinder deep learning applied domains. human learn novel concept examples deep *equal contribution college computer science engineeringchongqing university technology chongqing china college computer information sciencechongqing normal university chongqing china school software engineeringchongqing university chongqing china. correspondence chuanyun <qq.com>. model learn concept small training data don’t fall overﬁtting? opening challenging question machine learning. traditional deep learning approaches learn functions high-dimensional sample lower-dimensional space least average training error. nature learn common global representations training data detecting classifying patterns input. difﬁcult draw representation features small training distribution training data might identical test set. training data sparse diverse becomes difﬁcult. inconsistent human beings cognitive behaviors human easily distinguish completely different kinds objects apple. propose hypothesis lower cognitive level human recognizes objects speciﬁc context contrasting objects context her/his memory instead drawing universal representations building global mapping function. based insights paper introduced local contrast learning approach deep learning. makes ideas human cognitive behavior local contextcognition always base local context depends local context. local context consists objects drawn objects global context cognition. context cannot provide enough information cognition local context must built gathering information recalling memories. human being local context must small scenario target objects. number targets usually less seven. contrastin order identify object recognition iteratively executed contrasting object contrastive object local context contrast results different contrastive objects compared among them. process local contrast cognition shown figure figure process local contrast cognition. local contrast context recognizing object contrastive objects. contrastive pairs built moving attention focused every contrastive object. difference representations created contrasting every contrastive pair. recognition results contrasting difference representations. amples group contrastive samples contrasting one. ﬁrstly generates large amount contrastive sample groups randomly sampling classes exemplars training shufﬂing them feeds sample groups parametric model encode differences last outputs activations contrastive samples contrasting differences. learn recognization different local contexts rather single global context. workﬂow shown figure consists three major components contrast cognitive context constructor difference embedding generator difference perceptron cccc responsible constructing local cognitive context labelled support denoted {s··· sc··· ssc} number training sample category. training sample category. {c··· cc··· csc} corresponding label numbers categories. consists recognizing objects category contrastive objects different categories denoted deﬁned recognizing object contrastive objects numbers contrastive objects lcc. contrastive objects sample whose category equal called contrastive positive object sample equal contrastive objects except contrastive positive object called contrastive negative object contrasting cognitive context constructor including category sampler object sampler order shufﬂer local cognitive context contrastive pair recognizing object contrastive positive object contrastive negative object difference embedding generator parametric model weights shared difference embedding difference perceptron contrastive pair local activation. sample category contrastive objects.in order build cccc randomly samples classes recognizing class contrast classes randomly samples instances class ﬁnally randomly shufﬂes instances. order contrastive objects critical. word different created contrastive objects organizing different order. large quantity created small quantity training samples. example small omniglot dataset classes instances class case equals different generated. cccc provide large enough number training model. critical factor assure training stand overﬁtting facing tiny training set. responsible contrasting recognizing object contrastive objects generating vector representing differences between them. ﬁrstly creates contrastive pairs grouping recognizing object contrastive object together contrastive pairs iteratively identical parametric model order input labelled training number contrastive objects randomly sample categories contrastive categories. select anyone contrastive positive category contrastive categories others contrastive negative categories. iteratively sample image contrastive negative categories contrastive negative objects. sample images contrastive positive category among recognizing object another contrastive positive object. concatenate contrastive positive objects contrastive negative objects contrastive objects randomly shufﬂe theoretically embedding model kind parametric model used encoder. paper resnet. must emphasized pair objects inputted embedding model among recognizing object another contrastive object like dpsl unlike siamese network matching nets triplet network prototypical networksinto objects inputted one. important avoiding overﬁtting. case training samples object inputted embedding model different inputs whereas objects inputted embedding model different inputs. another view interpret this embedding model maps contrastive object high-dimensional space low-dimensional space condition recognizing object. difﬁcult learn conditional embedding object embedding enforce embedding model sense difference recognizing object contrastive object instead remembering object features. moreover embedding condition recognizing object local rather global. model must learn embed different rather whole training dataset context model must learn adapt tremendous quantity context instead single global context. must noted outputs embedding vector dimensional space representing difference recognizing object contrastive object instead similarity metric embedding representation single object feeding similarity function. robust high-dimensional object vector dimension dimension difference perceptron cpla. parametric model fully-connected neural network. emphasized inputted time instead one. determinated difference embedding. cpla indicates contrastive object positive negative. activation positive object obviously distinguished negative object however value activation cannot directly corresponding similarity recognizing object contrastive object. direct rank activations entire training meaningless contrast results local contrast context. train global similarity metric contrastive objects. instead locally identiﬁes positive object lcc. emphasized computed difference embedding instead difference embedding dei. means cpla result contrasting difference embeddings rather mapping difference embedding. provided implementation called local contrastive neural network image classiﬁcation lcnn multiple layer deep neural network using resnet fully-connected layer recognizing image contrastive images contrastive pair stacked input channels resnet whose outputs concatenated input fully-connected layer output cpla vector whose element corresponding contrastive object. correspondence cpla contrastive pair soft relation established training. paper make activation negative object tend activation positive object tend zero. therefore contrastive object minimum activation positive negative. order tune parameters lcnn constructed contrastive loss function minimizing loss function enlarge activation positive object negative objects. one-shot recognition extreme case. however industrial applications order improve classiﬁcation accuracy few-shot recognition practicable. architecture few-shot lcnn shown figure few-shot recognizing samples built identical lcnn computing cpla. finally cpla summed few-shot cpla. similar one-shot recognition minimum few-shot cpla corresponding positive object. figure network architecture lcnn few-shot. ﬁgure shows architecture -way -shot classiﬁcation. contrasting cognitive context constructor build recognizing objects. few-shot recognizing samples. built unstacking few-shot lcc. iteratively inputted lcnn generating cpla. cpla summed cpla few-shot lcc. contrastive sample minimum cpla recognized positive sample. figure network architecture local contrastive neural network. input included contrastive objects recognizing object local contrastive context image size. contrastive pairs. difference embedding generator resnet iteratively contrastive pair. concatenating difference embedding. contrastive pair local activation. minimum contrastive pair local activation positive object. number training mini-batch. number contrastive objects lcc. cpla expected cpla. equals contrastive objects positive object otherwise equals parameters lcnn including parameters wdeg parameters order minimize loss stochastic gradient descent momentum optimization algorithm used like resnet. momentum optimization algorithm ﬁxed learning rate starts iterations learning rate respectively hyper parameters. variance scaling initializer used initializing weight parameters. order evaluate one-shot classiﬁcation performance lcnn carried classiﬁcation experiments benchmark datasets omniglot casia-hwdb.. according conventional one-shot test protocol categories datasets divided subsets training test categories training disjoint categories test set. model trained categories training tested categories test set. tion functions convolutional layers. ﬁrst layer convolutional layer output channels followed pre-activation residual units output channels units output channelsn units output channels. last layers activation function global average pooling outputs dimension vector. number layers determined hyper parameter number layers lcnn equals learning rate decayed learning steps larger decayed learning steps larger maximum training steps hyperparameter. speciﬁed following experiments steps learning rate decay separately number layers lcnn training mini-batch size every test result performed test might different three kinds randomness computational error test times test report mean classiﬁcation accuracy conﬁdential interval. omniglot dataset consists images across classes images class different alphabets total. dataset divided background alphabets characters evaluation alphabets characters. according suggestion lake background used training one-shot learning results reported using alphabets evaluation set. one-shot classiﬁcation results reported such mannmatching nets neural statistician convnet memory module prototypical networks maml metanet accepted suggestion results dont match results section comparisons next section them. except training alphabets characters lake provided smaller background sets background small background small contains alphabets respectively contains characters characters. furthermore models perform limited training classes built tiny subsets picking ﬁrst ﬁrst characters alphabet omniglot background respectively called background tiny background tiny subsets respectively characters characters samples samples. trained lcnn models training evaluating performance. exactly compare one-shot learning performance lake developed standard one-shot test protocol call test protocol. test protocol provided groups within-alphabet one-shot classiﬁcation tasks task includes characters selected alphabet evaluation alphabets. character examples drawn persons test example another training example. test task given test example select example training examples identical character test example. one-shot test trials constructed test tasks classiﬁcation accuracy calculated. section experiments carried trials. experiments ﬁrstly using cccc mini-batch constructed omniglot training size experiments. number contrastive objects performance reported classiﬁcation. lcnn training. test created previous test trials trial lcc. augmented training degrees rotations resized images pixels. comparing performance small training neural networks approaches selected matching networks baseline. implemented basic version without fully-conditional embedding basic version convolutional network trained learn independent embeddings examples training test consists stack modules convolution ﬁlters followed batch normalization relu nonlinearity max-pooling. maximum training steps model trained dataset omniglot tiny omniglot tiny omniglot small omniglot small. results reported tested previous test trials. using characters training lcnn achieved one-shot classiﬁcation accuracy outperforms state-of-the-art accuracy established also better humans identiﬁcation accuracy siamese convolutional network achieved accuracy much worse lcnn. table one-shot classiﬁcation accuracy omniglot based test protocol. results accuracies averaged runs conﬁdence intervals reported. train tiny tiny consists ﬁrst character ﬁrst characters omniglot background set. model acc. human pixel affine model deep boltzmann machines convolutional siamese matching nets tiny matching nets tiny matching nets small matching nets small small small lcnn tiny lcnn tiny lcnn small lcnn small lcnn using omniglot background small omniglot background small training lcnn respectively achieved accuracies outperformed achieved .%.% respectively also outperformed matching nets achieved furthermore using omniglot background tiny omniglot background tiny lcnn achieved one-shot classiﬁcation accuracies. case even though matching nets convolutional layers serious overﬁtting training accuracies however test accuracies lcnn outperformed using percent samples bpl. variant tests proposed vinyals santoro different split background evaluation different test trails test protocol. vinyals’ randomly picked characters omniglots background evaluation training characters test set. vinyals split training test identical alphabets different characters. however test protocol training test completely different alphabets model trained alphabets used classify alphabets. challenging recognize images novel alphabets novel characters identical alphabets. researches also follow vinyals mannmatching nets neural statistician convnet memory module prototypical networks maml metanet. test trials constructed test trial consists characters picked test set. image character picked contrastive object images picked recognizing objects. metanet trained tested bpls split training alphabets classes however formed trials evaluation classes test model cannot completely match bpls result evaluated standard trials. order extensively evaluate performances lcnn best match approaches section trained tested lccn separately test protocol provided vinyals characters test protocol provided lake characters. furthermore order show advantages lcnn small data trained lcnn ﬁrst characters ﬁrst characters training characters. order keep results reproducible available comparison unlike approaches randomly split every used split every similar previous section also trained lcnn background small background tiny formed trials evaluation rather standard trials provided lake. experiments section test trials created test using algorithm number test trials ntrial computed equation ntrial number characters test set. number sample character. number contrastive objects nshot numbers shots. carried -way -way one-shot -shot classiﬁcations comparisons published results shown table models labeled trained test protocol provided vinyals training characters test characters training characters test characters belong alphabets. models labeled small tiny trained test protocol provided lcnn outperformed prior approaches using characters furthermore results using characters comparable published results used characters characters. comparing results using characters results using characters latter obviously higher former demonstrate training lcnn omniglot character almost enough data cannot improve performance. proves lcnn learn enough knowledge small data discriminating objects. conversely training data might imported noises confused models declined performance instance -way one-shot accuracy model lcnn slightly lower accuracy model lcnn comparing models lcnn tiny lcnn small lcnn models lcnn lcnn lcnn accuracies former lower latter. proves challenging recognize images novel alphabets novel characters identical alphabets. like test omniglot evaluated lcnn casiahwdb. chinese handwritten character dataset machine learning diversiﬁed confusing omniglot. casia-hwdb. widely used dataset includes character classes images class written drawers. therefore order extensively evaluate performance provide benchmark one-shot classiﬁcation approaches handwritten chinese characters deﬁned one-shot classiﬁcation handwritten chinese characters. casia-hwdb. includes training subset images character test subset images character. however ﬁrst images character training subset evaluating performance one-shot classiﬁcation. pick last characters test characters respectively pick ﬁrst characters three different training hwdb hwdb hwdb. images resized augmented. experiments section test trials created using algorithm test number test trials comparing performance hwdb neural networks approaches selected matching networks baseline. setup matching networks setup previous section however maximum training steps carried -way one-shot -shot classiﬁcations. results comparing baselines lcnn shown table experiments lcnn outperforms baseline. lcnn characters achieved accuracy outperformed accuracy matching networks characters. demonstrates lcnn achieve high performance using small data. comparing results lcnn hwdb lcnn hwdb lcnn hwdb achieved almost one-shot -shot classiﬁcation accuracy using percent samples lcnn hwdb. demonstrates lcnn learn enough knowledge small data discriminating objects. human identify novel object seeing samples class. explained principles compositionality causality learning learn others principles mainly base logical reasoning however logically thinking human recognizes objects intuition based innate abilities experiences. contrast kind innate ability also kind enriching experiences. contrast capability also advanced learning. simulates capability recognization. success partially attributed extremely large quantity three randomness randomly selecting classes randomly selecting samples randomly shufﬂing contrastive objects. enforce lcnn learn distinguish objects instead representing remembering training object pattern. learning contrasting local context another crucial factor. contrasting local context enforce lcnn adapt different local context thus lcnn cannot remember context information case tiny samples lcnn would encounter overﬁtting. course super deep neural network resnet important factor depth provide enough ﬂexibility adapting tremendous local context. however assure super deep neural network avoid overﬁtting case tiny samples. therefore super deep neural network contributes high classiﬁcation accuracy contributes success training. design difference perceptron contrast difference embedding instead learning similarity metric. makes contrast objects table one-shot few-shot accuracy omniglot based variant test protocol. results accuracies averaged times runs conﬁdence intervals reported. reported. model pixels nearest neighbor mann matching nets neural statistician convnet memory module prototypical networks maml metanet lcnn lcnn lcnn metanet lcnn tiny lcnn small lcnn table -way classiﬁcation accuracy hwdb. results accuracies averaged times runs conﬁdence intervals reported.the models labeled hwdb hwdb hwdb respectively trained ﬁrst characters hwdb samples character. number training classes number samples class essential factor improve performance. classes samples class improve test accuracy. however accuracy increased sample less. case training sample enough increase sample cannot almost improve classiﬁcation accuracy similar human beings cognitive behavior human learn recognize object samples human cannot almost learn redundant similar samples. cases increase sample might worsen accuracy samples maybe bring noise interfere training. fundamental tool setting artiﬁcial intelligence systems. high level architectures invented based lcl. example hierarchical lcnn could used distinguish hierarchical classes. objects sequential sequential lcnn could created. proposed novel deep learning approach named local contrast learning alleviating deep models overﬁtting resulted lack training samples. enforce deep model adapt tremendous local contexts carefully capture difference contrastive object recognizing object. able stably successfully train deep neural network than layers using dozens sample classes tens samples class. approach achieves state-of-the-art results using percent samples baselines. results proves deep model trained using small data. kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. proceeding icml’ volume kumar carneiro gustavo reid others. learning local image descriptors deep siamese triplet convolutional networks minimising global loss functions. cvpr lake salakhutdinov tenenbaum human-level concept learning probabilistic program induction supplementary material. science issn ./science.aab. lake brenden salakhutdinov ruslan tenenbaum josh. one-shot learning inverting compositional causal process. advances neural information processing systems cheng-lin wang da-han wang qiufeng. online ofﬂine handwritten chinese character recognition benchmarking databases. pattern recognition issn ./j.patcog.... santoro adam bartunov sergey botvinick matthew wierstra daan lillicrap timothy. meta-learning memory-augmented neural networks. international conference machine learning vinyals oriol blundell charles lillicrap wierstra daan others. matching networks shot learning. advances neural information processing systems wang jiang song yang leung thomas rosenberg chuck wang jingbin philbin james chen ying. learning ﬁne-grained image similarity deep ranking. cvpr", "year": 2018}