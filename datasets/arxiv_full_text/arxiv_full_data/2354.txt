{"title": "Being Bayesian about Network Structure", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In many domains, we are interested in analyzing the structure of the underlying distribution, e.g., whether one variable is a direct parent of the other. Bayesian model-selection attempts to find the MAP model and use its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed ordering over network variables. This allows us to compute, for a given ordering, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orderings rather than over network structures. The space of orderings is much smaller and more regular than the space of structures, and has a smoother posterior `landscape'. We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach.", "text": "many domains interested structure variable model-selection structure amount available might many models non-negligible terior. thus want compute bayesian poste­ rior feature models contain paper approach task. first show ficiently compute exponential networks consistent network variables. given ordering data posterior result basis algorithm imates bayesian posterior feature. proach uses markov chain monte carlo method orderings structures. regular space structures smoother posterior results pare approach full model averaging sible) mcmc network structures non-bayesian last decade great deal research cused problem learning bayesian networks data learn model cision making substitute human expert. goal learn model system prediction domain structure. bayesian nisms genes cell produce proteins cause genes express themselves case main goal discover expression high-scoring alence class) indeed shown highest ders magnitude cases model selection nately ation hold. gene expression possible sands genes experiment random variable hundred experiments single data case). cases like this amount data small relative likely many models explain well. model selection models model true representation tures approximately unique structure certain features edges extract variables ministically related goal therefore edge model selected interested bayesian work properties. first efficient networks constant nodes. equation probability pute posterior edges markov blankets-over set. second idea mcmc approach network variables network structures; allowing tionary pirical approach structures. mixes much faster reliably network structures. networks posterior poor convergence different estimates. approach dominant sparse data outperforms structures bootstrap struct simplest approach pick single high-scoring approximation. size model posterior peaked around single model approximation reasonable tion many interesting application) size model. case usually number high-scoring poor approximation. structures high scoring manner quite sensitive use. example structures lar. restricted consider beam-search. risk getting biased sample structures. york based markov chain monte carlo simulation. chain space possible distribution posterior generate walk markov chain. assuming process structures clear rapidly domains. space structures probability boring structures mixing rate markov chain quite slow. posterior edge case apply closed form uj-< analysis restrict contain since terms consist subsets disturbed parents constraint proposition parents simply restrict ut-< note choice families different events. hence markov blanket previous tion given predetermined though assumption cases clearly little therefore possible elegant therefore form solution monte carlo algorithm define introduce nature priors used previous sulting original uniform form graphs consistent likely; tent orderings graph consistent priors unfortunate tion. standard used particularly task rather simple easy work with. fact ubiquitous uniform network structures) structures argue that causal discovery pdags appropriate; networks nally ordering obtained flipping consider nodes terms terms corresponding ordering-< precede succeed change parent sets same. further­ potential more terms also common -parent sets remain same. thus need subtract number highest-scoring families. again procedure executed once beginning process. decreasing xi's list. consider particular ordering extract list families consistent know families list score better thus best family extracted list factor better choose restrict attention families list assumption families negligible high-scoring good full enumeration. ming orderings ming equivalence evaluation different prior need exhaustive exact bayesian computation enumera­ orderings enumeration hypotheses. struc­ tion possible many variables tures limited domains variables. took data sets -vote flare-uci repos­ itory selected five variables define incremental value erated datasets bayesian averaging methods. figure compares results datasets. small amounts data approaches slightly illustrates different prior fami­ lies ignoring families augment low-scoring fami­ lies low-scoring assume sub­ family score score -score. family that possible parents score found prune extensions figure plots progression samples. graph plot score logp)) \"current\" candidate network found greedy hill climbing search network case structure-mcmc bayesian averaging networks. experimented nine-variable pler burn-in period steps sampled every steps; experimented samples. rapidly reached plateau. hand mcmc runs network structures reached different levels scores even though much larger number iterations. phenomenon examples alann instances. tween sets graphs. seemed mix. structure sampler mixes hand examine samples ordering­ mcmc converges high-scoring lieve stationary distribution contrast stayed different regions first itera­ tions. situation based mcmc sampler stances. case structure starts empty network reach level score achieved runs starting found greedy hill climbing search. moreover lat­ runs seem fluctuate seed. note runs show differences thus sub-optimal least less probable structure posterior optimum; sampler stuck local \"hill\" cannot escape. lat­ space structures hypothesis take long structures time reach similar level scores indicating different part space stationary havior reached. examining posterior tures different runs. figure compares posterior assigned different runs probability structure-mcmc. although different probability differ radically. several features assigned probability features close samples probability close samples other. hile behavior less com­ runs seeded greedy structure even there. start place) gets trapped different local neighborhood runs order based mcmc sampler tightly corre­ lated. figure compares runs starting ordering consistent random order. predictions similar observation smoother posterior space networks work lead huge difference score ordering less sensitive bations. scores large space structures; ferences scores individual out. furthermore consistent hence ordering unlikely disparity larger datasets. data grows posterior landscape becomes since effect single change score ampli­ fied across many samples. discussed above dataset large enough model selection often good clearly different sets features. types errors false positives false itive features negatives posi­ achieve different tradeoffs tive. different values tween type errors. thus method plot tradeoff note that applications structure care false positives false negatives. example biological application expected unrealistic would detect causal connections ited data. however false positives sizing important main concern left-hand-side curve part small number false pos­ itives. possible instances features form \"there directed path pdag network structure. pdag meaningful assume hidden vari­ ables correspond discussed pression posterior ing. however sample networks ordering estimate feature relative sampled networks order). also compared ordering-mcmc dominates (path features larger ordering-mcmc errors markov features believe features tions network cannot reliably small data set. section true bayesian posterior tural network features. mcmc sampling orderings ables rather directly ordering sampled markov chain com­ pute probability features features estimate probability samples. shown resulting mixes substantially prob­ therefore results stan­ ability features. dard mcmc structures highly dependent region space markov chain process happens gravitate. data sets data missing tending mcmc orderings values allowing average both. success­ combined mcmc algorithm full bayesian model averaging prediction well. finally domain order understand acknowledgments authors thank yoram singer useful discussions. daah--l- approach intelligent systems\" mation assurance national. michael sacher trust sherman senior lec­ tureship. computers funded basic equipment grant.", "year": 2013}