{"title": "A Digital Neuromorphic Architecture Efficiently Facilitating Complex  Synaptic Response Functions Applied to Liquid State Machines", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "Information in neural networks is represented as weighted connections, or synapses, between neurons. This poses a problem as the primary computational bottleneck for neural networks is the vector-matrix multiply when inputs are multiplied by the neural network weights. Conventional processing architectures are not well suited for simulating neural networks, often requiring large amounts of energy and time. Additionally, synapses in biological neural networks are not binary connections, but exhibit a nonlinear response function as neurotransmitters are emitted and diffuse between neurons. Inspired by neuroscience principles, we present a digital neuromorphic architecture, the Spiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex synaptic response functions without requiring additional hardware components. We consider the paradigm of spiking neurons with temporally coded information as opposed to non-spiking rate coded neurons used in most neural networks. In this paradigm we examine liquid state machines applied to speech recognition and show how a liquid state machine with temporal dynamics maps onto the STPU-demonstrating the flexibility and efficiency of the STPU for instantiating neural algorithms.", "text": "michael smith∗ aaron hill∗ kristofor carlson∗ craig vineyard∗ jonathon donaldson∗ david follett† pamela follett†‡ john naegle∗ conrad james∗ james aimone∗ abstract—information neural networks represented weighted connections synapses neurons. poses problem primary computational bottleneck neural networks vector-matrix multiply inputs multiplied neural network weights. conventional processing architectures well suited simulating neural networks often requiring large amounts energy time. additionally synapses biological neural networks binary connections exhibit nonlinear response function neurotransmitters emitted diffuse neurons. inspired neuroscience principles present digital neuromorphic architecture spiking temporal processing unit capable modeling arbitrary complex synaptic response functions without requiring additional hardware components. consider paradigm spiking neurons temporally coded information opposed non-spiking rate coded neurons used neural networks. paradigm examine liquid state machines applied speech recognition show liquid state machine temporal dynamics maps onto stpu—demonstrating ﬂexibility efﬁciency stpu instantiating neural algorithms. neural-inspired learning algorithms achieving state performance many application areas speech recognition image recognition natural language processing information concepts person image represented synapses weighted connections neurons. success neural network dependent training weights neurons network. however training weights neural network non-trivial often high computational complexity large data sets requiring long training times. contributing factors computational complexity neural networks vector-matrix multiplications work supported sandia national laboratories laboratory directed research development program hardware acceleration adaptive neural algorithms grand challenge project. sandia national laboratories multi-mission laboratory managed operated sandia corporation wholly owned subsidiary lockheed martin corporation department energys national nuclear security administration contract de-ac-al. conventional computer processors designed process information manner neural algorithm requires recently major advances neural networks deep learning coincided advances processing power data access. however reaching limits moore’s terms much efﬁciency gained conventional processing architectures. addition reaching limits moore’s conventional processing architectures also incur neumann bottleneck processing unit’s program data memory exist single memory shared data them. contrast conventional processing architectures consist powerful centralized processing unit operate mostly serialized manner brain composed many simple distributed processing units sparsely connected operate parallel. communication neurons occurs synaptic connection operate independently neurons involved connection. thus vector-matrix multiplications implemented efﬁciently facilitated parallel operations. additionally synaptic connections brain generally sparse information encoded combination synaptic weights temporal latencies spike synapse biological synapses simply weighted binary connection rather exhibit non-linear synaptic response function release dispersion neurotransmitters space neurons. biological neurons communicate using simple data packets generally accepted binary spikes. contrast neuron models used traditional artiﬁcial neural networks commonly rate coded neurons. rate coded neurons encode information neurons real-valued magnitude output neuron—a larger output represents higher ﬁring rate. rate coded neurons stems assumption ﬁring rate fig. high level overview stpu. stpu composed leaky integrate neurons. neuron associated temporal buffer inputs mapped neuron time delay. neuronal encoding transformation addresses connectivity efﬁcacy temporal shift. functionality stpu mimics functionality biological neurons. functionality biological neurons. design stpu based following three neuroscience principles observed brain brain composed simple processing units operate parallel sparsely connected neuron local memory maintaining temporal state information encoded connectivity efﬁcacy signal propagation characteristics neurons. high-level overview biological neuron components onto stpu shown figure stpu derives dynamics leaky integrate neuron model neuron maintains membrane potential state variable tracks stimulation time step based following differential equation variable time constant ﬁrst-order dynamics index presynaptic neuron weight connecting neuron neuron time spike neuron synaptic delay neuron spike dynamic synaptic response function input spike. model neuron exceeds threshold synapses input neurons destination neurons deﬁned weight matrix given time weights inputs neurons change time. unique stpu neuron local temporal memory buffer composed memory cells model synaptic delays. biological neuron ﬁres latency associated arrival spike soma postsynaptic neuron time required propagate axon presynaptic neuron time neuron important piece information whereas temporally coded neurons encode information based spike neuron arrives another neuron. temporally coded information shown powerful rate coded information biologically accurate based neuroscience principles present spiking temporal processing unit novel neuromorphic hardware architecture designed mimic neuronal functionality alleviate computational restraints inherent conventional processors. neuromorphic architectures shown strong energy efﬁciency powerful scalability aggressive speed-up utilizing principles observed brain. build upon efforts leveraging beneﬁts energy consumption scalability time speed include efﬁcient implementation arbitrarily complex synaptic response functions digital architecture. important synaptic response function strong implications spiking recurrent neural networks also examine liquid state machines show constructs available stpu facilitate complex dynamical neuronal systems. examine stpu context lsms stpu general neuromorphic architecture. spiked-based algorithms implemented stpu section present stpu. high level comparison neuromorphic architectures presented section iii. present lsms section section examine lsms onto stpu show results running stpu. conclude section propagate dendrite soma postsynaptic neuron temporal buffer represents different synaptic junctions dendrites lower index value temporal buffer constitutes dendritic connection closer soma and/or shorter axon length larger index value. thus synapses stpu speciﬁed weight wkjd source input neuron destination neuron cell temporal buffer allows multiple connections neurons different synaptic delays. time step summation product inputs synaptic weights occurs added current value position ikwkjd temporary state temporal buffer. value cell temporal buffer shifted position ˆrd−. values bottom buffer neuron. biological neurons neuron ﬁres binary spike propagated axon synapse deﬁnes connection neurons. purpose synapse transfer electric activity information neuron another neuron. direct electrical communication take place rather chemical mediator used. presynaptic terminal action potential emitted spike causes release neurotransmitters synaptic cleft synaptic vescles. neurotransmitters cross synaptic cleft attach receptors postsynaptic neuron injecting positive negative current postsynaptic neuron. chemical reaction neurotransmitters broken receptors postsynaptic neuron released back synaptic cleft presynaptic neuron reabsorbs broken molecules synthesize neurotransmitters. terms electrical signals propagation activation potentials axon digital signal shown figure however chemical reactions occur synapse release reabsorb neurotransmitters modeled analog signal. behavior synapse propagating spikes neurons important ramiﬁcations dynamics liquid. equation synaptic response function represented following zhang dirac delta function used synaptic response function convenient implementation digital hardware. however dirac delta function exhibits static behavior. zhang show dynamical behavior modeled synapse using ﬁrst-order response presynaptic spike time constant ﬁrst-order response heaviside step function normalizes ﬁrstorder response function. dynamical behavior also implemented using second-order dynamic model fig. spike propagation along axon across synapse. spike propagated axon generally accepted binary spike. upon arrival synapse spike initiates chemical reaction synaptic cleft stimulates postsynaptic neuron. chemical reaction produces analog response soma postsynaptic neuron. stpu arbitrary synaptic response functions modeled efﬁciently using temporal buffer. synaptic response function discretely sampled encoded weights connecting neuron another mapped corresponding cells temporal buffer. time constants second order response normalizes second-order dynamical response function. zhang showed signiﬁcant improvements accuracy dynamics liquid using dynamical response functions. implementing exponential functions hardware expensive terms resources needed implement exponentiation. considering stpu composed individual parallel neuronal processing units neuron would need exponentiation functionality. including hardware mechanisms neuron exponentiation would signiﬁcantly reduce number neurons orders magnitude limited resources fpga. rather explicitly implement exponential functions hardware temporal buffer associated neuron. exponential function discretely sampled value sample assigned connection weight wkjd presynaptic neuron corresponding cell temporal buffer postsynaptic neuron thus single weighted connection neurons expanded multiple weighted connections neurons. shown graphically figure temporal buffer allows efﬁcient implementation digital signal propagation axon neuron mesh enabled temporal buffer available neuron stpu. truenorth provides highly programmable facilitate additional neural dynamics. spinnaker provides ﬂexibility neuron model however complex biological models computationally expensive. synapse model programmable stpu temporal buffer discretely sampling arbitrary synapse model. neuron model spinnaker optimized simpler synaptic models. complex synaptic models incur cost computational complexity. stpu ﬁrst neuromorphic architecture. four prominent neuromorphic architectures ibm’s truenorth chip stanford neurogrid heidelberg brainscales machine manchester spiking neural network architecture stanford neurogrid heidelberg brainscales analog circuits truenorth spinnaker digital circuits. stpu also digital system focus comparison truenorth spinnaker. truenorth chip leverages highly distributed crossbar based architecture designed high energy-efﬁciency composed cores. base-level neuron highly parametrized neuron. truenorth core binary crossbar existence synapse encoded junction individual neurons assign weights particular sets input axons. crossbar architecture allows efﬁcient vector-matrix multiplication. truenorth allows point-to-point routing. neurons core programmed spike destination addressed single particular core could core enabling recurrence different core. crossbar inputs coupled delay buffers insert axonal delays. neuron natively able connect multiple cores connect single neuron different temporal delays. work around neuron replicated within core mapped different cores. multiple temporal delays neurons obvious mechanism implementation spinnaker massively parallel digital computer composed simple cores emphasis ﬂexibility. unlike stpu truenorth spinnaker able model arbitrary neuron models instruction provided core. spinnaker designed sending large numbers small data packages many destination neurons. spinnaker designed modeling neural networks could potentially used generally ﬂexibility. stpu architecture falls truenorth spinnaker architectures. stpu implements less parameterized neuron truenorth however routing neural spikes ﬂexible allows multicast similar spinnaker rather unicast used truenorth. distinguishing feature stpu temporal buffer associated neuron giving stpu -dimensional routing. high-level summary comparison stpu truenorth spinnaker shown table liquid state machine neuro-inspired algorithm mimics cortical columns brain. conjectured cortical microcircuits nonlinearly project input streams high-dimensional state space. high-dimension representation used input areas brain learning achieved. cortical microcircuits sparse representation fading memory—the state microcircuit forgets time. lsms able mimic certain functionality brain noted lsms explain brain operates does. machine learning lsms variation recurrent neural networks fall category reservoir computing along echo state networks lsms differ echo state machines type neuron model used. lsms spiking neurons echo state machines rate coded neurons non-linear transfer function. lsms operate temporal data composed multiple related time steps. lsms composed three general components input neurons randomly connected leakyintegrate spiking neurons called liquid readout nodes read state liquid. diagram shown figure input neurons connected random subset liquid neurons. readout neurons connected neurons liquid subset them. connections neurons liquid based probabilistic models brain connectivity represent neurons euclidean distance variables chosen constants. paper dimensional grid deﬁne positions neurons liquid. liquid functions temporal kernel casting input data higher dimension. neurons allow temporal state carried time step another. lsms avoid problem training recurrent neural models section implement stpu. previous implementations lsms hardware however cases fpga vlsi chip designed speciﬁcally hardware implementation lsm. also zhang present low-powered vlsi hardware implementation lsm. schrauwen implement fpga chip. contrast work stpu developed general neuromorphic architecture. neuroscience work algorithms developed stpu spike sorting using spikes median ﬁltering currently stpu simulator implemented matlab well implementation fgpa chip. matlab simulator one-to-one correspondence hardware implementation. given constructs provided stpu liquid composed neurons maps naturally onto stpu. second-order synaptic response function equation based work zhang found second-order response function produced dynamics liquid allowing neural signals persist longer input sequence ﬁnished. lead improved classiﬁcation results. following zhang synaptic properties liquid including parameters connection probabilities liquid neurons deﬁned equation synaptic weights given table types neurons excitatory inhibitory observed brain liquid made network nuerons excitatory neurons inhibitory. probability synapse existing neurons weights neurons dependent types considered neurons. denotes presynaptic postsynaptic neurons connected synapse. example denotes connection excitatory presynaptic neuron inhibitory postsynaptic neuron. excitatory neurons increase action potential target neurons inhibitory neurons decrease action potential connections generated neurons liquid neurons randomly connected according equation parameters given table input neuron randomly connected subset neurons liquid weight chosen uniformly random. implement second-order synaptic response function equation sampled discrete time steps multiplied synaptic weight value neurons speciﬁed table discretely sampled weights encoded multiple weights corresponding cells temporal buffer postsynaptic neuron. implementation synaptic delay excitatory neurons. inhibitory neurons respectively. neurons fig. liquid state machine composed three components input neurons liquid—a recurrent spiking neurons readout neurons plastic synapses read state neurons liquid. training synaptic weights liquid readout nodes similar extreme machine learning random non-recurrent neural network non-temporal data assumed temporal integration encompassed liquid. thus liquid acts similarly kernel support vector machine streaming data employing temporal kernel. general weights connections liquid change although studies looked plasticity liquid readout neurons neurons plastic synapses allowing synaptic weight updates training. using neurons ﬁring state liquid temporal aspect learning temporal data transformed static learning problem. temporal integration done liquid additional mechanisms needed train readout neurons. classiﬁer used often linear classiﬁer sufﬁcient. training readout neurons done batch on-line manner lsms successfully applied several applications including speech recognition vision cognitive neuroscience practical applications suffer fact traditional lsms take input form spike trains. transforming numerical input data spike data non-temporal data represented temporally nontrivial. second-order response function counterintuitive since second-order response function perpetuates signal liquid longer. however examining ﬁrst-order second-order response functions shown figure shows ﬁrst-order response function larger initial magnitude quickly subsides. secondorder response function lower initial magnitude slower decay giving consistent propagation spike time. adjusting value accommodate behavior ﬁrst-order response function shows improvement made separation values spiking rate classiﬁcation accuracy. despite improvement ﬁrst-order response function achieve better performance second-order response function classiﬁcation accuracy. ﬁrst-order response function better separation score translate better accuracy. input encoding schemes traditional lsms input temporally encoded form spike train. unfortunately datasets temporally encoded rather numerically encoded. spike train input aligns neuroscience practically non-trivial encode information temporally brain does. therefore examine three possible encoding schemes rate encoding magnitude numeric value converted rate spike train rate liquid encoding magnitude numeric value converted representation given precision current injection. rate encoding requires time steps encode single input converting magnitude rate. similar binning information loss. encoding requires time step however requires inputs standard input convert magnitude plastic readout neurons connected neurons liquid. training done off-line using linear classiﬁer average ﬁring rate neurons liquid. examine effect various linear classiﬁers below. evaluate effect different parameters liquid state machine data spoken digit recognition arabic digits dataset composed time series mel-frequency cepstral coefﬁcients utterances digit speakers repetitions digit mfccs taken male female native arabic speakers ages dataset partitioned training speakers test speakers. scale variables evaluate performance examine classiﬁcation accuracy test measure separation liquid training set. good separation within liquid state vectors trajectories class distinguishable other. measure separability liquid state vectors liquid perturbed given input sequence follow deﬁnition norton ventura inter-class distance intra-class variance. separation ratio distance classes divided class variance. mean difference every pair classes norm number classes center mass class. given class intra-class variance mean variance state vectors inputs center mass class investigate various properties liquid state machine namely synaptic response function input encoding scheme liquid topology readout training algorithm. also consider impact liquid. neuron spike exceeds thus signiﬁcant impact dynamics liquid. beginning base value consider effects decreasing values default parameters reservoir size feed magnitude inputs input neurons linear train synapses readout neurons. synaptic response functions ﬁrst investigate effect synaptic response function using default parameters. using average separation values average spiking rates classiﬁcation accuracy linear given table iii. highlighted bold second-order synaptic function achieves largest separation values training testing lowest average spike rate highest classiﬁcation accuracy. average spike rate signiﬁcantly higher ﬁrst-order response function table shows separation values average spiking rates accuracy linear test input encoding schemes various values average spiking rate gives percentage neurons ﬁring liquid time series provides insight sparse spikes within liquid. table shows representative subset values used bold values represent highest separation value classiﬁcation accuracy encoding scheme. results show value signiﬁcant effect separation liquid well classiﬁcation accuracy svm. expected dynamics liquid dictated neurons ﬁre. lower threshold allows spikes indicated increasing values average spiking rates values decrease. overall using rate encoding produces greatest values separation. however signiﬁcant variability values change. rate encoding greatest accuracy achieved separation value. encoding schemes separation classiﬁcation accuracy appear correlated. greatest classiﬁcation accuracy achieved current injection. liquid topology topology liquid determines size liquid inﬂuences connections within liquid distance neurons impacts connections made neurons. cubic liquid densely connected compared column liquid section examine using liquids grids before consider different values separation values average spike rates accuracy linear given table values provided largest separation values topology conﬁguration encoding scheme combination. again value signiﬁcant impact separation liquid classiﬁcation accuracy. greatest separation values classiﬁcation accuracies topology highlighted bold. topologies current injection achieves highest classiﬁcation accuracy. interestingly separation values across encoding schemes topologies correlate accuracies. within encoding scheme topology however accuracy generally improves separation increases. current injection different topologies appear signiﬁcant impact classiﬁcation accuracy except topology decrease accuracy. increased number liquid nodes used input svm. converse true encoding topology achieves highest accuracy possibly increased number inputs representation input. readout training algorithms plastic synapses trained signiﬁcant effect performance lsm. traditionally lsms linear classiﬁer based assumption liquid transformed state space problem linearly separable. linear models represented weights threshold—which implemented neuromorphic hardware. using linear model liquid classiﬁcation done stpu avoiding overhead going chip make prediction. consider four linear classiﬁers linear linear discriminant analysis ridge regression logistic regression. algorithms default parameters statistics machine learning toolbox matlab. topologies values achieved highest classiﬁcation accuracy linear previous experiments. also limit examining current injection input scheme current injection consistently achieved highest classiﬁcation accuracy. results shown table consistently achieves highest classiﬁcation accuracy considered classiﬁers. highest classiﬁcation accuracy achieved paper presented spiking temporal processing unit stpu—a novel neuromorphic processing architecture. well suited efﬁciently implementing neural networks synaptic response functions arbitrary complexity. facilitated using temporal buffers associated neuron architecture. capabilities stpu including complex synaptic response functions demonstrated implementing functional mapping implementation onto stpu architecture. neural algorithms grow scale conventional processing units reach limits moore’s neuromorphic computing architectures stpu allow efﬁcient implementations neural algorithms. however neuromorphic hardware based spiking neural networks achieve energy. thus research needed understand develop spiking-based algorithms. deng hinton kingsbury types deep neural network learning speech recognition related applications overview proceedings international conference acoustics speech signal processing ciresan meier schmidhuber multi-column deep neural networks image classiﬁcation proceedings ieee conference computer vision pattern recognition. ieee computer society socher perelygin chuang manning potts recursive deep models semantic compositionality sentiment treebank proceedings conference empirical methods natural language processing. association computational linguistics october follett roth follett dammann white matter damage impairs adaptive recovery cortical damage silico model activity-dependent plasticity journal child neurology vol. merolla arthur alvarez-icaza cassidy sawada akopyan jackson imam nakamura brezzo esser appuswamy taba amir flickner risk manohar modha million spiking-neuron integrated circuit scalable communication network interface science august zhang choe digital liquid state machine biologically inspired learning application speech recognition ieee transactions neural networks learning systems vol. maass natschl¨ager markram real-time computing without stable states framework neural computation based perturbations neural computation vol. nov. verzi vineyard vugrin galiardi james aimone optimization-based computation spiking neurons proceedings ieee international joint conference neural network accepted. dayan abbott theoretical neuroscience computational mathematical modeling neural systems ser. computational neuroscience. cambridge london press benjamin mcquinn choudhary chandrasekaran j.-m. bussat alvarez-icaza arthur merolla boahen neurogrid mixed-analog-digital multichip system large-scale neural simulations proceedings ieee vol. schemmel briiderle griibl hock meier millner wafer-scale neuromorphic hardware system large-scale neural modeling proceedings ieee international symposium circuits systems furber galluppi temple plana spinnaker project proceedings ieee vol. severa carlson parekh vineyard aimone formal assessing strengths weaknesses neural architectures? case study using spiking cross-correlation algorithm nips workshop computing spikes banerjee basu liquid state machine dendritically enhanced readout low-power neuromorphic vlsi implementations ieee transactions biomedical circuits systems vol. hammami bedda improved tree model arabic speech recognition proceddings ieee international conference computer science information technology", "year": 2017}