{"title": "Blind Pre-Processing: A Robust Defense Method Against Adversarial  Examples", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Deep learning algorithms and networks are vulnerable to perturbed inputs which is known as the adversarial attack. Many defense methodologies have been investigated to defend against such adversarial attack. In this work, we propose a novel methodology to defend the existing powerful attack model. We for the first time introduce a new attacking scheme for the attacker and set a practical constraint for white box attack. Under this proposed attacking scheme, we present the best defense ever reported against some of the recent strong attacks. It consists of a set of nonlinear function to process the input data which will make it more robust over the adversarial attack. However, we make this processing layer completely hidden from the attacker. Blind pre-processing improves the white box attack accuracy of MNIST from 94.3\\% to 98.7\\%. Even with increasing defense when others defenses completely fail, blind pre-processing remains one of the strongest ever reported. Another strength of our defense is that it eliminates the need for adversarial training as it can significantly increase the MNIST accuracy without adversarial training as well. Additionally, blind pre-processing can also increase the inference accuracy in the face of a powerful attack on CIFAR-10 and SVHN data set as well without much sacrificing clean data accuracy.", "text": "szegedy computer vision biggio malware detection dnns vulnerable adversarial examples generated slightly changing inputs introducing random noise inputs. image classiﬁcation also popular ﬁelds facing vulnerability adversarial examples similarly behaviour popular convolutional neural network models exception shows resistance adversarial examples. various works generating adversarial attacks developing corresponding defense methods. attacking classiﬁed major categories. case attacker full access network model parameter known white attack. hand black attack attacker sees model black play data. fast gradient sign method popular white attack method uses sign gradient loss function different authors proposed different attack models adapt attacking techniques beating improving defenses recent attack methods achieved even better success causing classify whole mnist data wrong classes. example projected gradient descent powerful white attack till date many defense method proposed beat adversarial attacks. achieved good successes mainly weak attacks fast gradient sign method jacobian slaiency attack deep learning algorithms networks vulnerable perturbed inputs known adversarial attack. many defense methodologies investigated defend adversarial attack. work propose novel methodology defend existing powerful attack model. ﬁrst time introduce attacking scheme attacker practical constraint white attack. proposed attacking scheme present best defense ever reported recent strong attacks. consists linear function process input data make robust adversarial attack. however make processing layer completely hidden attacker. blind pre-processing improves white attack accuracy mnist even increasing defense others defenses completely fails blind preprocessing remains strongest ever reported. another strength defense that eliminates need adversarial training signiﬁcantly increase mnist accuracy without adversarial training well. additionally blind pre-processing also increase inference accuracy face powerful attack cifar- svhn data well without much sacriﬁcing clean data accuracy. deep neural network achieved great success performing classiﬁcation image detection speech recognition successes lead another interesting topic well perform practically depends heavily well address issues concerning security complex models. observed however none could recover current state accuracy powerful attacks. major improvement defenses came defending model started using adversarial examples training defense major drawbacks. first defender know attack model. difﬁcult choose adversarial attack method used generate adversary training. second would double training cost. still recent defenses technique strengthen defense. work look eliminate need using adversarial training processing input objective make neural network robust. moreover observed defenses mentioned loses strength attack strength increased. implementing proposed defense look make defense robust wide variety attacks. inspired recent coding thermometer encoding input data work propose blind pre-processing input data. process input data using tanh function batch normalization thermometer encoding encoding. techniques work combination discrete data defend adversarial attacks. however change attack introducing engineering solution gradient problem suggested recent obfuscated gradients paper work obfuscating gradient obvious white defense model would work presence powerful attack model. speciﬁcally none would work gradient shattering stochastic gradient vanishing gradient defense weapon. also suggested thermometer encoding using stochastic gradient could fool defense proposed attacking model. however introduce practical white attack attacker access model weights input gradient training algorithm pre-processing layer. obfuscated logic design proposing hardware assisted pre-processing layer counter obfuscated gradient’s false sense security strongly believe attack model many freedom restricting defense constraints. result believe future attack model evaluate models strength considering blind processing layer front model. defense realistic practical application autonomous vehicles. proposed defense method makes robust defend strongest attack logit space gradient ascent attack also general strong attack suggested anish modiﬁed version attack discrete data. main contribution work could achieve high accuracy mnist without adversary training ﬁrst time. include adversary training defense methodology provides best accuracy mnist cifar svhn till date. increasing attack strength defenses completely fails blind preprocessing still defends causing minimal accuracy loss. contributions work summarized follows derived previous analysis adversarial examples introducing linearity would increase defense adversarial attack. result introduced series functions performing linear transformation input data build robust defense. making model robust using pre-processing propose obfuscated logic based implementation pre-processing layer network fail obfuscated gradients. modiﬁcation also make defense obfuscated gradient succeed again. proposed defense resulted increasing accuracy mnist data adversarial training without adversarial training. best achieved ls-pga attack without adversarial examples. even adversarial training previous attempts could reach showed increasing attack strength defense remains strongest. analysis provides detailed understanding blind pre-processing stands advent adversary attack others fail. next sections summarize attack model blind processing scheme defend attack models. section proposed defense technique described following sections represents results analysis proposed method. work assume adversary complete access network popularly known white attack. observed previous works defense model performs well white attack naturally perform better black attack since black attack becomes really weak unavailability network parameters. general defense wants prove effectiveness madry introduced projected gradient descent achieved success fooling miss classify mnist dataset work show generate universal adversary among ﬁrst order approaches. suggested training network kind adversary would make neural network robust. taking work forward algorithm modiﬁed discrete input known discrete gradient ascent logit spcae projected gradient ascent. since method suitable discrete inputs choose evaluate model attack. additionally success white attack tempted evaluate defense model finally based success attacks varying defenses summarize strength table closely modiﬁed versions. attacks rely ﬁrst order information could labeled universal attack. since ls-pga/pgd attack model depends ﬁrst order information defending lspga attack would certainly make model robust wide variety attack models. attack takes place placing pixel random bucket within step attack look bucket values within true value select value harm. outcome attack vary depending initialization. attack needs several times desired result. case logit space projected gradient ascent ﬁrst discrete encoding relaxed continuous space perform projected gradient ascent. changing values attack step certainly change strength attack. report performance defense varying attack strength section recent attack developed reported generalized powerful till now. fool thermometer encoding also recent defenses. next section based analysis introduce concept blind processing also present obfuscated gradient actually work defense defense model. moreover present attacking scheme future attacking models test evaluate attack models. obfuscated gradient occur gradient shattering stochastic gradient gradient vanishing analysis presented evident defense uses weapon defend adversary eventually fail. reported obfuscated gradient gives false sense security. generalized attack caused recent defense models fail. thermometer encoding exception gradient shattering stochastic gradient issue. defense work need make obfuscated gradient based defense function properly advent powerful attack. ﬁnal conclusion typical white attack impossible defend white adversarial attack. even though defense works success small. conclude attacker many freedom defense constrained different optimization problems. result propose practical dealing problem hand. propose blind processing processing layer absolutely unknown attacker. defense model attacker access function pre-processing layer. obfuscated logic hide design pre-processing layer. attacker access input output process never predict functionality model. previous work done ﬁeld using hybrid polymorphic logic gate obscure functionality logic gate. propose using logic gate pre-process data practical application autonomous vehicle. attacker never replicate module reason. firstly layout using polymorphic logic looks kinds logic. hacker never predict functionality hacking module analyzing layout. secondly attacker never guess functionality using pattern input output pre-processing layer logic assisted. whoever never access pre-processing module generate input output data. user always power particular train model switch particular functionality using another key. additionally user also different different pre-processing functions. result would become impossible attacker guess functionality defense model inference mode well. whole pre-processing layer becomes blind pre-processing layer attacker. additionally kind logic also secured. pre-processing layer input logic gate total number would typical mnist classiﬁer input function would even assume preprocessing layer simple operation still would close possible combination. practically impossible attacker hack limited amount time. proposal assure security preprocessing layer deﬁne blind pre-processing. suggested defence must deﬁne attackers accessibility make defenses practical. case blind pre-processing keeps attribute white attack tact. attacker still access following information however denying accessibility attacker. firstly attacker predict pre-processing layer access layer gradients. secondly access query allowed without pre-processing layer. attributes constitute blind pre-processing. analysis concluded impossible traditional defenses defend previous attacking scheme. consequence introduce concept blind pre-processing encounter obfuscated gradient issue. result attacking scheme evaluate attack model defense model. strongly claim blind pre-processing generalized attack method suggested becomes failure. suggest future attack model generate adversary without back propagating pre-processing layer generate adversary using data pre-processing layer. since scheme white attack practical gives attacker form challenge. thus attack want succeed must prove worth beating obfuscated gradient based defenses without approximating gradient. blind pre-processing defense model approximating pre-processing function close impossible alone gradient. analysis existence adversary deep neural network goodfellow concluded deep neural networks exhibit vulnerability adversarial examples extreme linearity. linearity models reason resist adversary. example mnist data input precision bit. result common notion would network would respond differently input instead really small. however practical experiments show behave differently even really small weight matrix dimension average value perturbation would result \u0001*m*n increase activation. thus increased dimension noise keeps increasing linearly. suggests that sufﬁcient large input dimension network always vulnerable adversary. would provide possible directions solve issue. introduce non-linearity network eliminate unnecessary information input data. first tricky introducing non-linearity inside neural network creates problem calculating gradient. previously different non-linear activation functions investigated none worked difﬁculty computing gradients poor accuracy real test data set. work choose second direction adding pre-processing layer front neural network model gradient issue. additionally blind processing makes obfuscated gradient issue suggested nulliﬁed. moreover adding preprocessing layer front model hamper accuracy clean data. proposed blind pre-processing tanh batch normalization process input data attempt make robust. shown input data goes linear transformation ﬁrst tanh function. batch normalization layer also added similar layer learnable parameters. blind pre-processing input data quantized different levels. presented vectorized form coded value. finally encoded using thermometer encoding basically inverts digits ﬁrst appears. observe adding blind pre-processing qunatization made defense model robust. experiment quantize input data blind pre-processing. experiment found adversarial training longer necessary mnist. also believe function uses local information works well pre-processing layer. moreover preprocessing sacriﬁce much accuracy clean data time. previously thermometer encoding quantization suggested form pre-processing. proposed blind pre-processing layer consists tanh batch normalization layer. could possible function used blind pre-processing layer. description functionality layers presented here tanh sigmoid function popular activation functions sigmoid tanh functions. functions used typical deep neural networks. mathematically expressed batch normalization function normalization layer similar batch normalization layer typical convolutional neural network. however learnable parameters. rather pre-processing layer front original neural network model.the function written this denote input output tensor respectively. represents standard deviation mean input data particular batch. linear layer basically makes mean input data zero standard deviation one. used tanh place sigmoid work. combinations found perform better other. input image passed tanh layer works ﬁlter. followed ﬁlter maximum smoothing. data goes batch normalization layer followed level quantization. finally encoded values feed model training. proposed defense method ﬁrst tested simple lenet architecture mnist data set.mnist hand written digit training data testing data. used ls-pga attack model evaluate model described section ls-pga step-of-attack following experiments. here indicates much input pixels changes attack step described section later section also varied parameters effects performance proposed defense method. table indicates performance tanh better sigmoid ﬁltering function. result experiment choose tanh instead sigmoid. meanwhile thermometer encoding fails completely adversarial training removed. whereas tanh function improved accuracy even without adversary training. tanh smoothing layer investigate effect layer. based experiment observe maximum smoothing performs better average smoothing. table tabulates accuracy mnist different combination blind pre-processing layers discussed above.it gives insight performance attack model. experiment performed using different types training. clean training performed original training data mnist. adversarial training includes adversarial examples inside training data. training result evaluated using clean data attack data. clean data test basically tells well network performs classiﬁcation task attack. point combination works better defense method causing clean data accuracy marginally. however including adversarial training loss accuracy could recovered. quite obvious adversarial training increases overall robustness network. batch normalization turns powerful tool defense include adversarial training. result tanh smoothing ﬁlter fails without support batch normalization. best result without adversary training could obtained major contribution defense. previously thermometer encoding madry found that pgd/ls-pga attack accuracy drops zero mnist data without adverarial training.moreover combinations include tanh smoothing also performed really well defend adversary without adversary training. based experimental results could concluded adverarial function batch normalization layer. result batch size becomes important tuning parameter defense optimize. also investigated defense efﬁcacy using varying batch size blind preprocessing. increasing batch size would increase accuracy overall data set. batch size batch normalization normalizing whole image. experiments found optimal batch size achieve highest accuracy without adversary training shown adversarial training included general accuracy increases increasing batch size. case adversary training found optimum batch size leads best accuracy. however looking clean training decided choose optimum batch size experiments. ﬁgure line goes little blue additionally adversarial training included proposed method accuracy could improved. case combination tanh function batch normalization provides best performance result. smoothing helps defend attack adversary training included. next conduct similar analysis another famous dataset i.e. cifar svhn. street view house numbers dataset real world image testing machine learning algorithms. svhn data used resnet- architecture experiment achieved state-of-the-art accuracy. instead used tanh function blind pre-processing svhn dataset. data used step attack attack model parameter. choose different attack model parameter different data sets reported thermometer encoding defense. make comparison fair. table- could easily observe effect defense changes svhn dataset. firstly batch normalization still works strong defense causes clean data accuracy lot. tanh smoothing performs better thermometer encoding without adversarial training. looking clean data accuracy propose keep tanh function defending adversary svhn dataset. proposed defense method combined adversarial training could achieve accuracy. adversarial training used since removed svhn data encounter loss accuracy clean data. batch normalization layer worked component defense mnist without adversary training. however including adversary training blind pre-processing could recover state accuracy svhn data well. problem using batch normalization smoohting sacriﬁce clean data accuracy. using smoohting pre-processing layer clean data accuracy drops respectively. result even though provide form non-linearity fail defense. thus conclusion work non-linear function sacriﬁce clean data accuracy might potential candidate robust defense. cifar data used resnet- architecture little modiﬁcation attack strength. even though thermometer encoding reported using wider network helps increasing accuracy adversarial examples wide resnet found perform really well defending adversary. however paper wanted show performance functional ﬁlter improving accuracy previous defense model leave analysis choice architecture future investigation. choose attack parameter cifar equal number step attack equal similar svhn found good result using tanh pre-processing ﬁlter cifar data. additionally observed thermometer encoding produced lower accuracy even without adversarial training. could improve accuracy without adversary training maintaining close accuracy clean data. using adversarial training could close accuracy cifar regardless data proposed defense method helps improving accuracy without adversary training. however achieve better accuracy need include adversarial training. accuracy reported paper higher original thermometer encoding paper used deeper architecture makes thermometer defense robust. observing result svhn cifar clear smoothing causes sacriﬁce clean data accuracy. results model learning properly training. result combine smoothing worst result datasets. thus chose keep tanh function proposed defense cifar svhn dataset. next section analyze expreimental result explain relative phenomenon. based experimental results discussed previous section shows blind pre-processing defend mnist data without adversarial training. blind preprocessing also defend svhn cifar adversarial training. thus claim need adversarial training mnist data eliminated. section present in-depth analysis mnist data establish claim. also report black attack accuracy section prove attacks strength even further. order demonstrate strength defense show defense terms defending advent strong attacks changing value strength attack modiﬁed. larger value indicates stronger attack. observed performance previous defenses decreases exponentially increasing attack strength shown table however defense strength decreases small amount increasing attack strength another proof success universal robust defense. level change mnist data set. makes interesting result achieved even without adversarial training. defense model combined adversarial training would become strongest defense ever reported mnist data set. hand accuracy thermometer encoding drops even using adversarial training. analyse robustness defense carlini wagner method now. mean distance change cause miscalssiﬁcation carlini wagner attack metrice evaluate defenses robustness. also found overall strength defense depends accuracy clean data well. order analyze ﬁrst deﬁne important parameter called attacked data accuracy ratio assume accuracy clean data attacked data then described earlier changing value would change accuracy since decides much input image would changed cause miss classiﬁcations. defense withstand power full attack even value increased previous defense method even weaker attacks like fgsm withstand large increasing that increasing performance defense clean data accuracy actually goes little bit. better defense performance usually comes little sacriﬁce clean data accuracy. since performed best mnist also maintained greater value indicating hamper clean data accuracy most. mnist small dataset effect negligible. however svhn cifar effect accuracy degradation clean data severe. that’s choose drop model. found tanh optimizes clean data accuracy attacked data accuracy best two. could also explain phenomenon working cifar analysing pixel intensity distribution dataset. order understand robustness defense intuitively necessary understand frequency distribution dataset pixels. shown whole dataset’s pixel distribution completely changes applying blind pre-processing. understand redistribution image pixels plays important role robustness defense. typical cifar dataset frequency distribution displayed pixel values axis appearing frequency axis. distribution’s standard deviation changes completely applying tanh processing. point applying blind pre-processing pixel values clustered around certain pixel range. processing tanh pixel standard deviation increases becomes scattered. result pixel difference edges digit object becomes larger. thus fooling network becomes harder introducing noises less impact images larger transition near edges. however cifar change distribution much like mnist. inability change distribution reason failure cifar dataset. presented robust defense method uses combination pre-processing layers defend adversarial attacks. choose tanh batch normalization blind pre-processing layer. processing layer completely unknown accessible attacker. could defend mnist data ﬁrst time without using adversarial training. reduces training time complexity defense model. combination tanh batch normalization worked improving mnist accuracy close zero without adversarial examples. moreoever showed increasing attack defense defenses fail defense remains strongest ever reported. additionally proposed model worked better recent defenses svhn cifar well. required adversarial training recover accurrently conducting experiments variety attack. good result defending fgsm carlini wagner attack also black box. also ﬁrst defend fgsm attack without adversarial training reaching close accuracy. carlini wagner attack defense model requires highest amount input perturbation miss-classify input three distance metrices. strongest attack recently released succeeded breaking iclr defend models. even though tested obfuscated gradient attack reasonable amount success concept blind pre-processing actually makes attack void. references andor daniel alberti chris weiss david severyn aliaksei presta alessandro ganchev kuzman petrov slav collins michael. globally normalized transitionbased neural networks. arxiv preprint arxiv. athalye anish carlini nicholas wagner david. obfuscated gradients give false sense security circumventing defenses adversarial examples. https//arxiv.org/abs/.. biggio battista corona igino maiorca davide nelson blaine šrndi´c nedim laskov pavel giacinto giorgio roli fabio. evasion attacks machine learning test time. joint european conference machine learning knowledge discovery databases springer xiong wayne droppo jasha huang xuedong seide frank seltzer mike stolcke andreas dong zweig geoffrey. achieving human parity arxiv preprint conversational speech recognition. arxiv. kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition jacob buckman aurko colin raffel goodfellow. thermometer encoding resist adversarial examples. international conference learning representations https//openreview. net/forum?id=ssu--cw. accepted poster. ˛adry aleksander makelov aleksandar schmidt ludwig tsipras dimitris vladu adrian. towards deep learning models resistant adversarial attacks. stat moosavi-dezfooli seyed-mohsen fawzi alhussein frossard pascal. deepfool simple accurate method fool deep neural networks. proceedings ieee conference computer vision pattern recognition papernot nicolas mcdaniel patrick somesh fredrikson matt celik berkay swami ananthram. limitations deep learning adversarial settings. security privacy ieee european symposium ieee papernot nicolas mcdaniel patrick somesh swami ananthram. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium ieee szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv.", "year": 2018}