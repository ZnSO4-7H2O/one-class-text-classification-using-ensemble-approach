{"title": "Query-Efficient Black-box Adversarial Examples", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Current neural network-based image classifiers are susceptible to adversarial examples, even in the black-box setting, where the attacker is limited to query access without access to gradients. Previous methods --- substitute networks and coordinate-based finite-difference methods --- are either unreliable or query-inefficient, making these methods impractical for certain problems.  We introduce a new method for reliably generating adversarial examples under more restricted, practical black-box threat models. First, we apply natural evolution strategies to perform black-box attacks using two to three orders of magnitude fewer queries than previous methods. Second, we introduce a new algorithm to perform targeted adversarial attacks in the partial-information setting, where the attacker only has access to a limited number of target classes. Using these techniques, we successfully perform the first targeted adversarial attack against a commercially deployed machine learning system, the Google Cloud Vision API, in the partial information setting.", "text": "classiﬁer often binary api. furthermore often attacker access subset classiﬁcation outputs knowledge setting denote partialinformation setting considered prior work. prior work considering constrained threat models considered black-box restriction describe above; previous work primarily uses substitute networks emulate attacked network attack substitute traditional ﬁrst-order white-box methods however discussed thoroughly approach unfavorable many reasons including imperfect transferability attacks substitute original model computational query-wise cost training substitute network. recent attacks used ﬁnite difference methods order estimate gradients black-box case still expensive requiring millions queries generate adversarial image imagenet classiﬁer. effects throughput high latency rate limiting commercially deployed black-box classiﬁers heavily impact feasibility current approaches black-box attacks real-world systems. present approach generating black-box adversarial examples based natural evolutionary strategies provide motivation algorithm terms ﬁnite difference estimation random gaussian bases. demonstrate effectiveness method practice generating adversarial examples several orders magnitude fewer queries compared existing methods. consider constrained partial-information setting present algorithm attacking neural networks conditions. demonstrate effectiveness method showing reliably produce targeted adversarial examples access partial input-output pairs. newfound tractability given methods generate ﬁrst transformation-tolerant blackbox adversarial examples perform ﬁrst targeted attack google cloud vision demonstrating effectiveness proposed method large commercial systems opaque points potential exploitation particularly face rising popularity neural networks real-world systems. commercial proprietary systems however adversarial examples must considered much restrictive threat model. first settings black-box meaning attacker access input-output pairs rather taking component-wise ﬁnite differences previous state-of-the methods natural evolutionary strategies natural evolutionary strategies method derivative-free optimisation based idea search distribution particular rather maximizing objective function directly maximizes expected value loss function search distribution. demonstrated section allows gradient estimation fewer queries typical ﬁnite-difference methods. concretely loss function current parameters manner similar choose search distribution random gaussian noise around current image evaluating gradient search distribution yields following variance-reduced gradient estimate similarly employ antithetic sampling generate batches values; rather generating values instead draw values optimization empirically shown improve performance nes. closer inspection special case described suggests alternative view algorithm. particular note antithetic sampling used gradient estimate written following represents directional derivative direction propose variant inspired treatement method generating black-box adversarial examples. relate special case ﬁnite difference method gaussian bases providing theoretical comparison previous attempts black-box adversarial examples. demonstrate method effective efﬁciently synthesizing adversarial examples; method require substitute network orders magnitude faster optimized ﬁnite differencebased methods reliably produce blackbox adversarial examples cifar- imagenet classiﬁers. propose approach synthesizing targeted adversarial examples partial information setting attacker access top-k outputs classiﬁer demonstrate effectiveness. robust black-box examples. inability standard-generated adversarial examples remain adversarial transformation noted expectation transformation algorithm introduced. integrating method presented work generate ﬁrst transformation-tolerant black-box adversarial examples. targeted adversarial examples several-thousand-way commercial classiﬁer. method generate adversarial examples google cloud vision commercially-deployed system. attack commercial classiﬁer order magnitude demonstrates applicability reliability method. outline technical components approach allowing attack constructed threat model. first describe application natural evolutionary strategies then outline strategy used construct adversarial examples partial-information setting. classes yk}. normal settings given image label generating adversarial example targeted yadv acheived using standard ﬁrst-order attacks. attacks involve essentially ascending estimated gradient however case unavailable classiﬁer. resolve this propose following algorithm. rather beginning image instead begin image original target class. yadv top-k classes perform following iterated optimization represents projection onto \u0001-box particular concurrently perturb image maximize adversarial probability projecting onto boxes decreasing sizes centered original image maintaining adversarial class remains within top-k times. practice implement iterated optimization using backtracking line search several iterations projected gradient descent alternatingly updating reaches desired value yields adversarial example \u0001-away maintaining adversarial classiﬁcation original image. threat model chosen model constraints attacking deep neural networks deployed real world. access gradients logits internals. similar previous work deﬁne black-box mean access gradients logits network internals unavailable. furthermore attacker knowledge network architecture. attacker access output classiﬁer prediction probabilities class. limited number queries. real-time models like self-driving cars format input allows make large number queries network cases proprietary models like google cloud vision rate-limited simply unable support large effectively randomly drawn gaussian vectors size width height channels. well-known result vectors nearly orthogonal; formalization says n-dimensional space randomly sampled gaussian vectors thus gradient seen essentially clipping space ﬁrst gaussian basis vectors performing ﬁnite-differences estimate. concretely considering matrix columns projection results concentration theory analyze estimate either following simple canonical bound complex treatment given gaussian-projected ﬁnite difference gradient estimates bounds demonstrated works detail algorithm’s interaction dimensionality scaling various factors. next consider partial-information setting described previous section. particular assume access probabilities gradient approximations methods described section partial-information setting discussed section also consider work case full output classiﬁer unavailable attacker. accurately reﬂects state commercial systems even list possible classes unknown attacker google cloud vision amazon’s rekognition clarifai api. attackers goals untargeted targeted misclassiﬁcation targeted attacks strictly harder. successful targeted adversarial example classiﬁed speciﬁc target class. untargeted adversarial example misclassiﬁed. notably omit wall-clock time attack security parameter threat model. metric indicative hardware resources used attack efﬁcacy attack itself query count realistic practical measure. evaluate effectiveness black-box attack generating targeted adversarial examples neural networks trained cifar- imagenet. demonstrate attack cifar- network carlini wagner inceptionv network blackbox setting assuming access output probabilities classiﬁers. classiﬁers randomly choose examples test example choose random target class. projected gradient descent gradient estimates maximizing probability target class constraining maximum perturbation ﬁxed hyperparameters across attacks single classiﬁer attack produce adversarial image time table summarizes results experiment. attack highly effective query-efﬁcient success rate cifar- mean queries black-box classiﬁer example success rate imagenet mean queries blackbox classiﬁer example. figures show sample adversarial examples produced. figures show distribution number queries required produce adversarial example cases attack requires small number queries. evaluate effectiveness black-box attack generating adversarial examples fool classiﬁers distribution transformations using expectation-overtransformation task given distribution transformations constraint attempt adversarial example table quantitative analysis targeted adversarial attacks perform randomly chosen test images randomly chosen target classes. attacks limited million queries image adversarial perturbations constrained hyperparameters used images dataset. evaluate effectiveness partial-information black-box attack generating targeted adversarial examples inceptionv network given access class probabilities total labels. randomly choose examples test example choose random target class. source-target pair example target class test initialize image partial-information attack construct targeted adversarial example. gradient estimates constraining maximum perturbation ﬁxed hyperparameters across attacks attack produce adversarial example time order demonstrate relevance applicability approach real-world system attack google cloud vision commercially available computer vision suite offered google. particular attack general object labeling classiﬁer performs nway classiﬁcation given image. case considerably challenging even typical black-box setting. number classes large unknown full enumeration labels unavailable. classiﬁer returns conﬁdence scores label assigns image seem neither probabilities logits. classiﬁer return scores labels instead returns unspeciﬁed-length list labels varies based image. despite challenges successfully demonstrate ability system generate black-box adversarial examples untargeted attack targeted attack. figure shows unperturbed image correctly labeled several including weapon ﬁrearm. algorithm presented work rather maximizing probability target class write following loss function based attack solve optimization problem using estimate gradient classiﬁer. note classiﬁer’s output probability label given input. evaluation randomly choose examples imagenet validation example randomly choose target class. choose distribution transformations degree rotation unif ﬁxed hyperparameters across attacks perform attack achieve greater adversariality random sample transformations. table shows results experiment. achieve mean attack success rate attacks mean queries example. figure shows samples adversarial examples robust note expand deﬁnition misclassiﬁcation encompass semantic similarity—that uninterested modiﬁcation induces classiﬁcation persian tabby cat. applying presented algorithm loss function yields adversarial example shown figure deﬁnitively demonstrating applicability method real-world commercial systems. figure shows unperturbed image correctly labeled several skiing-related classes including skiing ski. partial-information attack force image classiﬁed dog. note label appear output unperturbed image. initialize algorithm photograph partial-information attack synthesize image looks like skiers classiﬁed dog. szegedy ﬁrst demonstrated neural networks vulnerable adversarial examples. number techniques developed generate adversarial examples white-box case attacker assumed full access model parameters architecture. previous work shown adversarial examples generated black-box case training substitute model exploiting white-box techniques substitute however attack unreliable assumes adversarial examples transfer target model. best adversarial images less effective cases attacks entirely fail transfer ensembles substitute models need used attack require transferability assumption. recent attempts ﬁnite differences estimate gradient instead training substitute models however even various query reduction techniques large number queries required generate single attack image potentially rendering real-world attacks transformationtolerant adversarial examples intractable. comparison method uses several orders magnitude fewer queries. prior work demonstrated black-box methods feasibly attack real-world commercially deployed systems including image classiﬁcation apis clarifai metamind google amazon speech recognition system google work advances prior work machine learning systems deployed real world demonstrating highly effective queryefﬁcient attack google cloud vision partial-information setting scenario explored prior work. work present algorithm based natural evolutionary strategies allows generation adversarial examples black-box setting withtraining substitute network. also introduce partial-information setting restricted black-box situation better models large-scale commercial systems present algorithm crafting targeted adversarial examples setting. motivate algorithm formulation ﬁnite differences random normal projection demonstrate empirical efﬁcacy method generating black-box adversarial examples orders magnitude efﬁcient previous work cifar- imagenet datasets. using combination described algorithm algorithm generate ﬁrst robust black-box adversarial examples constitutes step towards attacking real-world systems. also demonstrate efﬁcacy partial-information attack. finally synthesize targeted adversarial examples commercial google cloud vision demonstrating ﬁrst targeted attack partial-information system. results point promising method efﬁciently reliably generating black-box adversarial examples.", "year": 2017}