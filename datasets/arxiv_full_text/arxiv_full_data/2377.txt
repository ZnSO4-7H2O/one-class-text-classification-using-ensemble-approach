{"title": "An Efficient Message-Passing Algorithm for the M-Best MAP Problem", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Much effort has been directed at algorithms for obtaining the highest probability configuration in a probabilistic random field model known as the maximum a posteriori (MAP) inference problem. In many situations, one could benefit from having not just a single solution, but the top M most probable solutions known as the M-Best MAP problem. In this paper, we propose an efficient message-passing based algorithm for solving the M-Best MAP problem. Specifically, our algorithm solves the recently proposed Linear Programming (LP) formulation of M-Best MAP [7], while being orders of magnitude faster than a generic LP-solver. Our approach relies on studying a particular partial Lagrangian relaxation of the M-Best MAP LP which exposes a natural combinatorial structure of the problem that we exploit.", "text": "much eﬀort directed algorithms obtaining highest probability conﬁguration probabilistic random ﬁeld model known maximum posteriori inference problem. many situations could beneﬁt single solution probable solutions known m-best problem. paper propose eﬃcient message-passing based algorithm solving m-best problem. speciﬁcally algorithm solves recently proposed linear programming formulation mbest orders magnitude faster generic lp-solver. approach relies studying particular partial lagrangian relaxation m-best exposes natural combinatorial structure problem exploit. introduction large number problems computer vision natural language processing computational biology formulated search probable state discrete probabilistic graphical model known inference problem. number applications beneﬁt single best solution rather list m-best hypotheses. example sentences often ambiguous machine translation systems beneﬁt working multiple plausible parses sentence. computational biology practitioners often interested computing stable conﬁgurations protein structure. moreover computing m-best hypotheses useful assessing sensitivity model w.r.t. variations input and/or parameters model. known m-best problem interestingly algorithms m-best problem closely followed development algorithms solving problem. similar ﬁrst family algorithms m-best junction-tree based exact algorithms feasible low-treewidth graphs. high-treewidth models belief propagation typically used perform approximate inference yanover weiss showed pseudomax-marginals produced used compute approximate m-best maps. however development linear programming relaxations concurrence between m-best longer true. message-passing algorithms solving available soon studied algorithm known solving m-best discrepancy merely theoretical concern large-scale empirical comparisons found message-passing algorithms signiﬁcantly outperform commercial solvers importantly message-passing algorithms applied largescale problems solvers like cplex simply would scale. thus apply m-best real instances appearing computational biology computer vision must develop scalable distributed message-passing algorithms. overview. principal contribution paper develop eﬃcient message-passing algorithm m-best problem discrete undirected graphical models speciﬁcally markov random fields approach studies particular partial lagrangian relaxation m-best exposes natural modular structure problem. graphs cycles lagrangian relaxation involves exponentially large dual variables dynamic subgradient method solving lagrangian dual. dsms recently formalized class methods interleave separation hand consider discrete random variables taking value ﬁnite label denote tuple ×i∈axi. ease notation shorthand x{ij}. vector map. graph deﬁned functions deﬁning energy techniques developed paper naturally applicable higher-order mrfs well. however simplify exposition restrict pairwise energy functions. integer program. inference typically integer programming problem high-level algorithm brings m-best equal footing vis-a-vis message-passing algorithm solving corresponding lp-relaxation. importantly algorithm retains guarantees formulation fromer globerson orders magnitude faster. similar observations algorithm enables solving m-best large instances unsolvable generic solvers. outline. begin brief history m-best problem section present preliminaries background section revisit m-best formulation fromer globerson section study lagrangian dual present message-passing dual ascent algorithm tree-mrf section general mrfs section problem ﬁnding solutions general combinatorial optimization problem typically studied context k-shortest paths search graph. lawler proposed general algorithm compute solutions large family discrete optimization problems ideas used lawler’s algorithm form basis algorithms m-best problem. complexity ﬁnding best solution number variables lawler’s algorithm solves problems. best solution among problems second best solution original problem. thus multiplicative overhead solutions iteratively found. hamacher queyranne reduced overhead assuming access algorithm compute ﬁrst second best solutions. dechter colleagues recently provided dynamic-programming algorithms m-best exponential treewidth. yanover weiss proposed algorithm requires access max-marginals. thus certain classes mrfs allow eﬃcient exact computation max-marginals e.g. binary pairwise supermodular mrfs m-best solutions found arbitrary treewidth graphs. moreover approximate mbest solutions found approximating max-marginal computation e.g. loopy recently fromer globerson provided view m-best problem. proposed algorithm repeatedly partitions space solutions solves nd-best within partition also known local polytope. relaxation known tight special cases like treegraphs binary submodular energies meaning optimal vertex local polytope inmap integer program suggested natural relaxation case mbest integer program exclusion constraints linear constraints. fromer globerson introduced concise representation polytope called assignment-excluding local poly) excludes previous solutions notice algorithm requires solving linear program aelp iteration. large problems arising computer vision computational biology solving standard lp-solver even infeasible. next section present proposed message-passing algorithm solving m-best ﬁrst restrict attention tree-structured mrfs. simple enough scenario describe main elements approach; discuss general case section tree mrfs single spanning tree inequality simplify notation refer simply m-best written idea exploiting structure. partial lagrangian immediately exposes structure problem primal formulation obfuscating namely spanning tree inequality distributes according tree structure i.e. integral vertices. thus minimization eﬃciently performed running combinatorial optimization algorithm perturbed need solved generic solver. next able eﬃciently evaluate lagrangian need able optimize lagrangian dual. value primal problem tightest lowerbound obtained solving lagrangian dual problem maxλ≥ since non-smooth concave function achieved supergradient ascent algorithm analogous subgradient descent minimizing non-smooth convex functions since constrained variable follow projected supergradient ascent algorithm iteratively updating lagrange multipliers according fol+ supergradient step-size projection operator sequence multipliers {αt} satisﬁes projected susupergradient intuitive interpretation. recall lagrangian relaxation minimizes linear combination energy value spanning tree inequality violates weighting given spanning tree constraints i.e. diﬀerent previous solution supergradient w.r.t. positive cost violating constraint increase update thus encouraging next solution satisfy spanning tree constraints. conversely constraints satisﬁed supergradient negative indicating over-penalizing violations reduced allow lower energy solutions. tightness lagrangian relaxation. primal problem strong duality holds. thus projected supergradient algorithm described exactly solves m-best fromer globerson total complexity algorithm number dual ascent iterations number nodes largest label space i.e. maxi |xi|. section build basic ideas previous section develop algorithm general graphs contain exponentially many spanning trees. recall m-best general graphs given order optimize exponentially large dual variables follow dynamic supergradient method overview). intuitively dsms thought dual-procedure cutting-plane algorithm. constraint management block given current primal point constraint management block augments index active dual variables index optionally block also choose drop dual variables index set. block described detail below. primal dual blocks discussed next subsection describe supergradient eﬃciently computed general graphs. describe constraint management block detail. develop intuition step recall complementary slackness condition tells given pair optimal primal dual variables thus intuitively active must focus dual variables corresponding violated inequalities. maximum violated oracle adds active note process dualized version cutting-plane method fromer globerson instead adding violated spanning tree inequality include index dual variable working set. shown dynamic supergradient method maximum-violation-oracle constraint management block guaranteed converge optimum lagrangian relaxation choice stepsize rules standard supergradient methods. rigorous proof found dsms actually allow dual variables removed active well certain conditions. however keep exposition simple match implementation discuss here refer reader optimal primal solution current setting case tree-mrfs could compute optimal solution dynamic programming. general mrfs supergradient computation involves solving note expanded partial lagrangian efﬁciently evaluated running two-pass max-product tree-structured subproblems. number tree-structured subproblems equal optimizing expanded partial lagrangian. dual problem expanded partial lagrangian given maxλ≥δ∈∆ previous section described dual partial lagrangian optimized dynamic supergradient method. optimizing dual expanded lagrangian similar described procedure minor modiﬁcation dual variable always stays active supergradient w.r.t. given used solve problem. however unlike two-pass max-product used tree-mrfs message-passing algorithms typically require many hundreds iterations converge. running iterations step supergradient ascent become prohibitively slow. speeding process realize partial lagrangian evaluating diﬃcult suggests constraints dualized till partial lagrangian becomes tractable. precisely using ideas dual-decomposition literature expanding partial lagrangian. order expand partial lagrangian ﬁrst identify tractable subcomponents. spanning-tree inequalities already tree structured. convert collection tree-structured factors. i.e. collection spanning trees edge appears least tree approach doesn’t really need trees spanning describe following spanning-tree cover keep notation simple. slight abuse notation denote subset primal variables also assign spanning tree inequality active copy primal variables finally node energy spanning tree factor λmjµm variables write existing partial lagrangian enforced subtracting mean dual variables. overall dual update w.r.t. given stepsize zero-projection operator i.e. entire algorithm summarized algorithm call algorithm steelars spanning tree inequality lagrangian relaxation scheme. grangian however similar argument tree-mrf case problem thus strong duality holds partial lagrangians achieve optimum. implies even non-tree mrfs steelars exactly solves m-best fromer globerson introduce approximation integer program. moreover guarantee depend choice spanning-tree cover fact tree cover used. finally note described expanded partial lagrangian terms tree-structured subproblems. however eﬃcient subproblem used e.g. submodular subproblems solved graph-cuts -label submodular mrfs problems guaranteed tight m-best moreover tree decomposition required -label submodular factor eﬃciently minimized graph-cuts baselines. compared algorithm stripes algorithm fromer globerson lawler-nilsson algorithm bmmf algorithm yanover weiss recall stripes directly solve m-best rather solves sequence nd-best encapsulated lawler-nilsson partitioning scheme. tradeoﬀ eﬃciency accuracy would eﬃcient directly solve m-best fractional thus partitioning scheme performs better. note steelars could also encapsulated lawlernilsson partitioning scheme straightforward manner. however wish study performance lagrangian relaxation m-best directly leave partitioning scheme extension future work. implementation details. implementations stripes lawler-nilsson provided authors bmmf provided authors stripes iteration solved using library. steelars implemented matlab max-product written eﬃciency. experiments performed -bit -core intel machine timing reported cputime. following chose stepsize iteration number times objective value decreased iteration next. rule convergence guarantees standard decaying rule empirically performs much better. integer primal extraction. steelars dualascent algorithm thus always maintains feasible dual solution necessarily primal feasible solution interested however since primal block repeatedly called computing supergradient simply keep track best primal feasible solution produced output algorithm. evaluation. compare diﬀerent algorithms metrics run-time accuracy solutions returned. tree-mrfs methods guaranteed return exact solutions thus simply compare run-times. general mrfs follow protocol measure relative accuracy diﬀerent methods. speciﬁcally pool solutions returned methods note solutions pool method report fraction solutions contributed. results demonstrate eﬀectiveness steelars primarily terms eﬃciency. show since steelars message-passing algorithm signiﬁcantly faster generic solver sometimes orders magnitude even though guaranteed converge solution. steelars naturally scales large instances previously unsolvable using solvers. tree mrfs generated synthetic problems sampling random spanning trees nodes. variable could take labels. node edge potentials sampled standard gaussians. m-best guaranteed tight thus stripes steelars produces precisely answers. fig. shows time taken algorithms function size tree averaged samplings parameters. note x-axis log-scale. increase stripes quickly becomes intractable gplk ultimately running memory. however steelars shows much better behaviour runtime. fig. also shows value dual best feasible integer primal produced steelars function number iterations. recall steelars iteration corresponds running max-product single tree. fig. shows generally number iterations pretty would require call compute maxmarginals partitioning scheme thus steelars much slower optimal thing tree. course bmmf would also require multiple iterations synchronous loopy graphs checked next. -label submodular mrfs. scenario constructed grids. variable could take labels. sampled node edge energies gaussians ensured edge energies submodular. allows graph-cuts optimizing submodular factor. fig. shows value dual best feasible integer primal function number iterations. sharp falls dual correspond constraint management block calling separation oracle increment working set. iteration involves call however max-ﬂow algorithms general implementation particular highly eﬃcient. fig. shows run-time algorithms function steelars fastest algorithms becoming intractable quickly. unfortunately fig. shows also least accurate validating choice fromer globerson solve directly partitioning scheme instead. plan follow direction. interestingly fig. seems suggest bmmf performs worse stripes even though bmmf simply involves calls loopy believe artifact caused fact bmmf implementation written matlab particularly optimized. moreover speciﬁc experiment could made faster computing maxmarginals approach instead loopy high-level diﬀerence bmmf steelars analogous diﬀerence loopy mplp message-passing algorithms solves relaxation provides improving lower-bounds. general mrfs. scenario constructed grid graphs well variable could take -labels edge energies restricted attractive. used standard two-tree decomposition grid graph. thus iteration product observed trends similar submodular mrfs case terms number iterations required steelars converge relative standing w.r.t. baselines. case conclusion presented ﬁrst message-passing algorithm solving relaxation m-best problem discrete undirected graphical models. approach used particular lagrangian relaxation construct partial lagrangian allowed combinatorial optimization algorithms. handle exponentially large constraints used dynamic supergradient scheme essentially dual procedure cutting-plane algorithm. message-passing algorithm retains guarantees formulation fromer globerson orders magnitude faster. extracting diverse m-best solutions. number applications especially computer vision mbest solutions essentially minor perturbations other. concurrent work also presented solution diverse m-best problem given measure ‘distance’ solutions block solutions within kdistance-ball previous solutions. future work. number interesting directions front short-term interested encapsulating steelars inside lawlernilsson partitioning scheme similar stripes increase accuracy method. since m-best often fractional another direction tighten e.g. using techniques proposed sontag would also interesting compare lp-relaxation based methods like stripes steelars heurisitic-search methods acknowledgements. thank sebastian nowozin human encyclopedia optimization pointing literature dynamic subgradient methods; amir globerson sharing stripes implementation useful discussions; danny tarlow discussions helped formalize formulation.", "year": 2012}