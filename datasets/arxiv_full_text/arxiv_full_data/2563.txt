{"title": "Learning Convolutional Neural Networks for Graphs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.", "text": "figure receptive ﬁeld size ﬁeld moved image left right bottom using particular stride zero-padding values read receptive ﬁelds transformed linear layer convolutional architecture node sequence receptive ﬁelds created shapes receptive ﬁelds fully determined hyper-parameters. nodes edges multiple discrete continuous attributes multiple types edges. similar convolutional neural network images construct locally connected neighborhoods input graphs. neighborhoods generated efﬁciently serve receptive ﬁelds convolutional architecture allowing framework learn effective graph representations. proposed approach builds concepts convolutional neural networks images extends arbitrary graphs. figure illustrates locally connected receptive ﬁelds images. image represented square grid graph whose nodes represent pixels. seen traversing node sequence generating ﬁxed-size neighborhood graphs nodes. neighborhood graphs serve receptive ﬁelds read feature values pixel nodes. implicit spatial order pixels sequence nodes neighborhood graphs created left right bottom uniquely determined. holds problems sentence determines numerous important problems framed learning graph data. propose framework learning convolutional neural networks arbitrary graphs. graphs undirected directed discrete continuous node edge attributes. analogous image-based convolutional networks operate locally connected regions input present general approach extracting locally connected regions graphs. using established benchmark data sets demonstrate learned feature representations competitive state graph kernels computation highly efﬁcient. given collection graphs learn function used classiﬁcation regression problems unseen graphs. nodes graphs necessarily correspondence. instance graph collection could model chemical compound output could function mapping unseen compounds level activity cancer cells. sequence words. however numerous graph collections problem-speciﬁc ordering missing nodes graphs correspondence. instances solve problems determining node sequences neighborhood graphs created computing normalization neighborhood graphs unique mapping graph representation vector space representation. proposed approach termed patchy-san addresses problems arbitrary graphs. input graph ﬁrst determines nodes neighborhood graphs created. nodes neighborhood consisting exactly nodes extracted normalized uniquely mapped space ﬁxed linear order. normalized neighborhood serves receptive ﬁeld node consideration. finally feature learning components convolutional dense layers combined normalized neighborhood graphs cnn’s receptive ﬁelds. figure illustrates patchy-san architecture several advantages existing approaches first highly efﬁcient naively parallelizable applicable large graphs. second number applications ranging computational biology social network analysis important visualize learned network motifs patchy-san supports feature visualizations providing insights structural properties graphs. third instead crafting another graph kernel patchy-san learns application dependent features withneed feature engineering. theoretical contributions deﬁnition normalization problem graphs complexity; method comparing graph labeling approaches collection graphs; result shows patchy-san generalizes cnns images. using standard benchmark data sets demonstrate learned cnns graphs efﬁcient effective compared state graph kernels. related work graph kernels allow kernel-based learning approaches svms work directly graphs kernels graphs originally deﬁned similarity functions nodes single graph representative classes kernels skew spectrum kernel kernels based graphlets latter related work builds kernels based ﬁxed-sized subgraphs. subgraphs often called motifs graphlets reﬂect functional network properties however combinatorial complexity subgraph enumeration graphlet kernels restricted figure illustration proposed architecture. node sequence selected graph graph labeling procedure. nodes sequence local neighborhood graph assembled normalized. normalized neighborhoods used receptive ﬁelds combined existing components. subgraphs nodes. effective class graph kernels weisfeiler-lehman kernels kernels however support discrete features memory linear number training examples test time. patchy-san uses possible labeling procedure compute receptive ﬁelds. deep graph kernels graph invariant kernels compare graphs based existence count small substructures shortest paths graphlets subtrees graph invariants contrast patchy-san learns substructures graph data limited predeﬁned motifs. moreover graph kernels training complexity least quadratic number graphs prohibitive large-scale problems patchy-san scales linearly number graphs. graph neural networks recurrent neural network architecture deﬁned graphs. gnns apply recurrent neural networks walks graph structure propagating node representations ﬁxed point reached. resulting node representations used features classiﬁcation regression problems. gnns support discrete labels perform many backpropagation operations edges nodes graph learning iteration. gated graph sequence neural networks modify gnns gated recurrent units output sequences recent work extended cnns topologies differ low-dimensional grid structure methods however assume global graph structure correspondence vertices across input examples. perform convolutional type operations graphs developing differentiable variant speciﬁc graph feature. cnns inspired earlier work showed visual cortex animals contains complex arrangements cells responsible detecting light small local regions visual ﬁeld cnns developed applied image speech text drug discovery problems predecessor cnns neocognitron typical composed convolutional dense layers. purpose ﬁrst convolutional layer extraction common patterns found within local regions input images. cnns convolve learned ﬁlters input image computing inner product every image location image outputting result tensors whose depth number ﬁlters. graphs graph pair vertices edges. number vertices number edges. graph represented adjacency matrix size edge vertex vertex otherwise. case vertex position moreover adjacent. node edge attributes features attain value node edge graph. term attribute value instead label avoid confusion graph-theoretical concept labeling. walk sequence nodes graph consecutive nodes connected edge. path walk distinct nodes. write denote distance length shortest path -neighborhood node nodes adjacent labeling node partitions. patchy-san utilizes graph labelings impose order nodes. graph labeling function vertices ordered real numbers integers. graph labeling procedure computes graph labeling input graph. clear context labeling refer both graph labeling procedure compute ranking function ...|v every labeling induces ranking labeling graph injective determines total order vertices unique adjacency matrix vertex position moreover every graph labeling induces partition examples graph labeling procedures node degree measures centrality commonly used analysis networks. instance betweeness centrality vertex computes fractions shortest paths pass weisfeiler-lehman algorithm procedure partitioning vertices graph. also known color reﬁnement naive vertex classiﬁcation. color reﬁnement attracted considerable interest community since applied speed-up inference graphical models method compute graph kernels patchy-san applies labeling procedures among others impose order nodes graphs replacing application-dependent orders missing. isomorphism canonicalization. computational problem deciding whether graphs isomorphic surfaces several application domains. graph isomorphism problem known np-hard. several mild restrictions known instance graphs bounded degree canonicalization graph graph ﬁxed vertex order isomorphic represents entire isomorphism class. practice graph canonicalization tool nauty shown remarkable performance learning cnns arbitrary graphs cnns applied images receptive ﬁeld moved image particular step size. receptive ﬁeld reads pixels’ feature values channel once patch values created channel. since pixels image implicit arrangement spatial order receptive ﬁelds always moved left right bottom. moreover spatial order uniquely determines nodes receptive ﬁeld nodes mapped vector space representation consequently values read pixels using different locations receptive ﬁeld assigned relative position pixels’ structural roles identical. show connection cnns patchy-san frame cnns images identifying sequence nodes square grid graph representing image building normalized neighborhood graph receptive ﬁeld node identiﬁed sequence. graph collections application-dependent node order missing nodes graphs aligned need determine graph sequences nodes create neighborhoods unique mapping graph representation vector representation nodes similar structural roles neighborhood graphs positioned similarly vector representation. address problems leveraging graph labeling procedures assigns nodes different graphs similar relative position respective adjacency matrices structural roles within graphs similar. given collection graphs patchy-san applies following steps graph select ﬁxed-length sequence nodes graph; assemble ﬁxed-size neighborhood node selected sequence; normalize extracted neighborhood graph; learn neighborhood representations convolutional neural networks resulting sequence patches. node sequence selection process identifying input graph sequence nodes receptive ﬁelds created. algorithm lists procedure. first vertices input graph sorted respect given graph labeling. second resulting node sequence traversed using given stride visited node algorithm executed construct receptive ﬁeld exactly receptive ﬁelds created. stride determines distance relative selected node sequence consecutive nodes receptive ﬁeld created. number nodes smaller algorithm creates all-zero receptive ﬁelds padding purposes. nodes identiﬁed previous step receptive ﬁeld constructed. algorithm ﬁrst calls algorithm assembles local neighborhood input node. nodes neighborhood candidates receptive ﬁeld. algorithm lists neighborhood assembly steps. given inputs node size receptive ﬁeld procedure performs breadth-ﬁrst search exploring vertices increasing distance adds vertices number collected nodes smaller -neighborhood vertices recently added collected least vertices neighbors add. note time size possibly different receptive ﬁeld node constructed normalizing neighborhood assembled previous step. illustrated figure normalization imposes order nodes neighborhood graph unordered graph space vector space linear order. basic idea leverage graph labeling procedures assigns nodes different graphs similar relative position respective adjacency matrices structural roles within graphs similar. formalize intuition deﬁne optimal graph normalization problem aims labeling optimal relative given collection graphs. problem collection unlabeled graphs nodes injective graph labeling procedure distance measure graphs nodes distance measure matrices. find figure normalization performed graphs induced neighborhood root node graph labeling used rank nodes create normalized receptive ﬁelds size node attributes size edge attributes. normalization also includes cropping excess nodes padding dummy nodes. vertex attribute corresponds input channel respective receptive ﬁeld. algorithm receptivefield create receptive field input vertex graph labeling receptive ﬁeld size neighassemb gnorm normalizegraph return gnorm random expected difference distance graphs vector space distance graphs graph space minimized. optimal graph normalization problem generalization classical graph canonicalization problem. canonical labeling algorithm however optimal isomorphic graphs might perform poorly graphs similar isomorphic. contrast smaller expectation optimal normalization problem better labeling aligns nodes similar structural roles. note similarity determined proof reduction subgraph isomorphism. patchy-san solve optimization problem. instead compare different graph labeling methods choose performs best relative given collection graphs. collection graphs theorem sequence pairs graphs sampled independently uniformly random theorem enables compare different labeling procedures unsupervised manner comparison corresponding estimators. assumption smaller estimate smaller absolute difference. therefore simply choose labeling minimal. assumption holds instance edit distance graphs output receptive ﬁeld compute ranking using subject vertices according else dummy nodes else construct subgraph vertices canonicalize respecting prior coloring return graph normalization problem application appropriate graph labeling procedures normalization local graph structures core proposed approach. within patchy-san framework normalize neighborhood graphs vertex labeling vertices therefore constrained graph distance vertices closer always ranked higher deﬁnition ensures always rank closer vertex higher ranked vector space representation. since labeling methods injective necessary break ties same-label nodes. nauty nauty accepts prior node partitions input breaks remaining ties choosing lexicographically maximal adjacency matrix. known graph isomorphism ptime graphs bounded degree constant size neighborhood graphs algorithm runs time polynomial size original graph average time linear experiments verify computing canonical labeling graph neigborhoods adds negligible overhead. algorithm lists normalization procedure. size input larger ﬁrst applies ranking based select nodes recomputes ranking smaller nodes. size smaller adds disconnected dummy nodes. finally induces subgraph vertices canonicalizes graph taking ranking prior coloring. relate patchy-san cnns images follows. theorem given sequence pixels taken image. applying patchy-san receptive ﬁeld size stride zero padding normalization sequence identical ﬁrst layer receptive ﬁeld size stride zero padding. patchy-san able process vertex edge attributes number vertex attributes number edge attributes. input graph applies normalized receptive ﬁelds vertices edges results tensor. reshaped tensors. note number input channels. apply -dimensional convolutional layer stride receptive ﬁeld size ﬁrst second tensor. rest architecture chosen arbitrarily. merge layers combine convolutional layers representing nodes edges respectively. complexity implementation patchy-san’s algorithm creating receptive ﬁelds highly efﬁcient naively parallelizable ﬁelds generated independently. show following asymptotic worst-case result. theorem number graphs receptive ﬁeld size width complexity computing given labeling graph vertices edges. patchy-san worst-case complexity exp)) computing receptive ﬁelds graphs. proof node sequence selection requires labeling input graph retrieval highest ranked nodes. creation normalized graph patches computational effort spent applying labeling procedure neighborhood whose size larger maximum degree input graph neighborhood returned algorithm term comes worst-case complexity graph canonicalization algorithm nauty node graph experiments conduct three types experiments runtime analysis qualitative analysis learned features comparison graph kernels benchmark data sets. assess efﬁciency patchy-san applying real-world graphs. objective compare rates receptive ﬁelds generated rate state cnns perform learning. input graphs part collection python module graphtool. given graph used patchy-san compute receptive ﬁeld nodes using -dimensional weisfeiler-lehman algorithm normalization. torus periodic lattice nodes; random random undirected graph nodes degree distribution kmax power network representing topology power grid polbooks co-purchasing network books politics published presidential election; preferential preferential attachment network model newly added vertices degree astro-ph coauthorship network authors preprints posted astrophysics arxiv email-enron communication network generated half million sent emails experiments commodity hardware single cpu. figure visualization features learned -dimensional normalized receptive ﬁelds size torus preferential attachment graph co-purchasing network political books random graph instances graphs nodes depicted left. visual representation feature’s weights graphs sampled rbms setting hidden node corresponding feature zero. yellow nodes position adjacency matrices. figure depicts receptive ﬁelds second rates input graph. receptive ﬁeld size patchy-san creates ﬁelds rate except email-enron rate respectively. largest tested size ﬁelds created rate least convolutional dense layers learns rate training examples second machine. hence speed receptive ﬁelds generated sufﬁcient saturate downstream cnn. visualization experiments’ qualitatively investigate whether popular models restricted boltzman machine combined patchy-san unsupervised feature learning. every input graph generated receptive ﬁelds nodes used input rbm. hidden nodes trained epochs contrastive divergence learning rate visualize features learned single-layer -dimensional weisfeiler-lehman normalized receptive ﬁelds size note features learned correspond reoccurring receptive ﬁeld patterns. figure depicts features samples drawn four different graphs. tein d&d. mutag data nitro compounds classes indicate whether compound mutagenic effect bacterium. consists chemical compounds classes indicate carcinogenicity male female rats chemical compounds screened activity non-small cell lung cancer ovarian cancer cell lines proteins graph collection nodes secondary structure elements edges indicate neighborhood amino-acid sequence space. graphs classiﬁed enzyme non-enzyme. data protein structures classiﬁed enzymes non-enzymes. experimental set-up. compared patchy-san shortest-path kernel random walk kernel graphlet count kernel weisfeiler-lehman subtree kernel similar previous work height parameter size graphlets chose decay factor performed -fold cross-validation lib-svm using folds training testing repeated experiments times. report average prediction accuracies standard deviations. patchy-san used dimensional normalization width equal average number nodes receptive ﬁeld sizes experiments used node attributes. addition experiments combined receptive ﬁelds nodes edges using merge layer make fair comparison used single network architecture convolutional layers dense hidden layer softmax layer experiments. ﬁrst convolutional layer output channels second conv layer output channels stride ﬁeld size convolutional layers rectiﬁed linear units. dense layer rectiﬁed linear units dropout rate dropout relatively small number neurons needed avoid overﬁtting smaller data sets. hyperparameter optimized number epochs batch size mini-batch gradient decent algorithm rmsprop. implemented theano wrapper keras also applied logistic regression classiﬁer patches moreover experiments set-up larger social graph data sets compared patchy-san previously reported results graphlet count deep graphlet count kernel used normalized node degree attribute patchy-san highlighting advantages easily incorporate continuous features. results. table lists results experiments. omit results almost identical nci. despite using one-ﬁts-all architecture cnns accuracy highly competitive existing graph kernels. cases receptive ﬁeld size results best classiﬁcation accuracy. relatively high variance explained small size benchmark data sets fact cnns hyperparameters tuned individual data sets. similar experience image text data expect patchy-san perform even better large data sets. moreover patchy-san times efﬁcient efﬁcient graph kernel expect performance advantage much pronounced data sets large number graphs. results betweeness centrality normalization similar exception runtime increases logistic regression applied patchy-san’s receptive ﬁelds performs worse indicating patchy-san works especially well conjunction cnns learn non-linear feature combinations share weights across receptive ﬁelds. patchy-san also highly competitive social graph data. signiﬁcantly outperforms kernels four data sets achieves ties rest. table lists results experiments. conclusion future work proposed framework learning graph representations especially beneﬁcial conjunction cnns. combines complementary procedures selecting sequence nodes covers large parts graph generating local normalized neighborhood representations nodes sequence. experiments show approach competitive state graph kernels. directions future work include alternative neural network architectures rnns; combining different receptive ﬁeld sizes; pretraining rbms autoencoders; statistical relational models based ideas approach. acknowledgments many thanks anonymous icml reviewers provided tremendously helpful comments. research leading results received funding european union’s horizon innovation action program under grant agreement -types. atlas homma toshiteru marks robert artiﬁcial neural network spatio-temporal bipolar patterns application phoneme classiﬁcation. anderson d.z. neural information processing systems bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression proceedings python scientiﬁc compiler. computing conference berkholz christoph bonsma paul grohe martin. tight lower upper bounds complexity canonical colour reﬁnement. proceedings european symposium algorithms bruna joan zaremba wojciech szlam arthur lecun yann. spectral networks locally connected networks graphs. international conference learning representations debnath asim kumar compadre rosa lopez debnath gargi shusterman alan hansch corwin. structure-activity relationship mutagenic aromatic heteroaromatic nitro compounds. correlation molecular orbital energies hydrophobicity. med. chem. iparraguirre jorge bombarell rafael hirzel timothy aspuruguzik alan adams ryan convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems freund yoav haussler david. unsupervised learning distributions binary vectors using layer netadvances neural information processing works. systems fukushima kunihiko. neocognitron self-organizing neural network model mechanism pattern recognition unaffected shift position. biological cybernetics gaertner thomas flach peter wrobel stefan. graph kernels hardness results efﬁcient alternatives. proceedings annual conference computational learning theory kersting kristian ahmadi babak natarajan sriraam. counting belief propagation. proceedings twenty-fifth conference uncertainty artiﬁcial intelligence kersting kristian mladenov martin garnett roman grohe martin. power iterated color reﬁnement. proceedings twenty-eighth aaai conference artiﬁcial intelligence leskovec jure lang kevin dasgupta anirban mahoney michael community structure large networks natural cluster sizes absence large internet mathematics well-deﬁned clusters. milo shen-orr shai itzkovitz shalev kashtan nadav chklovskii dmitri alon uri. network motifs simple building blocks complex networks. science shervashidze nino vishwanathan s.v.n. petri tobias mehlhorn kurt borgwardt karsten efﬁcient graphlet kernels large graph comparison. proceedings international conference artiﬁcial intelligence statistics toivonen hannu srinivasan ashwin king ross kramer stefan helma christoph. statistical evaluation predictive toxicology challenge bioinformatics wale nikil karypis george. comparison descriptor spaces chemical compound retrieval classiﬁcation. proceedings international conference data mining wallach izhar dzamba michael heifets abraham. atomnet deep convolutional neural network bioactivity prediction structure-based drug discovery. corr abs/.", "year": 2016}