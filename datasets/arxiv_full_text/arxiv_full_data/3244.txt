{"title": "On Approximate Inference for Generalized Gaussian Process Models", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "A generalized Gaussian process model (GGPM) is a unifying framework that encompasses many existing Gaussian process (GP) models, such as GP regression, classification, and counting. In the GGPM framework, the observation likelihood of the GP model is itself parameterized using the exponential family distribution (EFD). In this paper, we consider efficient algorithms for approximate inference on GGPMs using the general form of the EFD. A particular GP model and its associated inference algorithms can then be formed by changing the parameters of the EFD, thus greatly simplifying its creation for task-specific output domains. We demonstrate the efficacy of this framework by creating several new GP models for regressing to non-negative reals and to real intervals. We also consider a closed-form Taylor approximation for efficient inference on GGPMs, and elaborate on its connections with other model-specific heuristic closed-form approximations. Finally, we present a comprehensive set of experiments to compare approximate inference algorithms on a wide variety of GGPMs.", "text": "generalized gaussian process model unifying framework encompasses many existing gaussian process models regression classiﬁcation counting. ggpm framework observation likelihood model parameterized using exponential family distribution paper consider efﬁcient algorithms approximate inference ggpms using general form efd. particular model associated inference algorithms formed changing parameters thus greatly simplifying creation task-speciﬁc output domains. demonstrate efﬁcacy framework creating several models regressing non-negative reals real intervals. also consider closed-form taylor approximation efﬁcient inference ggpms elaborate connections model-speciﬁc heuristic closed-form approximations. finally present comprehensive experiments compare approximate inference algorithms wide variety ggpms. keywords gaussian processes bayesian generalized linear models non-parametric regression exponential family approximate inference recent years gaussian processes non-parametric bayesian approach regression classiﬁcation gaining popularity machine learning computer vision. example recent work kapoor demonstrated promising results object classiﬁcation using classiﬁcation active learning. several properties desirable solving complex regression tasks found computer vision. first bayesian formulation learned robustly small training sets important tasks amount training data sparse compared dimension model second regression produces predictive distribution single predicted value thus providing probabilistic approach judging conﬁdence predictions e.g. active learning. third based kernel functions input examples allows diverse image representations incorporation prior knowledge computer vision task finally framework kernel hyperparameters learned maximizing marginal likelihood evidence training data. typically efﬁcient standard cross-validation allows expressive kernels e.g. compound kernels model different trends data multiple kernel learning features optimally combined weighting kernel function feature. advantages regression classiﬁcation applied many computer vision problems object classiﬁcation human action recognition estimation eye-gaze recognition tracking counting people crowd modeling anomaly detection stereo vision interpolation range data non-rigid shape recovery human pose recovery latent-space models human pose however despite successes many methods attempt shoe-horn computer vision task standard regression framework. particular standard regresses continuous real-valued function often used predict discrete non-negative integers non-negative real numbers real numbers ﬁxed interval hence heuristics often required convert real-valued prediction valid task-speciﬁc output optimal bayesian setting. example chan real-valued prediction must truncated rounded generate proper count prediction obvious predictive distribution real-values converted counts. developing model regression tasks requires ﬁrst ﬁnding suitable distribution output variable deriving approximate inference algorithm. task simpliﬁed considerably recourse unifying framework call generalized gaussian process model ggpm inspired generalized linear model aims consolidate parametric regression methods unifying framework. similarly ggpm uniﬁes many bayesian non-parametric regression methods using priors exponential family. ggpm observation likelihood output parameterized using generic form exponential family distribution. approximate inference algorithms derived depend parameter functions elucidating terms needed algorithm. different models created simply changing parameters likelihood function thus easing development models task-speciﬁc output domains. note analogous glms common iteratively reweighted least-squares algorithm derived estimate associated regression models. paper intended survey existing regression models well develop unifying regression framework models associated approximate inference algorithms. besides formalizing ggpm framework contributions paper -fold derive closed-form approximate inference method ggpms based taylor approximation show model-speciﬁc closed-form approximations kapoor chan vasconcelos heikkinen special cases; analyze existing approximate inference algorithms using generic form exponential family distribution; using ggpm framework propose several models regressing non-negative interval real outputs; conduct comprehensive experiments comparing efﬁcacy approximate inference algorithms synthetic real data sets. remainder paper organized follows. section ﬁrst review gaussian process regression related work. section introduce ggpm framework section discuss existing novel models within ggpm framework. section derive efﬁcient approximate inference algorithms. next compare approximate posteriors section discuss initialization strategies hyperparameter estimation section finally section present experiments compare approximate inference algorithms well demonstrate efﬁcacy proposed models. random process represents distribution functions completely speciﬁed mean covariance functions simplicity assume mean function zero. covariance function determines class functions represent given input vectors speciﬁes corresponding function values jointly gaussian covariance matrix entries function estimated training input vectors corresponding noisy observations first given inputs noisy outputs posterior distribution corresponding function values obtained bayes’ rule marginal likelihood measures data averaged probable functions. hence kernel hyperparameters selected probable latent function model data well. sigmoid function e.g. logistic probit functions maps real number range however since observation likelihood longer gaussian computing posterior predictive distributions longer analytically tractable. development several approximate inference algorithms markov-chain monte carlo variational bounds laplace approximation expectation propagation alternative approximate inference classiﬁcation task approximated regression problem observations extended several ways regression tasks univariate outputs. robust regression obtained replacing gaussian observation likelihood laplace cauchy likelihood student-t likelihood paciorek schervish uses binomial likelihood model event occurrence data. spatial statistics kriging method developed interpolating spatial data essentially uses model regression. diggle vanhatalo vehtari vanhatalo extend framework modeling counting observations assuming poisson observation likelihood spatial prior. similarly chan vasconcelos develops method bayesian poisson regression applying gaussian prior linear weights poisson regression. using log-gamma approximation kernelizing yields closed-form solution resembles speciﬁc output-dependent noise. finally savitsky savitsky vannucci presents bayesian formulation hazard model replacing linear covariate prior also studies counting models using poisson negative binomial likelihoods context bayesian variable selection. table summarizes previous work bayesian regression models using priors along approximate inference algorithms proposed them. goal paper generalize many models uniﬁed framework. probit likelihood multiclass classiﬁcation obtained using probit softmax sigmoid function. linearly mixes independent priors obtain semiparametric latent factor model. paper consider univariate outputs single prior; extending ggpm multivariate outputs topic future work. previous works ggpms include diggle focuses geostatistics using poissonbinomial-ggpms paciorek schervish uses binomial-ggpm context testing non-stationary covariance functions savitsky mainly interested variable selection adding priors kernel hyperparameters ggpm. works perform inference using mcmc plugging different likelihood functions without exploiting exponential family form. mcmc tends slow convergence problems observed diggle contrast paper focuses efﬁcient algorithms approximate inference derives general forms exploiting exponential family form. create plug-and-play aspect models exploit later create several novel models little extra work. seeger tresp choi also brieﬂy mention connection assumed output likelihoods gpr/gpc exponential family study approximate inference models depth. ggpm interpreted bayesian approach generalized linear models gaussian prior placed linear weights previous works bayesian glms consider different priors. typical approach form bayesian hierarchical applying conjugate prior parameters recent work focuses inducing sparsity latent function e.g. seeger nickisch seeger assume factorial heavy-tailed prior distribution kernelizable factorial assumption. hannah proposes mixture glms based dirichlet process allow different regression parameters different areas input space performs inference using mcmc. used non-linear kernel ggpm bayesian kernelized non-linear regression. zhang also proposes bayesian kernelized using hierarchical model sparse prior evaluates models classiﬁcation problems. finally cawley proposes non-bayesian version called generalised kernel machines based kernelizing iterated-reweighted least-squares algorithm ggpm bayesian formulation gkm. respect previous work paper presents algorithms approximate inference depth. also propose novel models regression non-negative reals real intervals show connections heuristic methods using taylor approximation. furthermore comprehensive experiments presented compare performance inference algorithms wide variety likelihood functions. ﬁrst note different models obtained changing form observation likelihood standard assumes gaussian observation likelihood essentially uses bernoulli distribution chan vasconcelos uses poisson likelihood counting. likelihood functions instances single-parameter exponential family distribution likelihood given observation possible values natural parameter exponential family distribution dispersion parameter. sufﬁcient statistic θ+c)dy log-partition function normalizes distribution. mean variance sufﬁcient statistic functions ﬁrst second derivatives w.r.t. exponential family generalizes wide variety distributions different output domains hence unifying framework created analyzing model likelihood takes generic form generalized gaussian process models consider framework generic bayesian model regresses inputs outputs encompasses many popular models. following formulation glms model composed three components prior knowledge functional relationship output mean latent function hand effect kernel function adaptively warp link function data. many trends represented kernel function important note functions cannot naturally represented kernel function positive-deﬁnite constraint. hence directly specifying link function necessary cases. ggpm uniﬁes many bayesian regression methods using priors model arising speciﬁc instantiation parameter functions given training examples novel input predictive distribution obtained marginalizing posterior latent function similar standard gpr/gpc dispersion treated hyperparameter estimated along kernel hyperparameters maximizing marginal likelihood. inference ggpms follows closely standard gpr/gpc given training examples input vectors corresponding observations goal generate predictive distribution output corresponding novel input predictive distribution obtained using steps. ﬁrst step calculate posterior distribution latent function values best explains training examples corresponds training parameter estimation phase regression model. second step distribution latent function value novel input calculated followed predictive distribution formally given training inputs latent values jointly gaussian according prior kernel matrix. posterior distribution obtained conditioning training outputs applying bayes’ rule posterior distribution latent values inputs best describe provided input/output pairs. next predictive distribution novel input obtained ﬁrst predicting latent function value conditioned inputs joint distribution latent function values also gaussian prior conditional distribution obtained using conditional gaussian theorem note non-gaussian likelihoods posterior predictive distributions cannot computed analytically closed-form. discuss efﬁcient approximate inference algorithms section note dependence hyperparameters. marginal likelihood measures data averaged probable latent functions. hence criteria selects kernel hyperparameters probable latent function model data well. section using ggpm framework propose several novel models regression different output domains. obtain bayesian regression non-negative real number outputs adopting gamma inverse-gaussian distributions propose beta-ggpm regresses real numbers interval. also consider existing models within ggpm framework also discuss role link function selection criteria. table presents examples efds table shows expectations link functions corresponding ggpm. table encompasses existing novel models. changing parameters form speciﬁc observation likelihood easily obtain wide range models different types outputs e.g. gamma inverse gaussian non-negative reals beta interval reals etc. binomial distribution models probability certain number events occurring independent trials event probability individual trial fraction events. assuming canonical link function hence mean related latent space logistic function. binomial-ggpm equivalent model using logistic function. model naturally accommodate uncertainty labels using fractional common interpretation probit class probability arises noisy heaviside step function here ggpm framework provides insight probit logistic models correspond bernoulli-ggpms using different link functions. canonical link function assumes mean exponential latent function. cause problems actual mean trend different illustrated figure count actually follows linear trend. address problem non-linear kernel function counteract exponential link function. case ideal kernel function logarithm function. however positive deﬁnite kernel hence changing kernel cannot recover linear trend exactly. furthermore using kernel poor extrapolation capabilities limited extent function illustrated figure figures illustrate difference standard linearized poisson ggpms. standard poisson-ggpm cannot correctly model linear trend resulting poor data extremes linearized poisson follows linear trend. figure examples count regression using ggpm linear kernel different likelihood functions poisson; linearized poisson; com-poisson; linearized compoisson. data follows linear trend underdispersed. shows learned latent function bottom shows predictive distributions. background color indicates count probability limitation poisson distribution models equidispersed random variable i.e. variance equal mean. however cases actual random variable overdispersed underdispersed next consider counting distributions handle overdispersion underdispersion. negative binomial distribution model counting data overdispersed mean given scale parameter adjusts variance. variance written function mean always exceeds mean negative binomial reduces poisson distribution. mean parameter dispersion parameter. compoisson smooth interpolation three distributions geometric poisson bernoulli distribution overdispersed underdispersed partition function closed-form expression estimated numerically precision note also function affects optimization dispersion canonical link function com-poisson ggpm includes dispersion hyperparameter decouples variance poisson mean thus allowing control observation noise output. figures show examples using com-poisson-ggpm underdispersed counting data linear trend. note variance prediction much lower com-poisson models poisson models thus illustrating com-poisson ggpm effectively estimate dispersion data. com-poisson proposed guikema goffelt thus com-poisson ggpm nonlinear bayesian extension using prior latent function. section consider ggpms non-negative real number outputs. based different parameterizations gamma distribution based inverse gaussian. main difference likelihood functions amount observation noise. particular variance gamma distribution whereas inverse gaussian dispersed variance gamma distribution model non-negative real number observations. distribution parameterized mean shape parameter exponential distribution special case gamma distribution ggpm distribution parameter hyperparameter. since must implicitly negative function −e−η link function alternatively gamma distribution parameterized shape parameter scale hyperparameter fixing scale hyperparameter gamma distribution reduces chi-square distribution. since must positive candidate link function inverse gaussian distribution another model non-negative real number outputs mean parameter inverse dispersion. since implicitly negative −e−η link function approximation digamma function beta-ggpm maps observations interval real values latent space using logit function. similar idea explored study estimating reﬂectance spectra values reﬂectance spectra output regressed using logit-transformed speciﬁcally output values ﬁrst transformed real values using logit function standard model estimated transformed output. show relationship logit-transformed beta-ggpm section previous examples several strategies used selecting link parameter functions obtain mapping latent space parameters. using canonical link function simpliﬁes calculations make undesired assumptions mapping latent space distribution mean modifying link function allows desired mean trend selecting mapping parameter function main hurdles select function satisﬁes implicit constraints parameter imposed log-partition function select function deﬁned values input accommodate prior. figure plots log-partition functions link functions model. example implicit negative constraint gamma-shape likelihood log-partition function. finally worth noting difference warped ggpms. warped learns function warp output gpr. warping function plays similar role link function ggpm notable difference. snelson warping applied directly output variable applied resulting result predictive distribution gaussian whereas arbitrary hand ggpm link function applies warping latent value mean parameter hence form output distribution preserved. addition warping function learned snelson whereas paper assume link function ﬁxed given ggpm. certainly principle link function could learned. however note link function kernel function control mapping latent space parameter space. hence link function equivalently inverse hyperbolic tangent function applied data scaled section derive approximate inference algorithms ggpms based general form exponential family distribution method approximate inference mcmc draw samples posterior computationally intensive instead consider methods approximate posterior gaussian. consider closed-form approximation inference based taylor approximation show taylor approximation justify common heuristics pre-processing steps principled inference; prove label regression taylor approximation classiﬁcation closed-form bayesian poisson regression taylor approximation poisson-ggpm regression logtransformed outputs actually taylor approximation gamma-ggpm regression logit-transformed outputs taylor approximation beta-ggpm. finally consider several approximate inference algorithms previously noted nickisch rasmussen inference approximations work ﬁnding gaussian approximation true posterior. similarly ggpms approximate inference also ﬁnds suitable gaussian approximation true posterior i.e. section present novel closed-form approximation inference based applying taylor approximation likelihood term ﬁrst deﬁne following derivative functions observation log-likelihood taylor approximation closed-form approximation interpreted performing targets target-speciﬁc non-i.i.d. observation noise targets function expansion point selected non-linear transformation observations interpreted gaussian approximation observation noise transformation space noise dependent expansion points. standard noise i.e. cases noise dependent expansion points particular properties observation likelihood. instances closed-form taylor approximation different models explored section advantage taylor approximation efﬁcient non-iterative method complexity standard gpr. section exploit efﬁciency speed approximate inference methods hyperparameter estimation. targets function expansion point chosen non-linear transformation observations assuming output occurs close mean distribution reasonable choice expansion point denote canonical expansion point. derivatives targets simpliﬁed formulation gives insight common preprocessing transformations used gpr. using taylor approximation show forms preprocessing actually making speciﬁc assumptions output noise. addition several heuristic methods shown instances approximate inference using taylor approximation. explored speciﬁc cases section finally worth noting relationship ggpm taylor approximation warped warped also apply warping apply main difference warped noise transformed space modeled i.i.d. gaussian resulting arbitrary predictive distribution hand taylor approximation models noise according warping function hence predictive distribution preserved. standard uses modiﬁed targets noise terms discussed earlier. additional penalty term dispersion arises non-gaussianity observation noise. derivatives using conjugate gradient derived appendix show heuristic methods using transformed outputs explained taylor approximate inference ggpms speciﬁc likelihoods. derivations including derivative functions targets used taylor approximate inference given appendix hence taylor approximation binomial-ggpm equivalent latent space binomial model targets scaled effective noise term target values equivalent label regression scale factor. hence label regression interpreted taylor approximation inference. scaling targets irrelevant latent space used classifying based sign however scaling important computing actual label probabilities using predictive distribution. interpretation explains label regression tends work well practice e.g. kapoor poisson based canonical expansion point constant prevent taking logarithm zero hence target effective noise taylor approximation taylor approximation exactly closed-form approximation proposed bayesian poisson regression chan vasconcelos derived different using approximation log-gamma distribution. note equivalent using standard outputs standard practice statistics literature observations positive values. hence practice applying log-transformed outputs equivalent assuming gamma likelihood using taylor approximate inference. hence logit-transformed outputs heikkinen equivalent using taylor approximate inference beta likelihood approximation trigamma function. alternatively another choice canonical expansion point used experiments. laplace approximation laplace approximation gaussian approximation posterior maximum hence laplace approximation speciﬁc case closed-form taylor approximation section expansion point maximum true posterior evaluated ˆwˆu iteration expansion point moved closer maximum target vector updated. note update form mean closed-form taylor approximation. hence taylor approximation could also considered one-iteration laplace approximation using expansion point initial point. last line follows ﬁrst-derivative condition maximum. note dependent kernel matrix hence iteration optimization recompute value. derivatives marginal presented supplemental notation indicates sites without approximation posterior distribution given observations except since terms gaussian integral computed closed-form. next site parameters selected match moments p)q¬i tiq¬i. requires ﬁrst calculating moments iterates site i.e. observation iteratively convergence. note general guaranteed converge. although usually well behaved data log-likelihood concave approximation initialized prior finally moments analytically tractable form predictive distribution) approximate integration usually required. general convergence also depends accuracy moment approximations. section discuss variational approximation maximizes lower-bound marginal likelihood minimizing divergence approximate posterior true posterior. type approximate inference ﬁrst applied robust regression manfred archambeau later classiﬁcation nickisch rasmussen paper extend ggpm. finally optimal approximate posterior obtained maximizing respect variational parameters using standard optimization techniques note lower bound marginal likelihood hence model hyperparameters also estimated maximizing practice model hyperparameters approximate posterior estimated time jointly maximizing respect parameters hence derivatives require expectations certain likelihood link functions expectations closed form solutions. cases need approximated. expectation ¯η−m√ hence alternatively expectations required gaussian. alternative forms allow intuitive comparison method laplace approximation given next section. interesting comparison made predictive latent distribution using laplace approximation particular laplace approximation latent mean depends ﬁrst derivative observation log-likelihood whereas variational method latent mean depends expectation ﬁrst derivative using similarly laplace approximation effective noise term inverse derivative observation log-likelihood term depends estimate derivative differencing derivative around given section studied closed-form taylor approximation discussed connections output-transformed gps. also discussed popular approximate inference methods context ggpms. using general form likelihood identify speciﬁc quantities required algorithm terms parameters summarized table example requires expectations approximate predictive distribution whereas minimization requires expectations gaussian distribution. result practical consequences implementation likelihood functions simpliﬁed since derivatives expectations simple functions need implemented; elucidate expectations require numerical approximation. furthermore different likelihood link functions combined novel ways without much additional implementation effort. ggpm implemented matlab extending gpml toolbox include implementations generic exponential family distribution using parameters closed-form taylor approximation inference; moments predictive distributions approximated using numerical integration necessary. empirically found sensitive accuracy approximate integrals exhibited convergence problems less accurate approximations used hyperparameters optimized maximizing marginal likelihood using existing scaled conjugate gradient method gpml. code made available. section provide theoretical experimental comparison approximate posteriors section particular show efﬁcacy approximate inference method inﬂuenced properties likelihood evaluation metrics datasets. ﬁrst compare latent posteriors taylor laplace approximations latent variable. consider example using gamma-shape likelihood corresponding true latent posterior figure ﬁrst derivative log-posterior ﬁrst term assumed convex monotonically decreasing second term linear function negative slope hence also convex monotonically decreasing. mean laplace approximation mode true posterior i.e. zero crossing marked star figure taylor approximation equivalent iteration newton’s method starting expansion point hence geometrically mean taylor approximation zero-crossing tangent line expansion point figure since convex monotonically decreasing zero-crossing point tangent line always less zero-crossing point therefore taylor mean always smaller laplace mean. monotonically decreasing convex posterior skewed right mean larger mode. since matches mean approximation mean true posterior mean must larger mode claim suggests posterior means follow particular order -dimensional posterior. general multi-dimensional case difﬁcult prove similar result since dimensions latent posterior correlated kernel matrix. nonetheless empirically show ordering holds multivariate posteriors average. synthetic experiment using gamma-shape likelihood dispersion parameter kernel function scale bandwidth given bandwidth dispersion pair different functions ﬁrst randomly sampled gamma-shape ggpm. function points used training multivariate means approximate posteriors calculated using taylor laplace respectively. differences means averaged trials plotted figure parameter settings ordering holds multivariate means average i.e. mean always less mean always less mean. note that dispersion level increases difference three means also increases. increasing figure comparing approximate posterior means taylor laplace methods gamma-shape gamma-scale likelihood functions. ﬁrst shows true latent posterior approximation second shows ﬁrst derivative log-posterior mean laplace approximation zero-crossing point mean taylor approximation zero-crossing tangent line expansion point figure difference approximate posterior means gamma-shape likelihood function. compares taylor laplace bottom compares laplace plots show difference w.r.t dispersion parameter kernel bandwidth dispersion level stretch observation log-likelihood term. scale derivative result zero-crossing point tangent line move zero-crossing similarly stretching observation log-likelihood also moves mean mode thus increasing difference kernel bandwidth increasing bandwidth also stretches prior term. scales derivatives result difference also increases similar way. hand bandwidth increases difference decreases. increasing bandwidth squashes prior making uniform. result prior less inﬂuence mean posterior compared likelihood term mean converges mean likelihood term. ordering posterior means also suggest predictive means follow similar order i.e. predictions using less predictions using always larger difﬁcult prove theoretically since variance posterior affects predictive mean observed empirically examples well experiments real data section demonstrated figure uses gammashape ggpm four inference methods. training samples distributed three distinct regions similar trends exponential function. four inference methods well capture exponential trend. however given input latent values exhibit ordering claim seen second figure addition predictive means also follow ordering. methods almost overlapped latent means. optimize divergence true posterior approximate difference optimizes optimizes unimodal distribution minimizing either correctly capture mean systematic ordering approximate posterior predictive means suggests differences prediction accuracy approximation methods inﬂuenced distribution test data. test data points below mean curve better accuracy systematically underpredicts. hand points above mean curve better accuracy. effect illustrated figure conﬁguration data predictive mean function passes middle points extremal points. thus smallest prediction error middle region largest error extremes. contrast lowest error points near ends largest error middle region. test data contains points middle figure taylor approximation lower average predictive error. contrast test data contains points extremes figure lower average error. figure examples comparing different inference methods. example shows learned gammash-ggpm regression models four different inference methods taylor laplace kld. middle shows difference predictive distributions bottom shows difference latent functions. quantify difference predictive accuracy test generated randomly sampling points neighborhood training point. inference method evaluated using measures mean absolute error measures goodness-of-ﬁt; mean negative predictive density evaluated test points measures well model predicts entire density evaluation results presented table ﬁrst example test points regions achieves lowest versus hand second example test data middle region accurate among four methods finally likelihood changed gamma-shape gamma-scale results four inference methods reversed examples since derivative log-likelihood gamma-scale concave. evaluate results taylor method always performs worse three methods. examples claim concluded performances different inference methods highly affected dataset distributions likelihood functions evaluation metrics. systematic ordering latent posterior means affect prediction accuracy test unbalanced skewed. real-life datasets often noisy high-dimensional unlikely single inference method dominate datasets metrics. estimation ggpm hyperparameters maximizing marginal likelihood often suffers problem multiple local optima. figure shows negative log-marginal likelihood function kernel width scale hyperparameters four inference methods rainfall dataset surface four likelihood functions least local optima laplace methods produce similar surfaces. common approach hyperparameter estimation initialize log-hyperparameters zero optimize using scaled conjugate gradient method however example figure using initialization strategy leads different hyperparameter estimates inference method. illustrated figure converge similar local optimum left whereas laplace converges different local minimum right. hence robust initialization method required better explore search space ensure good optimum found inference method. strategy optimization procedure many times using large random initializations. however inference algorithms e.g. computational burden large. figure contour plot negative marginal likelihood function kernel width scale hyperparameters four approximate inference methods. iterations scaled conjugate gradient minimization initialization plotted green. propose efﬁcient initialization strategy uses taylor inference quickly good initializations inference methods procedure illustrated figure taylor inference used optimize hyperparameters using random initializations resulting convergence several local optima different marginal likelihoods. unique local optima used initializations laplace methods results presented figures cases taylor-initialized laplace recover local optima randomly-initialized versions signiﬁcant reduction computational cost hyperparameter resulting largest marginal likelihood selected estimate. figure illustration using taylor inference initialize inference methods hyperparameter estimation. candidate local optima found using taylor method random initializations; comparison local optima laplace method initialized taylor random points. comparison local optima method initialized taylor random points. three dataset compare initialization methods using relative change ∆mae maet −maer maet maer maes using taylor-initialization random-initialization respectively. relative change calculated analogous way. cases relative changes small statistically signiﬁcant similar conclusion holds relative change nlp. taylor-initialization yields signiﬁcant reduction computational cost. example taylor-initialized times faster random-initialized furthermore help taylor initialization encounter convergence problems experiments demonstrate taylor-initialization speedup hyperparameter estimation inference methods maintaining quality fully random initialization. section present experiments using ggpms inference methods wide variety real-world datasets. section consider ﬁnite counting data binomial-ggpm. section experiment regression non-negative reals inference methods best hyperparameter selected largest marginal likelihood training set. results indicate performances different inference methods highly affected likelihoods evaluation metrics datesets. inference method even taylor method best performance. furthermore hypothesis testing results show difference statistically signiﬁcant; laplace approximation often perform comparably. section apply binomial-ggpm tokyo rainfall dataset. dataset records number occurrences rainfall tokyo every calendar rainfall occurrence given calendar follow binomial distribution. assign occurrence times outputs binomial model. input feature calendar kernel used. presented earlier figure shows negative marginal likelihood function kernel width scale figure shows results taylor-initialization yielded local minima correspond different interpretations data. best interpretations presented figures using four inference methods. figure uses larger kernel width resulting smoother function shows clear seasonal pattern tokyo described kitagawa winter rainy season late june mid-july stable summer late july aug. generally occasional typhoon sept. oct. would difﬁcult identify trends looking original data. finally figure depicts curves remaining http//www.stat.uni-muenchen.de/service/datenarchiv/tokio/tokio e.html since input feature calendar cyclic kernel wraps around used better model correlation days. paper adopt cyclic kernel follow settings reference methods. figure binomial-ggpm mean functions tokyo rainfall data comparisons methods binomial-ggpm mean function using large kernel width comparison biller fahrmeir binomial-ggpm using small kernel width comparison taylor inference biller comparison laplace inference fahrmeir three local minima kernel bandwidth either small large. laplace methods almost overlapped estimates local optima. taylor method also captures similar trends three methods. extension replaces linear function cubic spline. model parameter used control tradeoff data-ﬁt smoothness cubic function. fahrmeir estimated cross-validation resulting local minima larger yields relatively smoother curve close smoother estimate using binomial-ggpm curve quite similar rougher estimate binomial-ggpm using laplace inference. difference local optima attributed different smoothness levels. second model fully bayesian approach regression splines automatic knot placement. poisson distribution parameter placed number knots. experimental results show rainfall dataset sensitive choice smaller resulting function smooth increasing value allows ﬂexible function. estimate similar rougher curve estimated taylor value decreased curve becomes similar smoother binomial-ggpm estimate comparisons shapes resulted curves highly affected adoption optimal hyperparameters. negative marginal likelihood metric smoother curve favored taylor method rougher curve selected three inference methods. abalone predict abalone physical measurements; housing predict housing values suburbs boston; auto-mpg estimate city-cycle fuel consumption miles gallon; servo predict rise time servo-mechanism terms gain settings choices mechanical linkages. four datasets summarized table lists output range input dimension size training test sets experiments follow testing protocol. given number training samples randomly selected dataset remaining data used testing. process repeated times means standard deviations reported. test statistical signiﬁcance friedman test non-parametric test differences several related samples based ranking. candidate hyperparameters trial taylor-initialization procedure described section candidate experimental results presented table non-negative ggpms typically perform better standard gpr. expected section log-transformed performs similarly gammash-ggpm taylor inference. however small performance differences different marginal likelihoods used estimate hyperparameters; former based standard marginal latter based taylor marginal includes extra penalty term dispersion hyperparameter. learned warping functions four datasets also log-like hence also similar performance. inv.-gaussian-ggpm taylor inference also viewed using standard outputs observation noise output dependent beneﬁt property auto-mpg abalone datasets since inv.gaussian-ggpm taylor inference achieves smallest ﬁtting errors. servo housing dataset likelihood worse gammashlikelihood. next rank likelihood function based resulting performance either friedman test determine signiﬁcance. table shows average rankings values various likelihood combinations. looking best ranked likelihood function statistically signiﬁcant cases except auto-mpg gammashhas slightly better ranking inv.-gaussian difference marginally signiﬁcant since likelihood smallest average rankings three ggpms taken consideration dataset given. looking performance also signiﬁcant differences ranking likelihoods dataset. note though many cases difference likelihoods signiﬁcant mainly relative performance different inference methods changes likelihood functions e.g. servo taylor inference performs better compared inference methods using gammash ranking reversed using gammasc. indicates effects likelihood functions dataset dependent choice likelihood function important. table average rankings values different likelihood combinations using friedman test. grouping bolded rankings differ signiﬁcantly non-bold rankings other. table average rankings values different inference methods using friedman test. best performing likelihood function considered. grouping bolded rankings differ signiﬁcantly non-bold rankings other. next compare approximate inference methods best-performing likelihood functions dataset. table shows average rankings corresponding values using friedman test. looking ranking based table similar rankings dataset differences statistically signiﬁcant. similarly also similar rankings dataset except abalone. datasets best ranking result statistically signiﬁcant suggesting rank orderings consistent. finally statistically better ranking pooling datasets although difference large; outperforms two-thirds time. results similar looking rankings based table first looking rankings datasets before almost identical rankings better two-thirds time finally typically dominates ranking datasets. interestingly datasets signiﬁcant difference inference algorithms. words best inference algorithm change trail based particular training test set. summary performances inference methods highly affected likelihoods datasets evaluation metrics. given dataset choice likelihood large impact predictive density likelihood usually dominating less prediction error typically likelihood achieve error. given correct likelihood performance inference methods tends similar e.g. similar rankings evaluated similar ranking mae. however given wrong likelihood performance inference algorithms highly affected dataset evaluation metrics. experiment consider conversion device-dependent values deviceilluminantindependent reﬂectance spectra. heikkinen conversion cast regularized regression problem input color values output reﬂectance values sampled wavelengths. particular reﬂectance spectra ﬁrst scaled interval mapped real value inverse hyperbolic tangent function regularized regression framework applied. method heikkinen equivalent applying standard logit-transformed spectral values discussed section note hyperparameters ﬁxed heikkinen whereas using interpretation hyperparameters estimated automatically using maximum marginal likelihood. since regression output constrained unit interval consider beta-ggpm spectra reﬂectance regression perform experiments munsell dataset consisting rgb/spectral pairs. following protocol heikkinen used training testing results averaged trials. also used smaller training consisting original training set. hyperparameters learned using maximum marginal likelihood. experimental results presented table first standard performs worse beta-ggpm mismatch output domain actual outputs. effect pronounced less training data available; using smaller training average error drops around beta-ggpm versus next beta-ggpm taylor inference gauss-ggpm+arctanh almost values three error metrics consistent conclusion beta-ggpm taylor inference viewed using standard logit transformation outputs. finally three inference methods perform similarly beta-ggpm dataset terms average error. table shows friedman test results different inference combinations. full training similar average rankings ranked worse. however differences statistically signiﬁcant similar average error values. reduced training best ranking followed again rankings statistically signiﬁcant although marginally better perform counting experiments using ggpms poisson-based likelihoods. cases predictions based mode distribution ggpms rounded truncated mean gpr. ﬁrst experiment perform crowd counting using ucsd crowd counting dataset chan dataset contains -dimensional features extracted images corresponding number people image different directions goal predict number people using image features. right crowd contains people left crowd dataset consists feature/count pairs direction following chan training testing. predict using poisson com-poisson ggpms exponential mean mapping well versions using linear mean mapping section compound linear plus kernel used models. crowd counting results presented table right crowd poissoncom-poisson-ggpms perform better using exponential mapping versus linear mapping. large number people right crowd leads non-linear trend feature space. contrast linearized link functions perform better left crowd indicating linear trend data looking likelihood functions poisson likelihood higher accuracy right crowd whereas com-poisson better left crowd. main difference com-poisson provides ﬂexibility control variance observation noise helps left crowd. hypothesis testing randomly selected small training sets consisting feature/count pairs original training dataset. learned ggpms evaluated original test dataset experimental results presented table table shows average rankings values using friedman test different likelihoods combinations. poissoncom-poisson-ggpms perform signiﬁcantly better corresponding linearized models right crowd vice versa linearized link functions perform better left crowd. consistent observations experiments original training test datasets. table average rankings values different likelihood combinations using friedman test. bolded rankings differ signiﬁcantly non-bold rankings l-com linear com-poisson) second experiment ggpm used estimation face images. fgnet dataset consists face images people different ages input vector ggpm facial features extracted using active appearance models output face. used leave-oneperson-out testing zhang yeung evaluate performance difference ggpms. fold images person used testing people used training dataset. experiment results estimation presented table linear poisson-ggpm lowest versus standard gpr. table shows friedman test results different likelihood combinations results indicate linearized poisson ggpm signiﬁcantly better ranking ggpms. table shows friedman test different inference combinations. although small differences inference algorithms likelihood rankings similar indicates inference method dominates terms mae. general laplace perform similarly statistically signiﬁcant difference them. moreover taylor approximation outperform methods ggpms e.g. neg. binomial-ggpm difference statistically signiﬁcant. finally figure presents example prediction test person. method warped poisson ggpm poisson ggpm poisson ggpm linearized poisson ggpm linearized poisson ggpm linearized poisson ggpm neg. binomial ggpm neg. binomial ggpm neg. binomial ggpm experiments considered variety likelihood functions inference methods datasets. general choice likelihood function important choice approximate inference algorithm. using likelihood function matches output domain observation noise typically lead better performance standard link function also affect ﬁnal result selection link function dataset dependent similar choice kernel function. looking inference methods typically similar performance laplace inference also achieves comparable results. taylor inference sometimes yields smallest maes often larger three inference. suggests accurate estimation terms always lead smaller ﬁtting error terms mae. performances different inference methods also highly affected distribution training test data systematic ordering observed posterior means finally many datasets dominant approximate inference algorithm yielding mixed results average rankings differences statistically signiﬁcant. paper studied approximate inference generalized gaussian process models. ggpm unifying framework existing models observation likelihood model parameterized using exponential family distribution allows approximate inference algorithms derived using general form efd. particular model formed setting parameters instantiates particular observation likelihood function output domain. addition observation likelihood ggpm also link function controls mapping latent variable mean output distribution. appropriately setting link function mean trends learned would otherwise possible standard positive-deﬁnite kernel functions. also study approximate inference method based taylor approximation non-iterative computationally efﬁcient. taylor approximation justify many common heuristics modeling principled inference particular likelihood function. also present approximate inference ggpms using laplace approximation expectation propagation divergence minimization. furthermore demonstrate posterior means taylor laplace approximations usually speciﬁc ordering result particular method approximation. consequence since posterior means biased prediction error particular inference algorithm heavily depends distributions training test data. finally perform hyperparameter estimation using taylor approximation initialize less-efﬁcient approximate inference methods. initialization procedure greatly increase speed learning phase signiﬁcantly affecting quality hyperparameters. addition notice convergence issues using initialization procedure. conduct comprehensive experiments using variety likelihood functions approximate inference methods datasets. experiments found selection correct likelihood function larger impact prediction accuracy approximate inference method. indeed many cases dominant inference method differences average ranking statistically signiﬁcant. whereas appropriate choice likelihood function link function improved accuracy signiﬁcantly. finally paper considered univariate observations ggpm. future work multivariate exponential family distribution form multivariate ggpm. model would encompass existing multivariate ordinal regression multi-class classiﬁcation semiparametric latent factor models authors thank rasmussen williams gpml code work supported city university hong kong research grants council hong kong special administrative region china", "year": 2013}