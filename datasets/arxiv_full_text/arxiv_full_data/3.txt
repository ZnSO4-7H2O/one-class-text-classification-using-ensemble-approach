{"title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue  Response Generation", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML", "I.5.1; I.2.7"], "abstract": "We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.", "text": "introduce multiresolution recurrent neural network extends sequence-to-sequence framework model natural language generation parallel discrete stochastic processes sequence high-level coarse tokens sequence natural language tokens. many ways estimate learn high-level coarse tokens argue simple extraction procedure sufﬁcient capture wealth high-level discourse semantics. procedure allows training multiresolution recurrent neural network maximizing exact joint log-likelihood sequences. contrast standard loglikelihood objective w.r.t. natural language tokens optimizing joint log-likelihood biases model towards modeling high-level abstractions. apply proposed model task dialogue response generation challenging domains ubuntu technical support domain twitter conversations. ubuntu model outperforms competing approaches substantial margin achieving state-of-the-art results according automatic evaluation metrics human evaluation study. twitter model appears generate relevant on-topic responses according automatic evaluation metrics. finally experiments demonstrate proposed model adept overcoming sparsity natural language better able capture long-term structure. work carried ﬁrst author research. email {iulian.vlad.serbanyoshua.bengioaaron.courville}umontreal.ca email {tklingergtesaurokrtalamadzhou}us.ibm.com cifar senior fellow recurrent neural networks gaining popularity machine learning community impressive performance tasks machine translation speech recognition results spurred cascade novel neural network architectures including attention memory pointer-based mechanisms majority previous work focused developing neural network architectures within deterministic sequence-to-sequence framework. words focused changing parametrization deterministic function mapping input sequences output sequences trained maximizing log-likelihood observed output sequence. instead pursue complimentary research direction aimed generalizing sequence-to-sequence framework multiple input output sequences sequence exhibits stochastic process. propose class models called multiresolution recurrent neural networks model multiple parallel sequences factorizing joint probability sequences. particular impose hierarchical structure sequences information high-level sequences ﬂows low-level sequences architecture exhibits objective function training joint log-likelihood observed parallel sequences biases model towards modeling high-level abstractions. test time model generates ﬁrst high-level sequence afterwards natural language sequence. hierarchical generation process enables model complex output sequences long-term dependencies. researchers recently observed critical problems applying end-to-end neural network architectures dialogue response generation neural networks unable generate meaningful responses taking dialogue context account indicates models failed learn useful high-level abstractions dialogue. motivated shortcomings apply proposed model task dialogue response generation challenging domains goal-oriented ubuntu technical support domain non-goal-oriented twitter conversations. domains model outperforms competing approaches. particular ubuntu model outperforms competing approaches substantial margin according human evaluation study automatic evaluation metrics achieving state-of-the-art result. start introducing well-established recurrent neural network language model rnnlm variants applied diverse sequential tasks including dialogue modeling speech synthesis handwriting generation music composition sequence discrete variables called tokens vocabulary rnnlm probabilistic generative model parameters decomposes probability tokens hidden state update function assume either lstm gating unit gating unit throughout rest paper. lstm gating unit consider hidden state lstm cell cell input hidden states concatenated. matrix rdh×|v input word embedding matrix column contains embedding word index word embedding dimensionality. similarly matrix rdh×|v output word embedding matrix. according model probability observing token position increases context vector high dot-product word embedding work builds upon sordoni proposed hierarchical recurrent encoder-decoder model model exploits hierarchical structure queries order model user search session hierarchical sequences sequence queries sequence words query. serban continue direction proposing exploit temporal structure inherent natural language dialogue. model decomposes dialogue hierarchical sequence sequence utterances sequence words. speciﬁcally model consists three modules encoder context decoder rnn. sequence tokens encoded real-valued vector encoder rnn. turn given input context updates internal hidden state reﬂect information point time. produces real-valued output vector decoder conditions generate next sequence tokens space limitations refer reader additional information model architecture. hred model modeling structured discrete sequences appealing three reasons. first naturally captures hierarchical structure want model data. second context acts like memory module remember things longer time scales. third structure makes objective function stable w.r.t. model parameters helps propagate training signal ﬁrst-order optimization methods consider problem generatively modeling multiple parallel sequences. sequence hierarchical level corresponding utterances bottom level tokens. formally ﬁrst sequence length n’th constituent sequence consisting discrete tokens vocabulary similarly second sequence also length n’th constituent sequence consisting discrete tokens vocabulary experiments sequence consist words dialogue utterance sequence contain coarse tokens w.r.t. utterance build probabilistic generative model tokens constituent sequences parameters generative model. assume independent conditioned factor probability sequences refer distribution coarse sub-model distribution natural language sub-model. coarse sub-model parametrize conditional distribution hred model described subsection applied sequences natural language sub-model parametrize hred model applied sequences difference. coarse prediction encoder gru-gated encodes previously generated tokens real-valued vector concatenated context figure computational graph multiresolution recurrent neural network lower part models stochastic process coarse tokens upper part models stochastic process natural language tokens. rounded boxes represent real-valued vectors variables represent coarse tokens natural language tokens respectively. given input natural language decoder rnn. coarse prediction encoder important encodes high-level information transmitted natural language sub-model. unlike encoder coarse-level sub-model encoding used generate natural language therefore uses different word embedding parameters. generation time coarse sub-model generates coarse sequence corresponds high-level decision natural language sequence contain conditioned coarse sequence coarse prediction encoder natural language sub-model generates natural language sequence model illustrated figure assume observed optimize parameters w.r.t. joint log-likelihood sequences. test time generate response sequence exploit probabilistic factorization approximate maximum posteriori estimate consider task natural language response generation dialogue. dialogue systems developed applications ranging technical support language learning entertainment dialogue systems categorized different types goal-driven dialogue systems non-goal-driven dialogue systems demonstrate versatility mrrnn apply goal-driven non-goal-driven dialogue tasks. focus task conditional response generation. given dialogue context consisting utterances model must generate next response dialogue. ubuntu dialogue corpus goal-driven dialogue task consider technical support ubuntu operating system ubuntu dialogue corpus corpus consists million natural language dialogues extracted ubuntu internet relayed chat channel. users entering chat channel usually speciﬁc technical problem. users ﬁrst describe problem afterwards users help resolve technical problems range software-related issues hardware-related issues informational needs additional details given appendix twitter dialogue corpus next task consider non-goal-driven task generating responses twitter conversations. twitter dialogue corpus extracted ﬁrst half using procedure similar ritter unlike ubuntu domain twitter conversations often noisy necessarily center around single topic. perform minimal preprocessing dataset remove irregular punctuation marks afterwards tokenize dataset split training validation test sets containing respectively dialogues. noun representation procedure aims exploit basic high-level structure natural language discourse.it based hypothesis dialogues topic-driven topics characterized nouns. addition tokenizer used hred rnnlm model requires part-of-speech tagger identify nouns dialogue. procedure uses predeﬁned stop words ubuntu twitter respectively. maps natural language utterance coarse representation extracting nouns using tagger removing stop words repeated words dialogue utterances without nouns assigned \"no_nouns\" token. procedure also extracts tense utterance adds beginning coarse representation. activity-entity representation procedure speciﬁc ubuntu technical support task aims exploit domain knowledge related technical problem solving. motivated observation dialogues centered around activities entities. example common users state speciﬁc problem want resolve e.g. install program driver doesn’t work response questions users often respond speciﬁc instructions e.g. website download software execute command cases clear principal information resides technical entities verbs therefore advantageous explicitly model structure. motivated observation procedure uses activities created manual inspection technical entities frequent terminal commands extracted automatically available package managers web. procedure uses tagger extract verbs natural language utterance. maps twitter’s terms service allowed redistribute twitter content. therefore tweet made public. available www.iulianserban.com/files/twitterdialoguecorpus. zip. natural language coarse representation keeping verbs activity well entities technical entity activity found utterance representation assigned \"none_activity\" token. procedure also appends binary variable coarse representation indicating terminal command detected utterance. finally procedure extracts tense utterance adds beginning coarse representation. extraction procedures applied utterance level therefore exists one-to-one alignment coarse sequences natural language sequences also exists one-to-many alignment coarse sequence tokens corresponding natural language tokens exception special tokens. details given appendix models implemented theano optimize models based training joint log-likelihood coarse sequences natural language sequences using ﬁrst-order stochastic gradient optimization method adam train models using early stopping patience joint-log-likelihood choose hyperparameters based joint log-likelihood validation set. deﬁne frequent words vocabulary word embedding dimensionality size models exception rnnlm hred twitter embedding dimensionality size apply gradient clipping stop parameters exploding test time beam search size generating model responses. details given appendix compare models several baselines used previously literature. ﬁrst standard rnnlm lstm gating function test time similar seqseq lstm model second baseline hred model lstm gating function decoder gating function encoder context proposed dialogue response generation serban source code baseline models made publicly available upon acceptance publication. ubuntu twitter specify rnnlm model hidden units lstm gating function. ubuntu specify hred model hidden units respectively encoder context decoder rnn. twitter specify hred model hidden units respectively encoder context decoder rnn. third baseline latent variable latent variable hierarchical recurrent encoder-decoder proposed serban exact vhred models serban ubuntu introduce fourth baseline called hred activity-entity features access past activity-entity pairs. model similar natural language sub-model mrrnn model difference natural language decoder conditioned real-valued vector produced encoding past coarse-level activity-entity sub-sequences. baseline helps differentiate model observes coarse-level sequences additional features model explicitly models stochastic process coarse-level sequences. specify model hidden units respectively encoder context decoder rnn. specify encoding past coarse-level activity-entity sub-sequences hidden units. coarse sub-model parametrized bidirectional-hred model hidden units respectively coarse-level encoder context decoder rnns. natural language sub-model parametrized conditional hred model hidden units respectively natural language encoder context decoder rnns. coarse prediction encoder parametrized hidden units. table ubuntu evaluation using precision recall accuracy metrics w.r.t. activity entity tense command ground truth utterances human ﬂuency relevancy scores given scale evaluation methods long known accurate evaluation dialogue system responses difﬁcult recently shown automatic evaluation metrics adapted evaluation including word overlap-based metrics bleu meteor either correlation human judgment system performance. therefore carry in-lab human study evaluate ubuntu models. recruit human evaluators show dialogue contexts ground truth response candidate responses context example compare candidate responses ground truth response dialogue context rate ﬂuency relevancy scale setup similar evaluation setup used koehn monz comparable details given appendix propose metrics evaluating model responses ubuntu compare activities entities model generated response ground truth response. ground truth model responses mapped respective activity-entity representations using automatic procedure discussed section overlap activities entities measured according precision recall f-score. based careful manual inspection extracted activities entities believe metrics particularly suited goal-oriented ubuntu dialogue corpus. activities entities reﬂect principal instructions given responses resolving technical problems. therefore model able generate responses actions entities similar ground truth human responses often lead solving users problem likely yield successful dialogue system. reader encouraged verify details completeness activity-entity representations appendix scripts generate noun activity-entity representations evaluate dialogue responses w.r.t. activity-entity pairs available online. results results ubuntu given table mrrnns clearly perform substantially better baseline models w.r.t. human evaluation automatic evaluation metrics. mrrnn noun representations achieves higher scores w.r.t. entities compared models human evaluators also rate ﬂuency relevancy substantially higher models. mrrnn activity representations achieves higher scores w.r.t. activities compared models nearly higher scores w.r.t. entities compared baselines. human evaluators also rate ﬂuency substantially higher baseline models. howeverits relevancy rated slightly higher compared hred model believe caused human evaluators likely noticing software entities actions dialogue responses overall results demonstrate mrrnns learned model high-level goal-oriented sequential structure ubuntu. setting rules iptables command writes changes etciptables. rules backup messing anything sudo iptables-save something backup rules restore sudo iptables-restore something community.. difference /.bashrc /.bashrc. local they’re different ﬁles default /.bashrc sources /.bashrc. local sorry could undersatnd... write terminal gedit /.bashrc opens open /.bashrc. local gedit /.bashrc. local \"... open blank nothing inside response mrrnn act. -ent. using xchat right mrrnn noun xchat-gnome vhred correct hred mrrnn act. -ent. don’t reason need iptables mrrnn noun using ubuntu vhred hred thanks mrrnn act. -ent. different mirror mrrnn noun something vhred dont know hred mrrnn act. -ent. open gedit /.bashrc called something mrrnn noun empty.. vhred it’s /.bashrc /.bashrc hred trying model responses shown table general mrrnn responses coherent topic-oriented compared model responses usually produce generic responses particular mrrnn activity-entity representation appears give goal-oriented instructions compared mrrnn noun representation additional examples shown appendix evaluation methods twitter similar ubuntu metrics precision recall metrics model responses ground truth responses w.r.t. noun representation. reason propose metrics similar reason given ubuntu metrics related entities good model response includes nouns ground truth response. also compute tense accuracy ubuntu. furthermore three embedding-based textual similarity metrics proposed embedding average embedding extrema embedding greedy three metrics based computing textual similarity ground truth response model response using word embeddings. three metrics measure topic similarity model-generated response topic ground truth response metrics yield high score. highly desirable property dialogue systems open platform twitter however also substantially different measuring overall dialogue system performance appropriateness single response would require human evaluation. results results twitter given table responses mrrnn noun representation better models precision recall w.r.t nouns. mrrnn also better models w.r.t. tense accuracy vhred embeddingbased metrics. accordance previous results indicates model learned generate on-topic responses thus explicitly modeling stochastic process nouns helps learn high-level structure. conﬁrmed qualitative inspection generated responses clearly topic-oriented. table appendix. closely related work model proposed jointly models natural language text high-level discourse phenomena. however models discrete class sentence high level must manually annotated humans. hand mrrnn models sequence automatically extracted high-level tokens. recurrent neural network models table twitter evaluation using precision recall accuracy metrics w.r.t. noun representation tense accuracy embedding-based evaluation metrics ground truth utterances. stochastic latent variables variational recurrent neural networks chung also closely related work. models face difﬁcult task learning high-level representations simultaneously learning model generative process high-level sequences low-level sequences difﬁcult optimization problem. addition this models assume high-level latent variables continuous usually gaussian distributions. recent dialogue-speciﬁc neural network architectures model proposed also relevant work. different mrrnn require domain-speciﬁc hand-crafted high-level representations human-labelled examples usually consist several sub-components trained different objective function. proposed multiresolution recurrent neural network generatively modeling sequential data multiple levels abstraction. trained optimizing joint log-likelihood sequences level. apply mrrnn dialog response generation different tasks ubuntu technical support twitter conversations evaluate human evaluation study automatic evaluation metrics. ubuntu mrrnn demonstrates dramatic improvements compared competing models. twitter mrrnn appears generate relevant on-topic responses. even though abstract information implicitly present natural language dialogues explicitly representing information different levels abstraction jointly optimizing generation process across abstraction levels mrrnn able generate ﬂuent relevant goal-oriented responses. results suggest ﬁne-grained abstraction provides architecture increased ﬂuency predicting natural utterances coarse-grained abstraction gives semantic structure necessary generate coherent relevant utterances. results also imply simply matter adding additional features prediction mrrnn outperforms competitive baseline augmented coarse-grained abstraction sequences features rather combination representation generation multiple levels yields improvements. finally observe architecture provides general framework modeling discrete sequences long coarse abstraction available. therefore conjecture architecture successfully applied broader natural language generation tasks generating prose persuasive argumentation tasks involving discrete sequences music composition. leave future work. authors thank ryan lowe michael noseworthy caglar gulcehre sungjin harm vries song feng ching participating helping human study. authors thank orhan firat caglar gulcehre constructive feedback thank ryan lowe nissan joelle pineau making ubuntu dialogue corpus available public. graves generating sequences recurrent neural networks. arxiv.. graves wayne danihelka neural turing machines. arxiv.. hinton deep neural networks acoustic modeling speech recognition shared c.-w. lowe serban noseworthy charlin pineau evaluate dialogue system empirical study unsupervised evaluation metrics dialogue response generation. arxiv.. serban sordoni lowe charlin pineau courville bengio hierarchical latent variable encoder-decoder model generating dialogues. arxiv preprint arxiv.. twitter preprocess dataset using moses tokenizer extracted june https//github.com/ moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl. noun-based procedure extracting coarse tokens aims exploit high-level structure natural language discourse. speciﬁcally builds hypothesis dialogues general topic-driven topics characterized nouns inside dialogues. point time dialogue centered around several topics. dialogue progresses underlying topic evolves well. addition tokenizer required previous extraction procedure procedure also requires part-of-speech tagger identify nouns dialogue suitable language domain. extracting noun-based coarse tokens deﬁne stop words twitter stop words ubuntu containing mainly english pronouns punctuation marks prepositions extract coarse tokens applying following procedure dialogue apply tagger version developed owoputi colleagues extract pos. twitter parser trained twitter corpus developed ritter ubuntu parser trained chat corpus developed forsyth martellwhich extracted chat channels similar ubuntu dialogue corpus. \"no_nouns\" token utterances contain nouns. ensures coarse sequences empty. also forces coarse sub-model explicitly generate least token even actual nouns generate. utterance tags detect three types time tenses past present future tenses. append token indicating tenses present beginning utterance. tenses detected append token \"no_tenses\". before exists one-to-many alignment extracted coarse sequence tokens natural language tokens since procedure also maintains ordering special placeholder tokens exception \"no_nouns\" token. cut-off vocabulary coarse tokens twitter ubuntu datasets excluding special placeholder tokens. average twitter dialogue training contains coarse tokens ubuntu dialogue training contains coarse tokens. twitter’s terms conditions unfortunately allowed publish preprocessed dataset. www.cs.cmu.edu/~ark/tweetnlp/ input tagger replace unknown tokens word \"something\" remove special placeholder tokens reduce consecutive sequence spaces single space. ubuntu also replace commands entities word \"something\". twitter also replace numbers word \"some\" urls word \"somewhere\" heart emoticons word \"love\". deﬁne nouns words tags containing preﬁx \"nn\" according ptb-style tagset. note utterance contain several sentences. therefore often happens utterance contains model statistics unigram bigram language models presented table noun representations ubuntu twitter training sets. table shows substantial difference bits words unigram bigram models suggests nouns signiﬁcantly correlated other. activity-entity-based procedure extracting coarse tokens attempts exploit domain speciﬁc knowledge ubuntu dialogue corpus particular relation providing technical assistance problem solving. manual inspection corpus shows many dialogues centered around activities. example common users state speciﬁc problem want resolve e.g. install program driver doesn’t work it?. response queries users often respond speciﬁc instructions e.g. website download software execute command addition technical entities principle message conveyed utterance resides verbs e.g. install work download execute. therefore seems clear dialogue system must strong understanding activities technical entities effectively assist users technical problem solving. seems likely would require dialogue system able relate technical entities other e.g. understand ﬁrefox depends library conform temporal structure activities e.g. understanding install activity often followed download activity. therefore construct word lists activities technical entities. construct activity list based manual inspection yielding list verbs. activity develop list synonyms conjugations tenses words. also wordvec word embeddings trained ubuntu dialogue corpous training identify commonly misspelled variants activity. result dictionary maps verb corresponding activity constructing technical entity list scrape publicly available resources including ubuntu linux-related websites well debian package manager apt. similar activities also wordvec word embeddings identify misspelled paraphrased entities. results another dictionary maps words corresponding technical entity. total technical entities. addition also compile list frequent commands. examples extracted activities entities commands found appendix. apply technical entity dictionary extract technical entities. apply tagger version developed owoputi colleagues trained chat corpus developed forsyth martell before. input tagger technical entities token \"something\". transformation improve tagging accuracy since corpus parser trained contain technical words. given tags extract verbs correspond activities.. verbs entire utterance tagger identiﬁed ﬁrst word noun assume ﬁrst word fact verb. this parser work well tagging technical instructions imperative form e.g. upgrade ﬁrefox. activities detected append token \"none_activity\" coarse sequence. also keep urls paths. remove repeated activities technical entities maintaining order tokens. command found inside utterance append \"cmd\" token utterance. otherwise append \"no_cmd\" token utterance. enables coarse sub-model predict whether utterance contains executable commands. manual inspection extracted coarse sequences show technical entities identiﬁed high accuracy activities capture main intended action majority utterances. high quality extracted activities entities conﬁdent used evaluation purposes well. another anybody anyone anything either everybody everyone everything many mine much neither nobody none nothing another others several somebody someone something whatever whichever whoever whomever whose another anybody anyone anything either everybody everyone everything many mine much neither nobody none nothing another others several somebody someone something whatever whichever whoever whomever whose able abst accordance according accordingly across actually added adopted affected affecting affects afterwards almost alone along already also although always among amongst announce another anybody anyhow anymore anyone anything anyway anyways anywhere apparently approximately arent arise around aside asking auth available away awfully back became become becomes becoming beforehand begin beginning beginnings begins behind believe beside besides beyond biol brief brieﬂy came cannot can’t cant cause causes certain certainly come comes contain containing contains could couldnt date didn’t different doesn’t done don’t dont downwards effect eight eighty either else elsewhere ending enough especially et-al even ever every everybody everyone everything everywhere except ﬁfth ﬁrst followed following follows former formerly forth found four furthermore game gave gets getting give given gives giving goes going gone gonna good gotten great happens hardly hasn’t haven’t hence hereafter hereby herein heres hereupon hither home howbeit however hundred i’ll immediate immediately importance important indeed index information instead invention inward isn’t it’ll i’ve keep keeps kept keys know known knows largely last lately later latter latterly least less lest lets like liked likely line little look looking looks made mate mainly make makes many maybe mean means meantime meanwhile merely might million miss moreover mostly much must name namely near nearly necessarily necessary need needs neither never nevertheless next nine ninety nobody none nonetheless noone normally noted nothing nowhere obtain obtained obviously often okay omitted ones onto others otherwise ought outside overall owing page pages part particular particularly past people perhaps placed please plus poorly possible possibly potentially predominantly present previously primarily probably promptly proud provides quickly quite rather readily really recent recently refs regarding regardless regards related relatively research respectively resulted resulting results right said saying says section seeing seem seemed seeming seems seen self selves sent seven several shall shed she’ll shes shouldn’t show showed shown showns shows signiﬁcant signiﬁcantly similar similarly since slightly somebody somehow someone somethan something sometime sometimes somewhat somewhere soon sorry speciﬁcally speciﬁed specify specifying state states still stop strongly substantially successfully sufﬁciently suggest sure take taken taking tell tends thank thanks thanx that’ll thats that’ve thence thereafter thereby thered therefore therein there’ll thereof therere theres thereto thereupon there’ve theyd they’ll theyre they’ve thing things think thou though thoughh thousand throug throughout thru thus time together took toward towards tried tries truly trying tweet twice unfortunately unless unlike unlikely unto upon used useful usefully usefulness uses using usually value various vols wanna want wants wasn’t welcome well we’ll went weren’t we’ve whatever what’ll whats whence whenever whereafter whereas whereby wherein wheres whereupon wherever whether whim whither whod whoever whole who’ll whomever whos whose widely willing wish within without won’t words world would wouldn’t yeah youd you’ll youre you’ve zero accept activate appoint attach backup boot check choose clean click comment compare compile compress change afﬁrm connect continue administrate copies break create debug decipher decompress deﬁne describe debind deattach deactivate download adapt eject email conceal consider execute close expand expect export discover correct fold freeze deliver grab hash import include install interrupt load block log-in log-out demote build clock bind more mount move navigate open arrange partition paste patch plan plug post practice produce pull purge push queries quote look reattach reboot receive reject release remake delete name replace request reset resize restart retry return revert reroute scroll send display shutdown size sleep sort split come-up store signup get-ahold-of test transfer uncomment de-expand uninstall unmount unplug unset sign-out update upgrade upload delay enter support prevent loose point contain access share sell help work mute restrict play call thank burn advice force repeat stream respond browse scan restore design refresh bundle implement programming compute touch overheat cause affect swap format rescue zoomed detect dump simulate checkout unblock document troubleshoot convert allocate minimize maximize redirect maintain print spam throw sync contact destroy ubuntu_. dmraid vncserver tasksel aegis mirage system-conﬁg-audit uifiso aumix unrar dell hibernate ucoded ﬁnger zoneminder ucfg macaddress ia-libs synergy aircrack-ng pulseaudio gnome bittorrent systemsettings cups ﬁnger xchm uwidget vnc-java linux-source ucommand.com epiphany avanade onboard uextended substance pmount lilypond proftpd unii jockey-common units xrdp mpcheck cruft uemulator ulivecd amsn ubuntu_. acpidump uadd-on gpac ifenslave pidgin soundconverter kdelibs-bin esmtp travel smartdimmer uactionscript scrotwm fbdesk tulip beep nikto wine linux-image azureus makeﬁle uuid whiptail alex junior-arcade libssl-dev update-inetd uextended uaiglx sudo dump lockout overlayscrollbar xubuntu mdfiso linux-libc-dev lm-sensors lxde install-info xsensors gutenprint sensors ubuntu_. fatrat fglrx equinix libjpeg-dbg umingw update-inetd ﬁrefox devede cd-r tango mixxx uemulator compiz libpulse-dev synaptic ecryptfs crawl ugtk+ tree perl tree ubuntu-docs libsane gnomeradio uﬁlemaker dyndns libfreetype daemon xsensors vncviewer indicator-applet nvidia- rsync members qemu mount rsync macbook gsfonts synaptic ﬁnger john xsensors screen inotify signatures units ushareware ufraw bonnie fstab nano bless bibletime irssi ujump foremost nzbget ssid onboard synaptic branding hostname radio hotwire xebia netcfg xchat lazarus pilot ucopyleft java-common ifplugd ncmpcpp uclass gnome sram binfmt-support vuze java-common sauerbraten adapter login alias apt-get aptitude aspell basename break builtin bzip case cfdisk chgrp chmod chown chroot chkconﬁg cksum comm command continue cron crontab csplit curl date ddrescue declare diff diff dircolors dirname dirs dmesg echo egrep eject enable eval exec exit expect expand export expr false fdformat fdisk fgrep fold fsck function fuser gawk getopts grep groupadd groupdel groupmod groups gzip hash head history hostname htop iconv ifconﬁg ifdown ifup import install jobs join kill killall less link local locate logname logout look lprm lsof mkdir mkﬁfo mknod more most mount mtools nohup notify-send nslookup open passwd paste ping pkill popd printf pushd quota quotacheck quotactl read readonly rename return rmdir rsync screen sdiff select shift shopt shutdown sleep slocate sort source split stat strace sudo suspend sync tail test time timeout times touch tput traceroute true tsort type ulimit umask unalias uname unexpand uniq units unrar unset unshar until useradd userdel usermod users uuencode uudecode vmstat wait watch whereis which while whoami write xargs xdg-open admin purge models trained learning rate batches size either size gradients clipped truncate backpropagation batches tokens validate entire validation every training batches. choose almost identical hyperparameters ubuntu twitter models since models appear perform similarly w.r.t. different hyperparameters since statistics datasets comparable. frequent words twitter ubuntu natural language vocabulary models assign words outside vocabulary special unknown token symbol. mrrnn coarse token vocabulary consisting frequent tokens coarse token sequences. compute cost beam search log-likelihood tokens beam divided number tokens contains. lsmt model performs better beam search allowed generate unknown token symbol however even still performs worse hred model across metrics except command accuracy. based preliminary experiments found slightly different parametrization hred baseline model worked better twitter. encoder bidirectional encoder hidden units forward backward rnns each context decoder hidden units each. furthermore decoder computes dimensional real-valued vector hidden time step multiplied output context rnn. output feed one-layer feed-forward neural network hyperbolic tangent activation function decoder conditions human evaluators either study work english speaking environment indicated experience using linux operating system. ensure high quality ground truth responses human evaluators asked evaluate responses ground truth contained least technical entity. starting evaluators shown short annotated example brief explanation give annotations. particular evaluators instructed following reference figure model response examples given section. model responses downloaded www.iulianserban.com/files/twitterdialoguecorpus.zip www.iulianserban.com/files/ ubuntudialoguecorpus.zip. context seriously... xorg running apps i’ve seeing ever since upgraded anyone else seeing would kind tests something enjoy difference... saying xorg case would window manager without composite try. much. need serious help fullscreen video playback. already asked lubuntu might better chance video ﬁles either player hang audio/video goes badly sync right away. know work ﬁles even like hours browsing page reading problem comes again. idea look anymore reproduce even process building desktop wondering using start restart windows session girlfriend logs plenty memory supports necessary virtualisation technologies. install virtualbox sure command line method speciﬁed installed operating system make command startup command ending hello setup default gateways graphically network manager can’t default gateway device...? well.. hows gonna work response mrrnn act. -ent. using mrrnn noun window manager using hred act.-ent. don’t know help that sorry. hred idea that want know lstm something mrrnn act. -ent. compile kernel mrrnn noun linux kernel something hred act.-ent. version ubuntu hred want lstm mean mrrnn act. -ent. works mrrnn noun hred act.-ent. video card hred yeah know don’t know that don’t know lstm mean mrrnn act. -ent. installed virtualbox mrrnn noun trying install ubuntu hred act.-ent. don’t know that sorry. don’t know that. hred thanks i’ll give shot. lstm thanks i’ll that. mrrnn act. -ent. looking mrrnn noun default gateway hred act.-ent. trying hred want static dont know lstm mrrnn act. -ent. mrrnn noun something something something hred act.-ent. mount mount says mount special device /dev/sda exist hred mean mount lstm talking response mrrnn act.-ent. looking mrrnn noun gconf-editor hred act.-ent. want hred yeah fresh install ubuntu worked ﬁne. lstm mrrnn act. -ent. thank mrrnn noun need mount /mnt /media hred act.-ent. don’t know trying able mount manually don’t know hred don’t know that want able mount /media lstm mrrnn act. -ent.can tell mrrnn noun join ubuntu-offtopic hred act.-ent. trying hred know dont know install lstm mrrnn act. -ent.do know check bios mrrnn noun disable bios hred act.-ent. don’t want reboot want able boot stick don’t want every time want hred want might want hardware lstm mrrnn act. -ent.what version installed hred act.-ent. install latest version wine latest version dont know install hred lstm mrrnn act. -ent. see. mrrnn noun libreofﬁce hred act.-ent. lock /var/lib/dpkg/lock open unable locate package something hred don’t know don’t know lstm installed mrrnn act. -ent. working. mrrnn noun network manager hred act.-ent. name wireless card hred using ndiswrapper lstm mean mrrnn act. -ent. looking mrrnn noun address hred act.-ent. trying hred thanks i’ll lstm command found guys trying install libreofﬁce repositories throwing wobbly. anyone help looks like dependency problems attempt resolve throws wobbly <url apt-get install yeah looks like don’t java install need apt-get install string apt-get purge libreofﬁce looks like installed previously update realized removed earlier wasn’t working. re-install. apt-get purge libreofﬁce complains unmet dependencies/ able reinstall throws errors seen pastebin jwrigley check /var/lib/libreofﬁce/share/prereg exists doesn’t appear exist terminal apt-get command line completion helps apt-get purge libreofﬁce yeah now. sorry. telling dependencies apt-get install awesome well libreofﬁce still working anyone help wireless working whats trouble doesnt show wireless networks i’ve core macbook right anyone questions. comments thread <url> well photoshop illustrator? pretty much well current macbook pros maybe bigger haha her. she’s awesome. hahahahahahahaha amazed. thinking dudz? hanging library past couple hours makes feel like i’ll great test nerd haha what? changed like now? feel changing like change. christian bale must prepping gordon’s sticks commercial. oscar mrrnn love kids vhred you’re cute hred happy lstm that’s spirit mrrnn johnny depp best vhred something hred glad enjoyed lstm great actor. mrrnn love song vhred happy birthday hred love lstm glad liked mrrnn macbook pro??? vhred want hred sure you’ll soon. sure it’ll worth lstm thanks heads i’ll check out. mrrnn yes. love justin bieber vhred love her. hred think lstm think she’s little something. mrrnn it’s spring break vhred excited hred something lstm something mrrnn agree news vhred thank support hred what’s news? lstm agree think it’s thing think it’s thing mrrnn bieber fever? <smiley> vhred email i’ll send hred thank lstm thank <smiley> mrrnn yeah it’s library vhred mean? hred haha glad lstm yeah like know mrrnn christian bale. know he’s talking about. know he’s talking about. lol. something vhred harry potter movie? hred think it’s good movie it’s good movie. lstm going watch that. mrrnn it’s great vhred little tired. you? hred it’s since i’ve here. hred it’s going long day. mrrnn what’s netball??? vhred going tomorrow. hred going <number>’s. lstm what’s score?", "year": 2016}