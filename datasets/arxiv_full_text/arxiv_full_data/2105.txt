{"title": "Gamma Processes, Stick-Breaking, and Variational Inference", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithms on nonnegative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors.", "text": "bayesian nonparametric models machine learning focused dirichlet process beta process variants gamma process recently emerged useful nonparametric prior right. current inference schemes models involving gamma process restricted mcmc-based methods limits scalability. paper present variational inference framework models involving gamma process priors. approach based novel stick-breaking constructive deﬁnition gamma process. prove correctness stick-breaking process using characterization gamma process completely random measure explicitly derive rate measure construction using poisson process machinery. also derive error bounds truncation inﬁnite process required variational inference similar truncation analyses nonparametric models based dirichlet beta processes. representation used derive variational inference algorithm particular bayesian nonparametric latent structure formulation known inﬁnite gamma-poisson model latent variables drawn gamma process prior poisson likelihoods. finally present results algorithms nonnegative matrix factorization tasks document corpora show compare favorably sampling-based techniques variational approaches based beta-bernoulli priors. gamma process versatile pure-jump l´evy process widespread applications various ﬁelds science. late emerging increasingly popular prior bayesian nonparametric literature within machine learning community; recently applied exchangeable models sparse graphs well nonparametric ranking models also used prior inﬁnite-dimensional latent indicator matrices latter application earliest bayesian nonparametric approaches count modeling thought extension venerable indian buﬀet process modeling latent structures feature occur multiple times datapoint instead simply binary. ﬂexibility gamma process models allows applied wide variety bayesian nonparametric settings relative complexity makes principled inference nontrivial. particular direct applications gamma process bayesian nonparametric literature markov chain monte carlo samplers posterior inference often suﬀers poor scalability. bayesian nonparametric models— particular involving dirichlet process beta process—a successful thread research considered variational alternatives standard sampling methods ﬁrst derives explicit construction underlying weights atomic measure component random measures underlying inﬁnite priors; so-called stick-breaking processes dirichlet beta processes yield construction. weights truncated integrated mean-ﬁeld variational inference algorithm. instance stick-breaking derived dirichlet process seminal paper sethuraman turn used variational inference dirichlet process models similar stick-breaking representations special case indian buﬀet process beta process constructed naturally mean-ﬁeld variational inference algorithms nonparametric models involving priors variational inference algorithms shown scalable sampling-based inference techniques normally used; moreover work full model posterior without marginalizing variables. paper propose variational inference framework gamma process priors using novel stick-breaking construction process. characterization gamma process completely random measure allows leverage poisson process properties arrive simple derivation rate measure stick-breaking construction show indeed equal l´evy measure gamma process. also poisson process formulation derive bound error truncated version compared full process analogous bounds derived dirichlet process indian buﬀet process beta process then particular example focus inﬁnite gammapoisson model model prior inﬁnitely wide latent indicator matrices non-negative integer-valued entries; column associated parameter independently drawn gamma distribution matrix values independently drawn poisson distributions parameters means. develop mean-ﬁeld variational technique using truncated version stick-breaking construction sampling algorithm uses monte carlo integration parameter marginalization similar baseline inference algorithm comparison. finally compare algorithms nonnegative matrix factorization task involving psychological review nips york times document corpora. related work. knowledge previous exposition explicit recursive stick-breaking-like construction gamma extension instance variational algorithms priors. general inverse l´evy measure algorithm requires inversion exponential integral generalized construction technique applied gamma process; since closed form solution inverse exponential integral known techniques give analytic construction weights hence cannot adapted variational techniques straightforward manner. constructive deﬁnitions gamma process include discusses sampling-based scheme weights gamma process sampling poisson process. further characterization dirichlet process normalized gamma process possibly utilized sampling gamma process weights knowledge existing methods variational inference employ approaches. alternative gamma process-based models count modeling recent research examined negative binomial-beta process variants stick-breaking construction readily extends models since beta process priors. beta stick-breaking construction also used variational inference beta-bernoulli process priors though scalability issues applied count modeling problems addressed work show experimental section. completely random measures completely random measure space deﬁned stochastic process disjoint borel subsets random variables independent. canonical constructing completely random measure ﬁrst take σ-ﬁnite product measure draw countable points poisson process borel σ-algebra rate pkδωk measure notation weights ﬁnite seen campbell’s theorem governed total mass base measure completeness note completely random measures deﬁned three components ﬁxed atoms deterministic measure random discrete measure. third component explicitly generated using poisson process though ﬁxed component readily incorporated construction recursive generate weights random measures given stickbreaking unit interval subdivided fragments based draws suitably chosen distributions. example sick-breaking construction dirichlet process given propose simple recursive construction gamma process based stick-breaking construction beta process proposed particular augment slightly modiﬁed stick-breaking beta process independent gamma-distributed random measure show deﬁned above. show directly deriving rate measure marked poisson process using product distribution formulae. proposed stick-breaking construction follows iid∼ beta process stick-breaking construction product beta random variables allows interpret corresponding stick broken inﬁnite number pieces. note expected weight atom round αi/ci. parameter therefore used control weight decay cadence along representation provides clearest view construction somewhat cumbersome deal practice mostly introduction additional gamma random variable. reduce number random variables noting product beta gamma random variable distribution; also perform change variables proof. note that construction round weighted forms poisson point process since drawn poisson distribution. particular sets marked poisson process atoms poisson process marked note measure deﬁned improper gamma distribution p−e−cp σ-ﬁnite sense decompose countable union disjoint intervals ﬁnite measure. particular measure interval form speciﬁed theorem variational inference algorithm since variational distributions almost parameters variables construction lend simple closed-form exponential notation therefore relates construction stick-breaking construction indian buﬀet process bernoulli probabilities generated products beta random variables iid∼ beta. particular view construction variational algorithm requires truncation level number atoms tractability. therefore need analyze closeness marginal distributions data drawn full prior truncated prior stick-breaking prior weights integrated out. construction leads simpler truncation analysis truncate number rounds automatically truncates atoms ﬁnite number. analysis stick-breaking gamma process base measure poisson likelihood process denote precisely model develop variational inference next section. denote gkδωk recursively constructed pkδωk poisson. model obtain following result analogous error bounds derived nonparametric models literature. proof. starting intuition truncate process rounds error marginal distribution data depend probability positive indicator values appearing atoms round discussed section focus inﬁnite gamma-poisson model gamma process prior used conjunction poisson likelihood function. integrating weights gamma process process known yield nonparametric prior sparse inﬁnite count matrices note approach easily applicable models involving gamma process priors. iid∼ poisson also place gamma priors gamma gamma gamma. denoting data latent prior variables model hyperparameters respectively full likelihood written truncate inﬁnite gamma process atoms take total number datapoints. denotes latent variables excluding poisson-gamma prior; instance factor analysis topic models contains dirichlet-distributed factor variables mean-ﬁeld variational inference involves minimizing divergence model posterior suitably constructed variational distribution used tractable alternative actual posterior distribution. propose fully-factorized variational distribution poissongamma prior follows instead working actual divergence full posterior factorized proxy distribution variational inference maximizes canonically known evidence lower bound function since using exponential family variational distributions leverage closed form variational updates exponential families wherever perform gradient ascent elbo parameters distributions closed form updates. list updates distributions prior below. closed-form updates hyperparameters follows document corpus count matrix matrix models factor loadings matrix models actual factor counts documents. k−truncated poisson-gamma prior dirichlet prior columns variational distribution consequently gets dirichlet distribution multiplied variational dirichlet hyperparameters. setup immediately lend closed form updates resort gradient ascent. gradient elbo respect variational hyperparameter practice however found closed-form update facilitated simple lower bound elbo converge faster. describe update here. first note part elbo relevant potential closed form variational update written baseline also derive compare standard mcmc sampler model. construction sampling model. avoid inferring latent variables atom weights poissongamma prior monte carlo techniques integrate aﬀects posterior inference indicators round indicators hyperparameters posterior distribution closed form likelihood latent variables φ−g. re-write vocabulary-document count matrices. recall count matrix modeled matrix models factor loadings matrix models actual factor counts documents.. poisson-gamma prior symmetric dirichlet priors columns sampling steps described next. calculate posterior distribution using monte carlo techniques described above. discretize search space around current values lower upper bounds chosen unnormalized posterior falls search space also clipped drawn multinomial distribution search values normalization. search space discretized using appropriate upper lower bounds above sampled using monte carlo techniques. drawn multinomial distribution search values normalization. baseline also derive compare standard mcmc sampler model. construction sampling model. avoid inferring latent variables atom weights poisson-gamma prior monte carlo techniques integrate aﬀects posterior inference indicators round indicators hyperparameters posterior distribution closed form likelihood latent variables π−g. complete updates described supplementary. implemented analyzed performance three variational algorithms corresponding three diﬀerent priors poisson-gamma process prior paper bernoulli-beta prior prior along mcmc gamma prior models addition four algorithms symmetric dirichlet prior columns added corresponding variational distributions variables collection denoted above. held-out per-word test log-likelihoods times required update variables iteration comparison metrics data used training. used likelihood metric samples replaced expectations variational distributions. synthetic data. warm-up consider performances synthetic data generated model. generate weighted atoms gamma prior using stick-breaking construction poisson likelihood generate values atom yield indicator matrix simulated vocabulary terms generated generated poi. measure test likelihood every iteration average results across random restarts. measurements plotted ﬁg.a. shown vgp’s measured heldout likelihood converges within iterations. traceplot shows ﬁrst thirty heldlikelihoods measured burn-in. per-iteration times seconds minutes respectively. learned online values oscillating around snbp refers poisson-gamma mixture sampler traceplot shows ﬁrst likelihoods measured burn-in iterations. performed similarly algorithms though slightly worse. real data. used similar framework model count data psychological review nips york times corpora. vocabulary sizes respectively document counts respectively. dataset three variational algorithms random restarts each measuring held-out log-likelihoods per-iteration runtimes diﬀerent values truncation factor learning rates gradient ascent updates kept order gradient steps iteration. representative subset results shown ﬁgs.b used vague gamma priors hyperparameters variational algorithms improper priors sampler. found test likelihoods independent initializations. results variational algorithms dependent dirichlet prior noted ﬁg.b. therefore used learned test likelihood iterations heuristic select found three variational algorithms attain similar test likelihoods across four datasets hours time slight edge vibp. sampler somehttp//psiexp.ss.uci.edu/research/programs data/toolbox.htm http//www.stats.ox.ac.uk/ teh/data.html https//archive.ics.uci.edu/ml/datasets/bag+of+words unexpectedly attain competitive score dataset unlike synthetic case. instance shown ﬁg.c oscillated around psyrev dataset whereas variational algorithms attained comparison process sampler attains iteration iterations burn-in. also seen ﬁg.c faster convergence among three variational algorithms vibp scaled best small medium datasets function truncation factor updates closed-form spite learn additional weight matrix running times competitive small values datasets. however large dataset orders magnitude faster bernoulli-beta algorithms example truncation atoms took around seconds iteration whereas vibp took minutes. scaled poorly datasets seen ﬁgs.d reason three-fold learning parameters additional matrix directly aﬀected dimensionality gradient updates variables taylor approximation required gradient updates sampler required around minutes iteration small datasets hour minutes average nyt. summarize found post running times competitive fastest algorithm small medium datasets outperform methods completely large dataset providing similar accuracy compared variational algorithms similar models measured held-out likelihood. also fastest converge typically taking less iterations. compared variational method substantially faster produces higher likelihood scores real data. described novel stick-breaking representation gamma processes used derive variational inference algorithm. algorithm shown scalable large datasets variational algorithms related models attaining similar accuracy also outperforms samplingbased methods. expect recent improvements variational techniques also applied algorithm potentially yielding even scalability. figure plots held-out test likelihoods per-iteration running times. plots psyrev respectively. plots psyrev dataset. algorithm trace colors common plots. text full details.", "year": 2014}