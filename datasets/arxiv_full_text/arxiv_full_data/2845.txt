{"title": "Variational Inference for Policy Gradient", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Inspired by the seminal work on Stein Variational Inference and Stein Variational Policy Gradient, we derived a method to generate samples from the posterior variational parameter distribution by \\textit{explicitly} minimizing the KL divergence to match the target distribution in an amortize fashion. Consequently, we applied this varational inference technique into vanilla policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations for reinforcement learning problems.", "text": "inspired seminal work stein variational inference stein variational policy gradient derived method generate samples posterior variational parameter distribution explicitly minimizing divergence match target distribution amortize fashion. consequently applied varational inference technique vanilla policy gradient trpo bayesian neural network parameterizations reinforcement learning problems. suppose random sample base distribution e.g. able generate induced distribution general invertible diﬀerentiable transformation goal regard variational distribution match true distribution kl||p) minimized. maximum likelihood estimation. perform stochastic gradient descent using optimal method related interesting seminal stein variational inference major diﬀerence later uses kernelized stein variational gradient determinate repulsive force. generate policy distribution bayesian neural network. suppose parameter policy network parameter able generated base distribution invertible diﬀerentiable transformation function adopt complicated diﬀerentiable transformation functions simple example weight connection neuron could realized weight parameter able generate stochastic multi-modal policy distribution represented neural network several hidden layers. formulation originally proposed diﬃculty formuα r)dθ. able bypass calculating gradient log-probability exciting idea stein variational inference similarly suppose generate sample transforming random noise using induced variational distribution transformation. optimization objective match induced variational distribution introduce varational inference vanilla policy gradient reinforce given realization network parameter order generate stochastic policy distribution introduce another random noise second invertible diﬀerentiable transformation induces stochastic policy distribution closed form adopt simple general representative generative model. policy parameter generated noise transformation neural network parameterized another noise generate action another transformation parameterized policy network distribution. mean variance networks weight parameter used generate posterior distribution action mean continuous control problems represents mean variance networks. variational policy parameter distribution minimizing divergence variation distribution generated based transformation optimal posterior parameter distribution kl|| exp{r}). general parameterization policy instead regarding parameter policy take random variable introduce auxilliary network parameter action generated noise transformation induces corresponding policy distribution example could gives general representation policy compared previous formulation section furthermore easy introduce multimodal distribution stochastic actions. motivation combine fast convergence sample eﬃciency trpo exploration introduced variational inference posterior policy parameter distribution. adopt variational policy gradient trpo. important point calculate divergence current previous policies eﬃciently?", "year": 2018}