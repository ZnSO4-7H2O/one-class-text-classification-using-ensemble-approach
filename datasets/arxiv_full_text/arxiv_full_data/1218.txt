{"title": "Gradual DropIn of Layers to Train Very Deep Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.", "text": "however training deep network difﬁcult open research problem difﬁcult train deep networks error norm backpropagation grow vanish exponentially. addition large training datasets necessary network millions weights. suggest dynamic architecture grows during training process allows training deep networks. illustrate dropin layer layers skipped start training though present. allows weights included layers start converging. number iterations dropin layer increasingly includes activations inserted layers gradually trains weights theses added layers. dropin follows philosophy embedded within curriculum learning curriculum learning starts easier problem incrementally increases difﬁculty. starts training shallow architecture after convergence begins dropin incrementally modiﬁes architecture slowly include units layers. addition dropin used mode analogous dropout regularization deep neural network training. instead setting random activations zero done dropout dropin sets activations activations previous layer. demonstrate noise mixing activations previous layers provides regularization training. addition dropin dropout viewed training large collection networks varied architectures extensive weight sharing. introduce concept dynamically growing neural network training. particular untrainable deep network starts trainable shallow network newly added layers slowly organically added training thereby increasing network’s depth. accomplished layer call dropin. dropin layer starts passing output previous layer increasingly including units layers feedforward backpropagation. show deep networks untrainable conventional methods converge dropin layers interspersed architecture. addition demonstrate dropin provides regularization training analogous dropout. experiments described mnist dataset various expanded lenet architectures cifar dataset architecture expanded layers imagenet dataset alexnet architecture expanded layers -layer architecture. past years state-of-the-art results image recognition object detection face recognition speech recognition machine translation image caption generation driverless technology applications required increasingly deeper neural networks. network depth refers number layers architecture. well known adding layers neural networks makes expressive year imagenet challenge held teams expected given image detect localize recognize object image. deep convolutional neural networks dominated competition since krizhevsky year since winner competition used deeper network previous year’s winner sutskever investigate difﬁculty training deep networks conclude proper initialization momentum necessary. glorot bengio recommend initialization method called normalized initialization allow training deep networks. recently improved upon normalized initialization method changing distribution take account relu layers. hinton proposed ﬁrst training layer layer unsupervised fashion transformed version input could realized. erhan later characterized mathematics unsupervised pre-training offered explanation success. sussillo abbott suggest initialization scheme called random walk initialization based scaling initial random matrices correctly. multiplying error gradient correctly scaled random matrix layer unbiased random walk formed. papers show results experiments networks consisting hundreds layers. raiko introduce concept skip connections adding linear transformation usual nonlinear transformation input unit. skip connections separate linear non-linear portions activations allow linear part skip higher layers. similar dropin ways purpose dropin differs skip connections dropin need learn parameters. romero suggest training thin deep student network larger shallower teacher network. authors accomplish utilizing output teacher’s hidden layers hint student’s hidden layers. srivastava propose architecture named highway networks output layer’s neuron contains combination input output. highway networks carry gates inspired long short-term memory recurrent neural networks regulate much input carried next layer. authors demonstrate structure permits training networks hundreds layers parameters learned along parameters network. zhang applied highway networks lstm recurrent neural netbreuel discusses dynamic network describes biologically plausible reconﬁgurable network. network different units weighted dynamically produce different conﬁgurations. allows single network perform multiple tasks. dropin represents different type dynamic network grows training rather reconﬁgures task. well-known dropout method effective means improve training deep neural networks. training dropout randomly zeros neuron’s output activation probability called dropout ratio network cannot rely particular conﬁguration. reduces overﬁtting training data resulting network robust better generalizes unseen data. dropout samples exponential number different ‘thinned’ networks dropin samples exponential number different thinner shallower sub-networks. like dropout dropin randomly changes conﬁguration network cannot rely particular conﬁguration. baldi sadowski provide theoretical basis understanding dropout demonstrating dropout regulates training prevents overﬁtting approximating average large ensemble networks. similar theoretical understanding also apply dropin. modes running dropin ﬁrst gradually include skipped layers refer gradual dropin second regularizer named regularizing dropin. figure provides visual reference dropin unit works. gradual dropin initially passes activations previous layer effectively skipping layers. iteration number ratio computed dropin length number iterations reduces number activations copied layer drops total number activations layer remaining activations accepted layer backpropagation trains weights newly added units. regularizing dropin dropin probability ratio static value case dropin works analogously dropout instead setting values zero activations previous layer choice activations come layer done evolving random fashion iteration. follow notation dropout paper show formally. namely start neural network composed number layers layer index. also represents vector outputs layer input next layer data input ﬁrst layer. addition weights biases layer allow track evolving nature network include training iteration number layer’s unit index number dropin requires size layer previous layer same. hence also implemented resize layer allow reshaping layer’s output user-speciﬁed size. resize layer modiﬁes input user-speciﬁed height width number channels/ﬁlters. resize layer allows dropin work layers even sizes different. purpose section demonstrate effectiveness dropin several standard datasets architectures trainable standard methods. attempt made optimize architecture hyperparameters higher accuracy main objective show deep architecture converge without dropin converge however results sections also demonstrate increase accuracy using deeper network imagenet. structure several networks. naming convention {layer type}{layer number}-{number outputs}. example conv represents convolutional layer numbered outputs ﬁlters sized dropin layers denoted dropin depicted figure section show deep networks trainable using gradual dropin expanded lenet mnist data. section show effect dropin length training accuracy expanded cifar- network small performance gain possible added layers. section show expanded alexnet architecture increases accuracy trainable standard network architecture classiﬁcation mnist provided caffe package -layer lenet consisting convolutional/max-pooling layers followed fully-connected layers inspired work increased number convolutional layers denote lenet. added layers learned convolution ﬁlter change size outputs. added dropin layers convolutional layers called network lenet dropin. ﬁrst looked created lenet lenet dropin architectures. lenet converge standard training time iterations given multiple realizations training process. however utilizing dropin units able lenet dropin converge iterations hyperparameters. figure show results several different dropin lengths network. different lengths indicate robustness dropin length simpler networks that general shorter dropin lengths provide marginally better results. note case added layers increase overall accuracy netlook number layers affects figure different training dropin. plots dropin length iterations dropin length iterations. plot present three different networks convolutional layers dropin lengths three network depths gradual dropin method allowed networks converge. deeper networks require greater number iterations reach level accuracy shallower networks expected greater number weights train. also networks converge quickly shorter dropin length indicating shorter dropin lengths desirable. figure test data classiﬁcation accuracy training -layer cifar- architecture dropin. curves show classiﬁcation accuracies different dropin lengths three convolutional layer architecture trains quickly attains good accuracies. convolutional layers replicated obtain -layer model corresponds depth cifar- models experiments highway networks detailed architectures compared table shown table sizes layers entering dropin layer kept simplicity. every convolutional layer weight initialization gaussian standard deviation bias initialization constant convolutional layer followed rectiﬁed linear unit local normalization. length training learning rates schedule modiﬁed iterations. modiﬁcation trained satisfactorily provided reasonable comparison. numerous attempts training -layer network without dropin layers failed converge. similar attempts train network dropin layers successfully converge primary result study. experiments performed varying dropin length. figure shows accuracy curves dropin length table compares ﬁnal accuracies. ﬁnal accuracies show marginal improvement longer versus shorter lengths cifar- results relatively independent length value. furthermore ﬁnal accuracies -layer architecture less better original -layer architecture implies cifar- dataset deeper networker provides marginal improvement. imagenet large image database based nouns wordnet. image database used imagenet large scale visual recognition challenge commonly used basis comparison deep learning literature. database contains million training testing images covering categories. caffe website provides architecture hyperparameter ﬁles slightly modiﬁed alexnet. downloaded architecture hyper-parameter ﬁles website expanded architecture layers layers duplicating convolutional layers shown columns respectively table alexnet dropin includes dropin layer every duplicated layer used create alexnet multiple attempts training alexnet architecture conventional manner converge. tests expanded architecture hyper-parameters kept provided caffe website experiments varying dropin hyperparameter dropin length. table shows ﬁnal accuracy results training iterations range lengths. figure compares accuracy training experiments. contrast results cifar- dropin length makes difference imagenet. believe deeper architecture increases classiﬁcation accuracy larger datasets hence improvement smaller dropin lengths prominent. figure table conclude shorter lengths better longer ones. length less ﬁrst scheduled drop learning rate iteration network better trained. however difference dropin length negligible implying lengths less ﬁrst scheduled learning rate drop equivalent. vggn networks created visual geometry group second place image classiﬁcation category imagenet contest. networks trained database alexnet architecture discussed section contained layers. table architecture alongside refer convolutional layers stride padding maxpooling layers stride paper authors describe difﬁculty training deep networks utilized weight transfer method enable network converge training possible train deep neural network ﬁrst training shallow network using weights initialize deeper network believe addition easier training full network layers place leads better trained network. supported research feature visualization zeiler fergus demonstrate higher layers abstract representations. training place means learned representations conform well representation given layer training shallow network initializing weights deeper network might not. instead training smaller networks propose gradual dropin method. studies utilized prototxt referenced caffe website solver appropriate parameters authors’ paper. using traditional training methods able train architecture; failed begin converging multiple realizations. using template augment dropin layers create dropin based evidence presented section choose test dropin length found lengths began converge well limited time resources chose report length paper. results training dropin shown figure alongside vgg. gradual dropin difﬁcult train network converge. real power gradual dropin method; withtraining additional shallower network able directly train thus saving effort practitioner. using dropin regularization results experiment shown figure dropin dropout probability ratios tests hyper-parameters same. ﬁgure shows removing dropout causes visible degrading accuracy iterations kind degradation happen dropin. instead accuracy curve similar curve dropout small degradation overall performance. believe degradation dropin network difﬁcult train dropout network. however ﬁnal accuracy network dropin higher architecture without dropout experiment demonstrates dropin provides regularization since degradation found case without dropout absent. challenges deep learning practitioners determine good choices hyper-parameter values architecture given application dataset. dropin dropout provide easier test choices architecture running experiments many different architectures. chitecture depths widths respectively. since adding layers necessarily increase accuracy gradual dropin mode little effect figures visible effect figure substantial improvement implies beneﬁt additional depth. similarly making dropout ratio varies perhaps provides guidance minimum number neurons layer. decreasing probability neurons retained error typically range probability ratios error plateaus threshold probability error increases. multiplying number neurons layer threshold probability approximately determine minimum number neurons must retain marginal harm accuracy. major result paper deeper architectures cannot converge using standard training methods become trainable slowly adding layers training. addition indications dropin layers help regularize training network. found general shallow network trainable deeper network additional layers added dropin layer also trainable. large dataset like imagenet adding additional layers increases accuracy. explored training tailored dropin lengths different dropin layers network. addition comparing dropin initializing weights training separate shallow network tested; planned future work reported elsewhere. also plan test dropin within architectures recurrent neural networks. future work also includes training networks hundreds layers using asynchronous dropin layers added starting different iterations. addition wish test training entire deep network initially thin units added layers training. furthermore plan study methodology developed learn data automatically optimize architecture training thus learn adapt application based data.", "year": 2015}