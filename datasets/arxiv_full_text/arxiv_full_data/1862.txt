{"title": "Generative Interest Estimation for Document Recommendations", "tag": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "abstract": "Learning distributed representations of documents has pushed the state-of-the-art in several natural language processing tasks and was successfully applied to the field of recommender systems recently. In this paper, we propose a novel content-based recommender system based on learned representations and a generative model of user interest. Our method works as follows: First, we learn representations on a corpus of text documents. Then, we capture a user's interest as a generative model in the space of the document representations. In particular, we model the distribution of interest for each user as a Gaussian mixture model (GMM). Recommendations can be obtained directly by sampling from a user's generative model. Using Latent semantic analysis (LSA) as comparison, we compute and explore document representations on the Delicious bookmarks dataset, a standard benchmark for recommender systems. We then perform density estimation in both spaces and show that learned representations outperform LSA in terms of predictive performance.", "text": "abstract. learning distributed representations documents pushed state-of-the-art several natural language processing tasks successfully applied ﬁeld recommender systems recently. paper propose novel content-based recommender system based learned representations generative model user interest. method works follows first learn representations corpus text documents. then capture user’s interest generative model space document representations. particular model distribution interest user gaussian mixture model recommendations obtained directly sampling user’s generative model. using latent semantic analysis comparison compute explore document representations delicious bookmarks dataset standard benchmark recommender systems. perform density estimation spaces show learned representations outperform terms predictive performance. many people internet become primary source reading material. however vast amount available content challenge interesting articles. recommender systems textual data address problem. conceptual approaches recommender systems collaborative content-based ﬁltering. collaborative ﬁltering user attributes information historic behavior leveraged make recommendations. corpus documents previously read documents user underlying topic groups inferred. articles read group users recommended similar users. common approach downsides. collaborative ﬁltering suﬀers cold-start problem. relevance documents estimated visited users. thus enough empiric records collected ﬁrst. records need contain enough overlap user interests eﬀectively infer underlying interest groups connect users documents. contrast content-based ﬁltering requires preferred documents user. documents grouped semantically content overlap users’ preferences required. challenge content-based ﬁltering however useful semantic representations documents. content-based recommender systems documents must embedded common space. space allows relate documents other. several approaches create semantic vector representations text documents. learned representations compare latent semantic analysis described below. common document representation bag-of-words documents represented contained words. vector document length overall vocabulary. elements vector corresponding word contained document zero otherwise. note representation looses information ordering words. vectors often reduced dimensionality using svd. applying features known described additions include weighting elements vector using term frequency measures tf-idf however usefulness weightings practice questioned recently learning vector representations large text corpora become computationally feasible learned representations improvements state-of-the-art natural language problems part-of-speech tagging named entity recognition mikolov extended idea order learn representations phrases documents. document representations particular interest content-based recommender systems since learned vectors show promising semantic properties given ﬁxed-length vector representations documents multiple approaches suggest potentially relevant articles common approach compute relevance score document unseen user. compute scoring using nearest neighbor queries proposed cover hart unseen document nearest documents user documents queried using distance measure cosine distance euclidean distance. score composed computed distances. limitations approach arbitrary choice distance function alternative using nearest neighbors density estimation using generative models model interests user probability distribution semantic space. assuming user’s history generated interest model using gaussian mixture model allows capture user’s interest spread across multiple clusters semantic space. gmms common choice modelling densities work suggest combining learned document representations generative density estimation using gmms. focus content-based recommendations domain text documents. section compare approach related work recommendation systems learned representations density estimation. section introduces approach call density-based interest estimation. following conduct benchmark delicious bookmark dataset compare learned representations representations section summarize ﬁndings point directions research. content-based recommender systems form active area research. overview state-of-the-art methods refer lops adomavicius tuzhilin work diﬀers systems ways. first learned document representations used instead common representation described mikolov second density estimation obtain generative model user interest. learned representations recently proposed content-based recommender systems. musto study diﬀerence several document representations content-based recommender systems. suggest learned representations well-performing option compared latent semantic indexing random indexing. word representations comparison closely related document representations work. kataria agarwal present another approach using learned representations content-based recommendations. propose two-step solution recommending user tags using document embeddings obtain remarkable results recommendations. focus recommending documents instead tags. building research work suggest generative approach allows recommend documents sampling estimated user interest rather scoring sorting candidate documents. based learned document representations employ generative model user interest estimate density. westerveld explain usage probabilistic models classiﬁcation tasks used work. another common approach usage multinomial distribution independency assumption used naive bayes classiﬁers however approaches density estimation learned representations explored paper. general approach involves three steps preprocessing computation document representations density estimation document vectors compare latent semantic analysis learned representations. inputs method corpus documents documents representing user’s preference. text document preprocessed removing special-characters conversion lower-case described srividhya anitha neither stemming used stop words removed avoid introducing bias removing potentially useful information word classes contain. computing document vectors information captured endings words discarded automatically relevant task. tokenized documents compute representations ways explained next subsections. latent semantic analysis aims representing words general semantic structure. involves transforming documents vector space model followed latent semantic indexing ﬁrst described dumais deerwester claim provide linguistic notion capturing synonymy polysemy. work shows related semantic categories close created vector space. works many cases groups words similar co-occurrence patterns together idea learned word representations proposed mikolov start random vectors chosen ﬁxed length. shallow neural network trained large text corpus. objective correctly predict current word context surrounding words. using backpropagation algorithm gradient errors respect weights classiﬁer input vector calculated. update classiﬁer word-vectors surrounding words using gradient descent. several modiﬁcations architecture. suggest papers mikolov collobert weston comparison insights. learn distributed representations documents mikolov extended model explained. classiﬁer initialized random paragraph vector addition context words. paragraph vector trained gradient descent resulting paragraph vector covers overall semantics text. authors suggest versions algorithm. either train classiﬁer paragraph vectors train word vectors well. train paragraph vectors found training word vectors improve performance signiﬁcantly. given context words multiple plausible choices current word. thus expect paragraph vector converge vector best helps resolve ambiguity. training document trained vector representation semantic space similar documents close estimate probability distribution assuming document i.i.d. realization users interest distribution further model distribution weighted mixture gaussians. called gaussian mixture model using expectation maximization. estimation takes place semantic space document representations. using mixture model capture fact users interested multiple topics spread across semantic space. introduces parameter speciﬁes number distributions. close amount topics user interested quality model capture user interest heavily depends semantic space thus quality document representations. therefore next section compare density estimation both learned representations. conduct benchmark delicious dataset common literature collaborative content-based recommender systems. start explaining dataset apply pre-processing steps. afterwards explore spaces produced learned representations crawled documents dataset then explain measure evaluating user interest models compare performance representation spaces delicious bookmarks dataset contains bookmarks users order embed bookmarked pages crawl corresponding websites extract main content detecting largest text area. step discard bookmarks accessible less characters content resulting documents. discard users less bookmarked pages following cantador ﬁnal dataset consists bookmarks diﬀerent users. average user holds bookmarked documents. evaluating user interests’ models explore semantic spaces constructed learned representations. tf-idf weighted occurrence vectors consider terms tf-idf scores within range collecting count vectors reduce dimensionality components using singular value decomposition learned representations paragraphvec algorithm described mikolov vector size training iterations noise-contrastive examples. also reduce dimensionality learned representations components using kernel kernel. figure compare learned vector spaces representations. ﬁgure shows corpus documents four randomly selected user proﬁles highlighted. space documents user proﬁle spread learned representations seem implicitly group users clusters. following section show allows model user interests accurately. fig. embedded corpus documents mapped space using t-sne four randomly chosen user proﬁles highlighted. without prior knowledge documents user grouped together. goal text-based recommender systems suggest useful documents obvious measure formally. large empiric study would possible perform evaluation based user interaction recommendations. however evaluation feasible many cases. therefore common approach measure quality based predictive performance user behavior. rate splits users measure predictive performance user interests. user proﬁle contains samples underlying distribution expect corpus documents even high-density areas user interest. therefore expect rates vastly results shown estimating user interests learned representations signiﬁcantly outperforms using representations. conﬁrms assumption previous section learned representations group documents helps capturing user interests. table performance generative user interest model learned representation space. error calculated average number test documents sampling respective user interest. proposed method signiﬁcantly improves performance common approach. proposed generative approach modelling user interests based user proﬁles preferred documents. based recently introduced method learned document representations model eﬀectively captures user interest. show comparison established approach benchmark common literature recommender systems. conclude learned representations create eﬀective semantics content-based recommendations. exploration needed fully understand reasons eﬀectiveness. areas learned representations established methods preferable another worthwhile direction research. adomavicius tuzhilin. toward next generation recommender systems survey state-of-the-art possible extensions. ieee transactions knowledge data engineering bhatia survey nearest neighbor techniques. arxiv preprint cantador brusilovsky kuﬂik. workshop information heterogeneity fusion recommender systems proceedings conference recommender systems recsys york acm. collobert weston. uniﬁed architecture natural language processing deep neural networks multitask learning. proceedings international conference machine learning pages hofmann. collaborative ﬁltering gaussian probabilistic latent semantic analysis. proceedings annual international sigir conference research development informaion retrieval pages kataria agarwal. distributed representations content-based personalized recommendation. ieee international conference data mining workshop pages ieee landauer foltz laham. introduction latent musto semeraro gemmis lops. word embedding techniques content-based recommender systems empirical evaluation. recsys posters ser. ceur workshop proceedings castells volume turian ratinov bengio. word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics pages association computational linguistics", "year": 2017}