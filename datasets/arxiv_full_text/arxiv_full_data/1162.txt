{"title": "Learning Multi-Scale Representations for Material Classification", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The recent progress in sparse coding and deep learning has made unsupervised feature learning methods a strong competitor to hand-crafted descriptors. In computer vision, success stories of learned features have been predominantly reported for object recognition tasks. In this paper, we investigate if and how feature learning can be used for material recognition. We propose two strategies to incorporate scale information into the learning procedure resulting in a novel multi-scale coding procedure. Our results show that our learned features for material recognition outperform hand-crafted descriptors on the FMD and the KTH-TIPS2 material classification benchmarks.", "text": "recent success feature learning techniques raises question well established hand-crafted features material recognition replaced automatically learned ones. known multi-scale representations competitive performance task however current feature learning techniques include multi-scale representations. therefore investigate applicability different feature learning techniques material recognition task well bring multi-scale information feature learning process. contributions present ﬁrst study applying unsupervised feature discovery algorithms material recognition show improved performance hand-crafted feature descriptors. further investigate different ways incorporate multi-scale information feature learning process. hereby propose ﬁrst multi-scale coding procedure results joint representation multi-scale patches recent progress sparse coding deep learning made unsupervised feature learning methods strong competitor hand-crafted descriptors. computer vision success stories learned features predominantly reported object recognition tasks. paper investigate feature learning used material recognition. propose strategies incorporate scale information learning procedure resulting novel multi-scale coding procedure. results show learned features material recognition outperform hand-crafted descriptors kth-tips material classiﬁcation benchmarks. perceiving recognizing material fundamental aspect visual perception. enables humans make predictions world interact ease. contrast texture recognition requires generalization large variations material instances discriminance visually similar materials. efﬁcient material recognition solution wide range uses context awareness robot manipulation. studies shown material recognition real-world scenarios solved recently task pushed even less constraint settings. flickr material dataset collects photos flickr samples common material demonstrate difﬁculties material recognition. particular incorporate large number different descriptors bayesian framework provides initial result dataset well established manually designed feature descriptor like variants still shown powerful methods feature descriptors able achieve state-of-art performance material recognition task. non-trivial come good design visual features efforts clearly needed explore question automatically learn features challenging relevant problem. related work material recognition recognition materials appearance received signiﬁcant attention vision community. curet database ﬁrst proposed address recognition problem single material instance motivated progress texture research later research shifted focus towards whole material class emphasizing challenges like scale variation intra-class variation. presented flickr material dataset used images flickr photos captured unknown real-world conditions. showed signiﬁcant improvement previous results using simple combination color multi-scale together rendered data. proposed kernel descriptor achieved state-ofart performance recently proposed anvariant descriptor obtain improvements previous studies. efforts based hand designed descriptors approach investigates learning based approach starts pixel information. feature learning separate line research using learned features tackle recognition problems. typical supervised learning setting given examples associated labels ym}. goal learn model predict labels example idea behind unsupervised feature discovery better representation data ease ﬁnal learning problem. machine learning community rich models feature discovery proposed. examples includes sparse coding restricted boltzmann machines various autoencoder-based models spike-and-slab sparse coding recently proposed combine advantages sparse coding restricted boltzmann machines shown superior performance. based model show extend multi-scale feature learning multiscale feature representation material recognition. multi-scale representation already early texton work included multi-scale ﬁlters enrich representation. although clustering step seen form feature learning ﬁlters hand-crafted. also work seen extension multi-scale substantially improved performance. feature extraction schemes entirely hand-crafted. recently multi-scale convolutional neural network trained pixels extract dense feature vectors encode regions multiple size centered pixel performed scene labeling proposed. differs multi-scale feature learning approach learn representation jointly across scales. image codes derived representation directly encode multi-scaled information. figure illustrates multi-scale codes learned model. seen broad application success feature learning techniques object recognition material recognition still relies hand-crafted features. appearance material classes seem special many ways. first samples seem obey stronger manifold assumption appearance varies rather smoothly w.r.t. changes lighting direction orientation scale. objects drastic changes occur pronounced structure edge information plays important role. section ﬁrst described framework feature learning summarize several models investigated tasks. afterwards propose novel multiscale feature learning strategies order accommodate multi-scale information important material recognition. commonly used patch-based unsupervised feature learning framework illustrated figure first random patches extracted training images feature mapping learned model obtained encode patches covering input image pool codes together order form ﬁnal feature representation altering model used feature mapping different feature representations. sparse coding sparse coding visual feature coding illustrated figure originally proposed olshauen field unsupervised learning model low-level sensory processing humans. recently used self-taught learning framework spike-and-slab sparse coding spike-andslab sparse coding goodfellow recently proposed combine merits feature learning methods like sparse coding rbms. ﬁrst layer real-valued d-dimensional visible vector corresponding pixel value position second layer consists different kinds latent variables binary spike variables realvalued slab variables spike variable gates slab variable jointly deﬁne hidden unit hisi. process formally described follows logistic sigmoid function biases spike variables govern linear dependence respectively diagonal precision matrices respective conditionals denotes element-wise product column constrained unit norm restricted diagonal matrix diagonal matrix scalar. particular interpreted series ﬁlters used sparsely represent data. graphical model describing shown figure model shown outperform previous feature learning technique best performer recent transfer learning challenge trains model patches limited number training images large number unlabeled image data coded training data test data learned model standard linear used classiﬁcation learned representation data. auto-encoder auto-encoder illustrated figure another popular model widely used learning feature representation deep learning community. ﬁrst phase mapped latent representation nonlinear function sigmoid function issue model introduces separate priors control activation units magnitude activated units separately. though similar structured model known ssrbm also proposed feature learning non-factorial posterior model grants better discriminative capability selectively activating small features given input. model learning variational algorithm used model learning. variant algorithm modiﬁcation e-step compute variational approximation posterior rather posterior itself. detail variational e-step maximize energy functional respect distribution unobserved minimize kullback-leibler divergence dkl||p drawn restricted family distributions ensure tractable. details refer reader scale information critical element material texture recognition problem. showed explicit treatment scale necessary material recognition realistic settings. performed manifold alignment respect scale real synthesized data turned crucial using generated data improve recognition rate. similarly local descriptors like limited small spatial support area several extensions multi-scale descriptor also shown yield strong performance improvements. therefore propose different strategies include multiscale information feature learning ﬁrst strategy perform encoding multiple scales stack obtained codes code classiﬁcation. convolve patch different sized gaussians encoding order represent scale information. common dictionary represecond strategy ﬁrst construct multi-scale pyramid image apply feature learning directly pyramid obtained codes classiﬁcation. contrast approach approach yields ﬁlters/codes model patch jointly across scales. graphical model describing shown figure denotes joint representation visible units speciﬁc scale inference carried model different scales seen decomposition larger multi-scale patch includes scales. figure shows ﬁlters learned manner. ﬁlter reaches across scales. experiments investigate learning framework used feature discovery material recognition task compare approach state-ofthe-art kth-tips databases. kth-tips database flickr material database experiments. example images shown figure kth-tips database designed study material recognition special focus generalization novel instance materials. includes images material categories category different instances. instances imaged varying viewing angles lighting conditions scales gives total images instance. instances training test category. collected flickr photos including common material categories images category images total. experiment randomly split half training half testing suggested compare learned features hand-crafted features recognition rates databases standard classiﬁers single scale experiments compare several variants. multi-scale approaches consider texton multiscale learning side compare vector quantization sparse coding auto encoders spike-and-slab approach. particular include comparison local quantization pattern recently introduced variant descriptor kernel descriptor shown state-of-art performance database. experiments size dictionary consistency. features. detail learned features apply kmeans clustering auto-encoder sparse coding model patch data vary patch size hand-crafted features examine original several variants including uniform-lbp rotation invariant-lbp rotation invariant uniform-lbp described experimental results shown figure figure entry results linear kernel kernel. datasets model combination linear kernel outperforms hand-crafted learned features. performance kth-tipsa respectively improves respectively. best performance achieved patch size veriﬁed parameter found cross-validation training set. attribute decrease performance patch size lack data learn required number parameters. best performance feature learning technique typically obtained combination linear kernel handcrafted features rely non-linear kernel. another appealing property learned features computational point view. based results found feature perform better learning approaches handcrafted features single-scale setting hence developed model multi-scale approaches following experiments. model learned performance degrades still outperforms single scale color-patch; representing data model learned performance even improves single scale descriptors. indicates features learned model speciﬁc dataset actually eligible capture common characteristics generalize different data within similar context. joint multi-scale model described section particular also investigate combination color information model concatenate codes code base patch size. hand-crafted features include multi-scale also texton ﬁlter though ﬁlter proposed long time still shows relative good performance similar recognition tasks hereby also used baseline results experiments. furthermore ﬁlter banks manually designed also contain ﬁlters multiple scales count multi-scale hand-crafted feature although textons also learned proper clustering algorithm k-means. experimental results shown figure figure mlbp shows better performance textons experiments. model produces slightly worse performance mlbp kth-tips improvement msc. including color information improves performance overall improvement best hand-crafted descriptor. numbers database beat best hand-crafted feature respectively. database inclusion color information yield additional improvements. joint multi-scale coding consistently improves stacked approach model. comparison state-of-the-art descriptors papers follow experimental protocol reproduced additional settings order provide points comparison state-of-the-art. follow protocol take samples class training fourth testing kth-tips-a data report averages random partitions simple classiﬁer feature learned single scale patch size achieved signiﬁcantly better reported results lqp. also additional experiments database following settings i.e. performing trials computing average multi-scale collaborated representation average recognition rate standard deviation comparable best single kernel descriptor additionally investigate transferring representations across databases. detail ﬁxed patch size trained single scale kth-tips database encoded data classiﬁcation vice versa. combined results figure table encoding image data kth-tips discussion visualization models figure shows visualization proposed multi-scale spike-and-slab sparse coding model. ﬁlter multi-scale response. looked larger range ﬁlters reveals interesting properties. ﬁlters similar structure across scales vary strongly. observation strong performance numbers experiments conclude multiscale code indeed captures additional information edge structures propagate scales. effect patch size feature learning results single scale descriptors dependent patch size. size patch determines locality descriptor therefore affects descriptor generalized different instances experience seems overall optimal patch size suggests need several candidates speciﬁc dataset select best use. experiments found patch size chosen based cross-validation training set. furthermore multi-scale approach resolve problem learning representation across multi-scales. scale information time improvements incorporating scale information however kth-tips database descriptor learned single scale performs best. related properties speciﬁc dataset also nature designed multi-scale descriptor. strategies multi-scale descriptors involve redundancy between every scale degrade classiﬁcation performance return redundancy also encodes scale information could improve performance ﬁnal performance affected factors jointly. kth-tips database material images taken strictly controlled conditions particular different scales instances improvement scale information limited case redundancy still affect classiﬁcation rate negatively. particular explains multiscale descriptors already incorporate information table recognition rates different number scales available training dataset kth-tipsa database. original database covers different scales spanning octaves indexed here. model patch size worse results single-scale descriptor. contrast case database images collected flickr photos arbitrary conditions scale information become signiﬁcantly important surpass inﬂuence redundancy makes multi-scale descriptor beat components single scale. real world application seems closer latter situation thus multi-scale descriptor preferable sense. validate analysis design additional experiments making subset training data kth-tips data order align settings database. compared standard settings kth-tips database training data covering different scales appeared training test partition images taken scales feature learning. note setting also resembles situation database data imaged unknown conditions training data cannot include scale information test data. would like multi-scale feature learning provide extra power basic feature learning model. experimental results shown table table observed limited scales training data like three scales indeed outperforms basic single scale model. color gray-scale color information serves important visual recognition could also lead confusion careful incorporate color information. interesting compare results multi-scale joint representation grayscale color kth-tips database color information improvement grayscale representation gray-scale version achieved best performance database. could explained large variation color information data causes confusion whereas color simpler informative classiﬁcation kth-tips. investigated different feature learning strategies task material classiﬁcation. results match even surpass standard hand-crafted descriptors. furthermore extended feature learning techniques incorporate scale information. propose ﬁrst coding procedure learns encodes features joint multi-scale representation. comparison learned features state-of-the-art descriptors shows improved performance standard material recognition benchmarks.", "year": 2014}