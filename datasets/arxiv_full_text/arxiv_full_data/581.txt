{"title": "Prediction-Adaptation-Correction Recurrent Neural Networks for  Low-Resource Language Speech Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\\it correction} network uses auxiliary information given by a {\\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further.", "text": "paper investigate prediction-adaptationcorrection recurrent neural networks lowresource speech recognition. pac-rnn comprised pair neural networks correction network uses auxiliary information given prediction network help estimate state probability. information correction network also used prediction network recurrent loop. model outperforms state-of-theart neural networks iarpa-babel tasks. moreover transfer learning language similar target language help improve performance further. index terms— lstm pac-rnn multilingual behavior prediction adaptation correction widely observed human speech recognition example listeners guess next wait conﬁrm guess. adjust listening effort predicting speaking rate noise condition based current information predict adjust letter sound mapping based talker’s pronunciations. previously proposed prediction-adaptationcorrection tries emulate mechanisms using dnns; prediction predicts next phoneme correction estimates current state probability based current frame hypothesis prediction dnn. model showed promising results timit unclear whether similar gain could achieved larger tasks prediction information might already incorporated language models. here successfully apply pac-rnn lvcsr several low-resource languages currently used iarpa-babel program. work supported part intelligence advanced research projects activity department defense army research laboratory contract number wnf--c-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa dod/arl u.s. government. addition study effect transfer learning recurrent architectures. recurrent networks lstms known require large amount training data order perform well iarpa-babel tasks multiple groups incorporated multilingual training order alleviate data limitation issues. popular approach multi-task training using dnns. multi-task setup single trained generate outputs multiple languages tied parameters. approach used robust feature extraction bottleneck features classiﬁers hybrid dnn-hmm approaches karaﬁ´at found using cmllr transformed features inputs hybrid could further improve performance. however believe none research investigated recurrent networks resource languages multilingual scenario. work presented extension based multilingual framework ﬁrst extract features using multilingual networks train different hybrid neural network architectures. experiments show lstms outperform dnns pac-rnn provides biggest gains task. additional improvements observed models adapted networks trained languages similar target language. rest paper organized follows. section review pac-rnn model describe enhanced version incorporates lstm. section describe multilingual system used pacrnn. explain experiments results section pac-rnn used work follows previous work fig. illustrates structure pac-rnn studied paper. main components model correction prediction dnn. correction estimates state posterior probability pcorr given observation feature vector information prediction time prediction predicts pred contextual window size used prediction study. addition speciﬁc example shown fig. hidden layer output projected lower dimension hcorr prediction dnn. train pac-rnn need provide supervision information prediction correction dnns. mentioned correction estimates state posterior probability thus state label frame cross-entropy criterion used. prediction follow phoneme label prediction targets. interpolation weight study unless otherwise stated total number frames training utterance. note standard pacrnn described here correction model prediction model dnns. point onwards call particular setup pac-rnn-dnn. lstms improved speech recognition accuracy many tasks dnns enhance pac-rnn model lstm replace used correction model. input lstm acoustic feature concatenated information prediction model prediction model also lstm observe performance gain experiments. keep simple prediction model features used work follow previous work hierarchical architecture realized concatenation dnns bottleneck layer. outputs layer ﬁrst used input features second whose outputs layer used ﬁnal features standard gmm-hmm training. iarpa-babel program focuses spoken term detection low-resource languages goal program reduce amount time needed develop spoken term detection capabilities language. data babel program consists collections speech growing list languages. project fourth year. work consider full pack languages released ﬁrst years source languages languages third year target languages languages also contain mixture microphone data recorded train test utterances. purpose paper downsampled wideband data treated rest recordings. target languages focus limited language pack condition includes hours transcribed training data. condition excludes human generated pronunciation dictionary. unlike previous years program usage data permitted language modeling vocabulary expansion. language used tied-state triphone cd-hmms states gaussian components state. grapheme-based dictionaries used target languages. note iarpa-babel languages difference phonetic graphemic systems often less output targets states. train multilingual kept frames appear frames actual speech. reduced total amount frames multilingual around hours. observed loss accuracy also reduced training time signiﬁcantly. discriminative training done cd-hmms using minimum bayes risk criterion data cleaned ﬁltered using techniques described language modeling n-gram created training data transcripts data. combined using weighted interpolation. vocabulary included words appeared training transcripts augmented frequent words web. chose words looking rate reduction augmented train vocabulary frequent words web. report results -hour development set. evaluation year. work follow similar approach replacing recurrent architectures bn-cmllr features taken network trained multilingual fashion adapted target language. pac-rnn features stacked context frames downsampled factor following context expansion used lstm. output state label also delayed utilize information future. multilingual training follows targets language pooled together softmax layer. adapting multilingual target language done performing additional ﬁnetuning steps sequentially using data target language. previous work shows using language closest target language pool source languages train second serve better initialization model multilingual second dnn. closest language identiﬁed acoustic data training language identiﬁcation system. ﬂowchart lid-based multilingual system trained shown fig. start adapting ﬁrst data target language. instead using second multilingual initialize train second random initialization using closest language’s data output targets. converges ﬁnal adaptation target language. pac-rnn model prediction unit hidden layer -unit bottleneck layer. correction model systems unit hidden layers lstm memory cells. correction model’s projection layer contains units. models randomly initialized without either generative discriminative pretaining. momentum used ﬁrst epoch momentum used subsequent epochs. found turning momentum ﬁrst epoch helps improve performance ﬁnal model. train learning rate mini-batch used ﬁrst epoch. learning rate increased second epoch kept development training criterion longer improves condition learning rate halved. similar schedule used train lstms pac-rnns except learning rates reduced used training. implemented hybrid models using computational network toolkit truncated backpropagation-through-time used update model parameters utterance segmented multiple chunks. speed training process multiple utterances simultaneously batch. found reduces training time improves quality ﬁnal model. study bptt segment contains frames process utterances simultaneously. decoding posteriors generated cntk kaldi toolkit generates recognition results. table summarizes wers achieved different models evaluated study. ﬁrst three rows results systems. multilingual closest language systems adapted target language whole stacked network. hybrid systems input features extracted ﬁrst adapted multilingual sbn. hybrid system outperforms multilingual similar closest language system. lstm improves upon around pacrnn-dnn outperforms lstm another percent across languages. simply replacing correction model single layer lstm observe even improvements. target language closest language models monolingual adapted multilingual closest language hybrid models lstm pacrnn-dnn pacrnn-lstm hybrid models closest language initialization lstm pac-rnn-dnn pac-rnn-lstm adapt target language. lower part table summarizes results. shown lstm models perform signiﬁcantly better baseline system. using pac-rnn model yields noticeable improvement lstm. similarly pac-rnn-lstm improve results. paper explored pac-rnn model lowresource language speech recognition. results multiple languages demonstrated pac-rnn achieves better performance dnns lstms. also showed replacing correction model pac-rnn lstm could enhance model. moreover multilingual experiment results show traditional transfer learning approaches also applied pac-rnn architecture. future work includes applying pac-rnn tasks conventional models work well extending predicting additional information speech signal speaker speaking rate noise. authors would like thank everyone babelon team feedback support various resources. work uses following language packs cantonese turkish pashto tagalog vietnamese assamese bengali zulu tamil cebuano kurmanji swahili guenther perkell neural model speech production application studies role auditory feedback speech speech motor control normal disordered speech senior beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association t¨uske golik nolden schl¨uter data augmentation feature combination multilingual neural networks improve performance low-resource languages proc. interspeech eversole seltzer guenter kuchaiev seide wang droppo huang zhang zweig rossbach currey stolcke slaney introduction computational networks computational network toolkit tech. rep. microsoft research http//cntk.codeplex.com. povey ghoshal boulianne burget glembek goel hannemann motl´ıˇcek qian schwarz silovsk´y stemmer vesel´y kaldi speech recognition toolkit proc. asru", "year": 2015}