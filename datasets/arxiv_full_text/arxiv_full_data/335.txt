{"title": "Pose-Selective Max Pooling for Measuring Similarity", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we deal with two challenges for measuring the similarity of the subject identities in practical video-based face recognition - the variation of the head pose in uncontrolled environments and the computational expense of processing videos. Since the frame-wise feature mean is unable to characterize the pose diversity among frames, we define and preserve the overall pose diversity and closeness in a video. Then, identity will be the only source of variation across videos since the pose varies even within a single video. Instead of simply using all the frames, we select those faces whose pose point is closest to the centroid of the K-means cluster containing that pose point. Then, we represent a video as a bag of frame-wise deep face features while the number of features has been reduced from hundreds to K. Since the video representation can well represent the identity, now we measure the subject similarity between two videos as the max correlation among all possible pairs in the two bags of features. On the official 5,000 video-pairs of the YouTube Face dataset for face verification, our algorithm achieves a comparable performance with VGG-face that averages over deep features of all frames. Other vision tasks can also benefit from the generic idea of employing geometric cues to improve the descriptiveness of deep features.", "text": "abstract. paper deal challenges measuring similarity subject identities practical video-based face recognition variation head pose uncontrolled environments computational expense processing videos. since frame-wise feature mean unable characterize pose diversity among frames deﬁne preserve overall pose diversity closeness video. then identity source variation across videos since pose varies even within single video. instead simply using frames select faces whose pose point closest centroid k-means cluster containing pose point. then represent video frame-wise deep face features number features reduced hundreds since video representation well represent identity measure subject similarity videos correlation among possible pairs bags features. ofﬁcial video-pairs youtube face dataset face veriﬁcation algorithm achieves comparable performance vgg-face averages deep features frames. vision tasks also beneﬁt generic idea employing geometric cues improve descriptiveness deep features. paper interested measuring similarity source variation among videos subject identity particular. motivation work followed. given face video visually affected confounding factors identity head pose compare another video hopefully measuring similarity subject identity even frame-level feature characterizes mixed information. indeed deep features convolutional neural networks trained face images identity labels generally robust variation head pose refers face’s relative orientation respect camera primary challenge uncontrolled environments. therefore emphasis paper deep learning frame-level features. instead care improve video-level representation’s descriptiveness rules confusing factors induces similarity factor interest treat frame-level feature vector video random vector assume highly-correlated feature vectors identically distributed. task represent whole image sequence instead modeling temporal dynamics state transition sample mean variance approximate true distribution implicitly assumed normal distribution. fig. example chosen faces. shows ﬁrst frames -frame sequence woody allen looks right sometimes. time face slightly slanting. bottom frames selected according variation poses. disclaimer source owning youtube video allows republishing face images. assumption might hold given natural image statistics untrue particular video. even features gaussian random vectors taking mean makes sense frame-level feature characterizes identity. variation identity video construction. however even face features still normally contain identity pose cues. surely feature mean still characterize identity pose. even worse decouple cues take mean. instead want video feature represent subject identity better preserve overall pose diversity likely exists among frames. disregarding minor factors identity source variation across videos since pose varies even within single video. proposed frame selection algorithm retains frames preserve pose diversity. based selection design algorithm compute identity similarity sets deep face features pooling correlation. instead pooling frames frame selection algorithm highlighted ﬁrstly pose quantization k-means pose selection using pose distances k-means centroids. reduces number features tens hundreds still preserving overall pose diversity makes possible process video stream real time. fig. shows example sequence youtube face dataset algorithm also serves sample video frames frames chosen pool single number similarity videos many pairs images. metric pool many correlations normally mean max. taking essentially ﬁnding nearest neighbor typical metric measuring similarity closeness point sets. work correlation bags frame-wise features employed measure likely videos represent person. video represented single frame’s feature induces nearest neighbors sets selected frames treat frame data point. essentially pairwise pooling process. ofﬁcial video-pairs dataset algorithm achieves comparable performance state-of-the-art averages deep features frames. fig. analysis rank- identiﬁcation varying poses google’s facenet recently established megaface million face benchmark examined primary variation looking left/right inducing proﬁle. colors represent identiﬁcation accuracy going white color indicates combinations poses exist test set. distractors distractors. ﬁgure adapted megaface’s facescrub results. cosine similarity correlation well-deﬁned metrics measuring similarity images. simple adaptation videos randomly sampling frame video. however correlation random image samples might characterize cues identity existing works measuring similarity videos using manifold-to-manifold distance however straightforward extension image-based correlation preferred simplicity temporal mean pooling impact different spatial pooling methods mean pooling pooling pooling discussed literature however pooling time domain straightforward spatial pooling. frame-wise feature mean straightforward video-level representation robust statistic. despite that temporal mean pooling conventional represent video average pooling video-level representation mean encoding face recognition feature averaging action recognition mean pooling video captioning measuring similarity subject identity useful face recognition face veriﬁcation sure face identiﬁcation well. face veriﬁcation decide whether modalities containing faces represent person different people thus important access control re-identiﬁcation tasks. face identiﬁcation involves one-to-many similarity namely ranked list one-to-one similarity thus important watch-list surveillance forensic search tasks. identiﬁcation gather information speciﬁc individuals recognized test time image group images presented deep learning face veriﬁcation number benchmarks labeled face wild dataset well solved deepface deepid facenet visual geometry group university oxford released deep face model called vgg-face descriptor also gives comparable performance lfw. however real world pictures often taken uncontrolled environment considering number image parameters allowed vary simultaneously logical consider divide-and-conquer approach studying source variation separately keeping variations constants control experiment. separation variables widely used physics biology multivariate problems. data-driven machine learning seems remain variations realistic data given idea letting deep neural networks learn variations existing enormous amount data. example facenet trained using private dataset subjects indeed robust poses illustrated fig. however features conventional networks suach deepface vgg-face normally not. moreover unconstrained data fused variations contain biases towards factors identity since feature might characterize mixed information identity low-level factors pose illumination expression motion background. instance pose similarities normally outweigh subject identity similarities leading matching based pose rather identity. result critical decouple pose identity. facial expression confuses identity well also necessary decouple too. paper face expression considered minor compared pose. similarly want measure similarity face expression need decouple identity. example facial expression recognition class training data formed face videos expression across different people. moreover many different application scenarios face veriﬁcations. web-based applications veriﬁcation conducted comparing images images. images person taken different time different conditions. identity high-level factors gender ethnicity considered paper remain video. online face veriﬁcation alive video rather still images used. speciﬁcally existing video-based veriﬁcation solutions assume gallery face images taken controlled conditions however gallery often built uncontrolled. practice camera could take picture well capture video. information describing identities video image using fully live video stream require expensive computational resources. normally need video sampling temporal sliding window. section explain treatment particularly real-world images various head poses images ytf. many existing methods make certain assumption holds faces properly aligned. fig. example pose space. shown -frame woody allen sequence ytf. three axises represent rotation angles pitch roll respectively. primary variation turning left/right inducing proﬁle. pattern exists pose distribution obviously clusters sequence extreme case reducing computation construction video contains single subject. video formalised frames frame contains face. given homography correspondence facial landmarks entirely possible estimate rotation angles face frame. concretely head pose estimator gives rotation-angle vector pose estimation would like select frames signiﬁcant head poses. intuition preserve pose diversity downsampling video time domain. learn fig. google’s facenet face features learned deep trained identity-labelled data invariant head poses long training inputs particular identity class include almost possible poses. also true minor source variations illumination expression motion background among others. then identity source variation across classes since factor identity varies even within single class. without huge training data google instead hope testing inputs particular identity class include poses diverse possible. straightforward full video indeed preserves possible pose variations video computing deep features frames computationally expensive. taking representing line coordinate system example needs either parameters intercept gradient points line. similarly problem becomes compact pose representation testing video involves following criteria. first pose representation compact terms non-redundancy closeness. non-redundancy hope retain frames possible. pose closeness observe fig. certain patterns exist head pose distribution close points turn cluster together. observation occurs sequences well. result want select frames video clustering head poses. widely-used k-means clustering aims partition point subsets minimize within-cluster squared distances treat cluster class want minimize intra-class within-cluster distance. second pose representation representative terms diversity intuitively want retain faces poses different possible. treat frame’s estimated pose point approximate polygon formed selected points close true polygon formed points possible. measure diversity using selected points want maximize inter-class between-cluster distance. pose observations partition observations disjoint subsets minimize within-cluster well maximize between-cluster still minimizing number clusters mean points respectively. objective differs k-means considering between-cluster distance makes similar multi-class however still essentially k-means. solve really need alternative minimization limited number choices empirically enumerated cross validation. ﬁxed solving eqn. follows similar procedure multi-class mixture classes clusters every point hard-assigned single cluster done k-means. subsequent selection poses straightforward selected poses form subset m-dimensional k-sparse impulse vector binary values indicating whether index chosen respectively. selection frames follow index activation vector well. selection reduces number images required represent face tens hundreds preserving pose diversity considered formation clusters. frontalize chosen faces called face alignment pose correction/normalization. operations summarized algorithm note landmarks perfectly aligned. priority given salient ones center corners nose mouth corners chin. properties symmetry also preserved. example mirror detected horizontally. however proﬁle frontalized. input output pose-corrected down-sampled face video landmark detection detect facial landmarks frame correspondence frames known. homography estimation estimate approximate model sequence faces known correspondence landmarks. pose estimation compute rotation angles frame using landmark correspondence obtain sequential head poses pm}. pose quantization cluster subsets solving eqn. estimated pose centroids might pseudo pose pose selection cluster compute distances pose point pose centroid select closest pose point represent cluster selected poses form subset index activation vector. face selection follow select frames form subset face alignment warp face according landmarks ﬁxed canonical positions. section explain correlation guided pooling deep face features verify whether selected frames able well represent identity regardless pose variation. face alignment feature descriptor function maps corrected dimensionality unit euframe clidean norm. video represented normalized frame-wise also arrange features veriﬁed able produce features well representing identity information. layers including several stacked convolution-pooling layer fully-connected layer softmax layer. since model trained face identiﬁcation purpose respect identities output second last fully-connected layer feature descriptor returns -dim feature vector input face. given pair videos subject respectively want measure similarity since claim proposed features well represent identity instead measure similarity sets features deﬁned correlation among possible pairs features namely element correlation matrix fig. pooling correlation matrix axis coordinates time step video. gives example different subjects bottom shows person. responses highlighted boxes. faces shown copyright consideration. pairs sec. need compute correlations pool single number similarity measure. time domain also serves pushing images image. metric mean median majority histogram mean widelyused. insight taking mean frame highly correlated another video usually appear twice temporal sliding window. plot bags features common feature space similarity essentially closeness sets points. sets non-overlapping measure closeness points sets distance nearest neighbors essentially pooling correlation. similar spatial pooling invariance taking correlation matrix shown fig. preserves temporal invariance largest correlation appear time step among selected frames. since identity consistent video claim videos contain similar person long pair frames video highly correlated. computation videos’ identity similarity summarized algorithm input output similarity score subject identity. face selection alignment algorithm video obtain frames faces aligned. deep video representation generate deep face features frames obtain sets features pooling correlation compute similarity according eqn. face detection frame-by-frame detection using dlib’s hog+svm based detector trained cropped face images lfw. works better faces wild opencv’s cascaded haar-like+boosting based detector. face alignment opencv’s warpafﬁne afﬁne-warping center eyes mouth. deep face representation second last layer output vgg-face using caffe conveniece consider using matconvnet-vlfeat instead caffe. vgg-face trained using face images size average face image subtracted used veriﬁcation purpose without re-training. however average face subtraction unavailable unnecessary given inputting image. result directly input face image vgg-face network without mean face subtraction. video-based face recognition database epfl captures people facing web-cam mobile-phone camera controlled environments. however frontal faces thus university surrey university queensland capture subjects various various well-quantized poses controlled environments respectively. since poses well quantized hardly verify pose quantization selection algorithm them. mcgill nicta capture videos subjects surveillance videos subjects uncontrolled environments respectively. however database size small. youtube faces dataset india mvie face database collect videos people videos actors uncontrolled environments respectively. quite existing work veriﬁed imfdb. result dataset chosen verify proposed video-based similarity measure face veriﬁcation. built using names subjects included dataset search youtube videos individuals. then screening process reduced original videos subjects videos subjects. codes available https//github.com/eglxiang/vgg_face dataset available http//www.cs.tau.ac.il/˜wolf/ytfaces/ codes tutorial https//github.com/eglxiang/ytf fig. examples video-pairs. instead using full video choose faces bottom row. disclaimer ﬁgure adapted vgg-face’s presentation follows vgg-face’s republishing permission. best dataset. fig. presents receiver operating characteristic curve obtained compute video-video similarity scores. look curve ﬁrst level false positive rate bear high true positive rate another close curve towards top-left corner. namely measure area curve hope large possible. testing quite close vgg-face uses temporal mean pooling. however selective pooling strategy much fewer computation credited face selection. cross validations training. later creator sends list errors ground-truth label provides corrected list video pairs updated ground-truth labels. result proposed algorithm corrected video pairs. fig. updates curve identical result initial list. work propose frame selection algorithm identity similarity measure employs simple correlations learning. veriﬁed fast videobased face veriﬁcation achieves comparable performance vgg-face. particularly selection pooling signiﬁcantly reduce computational expense processing videos. veriﬁcation proposed algorithm include evaluation video-based face expression recognition. shown fig. assumption group sparsity might hold imperfect alignment. extended cohna-kanade dataset include mostly well-aligned frontal faces thus suitable research purpose. experiments conducted bu-dfe database contains subjects displaying acted facial expressions moderate head pose variations. generic problem underneath variable disentanglement real data take-home message employing geometric cues improve descriptiveness deep features.", "year": 2016}