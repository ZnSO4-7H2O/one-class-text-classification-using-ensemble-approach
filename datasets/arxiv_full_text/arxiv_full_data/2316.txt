{"title": "Soft-Robust Actor-Critic Policy-Gradient", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Robust Reinforcement Learning aims to derive an optimal behavior that accounts for model uncertainty in dynamical systems. However, previous studies have shown that by considering the worst case scenario, robust policies can be overly conservative. Our \\textit{soft-robust} framework is an attempt to overcome this issue. In this paper, we present a novel Soft-Robust Actor-Critic algorithm (SR-AC). It learns an optimal policy with respect to a distribution over an uncertainty set and stays robust to model uncertainty but avoids the conservativeness of robust strategies. We show convergence of the SR-AC and test the efficiency of our approach on different domains by comparing it against regular learning methods and their robust formulations.", "text": "robust reinforcement learning aims derive optimal behavior accounts model uncertainty dynamical systems. however previous studies shown considering worst case scenario robust policies overly conservative. soft-robust framework attempt overcome issue. paper present novel soft-robust actorcritic algorithm learns optimal policy respect distribution uncertainty stays robust model uncertainty avoids conservativeness robust strategies. show convergence sr-ac test efﬁciency approach different domains comparing regular learning methods robust formulations. markov decision processes commonly used model sequential decision making stochastic environments. strategy maximizes accumulated expected reward considered optimal learned sampling. however besides uncertainty results stochasticity environment model parameters often estimated noisy data change testing second type uncertainty signiﬁcantly degrade performance optimal strategy model’s prediction. robust mdps proposed address problem framework transition model assumed belong known uncertainty optimal strategy learned worst parameter realizations. although robust approach computationally efﬁcient example consider business scenario agent’s goal make much money possible. either create startup make fortune also result bankruptcy. alternatively choose live school teaching almost risk reward. choosing teaching strategy agent overly conservative account opportunities invest promising projects. claim could relax conservativeness construct softer behavior interpolates aggressive robust. ideally soft-robust agent stay agnostic outside ﬁnancing uncertainty still able take advantage startup experience. type dilemma found various domains. ﬁnancial market investors seek good trade-off risk high returns regarding portfolio management strategic management product ﬁrms must choose amount resources innovation. conservative strategy would consist innovating necessary conditions paper focus learning soft-robust policy incorporating soft-robustness online actor-critic algorithm show convergence properties. existing works mitigate conservativeness robust either introducing coupled uncertainties assuming prior information uncertainty dynamic programming techniques estimate robust policy. however methods present limiting restrictions non-scalability ofﬂine estimation. besides computationally efﬁcient batch learning online algorithm signiﬁcant interest timestep agent state chooses action according stochastic policy maps state probability distribution action space denoting distributions gets reward brought state probability policy-gradient policy-gradient methods commonly used learn agent policy. policy parametrized estimated optimizing objective function using stochastic gradient descent. typical objective considered average reward function reward time aperiodic irreducible transition model agent operates stationary distribution markov process induced policy gradient objective previously shown expected differential reward associated state-action pair gradient used update policy parameters according βt∇θjp positive step-size actor-critic algorithm theoretical analysis empirical experiments shown regular policy-gradient methods present major issue namely high variance gradient estimates results slow convergence inefﬁcient sampling first proposed barto actor-critic methods attempt reduce variance using critic estimates value function. borrow elements value function policy-based methods. value function estimate plays role critic helps evaluating performance policy. policy-based methods actor uses signal update policy parameters direction gradient estimate performance measure. appropriate conditions resulting algorithm tractable converges locally optimal policy deep q-networks deep q-networks proven capability solving complex learning tasks atari video games qlearning watkins dayan typically learns robust mdps detect non-adversarial stateactions pairs along trajectory result less conservative results something cannot performed solving planning problem works attempted incorporate robustness online algorithm policy optimization although approaches deal large domains sampling procedure required critic estimate tamar differs strictly-speaking actor-critic. mankowitz authors introduce robust version actor-critic policy-gradient convergence results shown actor updates. moreover works target robust solution conservative. review existing methods section compare approach. best knowledge proposed work ﬁrst attempt incorporate soft form robustness online algorithm convergence guarantees besides computationally scalable. deal curse dimensionality using function approximation parameterizes expected value within space much smaller dimension state space. ﬁxing distribution uncertainty induced softrobust actor-critic learns locally optimal policy online manner. mild assumptions distributions uncertainty show novel soft-robust actor-critic algorithm converges. speciﬁc contributions soft-robust derivation objective function policy-gradient; sr-ac algorithm uses stochastic approximation learn variant distributionally robust policy online manner; convergence proofs sr-ac; experiment framework different domains shows efﬁciency soft-robustness. proofs found appendix. section introduce background material related soft-robust approach. robust robust tuple ﬁnite state-space ﬁnite actions immediate reward function deterministic bounded transition matrices. assume structured known rectangularity assumption given state uncertainty family transition models represent vectors transition probabilities action arranged block. interchangeably term soft-robust expected differential reward soft-robust value function. soft-robust stationary distribution performance objective cannot written expectation reward stationary distribution added measure transition models. deﬁne average transition model ep∼ω. corresponds transition probability results distributing transition models according analogy transition probability minimizes reward given state action robust transition function average model rather selects expected distribution uncertainty state action. assumption show transition deﬁned irreducible aperiodic ensures existence unique stationary denote ¯dπ. proposition assumption average transition matrix ep∼ω irreducible aperiodic. particular admits unique stationary distribution. regular mdps soft-robust average reward satisﬁes poisson equation ﬁrst stated discounted reward case lemma mannor following proposition reformulates result average reward. proposition poisson equation enables establish equivalence expectation stationary distributions uncertainty stationary distribution average transition model naming indeed following ep∼ω non-linear function neural network used approximator q-function. referred q-network. agent trained optimizing induced loss function thanks stochastic gradient descent. like actor-critic online algorithm aims ﬁnding optimal policy. main difference actor-critic off-policy learns greedy strategy following arbitrary behavior soft-robust framework unlike robust mdps maximize worst-case performance prior transition models distributed uncertainty set. distribution denoted structured structure mannor intuitively thought adversary distributes different transition models. product structure means adversarial distribution depends current state agent without taking account whole trajectory. deﬁnes probability distribution independently state. assume non-diffuse. implies uncertainty non-trivial respect sense distribution affect zero mass models. soft-robust objective throughout paper make following assumption assumption policy markov chains resulting mdps transition laws irreducible aperiodic. distribution introduces softer form robustness objective function averages uncertainty instead considering worst-case scenario. also gives ﬂexibility level robustness would like keep. robust strategy would consist putting mass pessimistic transition models. likewise distribution puts mass target model would lead aggressive behavior result model misspeciﬁcation. improve gradient estimate reducing variance. direct method subtract baseline previous gradient update. easy show affect gradient derivation. particular bhatnagar proved value function minimizes variance. therefore proper baseline choose. thus write following section present sr-ac algorithm deﬁned algorithm novel approach incorporates variation distributional robustness online algorithm effectively learns optimal policy scalable manner. mild assumptions resulting two-timescale stochastic approximation algorithm converges locally optimal policy. uncertainty nominal model without uncertainty provided inputs. practice nominal model uncertainty respectively estimate transition model resulting data sampling corresponding conﬁdence interval. distribution uncertainty also provided. corresponds prior information uncertainty set. step-size sequences consist small non-negative numbers properly chosen user policy-gradient methods consider class parametrized stochastic policies estimate gradient objective function respect policy parameters order update policy direction estimated gradient optimal parameters thus obtained denoted clear context omit subscript notation ease. make following assumption standard policy-gradient litterature assumption mapping continuously differentiable respect order manage large state spaces also introduce linear approximation deﬁne ψxa. sutton showed features satisfy compatibility condition approximation locally optimal place still point roughly direction true gradient. case soft-robust average reward deﬁnes soft-robust gradient update possesses ability incorporate function approximation stated following result. main difference sutton combine dynamics system distributed transitions uncertainty set. theorem linear update soft-robust average reward critic based estimate soft-robust tderror detail further. setting soft-robust value function plays role critic according actor parameters updated. exploit critic improve policy updating policy parameters direction gradient estimate soft-robust objective process repeated convergence. consider unbiased estimates respectively. calculating requires estimate soft-robust average-reward obtained averaging samples given immediate reward distribution order estimate soft-robust differential value linear function approximation. considering ddimensional feature extractor state space approximate ddimensional parameter vector tune using linear results following soft-robust td-error ˆjt+ convergence algorithm established applying theorem bhatnagar exploits borkar’s work two-timescale algorithms convergence result presented theorem theorem previous assumptions given exists parameter vector obtained using algorithm supπteπt sr-ac algorithm converges almost surely \u0001-neighborhood local maximum consider simpliﬁed formulation startup teaching dilemma described section problem modeled -state action corresponds strategy. illustration construction given graphically figure starting state agent chooses three actions. action lead high reward case success catastrophic case failure. action leads positive reward case success possibility negative reward. action lead intermediate positive reward slight risk negative reward. depending action chose succeeded agent brought right-hand states receives corresponding reward. brought back episode. assume probability success three actions. -tuple represents cart position cart speed pole angle respect vertical angular speed respectively. agent make possible actions apply constant force either right left pole. gets positive reward pole fallen stayed boundary sides screen. terminates agent receives reward since episode lasts timesteps maximal reward agent episode. experiment generate uncertainty training. single-step sample different probabilities success using uniform distribution cart-pole sample different lengths normal distribution centered nominal length pole length thus generates different transition function. sample average model ﬁxing realization dirichlet distribution. soft-robust update applied taking optimal action according average transition function. trained agent nominal model experiment. soft-robust agent learned using sr-ac single-step mdp. cart-pole soft-robust version algorithm. soft-robust analyze performance sr-ac training soft-robust agent single-step mdp. regular algorithm derive aggressive policy learn robust behavior using robust formulation consists replacing td-error robust td-error implemented mankowitz derived soft-robust agent compared resulting aggressive robust strategies respectively. soft-robust robustness already incorporated q-network addressed performs online estimation q-function minimizing timestep following robust td-error experiment train agent nominal model incorporate soft-robustness learning. soft-robust policy learned thanks sr-ac single-step mdp. linear function approximation features estimate value function. cartpole using neural network fullyconnected hidden layers weights layer relu activations. chose adam optimizer minimize induced loss functions. used constant learning rates worked well practice. agent trained mtrain episodes. testing averaged performance mtest episodes parameter setting. hyper-parameter values found appendix. shashua mannor mankowitz however outperformed soft-robust agent around nominal model. furthermore soft-robust strategy shows equilibrium aggressiveness robustness thus leading better performance non-robust agent larger pole lengths. trained soft-robust agent weighting distributions remarked depending structure soft-robustness interpolates aggressive robust behaviors figure comparison robust soft-robust aggressive agents training. training epoch corresponds episodes. soft-robust strategy conservative robust one. figure shows evolution performance three agents training. becomes stable along training time conﬁrms convergence sr-ac particular. aggressive agent performs best highest reward reach nominal model. soft-robust agent gets rewards aggressive robust agent performs worst pessimistic learning method. evaluation strategy represented figure probability success gets performance aggressive agent drops robust soft-robust agents although performs best probability success gets close robust agent stays stable independently parameters underperforms soft-robust agent presents best balance high reward risk. remarked depending weighting distribution initially soft-robustness tends less aggressive. fact high mass probabilities success leads less aggressive strategies incorporating distribution uncertainty thus gives signiﬁcant ﬂexibility level aggressiveness assigned soft-robust agent. cart-pole figure show performance three strategies different values pole length. similarly previous example non-robust agent performs well around nominal model reward degrades extreme values pole length. robust agent keeps stable reward model uncertainty consistent results obtained di-castro figure average reward different probabilities success. soft-robust policy less sensitive uncertainty aggressive policy performs better robustness optimistic scenarios. paper related several domains robust distributionally robust mdps actor-critic methods online learning stochastic approximation algorithms. work solves problem conservativeness encountered robust mdps incorporating variational form distributional robustness. sr-ac algorithm combines scalability large scale state-spaces online estimation optimal policy actor-critic algorithm. table compares proposed algorithm previous approaches. many solutions addressed mitigate conservativeness robust mdp. mannor relax state-wise independence property uncertainty assume coupled planning problem stays tracktable. another approach tends assume priori information parameter set. methods include distributionally robust mdps optimal strategy maximizes expected reward adversarial distribution uncertainty set. ﬁnite known mdps structural assumptions considered distributions max-min problem reduces classical robust mdps solved efﬁciently dynamic programming however besides becoming untracktable largesized mdps methods ofﬂine learning approach cannot adapt level protection model uncertainty lead overly conservative results work solutions issue addresses online algorithm learns transitions purely stochastic adversarial. although ensures less conservative results well regret method sticks robust objective strongly relying ﬁnite structure state-space. alleviate curse dimensionality incorporate function approximation objective value deﬁne linear functional features. first introduced barto later addressed bhatnagar actor-critic algorithms online learning methods ﬁnding optimal policy. used formulation bhatnagar baseline algorithm proposed. difference work incorporate soft-robustness. relates sense bayesian actor-critic setup critic returns complete posterior distribution value functions using bayes’ rule study keeps frequentist approach meaning algorithm updates return point estimates average value-function prevents tracktability issues besides enabling distribution ﬂexible. another major distinction bayesian approach incorporates prior distribution model parameters whereas soft-robust method considers prior different transition models uncertainty set. mankowitz tamar authors incorporate robustness policy-gradient methods. sampling procedure required critic estimate tamar differs strictlyspeaking actor-critic. robust version actor-critic policy-gradient introduced mankowitz convergence guarantees shown robust policy-gradient ascent. methods target robust strategy whereas seek soft-robust policy less conservative protecting model uncertainty. presented sr-ac framework able learn policies keep balance aggressive robust behaviors. sr-ac requires stationary distribution average transition model compatibility conditions deriving soft-robust policy-gradient. shown ensures convergence sr-ac. ﬁrst work attempted incorporate soft form robustness online actor-critic method. approach shown computationally scalable large domains computational price. experiments also shown soft-robust agent interpolates aggressive robust strategies without overly conservative leads outperform robust policies model uncertainty. subsequent experiments test efﬁciency chosen weighting uncertainty thought adversary distributes different transition laws. current setting adversarial distribution stays constant without accounting rewards obtained agent. future work address problem learning sequential game induced evolving adversarial distribution derive optimal soft-robust policy. extensions work also consider non-linear objective functions higher order moments respect adversarial distribution.", "year": 2018}