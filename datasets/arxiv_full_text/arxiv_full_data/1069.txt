{"title": "Robust Stochastic Configuration Networks with Kernel Density Estimation", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Neural networks have been widely used as predictive models to fit data distribution, and they could be implemented through learning a collection of samples. In many applications, however, the given dataset may contain noisy samples or outliers which may result in a poor learner model in terms of generalization. This paper contributes to a development of robust stochastic configuration networks (RSCNs) for resolving uncertain data regression problems. RSCNs are built on original stochastic configuration networks with weighted least squares method for evaluating the output weights, and the input weights and biases are incrementally and randomly generated by satisfying with a set of inequality constrains. The kernel density estimation (KDE) method is employed to set the penalty weights for each training samples, so that some negative impacts, caused by noisy data or outliers, on the resulting learner model can be reduced. The alternating optimization technique is applied for updating a RSCN model with improved penalty weights computed from the kernel density estimation function. Performance evaluation is carried out by a function approximation, four benchmark datasets and a case study on engineering application. Comparisons to other robust randomised neural modelling techniques, including the probabilistic robust learning algorithm for neural networks with random weights and improved RVFL networks, indicate that the proposed RSCNs with KDE perform favourably and demonstrate good potential for real-world applications.", "text": "neural networks widely used predictive models data distribution could implemented learning collection samples. many applications however given dataset contain noisy samples outliers result poor learner model terms generalization. paper contributes development robust stochastic conﬁguration networks resolving uncertain data regression problems. rscns built original stochastic conﬁguration networks weighted least squares method evaluating output weights input weights biases incrementally randomly generated satisfying inequality constrains. kernel density estimation method employed penalty weights training samples negative impacts caused noisy data outliers resulting learner model reduced. alternating optimization technique applied updating rscn model improved penalty weights computed kernel density estimation function. performance evaluation carried function approximation four benchmark datasets case study engineering application. comparisons robust randomised neural modelling techniques including probabilistic robust learning algorithm neural networks random weights improved rvfl networks indicate proposed rscns perform favourably demonstrate good potential real-world applications. many real-world applications sample data collected various sensors contaminated noises outliers makes troubles building neural networks sound generalization. past years robust data modelling techniques received considerable attention ﬁeld applied statistics machine learning well known cost function plays important role robust data modelling. m-estimator hampels hyperbolic tangent estimates employed cost function aiming alleviate negative impacts outliers modelling performance. assumption additive noise output follows cauchy distribution mean squared error used cost function robust learning algorithm based m-estimator cost function random sample consensus proposed deal outliers algorithm successfully applied computer vision image processing besides methods mentioned above results robust data regression using support vector machine reported svm-based approaches demonstrate limits handle uncertain data regression problems higher level outliers. back-propagation algorithms training neural networks suffer many shortcomings learning parameter setting slow convergence local minima. thus useful develop advanced learning techniques resolving data regression problems particular stream data online data modelling tasks. background randomized methods training neural networks developed last decades readers refer recently published survey paper details milestones topic studies robust data modelling techniques based random vector functional-link networks reported speciﬁcally hybrid regularization model assumption sparsity outliers used training process probabilistic robust learning algorithm neural networks random weights proposed however learning parameters used prnnrw must properly quite difﬁcult done practice. improved version rvfl networks built using kde-based weighted cost function suggested. unfortunately signiﬁcance scope setting random weights biases rvfl networks addressed. looked practical issues common pitfalls rvfl networks clearly revealed impact scope setting modelling performance rvfl networks. ﬁndings reported motivates investigate robust data regression problem using advanced randomized learner model termed stochastic conﬁguration networks built incrementally assigning random weights biases supervisory mechanism paper aims develop robust version scns uncertain data regression. based construction process scns utilise weighted least squares objective function evaluating output weights scns resulting approximation errors present model used incrementally conﬁgure hidden nodes constrained random parameters. course building rscns penalty weights representing degree contribution individual data samples objective function updated according newly constructed function. work alternating optimization technique employed implement rscn model. proposed algorithm termed rsc-kde evaluated using function approximation four benchmark datasets different levels artiﬁcial outliers engineering application experimental results indicate proposed rsc-kde outperforms existing methods terms effectiveness robustness. remainder paper organized follows section brieﬂy reviews stochastic conﬁguration networks kernel density estimation method. section details proposed rsc-kde algorithm. section reports experimental results comparisons discussion. section concludes paper remarks. section reviews proposed framework input weights biases randomly assigned light supervisory mechanism output weights evaluated solving linear least squares problem. details scns read special case real-valued function deﬁned norm becomes |dx)/ inner product becomes ψψdx. given target function suppose already built single layer feed-forward network hidden nodes current residual error denoted reach acceptable tolerance level. framework scns provides effective solution leading βlgl residual error falls expected tolerance construction process scns starts small sized network incrementally adds hidden nodes followed computing output weights. procedure keeps going model meets certain termination criterion. remarkable merits scns summarized follows requirement prior knowledge architecture constructed network given task; scope random weights biases adjustable automatically determined data rather ﬁxed setting end-users; input weights biases randomly assigned inequality constraint guarantee universal approximation property. remark past decades randomized methods training neural networks suffer misunderstandings constraint random assignment unfortunately many published works authors blindly wrongly randomness constructing randomized learner models mindlessly made misleading statements without scientiﬁc justiﬁcation pointed universal approximation theorems rvfl networks established fundamental signiﬁcant randomized learning theory. however theoretical results cannot provide practical useful guidance structure learning parameter settings algorithm design implementation aspects. proposed framework ﬁrstly touches base randomized learning techniques draws researches topic right tracks proper uses constrained random parameters. indeed inequalities proposed originally essential ensure universal approximation property. algorithm implementation perspectives scheme prevent scns over-ﬁtting must place. usually done machine learning performance validation used terminate learning process. section brieﬂy introduce kernel density estimation method weighting contributions training data learning process. basically kernel density estimator computes smooth density estimation data samples placing sample point function representing contribution density. readers refer details kernel density estimation method. robust data regression seeks capable learner model successfully learn true distribution uncertain data samples. important industrial applications collected data samples always contaminated outliers caused failure measuring transmission devices unusual disturbances. section details development robust i-th penalty weight representing contribution activation function number hidden nodes input weights biases stochastically conﬁgured according theorem represents output weights. generally speaking penalty weights determined according reliability sample easy understand higher reliability means trust data correctly represents process behavior lower reliability indicates less conﬁdence sample outlier noisy one. thus decreasing penalty weights training samples lower reliability eliminate even remove negative impacts learner model building. logical thinking combine original framework wls-based learning weighted version model’s residual error process building scns. words rscn model incrementally built stochastically conﬁguring hidden parameters based redeﬁned constraint evaluating output weights using solution given training samples denoted el−m]t el−q el−q] gl]t output vector hidden node input then obtain current hidden layer output matrix remaining question assign penalty weights along process building scns. recall probability density function residuals obtained estimated reliabilities samples determined. inspired work construct probability density function residual error follows paper alternating optimization strategy applied implementing rscns includes process building model suitable penalty weights control contribution contaminated samples. whole procedure begins assigning equal penalty weights samples building model followed updating penalty weights according repeating steps alternatively certain stopping criterion reached. clariﬁed penalty weights updated round process building model completed. distinguished original framework training samples contribute equally objective function newly developed rscns treat individual samples differently emphasis data samples higher reliability indeed corresponds lager values penalty weights. means training output corrupted outliers noises sample pair provide less contribution cost function relatively small value corresponding penalty weight. remark important issue design rscns termination criterion. mentioned remark performance validation data employed stopping condition. unfortunately method make sense cannot applied building rscns presence uncertainties validation data. however validation testing criterion still used clean data extracted true data distribution available. work assume clean validation data ready used purpose. proposed rsc-kde algorithm summarized following pseudo code. section reports simulation results function approximation four benchmark datasets keel industrial application proposed rsc-kde algorithm compared given inputs outputs lmax maximum number hidden nodes expected error tolerance pmax maximum times random conﬁguration imax maximum number alternating optimization; choose positive scalars {λmin λmax}; three randomized algorithms rvfl improved rvfl probabilistic learning algorithm prnnrw comparisons conducted several scenarios different system settings learning parameters noise levels. root mean squared error used evaluate generalization capability algorithm outlier-free test datasets. addition robustness analysis setting lmax given case study. input output values normalized artiﬁcially adding certain level outliers. maximum times random conﬁguration tmax rsc-kde sigmoidal activation function used simulations. training dataset contains points randomly generated uniform distribution test dataset size generated regularly spaced grid purposely introduce outliers training dataset variable percentage data points selected randomly corresponding function values substituted background noises values uniformly distributed show advantage rsc-kde uncertain data modelling make comparisons performance among four algorithms outlier figure depicts training samples outlier percentage figure shows learners’ performance test dataset four algorithms proposed rsc-kde exhibits best performance compared three methods. rvfl improved rvfl prnnrw examined different scope settings random figure training samples used function approximation along target function shown line; approximation performance test dataset four learning algorithms randomized learners’ performance. different network architectures used pair leading favorable performance. demonstrate test results four algorithms figure average errors standard deviations rmse plotted outlier percentage. clear proposed rsc-kde algorithm outperforms methods case. particular approximation performance three algorithms worse acceptable level. obviously scope setting improper randomized learner models expected perform reported outlier percentage becomes rscn outperforms randomized learner models aligns well consequence reported given relatively higher outlier percentage example similar strategy done function approximation problem applied introduce different percentages outliers training dataset. normalized training dataset variable percentage data points selected random associated output values substituted background noises uniformly distributed range finally contaminated output values distributed instead test dataset outlier-free assessment purpose. calculate mean values standard deviations rmse different percentage outliers. figure plot comparison results rvfl improved rvfl prnnrw shows proposed rsc-kde algorithm outperforms others outlier percentage. results reported table observed proposed rsc-kde algorithm outperform others four datasets outlier percentage despite results obtained three algorithms ‘best’ ones selected results various settings speciﬁcally figure prnnrw exhibits worst accuracy stock laser concrete performs better rvfl improved rvfl treasury. also table results prnnrw obtained appropriate combination figure indeed rvfl improved rvfl prnnrw common practice determine suitable scope setting reasonable network architecture based trial-and-error method proposed rsc-kde works robustly much less human intervention parameter setting. section make investigation merits proposed rsc-kde algorithm using dataset process industry figure depicts grinding process coarse fresh ball mill conveyor. meanwhile certain amount mill water added pipe maintain proper pulp density. stage steel balls within mill crush coarse ﬁner size alone knocking tumbling actions. grinding mixed pulp includes coarser ﬁner particles discharged continuously mill spiral selector classiﬁcation assistance dilution water mixed pulp. next pulp separated overﬂow underﬂow pulp. finally underﬂow pulp coarser particles recycled back mill re-grinding whilst overﬂow pulp ﬁner particles proceeded. seen particle size estimation plays important role circling procedure. particle size estimation mineral grinding process formulated regression problem three input variables including fresh feed rate mill water rate dilution water rate single output unmodelled dynamics namely denoted input vector output respectively. represent estimate particle size case study training samples test samples collected hardware-in-the-loop platform composed following subsystems optimal setting control subsystem human supervision subsystem control subsystem virtual actuator sensors subsystem virtual operation process subsystem. detailed descriptions operational functionalities subsystems readers refer input output values normalized then different levels outliers added normalized training dataset similar done previous simulations variable percentage data points selected randomly corresponding output values corrupted background noises followed uniform distribution result output values distributed range test samples outlier-free. test performance four algorithms depicted figure presented results rvfl improved rvfl prnnrw scope setting correspond ‘best’ records among trails using different easily found proposed rsc-kde algorithm outperforms three methods cases. although improved rvfln exhibits close performance outlier percentage relatively lower rsc-kde algorithm demonstrated best robustness even high outlier contamination rate. performance prnnrw improved compared still unacceptable comparison improved rvfl rsc-kde. rvfl improved rvfln remarkable difference results interestingly figure shows results prnnrw relatively higher outlier percentages slightly better rsc-kde. however result needs suitable scope setting time-consuming practice. contrast proposed rsc-kde lead good performance others without user-oriented trials parameter setting. table summarizes records proposed rsc-kde algorithm well ‘best’ results obtained rvfl improved rvfl prnnrw ‘most appropriate’ parameter setting impact scope setting three randomized algorithms seen comparing records outlier percentage. speciﬁcally test results four algorithms shown figure real estimated values normalized particle size plotted. ending work conduct robustness analysis parameters investigate impacts performance proposed rsc-kde algorithm. test results different combination reported table respectively. much difference among results different setting implying process necessary. case rsc-kde identical original algorithm appropriate setting architecture iteration times selected percentage outliers fair accuracy rsc-kde preferable stay within stable level provided equal larger similar case appropriate setting architecture uncertain data modelling problems appear many real-world applications signiﬁcant develop advanced machine learning techniques achieve better modelling performance. paper proposes robust version stochastic conﬁguration networks problem solving. empirical results reported work clearly indicate proposed rscns extensions recently developed framework great potential dealing robust data regression problems. implementation perspective design methodology needs assumption availability clean validation dataset helps prevent learner over-ﬁtting course incrementally constructing stochastic conﬁguration networks. practice however hypothesis always applicable. thus research topic necessary. plenty explorations expected various cost functions evaluating output weights development online version rscns distributed rscns large-scale data modelling.", "year": 2017}