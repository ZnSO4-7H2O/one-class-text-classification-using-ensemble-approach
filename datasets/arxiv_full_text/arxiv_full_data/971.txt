{"title": "A Bayesian encourages dropout", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout.", "text": "dropout techniques prevent learning overﬁtting. explained dropout works kind modiﬁed regularization. here shed light dropout bayesian standpoint. bayesian interpretation enables optimize dropout rate beneﬁcial learning weight parameters prediction learning. experiment result also encourages optimization dropout. recently large scale neural networks successfully shown great learning abilities various kind tasks image recoginition speech recognition natural language processing. although likely large scale neural networks fail learn appropriate parameter huge model complexity development several learning techniques optimization utilization large dataset overcome difﬁculty learning. successful learning technique ‘dropout’ considered important suppress overﬁtting success dropout attracts many researchers’ attention several theoretical studies performed aiming better understanding dropout explain dropout kind input invariant regularization technique prove applicability applications generalized linear model contrast formally provide bayesian interpretation dropout. standpoint think dropout solve model selection problem input feature selection selection number hidden internal states bayesian averaging model. inference dropout training considered approximate inference bayesian model averaging model weighted accordance posterior distribution dropout training considered approximate learning parameter optimizes weighted likelihoods possible models i.e. marginal likelihood. interpretation enables optimization ‘dropout rate’ learning optimal weights model i.e. posterior model. optimization dropout rate beneﬁts parameter learning better approximation marginal likelihood prediction closer distribution predictive distribution. note bayesian view ‘dropout’ already seen original paper dropout regards dropout training kind approximate bayesian learning however mentioned much would dropout training standard bayesian learning optimize dropout rate. explain dropout three-layered neural network neural network ﬁrst model dropout training applied also simplest models involves input feature selection problem hidden internal units selection problem. note concept dropout easily extended learning machines. matrix ml-dimensional vector respectively. nonlinear activation function sigmoid function. parameter optimized. consider model selection problem full model represented described above possible models consist subset input features hidden units full model. introduce diagonal mask matrices i-th diagonal elements binary represent subsets. sometimes represent mask variables. possible model represented follows; determination corresponds input feature selection problem determination corresponds hidden internal units selection problem. rewrite represent explicit dependence masks well parameter possible architectures ignoring redundancy brought hierarchy architecture number becomes huge practice. poses great challenge optimization best mask i.e. best model selection. instead choosing speciﬁc binary mask standard dropout takes approach stochastically possible models stochastic optimization. algorithm summarized follows; standard dropout learning initial estimate parameter pick pair sample random. randomly mask determining every element independently according dropout rate typically denotes bernoulli distribution probability determines step size decreased properly assure convergence. case training three-layered neural network regression ||·|| denotes euclidean norm. −||yt−y denotes expected value function respect distribution distribution case must correspond ones used choosing mask matrices learning. independent bernoulli distirbution probability used choosing three-layered neural network another approximate ep)p) apply dropout neural network also models often need compute weighted many independent random variables upper script denotes transpose vector weight vector diagonal mask matrix whose diagonal elements binary random variables. either sample input hidden unit treated ﬁxed value. fast dropout approximates weighted random variables single gaussian random variable utilizing lyapunov’s central limit theorem. gaussianization improves accuracy calculation expectation saving computation time. also shown gaussian approximation beneﬁcial learning. although idea dropout originates bayesian model averaging proposed artiﬁcial corruption model dropout interpreted kind adaptive regularization analogous artiﬁcial feature corruption interpreted kind regularization. viewpoint dropout regularizer used generalized linear model seen ﬁrst-order equivalent l-regularization transforming input estimate inverse diagonal fisher information matrix input transformation regularizer scale invariant conventional l-regularizer hold desirable property. cost function used dropout training analyzed. cost function used dropout training considered average cost function submodels. analyze difference average cost function submodels cost function average submodels show dropout brings input-dependent regularization term cost function average submodels. also pointed strongest regularization effect obtained dropout rate typical value used practice. also study treats dropout bayesian viewpoint. frey extend dropout ‘standout’ dropout rate adaptively trained. propose random masks used dropout training considered random variable bayesian posterior distribution submodels shares view dropout. however proposed adaptive dropout rate depends input variable implies different mask input assume consistent mask whole dataset tries estimate best models based entire training dataset. also clear update dropout rate works leaning posterior. following section show clear interpretation dropout bayesian standpoint provide theoretically solid optimize dropout rate. difference study existing studies discussed section standard bayesian framework parameters treated random variables inferred posterior except hyper parameter. dropout training section corresponds hyper parameter mask section corresponds parameter. denote m-dimensional binary mask vector whose elements take either y··· training data set. although omit dependence simplicity possible include dependence general. introducing distribution parameter call trial distribution following conventions marginal log-likelihood written denotes kullback-leibler divergence disp posterior distribution. lower bound valid trial distribution non-negativity kullback-leibler divergence becomes tight trial distribution corresponds posterior means close posterior minimize gap. optimization lower bound stochastic gradient descent leads dropout training algorithm explained section note need randomly sample inputoutput pair mask sample mask variable obey trial distribution identical dropout training explained section need assume pzizi case parameter corresponds dropout bayesian interpretation prediction dropout training also interpret output test input learning bayesian standpoint. bayesian framework output test input inferred expected value predictive distribution given however calculation becomes often intractable since exact calculation posterior expectation respect posterior needs summation variations. therefore consider replace posterior tractable trial distribution respect presumably closer desirable cost function marginal loglikelihood moreover enables approximate predictive distribution much accurately better approximates posterior best posterior which however cannot solved analytical form cases. consider optimize certain parametric distribution family parameterized best parameter obtained maximizes lower bound ﬁrst term denote exeq pectation respect trial distribution true distribution training yt|t expectations cannot evaluated analytically cases complex dependence expectations replaced random samples according recipe stochastic gradient descent. contrast rest terms corresponding derivatives last terms θ)/t summation usually intractable variations. however independence assumption decomposes intractable summation tractable summations binary allows direct evaluation terms practice without using monte carlo method. reason last terms depend sample well yt}. scalar denotes inverse effective number samples i.e. last terms correspond derivative last terms θ)/t scheduled think amount training data increasing algorithm proceeds. noted many variants algorithm considered. example introduce momentum term update parameters mini-batch consider emlike algorithm i.e. update repeatedly performed holds proceeds update updated following standard dropout training natural choice parameterization would represents common dropout rate input features. hereafter refer dropout optimized uniform scalar uniformly optimized rate dropout could consider parameterizations example parameterize differently letting hereafter refer dropout optimized element-wise feature-wise optimized rate dropout parameter mask distribution specify distribution subset independently denotes i-th subset subset could units layer. layer-wise setting dropout rate sometimes used practice e.g. data target binary label -dimensional input vector consisting dimensional informative features rest -dimensional non-informative features. label obeys ber. informative feature generated independently according informative feature weak correlation label non-informative feature generated independently according irrelevant label here denotes gaussian distribution mean variance samples generated training another samples generated validation. performance evaluated another test samples. diagonal matrix whose diagonal elements binary masks case gaussian approximation works effectively proposed approximating qθixi whose variance applying well-known formula approximates gaussian integral sigmoid function predictive distribution obtained compared four algorithms maximum likelihood estimation standard dropout algorithm dropout rate ﬁxed bayesian dropout algorithm optimizes uniform dropout rate bayesian dropout algorithm optimizes feature-wise dropout rates optimization need determine step sizes algorithm described section properly. values scheduled +t/d. step size used updating decreased slower used updating considering this chose best parameter shows best accuracy validation data among respectively. bayesian dropout algorithm inverse effective number samples simply features relevant input difﬁcult determine best uniform dropout rate. eventually optimized uniform dropout rate becomes zero. then test accuracies dropout takes almost value. fixed dropout rate shows slightly better accuracy although difference cannot visible fig. hand optimize feature-wise dropout rate test accuracy increases getting closer bayes optimal seen fig. dropout rate informative features selectively low. note dropout shows signiﬁcant regularization effect although hyperparameter tuned. input feature selection problem hidden internal units selection problem general difﬁcult requires compare vast number models large case three-layered neural network explained section well known fact number parameters proportional relation discrepancy generalization error training error non-singular statistical models trained maximum posteriori estimation uses ﬁxed prior maximum likelihood estimation example). contrast bayesian inference rely prediction single model weighted submodel predictions weights determined according posterior submodel.the marginal likelihood integrated possible submodels take account complexity model. fact helps prevent overﬁtting type explained section asymptotic behavior bayes generalization error studies algebraic geometry reveal asymptotic behavior singular statistical model different non-singular statistical model; generalization error increase necessary proportionally number parameters. partly explains reason recent neural network categorized singular statistical model resist overﬁtting opposed regular statistical model optimized maximum likelihood estimation estimation. several studies view dropout training kind approximate bayesian learning except original study however interpretation differ interpretation lacks solid theoretical foundation. stated cost function seen lower bound marginal log-likelihood. however marginal log-likelihood deﬁned logp] lower bound independent bernoulli distribution dropout rate. similar cost function proposed propose optimize dropout rate maximize eq]. basically view hidden variable rather parameter because inferred entire training dataset inferred sample sample deﬁnition interpret computation expectation submodel prediction respect submodel posterior bayesian solve model selection problem. shall note that context neural network nothing philosophy assigning prior parameter seeking posterior achieving best bayesian prediction example assigned gaussian distribution weight matrices search optimal prior vast space probability measures indeed daunting task however. aforementioned algorithms suffer runtime. computational point view standard dropout algorithm emerges efﬁcient bayesian compromise search. particular restricts search distribution element ones write j-th unit adjacent connection words algorithm looks speciﬁc family discrete valued distributions parametrized values ˜wij. standard dropout algorithm bridges stochastic feature selection beyesian averaging. j-th unit adjacent connection discrete valued distributions parametrized ˜wij search space becomes larger method consume runtime standard dropout algorithm. allowing freedom distribution however expect trained machine much data speciﬁc. apparent discussion consider dropconnect another parameterization distribution i.e. ˜wij obeys bernoulli distribution. need also group match speciﬁc task required model. example consider family time-series prediction model vector autoregression model family number parameters grow quickly state space dimension size time making search best posterior distribution difﬁcult. again diagonal matrices entries {ber reduce number hyperparameters assuming structure example ˜pkλi introduce intended space-time correlations. type parametrization allow resolve sophisticated feature selection problems considered intractable conventional methods. hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. zeiler matthew zhang sixin yann fergus rob. regularization neural networks using dropconnect. proceedings international conference machine learning", "year": 2014}