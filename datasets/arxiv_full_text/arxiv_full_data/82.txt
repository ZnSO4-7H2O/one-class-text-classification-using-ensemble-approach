{"title": "A Recurrent Neural Model with Attention for the Recognition of Chinese  Implicit Discourse Relations", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.", "text": "introduce attention-based bi-lstm chinese implicit discourse relations demonstrate modeling argument pairs joint sequence outperform word order-agnostic approaches. model beneﬁts partial sampling scheme conceptually simple achieves state-of-the-art performance chinese discourse treebank. also visualize attention activity illustrate model’s ability selectively focus relevant parts input sequence. true text understanding goals natural language processing requires capabilities beyond lexical semantics individual words phrases. natural language descriptions typically driven inter-sentential coherent structure exhibiting speciﬁc discourse properties turn contribute signiﬁcantly global meaning text. automatically detecting meaning units organized beneﬁts practical downstream applications question answering recognizing textual entailment sentiment analysis text summarization various formalisms terms semantic coherence frameworks proposed account contextual assumptions annotation schemata penn discourse treebank chinese discourse treebank instance deﬁne discourse units syntactically motivated character spans text augmented relations pointing second argument antecedent i.e. discourse unit arg. relations labeled relation type associated discourse marker. both pdtb cdtb distinguish explicit implicit relations depending presence marker sense classiﬁcation implicit relations challenging argument pairs lack marker important feature. consider instance following example cdtb implicit conjunction sides agree talks positive results motivation previous work implicit sense labeling heavily feature-rich requires domainspeciﬁc semantic lexicons recently resource-lean architectures proposed. promising neural methods attempt infer latent representations appropriate implicit relation classiﬁcation unfortunately models evaluated four top-level senses—sometimes even inconsistent evaluation setups. furthermore systems initially designed english pdtb involve complex taskspeciﬁc architectures discourse modeling techniques chinese received little attention literature still seriously underrepresented terms publicly available systems. more words chinese discourse relations implicit—compared english recently context conll shared task ﬁrst independent evaluation platform beyond class level established. surprisingly best performing neural architectures date standard feedforward networks wang schenk even though speciﬁc models completely ignore word order within arguments feedforward architectures claimed rutherford generally outperform thoroughly-tuned recurrent architecture. contribution work release ﬁrst attention-based recurrent neural sense classiﬁer speciﬁcally developed chinese implicit discourse relations. inspired zhou system practical adaptation recent advances relation modeling extended novel sampling scheme. contrary previous assertions rutherford model demonstrates superior performance traditional bag-of-words approaches feedfoward networks treating discourse arguments joint sequence. evaluate method within independent framework show performs well beyond standard class-level predictions achieving stateof-the-art accuracy cdtb test set. illustrate model’s attention mechanism provides means highlight parts input sequence relevant classiﬁcation decision thus enable better understanding implicit discourse parsing problem. proposed network architecture ﬂexible largely language-independent operates word embeddings. stands structural simplicity builds solid ground development towards textual domains. schmidhuber lstm) network predict senses discourse relations. model draws upon previous work lstm particular bidirectional mode operation attention mechanisms recurrent models combined techniques entity relation recognition annotated sequences speciﬁcally model ﬂexible recurrent neural network capabilities sequentially inspect tokens highlight parts input sequence informative discourse relation recognition task using weighting provided attention mechanism. furthermore model beneﬁts novel sampling scheme arguments elaborated below. system learned end-to-end manner consists multiple layers illustrated figure first token sequences taken input special markers inserted corresponding positions inform model start points argument spans. ensure general ﬂexibility modeling discourse units could easily extend additional context instance. experiments implicit arguments tokens respective spans considered. note that unlike previous works approach models arg-arg pairs joint sequence ﬁrst compute intermediate representations arguments separately. second input layer encodes tokens using one-hot vector representations subsequent embedding layer provides dense representation serve input recurrent layers. embedding layer initialized using pre-trained word vectors case -dimensional chinese gigaword vectors embeddings tuned network trained towards prediction task. embeddings unknown tokens e.g. markers trained backpropagation only. note that tokens markers pre-trained vectors represent source information prediction task. recurrent setup layer lstm networks bidirectional manner order better capture dependencies parts input sequence inspection left righthand-side contexts time step. lstm holds state representation continuous vector passed subsequent time step capable modeling long-range dependencies gated memory. forward backward lstms traverse sequence producing sequences vectors respectively summed together resulting sequence vectors reduced single vector ﬁnal softmax output layer order classify sense label discourse relation. vector obtained either ﬁnal vector produced lstm pooling using attention i.e. weighted model somewhat difﬁcult optimize using attention provides added beneﬁt interpretability weights highlight extent classiﬁer considers lstm state vectors token modeling. particularly interesting discourse parsing previous approaches provided little support pinpointing driving features argument span. partial argument sampling purpose enlarging instance space training items cdtb thus order improve predictive performance model propose novel partial sampling scheme arguments whereby model trained validated sequences containing arguments well single arguments. data point being token sequence argument expanded duplicate bi-argument samples balance frequencies single-argument samples. lines motivation support inclusion single argument training examples grounded linguistics machine learning respectively. first shown single arguments isolation evoke strong expectation towards certain implicit discourse relation demberg particular rohde horton psycholinguistic study implicit causality verbs. second procedure encourage model learn better representations individual argument spans support modeling arguments composition lecun aspects believe data augmentation technique effective reinforcing overall robustness model. implementational details train model using ﬁxed-length sequences tokens zero padding beginning shorter sequences truncate longer ones. lstm vector dimensionality matching embedding size. model regularized dropout rate layers weight decay lstm inputs. employ adam optimization using cross-entropy loss function mini batch size evaluate recurrent model conll shared task data include ofﬁcial training development test sets cdtb; table overview implicit sense distribution. accordance previous setups treat entity relations implicit exclude altlex relations. evaluation focus sense-only track subtask gold arguments provided system supposed label given argument pair correct sense. results shown table note that cdtb implicit relations appear almost three times often explicit relations. these appear within sentence. finally relations training labels. tions test outperforming best feedforward system wang word order-agnostic approaches. development test performances suggest robustness approach ability generalize unseen data. ablation study perform ablation study quantitatively assess contribution characteristic aspects model. first compare attention mechanism simpler alternative feeding ﬁnal lstm hidden vectors directly output layer. attention turned yields absolute decrease performance test substantial signiﬁcant according welch two-sample t-test second independently compare partial sampling scheme training standard argument pairs cdtb. here absence partial sampling scheme yields absolute decrease accuracy demonstrates importance achieving competitive performance task. performance pdtb side experiment investigate model’s language independence applying implicit argument pairs english pdtb. computational time constraints optimize hyperparameters instead train model using identical settings chinese expected lead suboptimal performance evaluation data. nevertheless measure accuracy pdtb test shows model potential generalize across implicit discourse relations different language. figure visualization attention weights chinese characters high intensities. underlined english phrases semantically structure-shared arguments. visualizing attention weights finally figure illustrate learned attention weights pinpoint important subcomponents within given implicit discourse relation. implicit conjunction relation weights indicate peak transition argument boundary establishing connection semantically related terms understandings–agree. entrels show opposite trend second arguments exhibit larger intensities entity relations follow characteristic writing style newspapers adding additional information reference entity. work presented ﬁrst attentionbased recurrent neural sense labeler speciﬁcally developed chinese implicit discourse relations. ability model discourse units sequentially jointly shown highly beneﬁcial terms state-of-the-art performance cdtb also terms insightful observations inner workings model attention mechanism. architecture structurally simple beneﬁts partial argument sampling easily adapted similar relation recognition tasks. future work intend extend approach different languages domains e.g. recent data sets narrative story understanding question answering believe recurrent modeling implicit discourse information driving force successfully handling complex semantic processing tasks. authors would like thank ayah zirikly philip schulz ding helpful suggestions early draft version paper also thank anonymous reviewers valuable feedback insightful comments. grateful farrokh mehryary technical support attention layer implementation. computational resources provided centre science finland arcada university applied sciences helsinki finland. research goethe university frankfurt supported project ‘linked open dictionaries funded german ministry education research", "year": 2017}