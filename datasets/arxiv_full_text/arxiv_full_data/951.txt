{"title": "On Fast Dropout and its Applicability to Recurrent Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients. The control of overfitting has seen considerably less attention. This paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. The derivatives of that regularizer are exclusively based on the training error signal. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets.", "text": "recurrent neural networks rich models processing sequential data. recent work advancing state focused optimization modelling rnns mostly motivated adressing problems vanishing exploding gradients. control overﬁtting seen considerably less attention. paper contributes analyzing fast dropout recent regularization method generalized linear models neural networks back-propagation inspired perspective. show fast dropout implements quadratic form adaptive per-parameter regularizer rewards large weights light underﬁtting penalizes overconﬁdent predictions vanishes minima unregularized training loss. derivatives regularizer exclusively based training error signal. consequence absence global weight attractor particularly appealing rnns since dynamics biased towards certain regime. positively test hypothesis improves performance rnns four musical data sets. recurrent neural networks among powerful models sequential data. capabilty representing measurable sequence sequence mapping arbitrary accuracy makes universal approximators. nevertheless given little attention last decades problems vanishing exploding gradients error signals either blowing decaying exponentially events many time steps apart rendered largely impractical exact problems supposed solve. made successful training impossible many tasks recently without resorting special architectures abandoning gradient-based optimization. successful application tasks long-range dependencies thus relied paradigms. former make long short-term memory approaches among best methods modelling speech handwriting latter rely sensible initializations leading echo-state networks publication nowadays considered landmark since shown even standard rnns trained right optimization method. sophisticated hessian-free optimizer employed initially research shown carefully designed ﬁrst-order methods optima similar quality. problem underﬁtting standard rnns dealt extent rnns practical many areas e.g. language modelling contrast problem overﬁtting standard rnns tackled few. noted using priors single optima parameters detrimental effects representation capability rnns global attractor constructed parameter space. case prior mode zero biases network towards solutions lets information exponentially fast time making impossible memorize events indeﬁnite amount time. graves proposes stochastically adaptively distort weights lstm-based rnns justiﬁed perspective variational bayes minimum description length principle. overﬁtting practically non-existent experiments conducted. untested whether approach works well standard rnns–along lines observations pachitariu sahani might hypothesize injected noise disturbs dynamics rnns much leads divergence training. deep neural network community recently embraced regularization method called dropout gist randomly discard units network training leading less interdependent feature detectors intermediary layers. here dropping merely means output unit zero. equivalent view complete outgoing weight vector zero questionable whether straight transfer dropout rnns possible. resulting changes dynamics every forward pass quite dramatic. reason pachitariu sahani dropout parts dynamic. i.e. connections feeding hidden output layer. contribution show using recent smooth approximation dropout regularizes rnns effectively. since approximation deterministic assert dynamic parts network operate reasonable regimes. show fast dropout keep rnns reaching rich dynamics training obvious relation classic dropout regularization structure paper follows. ﬁrst review rnns fast dropout novel analysis derivatives fast dropout leads interpretation perform decomposition loss based average output network’s units regularizer based variance. discuss form well suited rnns consequently conduct experiments conﬁrm hypothesis. deﬁne rnns terms components. ultimately interested output calculate given parameters network input secondly want learn parameters done design optimization function parameters commonly dubbed loss cost error function. calculating output given input sequence produce output done intermediary representation called hidden state layer dimensionalities inputs outputs hidden state time step. component layers sometimes referred unit neuron. depending associated layer input hidden output units. also denote units feed unit incoming units units unit feeds called outgoing units recurrent network {win wout wrec} weight matrices bias vectors. form parameters together initial hidden state dimensionalities weight matrices bias vectors initial hidden states determined dimensionalities input sequences well desired hidden layer output layer sizes. functions so-called transfer functions mostly coordinate-wise applied nonlinearities. call activations units pre-synaptic application post-synaptic afterwards. typical choices include logistic sigmoid +exp tangent hyperbolicus recently rectiﬁed linear recurrent weight matrix wrec zero recover standard neural network bishop applied independently time step. loss function adaption parameters restrict rnns supervised case given data consisting pairs rt×κ rt×ω. refers sequence length assume constant data set. interested adapt parameters network outputs rt×o close closeness typically formulated loss binary cross entropy log). loss locally differentiable ﬁnding good parameters performed gradient-based optimization nonlinear conjugate gradients stochastic gradient descent. gradients calculated efﬁciently back-propagation time fast dropout unit network assumed random variable. assure tractability ﬁrst second moments random variables kept sufﬁces good approximation. since pre-synaptic activation unit weighted incoming units safely assume gaussianity inputs central limit theorem. sufﬁcient efﬁcient ways propagate mean variance non-linearity denotes element-wise product non-linear transfer function before. input layer unit gaussian distributed diagonal covariance assumption furthermore bernoulli distributed variables indicating whether incoming complementary drop rate. weight vector assumed constant. neural network practice consist many nodes them output units directly contributing loss function others input units stem calculation come data set. component represents incoming unit might external input network hidden unit. general complex distribution depending highly nature given input function gaussian distributed obtain mean variance output follows forward propagation non-linearity calculation post-synaptic activation approximated well case logistic sigmoid tangent hyperbolicus done exactly case rectiﬁer rectiﬁer previously reported useful ingredient rnns found leads unstable learning behaviour preliminary experiments thus neglected study solely focusing tangent hyperbolicus. popular transfer functions softmax need approximated either sampling unscented transform obtain gaussian approximation mean variance obtained follows. since independent follows independent random variables assume components independent write furthermore independency assumption necessary lyapunov condition satisﬁed central limit theorem hold ensuring approximately gaussian. propagating mean variance sufﬁces determining presynaptic moments outgoing units. output whole model simplify matters ignore variance. loss functions take variance account sampling viable alternative well. fast dropout rnns extension fast dropout recurrent networks straightforward technical perspective. first note concatenate input vector time step hidden state previous layer single vector obtain corresponding weight matrix concatenation input hidden recurrent weight matrices wrec thus reduce computation step above. given forward pass used automatic differentiation theano calculate gradients. nevertheless contribute close inspection derivatives. prove useful since makes possible interpret fast dropout additional regularization term independent exact choice loss function. consider loss function data parameters machine learning wish loss minimal unseen data although access training dtrain. typical approach optimize another loss proxy hope good minimum correspond good minimum unseen data. learning often done optimization called regularizer. common example regularizer place prior parameters case function corresponds log-likelihood parameters. weight decay spherical gaussian inverse scale regularizers sophisticated e.g. rifai determine i.e. λ||θ|| directions input space model’s outputs invariant. recently dropout generalized linear models intepreted semi-supervised regularization term encouraging conﬁdent predictions wager seems difﬁcult bring objective function fast dropout form possible derivatives node. this perform back-propagation like calculations. prepost-synaptic activations component layer network. first note /∂wi /∂a· ∂a/∂wi according chain rule. since random variable described forms. case gaussian approximation summarize terms mean variance; approach used propagation possible closed form. case sampling single instantiation random variable propagate analysis cases follows. unit increase variance. exact interpretation depends loss many cases related expectation unit quite erroneous leads increase scatter output. fast dropout term encourages quadratic growth weights. unit decrease variance. before depends exact loss function mostly related expectation unit quite right makes reduction scatter desirable. fast dropout term encourages quadratic shrinkage weights. figure visualizations behaviour single unit. axes correspond presynaptic mean variance feeding unit loss measuring divergence target value applied indices color logarithmic scale. gradients loss shown vector ﬁeld plot. squared error shown left gaussian log-likelihood middle bernoulli log-likelihood right. ﬁrst plots optimium middle last little left. behaviour illustrated output units numerically inspecting values gradients pre-synaptic moments given loss. consider single unit loss measuring divergence output target value pre-synaptic variance enter loss ways respected either loss transfer function. three examples visualize pre-synaptic mean variance gradients respective loss values figure latter cases erroneous units ﬁrst increase variance move towards correct mean subsequently reduce variance. value zero-centred gaussian random variable since gaussian. scale independent current weight value determined post-synaptic moments incoming unit dropout rate error signal. conclude also case write want stress fact approximation well sampling case regularization term vanishes optima training loss. consequence global attractor formed makes method theoretically useful rnns. might argue fast dropout regularizing effect all. regularization inﬂuencing ﬁnal solution also optimization leading different optima. relationship weight decay already mentioned imposing gaussian distribution centred origin precision prior weights leads method called weight decay. probabilistically sound also works well empirically e.g. recalling derivative weight decay form ∂rwd/∂wi reinterpret rapprox weight decay term coefﬁcient weight-wise dependent current activations possibly negative. weight decay always slightly wrong training since derivative weight decay term match unregularized loss. order ∂l/∂θ ∂rwd/∂θ minimal cannot minimal unless relationship adaptive weight noise method units weights stochastic pratice implemented performing monte carlo sampling. similar technique closed form approximation. case layer dropout variables weights gaussian distributed covariance diagonal organized vector. assume gaussian solving shows rescaling sufﬁces case expectation. however simple variance i.e. equations solution depends thus independent input network. methods share property global attractor present loss prior part optimization ﬁxed. throughout experiments resort several tricks introduced recently stable efﬁcient optimization neural networks rnns especially. first make rmsprop optimizer divides gradient exponential moving average squares. approach similar adagrad uses window based average. found enhancing rmsprop nesterov’s accelerated gradient greatly reduces training time preliminary experiments. initialize rnns stable dynamics followed initialization protocol setting spectral radius speciﬁc value maximum amount incoming connections unit necessary centre inputs outputs. effect using recurrent weight matrix propagating states time also element-wise square advancing variances quantiﬁed. stability network coupled spectral radius recurrent weight matrix thus stability forward propagating variance related spectral radius element-wise square rec). since non-negative matrices non-singular matrices setting wrec full rank spectral radius assures rec) denotes taking absolute value element-wise. ﬁxed threshold since hidden-to-hidden connections hidden-to-output connections make hidden units quite distinct ways found beneﬁcial separate dropout rates. speciﬁcally hidden unit different probability dropped feeding table results midi data sets. numbers average negative log-likelihoods test represents work; plain rnn-nade results deep shows best results note rnn-nade deep employ various extensions model structure work i.e. structured outputs various forms depths. results best shallow model considered work. hidden layer next time step feeding output layer. taking step further also consider networks completely neglect fast dropout hidden-tooutput connections; ordinary forward pass used instead. note setting dropout rate zero since variance incoming units completely neglected. whether done treated another hyperparameter experiment. experiments done performing random search hyper parameters runs performed data set. report test loss model lowest validation error training runs using split improve speed organize sequences minibatches ﬁrst splitting sequences training validation chunks length zeros prepended sequences less time steps. test error reported unsplitted sequences. training rnns generatively model polyphonic music valuable benchmark rnns high dimensionality presense long well short term dependencies. data evaluated previously bengio model achieving best results rnn-nade makes speciﬁc assumptions data rnns attach assumptions inputs. data consists four distinct data sets namely piano-midi.de nottingham musedata jsbchorales dimensionality time step organized different piano rolls sequences binary vectors; component vectors indicates whether note occuring given time step. rnn’s output model sufﬁcient statistics bernoulli random variable i.e. describes probability note present time step output non-linearity network sigmoid projects points interval perform learning minimization average negative log-likelihood case average binary cross-entropy although common metric evaluating performance benchmarks accuracy restrict nll–the measure accuracy optimized merely proxy. present results fd-rnns compared various methods table method surpassed methods either incorporate speciﬁc assumptions data employ various forms depth want stress performed runs data more. shows relative ease obtain good results despite huge space potential hyper parameters. additional observation range eigenvalues recurrent weight matrix wrec training. performed additional experiment jsbchorales inspected eigenvalues test loss. found spectral radius ﬁrst increases sharply rather high value decreases slowly settle speciﬁc value. tried replicate behaviour plain rnns found rnns never exceeded certain spectral radius stuck. stands line observation section weights encouraged grow error high shrink convergence optimum. figure plot spectral radius training process stages. contributed ﬁeld neural networks ways. first analysed fast approximation dropout regularization method bringing derivative form loss regularized additive term. used form gain insights upon behaviour fast dropout neural networks general shown objective function bias solutions perform suboptimal unreguarized loss. second hypothesized beneﬁcial especially rnns conﬁrmed hypothesis conducting quantitative experiments already established benchmark used context learning recurrent networks. bayer osendorfer urban training neural networks implicit variance. proceedings international conference neural information processing iconip. graves fern´andez liwicki bunke schmidhuber unconstrained online handwriting recognition recurrent neural networks. advances neural information processing systems hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv..", "year": 2013}