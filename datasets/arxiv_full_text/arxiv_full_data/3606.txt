{"title": "Mapping Images to Scene Graphs with Permutation-Invariant Structured  Prediction", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Structured prediction is concerned with predicting multiple inter-dependent labels simultaneously. Classical methods like CRF achieve this by maximizing a score function over the set of possible label assignments. Recent extensions use neural networks to either implement the score function or in maximization. The current paper takes an alternative approach, using a neural network to generate the structured output directly, without going through a score function. We take an axiomatic perspective to derive the desired properties and invariances of a such network to certain input permutations, presenting a structural characterization that is provably both necessary and sufficient. We then discuss graph-permutation invariant (GPI) architectures that satisfy this characterization and explain how they can be used for deep structured prediction. We evaluate our approach on the challenging problem of inferring a {\\em scene graph} from an image, namely, predicting entities and their relations in the image. We obtain state-of-the-art results on the challenging Visual Genome benchmark, outperforming all recent approaches.", "text": "structured prediction concerned predicting multiple inter-dependent labels simultaneously. classical methods like achieve maximizing score function possible label assignments. recent extensions neural networks either implement score function maximization. current paper takes alternative approach using neural network generate structured output directly without going score function. take axiomatic perspective derive desired properties invariances network certain input permutations presenting structural characterization provably necessary sufﬁcient. discuss graph-permutation invariant architectures satisfy characterization explain used deep structured prediction. evaluate approach challenging problem inferring scene graph image namely predicting entities relations image. obtain state-of-the-art results challenging visual genome benchmark outperforming recent approaches. introduction structured prediction addresses problem classiﬁcation label space contains multiple inter-dependent labels. example semantic segmentation image pixel assigned label considering labels nearby pixels. similar problem task recognizing multiple entities relations image recognizing entity affects recognition others. structured prediction attracted considerable attention applies many learning problems poses unique theoretical applied challenges *equal contribution tel-aviv university israel google brain gonda brain research institute bar-ilan university ramat-gen israel. correspondence roei herzig <roeiherzigtau.ac.il> moshiko raboh <shikorabgmail.com> chechick <gal.chechikgmail.com> jonathan berant <joberantcs.tau.ac.il> amir globerson <amir.globersontau.ac.il>. typically structured prediction models deﬁne score function quantiﬁes well label assignment compatible consistent input setup inference task amounts ﬁnding label maximizes compatibility score maxy score-based approach separates scoring component implemented parametric model optimization component aimed ﬁnding label maximizes score. unfortunately general scoring function space possible label assignments grows exponentially input size. instance possible pixel label assignments large even small images. thus inferring label assignment maximizes scoring function computationally hard general case. alternative approach scored-based methods input structured output black neural network without explicitly deﬁning score function. raises natural question properties invariances must satisﬁed network? take axiomatic approach argue important property invariance particular type input permutation. prove invariance equivalent imposing certain structural constraints architecture network describe architectures satisfy constraints signiﬁcantly extending expressive power current structured prediction approaches. argue respecting permutation invariance important otherwise model would spend capacity learning invariance training time. conceptually approach motivated recent work deepsets asked similar question black-box functions sets. evaluate approach tackle challenging task mapping image scene graph describes entities image relations. describe model satisﬁes permutation invariance property show achieves state-of-the-art results competitive visual genome benchmark demonstrating power design principle. summary novel contributions paper first derive sufﬁcient necessary conditions deep structured prediction architecture. second improve state-of-the-art approach challenging pixel-tograph problem large dataset complex visual scenes. scored-based methods structured prediction deﬁne score function reﬂects degree compatible infer label solving maxy score functions previously used decompose simpler functions solving maxy performed efﬁciently. local maximization forms basic building block algorithms approximately maximizing achieve restrict depend small subset variables. renewed interest deep learning efforts integrate deep networks structured prediction including modeling functions deep networks. context widely-used score functions singleton pairwise fij. initial work used two-stage architecture learning local scores independently structured prediction goal later works considered end-to-end architectures inference algorithm part computation graph studies used standard inference algorithms loopy belief propagation mean ﬁeld methods gradient descent score-based methods provide several advantages. first allow intuitive speciﬁcation local dependencies labels translate global dependencies. second score function linear parameters linear learning problem natural convex surrogates making learning efﬁcient. third inference large label spaces often possible exact combinatorial algorithms empirically accurate approximations. however advent deep scoring functions learning longer convex. thus worthrethink architecture structured prediction models consider models inputs outputs directly without explicit score function. want models enjoy expressivity predictive power neural networks maintaining ability specify local dependencies labels ﬂexible manner. next section present approach consider natural question properties deep neural network used structured prediction. denote structured label entries score-based approach score deﬁned singleton scores pairwise scores overall score singleton pair scores. brevity also denote inference algorithm takes input local scores outputs assignment maximizing therefore abstractly view inference algorithm blackbox takes input nodeedge-dependent inputs returns label even without explicit score function numerous inference algorithms exist setup including belief propagation mean ﬁeld develop framework deep learning labeling algorithm algorithm black-box functions input labels output. next architecture algorithm have. follow several deﬁnitions. graph labeling function function whose input ordered node features ordered edge features example zi’s array values zij’s table values simplicity assume output labels thought labeling nodes. thus inference algorithms like graph labeling functions since take input output labels. however graph labeling functions need correspond inference algorithm natural requirement algorithm produces result given score function. example consider label space containing three variables assume inference algorithm takes input outputs label algorithm given permuted consistent input deﬁnes exactly score function ﬁrst scenario. hence would expect output label permuted namely output inference algorithms including mean ﬁeld satisfy symmetry requirement demotivated discussion structure necessary sufﬁcient guarantee graphpermutation invariant? note function takes input ordered therefore output could certainly differ output achieve permutation invariance intuitively contain certain symmetries. example permutation invariant architecture deﬁne function characterization restrictive cover permutation invariant functions. next theorem provides complete characterization figure shows corresponding architecture. theorem graph labeling function. graph-permutation invariant exist functions next prove black-box graph-permutation invariant function expressed namely show deﬁne implement permutation invariant function idea construct second argument contains information graph features including edges originated then function consists application black representation followed extracting label simplify notation assume edge features scalar extension vector case simple involves indexing. also assume uniquely identiﬁes node permuting names nodes maintains output. sign. design deep learning black-box hence need guarantee invariance input permutations. black-box satisfy invariance waste capacity learning training time. follows denote joint node edge features. thought container elements. next consider happens graph labeling function graph variables permuted permutation importantly edges case also permuted consistent node permutation deﬁnition node edge features. given permutation denote node edge features given elements node elements permuted according edge elements permuted accordingly. follows notation namely applied labels yields labels permuted next comes deﬁnition function whose output invariant permutations input graph. deﬁnition graph labeling function said graph-permutation invariant permutations satisﬁes figure illustrates desired invariance. property says long input describes node edge properties labeling output. indeed property would like have thus turn characterizing necessary sufﬁcient structure achieving figure schematic representation architecture theorem singleton features omitted simplicity. first features processed element-wise next summed create vector concatenated third representation entire graph created applying times summing created vector. graph representation ﬁnally processed together hash function buckets mapping node features index assume perfect deﬁne pairwise features vector size one-hot vector dimension coordinate. recall consider scalar indeed deﬁne second argument then since distinct stores pairwise features neighbors unique positions within coordinates. since contains feature whereas contains feature cannot simply since would lose information edges features originated from. instead deﬁne rl×l feature mapped distinct location. formally figure illustration proof construction theorem hash function size three-node input graph pairwise features applied zij. application yields vector three dark yellow columns correspond then vectors summed obtain three vectors. outer product resulting matrix zeros except row. dark blue matrix corresponds summed matrix isomorphic original matrix. complete construction outcome ﬁrst discard rows columns correspond original nodes then reduced matrix input given assume simplicity need contracted. output since invariant permutations indeed returns output original input. general graphs discussed complete graphs edges correspond valid feature pairs. many graphs however sparse certain structures. example n-variable chain graph sequence labeling edges. sparse graphs input would pairs rather features corresponding valid edges graph interested invariances preserve graph structure namely automorphisms graph. thus desired invariance automorphisms graph. easy theorem holds using rnns components. theorem allows arbitrary functions except input dimensionality. speciﬁcally functions involve highly expressive recursive computation simulate existing message passing algorithms algorithms learned data. course extended elaborate structures like lstms neural turing machines leave future work. theorem suggests function form graph permutation invariant. easy show composing functions also gpi. therefore iteratively providing output step part input next step maintain graph-permutation invariance. results recurrent architecture employ next section obtain state-of-the-art performance scene graph prediction. demonstrate beneﬁts axiomatic approach task inferring scene graphs images. problem input image annotated rectangles bound entities image known bounding boxes. goal label bounding correct entity category every pair entities relation form coherent graph known scene graph. scene graph nodes correspond bounding boxes labeled entity category edges correspond relations among entities could spatial functional thus image bounding boxes output variables. concept illustrated figure showing image motorcycle corresponding scene theorem provides general requirements designing architecture structured prediction. given problem choose speciﬁc architecture parameterization instance interesting consider algorithm like belief propagation implemented framework. following proof theorem would aggregate features would apply features. architecture course general construction. example could sketch input graph labeling performed reduced representation. introducing attention. attention powerful architectural component deep learning inference algorithms attention. show attention introduced framework. intuitively attention means instead aggregating features neighbors node weighs neighbors based relevance. example label entity image depend strongly entities spatially closer. implement attention architecture formally learn attention weights neighbors node scale features neighbor. also learn different attention weights individual features neighbor similar way. attention mask specifying weight node gives node achieve form extend single entry deﬁning eβzij keep deﬁnition next deﬁne substitute obtain desired form attention weights neighboring feature vectors graph below. pink image labeled motorcycle white labeled dog. boxes correspond nodes relation corresponds edge scene graphs typically sparse view scene graph complete pair unrelated entities connected ‘null’ edge. scene graph represented collection triplets model components label predictor takes input image bounding boxes outputs distribution labels entity relation. then scene graph predictor takes label distributions predicts consistent label distributions jointly entities relations. label prediction. module receives input image bounding boxes corresponding image entities outputs bbi) entity label probabilities another prerelation probabilities taking input patch cropped full image according bounding used second resnet predict relations using -channel tensor input three channels image channels binary masks subject entity object entity bounding boxes image patch provided network cropped covers subject entity object entity. providing binary masks breaks symmetry subject entity object entity allow network discriminate triplets scene graph prediction. module described trivially output variables yent predicted independently constructing architecture scene graph predictor harder. outline construction. entity classiﬁcation module following theorem features every bounding features pairs. classify relations added function ρrelation reuses representation created entity classiﬁcation. input ρrelation representation easy show entire network gpi. concatenation zfeatures zfeatures figure label predictor. entity recognition network network takes image patch cropped based bounding outputs classiﬁcation probabilities label. relation recognition network network takes input tensor containing image ﬁrst channels binary masks subject object entities remaining channels. ﬁnal softmax layer) zspatial bounding given addition used conﬁdences relation step apply function receives entity features relation features output updated conﬁdences entities relations. composing functions module gpi. describe implementation three components network fc-layers. receives subject features relations features entity features outputs vector size next entity aggregate using attention mechanism described section calculate weights implement layer receives input outputs scalar. fc-layer network receiving entity features context features outputs aggregated similar attention mechanism entities resulting vector representing entire graph. consists ρentity classiﬁes entities ρrelation classiﬁes relations. ρentity three fc-layer network size receives input outputs vector scalar entity class. unlike theorem allow direct access maintains property improved learning practice. ﬁnal output conﬁdence linear interpolation current conﬁdence eatures conﬁdence controlled learned forget gate i.e. output forget· eatures ρrelation relation classiﬁer analogous entity classiﬁer receiving input relation features graph representation dataset. evaluated approach visual genome dataset consists images annotated bounding boxes entities relations. distribution entity classes relations long-tailed total unique entity classes unique relations. allow apple-to-apple comparison previous studies dataset used preprocessed data including train test splits provided dataset average entities relations image. evaluation used entity categories relations tune hyper-parameters also split training data randomly selecting examples resulting ﬁnal k/k/k split train/validation/test sets. training procedure. trained networks using adam input images resized conform resnet architecture. ﬁrst trained module trained module using best model. follows particular chosen values tuned validation set. trained relation network cross-entropy loss positive-to-negative ratio performed early-stopping epochs. chose batch size also used data augmentation techniques translation rotation improve results. loss function cross entropy losses entities relations image. loss penalized entities times strongly relations penalized negative relations times weakly positive relations. used batch size early-stopped epochs. recurrent application performed steps. evaluation. deﬁned three different subtasks inferring scene graphs focus sgcls given ground-truth bounding boxes entities predict entity categories relations categories. predcls given bounding boxes annotated entity labels predict relations. following used recallk evaluation metric. measures fraction correct ground-truth triplets appear within conﬁdent triplets proposed model. differ whether enforce graph constraints model predictions. ﬁrst protocol requires top-k triplets assign consistent class entity relation. rules putting triplet pair bounding boxes. also rules inconsistent assignment like bounding labeled entity triplet another entity another triplet. second evaluation protocol enforce constraints. models baselines. compare four variants approach reported results four baselines currently state-of-the-art various scene graph subtasks. models data split pre-processing work leverages word embeddings ﬁne-tune likelihood predicted relations. model passes messages between entities relations iteratively reﬁnes feature used prediction. table lists recall recall four variants approach compared three baselines evaluating graph constraints. approach performs well linguistic outperforms baselines predcls sgcls. table provides similar comparison evaluating without graph constraints; linguistic performs best. details supplemental material. figure illustrates model behavior. predicting isolated labels mislabels several entities corrected joint prediction column shows system learned attend nearby entities column shows stronger attention learned classes bird presumably usually informative common classes like tree. figure input image bounding boxes ground-truth scene graph. fails recognize entities relations gpilinguistic ﬁxes incorrect predictions. window signiﬁcant neighbor tree. entity bird receives substantial attention tree building less informative. signiﬁcant recent interest extending deep learning structured prediction. much work semantic segmentation convolutional networks became standard approach obtaining singleton scores various approaches proposed adding structure top. approaches used variants message passing algorithms unrolled computation graph studies parameterized parts message passing algorithm learned parameters recently gradient descent also used maximizing score functions alternative approach deep structured prediction greedy decoding label inferred time based previous labels. popular sequence-based applications like dependency parsing works rely sequential structure concept architectural invariance recently proposed deepsets invariance consider much less restrictive hence results substantially different architectures. extracting scene graphs images provides semantic representation later used reasoning question answering image retrieval forefront machine vision research integrating challenges like object detection action recognition detection human-object interactions presented deep learning approach structured prediction constrains architecture invariant structurally identical inputs. score-based methods approach relies pairwise features capable describing inter-label correlations thus inheriting intuitive aspect score-based approaches. however instead maximizing score function directly produce output invariant equivalent representations pairwise terms. axiomatic approach extended many ways. image labeling geometric invariances desired. cases invariance feature permutations desirable. leave derivation corresponding architectures future work. finally cases invariant structure unknown discovered data related work lifting graphical models would interesting explore algorithms discover symmetries deep structured prediction. belanger david yang bishan mccallum andrew. end-to-end learning structured prediction energy networks. precup doina whye proceedings international conference machine learning volume pmlr hung huynh tuyen riedel sebastian. automorphism groups graphical models lifted proceedings twentyvariational inference. ninth conference uncertainty artiﬁcial intelligence uai’ arlington virginia united states auai press. http//dl.acm. org/citation.cfm?id=.. chen danqi manning christopher. fast accurate dependency parser using neural networks. proceedings conference empirical methods natural language processing chen liang chieh papandreou george kokkinos iasonas murphy kevin yuille alan semantic image segmentation deep convolutional nets fully connected crfs. proceedings second international conference learning representations farabet clement couprie camille najman laurent lecun yann. learning hierarchical features scene labeling. ieee transactions pattern analysis machine intelligence gygli michael norouzi mohammad angelova anelia. deep value networks learn evaluate iteratively reﬁne structured outputs. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. ieee conference computer vision pattern recognition cvpr vegas june johnson justin krishna ranjay stark michael lijia shamma david bernstein michael fei-fei. image retrieval using scene graphs. ieee conference computer vision pattern recognition cvpr krishna ranjay yuke groth oliver johnson justin hata kenji kravitz joshua chen stephanie kalantidis yannis li-jia shamma david visual genome connecting language vision using crowdsourced dense image annotations. international journal computer vision lafferty mccallum pereira conditional random ﬁelds probabilistic models segmenting labeling sequence data. proceedings international conference machine learning zaheer manzil kottur satwik ravanbakhsh siamak poczos barnabas salakhutdinov ruslan smola alexander deep sets. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems curran associates inc. zellers rowan yatskar mark thomson choi yejin. neural motifs scene graph parsing global context. arxiv preprint arxiv. abs/. http//arxiv.org/ abs/.. zheng shuai jayasumana sadeep romera-paredes bernardino vineet vibhav zhizhong dalong huang chang torr philip conditional random ﬁelds recurrent neural networks. proceedings ieee international conference computer vision meshi sontag jaakkola globerson learning efﬁciently approximate inference dual losses. proceedings international conference machine learning york acm. newell alejandro huang zhiao deng jia. associative embedding end-to-end learning joint detection grouping. advances neural information processing systems curran associates inc. wenzhe chang baobao. effective neural network model graph-based dependency parsing. proceedings annual meeting association computationa linguistics pennington jeffrey socher richard manning christopher glove global vectors word representation. empirical methods natural language processing http //www.aclweb.org/anthology/d-. plummer bryan mallya arun cervantes christopher hockenmaier julia lazebnik svetlana. phrase localization visual relationship detection comprehensive image-language cues. iccv raposo david santoro adam barrett david pascanu razvan lillicrap timothy battaglia peter. discovering objects relations entangled scene representations. arxiv preprint arxiv.", "year": 2018}