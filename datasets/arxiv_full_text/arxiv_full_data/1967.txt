{"title": "Self corrective Perturbations for Semantic Segmentation and  Classification", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.", "text": "convolutional neural networks subject great importance past decade great strides made utility producing state performance many computer vision problems. however behavior deep networks fully understood still active area research. work present intriguing behavior pre-trained cnns made improve predictions structurally perturbing input. observe perturbations referred guided perturbations enable trained network improve prediction performance without learning change network weights. perform various ablative experiments understand perturbations affect local context feature representations. furthermore demonstrate idea improve performance several existing approaches semantic segmentation scene labeling tasks pascal dataset supervised classiﬁcation tasks mnist cifar datasets. convolutional neural networks achieved state results several computer vision benchmarks ilsvrc pascal past years. despite overwhelming success recent results highlighted sensitive small adversarial noise input easily fooled using structured noise patterns understand learn complex meaningful representations time easily fooled simple imperceptible perturbations still remains open research problem. work goodfellow szegedy among others bring intriguing properties neural networks introducing perturbations either hidden layer weights input image. approaches focused understanding effect adversarial noise deep networks work present interesting observation input perturbations enable correct mistakes. perturbations exploit local neighborhood information network’s prediction turn results contextually smooth predictions. almost based approaches output obtained using single forward pass prediction time. proposed approach prediction made network forward pass generate perturbations input. speciﬁcally backpropagate gradient prediction error input. would like emphasize error gradients generated purely based network’s prediction without knowledge ground truth. perturb input image adding scaled version gradient signal. back network prediction. figure shows example self-corrective behavior generated perturbations segmentation classiﬁcation tasks. example shows perturbations input image could viewed form structured distortion added input context gets ampliﬁed pixel’s neighborhood enables network correct mistakes. proposed approach simple easy implement require retraining modiﬁcation network’s architecture. existing approaches improve performance segmentation classiﬁcation tasks geared towards novelties network architecture using large amount training data both. valid ways improve networks performance proposed approach highlights inherent behavior cnns used improve prediction without requiring additional learning training data. would like note behavior guided perturbations similar conditional random fields based approaches difference case explicit modeling context neighborhood interactions. since approach network independent doesn’t preclude networks model context explicitly show improvements networks too. best knowledge ﬁrst approach show existence self-corrective behavior cnns behavior improvement performance segmentation classiﬁcation tasks. summarize major contributions paper present novel intriguing observation exist structured perturbations used perturb input leads corrective behavior cnns. propose generalized framework improve performance pretrained model architecture independent requires learning assuming network trained end-to-end. recent years several approaches attempt analyze behavior cnns classiﬁcation problems. mahendran proposed approach invert function learned order generate faithful reconstruction input possible. performed minimizing regularized energy function approximates representation function learned deep network. another interesting work direction fooling images work nguyen extended yosinski main objective approaches synthesize images confuse maximizing activation individual neurons different layers deep network. leads interesting results images look like random noise classiﬁes different classes high conﬁdence. approaches closer spirit proposed approach ones predate recent ones szegedy goodfellow study shows exist adversarial examples result minor pertubations input causes misclassify input images classiﬁcation tasks; examples generated adding fraction gradient generated wiggling classiﬁer output direction target class. applications paper focuses semantic segmentation. research gone understanding expressive ability cnns problems. recent methods image segmentation fully convolutional networks long provided easy framework casts image segmentation problem pixelwise label classiﬁcation problem major difference work image level output generation backpropogation made possible work zeiler image level back propogation provides simple learn discriminative representation classes pixel level. several recent approaches crfasrnn zheng deeplab chen gcrf vemulapalli improved framework explicitly modeling context. crfasrnn casts iterations traditionally used post processing function image segmentation problem ensure label compatibility recurrent neural network. formulate steps required perform mean ﬁeld iteration including message passing learning label compatibility transform layer unrolled time iterations. unary potentials computed using fcn-s network reﬁned using strucure. casting layer perform endto-end training. recently propose train multiscale context aggregation module modiﬁed fcn-s network. context module improves performance base network combination based approaches. paper describe interesting property deep networks change predictions better using minor perturbations input. furthermore provide useful applications approach showing used improve prediction performance challenging computer vision tasks. study different aspects perturbations affect network representations. since approach generate guided perturbation different segmentation classiﬁcation tasks discuss separately. argued method generating gradients using network’s prediction lead inaccurate gradient information propagated network especially cases network’s output contains many misclassiﬁed pixels. insight provide work despite misclassiﬁcations deep network gradients input obtained network’s prediction general improve ﬁnal output deep network. figure illustrates approach semantic segmentation task. given input image perform forward pass compute output usually output softmax function gives class probability vector pixel. prediction output binarized setting probability conﬁdent class others zero. done pixel error gradient computed softmax layer setting modiﬁed output ground truth. rm×n×cin represent input image deep network rm×n×cout represent ground truth labeling number input channels cout number classes dimensionality input image. represent parameters network represent loss function optimized training. prediction time ypred predicted labeling. order generate error gradient backpropagation create pseudo ground truth labeling ypseudo modifying ypred follows initialize ypseudo ypred. component ypseudo represented cout-dimensional score backpropagated network input. backpropagated error gradient signal input represented figure visualization ﬁlter responses showing correct context propagated along fcn-s network. column ﬁlter responses forward pass using original input. column ﬁlter responses forward pass using perturbed input. column difference impact perturbations ﬁlter responses clear understanding happens forward pass step vastly changes network’s prediction visualize ﬁlter responses fcn-s network figure model chosen simpler architecture observed similar behavior deep architectures too. figure plot average ﬁlter responses different layers deep network upsampling bilinearly image size. observed inﬂuence added perturbations visually explicit pool layer difference ﬁlter responses column indicate information propagates layers early pool. next analyze pixels network predictions changed step step figure shows pixels classiﬁed wrongly forward pass step correctly classiﬁed ﬁnal output. hand shows pixels correctly classiﬁed step incorrect ﬁnal output. observe that correctly classiﬁed pixels step step mostly internal image additional contextual information available network switch prediction whereas small number misclassiﬁed pixels largely concentrated along boundary regions image context ambiguous. present visualization examples supplementary material. approximating ideal gradient direction experiment would like answer question ideal perturbations generated input? best ground truth generate error gradients softmax layer backpropagated generate perturbed input. perturbed input back network result vastly improved prediction shown figure perturbations ground truth signiﬁcantly improve performance information available prediction time. figure output fcn-s network output proposed approach pixels incorrectly classiﬁed fcn-s corrected approach pixels incorrectly classiﬁed approach fcn-s classiﬁed correctly. novelty current work ground truth gradient direction approximated well enough predicted gradient directions computed using network’s prediction. figure output fcn-s ground truth labeling output perturbed input using ground truth gradients output perturbed input using guided perturbations iteration respectively. understand extent usefulness predicted gradients performed experiment three steps outlined approach applied successive iterations figure shows output approach obtained ﬁrst iteration figure show output successive iterations. shows signiﬁcant improvement happens ﬁrst iteration subsequent iterations yield little improvement. observe similar behavior average pascal validation set. intuition based overlapping receptive ﬁelds receptive ﬁelds neighboring pixels deﬁne context interactions. advantage overlapping receptive ﬁelds neighborhood connectivity established automatically without explicitly specifying long errors made sparse respect pixel’s receptive ﬁelds error gradients accumulated entire network used perturb input image exhibit corrective behavior. effect seen type residual information propagated network results contextual smoothing. evident looking ﬁlter responses figure speciﬁcally column shows difference responses without observed pool responses peak activations occur around neighborhoods competing classes. perturb neighborhoods thus resulting contextually smooth predictions regions. ﬁgure show guided perturbations impact decisions made deep network considering local region input image tracking classiﬁcation scores score-fr layer across different values half ﬁgure patch interest image marked corresponding region figure half shows patch input image corresponding patch score-fr layer output fcn-s network upsampling image size. bottom half show different values guided perturbations perturbed patches score-fr output. notice scores become contextually smoother actual score values top- predicted classes grid marked blue ﬁgure plotted. observe range positive values correct class score dominates others across entire neighborhood. legend applies plots. best viewed screen. please zoom clarity. score-fr output marked blue box. immediately below following shown different values guided perturbations generated input perturbed patches output score-fr layer. important observations made ﬁgure input perturbations corresponding positive improve score output vast range values. visualization shows guided perturbations able operate local level leveraging neighborhood contextual information directly observed images score layer shown bottom row. even small negative value results large adverse effect score output without perceptible change perturbed patch. shows negative corresponds adversarial setting. discussion motivates choice using generate analyze input perturbations affect actual classiﬁer score show ﬁgure predictions deep network grid score-fr output ﬁgure different values clarity show predicted scores top- classes. score values grid position observe increases score true class keeps increasing scores confusing classes vary much. plots show trend observed across entire neighborhood grid. thus inferred perturbations input affect decision deep network contextually consistent manner. observe score true class drops signiﬁcantly even small negative consistent earlier observations. figure qualitative results pascal reduced validation set. rows compare result fcn-s part crfasrnn trained ms.coco dataset publicly released bottom compare complete crfasrnn framework. results found supplementary material. experiments section perform several experiments showing approach could seamlessly applied several pretrained deep networks. test method semantic segmentation task pascal dataset scene labeling task pascal context class dataset classiﬁcation tasks mnist cifar datasets results support approach able generalize across different types probiterates fact approach indeed architecture independent easily integrated even complex feedforward architectures like crfasrnn. table shows evaluation approach pascal test using fcn-s pretrained network base model demonstrate improvement shown method unbiased setting. value used test tuned validation set. scene labeling task dense pixel labeling task evaluated pascal context dataset. classes deﬁned challenge entails evaluating classes speciﬁed frequent labeled classes contain scene elements addition objects appear pascal segmentation challenge making much harder benchmark. evaluate approach task fcn-s model baseline trained standard training split images provided dataset. results generated validation consisting images shown table improve performance fcn-s network signiﬁcant given large size validation set. please note value tuned dataset rather best performing table used. network validation used section speed-performance trade-off guided perturbations generated input layer deep network improves performance base model. however computational overhead performing additional backward forward pass. alternative backward pass could performed intermediate layer deep network instead input layer. section provide results addressing trade computational time resulting performance perturbing layers input. observed table even using perturbed input late pool layer improvement performance remains almost constant computation time drops signiﬁcantly. experiment shows effect observed input also intermediate layers deep network hence leverages reducing computational cost. table results reduced validation images. ’-coco’ denotes model trained coco data addition dataset. numbers brackets show magnitude change compared corresponding base models. evaluate approach using mean intersection union metric commonly used semantic segmentation reported number pixels class predicted belong class total number pixels class formulated mean mnist cifar- classiﬁcation accuracy metric compare baseline. semantic segmentation pascal dataset evaluating approach semantic segmentation task. consists classes including background. following pretrained models baselines show improvement obtained using approach them fcn-s fcn-s models trained using dataset consists images. fcn-s-coco crfasrnn trained using images coco dataset using total images. deeplab evaluate deeplab-vgg resnet models atrous convolutions multi scale evaluation. also trained coco datasets. methods publicly available models time submission. single nvidia titanx experiments caffe library implementation. pretrained models used section obtained caffe model time submission. reported results computed iteration approach unless mentioned otherwise. table shows results applying proposed approach different pretrained models prediction time reduced validation images done observed proposed approach results increased performance listed pretrained models. retable trade-off performance computation times obtained truncating guided perturbations different layers across deep network. original time taken image. baseline performance comparison approaches behavior resembles contextual smoothing provided approaches popularly used semantic segmentation. section provide empirical evidence captures additional dependencies data compared pairwise interactions modeled crfs. table shows mean values cases with/without applied network outputs. results demonstrate indeed captures extra dependencies compared even improve upon outputs. guided perturbations strategies section perform ablative experiment perturb input image different ways order distinguish guided perturbations show yields improvement performance. explained section generate guided perturbation replace softmax output one-hot encoded vector class maximum conﬁdence. consider different methods modify label distribution obtained softmax function follows random-onehot class label chosen uniformly random manner used ground truth instead maximum probability class. uniform-label uniform label distribution produced assigning equal probability classes used encoding generate error gradient. top-label modiﬁed label distribution contains equal probability predicted classes used encoding generate error gradient. figure shows effect different types label distribution segmentation performance. outset observed gives best quantitative performance compared second best case uniform setting negative scores also observe perform corresponds adversarial setting. intuitively setting equivalent maximizing loss softmax classiﬁcation function training. hence backpropagated gradient always moves away correct class. approach always generated setting mentioned sections setting involving choosing random label generate one-hot vector softmax output results poor performance across values since gradient directions become random resulting perturbations adversely affect performance deep network perturbed input image. interesting case analyze figure performance uniform-label setting understand effect figure illustrates example showing difference error gradients generated using uniform-label setting different possible output score distributions cnn. example trained classify among classes. observe that unimodal case gradient signal generated uniform output label distribution relative magnitude gradient signal generated dominant gradient direction exactly opposite. however still gives better performance compared uniform label distribution. case score vector bimodal hence dominant directions gradient signal. notice gradient direction case still points towards correct class directions move away correct class expected. case uniform label distribution competing directions hence higher probability gradient move wrong direction. effect scaling parameter evaluate performance approach using fcn-s fcn-s networks range scaling parameter validation set. figure shows performance varies based scaling factor. observed improvement performance generally obtained wide range values indicates network’s behavior sensitive value though seems optimal value best performance depends deep model. fcn-s fcns network crfasrnn network experiments. section equation perturbed image given foli=)) nearest neighbor; number nearest neighbors weight associated nearest neighbor corresponds loss function. figure shows example network correctly classiﬁes perturbed input generated using procedure. evaluate performance classiﬁcation tested method standard datasets mnist cifar. mnist consists grayscale images digits cifar consists realistic images object classes. follow standard training/testing split cases. nearest neighbor equal weights experiments. mnist conv. layers fully-connected layers architecture cifar conv. fully-connected layers ----- architecture. table results classiﬁcation task mnist cifar datasets. dataset mnist cifar figure mean values several perturbations generated using different types label distributions validation range fcn-s base network. please refer section details. figure difference gradient signal generated between uniform-label setting case unimodal output score distribution bimodal output score distribution dominant gradient direction cases shown colored boxes. exact derivation computing gradient values given supplementary material. figure effect scaling factor performance fcn-s fcn-s networks evaluated reduced pascal validation set. best viewed screen. please zoom clarity. image classiﬁcation method described section semantic segmentation cannot applied directly classiﬁcation tasks. since context classiﬁcation task deﬁned naturally extract contextual information learned feature space. given input image ﬁrst extract feature deep network select nearest neighbors training using euclidean distance metric. perturb test image weighted average gradients generated using class selected nearest neighbors perform forward pass predict ﬁnal output. class nearest neighbor. following notation established table shows results classiﬁcation experiments. improves performance baseline datasets. however improvement performance high segmentation case could attributed reasons base networks learned strong representation context information classiﬁcation task relatively weak compared segmentation task. conclusion paper shown novel self-corrective behavior cnns segmentation classiﬁcation tasks. showed guided perturbations improve network’s performance without additional training network modiﬁcation. demonstrated effect several publicly available datasets using different network architectures. presented several experiments understand explain different aspects guided perturbations. believe behavior lead novel network designs better end-to-end training procedures. zheng jayasumana romera-paredes vineet huang torr. conditional random proceedings ﬁelds recurrent neural networks. ieee international conference computer vision pages", "year": 2017}