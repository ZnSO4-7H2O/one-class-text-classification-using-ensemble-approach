{"title": "A Focused Dynamic Attention Model for Visual Question Answering", "tag": ["cs.CV", "cs.CL", "cs.NE"], "abstract": "Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.", "text": "abstract. visual question answering problems attracting increasing interest multiple research disciplines. solving problems requires techniques computer vision understanding visual contents presented image video well ones natural language processing understanding semantics question generating answers. regarding visual content modeling existing methods adopt strategy extracting global features image video inevitably fails capturing ﬁne-grained information spatial conﬁguration multiple objects. extracting features auto-generated regions region-based image recognition methods cannot essentially address problem introduce overwhelming irrelevant features question. work propose novel focused dynamic attention model provide better aligned image content representation proposed questions. aware words question employs oﬀ-the-shelf object detector identify important regions fuse information regions global features lstm unit. question-driven representations combined question representation reasoning unit generating answers. extensive evaluation large-scale benchmark dataset clearly demonstrate superior performance wellestablished baselines. visual question answering active research direction lies intersection computer vision natural language processing machine learning. even though short history already received great research attention multiple communities. generally investigates generalization traditional problems visual input necessary considered. concretely provide correct answer human posed question concerning contents presented image video. besides contributing advancement involved research areas important applications blind person assistance image retrieval. coming solutions task requires natural language processing techniques understanding questions generating answers well computer vision techniques understanding contents concerned image. help core techniques computer perform reasoning perceived contents posed questions. recently advanced signiﬁcantly development machine learning methods learn proper representations questions images align fuse joint question-image space provide direct mapping joint representation correct answer. example consider following image-question pair image apple tree basket apples next question many apples basket?. answering question requires methods ﬁrst understand semantics question locate objects image understand relation image objects ﬁnally count generate answer correct number apples. ﬁrst feasible solution problems provided malinowski fritz used semantic language parser bayesian reasoning model understand meaning questions generate proper answers. malinowski fritz also constructed ﬁrst benchmark dataset named daquar contains images questions generated humans automatically following template extracting facts database shortly after released toronto–qa dataset contains large number images questions questions automatically generated thus answered without complex reasoning. nevertheless release toronto–qa dataset important since provided enough data deep learning models trained evaluated problem recently antol published currently largest dataset. consists three human posed questions answers given diﬀerent human subjects images found microsoft coco dataset answering questions requires complex reasoning common sense real-world knowledge making dataset suitable true visual turing test. authors split evaluation dataset tasks open-ended task method generate natural language answer multiple-choice task question method chose diﬀerent answers. current performing methods employ deep neural network model predominantly uses convolutional neural network architecture extract image features long short-term memory network extract representations questions. lstm representation vectors usually fused concatenation properly modeling image contents critical factors solving problems well. common practice existing methods modeling image contents extract global features overall image. however using global feature arguably insuﬃcient capture necessary visual information provide full understanding image contents multiple objects spatial conﬁguration objects informative background. issue relieved extent extracting features object proposals image regions possibly contain objects interest. however using features image regions provide much noise overwhelming information irrelevant question thus hurt overall performance. work propose question driven attention model able automatically identify focus image regions relevant current question. name proposed model focused dynamic attention visual question answering. model computers select recognize image regions well-aligned sequence words containing given question. recall example. answer question many apples basket? would ﬁrst localize regions corresponding words apples basket extract description features regions interest. compliments features selected image regions global image feature providing contextual information overall image reconstruct visual representation encoding long short-term memory unit. evaluate compare performance proposed model types tasks i.e. open-ended task multiple-choice task dataset largest benchmark dataset. extensive experiments demonstrate brings substantial performance improvement upon well-established baselines. rest paper organized follows. section review current models compare model. formulate problem explain motivation section describe model section section evaluate compare current state-of-the-art models. conclude work section received great research attention recently couple methods developed solve problem. similar model stacked attention networks proposed yang models attention mechanism combines words image regions. however convolutional neural network attention image regions based question word unigrams bigrams trigrams. further attention mechanism using object bounding boxes makes attention less focused. another model uses attention mechanism solving problems abc-cnn model described abc-cnn uses question embedding conﬁgure convolutional kernels deﬁne attention weighted image features. advantage model abc-cnn fold. first employs lstm network encode image region features order corresponds question word order. second handcrafted weights image features instead extracts features directly cropped image regions interest. sense eﬃcient abc-cnn visual contents modeling. another attention model visual question answering proposed work closely related work also applies weighted image question word features. however similar work object proposals select image regions instead whole image. diﬀerent work proposed model also employs information embedded order question words focuses corresponding object bounding boxes. contrast model proposed straightforwardly concatenate image region features question word features feed layer network. jiang propose another model combines image features lstm network encoding multimodal representation addition compositional memory units fuse image word feature vectors take interesting approach three convolutional neural networks represent image also question common representation multimodal space. multimodal representation softmax layer produce answer. another interesting approach worth mentioning work andreas semantic grammar parser parse question propose neural network layouts accordingly. train model learn compose network proposed network layouts using several types neural modules speciﬁcally designed address diﬀerent sub-tasks problem visual question answering problem represented predicting best answer given image question common practice common answers training thus simplify task classiﬁcation problem. following equation represents problem mathematically baseline methods show modest increase accuracy including image features believe image contains information increase accuracy much more. thus focus improving image features design visual attention mechanism learns focus question related image regions. proposed attention mechanism loosely inspired human visual attention mechanism. humans shift focus image region another understanding regions relate grasping meaning whole image. similarly feed model image regions relevant question hand showing whole image. model composed question image understanding components attention mechanism multimodal representation fusion network section describe individually. following common practice model uses lstm network encode question vector representation lstm network learns keep state feature vectors important question words thus provides question understanding component word attention mechanism. following prior work pre-trained convolutional neural network extract image feature vectors. speciﬁcally deep residual networks model used ilsvrc coco competitions places imagenet classiﬁcation imagenet detection imagenet localization coco detection coco segmentation extract weights layer immediately ﬁnal softmax layer regard visual features. extract features whole image speciﬁc image regions however contrary existing approaches employ lstm network combine local global visual features joint representation. attention mechanism works follows. image object uses wordvec word embeddings measure similarity question words object label. next selects objects similarity score greater extracts feature vectors objects bounding boxes pre-trained resnet model following question word order feeds lstm network corresponding object feature vectors. finally feeds lstm network feature vector whole image uses resulting lstm state visual representation. thus attention mechanism regard ﬁnal state lstm networks question image representation. start fusing single representation applying tanh question representation relu image representation proceed element-wise multiplication vector representations resulting vector fully-connected neural network. finally softmax layer classify multimodal representation possible answers. experiments visual question answering dataset largest complex image dataset visual question answering task. dataset contains three human posed questions answers given diﬀerent human subjects images found microsoft coco dataset figure shows representative examples found dataset. evaluation done following test splits test-dev test-std following tasks compare model baseline models provided dataset authors currently achieve best performance test-standard split multiple-choice task. model ﬁrst described standard implementation lstm+cnn model. uses lstm encode question features encode image. answer question multiplies last lstm state image features feeds result softmax layer classiﬁcation common answers. implementation uses deeper layer lstm network encoding question normalized image features showed crucial achieving state-of-the-art. transform question words vector form multiplying one-hot vector representation word embedding matrix. vocabulary size word embeddings dimensional. feed pre-trained resnet network dimensional weight vector layer last fully-connected layer. word image vectors feed separate lstm networks. lstm networks standard implementation layer lstm network dimensional state vector. ﬁnal state question lstm passed tanh ﬁnal state image lstm passed relu. element-wise multiplication resulting vectors obtain multimodal representation vector fully-connected neural network. table comparison baselines state-of-the-art models model test-dev test-standard data open-ended task. results recent methods including ibowimg dppnet d-nmn d-lstm provided compared with. table comparison baselines state-of-the-art models model test-dev test-standard data multiple-choice task. results recent methods including ibowimg dppnet d-lstm also shown comparison. compare model baselines provided authors results open-ended task listed table results multiple-choice task given table tables question image baselines using question words image respectively. baseline combines lstm network. lstm d-lstm lstm models layers actable observe proposed model achieves best performance benchmark dataset. outperforms state-of-the-art margin around model also employs attention focus speciﬁc regions. however attention model focusing spread regions include cluttered noisy background. contrast focuses selected regions extracts clean information answering questions. main reason outperform although methods based attention models. advantage employing focused dynamic attention signiﬁcant solving multiple-choice problems. table observe proposed model achieves best ever performance dataset. particular improves performance stateof-the-art margin quite signiﬁcant challenging task. d-lstm method employs deeper network enhance discriminative capacity visual features. however identify informative regions answering questions. contrast incorporates automatic region localization employing question-driven attention model. helpful ﬁltering irrelevant noise establishing correspondence regions candidate answers. thus gives substantial performance improvement. figure shows particularly diﬃcult examples what color type questions. focusing question related image regions model still able produce correct answer. figure show examples model focuses diﬀerent regions image depending words question. focusing right image region crucial answering unusual questions image questions small image objects dominant image object partly occludes question related region lead wrong answer representative examples questions require image object identiﬁcation shown figure observe focused attention enables model answer complex questions counting questions question guided image object identiﬁcation greatly simpliﬁes answering questions like ones shown fig. representative examples focusing question related objects helps answer what color type questions. question words bold matched image region. yellow region caption contains question word followed region label parenthesis cosine similarity fig. representative examples model focuses diﬀerent regions image depending question. question words bold matched image region. yellow region caption contains question word followed region label parenthesis cosine similarity fig. representative examples questions require image object identiﬁcation. question words bold matched image region. yellow region caption contains question word followed region label parenthesis cosine similarity work proposed novel focused dynamic attention model solve challenging problems. built upon generic object-centric attention model extracting question related visual features image well stack multiple lstm layers feature fusion. focusing identiﬁed regions speciﬁc proposed questions shown able ﬁlter overwhelming irrelevant informations cluttered background regions thus substantially improved quality visual representations sense answering proposed questions. fusing cleaned regional representation global context question representation lstm layers provided signiﬁcant performance improvement baselines benchmark datasets open-ended multiple-choices tasks. excellent performance clearly demonstrates stronger ability modeling visual contents also veriﬁes paying attention visual part tasks could essentially improve overall performance. future going explore along research line investigate diﬀerent attention methods visual information selection well better reasoning model interpreting relation visual contents questions.", "year": 2016}