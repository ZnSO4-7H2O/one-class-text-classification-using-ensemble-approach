{"title": "A Semisupervised Approach for Language Identification based on Ladder  Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset.", "text": "study address problem training neuralnetwork language identiﬁcation using labeled unlabeled speech samples form i-vectors. propose neural network architecture also handle out-of-set languages. utilize modiﬁed version recently proposed ladder network semisupervised training procedure optimizes reconstruction costs stack denoising autoencoders. show approach successfully applied case training dataset composed labeled unlabeled acoustic data. results show enhanced language identiﬁcation nist language identiﬁcation dataset. purpose automatic language recognition identify language spoken speech sample. many characteristics speech could used identify languages. languages made different sounds form phonemes possible distinguish languages based acoustic features present speech signal. course also lexical information. languages separable vocabulary sets words syntactic rules. study focus language identiﬁcation based solely acoustic information conveyed speech signal. applications language identiﬁcation systems include multilingual translation systems emergency call routing response time ﬂuent native operator might critical. impressive performance improvement obtained using deep neural networks automatic speech recognition motivated application dnns speaker language recognition. trained different purpose learn frame-level features used train secondary classiﬁer intended language recognition task dnns also applied directly train language classiﬁcation systems tion report performance nist language recognition i-vector machine learning challenge task i-vectors examples languages. also unlabeled training data also need address problem out-of-set languages appear unlabeled training data test set. study propose based approach addresses issues unlabeled training data out-ofset examples. previous learning approaches used unlabeled data pre-training initialize weights network followed normal supervised learning. standard pre-training strategy based greedy layer-wise procedure using either restricted boltzmann machines noisy autoencoders contrast aimed explicitly incorporate unlabeled data costfunction optimized training step. approach based recently proposed ladder network training procedure proven successful especially cases training dataset composed labeled unlabeled data addition supervised objective ladder network also unsupervised objective corresponding reconstruction costs stack denoising autoencoders. however instead reconstructing layer time greedily stacking unsupervised objective involving reconstruction layers optimized jointly parameters next section present brief overview ladder networks. describe semisupervised language identiﬁcation system based ladder networks. demonstrate performance improvement proposed method dataset nist language recognition ivector challenge goal train fully connected multi-layer using labeled unlabeled training data. layer network formalized linear transformation followed non-linear activation function softmax used non-linear activation function last layer compute output class distribution ladder network training procedure addition standard forward pass also apply corrupted forward pass implemented adding isotropic gaussian noise input hidden encoding procedure followed denoising decoding pass whose target result noise-less forward propagation layer. decoding procedure deﬁned follows efﬁcients non-linear functions comparison possible choices combinator function presented resulting encoder/decoder architecture thus resembles ladder figure illustrates ladder structure hidden layers. progress applied large labeled datasets rely unsupervised pretraining semisupervised learning might instead crucial success small amount labeled data available. language recognition i-vector challenge covers target languages unnamed out-ofset languages. labeled training data provided target languages approximately unlabeled examples covering target out-of-set languages provided development. test consisted test segments covering target out-of-set languages. speech duration audio segments used create i-vectors challenge sampled log-normal distribution mean approximately speech segments derived conversational telephone narrow-band broadcast speech data. speech segment represented i-vector components assume given training data consisting labeled examples labels also development unspeciﬁed labels. unspeciﬁed labels include target labels multiple additional out-of-set labels. since need take care labels construct whose soft-max output layer outputs corresponding labels out-ofset decision. note order train network explicit decision cannot rely in-set labeled data. batch normalization compute running mean variance hidden units batch whiten hidden units mini-batch. ladder networks batch normalization plays another crucial role preventing encoder decoder collapse trivial solution constant function easily denoised. note standard autoencoder cannot happen since cannot change input feature vector want reconstruct. simplify presentation ladder network ignored batch normalization step incorporated layer ladder network. cost function training network composed supervised objective unsupervised autoencoder objective. supervised cost standard likelihood function deﬁned average negative probability noisy output matching target given input note that unlike standard likelihood function computed based noisy network noise used regularize supervised learning similar weight noise regularization method dropout cost standard denoising autoencoder input reconstruction error autoencoder cost function reconstruction error input along network layers. error deﬁned distance denoised decoded value noise-less forward value layer. unsupervised ladder cost function layer’s width number training samples hyperparameter layer-wise multiplier determining importance denoising cost. noise-free hidden layer obtained input ˆzlt corresponding decoded layer. target encode/decode layer result noise-less forward propagation. note ladderautoencoder cost function labels therefore applied labeled unlabeled training data. recent study conducted extensive experimental investigation variants ladder network individual components removed replaced gain insights relative importance. authors found components necessary optimal performance. ladder networks found obtain state-of-the-art results standard datasets small portion labeled examples conﬁrms current trend deep learning that recent also assumes predeﬁned relative number examples. show experiment section that addition contribution ladder method cost function improves classiﬁcation performance. tried another standard unsupervised cost function based minimizing entropy distribution obtained soft-max layer thus encouraging decision unlabeled data focus languages {oos}. tried computing score different importance weights either noise-free network noisy network. however signiﬁcant performance gain using cost function. soft-max layer network provides distribution possible outputs. ﬁnal decision obtained selecting probable label. predictions made test data measure ratio number examples predicted. ratio pre-deﬁned ratio poos relabel examples expected ratio met. candidate examples relabeling ones probability probable language smallest. ratio pre-deﬁned ratio poos relabel fewer examples expected ratio met. relabeling done multiplying predicted probability reduction bias gives labels higher likelihood picked predicted label. experiment results show procedure indeed helps using standard network training based labeled data however improvement post-processing applied ladder network results standard network. created modiﬁed dataset hyper parameters tuning using labeled part training dataset. randomly selected languages languageset considered data remaining languages applied ladder network training scheme take maximum advantage information conveyed unlabeled data. ﬁrst used ladder reconstruction error cost function described previous section. used additional cost function composed components. ﬁrst score negative likelihood score labeled data soft-output noisy network. denote percentage examples unlabeled development poos. assumed poos given. assumed equal number examples classes unlabeled part training deﬁne estimated average label frequency unlabeled follows practice optimized network parameter using mini-batches computed score minibatch instead entire development set. size mini-batch large enough order assume based large numbers label frequency within mini-batch similar overall label frequency. cost function part original ladder framework. represents assumption labeled training data unlabeled development similar label statistics. shufﬂed iteration. turned unsupervised learning ladder method insensitive number epochs epoch iterations gave similar results. direct skip information encoder decoder used input layer using gaussian method described reconstruction error denoising layers compared noise-free network layers assigned weight input layer ﬁrst hidden layer weight layers. weights respectively. out-of-set. took subset examples labeled training set. used remaining labeled data construct development test contained examples languages. development test sets constructed languages proportion. cross-validation described used hyper-parameters ladder network. main parameters number layers size layer relative importance layer reconstruction score. also tuned relative weights score component network implemented i-vector input dimension passes four relu hidden layers size ﬁnal soft-max output layer encoding step gaussian noise standard deviation added input data intermediate hidden layers. training done mini-batches batch size note size secondary effect loss function averages predictions across minibatch computing loss. training consisted epoch iterations. order samples next report results several variants method described nist language recognition i-vector challenge combination models parameters applied test challenge submitted competition web-site. according challenge rules unknown subset test samples used compute score progress-set results remaining reported web-site. performance score deﬁned smaller number better. table shows progress-set results achieved different models parameters competition site. examined training procedures full ladder structure baseline variant weight ladder reconstruction score zero. baseline method still based noisyforward step which without layer-reconstruction cost function turned regularized learning method similar dropout model tested results using variants training cost function variant based likelihood score computed labeled data. second variant also used cross entropy average probabilities computed unlabeled data. optimal value chosen using cross-validation procedure labeled data described above. note ladder network uses unlabeled data training even contrast baseline method ignores unlabeled data table shows indeed beneﬁcial unlabeled data training well. best results obtained ladder training applied cost function besides likelihood term also used cross entropy average probabilities. table reveals importance cost function propose study combined ladder network training method yielded best results. next examined contribution postprocessing step described section table also shows ratio decisions test data submitted challenge site. tried improve results using post-processing step parameter poos ratio. results shown rightmost column table post-processing step improvement baseline model case likelihood cost function use. note case training step based labeled data. cases post-processing help. ladder model exhibited superior regularization evident cross validation tests made. table shows cross validation performance results ladder baseline model score deﬁned number languages table corresponds different split languages in-set out-of-set languages. selecting different sets languages in-set outof-set yield drastically different results. therefore repeated process times allow language out-of-set least once. stopping achieve best results. contrast ladder score imposes aggressive regularization avoids need early stopping. figure shows cost function minimized training function number epochs. cost function plotted training data test data. clear that baseline method problem overﬁtting could avoided ladder network overﬁtting problems. behavior another advantage ladder network training procedure train network local optimum without risk overﬁtting. conclude study utilized ladder network technique semi-supervised learning language identiﬁcation system. showed ladder network provided improved framework integrating unlabeled data supervised training scheme. ladder network also uses unlabeled data cost function optimized training step. enabled explicitly address out-of-set examples part training process. also used cost function encourages statistics predicted labels unlabeled dataset similar label statistics labeled training data. joint contribution ladder training methods cost function provided signiﬁcant reduction classiﬁcation errors applied nist language recognition challenge. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol.", "year": 2016}