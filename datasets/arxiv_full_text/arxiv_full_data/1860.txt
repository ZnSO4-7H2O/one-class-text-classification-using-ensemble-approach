{"title": "Continuous Semantic Topic Embedding Model Using Variational Autoencoder", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "This paper proposes the continuous semantic topic embedding model (CSTEM) which finds latent topic variables in documents using continuous semantic distance function between the topics and the words by means of the variational autoencoder(VAE). The semantic distance could be represented by any symmetric bell-shaped geometric distance function on the Euclidean space, for which the Mahalanobis distance is used in this paper. In order for the semantic distance to perform more properly, we newly introduce an additional model parameter for each word to take out the global factor from this distance indicating how likely it occurs regardless of its topic. It certainly improves the problem that the Gaussian distribution which is used in previous topic model with continuous word embedding could not explain the semantic relation correctly and helps to obtain the higher topic coherence. Through the experiments with the dataset of 20 Newsgroup, NIPS papers and CNN/Dailymail corpus, the performance of the recent state-of-the-art models is accomplished by our model as well as generating topic embedding vectors which makes possible to observe where the topic vectors are embedded with the word vectors in the real Euclidean space and how the topics are related each other semantically.", "text": "abstract. paper proposes continuous semantic topic embedding model ﬁnds latent topic variables documents using continuous semantic distance function topics words means variational autoencoder. semantic distance could represented symmetric bell-shaped geometric distance function euclidean space mahalanobis distance used paper. order semantic distance perform properly newly introduce additional model parameter word take global factor distance indicating likely occurs regardless topic. certainly improves problem gaussian distribution used previous topic model continuous word embedding could explain semantic relation correctly helps obtain higher topic coherence. experiments dataset newsgroup nips papers cnn/dailymail corpus performance recent state-of-the-art models accomplished model well generating topic embedding vectors makes possible observe topic vectors embedded word vectors real euclidean space topics related semantically. topic models give probability words appearing text discovering latent topic variables distribution words. probabilistic latent semantic indexing provided suggested semantic probabilistic technique analysis co-occurrences words documents latent dirichlet allocation gives bayesian probabilistic generative models allocating latent topic variables generalization plsi. model basically assumes occurrence vocabulary document inﬂuenced latent topic variable represented categorical distribution words. topic variables follow dirichlet distribution parametrized dirichlet priors documents play role mixing coefﬁcients topic distributions. latent variables topics dirichlet priors learned several unsupervised learning algorithms. solved intractability posterior distribution variational bayesian method presented markov chain monte carlo algorithm gibbs sampling inference lda. although methods successfully inferred model variational method converges fast introduced fastlda algorithm enhanced convergence speed times faster standard collapsed gibbs sampler proposed online inference gibbs sampling algorithms. proposed collapsed variational bayesian inference developed online variational bayes algorithm development applying large-scale data applications various area researched actively present. neural network extended area massively text processing well efﬁcient inference method using neural networks called auto-encoding variational bayes intractable posterior introduced method straightforwardly optimize variational lower bound estimator using standard stochastic gradient methods intractable posterior inferred efﬁciently ﬁtting approximate inference model. aevb naturally applied topic model successfully realized autoencoded variational inference topic model proposed paper mainly refer. topic distribution cannot imply correlation topics measured topic coherence count tried introduce multivariate gaussian distribution word-topic distribution fast collapsed gibbs sampling algorithm based cholesky decompositions virtue word representation called wordvec. asserted method achieved average higher topic coherence average. following researches using gaussian introduced models achieve better topic coherence shown state-of-the-art performance. avitm also showed higher topic coherence before. however methods considered inﬂuence word occurrence documents regardless topic assumption continuity words distribution per-topic. think continuous word-topic distribution something semanticity supposed consideration inﬂuence word probability occurrence explained chapter detail. thus paper proposes model named continuous semantic topic embedding model applies continuous word-topic distribution aevb method. presumed word-topic distribution represented continuous function semantic meaning placed semantic ﬁeld constructed euclidean space. considered several semantic distance functions make word-topic distributions real text data well. additionally introduce probabilistic model parameter represents global weight word regardless topics consideration assumption semantic distance word topic could give enough explanation probability words occurrence. claim global parameter missing part continuous topic model. order learn parameter depending datapoints apply full variational autoencoder method presented appendix furthermore model neural distance function learned model reﬂects semantic relation realistically introducing global weight representing commonly word occurs regardless topic. background paper corpus containing documents {w··· consisting vocabularies {w··· size vocabularies. document words {wd··· wdnd} means n-th word d-th document. every random variable regarded one-hot encoding -dimensional vector index elsewhere. latent dirichlet allocation latent dirichlet allocation model base generative model. assumes document latent variable dirichlet conjugate prior categorical distribution topics words categorical document parameter dirichlet distribution. model parameter rk×v representing probability word appearing topic. denotes number topics parameter categorical distribution vocabulary. gaussian order assume gaussian distribution word-topic distribution supposed mean covariance matrix topic denoting wdn|zdn since gaussian distribution continuous domain need continuous real vector. gaussian model uses word embeddings like wordvec could embedded dimension word embeddings. therefore wdn|zdn marginal likelihood analytically tractable variational autoencoder methods inference generative models. since latent variable collapsing needs variational posterior distribution variational parameter marginal log-likelihood corpus log-likelihood document represented called elbo represents kl-divergence. note ﬁrst term elbo kl-divergence varational posterior true posterior second term expected negative reconstruction error. uses gaussian distribution avoid difﬁculty reparametrization trick taking advantage laplace approximation dirichlet distribution gaussian distribution note mean diagonal covariance feed forward neural networks parameters variational posterior compute reconstruction error monte carlo sampling method generating sample simplest distance function satisﬁes condition mahalanobis distance deﬁned note topic embedding vector playing role semantic center word embedding space scaling factor. naturally word-topic distribution deﬁned inversely proportional distance like case zero distance. functions distance function density function cauchy distribution student t-distribution logistic distribution. difference much centralized decided variances. larger variance likely model could estimate words relatively far. paper mahalanobis distance used simplicity. probability certain word occurring topics decided semantic distance word topic also global tendency occurrence word itself. instance consider topic football word hat-trick semantically much closer word take word take speciﬁc relation topic football. however easily estimates word take occurring word hat-trick topic football hat-trick happening much often football game word take occurs frequently since commonly used word matter topic contained. therefore generative model factor representing weight vocabulary model real data accurately. note factor nothing semantical relation thus dependent topics. assumption depends global weight parameter dirichlet joint probability word occurring document would embedding vector word wemb word embedding topic mean vectors topic covariance matrix matrix simply speaking probability certain word occurring exponentially proportional square inverse semantic distance global model parameters expected learned weight note wemb represents model parameters confuse notation. deﬁnition different semantic distance deﬁned differently. equation makes possible maintain structure topic vectors word vectors time. combining marginal likelihood equation ultimately represents structure generative model cstem. inference cstem executed discussed above. next section derives process inference framework. variational objective function deﬁne variational objective function maximize assume variational posteriors topic prior global weight parameter induced laplace approximation dirichlet distribution equation suggest variational posteriors close distribution true priors order regularize them. clear even though parameters word-topic distribution could inferred variational like distribution well assume prior distribution infer assuming constants optimized stochastic method couple reasons; necessary regularize topic mean vectors covariance vectors make simple considering memory issue. matrix denotes softmax function calculated respectively. graphical model inference presented fig. using stochastic gradient variational bayes minibatches size sampling size monte carlo estimated elbo experiments takes space exceeding limit experiment resources. although full matrix certainly supposed bring better optimization used diagonal matrix still shows good enough result leave full matrix future work. model proposed above carried several experiments corpora newsgroups cnn/dailymail nips papers. every vocabulary trimmed meaningless words like auxiliary verbs prepositions articles singularized lemmatized stemmed nltk frequent words. newsgroups training documents test documents excluding documents whose number vocabularies less cnn/dailymail randomly picked training documents training test set. nips total papers split training test them. pre-trained wordvec glove initial vectors wemb better performance even though random initialization still works. since dimension word representation decides dimension topic embedding space dimension embedding space chosen dimension wordvec glove. every code implemented python tensorflow based code avitm implementation. model trained adam optimizer learning rate calculating machine nvidia tesla since experiments executed unsupervised learning standard measure models well required traditionally perplexity used purpose. however measure recently treated cannot reﬂect topic coherence well thus evaluation methods developed recently perplexity topic coherence measures npmi scoreumass calculated average value every pairs words topic compare model standard collapsed gibbs sampling prodlda outperforms model paper. expect similar perplexity topic coherence prodlda maintaining high topic coherence also implementing continuous structure order http//qwone.com/ jason/newsgroups/ http//scikit-learn.org/stable/datasets/twenty newsgroups.html http//cs.nyu.edu/ kcho/dmqa/ https//www.kaggle.com/benhamner/nips-papers/data http//www.nltk.org/ https//code.google.com/archive/p/wordvec/ https//nlp.stanford.edu/projects/glove/ https//github.com/akashgit/autoencoding topic models continuous representations topics words euclidean space. hyperparameters applied model mini-batch size epochs executed prodlda cstem epochs gibbs long elapsed time. table shows perplexities models. cstem follows right behind prodlda train test set. large train test set. seems overﬁts dataset models succeed minimize generalization error. cstem actually expected exceed prodlda’s perplexity cstem structural limit inference word-topic distribution prodlda limit distribution inferring weights freely. however perplexity nothing topic coherence thus compare topic coherence measures them. table shows cstem model show higher topic coherence models words used certain number. means model ﬁnds topic data better wide range prodlda ﬁnds small number keywords better others. expected phenomenon since cstem learns model structure topic-word relation generally euclidean space whereas model assume sharing space cstem speciﬁc weights topic. structure cstem therefore shows higher performance general aspect much well local perspective. thus model suitable purpose general topical analysis datasets. elapsed time model seconds collapsed gibbs prodlda cstem respectively. table provides comparison topic coherences cstem topic numbers. since much difference observed number topics topic coherence could topics text detail increase number topic without losing much topic coherence. table comparison topic coherences using newsgroup dataset. numbers means number words used evaluate scores topic. bold ones represent highest score among models experiment. higher numbers evaluation cstem shows better score others. table shows words whose global weight parameter topic. result implies model learns global weights high common words strong topical characteristic though topic-speciﬁc words list. considering this could surmise global weight word plays important roles; model dataset accurately adding free parameter learns semantic distance function semantically excluding global occurrence factor weight concentrating word-topic relation. second role global weight makes list geometrically closest words topic consist semantically closest words without words high global frequency like words table actually model without global weight learned topics contain common related words ﬁnds topic whose words mostly consist common words. think structure word-topic distribution ﬁltered factor topic model originally pursue. cases. moreover higher dimension used better ﬁnds topics word representation model. implies wordvec proper model glove model well higher dimension embedding space used initialization leads better performance easily predictable. thus could expect nicer results better word representation. hear ﬂoor admit resolve greatest someone fell deliver structure surgery lead better record heat imagine false recovery feature become utility performance index contour explore circle play randomize table comparison topic coherence word representation models used initialization word embedding matrix wemb. numbers parenthesis dimension embedding space. model generates embedding vectors every word topic euclidean space. expect word vectors located close semantically related topic vectors close word vectors belong them. thus plot words topics experiment using news dataset topics dimension reduction pca. even though distorts embedding vectors since dimension vectors reduced observing figure words words topic plotted check words topics gathered semantically expected. word vectors re-optimized according per-topic relations comparison initial word embedding vectors. besides virtue topic embedding euclidean space topics semantical similarity gathered geometrically observing embedding vector pca. fig. vector topic named instance close topic named world. actually similar meaning corpus since nations’ name topic world commonly mentioned articles israel korea iran. topical relations like criminal sports culture. could conclude semantic distance learned actually plays role semantic distance topics themselves. since distance deﬁned continuously could generate custom topic want inﬁnitely picking vector embedding space. main achievement paper make semantic distance semantic meaning adding global weight factors. fig. word topic embedding graph news corpus dimension reduction pca. represents position word embedding vector star represents position topic embedding vector plotted words consist words topic allowing repetition topics. radius word proportional global weight every location distorted dimension reduction. paper proposed topic model cstem generative model allocating latent topic variables learning semantic distance function global word weights. assumes probability words occurrence weight certain topic exponentially proportional inverse properly deﬁned semantic distance global occurrence word regardless topic belongs structure makes possible generate continuous embedding vectors words topics euclidean space well relation topic variables words. model latent variables infer inferred method. unlike requiring maximizing likelihood data point depend data point leads maximizing likelihood full dataset. executed optimization using reparametrization trick elbo derived monte carlo method. model results reasonable topic coherence well perplexity measures goodness topic models. additionally checked model appropriate purpose ﬁtting text data global analysis since model outperform models measure topic coherence words. learning time model takes longer prodlda degree still much shorter inference based sampling method collapsed gibbs lda. paper done experiments comparing recent topic model like gaussian cgtm enough resources. done concrete veriﬁcation. however paper still means something since achieved better performance prodlda short time continuous embeddings addition. lacks experiments effect choice semantic distance function mahalanobis function like gaussian distribution cauchy distribution. study semantic distance would meaningful. discussed above assume covariance matrix topic diagonal matrix memory issue. better environment provided full matrix give higher accuracy though much slower. additionally research applications using generated embedding vectors model done future.", "year": 2017}