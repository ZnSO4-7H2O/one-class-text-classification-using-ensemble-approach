{"title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "tag": ["cs.CV", "cs.AI"], "abstract": "This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form \"true/false\" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses.", "text": "paper presents restricted visual turing test story-line based deep understanding long-term multi-camera captured videos. given videos scene sequence story-line based queries task provide answers either simply binary form true/false accurate natural language description queries polar nonpolar consist view-based queries answered particular camera view scene-centered queries involves joint inference across different cameras. story lines collected cover spatial temporal causal understanding input videos. data queries distinguish recently proposed visual question answering images video captioning. vision system proposed perform joint video query parsing integrates different vision modules knowledge base query engine. system provides uniﬁed interfaces different modules individual modules reconﬁgured test method. provide benchmark dataset toolkit ontology guided story-line query generation consists hours videos captured four different locations queries split story lines also provide baseline implementation result analyses. past decades seen tremendous progress individual vision modules image classiﬁcation object detection especially competitions like pascal imagenet ilsvrc convolutional neural networks trained imagenet dataset figure illustation depth complexity proposed deep scene event understanding focuses largely unexplored task computer vision joint spatial temporal causal understanding scene event multi-camera videos. text details. proposed. tasks evaluated based either classiﬁcation detection accuracy focusing coarse level understanding data. area natural language text processing well-studied text-based question answering example chatterbot named eugene goostman reported ﬁrst computer program passed famed turing test event organized university reading. success text-based recent achievements individual vision modules inspired visual turing tests image-based questions story-line queries used test computer vision system. suggested suitable evaluation framework going beyond measuring accuracy labels bounding boxes. existing work focus images emphasize free-form open-ended q/a’s paper interested restricted visual turing test story-line based visual query answering long-term multi-camera captured videos. emphasizes joint spatial temporal causal understanding scenes events largely unexplored computer vision. restricted mean queries designed based selected ontology. figure shows examples dataset. consider question shall test whether computer vision system understands example conference room. input image bag-of-questions task provide natural language answer understand conference room input consists multi-camera captured videos storyline queries covering basic questions difﬁcult ones involving spatial temporal causal inference deeper understanding. speciﬁcally answer correctly computer vision system would need build scene-centered representation conference room detect track re-identify parse people coming room across cameras understand concept sitting chair etc. computer vision system unfold intermediate representation explicitly show derives answer enhances trust system gain correct understanding scene. long-term multi-camera captured videos. web-scale images emphasize breadth computer vision system learn handle different applications. images often album photo styles collected different image search engines flickr google bing facebook. paper focuses long-term multi-camera captured videos usually produced video surveillance also important data sources visual data epic important security enforcement applications. furthermore example figure shows mutli-camera videos facilitate much deeper understanding scenes events. types datasets complementary latter explored setting. free-form open-ended q/a’s restricted storyline based queries. free-form open-ended q/a’s usually collected crowd-sourcing platforms like amazon mechanical turk achieve diversities. however hard obtain well-posed pairs massive amount untrained workers internet. challenging even simple tasks like image labeling investigated imagenet dataset labeldataset video datasets paper impractical mturk collect story-line based queries covering long-term temporal ranges across multi-cameras. instead adopt selected sufﬁciently expressive ontology generating queries. following statistical principles stated geman al.’s turing test framework design easyto-use toolkit several people certain expertise create large number story lines covering different interesting important spatial temporal causal aspects videos quality queries answers controlled. quest integrated vision system. almost recent methods proposed image captioning based combination convolutional neural network recurrent neural network like long shortterm memory hand exciting much progress made terms performance. hand shows restricted setting tasks image captioning vqa. proposed entails integrated vision system cannot handled training convolutional recurrent neural networks directly best knowledge. present prototype vision system baseline implementation integrates different vision modules knowledge base query engine. overview multi-camera video dataset collection existing datasets either focusing single individual images short video sequences clear action event boundaries. multiple-camera video dataset includes rich activities indoor outdoor scenes. videos collected multiple cameras overlapping ﬁeld-of-views time window. variety types sensors used stationary video cameras located ground rooftop moving cameras mounted bicycles automobiles infrared cameras. camera parameters provided meta data. videos capture daily activities group people different events scene include routine ones abnormal ones large appearance structural variations exhibited. ontology guided story-line based query/answer collection interested selected ontology listed figure ontology sufﬁciently expressive represent different aspects spatial temporal causal understanding videos basic level ﬁne-grained level based ontology build toolkit story-line query generation following statistical principles stated queries organized multiple story lines designed evaluate computer vision system basic object detection queries complex relationship queries probe system’s ability reasoning physical social perspectives entails human-like commonsense reasoning. crosscamera referencing queries requires ability integrate visual signals multiple overlapping sensors. iii) integrated vision system build computer vision system used study organization modules designed different tasks interactions improve overall performance. designed principles mind ﬁrst well-established computer vision tasks shall incorporated built upon existing achievements; second modules shall loosely coupled allows user replace modules alternatives study performance integrated environment. deﬁne apis individual task connect modules pipeline. system processed input videos saved results knowledge-base fetches queries evaluation server another testing time. evaluation server provide service computer vision system interact evaluation server http connections. evaluation server iterates stream queries grouped scenes. scene queries grouped story lines. query available system previous story lines previous queries story line ﬁnished. correct answer provided system query. information used system adaptive ability learn provided answers. answer used figure illustration prototype vision system vtt. top-left input videos people playing baseball games. middle-left illustration ofﬂine parsing pipeline performs spatial-temporal parsing input videos. bottomleft visualization parsed results. bottom-right knowledge base constructed based parsing results form relation graph. top-right example story line queries. graph segments used answering queries highlighted. update previous understanding conﬂict resolved wrong interpretations discarded. figure shows example full workﬂow system. spent person-year total collect data build whole system. prototype system passed detailed third-party evaluation involving queries. plan release whole system computer vision community organize competition regular workshop near future. question answering natural effective communication human beings. integrating computer vision natural language processing well modal knowledge topic recent development deeper image scene understanding. visual turing test. inspired generic turing test principle geman proposed visual turing test object detection tasks images organizes queries story lines within queries connected complexities increased gradually similar conversations human beings. similar spirit malinowski fritz proposed multi-word method address factual queries scene images. dataset evaluation framework proposed paper adopt similar evaluation structure focus complex scenario features videos overlapping cameras facilitate broader scope vision tasks. image description visual question answering. beyond labels bounding boxes image tagging image captioning video captioning proposed recently. state-of-the-art methods shown however coarse level understanding image together natural language n-gram statistics sufﬁces generate reasonable captions. microsoft coco provides descriptions captions images. question answering focuses speciﬁc contents image evaluate system’s abilities using human generated question. unlike image description task generated sentence consider correct long describes dominant objects activities image human generated questions details even hidden knowledge require deduction. scenario pre-trained end-to-end system necessarily perform well question space large covered training data. converts image descriptions pairs. evaluates free-formed open-ended questions images question-answer pairs given human annotators. although encourages participants pursuit deep speciﬁc understanding image focuses content image address many fundamental aspects computer vision like scene parsing camera registration etc. moreover actions static concepts temporal information largely missing images. contributions paper makes main contribution deep scene event understanding presents visual turing test benchmark consisting long-term multi-camera captured video dataset large number ontology-guided storyline based queries. section introduce video dataset collected vtt. dataset organize data multiple independent scenes. scene consists video footage eight twelve cameras overlapping ﬁelds view time period. total number collections captured different locations indoor outdoor table gives summary data collections. rich events activities. activities captured dataset involves different degrees complexities simplest single-person actions group sport activities involve many dozens people. unknown action boundary. unlike existing action activity dataset action data point well segmented segment contains single action dataset consists multiple video streams. actions activities pre-segmented multiple actions happen time. characteristic preserves information spatial context action correlation multiple actions. varied scales view points. data collected resolution however difference cameras’ mounting points person occupies couple hundred pixels bird’s-eye views occlude entire view frame stands close ground camera. illumination variation. areas covered different cameras different illumination conditions areas covered dark shadows whereas areas heavy reﬂection. infrared cameras moving cameras. apart regular signals dataset provides infrared videos supplementary. moving cameras also provide additional challenges table summary selected subset data. bottom results detection tracking. detection calculated pascal based results faster-rcnn tracking mota motp calculated multiple object tracking benchmark based results complexity dataset. demonstrate difﬁculties dataset conduct experiments typical subset data using state-of-the-art object detection models multiple-object tracking methods summary data results shown table query ﬁrst-order logic sentence composed using variables predicates logical operators arithmetic operators quantiﬁers answer query either true false meaning whether fact stated sentence holds given data system’s state belief. formal language representation eliminates need natural language processing allows focus computer vision problems constrained predicates. evaluate computer vision systems asking sequence queries organized multiple story lines. story line explores natural event across period time similar conversations humans. beginning story line major objects interest deﬁned ﬁrst. vision system evaluation shall indicate whether detects objects. correct detection establishes mutual conversation context consecutive queries ensures vision system queries referring objects later interactions. system fails detect object however evaluation server skip queries regarding object. because neither answering queries correctly wrongly reveals system’s performance interpreting designated data. ontology. time either view-centric frame number particular video scene-centric wall clock time. location either point bounding represented diagonal points point speciﬁed either view-centric coordinates scene-centric coordinates example object definition query regarding person form ﬁrst-order logic sentence would look like non-deﬁnition queries. non-deﬁnition queries story line explores system’s spatial temporal causal understanding events scene regarding detected objects. query space consists possible combinations predicates ontology detected objects arguments. expressing complex activities relationships multiple predicates typically conjuncted form query. example suppose detected people conﬁrmed object detection queries following query states male female clear line sight time male∧female∧clear-line-of-sight. note location speciﬁed identiﬁed detected assume vision system track time. moreover story lines unfold ﬁne-grained knowledge event scene goes. particular given detected objects established context querying objects interacting detected ones becomes unambiguous. example shown figure even ball speciﬁed object deﬁnition queries people interacting ball identiﬁed becomes legitimate female catches ball time times locations speciﬁed object deﬁnition queries extension time period speciﬁed starting time ending time. correctly answering queries non-trivial requires joint cognitive reasoning based spatial temporal casual information across multiple cameras time period. currently created queries dataset. figure shows distribution predicates selected categories. though unbiased general consider predicates common important others thus make distribution non-uniform. example among occurrence object predicates person takes reasonable human activities major point interest. meanwhile also building query generation toolkit vatic rapid query creation respect statistical properties discussed geman implementation queries presented form documents shown figure easy parsing. designed implemented computer vision system perform test shown figure consists three major parts ofﬂine parsing pipeline decompose visual perception multiple sub-tasks knowledge base stores parsing results query engine answers queries searching knowledge base. system also features ﬂexible architecture visualization toolkit. ofﬂine parsing pipeline processes multiple-view videos. view ﬁrst processed single-view parsing pipeline video sequences multiple cameras handled independently. multiple-view fusion matches tracks multiple views reconciles results single-view parsing generates scene-based results answering questions. take advantage achievements various sub-areas computer vision organize pipeline modules focuses particular group predicates generating corresponding labels input data. every module gets access original video sequence products previous modules pipeline. implemented modules described follows. components derived state-of-the-art methods time developed system last year. scene parsing generates homography matrix sensor camera calibration also produces estimated depth segmentation label camera view. implementation derived employ generic graph-based data model store knowledge. detected objects actions attribute labels modeled nodes connections modeled edges. implementation parsing results stored resource description framework graphs triple expressions queried standard query language sparql given questions formal language query engine ﬁrst parses query transforms query sequence sparql statements. apache jena used execute statements return answers derived knowledge base. figure shows architecture query engine. figure screenshot visualization tool. shows videos four different views detected objects. bottom detected objects projected scene. videos scene share playback timeline. practice infeasible pre-calculate possible predicates save individual knowledge segment knowledge base. example pre-calculating clear-line-of-sight relationships would involve pairwise combination across detected humans. strategy obviously inefﬁcient portion data being queried predicate actually sparse. alternatively designed online computation module evaluates binary trinary relationships testing time predicates appear query. evaluation protocols. computer vision system talks evaluation server http connections. beginning evaluation system ﬁrst acquires session evaluation server. system repeatedly request next available scene storyline query session evaluation server. protocol evaluation server maintains states evaluation sessions internally ensures vision system cannot overwrite submitted answer query. system architected goals bearing mind ﬁrst want incorporate existing tasks computer vision; second architecture shall ﬂexible enough replacing module alternatives pursuit incremental improvements later. deﬁned apis vision task connect modules using remote procedure calls enables system focus logical connection modules provides implementation ﬂexibility individual components. practice deploy modules onto different dedicated machines. interfaces computation-intensive algorithms usually utilize internally pursuit faster calculation data parallelism. design allows system experiment platform switching alternative models implementations studying effects contributions query answering. prototype system evaluated independent third-party company collected datasets created polar queries subset data company invited administrate independent test grant worked. test testing data available system weeks story-line query evaluation. performed ofﬂine parsing within weeks deploying system small cluster consisting workstations. evaluation system utilize ground-truth answers received response consecutive queries. figure results breakdown. left right histogram unique number queries length accuracies breakdown histogram queries category accuracies breakdown. initions successfully detected non-deﬁnition queries either provided binary true/false answers claimed unable respond table shows accuracy ratio correctly answered queries number responded non-deﬁnition queries. note evaluation simplicity object deﬁnition queries included accuracy calculation establish mutual knowledge consecutive queries story line ensures evaluation server system discussing objects. therefore ground-truth answers queries actually always true. obtain accuracy object deﬁnition queries trivial method risk discussing objects consecutive queries. extending generating object deﬁnition queries answers false evaluating detection performance. queries serve establish conversation context therefore story lines starting object deﬁnition query whose ground-truth answer false randomly sample predicates relations generate remaining queries. breakdown number predicates. queries either three predicates. natural result choice avoid overcomplicating queries. number predicates increases accuracy prototype system decreases since wrong prediction predicates cause answering query incorrectly. queries three predicates mostly explained follows predicates queries mostly queries involving unary predicates operating object. predicate used deﬁne object unary predicate second predicate involved. breakdown category. looking accuracy categories prototype system perform well classic computer vision tasks however queries involving spatial reasoning interactions human objects scene still challenging open research. paper presented restricted visual turing test deeper scene event understanding longterm multi-camera videos. emphasizes joint spatial temporal causal understanding utilizing scene-centered representation story-line based queries. dataset queries distinguish proposed recent proposed visual question answering also presented prototype integrated vision system obtained reasonable results vtt. on-going work generating story-line based queries setting website holding competition. proposed competition release whole system playground. system architecture allows user substitute modules methods improvements. next steps create publicly available vision module market researchers evaluate different individual components perspective besides traditional metrics. acknowledgement. work supported darpa msee darpa simplex n--c-. would like thank josh walters colleagues systems third-party collaborator project administrated test alexander grushin colleagues i-a-i effort testing system. also thank members vcla ucla contributed perception algorithms published work baseline test.", "year": 2015}