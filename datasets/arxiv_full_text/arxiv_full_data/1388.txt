{"title": "Generalizing the Convolution Operator to extend CNNs to Irregular  Domains", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Convolutional Neural Networks (CNNs) have become the state-of-the-art in supervised learning vision tasks. Their convolutional filters are of paramount importance for they allow to learn patterns while disregarding their locations in input images. When facing highly irregular domains, generalized convolutional operators based on an underlying graph structure have been proposed. However, these operators do not exactly match standard ones on grid graphs, and introduce unwanted additional invariance (e.g. with regards to rotations). We propose a novel approach to generalize CNNs to irregular domains using weight sharing and graph-based operators. Using experiments, we show that these models resemble CNNs on regular domains and offer better performance than multilayer perceptrons on distorded ones.", "text": "convolutional neural networks become state-of-the-art supervised learning vision tasks. convolutional ﬁlters paramount importance allow learn patterns disregarding locations input images. facing highly irregular domains generalized convolutional operators based underlying graph structure proposed. however operators exactly match standard ones grid graphs introduce unwanted additional invariance propose novel approach generalize cnns irregular domains using weight sharing graph-based operators. using experiments show models resemble cnns regular domains offer better performance multilayer perceptrons distorded ones. cnns state-of-the-art performing supervised learning data deﬁned lattices contrary classical multilayer perceptrons cnns take advantage underlying structure inputs. facing data deﬁned irregular domains cnns cannot always directly applied even data present exploitable underlying structure. example data spatio-temporal time series generated internet things devices. typically consist datapoints irregularly spaced euclidean space. result graph deﬁned vertices devices edges connect neighbouring ones. examples include signals graphs including brain imaging transport networks words graphs. examples signal generally seen vector order graph. such vertex associated coordinate edges weights represent association corresponding vertices. disregarding graph mlps applied datasets. interested deﬁning convolutional operators able exploit well possible embedding euclidean space. motivation imitate gain performance allowed cnns mlps irregular domains. graph signal processing extended convolutional operators using spectral graph theory proposed operators applied deep learning obtain performance similar cnns regular domains despite fact differ classical convolutions. however slightly distorted domains obtained operators become non-localized thus failing taking account underlying structure. propose another approach generalizes convolutional operators taking account namely make sure proposed solution properties inherent convolutions linearity locality kernel weight sharing. apply directly graph structure. substitution convolutional layer stress resulting technique comparative benchmarks showing signiﬁcant improvement compared mlp. obtained operator happens exactly classical convolutional applied regular domains. outline paper follows. section discuss related work. section introduce proposed operator. section explain apply cnn-like structures. section contains experiments. section conclusion. graph-structured data bruna proposed extension using graph signal processing theory convolution deﬁned spectral domain related laplacian matrix graph. such case graph lattice construction analogous regular convolution deﬁned fourier domain. operation deﬁned spectral multipliers obtained smooth interpolation weight kernel explain ensures localization property. paper also deﬁne construction creates multi-resolution structure graph allowing support deep learning architecture. henaff extended bruna’s spectral network large scale classiﬁcation tasks proposed supervised unsupervided methods underlying graph structure isn’t already given. however graph irregular partly lose localization property convolution partially lost. spectral domain undirected respect graph domain sort rotation invariances also introduced. hence results graph domain aren’t ressembling convolution. case want deﬁne convolution supported locally. moreover want every input deﬁned different underlying graph structure learnt ﬁlters applied data embedded space regardless structure supported convolution training. properties also retained convolution deﬁned shapenet paper deﬁnes convolution data living non-euclidean manifolds. construction maintain locality allow reusing learnt ﬁlters manifolds. however requires least manifold embedding data geodesic polar coordinates system. although less speciﬁc proposed method strict generalization sense cnns special case case data sparse graham proposed framework implement spatially sparse efﬁciently. underlying graph structure embedded euclidean space convolution propose paper seen spatially sparse sense data non-zero coordinates vertices graph deﬁned case want deﬁne convolution inputs values point embedding space whereas regular case inputs values vertices underlying grid. points deﬁnes neighbourhoods. entry dataset column vector size dimension represents value taken certain point said activated point associated dimension dimension denote value taken either said embedded entries homogeneous size dimensions always associated points. must linear entry matrix unless entries homogeneous depends example case regular convolutions images toeplitz matrix doesn’t depend order meet locality condition ﬁrst want coordinates local meaning. impose lives space homogeneous. secondly want function values taken points contained certain neighbourhood results rows generally sparse. let’s attribute kernel weights form vector let’s deﬁne allocation matrices binary matrices non-zero coordinate column. must share weights across activated points maintain locality column must non-zero coordinate activated points neighbourhood. attributes let’s denotes let’s denotes block column vector matrices tensor product. hence deﬁned weight kernel allocation maintains locality vertices activated points entry deﬁnes complete oriented graph call entries homogeneous said static. case note otherwise different graph entry kernel weights re-used entries deﬁned different graphs. suppose given maps neighbourhood deﬁne subgraph contains edge allocation column non-zero coordinate edge generalized convolution couple supported underlying graph adjacency matrix. note underlying graph regular convolution lattice. also note family seen local receptive ﬁelds generalized convolution seen distributing kernel weights generalized convolution deﬁned operation kernel weight vector. denotes graph operator respect third rank tensor could also deﬁned bilinear operator graph signals note underlying graph depends entry. learnt ﬁlter reusable regardless entry’s underlying graph structure. case convolutions deﬁned ﬁxed graphs like two-dimensional euclidean space let’s suppose let’s denote generalized convolution shaped rectangular window suppose weight kernel size width height given unit scale. let’s consider grid size rectangle breaks squares side length let’s associate different weight square. then edge affect weight associated square within falls grid centered procedure allows weights shared across edges. illustrated ﬁgure note entries homogeneous activated points vertices regular grid matrix independant toepliz matrix acts regular convolution operator entries case regular convolution. example case section image dataset. entries homogeneous dimensions represent value pixel. case dimension pixel located entire coordinates. precisely images width height pixels located coordinates n}x{ hence pixels regular grid thus spaced constant distance let’s consider static underlying graph generalized convolution rectangular window deﬁned former section. then applying weight allocation strategy lead affect every weight kernel moving window except border pixel fall square moving grid position depicted ﬁgure indeed behaves exactly like moving window standard convolution except considers images padded zeroes borders. layers neurons forward-propagation deﬁned ld+. let’s deﬁne layers neurons located layers must contain many neurons points activated. terms ld+. such abusively term neuron instead point. generalized convolution layers interpreted follow. entry activates neurons layer. then convolution shape takes positions onto position associated activated neurons ld+. position connections drawn activated neurons located inside convolution shape destination associated neuron. subset weights affected connections according weight sharing strategy deﬁned allocation map. figure illustrates convolution shaped rectangular window. forward backward propagation applied using described neurons connections. generalized convolution operation activation function applied output neurons. pooling operation done spatially input layer divided patches size activated neurons included patch pooled together. unlike standard pooling operation number activated neurons patch vary. generalized convolution layers vectorized. multiple input channels multiple feature maps. shall naturally placed kind deep neural network structure cnn. thus irregular input spaces standard convolution layers regular input spaces. main strategies implement propagations. ﬁrst start derive vectorize implies handling semi-sparse representations minimize memory consumption adequate semi-sparse tensor products. instead decide neural network interpretation underlying graph structure whose edges amount neurons connections. mean sparse part computations included graph. also computations edge parallelized. let’s ﬁrst recall propagation formulae neural network point view. let’s denote value neuron located neuron neuron activated activation function denote prev neurons previous layer connected next next layer connected weight affected connection neurons bias term associated ld+. forward propagation values neurons determined call edges edges weight affected. edges denotes value destination neuron denotes value origin neuron. back propagation allows express derivative weight sets prev next edges determined graph structure turn determined beforehand procedure like described section particularization propagation formulae sets main difference standard formlulae. computations done batch entries hence graph structure used computations must contain weighted edges entries necessary entries made homogeneous neuron activated entry activated another entry deﬁned zero. third-rank tensor counterparts thus denoted third dimension indexes channels submatrix along neuron located denoted rows indexing entries columns indexing channels. counterparts ﬁrst third-rank tensor second vector dimensions denotes submatrix along kernel weight rows index feature maps columns index input channels. notations convolution formulae rewrites order measure gain performance allowed generalized irregular domains made series benchmarks distorded versions mnist dataset consisting images pixels. distort input domain plunged images euclidean space giving entire coordinates pixels. then applied gaussian displacement pixel thus making data irregular unsuitable regular convolutions. multiple values standard deviation displacement trained generalized compared number parameters. choosed simple standard architecture order better impact generalized layers. architecture used following generalized convolution layer relu pooling made feature maps followed dense layer softmax output layer. generalized convolution shaped rectangular window width height unit scale chosen equal original distance pixels. max-pooling done square patches side length dense layer composed hidden units terminated relu activation well. order number parameters compared dense layers hidden units followed output layer. training used stochastic gradient descent nesterov momentum regularization generalized distorded domain performs better mlp. indeed case domain distortion score zero-padding. error rate goes distortion. even generalized still performs better mlp. paper deﬁned generalized convolution operator. operator makes possible transport paradigm irregular domains. retains proprieties regular convolutional operator. namely linear supported locally uses kernel weights local operation. generalized convolution operator naturally used instead convolutional layers deep learning framework. typically created model well suited input data underlying graph structure. deﬁnition operator ﬂexible enough allows adapt weight-allocation input domain depending case distribution kernel weight done natural domain. however cases natural multiple acceptable methods deﬁne weight allocation. works plan study methods. also plan apply generalized operator unsupervised learning tasks. would like thank academic mentors vincent gripon grégoire mercier helped work well industrial mentor mathias herberts gave insights view applying designed model industrial datasets. work partly funded cityzen data company behind warp platform anrt cifre also european research council european union’s seventh framework program grant agreement number", "year": 2016}