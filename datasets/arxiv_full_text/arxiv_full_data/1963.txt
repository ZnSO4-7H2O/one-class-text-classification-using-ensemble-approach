{"title": "Expert Gate: Lifelong Learning with a Network of Experts", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.", "text": "able operate generic well ﬁne-grained classes addition performs action scene classiﬁcation. previous training data available direct solution would jointly train model different tasks domains. time task arrives along training data layers/neurons added needed model retrained tasks. solution three main drawbacks. ﬁrst risk negative inductive bias tasks related simply adversarial. second shared model might fail capture specialist information particular tasks joint training encourage hidden representation beneﬁcial tasks. third time task learned whole network needs re-trained. apart drawbacks biggest constraint joint training keeping data previous tasks. difﬁcult requirement especially data. example ilsvrc classes million images amounting data. alexnet model trained dataset difference size three orders magnitude. increasing amounts data collected becomes less less feasible store training data practical store models learned data. paper introduce model lifelong learning based network experts. tasks experts learned added model sequentially building learned before. ensure scalability process data previous tasks cannot stored hence available learning task. critical issue context addressed literature relates decision expert deploy test time. introduce gating autoencoders learn representation task hand test time automatically forward test sample relevant expert. also brings memory efﬁciency expert network loaded memory given time. further autoencoders inherently capture relatedness task another based relevant prior model used training expert ﬁne-tuning learningwithout-forgetting selected. evaluate method image classiﬁcation video prediction problems. deep learning data face situation train ever complicated models ever increasing amounts data. different models different tasks trained different datasets expert domain others. typical setting task comes dataset. learning task scene classiﬁcation based preexisting object recognition network trained imagenet requires adapting model classes ﬁne-tuning data. newly trained network performs well task degraded performance ones. called catastrophic forgetting major problem facing life long learning techniques tasks datasets added sequential manner. soft outputs model task data generate virtual labels) retraining phase works extent unlikely scale repeating scheme number times causes bias towards tasks exponential buildup errors older ones show experiments. moreover suffers drawbacks joint training described above. instead network jack trades master none stress need different specialist expert models different tasks also advocated therefore build network experts expert model added whenever task arrives knowledge transferred previous models. increasing number task specializations number expert models increases. modern gpus used speed training testing neural nets limited memory load relatively small number models time. obviate need loading models learning gating mechanism uses test sample decide expert activate reason call method expert gate. unlike train uber network performing vision tasks diverse semantic segmentation object detection human body part detection work focuses tasks similar objective. example imagine drone trained environment using frontal camera. optimal performance needs deploy different models different environments indoor outdoor forest. gating mechanism selects model based input video. another application could visual question answering system multiple models trained using images different domains. gating mechanism could data select associated task model. even could deploy models simultaneously selecting right expert model straightforward. using output highest scoring expert guarantee success neural networks erroneously give high conﬁdence scores shown also demonstrate experiments. training discriminative classiﬁer distinguish tasks also option since would require storing training data. need task recognizer tell relevance associated task model given test sample. exactly gating mechanism provides. fact also prefrontal cortex primate brain considered neural representations task context gating different brain functions propose implement task recognizer using undercomplete autoencoder gating mechanism. learn task domain gating function captures shared characteristics among training samples recognize similar samples test time. using layer under-complete autoencoder. autoencoder trained along corresponding expert model maps training data lower dimensional subspace. test time task autoencoder projects sample learned subspace measures reconstruction error projection. autoencoder lowest reconstruction error used like switch selecting corresponding expert model interestingly autoencoders also used evaluate task relatedness training time turn used determine prior model relevant task. show based information expert gate decide specialist model transfer knowledge learning task whether ﬁne-tuning learning-without-forgetting summarize contributions following. develop expert gate lifelong learning system sequentially deal tasks without storing previous data. automatically selects related prior task learning task. test time appropriate model loaded automatically deal task hand. evaluate gating network image classiﬁcation video prediction problems. rest paper organized follows. discuss related work section expert gate detailed section followed experiments section ﬁnish concluding remarks future work section multi-task learning goal develop system reach expert level performance multiple tasks tasks learned sequentially. such lies intersection multi-task learning lifelong learning. standard multi-task learning aims learning multiple tasks joint manner. objective knowledge different tasks called inductive bias order improve performance individual tasks. often shared model used tasks. beniﬁt relaxing number required samples task could lead suboptimal performance individual tasks. hand multiple models learned optimal task utilize inductive bias knowledge models determine related tasks utilize cluster tasks based mutual information gain using information task learning another. exhaustive process. alternative assume parameters related task models close original space lower dimensional subspace thus cluster tasks’ parameters. ﬁrst learn task models independently tasks within cluster help improving relearning models. multiple models multiple tasks ﬁrst examples using multiple models handling subset tasks jacobs trained adaptive mixture experts multispeaker vowel recognition used separate gating network determine network sample. showed setup outperformed single shared model. downside however training sample needed pass expert gating function learned. avoid issue mixture generalist model many specialist models proposed test time generalist model acts gate forwarding sample correct network. however unlike model approaches require data available learning generalist model needs retrained time task arrives. lifelong learning without catastrophic forgetting sequential lifelong learning knowledge previous tasks leveraged improve training tasks taking care forget tasks i.e. preventing catastrophic forgetting system obviates need storing training data collected lifetime agent learning task autoencoders learn distribution task data hence also capture meta-knowledge task. desired characteristics lifelong learning system outlined silver constraint storing previous training data looked previously silver mercer output previous task networks given training data called virtual samples regularize training networks tasks. improves task performance using knowledge previous tasks. recently learning without forgetting framework uses similar regularization strategy learns single network tasks ﬁnetune previously trained network tasks. contribution previous tasks/networks training networks determined task relatedness metrics previous knowledge used regardless task relatedness. demonstrates sequential training network tasks. experiments show shared model gets worse extended tasks especially task relatedness low. like recent architectures namely progressive network modular block network also multiple networks task. networks additional columns lateral connections previous nets. lateral connections mean layer network connected previous layer column also previous layers previous columns. allows networks transfer knowledge older newer tasks. however works choosing column particular task test time done manually authors leave automation future work. here propose autoencoder determine model consequently column selected particular test sample. consider case lifelong learning sequential learning tasks corresponding data come another. task learn specialized model transferring knowledge previous tasks particular build related previous task. simultaneously learn gating function captures characteristics task. gate forwards test data corresponding expert resulting high performance learned tasks. question learn gate function differentiate tasks without access training data previous tasks? learn dimensional subspace task/domain. test time select representation best test sample. using undercomplete autoencoder task. below ﬁrst describe autoencoder detail next explain selecting relevant expert estimating task relatedness autoencoder gate autoencoder neural network learns produce output similar input network composed parts encoder maps input code decoder maps code reconstruction input. loss function simply reconstruction error. encoder learns hidden layer lower dimensional representation higher dimensional representation input data guided regularization criteria prevent autoencoder copying input. linear autoencoder euclidean loss function learns subspace pca. however autoencoders non-linear functions yield better dimensionality reduction compared motivates choice model. autoencoders usually used learn feature representations unsupervised manner dimensionality reduction. here different goal. lower dimensional subspace learned undercomplete autoencoders maximally sensitive variations observed task data insensitive changes orthogonal manifold. words represents variations needed reconstruct relevant samples. fast easy optimize. relu also introduces sparsity hidden units leads better generalization. decoding fully connected layer followed sigmoid. sigmoid yields values allows cross entropy loss function. test time euclidean distance compute reconstruction error. test time learning autoencoders different tasks softmax layer takes input reconstruction errors different tasks autoencoders given test sample reconstruction error i-th autoencoder output loss function given input sample softmax layer gives probability task autoencoder indicating conﬁdence temperature. temperature value leading soft probability values. given conﬁdence values load expert model associated conﬁdent autoencoder. tasks overlap convenient activate expert model instead taking score only. done setting threshold conﬁdence values section given task associated data ﬁrst learn autoencoder task previous task associated autoencoder want measure task relatedness task task since access data task validation data current task compute average reconstruction error current task data made current task autoencoder likewise average reconstruction error made previous task autoencoder current task data. relatedness tasks computed exploit task relatedness ways. first select related task used prior model learning task. second exploit level task relatedness determine transfer method ﬁnetuning learning-without-forgetting found experiments outperforms ﬁne-tuning main hypothesis autoencoder domain/task thus better reconstructing data task autoencoders. comparing reconstruction errors different tasks’ autoencoders allows successfully forward test sample relevant expert network. stated regularized autoencoders opposing forces risk regularization term result score like behavior reconstruction error. result zero reconstruction loss means zero derivative could local minimum local maximum. however unregularized one-layer under-complete autoencoder these shown mean squared error criterion reconstruction loss estimates negative log-likelihood. need one-layer autoencoder regularization term pull energy unseen data narrowness code already acts implicit regularizer. preprocessing start robust image representation namely activations last convolutional layer alexnet pretrained imagenet. encoding layer pass input preprocessing step input data standardized followed sigmoid function. standardization data i.e. subtracting mean dividing result standard deviation essential increases robustness hidden representation input variations. normally standardization done using statistics data network trained case good strategy. because test time compare relative reconstruction errors different autoencoders. different standardization regimes lead non-comparable reconstruction errors. instead statistics imagenet standardization autoencoder. since large dataset gives good approximation distribution natural images. standardization apply sigmoid function input range network architecture design simple autoencoder complex layer deep model layer encoder/decoder encoding step consists fully connected layer followed relu make relu activation units tasks sufﬁciently related. case enforcing model give similar outputs task actually hurt performance. fine-tuning hand uses previous task parameters starting point less sensitive level task relatedness. therefore apply threshold task relatedness value decide ﬁne-tune. algorithm shows main steps expert gate training test phase. first compare method various baselines three image classiﬁcation tasks next analyze gate behavior detail bigger tasks followed analysis task relatedness measure finally test expert gate video prediction problem implementation details activations last convolutional layer alexnet pre-trained imagenet image representation autoencoders. experimented size hidden layer autoencoder trying sizes found optimal value neurons. good compromise complexity performance. task relatedness higher lwf; otherwise ﬁne-tuning. matconvnet framework experiments. start sequential learning three image classiﬁcation tasks order train scenes scene classiﬁcation caltech-ucsd birds ﬁnegrained bird classiﬁcation oxford flowers ﬁnegrained ﬂower classiﬁcation. simulate scenario agent robot prior knowledge exposed datasets sequential manner start table classiﬁcation accuracy sequential learning image classiﬁcation tasks. methods assume previous training data still available methods oracle gate select proper model test time. alexnet model pre-trained imagenet. compare following baselines single jointly-trained model assuming training data always available model jointly trained three tasks together. multiple ﬁne-tuned models distinct alexnet models ﬁnetuned separately task. test time oracle gate used i.e. test sample always evaluated correct model. multiple models distinct models learned learning-without-forgetting model task always using alexnet pre-trained imagenet previous task. combined oracle gate. single ﬁne-tuned model alexnet model sequentially ﬁne-tuned task. single model sequentially applied multiple tasks. task learned outputs previous network soft targets training samples. network ﬁrst trained task data without forgetting imagenet then network trained task data using imagenet task speciﬁc layers outputs soft targets; baselines multiple models rely oracle gate select right model test time. reported numbers upper bounds achieved practice. holds baseline assumes previous training data stored available. table shows classiﬁcation accuracy achieved test sets different tasks. expert gate system task ﬁrst select related previous task learn task expert model transferring knowledge related task model using ﬁnetuning. single ﬁne-tuned model single model also report intermediate results sequential learning. learning multiple models improves vanilla ﬁne-tuning scenes birds also reported however flowers performance degrades compared ﬁne-tuning. measure lower degree task relatedness imagenet flowers birds scenes might explain effect. single comparing ﬁne-tuned model multiple ﬁne-tuned models observe increasing drop performance older sequentially ﬁne-tuning single model tasks shows catastrophic forgetting good strategy lifelong learning. single model less sensitive forgetting previous tasks. however still inferior training exclusive models tasks older well newer tasks. lower performance previous tasks buildup errors degradation soft targets older tasks. results failing compensate forgetting sequence involving tasks. also adds noise learning process task. further previous tasks varying degree task relatedness. datasets systematically observed largest task relatedness values imagenet treating tasks equally prevents task getting beneﬁt imagenet multiple models setting. expert gate always correctly identiﬁes related task i.e. imagenet. based relatedness degree used birds scenes ﬁne-tuning used flowers. result best expert models learned task. test time gate mechanism succeeds select correct model test samples. leads superior results achieved learning strategies achieve comparable performance average joint training access tasks data. also performance multiple ﬁne-tuned models multiple models assume task label activating associated model. gate analysis goal experiment evaluate expert gate’s ability successfully selecting relevant network given test image. experiment tasks stanford cars dataset ﬁne-grained note numbers identical show similar trends. time experimentation code available implemented consultation authors used parameters provided them. classiﬁcation fgvc-aircraft dataset ﬁne-grained classiﬁcation aircraft actions human action classiﬁcation subset challenge last dataset multi-label annotations. sake consistency actions single label. newly added datasets bounding boxes instead full images images might contain object. total deal different tasks scenes birds flowers cars aircrafts actions along imagenet considered generalist model initial pre-existing model. compare joint training ﬁnetune imagenet pre-trained alexnet jointly tasks assuming data available. also compare setting multiple ﬁne-tuned models model maximum score selected expert gate follow regime previous experiment. related task always imagenet. based task relatedness threshold selected actions aircrafts cars ﬁne-tuned. table shows results. even though jointly trained model trained previous tasks data simultaneously average performance inferior expert gate system. explained negative inductive bias tasks negatively affect others case scenes cars. explained introduction deploying models taking score option many test samples conﬁdent model correct resulting poor performance. additionally size expert model around size autoencoder around almost order magnitude difference memory requirements. comparison discriminative classiﬁer finally compare discriminative classiﬁer trained predict task. classiﬁer ﬁrst assume data previous tasks stored even though line lifelong learning setup. thus serves upper bound. classiﬁer neural hidden layer composed neurons takes input data representation autoencoder gate output different tasks labels. table compares performance gate recognizing task data discriminative classiﬁer. further test scenario discriminative classiﬁer number stored samples task varying approaches accuracy gate samples. note size used datasets. larger datasets even higher number samples would probably needed match performance. spite access previous tasks data expert gate achieves similar performance discriminative classiﬁer. fact expert gate seen sequential classiﬁer classes arriving another. important results paper withever simultaneous access data different tasks expert gate based autoencoders manages assign test samples relevant tasks equally accurately discriminative classiﬁer. figure shows confusion cases expert gate. test samples even humans hard time telling expert activated. example scenes images containing humans also classiﬁed actions. deal cases preferable settings allow expert activated. done setting threshold probabilities different tasks. tested scenario threshold observed test samples analyzed multiple expert models. note case evaluate label given corresponding task missing ground truth possible tasks appearing image. leads average accuracy i.e. increase previous cases related task always imagenet. similarity images different tasks imagenet. also wide diversity imagenet classes enables cover good range tasks. mean imagenet task transfer knowledge from regardless current task nature? answer question three different tasks previous basgoogle street view house numbers svhn digit recognition charsk dataset character recognition natural images mnist dataset handwritten digits. charsk english exclude digits considering characters. previous pick related tasks actions scenes unrelated tasks cars flowers. focus method knowledge transfer. also consider imagenet possible source. consider following knowledge transfer cases scenes actions imagenet actions svhn letters imagenet letters svhn mnist imagenet mnist flowers cars imagenet cars. figure shows performance compared ﬁne-tuning tasks pre-trained alexnet along degree task relatedness. line indicates threshold task relatedness used previous experiments. case high score task relatedness uses knowledge previous task improves performance target task e.g. tasks less related method fails improve starts degrade performance tasks highly unrelated even fail reach good performance task case explained fact task pushing shared parameters different direction thus model fails reach good local minimum. conclude gate autoencoder succeeds predict task could help anlwf framework cannot. next evaluate expert gate video prediction context autonomous driving. state system video prediction dynamic filter network given sequence images task network predict next images. quite structured task task environment training data affect prediction results quite signiﬁcantly. autonomous vehicle uses video prediction needs able load correct model current environment. might data beginning becomes important learn specialists type environment without need storing training data. even data available joint training give best results domain show below. show experiments conducted three domains/tasks highway data train/test split; residential data longest sequences kitti dataset city data stuttgart sequence cityscapes dataset i.e. sequence dataset densely sampled frames. train/test split residential city datasets. train tasks using different regimes sequential training using single fine-tuned model joint training expert gate. video prediction seem applicable. experiment autoencoders gating function. task relatedness. video prediction results expressed average pixel-wise l-distance predicted ground truth images shown table similar trends observed image classiﬁcation problem sequential ﬁne-tuning results catastrophic forgetting model ﬁne-tuned dataset deteriorates original dataset ﬁne-tuning. joint training leads better results domain requires data training. expert gate system gives better results compared sequential joint training. numbers supported qualitative results well please refer supplementary materials ﬁgures. experiments show potential expert gate system video prediction tasks autonomous driving applications. figure qualitative results video prediction. left right last ground truth image predicted image using sequential ﬁne-tuning using expert gate. examining lane markers expert gate visually superior. conclusions future work context lifelong learning work focused exploit knowledge previous tasks transfer task. little attention gone related equally important problem select proper model test time. topic tackle paper. best knowledge ﬁrst propose solution require storing data previous tasks. surprisingly expert gate’s autoencoders distinguish different tasks equally well discriminative classiﬁer trained data. moreover used select related task appropriate transfer method training. combined gives powerful method lifelong learning outperforms state-of-the-art also joint training tasks simultaneously. current system uses related model knowledge transfer. future work explore possibility leveraging multiple related models training tasks instance exploring strategies balancing contribution different tasks relatedness degree rather varying learning rates. also mechanism decide merge tasks high relatedness degree rather adding expert model seems interesting research direction. acknowledgment ﬁrst author’s funded scholarship. grateful support leuven project cametron. authors would like thank matthew blaschko amal rannen triki valuable discussions.", "year": 2016}