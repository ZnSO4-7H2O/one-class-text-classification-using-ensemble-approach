{"title": "A neural network approach to ordinal regression", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.", "text": "research ordinal regression dated back ordinal statistics methods machine learning research attracted considerable attention recent years potential applications many data-intensive domains information retrieval page ranking collaborative ﬁltering image retrieval protein ranking bioinformatics. number machine learning methods developed redesigned address ordinal regression problem including perceptron kernelized generalization neural network gradient descent gaussian process large margin classiﬁer k-partite classiﬁer boosting algorithm constraint classiﬁcation regression trees naive bayes bayesian hierarchical experts binary classiﬁcation approach decomposes original ordinal regression problem binary classiﬁcations optimization nonsmooth cost functions methods roughly classiﬁed categories pairwise constraint approach multi-threshold approach former convert full ranking relation pairwise order constraints. latter tries learn multiple thresholds divide data ordinal regression important type learning properties classiﬁcation regression. describe simple eﬀective approach adapt traditional neural network learn ordinal categories. approach generalization perceptron method ordinal regression. several benchmark datasets method outperforms neural network classiﬁcation method. compared ordinal regression methods using gaussian processes support vector machines nnrank achieves comparable performance. moreover nnrank advantages traditional neural networks learning online batch modes handling large training datasets making rapid predictions. features make nnrank useful complementary tool large-scale data processing tasks information retrieval page ranking collaborative ﬁltering protein ranking bioinformatics. ordinal regression important supervised problem learning ranking ordering instances property classiﬁcation metric regression. learning task ordinal regression assign data points ﬁnite ordered categories. example teacher rates students’ performance using ordinal regression diﬀerent classiﬁcation order categories. contrast metric regression response variables ordinal regression discrete ﬁnite. ordinal regression methods diﬀerent advantages disadvantages. prank perceptron approach generalizes binary perceptron algorithm ordinal multi-class situation fast online algorithm. however like standard perceptron method accuracy suﬀers dealing non-linear data quadratic kernel version prank greatly relieves problem. class accurate large-margin classiﬁer approaches convert ordinal relations pairwise ranking constraints structural risk minimization thus applied medium size datasets without discarding pairwise preference relations. also overﬁt noise incomparable pairs. class powerful large-margin classiﬁer methods generalize support vector formulation ordinal regression ﬁnding thresholds real line divide data ordered categories. size optimization problem linear number training examples. however like support vector machine used classiﬁcation prediction speed slow solution sparse makes appropriate time-critical tasks. similarly another state-of-the-art approach gaussian process method also diﬃculty handling large training datasets problem slow prediction speed situations. describe neural network approach ordinal regression advantages neural network learning learning online batch mode training large dataset handling non-linear data good performance rapid prediction. method considered generalization perceptron learning multi-layer perceptrons ordinal regression. method also related classic generalized linear models ordinal regression unlike neural network method trained pairs examples learn pairwise order relations method works individual data points uses multiple output nodes estimate probabilities ordinal categories. thus method falls category multi-threshold approach. learning method benchmark datasets method yields performance better standard classiﬁcation neural networks comparable state-ofthe-art methods using support vector machines gaussian processes. addition method learn large datasets make rapid predictions. represent ordinal regression dataset consisting data points input feature vector ordinal category ﬁnite without loss generality assume order relation. standard classiﬁcation neural network without considering order categories goal predict probability data point belonging category input target encoding category vector element others goal learn function input vector probability distribution vector closer elements close zero subject contrast like perceptron approach neural network approach considers order categories. data point belongs category classiﬁed automatically lowerorder categories well. target vector elements zeros. thus goal learn function input vector probability vector close close estimate number categories belongs instead formulation target vector similar perceptron approach also related classical cumulative probit model ordinal regression sense consider output probability vector cumulative probability distribution categories formulation almost exactly neural network machinery ordinal regression. construct multi-layer neural network learn ordinal relations neural network inputs corresponding number dimensions input feature vector output nodes corresponding ordinal categories. hidden layers. without loss generality hidden layer construct standard two-layer feedforward neural network. like standard neural network classiﬁcation input nodes fully connected hidden nodes turn fully connected output nodes. likewise transfer function hidden nodes linear function sigmoid function tanh function used experiment. diﬀerence traditional neural network lies output layer. traditional neural networks soft node contrast output node neural network uses standard sigmoid function +e−zi withincluding outputs nodes. output node used estimate probability data point belongs category independently withsubjecting normalization traditional neural networks thus data point category target vector ﬁrst elements others sets target value output nodes targets instruct neural network adweights produce probability outputs close possible target vector. worth pointing using independent sigmoid functions output nodes guaranteed monotonic relation necessary desirable making predictions sophisticated approach impose inequality constraints outputs improve performance. training neural network ordinal regression proceeds similarly standard neural networks. cost function data point relative entropy square error target vector output vector. relative entropy cost function output nodes log). square eri= previous studies neural network cost functions show relative entropy square error functions usually yield similar results. experiments square error function standard back-propagation train neural network. errors propagated back output nodes output nodes hidden nodes ﬁnally input nodes. since transfer function output node +e−zi derivaindependent sigmoid function tive output node +e−zi thus error +e−zi propagated output node relative entropy cost function square error cost function. errors propagated neural networks adjust weights using gradient descent traditional neural networks despite small diﬀerence transfer function computation derivative training method traditional neural networks. network trained data online mode weights updated example batch mode weights updated bunch examples. test phase make prediction method scans output nodes order stops output node smaller predeﬁned threshold nodes left. index last node whose output bigger predicted category data point. eight standard datasets ordinal regression benchmark method. eight datasets originally used metric regression. ghahramani discretized real-value targets equal intervals corresponding ordinal categories. authors randomly split dataset training/test datasets repeated partition times independently. exactly partitions train test method. online mode train neural networks. parameters tune number hidden units number epochs learning rate. create grid three parameters hidden unit number range epoch number initial learning rate range training learning rate halved training errors continuously pre-deﬁned number epochs. experiments data split neural network parameters fully optimized training data without using test data. experiment parameters optimized training data train models training data optimal parameters starting diﬀerent initial weights. ensemble trained models used estimate generalized performance test data. average output neural network models used make predictions. evaluate method using zero-one error mean absolute error zero-one error percentage wrong assignments ordinal categories. mean absolute error root mean square diﬀerence assigned categories true categories data points. dataset training evaluation process repeated times data splits. thus compute average error standard deviation metrics ﬁrst compare method standard neural network classiﬁcation method implement nnrank nnclass using c++. nnrank nnclass share code minor diﬀerence transfer function output nodes derivative computation described section table shows nnrank outperforms nnclass case terms mean-zero error mean absolute error. datasets improvement nnrank nnclass sizable. instance stock pyrimidines datasets mean zero-one error nnrank less nnclass; four datasets mean absolute error reduced results show ordinal regression neural network consistently achieves better performance standard classiﬁcation neural network. futher verify eﬀectiveness evaluate performance method compare nnrank gaussian process methods support vector machine method implemented results three methods quoted table reports zero-one error eight datasets. nnrank achieves best results diabetes triazines abalone gp-ep pyrimidines auto boston gp-map machine stocks. table reports mean absolute error eight datasets. nnrank yields best results diabetes abalone gp-ep pyrimidines auto boston gp-map triazines machine stocks. described simple novel approach adapt traditional neural networks ordinal regression. neural network approach considered generalization one-layer perceptron approach multi-layer. standard benchmark ordinal regression method outperforms standard neural networks used classiﬁcation. furthermore benchmark method achieves similar performance state-of-the-art methods ordinal regression. compared existing methods ordinal regression method several advantages neural networks. first like perceptron approach method learn batch online mode. online learning ability makes method good tool adaptive learning real-time. multi-layer structure neural network non-linear transfer function give method stronger ﬁtting ability perceptron methods. large datasets iteratively training complex support vector machines gaussian processes. since training process method traditional neural networks average neural network users method tasks. third neural network method make rapid prediction models trained. ability learning large dataset predicting time makes method useful competitive ordinal regression tasks particularly tool time-critical large-scale ranking problems information retrieval page ranking collaborative ﬁltering emerging ﬁelds bioinformatics. currently applying method rank proteins according structural relevance respect query protein facilitate application approach make nnrank nnclass accept general input format freely available http//www.eecs.ucf.edu/∼jcheng/cheng software.html. directions improve neural network approach ordinal regression. direction design transfer function ensure monotonic decrease outputs neural network; direction derive general error bounds method binary classiﬁcation framework furthermore ﬂavors implementations multi-threshold multi-layer perceptron approach ordinal regression possible. since machine learning ranking fundamental problem wide applications many diverse domains page ranking information retrieval image retrieval collaborative ﬁltering bioinformatics believe exploration neural network approach ranking ordinal regression worthwhile.", "year": 2007}