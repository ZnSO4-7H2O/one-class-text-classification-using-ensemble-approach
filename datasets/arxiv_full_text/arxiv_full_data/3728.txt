{"title": "Variational Algorithms for Marginal MAP", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "abstract": "Marginal MAP problems are notoriously difficult tasks for graphical models. We derive a general variational framework for solving marginal MAP problems, in which we apply analogues of the Bethe, tree-reweighted, and mean field approximations. We then derive a \"mixed\" message passing algorithm and a convergent alternative using CCCP to solve the BP-type approximations. Theoretically, we give conditions under which the decoded solution is a global or local optimum, and obtain novel upper bounds on solutions. Experimentally we demonstrate that our algorithms outperform related approaches. We also show that EM and variational EM comprise a special case of our framework.", "text": "tasks listed order increasing diﬃculty max-inference problems shown npcomplete sum-inference p-complete mixed-inference nppp-complete practically speaking max-inference tasks number eﬃcient algorithms loopy max-product tree-reweighted mplp dual decomposition methods sum-inference problems similarly well-studied algorithms parallel max-inference also exist. perhaps surprisingly mixed max-sum inference much harder either maxsuminference problems alone. classic example illustrating fig. marginal simple tree structure still np-hard diﬃculty caused part operators commute order exchangeable. reason research marginal still relatively unexplored exceptions e.g. doucet park darwiche huang jiang daum´e paper extend concepts variational approaches marginal problem enabling host techniques applied. lead powerful algorithms estimating bounding marginal solutions global local optimality conditions characterized. relate algorithms existing similar approaches validate methods experimental comparisons. marginal problems notoriously difﬁcult tasks graphical models. derive general variational framework solving marginal problems apply analogues bethe tree-reweighted mean ﬁeld approximations. derive mixed message passing algorithm convergent alternative using cccp solve bp-type approximations. theoretically give conditions decoded solution global local optimum obtain novel upper bounds solutions. experimentally demonstrate algorithms outperform related approaches. also show variational comprise special case framework. graphical models provide powerful framework reasoning structured functions deﬁned many variables. term inference refers generically answering probabilistic queries computing probabilities ﬁnding optima. although np-hard worst case recent algorithms including variational methods mean ﬁeld algorithms collectively called belief propagation approximate solve problems many practical circumstances. three classic inference tasks include max-inference problems also called maximum posteriori probable explanation problems look likely conﬁguration. second type sum-inference problems calculate marginal probabilities distribution’s normalization constant finally marginal mixed max-suminference tasks seek partial conﬁguration variables maximizes variables’ marginal probability. free energy remains intractable typically approximate free energy combination singleton pairwise entropies require knowing τij. example bethe free energy approximation entropy variable pairwise mutual information. loopy interpreted ﬁxed point algorithm optimize bethe free energy. tree reweighted free energy another variant tained weighted collection spanning trees free energy upper bound true free energy also concave function optimizing ﬁxed point method gives tree reweighted algorithm. another related approach restricts subset distributions constraints entropy calculation tractable fully factored distributions. leads class mean ﬁeld approximations. leads maximum eqn. remains np-hard; variational methods interpreted relaxing local constraints leads linear relaxation original integer programming problem. note diﬀers lack entropy term; next section generalize similarity marginal problem. sum-inference task marginalizing variables model. without loss generality treated problem calculating partition function unfortunately straightforward calculation requires summing exponential number terms. variational methods class approximation algorithms transform inference continuous optimization problem typically solved approximately. start deﬁne marginal polytope simply transforming sum-inference problem make easier; marginal polytope objective function’s entropy remain intractable. however provides framework deriving algorithms approximating marginal polytope entropy many approximation methods replace locally consistent pairwise models singleton pairwise beliefs {τi|i {τij| consistent intersections main result work section derive dual representation marginal problem dual form generalizes suminference max-inference provides uniﬁed framework addressing marginal map. entropy nodes removed sum-inference free energy fsum. generalizes sum-inference max-inference sets empty nodes respectively. intuitively subtracting entropy objective marginal tends lower entropy causing probability mass concentrate optimal {x∗i figure example marginal query tree requires exponential time. summing shaded nodes makes unshaded nodes interdependent; koller friedman details. although similar maxsum-inference marginal signiﬁcantly harder either. classic example fig. shows even tree marginal np-hard. main diﬃculty arises because operators commute restricts eﬃcient elimination orders nodes eliminated nodes marginalizing destroy conditional independence among making diﬃcult represent optimize even part alone tractable denote subgraph induced i.e. join sets natural generalization eﬃcient tree structure marginal problem occurs tree along elimination order ﬁrst eliminates nodes call type graph tree. reasons relatively algorithms marginal map. expectation-maximization variational provide straightforward approach viewing parameters hidden variables; however many local maxima easily stuck sub-optimal conﬁgurations. jiang daum´e proposed message passing algorithm combining max-product sum-product little theoretical analysis. state-of-the-art approaches include markov chain monte carlo local search work propose general variational framework approximation algorithms marginal provide theoretical experimental results justify algorithms. selecting subtrees. selecting subtrees approximation straightforward selecting subtrees regular sum-inference. important property tree edges connected edges nodes therefore construct subtree ﬁrst selecting subtree join connected component edge ∂ab. simple extreme cases stand distributions clamped value. also maxτ∈m∗ fmix. generally holds satisﬁes without aﬀecting optimum. among sets special interest smallest convex includes i.e. convex hull theorem transforms marginal problem variational form decrease hardness marginal polytope free energy fmix remain intractable. fortunately well established techniques summax-inference directly applied giving derive approximate algorithms. spirit wainwright jordan either relax simpler outer bound like replace fmix tractable form give algorithms similar trbp restrict subset constraints free energy tractable give mean ﬁeld-like algorithm. sequel introduce several approximation schemes. mainly focus analogues although brieﬂy discuss mean ﬁeld connect section call fbethe truncated bethe free energy since obtained regular sum-inference bethe free energy truncating entropy mutual information terms involve nodes. tree φbethe equals true giving intuitive justiﬁcation. sequel give general conditions approximation give exact solution. simple scheme usually give high quality empirical approximations. similar regular bethe approximation leads nonconvex optimization derive message passing algorithms provably convergent algorithms solve solution towards integral points. thus practice solution free energy less likely integral bethe free energy causing diﬃculty applying theorem solutions well. dresses mixed-inference problem. given connections expect mixed message passing algorithms marginal combine maxproduct sum-product allowing weights approach zero keeping weights equal one. section derive mixed message scheme discuss optimality property ﬁxed points using reparameterization interpretation. start consider case strictly positive. using lagrange multiplier method similar yedidia wainwright show ﬁxed point following message passing scheme stationary point global optimality. turns approximation schemes give exact solutions circumstances. initially assume tree i.e. part tractable calculate given b-conﬁguration. suppose approximate proof. discussed section optimization restricted subset integral maxτ∈m∗ fmix. note objective function equals true free energy fmix tree means that relaxation maxτ∈m∗ fmix. standard relaxation argument completes proof. proof. proof involves showing satisˆ condition maxτ∈l second inequality follows showing point dual function gives upper bound third inequality follows fact sages sent nodes nodes; maxproduct messages sent nodes; messages sent nodes nodes novel interpreted sequel solving type local marginal problem. interestingly method bears similarity diﬀerences recent method jiang daum´e propose similar hybrid message passing algorithm sends usual maxproduct messages nodes. turns diﬀerence crucial analysis optimality conditions discuss later. positive weights) must explicitly considered. yedidia however apply positive weights close zero hope solution close enough marginal solution. surrogate free energy marginal deﬁned unfortunately always true. weiss showed max-inference approaches augmented term concave. generalize result arbitrary give error bound concave suppose subset nodes gc∪a subgraph induced nodes ec∪a call gc∪a semi-a-b subtree edges ec∪a\\eb form tree. words gc∪a semia-b tree tree ignoring edges within set. marginals satisfy admissibility mixedconsistency theorem suppose maxima unique. exist b-conﬁguration satisfying suppose subset gc∪a semi-a-b tree locally optimal sense smaller b-conﬁguration diﬀers following conditions satisﬁed gc∪a tree graph must tree. thus theorem implicitly assumes part tractable. markov chain fig. theorem implies solution locally optimal hamming distance i.e. coordinatewise optimal. however local optimality guaranteed theorem general much stronger part disconnected part interior regions connect part. hybrid algorithm jiang daum´e also reparameterization interpretation replaces constraint simple max-consistency; however change invalidates theorem important interpretation sum-product max-product algorithms reparameterization viewpoint message passing viewed moving mass between sum-marginals leaves product reparameterization original distribution; ﬁxed point marginals guaranteed satisfy consistency property. three mixed-consistency constraints exactly three types message updates constraint particular interest interpreted solving local marginal-map problem bij. turns constraint crucial ingredient mixed message passing enabling prove local optimality solution. mixed message scheme interesting suﬀer convergence problems happen loopy tree reweighted apply concaveconvex procedure used derive convergent algorithms maximizing bethe kikuchi free energy problem. i.e. m|qτ since meaning optimal vertices included maxτ∈m× fmix remains exact; however longer convex set. denoting marginal polytope similarly natural consider coordinate update restricted optimization e-step m-step intractable insert various approximations. particular approximating mean-ﬁeld inner bound leads variational interesting observation obtained using bethe approximation solve e-step linear relaxation solve m-step; case em-like update equivalent solving τiτj ∂ab. equivalently subset ∂ab. therefore treated special case taking forcing solution fall algorithm represents extreme tradeoﬀ encouraging vertex solutions sacriﬁcing convexity. optimality result theorem also applies solutions likely stuck local optima high non-convexity. fact gradient w.r.t. respectively. concave iterative linearization process called concave-convex procedure guaranteed monotonically increase objective function. yuille detailed discussion. iterative linearization process appealing interpretation. recall free energy marginal obtained dropping entropy nodes sum-inference free energy. essentially adds back lost entropy terms canceling eﬀect adjusting opposite direction. concept distinct technique used deriving mixed message passing truncated entropy term re-added weighted \u0001-small temperature. practice necessary require concave; particular appealing choose coincide bethe free energy sum-inference using truncated bethe approximation. interpretation transforming marginal problem sequence sum-inference problems often appears give better ﬁxed point solution. natural algorithm solving marginal problem expectation-maximization algorithm treating parameters hidden variables. section show algorithm seen coordinate ascent algorithm mean variant framework. connect restrict distributions product form pairs implemented truncated bethe approximation versions truncated approximations mix-trw assigns weights uniformly type-i subtrees mix-trw assigns weight uniformly type-i subtrees weight uniformly type-ii trees. avoid convergence issues cccp algorithms. comparison implemented max-product sum-product jiang daum´e hybrid extract solution maximizing max-marginals sum-marginals nodes. note three message passing algorithms non-iterative; since hidden markov chain tree terminate number steps equal graph diameter. sense fact suggests power solve marginal problem limited. also implemented standard starting random initializations picking best solution. well mix-bethe mix-trw despite better theoretical properties. explained fact mix-trw rarely returns integer solutions. general note truncated approximations including mix-trw mix-trw appear less accurate bethe approximation able provide upper bounds shown fig. fig. shows algorithms’ behavior time iteration cccp step mix-bethe mix-trw step algorithm. update monotonic easily stuck sub-optimal points; experimentally observed always terminated iterations. generate parameters randomly before binary states giving mixed potentials; also generated attractive-only potentials taking absolute value θij. since longer tree max-product sum-product jiang’s method fail converge case tried adding damping additional iterations also convergent alternative cccp sum-product sequential max-product. algorithm reports best result options. tion across algorithms. mix-bethe performs well across mixed attractive couplings. attractive couplings three message passing algorithms also perform well; probably attractive models typically dominant modes making problem easier. presented general variational framework solving marginal problems approximately. theoretically algorithms justiﬁed showing conditions solutions global local optima. experiments demonstrate truncated bethe approximation performs extremely well compared similar approaches. future directions include improving performance truncated approximation optimizing weights deriving optimality conditions applicable even component form tree studying mean ﬁeld-like approximations extending algorithms generalized message passing higher order cliques.", "year": 2012}