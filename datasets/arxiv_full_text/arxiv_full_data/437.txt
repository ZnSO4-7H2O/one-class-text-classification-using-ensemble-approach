{"title": "Entropy of Overcomplete Kernel Dictionaries", "tag": ["cs.IT", "cs.CV", "cs.LG", "cs.NE", "math.IT", "stat.ML"], "abstract": "In signal analysis and synthesis, linear approximation theory considers a linear decomposition of any given signal in a set of atoms, collected into a so-called dictionary. Relevant sparse representations are obtained by relaxing the orthogonality condition of the atoms, yielding overcomplete dictionaries with an extended number of atoms. More generally than the linear decomposition, overcomplete kernel dictionaries provide an elegant nonlinear extension by defining the atoms through a mapping kernel function (e.g., the gaussian kernel). Models based on such kernel dictionaries are used in neural networks, gaussian processes and online learning with kernels.  The quality of an overcomplete dictionary is evaluated with a diversity measure the distance, the approximation, the coherence and the Babel measures. In this paper, we develop a framework to examine overcomplete kernel dictionaries with the entropy from information theory. Indeed, a higher value of the entropy is associated to a further uniform spread of the atoms over the space. For each of the aforementioned diversity measures, we derive lower bounds on the entropy. Several definitions of the entropy are examined, with an extensive analysis in both the input space and the mapped feature space.", "text": "linear approximation theory considers linear decomposition given signal atoms collected so-called dictionary. relevant sparse representations obtained relaxing orthogonality condition atoms yielding overcomplete dictionaries extended number atoms. generally linear decomposition overcomplete kernel dictionaries provide elegant nonlinear extension deﬁning atoms mapping kernel function models based kernel dictionaries used neural networks gaussian processes online learning kernels. quality overcomplete dictionary evaluated diversity measure distance approximation coherence babel measures. paper develop framework examine overcomplete kernel dictionaries entropy information theory. indeed higher value entropy associated uniform spread atoms space. aforementioned diversity measures derive lower bounds entropy. several deﬁnitions entropy examined extensive analysis input space mapped feature space. index terms—generalized r´enyi entropy shannon entropy sparse approximation dictionary learning kernel-based methods gram matrix machine learning pattern recognition. signal image processing pattern recognition denoising compression sparse representation given signal consists decomposing elementary signals called atoms collected so-called dictionary. linear formalism written linear combination dictionary atoms. decomposition unique latter deﬁnes basis particular orthogonal dictionaries fourier basis. since much interest direction predeﬁned dictionaries based analytical form wavelets predeﬁned dictionaries widely investigated literature years owing mathematical simplicity structured dictionaries dealing orthogonality dealing sparsity analytical dictionaries perform poorly general rigide structure imposed orthogonality. within last years class dictionaries emerged dictionaries learned data thus ability adapt signal scrutiny. karhunen-lo`eve transform also called principal component analysis advanced statistics falls class relaxation orthogonality condition delivers increased ﬂexibility overcomplete dictionaries i.e. number atoms exceeds signal dimension. several methods proposed construct oversomplete dictionaries solving highly non-convex optimization problem method optimal directions singular-value-decomposition counterpart convexiﬁcation method overcomplete dictionaries versatile provide relevant representations owing increased diversity. several measures proposed quantify diversity given dictionary. simplest measure diversity certainly cardinality dictionary i.e. number atoms. measure simplistic several diversity measures proposed examining relations atoms either pairwise fashion thorough way. used measure characterize dictionary coherence largest pairwise correlation atoms using largest cumulative correlation atom atoms dictionary yields exhaustive babel measure last twenty years coherence variants used matching pursuit algorithm basis pursuit arbitrary dictionaries theoretical results approximation quality studied also extensive literature compressed sensing beyond literature linear approximation several diversity measures overcomplete dictionary analysis investigated separately literature within different frameworks. case distance measure corresponds smallest pairwise distance atoms often considered neural networks. indeed resource-allocating networks function interpolation network gaussian units assigned unit unit distant enough unit already network turns units operate atoms approximation model corresponding dictionary small distance measure. distance measure given dictionary relies nearest pair atoms thorough measure approximation measure corresponds least error approximating atom dictionary linear combination atoms. measure diversity investigated machine learning gaussian processes online learning kernels nonlinear adaptive ﬁltering recently kernel principal component analysis aforementioned methods consider reproducing kernel hilbert space formalism. allows generalize wellknown linear model used sparse approximation nonlinear atom substituted nonlinear given kernel function. yields so-called kernel dictionaries atom lives feature space latter section introduce sparse approximation problem conventional linear model well kernelbased formulation. conclude section outline issues addressed paper. consider banach space denoted input space. approximation theory studies representation given signal dictionary atoms estimating fractions signal scrutiny. linear approximation decomposition takes form representation unique atoms form basis approximating signal projection onto span atoms namely examples involve orthonormal bases include fourier transform discrete cosine transform well data-dependent karhunenlo`eve transform beyond orthogonal bases relaxation orthogonality provides ﬂexibility overcomplete dictionaries allows investigate different constraints properly sparsity representation. case coefﬁcients obtained promoting sparsity representation. optimization problem often called sparse coding assuming dictionary known. view vector sparsity promoted minimizing pseudo-norm counts number non-zero entries norm closest convex norm pseudo-norm since seminal work olshausen field considered learning atoms available data data-driven dictionaries widely investigated. large class approaches proposed solve iteratively optimization problem alternating dictionary learning sparse coding former problem essentially tackled maximum likelihood principle data maximum posteriori probability dictionary. latter corresponds sparse coding problem. best known methods solving optimization problem subject sparsity promoting constraint method optimal directions k-svd algorithm dictionary determined respectively moorepenrose pseudo-inverse scheme. details references therein. worth noting sparsity constraint yields difﬁcult optimization problem even model linear coefﬁcients atoms. eing deﬁned nonlinear transformation input space. linear kernel yields conventional linear model given literature linear sparse approximation nonlinear kernels gaussian kernel allows include study neural networks ressourceallocating networks nonlinear adaptive ﬁltering kernels gaussian processes. aforementioned diversity measures allow quantify heterogeneity within dictionary scrutiny. paper derive connections measures entropy information theory indeed entropy measures disorder randomness within given system. considering generalized r´enyi entropy englobes deﬁnitions given shannon hartley well quadratic formulation show overcomplete kernel dictionary given diversity measure lower-bounded entropy. results high values entropy illustrate atoms favorably spread uniformly space. provide comprehensive analysis kernel type entropy deﬁnition within r´enyi entropy framework well recent nonadditive entropy proposed tsallis finally provide entropy analysis feature space deriving lower bounds depending diversity measures. consequence connect diversity measures input feature spaces. remained paper organized follows. next section introduces sparse approximation problem conventional linear model well nonlinear extension kernel formalism. section presents used diversity measures quantifying overcomplete dictionaries section provides preliminary exploration results used throughout paper. section core work deﬁne entropy examine input space section extends analysis feature space. section concludes paper. girolami considered estimation quadratic entropy samples using parzen estimator based normalized kernel function. formulation investigated regularization networks particular leastsquares support vector machines order reduce computational complexity pruning samples contribute sufﬁciently entropy recently online learning scheme proposed ls-svm using approximation measure sparsiﬁcation criterion. paper derive missing connections criterion entropy maximization. richard bermudez honeine considered analysis quadratic entropy kernel dictionary terms coherence. provide paper framework analyse overcomplete dictionaries extensive examination input feature spaces generalizing entropy deﬁnitions types kernels. conducted analysis examines several diversity measures including limited coherence measure. k-svd algorithm longer used. turns estimation elements input space tough optimization problem known literature pre-image problem recently authors adjusted elements input space nonlinear adaptive ﬁltering kernels. another context authors estimated elements kernel nonnegative matrix factorization. either analysis synthesis overcomplete dictionaries grow number atoms increase heterogeneity atoms needed. diversiﬁcation requires atoms close other. depending deﬁnition closeness several diversity measures proposed literature. case closeness given terms metric given distance measure pairwise measure atoms approximation measure thorough measure. also case collinearity atoms considered coherence babel measures. diversity measures described detail section within formalism kernel dictionary diversity measures entropy information theory indeed viewpoint information theory viewed ﬁnite source alphabet. fundamental measure information entropy quantiﬁes disorder randomness given system set. also associated number bits needed average store communicate investigation. detailed deﬁnition entropy given section connections entropy aforementioned diversity measures associated kernel dictionary several entropy deﬁnitions also investigated including generalized r´enyi entropy tsallis entropy. finally section extends analysis rkhs studying entropy atoms nonlinear models provide challenging issue. formalism reproducing kernel hilbert spaces provides elegant efﬁcient framework tackle nonlinearities. signals mapped nonlinear function feature space follows here positive deﬁnite kernel feature space so-called reproducing kernel hilbert space. h··ih k·kh denote respectively inner product norm induced space space interesting properties reproducing property states function evaluated using κih. moreover kernel trick particular kernels roughly divided categories projective kernels functions data inner product radial kernels functions distance used kernels expressions given table kernels gaussian radialbased exponential kernels unit-norm kκkh paper restrict particular kernel. denote linear kernel yields conventional model given nonlinear kernels gaussian kernel provide models investigated neural networks gaussian processes kernel-based machine learning including celebrated support vector machines data given kernel induced rkhs deﬁned element takes form dealing approximation problem spirit element approximated compared linear case given easy model still linear coefﬁcients well atoms nonlinear respect indeed resulting optimization problem consists minimizing residual rkhs hand estimation coefﬁcients similar given linear case classical sparse coders investigated purpose. hand dictionary determination difﬁcult since model nonlinear thus conventional techniques section present measures quantify diversity given dictionary diversity measure associated sparsiﬁcation criterion online learning order construct dictionaries large diversity measures. dealing unit-norm atoms expression boils sparsiﬁcation criterion online learning studied ressource-allocating networks novelty criterion imposing lower bound distance measure dictionary. thus candidate atom included dictionary distance measure latter fall given threshold controls level sparseness. distance measure relies nearest atoms approximation measure provides exhaustive analysis quantifying capacity approximating atom linear combination atoms expression corresponds residual error projecting atom onto subspace spanned others atoms. nullifying derivative cost function respect coefﬁcient optimal vector coefﬁcients sparsiﬁcation criterion associated approximation measure studied recently system identiﬁcation kernel principal component analysis. criterion constructs dictionaries high approximation measure thus including candidate atom dictionary cannot well approximated linear combination atoms already dictionary given approximation threshold. literature sparse linear approximation coherence fundamental quantity characterize dictionaries. corresponds largest correlation atoms given dictionary mutually atoms dictionaries. initially introduced linear matching pursuit studied union bases basis pursuit arbitrary dictionaries analysis approximation quality work consider linear measure explore following coherence kernel dictionary initially studied coherence criterion sparsiﬁcation constructs lowcoherent dictionary thus enforcing upper bound cosine angle pair atoms case candidate atom included dictionary coherence latter exceed given threshold.this threshold controls level sparseness dictionary null value yields orthogonal basis. coherence relies correlated atoms dictionary thorough measure babel measure considers largest cumulative correlation atom atoms dictionary. babel measure deﬁned ways. ﬁrst connecting coherence measure deﬁnition related cumulative coherence namely second deﬁne babel measure investigating analogy norm operator indeed coherence ∞-norm gram matrix dealing unit-norm atoms babel measure explores matrix-norm γ-babel proceeding throughout paper rigorous analysis overcomplete dictionary terms diversity measure provide following results essential study. results provide attempt bridge different diversity measures. following theorems connect coherence dictionary babel measure quantifying babel measure γ-coherent dictionary vice-versa. following theorem known case unit-norm atoms. theorem γ-coherent dictionary babel measure concludes proof babel measure since left-hand-side expression upper bound coherence measure obtained aforementioned connection coherence babel measures given theorem kernel. expression shows entries gram matrix describes diversity dictionary elements result corroborated recently property investigated pruning ls-svm removing samples smallest entries gram matrix. diversity measure studied section yields lower bound entropy dictionary scrutiny. shown this consider ﬁrst babel measure γbabel dictionary. following babel deﬁnition entropy given lower-bounded follows results provide lower bounds entropy following observations. bounds increase number elements dictionary i.e. obvious diversity grows. decrease coherence babel measures increase increase distance approximation measures increase. results provide quantitative details confront fact that using sparsiﬁcation criterion online learning values coherence babel thresholds provide less correlated atoms thus diversity within dictionary opposed high values distance approximation thresholds. entropy measures disorder randomness within given system. r´enyi entropy provides generalization well-known entropy deﬁnitions shannon harley entropies well quadratic entropy deﬁned given order probability distribution governs elements dealing discrete random variables source coding deﬁnition restricted drawn probability distribution yielding expression large values entropy correspond uniform spread data. since probability distribution unknown practice often approximated parzen window estimator estimator takes form following provide lower bounds entropy overcomplete dictionary terms diversity measure. initially restrict case quadratic entropy ﬁrst gaussian kernel type kernel generalizing results order r´enyi entropy well tsallis entropy. well-known shannon entropy uniform distribution yields largest entropy. property seems extend case non-zero order including minentropy. table expressions well-known entropies. investigated quadratic entropy derived lower bounds diversity measure. turns results extended general r´enyi entropy tsallis entropy shown next. special cases former listed table including harley maximum entropy associated cardinality shannon entropy essentially gibbs entropy statistical thermodynamics quadratic entropy also called collision entropy well min-entropy smallest measure family r´enyi entropies. proof proof jensen’s inequality concavity r´enyi entropy nonnegative orders. first relation shannon entropy given exploring following inequality connection hartley entropy straightforward finally trickier study minentropy since smallest entropy measure family r´enyi entropies consequence strongest measure information content. provide lower bound min-entropy relations yields following inequality furthermore easily extend results class tsallis entropy also called nonadditive entropy deﬁned following expression given parameter aforementioned lower bounds r´enyi entropy extended tsallis entropy using instance well-known relation lower bounds quadratic entropy given sections explored orders r´enyi entropy tsallis entropy. proof bounds δ-approximate γ-babel dictionaries straightforward theorem deﬁnition lower bounds γ-coherent δdistant dictionaries trickier prove. show this former following relation entropy feature space provides measure diversity atoms distribution. following show entropy estimated feature space lowerbounded bound expressed terms diversity measure. examples radial functions scaling factor ensure integration gaussian radialbased exponential inverse mutliquadratic kernels given table applied feature space. radial kernels monotonically decreasing distance namely grows kxi−xjk decreasing. statement results following lemma; also lemma kernel form positive deﬁnite completely monotonic namely k-th derivative satisﬁes theorem consider overcomplete kernel dictionary lower bound distance measure bounded diversity measure given theorem parzen window estimator estimated dictionary atoms feature space upper-bounded used window function. expectation rkhs previously investigated literature. notion embedding borel probability measure deﬁned topological space rkhs studied detail algorithmic one-class classiﬁer. analogy entropy analysis input space conducted section propose revisit feature space given section. examining pairwise distance atoms investigated dictionary ﬁrst establish section vi-a topological analysis overcomplete dictionaries. analysis explored section vi-b study entropy atoms feature space. providing lower bounds terms diversity measures results provide connections entropy analysis conducted previous section. theorem dictionary non-zero approximation measure non-unit coherence measure babel measure low-bounded distance measure. proof proof straightforward δ-approximate therefore complete proof sufﬁcient show expression always strictly positive. indeed quadratic polynomial form since considering roots quadratic polynomial respect discriminant strictly negative since cannot zero. therefore polynomial real roots strictly positive. engan aase hakon husoy method optimal directions frame design acoustics speech signal processing proceedings. ieee international conference vol. vol. aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation signal processing ieee transactions vol. mairal bach ponce sapiro online dictionary learning sparse coding proceedings annual international conference machine learning icml tropp greed good algorithmic results sparse approximation ieee trans. information theory vol. mallat zhang matching pursuit time-frequency dictionaries ieee transactions signal processing vol. gilbert muthukrishnan strauss approximation functions redundant dictionaries using coherence proc. annual acm-siam symposium discrete algorithms society industrial applied mathematics vukovi´c miljkovi´c growing pruning sequential learning algorithm hyper basis function neural network function approximation neural netw. vol. oct. cartwright roll over boltzmann physics world girolami orthogonal series density estimation kernel eigenvalue problem neural computation vol. suykens gestel brabanter moor vandewalle least squares support vector machines. singapore world scientiﬁc pub. hastie tibshirani friedman elements statistical learning data mining inference prediction. springer series statistics springer second edition corollary consider overcomplete kernel dictionary lower bound distance measure bounded diversity measure given theorem shannon entropy generalized r´enyi entropy order lower bounded respectively used window function. window function yields shannon entropy paper provided framework examine linear kernel dictionaries notion entropy information theory. examining different diversity measures showed overcomplete dictionaries lower bounds entropy. various deﬁnitions explored here results open door bridging information theory diversity measures analysis synthesis overcomplete dictionaries input feature spaces. futur works studying connections entropy component analysis order provide thorough examination develop online learning approach. conducted analysis illustrated within framework kernel-based learning algorithms easily extended machines gaussian processes neural networks. worth noting work devise particular diversity measure quantifying overcomplete dictionaries spirit recent work said´e lengell´e honeine richard achkar nonlinear adaptive ﬁltering using kernel-based algorithms dictionary adaptation international journal adaptive control signal processing submitted. honeine kallas kernel non-negative matrix factorization without pre-image problem proc. ieee workshop machine learning signal processing september honeine kallas kernel nonnegative matrix factorization without curse pre-image ieee transactions pattern analysis machine intelligence submitted. paul honeine born beirut lebanon october received dipl.-ing. degree mechanical engineering m.sc. degree industrial control faculty engineering lebanese university lebanon. received ph.d. degree systems optimisation security university technology troyes france postdoctoral research associate systems modeling dependability laboratory since september assistant professor university technology troyes france. research interests include nonstationary signal analysis classiﬁcation nonlinear statistical signal processing sparse representations machine learning. particular interest applications sensor networks biomedical signal processing hyperspectral imagery nonlinear adaptive system identiﬁcation. co-author best paper award ieee workshop machine learning signal processing. past years published peerreviewed papers. honeine richard bermudez on-line nonlinear sparse approximation functions proc. ieee international symposium information theory june gilbert muthukrishnan strauss tropp improved sparse approximation quasi-incoherent dictionaries international conference image processing vol. sept. sriperumbudur vangeepuram reproducing kernel space embeddings metrics probability measures. thesis electrical engineering university california diego jolla aai. noumir honeine richard simple one-class classiﬁcation methods proc. ieee international symposium information theory usa) july cucker smale mathematical foundations learning bulletin american mathematical society vol. honeine approximation errors online sparsiﬁcation criteria", "year": 2014}