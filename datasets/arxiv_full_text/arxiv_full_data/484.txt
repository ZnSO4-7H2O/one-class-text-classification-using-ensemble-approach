{"title": "Cognitive Database: A Step towards Endowing Relational Databases with  Artificial Intelligence Capabilities", "tag": ["cs.DB", "cs.AI", "cs.CL", "cs.NE"], "abstract": "We propose Cognitive Databases, an approach for transparently enabling Artificial Intelligence (AI) capabilities in relational databases. A novel aspect of our design is to first view the structured data source as meaningful unstructured text, and then use the text to build an unsupervised neural network model using a Natural Language Processing (NLP) technique called word embedding. This model captures the hidden inter-/intra-column relationships between database tokens of different types. For each database token, the model includes a vector that encodes contextual semantic relationships. We seamlessly integrate the word embedding model into existing SQL query infrastructure and use it to enable a new class of SQL-based analytics queries called cognitive intelligence (CI) queries. CI queries use the model vectors to enable complex queries such as semantic matching, inductive reasoning queries such as analogies, predictive queries using entities not present in a database, and, more generally, using knowledge from external sources. We demonstrate unique capabilities of Cognitive Databases using an Apache Spark based prototype to execute inductive reasoning CI queries over a multi-modal database containing text and images. We believe our first-of-a-kind system exemplifies using AI functionality to endow relational databases with capabilities that were previously very hard to realize in practice.", "text": "experience senses. broad terms cognition refers process building knowledge capabilities using innate resources enriching external inputs experiences interactions applying knowledge solve problems feeds back towards knowledge building. deﬁnitions relevant animate objects also applicable scenarios inanimate entities simulate cognitive processes. focus particular cognitive process reading comprehension text contexts apply relational databases. relational model relationships between database values entities deﬁned schema level data types keys functional dependencies. relationships instance level left explored queries. strong sense actual semantics data mostly lies users’ minds expressed queries. take signiﬁcant diversion point view. postulate signiﬁcant latent knowledge database instance irrespective querying. capture latent knowledge propose artiﬁcial intelligence techniques take advantage contexts. sources latent information include structure database well types associated data values include unstructured natural language text strings numerical values images dates etc. together factors lead interintra-column semantic relationships. current systems limited support exploit information namely extensions text extenders rdf-based ontologies however queries rely mainly value-based predicates detect patterns. addition relational data model ignores many interintra-column relationships. thus traditional queries lack holistic view underlying relations thus unable extract exploit semantic relationships collectively generated various entities database relation. examples serve clarify mean latent knowledge. ﬁrst example considers human resources database. database contains relations information employees work history grade addresses family members more. lately issues employee john dolittle. professional interested names employees know john well. sure phone capabilities relational databases. novel aspect design ﬁrst view structured data source meaningful unstructured text text build unsupervised neural network model using natural language processing technique called word embedding. model captures hidden inter-/intra-column relationships database tokens diﬀerent types. database token model includes vector encodes contextual semantic relationships. seamlessly integrate word embedding model existing query infrastructure enable class sql-based analytics queries called cognitive intelligence queries. queries model vectors enable complex queries semantic matching inductive reasoning queries analogies predictive queries using entities present database generally using knowledge external sources. demonstrate unique capabilities cognitive databases using apache spark based prototype execute inductive reasoning queries multi-modal database containing text images. believe ﬁrst-of-a-kind system exempliﬁes using functionality endow relational databases capabilities previously hard realize practice. media) start making calls collecting information names people would like consult. much information obtain already hidden legacy database diﬀused hard include people worked john managed complimented complained provided technical services members small team wouldn’t nice could write query would hidden knowledge essentially provide names employees related john dolittle. previous example relied content database isolation. next example involves entities external database. suppose database active vacations featuring diving hiking skiing desert driving more. worried vacation package dangerous one. naturally ofﬁcial descriptions database always provide information. likely words accident danger wounded death even present database. words present data sources wikipedia news articles. suppose access external sources. wouldn’t nice could utilize external sources pose query expressing dangerous vacation packages. example well previous ones want also degree certainty associated potential answer. worth pointing distinguishes level intelligence looking known extensions relational systems. current systems needs pose query based knowledge relational schema. query assisted text-aware features text extender wordnet using rdf-based ontologies used identify synonyms related terms relax query allowing explore possibilities explicitly speciﬁed user course relaxation result obtaining larger result set. useful features assume user knows specify backbone query. example problems listed formulating eﬀective query daunting task. fact examples resemble research projects rather standard queries. also allow user specify query natural language pushes problem expressing query automated tool; again unclear tool approach problem tool’s writer ready recipe. highlights need tools enable richer querying. paper explore potential using natural language processing approaches endow databases query expression capabilities hard perhaps impossible realize practice reasonable cost terms storage overhead well processing time. unique aspect proposal ﬁrst represent data optionally schema relational database unstructured text document technique vector space models extract latent semantic relationships associations generated text. trained model represents semantic meaning words vectors enables operations vectors mimic cognitive operations natural language words. words represent relational entities values model fact captures intra-/intercolumn relationships relational database. integrate model existing standard query processing system expose novel vector-based cognitive operations class analytics queries called cognitive intelligence queries believe ﬁrst examples transperently augmenting relational database system. clearly many possible ways integrating capabilities database systems e.g. enhancing querying capabilities improving operational capabilities current focus enhancing relational databases believe approach applied database domains xml/rdf json databases document databases graph databases key-value stores. currently developing apache spark-based prototype implement vision ai-enhanced cognitive relational database. rest paper provides details design implementation system. section introduce vector space modeling process detail execution system envisage. section provide speciﬁcs data preparation building specialized vector space model. section discuss three signiﬁcant classes queries similarity queries inductive reasoning queries cognitive olap queries; also present cognitive extensions relational data model. also describe design cognitive user deﬁned functions section describes practical scenario demonstrates unique aspects cognitive database system ability invoke inductive reasoning queries multi-modal data also discuss query performance issues focus important building block nearest-neighbor computations. related work discussed section conclude outlining extensions future work success criteria section goal build cognitive relational database system extracts latent semantic information also enrich using external input transparently enhance query capabilities. achieve goals rely approach infers word meanings using distributional hypothesis states words neighborhood contribute other’s meanings specifically predictive implementation approach commonly referred word embedding assumes probabilistic language model capture relationships neighborhood words word embedding approach ﬁxes d-dimensional vector space associates vector continuous-valued real numbers word encode meaning word. thus given text corpus meaning word reﬂects collective contributions neighborhood words diﬀerent appearances word corpus. words closely related similar meaning appear often within close proximity same similar meaning words. words similar meaning meaning vectors point similar directions i.e. cosine similarity vectors high surprising application word-embedding vectors usage solving inductive reasoning problems computing analogies using vector algebra calculations past years number methods developed implement word embedding. recently unsupervised neural network based approach wordvec gained popularity performance ability capture syntactic well semantic properties words. wordvec easy adapt trained incrementally used building models structured unstructured data sources database context vectors produced either learning text transformed extracted database and/or using external text sources. learning database natural generating vectors apply word embedding method string tokens generated database would correspond sentence relation would correspond document. thus vectors enable dual view data relational meaningful text. illustrate process consider figures present simple customer sales table. figure shows english sentence-like representation fourth table using scope generated sentence context word embedding approach infers latent semantic information terms token associations co-occurrences encode vectors. thus vectors capture ﬁrst interintra-column relationships within aggregate relationships across relation compute collective semantic relationships. training unique token database would associated d-dimensional meaning vector used query source database. simple example relational entity custd semantically similar custb many common semantic contributors equivalently custa similar custc similar reasons. relational view table rather original table generate text representing database content. useful supporting particular class applications. consider scenario view table deﬁned view projects data bold columns case generated sentence-like representation would diﬀerent ﬁrst case. hence generate diﬀerent word embedding model. examples illustrates design feature cognitive database neighborhood context used building word embedding model determined relational view used. hence inferred semantic meaning relational entities reﬂect collective relationships deﬁned associated relational view. cognitive relational database designed extension underlying relational database thus supports existing relational features. cognitive relational database supports class business intelligence queries called cognitive intelligence queries. queries extract information relational database based part contextual semantic relationships among database entities encoded meaning vectors. figure presents phases end-to-end execution cognitive relational database system. ﬁrst optional phase involves generating token sequences database tables applying word embedding model training method unstructured text corpus created token sequences. following model training resultant vectors stored relational system table runtime query execution engine uses various user-deﬁned functions fetch trained vectors system table needed answer queries queries take relations input return relation output. queries augment capabilities traditional relational queries used conjuction existing operators approach characterized unique aspects using unstructured text representation structured relational data input training process using unsupervised word embedding technique generate meaning vectors input text corpus. every unique token input corpus associated meaning vector. elaborate aspects. data preparation stage takes relational table diﬀerent types input returns unstructured meaningful text corpus consisting sentences. transformation allows generate uniform semantic representation diﬀerent types. process textiﬁcation requires stages data pre-processing text conversion textiﬁcation phase processes relational seperately converts data diﬀerent data types text. scenarios want build model also captures relational column names. cases pre-processing stage ﬁrst processes column names processing corresponding data. variables varchar type preprocessing involves following actions prepend column attribute string variable creating single concept token group varchar tokens e.g. jpmorgan chase represented jpmorgan chase creating single token semantically similar sequences varchar tokens e.g. sequences tokens bank america bank america represented single compound token bank america using external mapping domain-speciﬁc ontologies create common representative token group diﬀerent input tokens. approach useful enabling transfer learning reusing training model group related tokens. pre-processing input text tokens uniform representations. addition text tokens current implementation supports numeric values images techniques applied datatypes date well. numeric values three diﬀerent approaches generate equivalent text representations creating string version numerical value e.g. value column name price represented either price ‘‘.’’ user-managed categorization user specify rules deﬁne ranges numeric values generate string tokens numeric values. example consider values column name cocoa contents. value replaced string token choc dark value replaced string token choc etc. user-directed clustering user choose values numerical columns cluster using traditional clustering algorithms k-means. numeric value replaced string representing cluster value lies image data approaches similar ones used numerical values. ﬁrst approach represents image string token e.g. string representing image path unique identiﬁer. second approach uses pre-existing classifers cluster images groups uses cluster information string representation image. example domain-speciﬁc deep neural network based classiﬁer cluster input images classes corresponding class information create string identiﬁers images. ﬁnal approach applies of-the-shelf image generators e.g. watson visual recognition system extract image features uses string identiﬁers image. example lion image represented following string features animal mammal carnivore bigcat yellow etc. text numeric values images replaced text representations relational table viewed unstructured meaningful text corpus used building word embedding model. null values types replace string column name null. methods outlined applied data types date spatial data types lattitude longitude. implementation build word embedding model relational database data. training approach operates unstructured text corpus organized collection english-like sentences separated stop words need labelling training data unsupervised training. another advantage unsupervised training users need feature engineering features training extracted automatically training process. model training classical implementation uses simpliﬁed -layer shallow neural network views input text corpus sequence sentences. word sentence code deﬁnes neighborhood window compute contributions nearby words. unlike deep learning based classiﬁers output vectors real values dimension unique token training scenario text token training represent either text numeric image data. thus model builds joint latent representation integrates information across diﬀerent modalities using untyped uniform feature vectors. sentence generated relational generally natural language english. therefore wv’s assumption inﬂuence word nearby word decreases word distances increases applicable. implementation every token training inﬂuence nearby tokens; i.e. view generated sentence words rather ordered sequence. primary keys. first classical discards less frequent words computations. implementation every token irrespective frequency assigned vector. second irrespective distance primary considered neighbor every word sentence included neighborhood window word. also neighborhood extends foreign occurrences value value key. values particular columns given higher weightage contributions towards meanings neighborhood words. implementation enables users specify diﬀerent weights diﬀerent columns model training cremental training i.e. training system takes input pre-trained model generated sentences returns updated model. capability critical database updated regularly rebuild model scratch every time. pre-trained model built database queried external source. sources publicly available general sources text speciﬁc domain text textiﬁed databases text formed diﬀerent subset tables database. pre-trained models example transfer learning model trained external knowledge base used either querying purposes basis model practice enterprise database systems well data warehouses built using many inter-related database tables. forming training corpus multiple tables nontrivial. numerous options including ally linking based foreign keys appearing table pointing tuples another table foreign present tokenization table follow foreign table tokenize ﬁelds interest table insert resulting sequences sequence generated table figure presents another example database table address resulting token sequence utilizes relationship between empl table address table; namely address table provides addresses employees database table empl. technically resulting token sequence based foreign address column table provides value column address table. straight forward tokenize foreign keys insert subsequence generated immediately generated depicted figure another possibility intermix subsequence within sequence following tokenization foreign keys values implemented using existing query execution infrastructure. distinguishing aspect cognitive intelligence queries contextual semantic comparison between relational variables implemented using user-deﬁned functions udfs termed cognitive udfs take typed relational values input compute semantic relationships using uniformly untyped meaning vectors. enables relational database system seamlessly analyze data diﬀerent types using query. execution presented figure system ﬁrst initializes in-memory spark dataframes external data sources loads associated word embedding model another spark dataframe invokes queries using spark sql. queries invoke scala-based cognitive udfs enable computations meaning vectors cognitive takes input either relational query variables constant tokens returns numeric similarity value measures semantic relationships input parameters. user control result query using numerical bound similarity result value predicate selecting eligible rows. user also ordering clauses desc order results based similarity value captures semantic closeness relational variables higher similarity value closer relational variables. udfs perform three tasks processing input relational variables generate tokens used training. involves potentially repeating steps executed data preparation stage creating compound tokens. numeric values centroid information identify corresponding clusters. images uses image name obtain corresponding text tokens training tokens extracted uses fetch corresponding meaning vectors finally uses fetched vectors execute similarity computations generate ﬁnal semantic relationship score. tional operation cognitive calculate similarity pair tokens computing cosine distance corresponding vectors. vectors cosine distance computed cosine distance value varies sets sequences individual pair-wise similarity values aggragated generate result. case sequences computation ﬁnal similarity value takes account ordering tokens diﬀerent pair-wise distances contribute diﬀerently ﬁnal value based relative ordering. example food items chicken item consisting similar corn item consisting although contain salt basic extensions invoked queries enable semantic operations relational variables. query uses udfs execute nearest neighbor computations using vectors current word-embedding model. thus queries provide approximate answers reﬂect given model. queries broadly classiﬁed four categories follows similarity/dissimilarity queries basic compares sets relational variables integrated existing query form similarity query. figure illustrates query identiﬁes similar customers comparing purchases. assume sales table contains customer transactions credit card company whose sales.items column contains items purchased transaction current query uses similarityudf computes similarity match sets vectors correspond items purchased corresponding customers. unlike food item scenario purchased item list viewed unordered items; individual pair-wise distances contribute equally ﬁnal result. query shown figure uses similarity score select rows related customers returns ordered similar customer sorted descending order similarity score. query easily tweaked identify dissimilar customers based purchases. modiﬁed query ﬁrst choose rows whose purchases lower similarity results ordered ascending form using keyword returns customers highly dissimilar given customer results ordered descending order using desc keyword query return customers somewhat dissimilar given customer. modiﬁed version query identify similar customers based overall purchasing pattern evidenced number rows. word embedding model creates vector customer name captures overall purchases made customer. then customers similar purchase patterns would vectors close using cosine distance metric. pattern observed query applied domains well e.g. identifying patients taking similar drugs diﬀerent brand names identifying food items similar ingredients recommending mutual funds similar investment strategies. next section similarity query applied data types images. select x.custid y.custid y.merchant valuesimudf similarity sales sales x.custid=’custa’ valuesimudf x.custid y.custid x.amount y.amount order similarity desc figure presents query executing similarity operations using numeric variable. sake example assume transactions amount value unique numerical value associated string token. scenario wants identify transactions sales table using similarity based purchase amount valuesimudf takes numeric values input parameters compares using overall context numerical values. similar amount would shares context least similar amount would completely different context amount example also illustrates combine value-based semanticbased comparisons query. third case provides illustration prediction query uses model externally trained using unstructured data source another database consider scenario recall various fresh fruit types possible listeria infection. example assumes built word embedding model using recall notices external source. assume recall document lists fruits impacted possible listeria infection e.g. apples peaches plums nectarines... model create vectors words vector word listeria closer vectors apples peaches plums etc. import model query sales database customers bought items aﬀected recall deﬁned external source. figure shows similarityudf used identify purchases contain items similar listeria apples. example demonstrates powerful ability queries enables users query database using token present database capability applied diﬀerent scenarios recent updatable information used query historical data. example model built using recall notices could used identify customers purchased medicines similar recalled medicines. inductive reasoning queries unique feature word-embedding vectors capability answer inductive reasoning queries enable individual reason part whole particular general solutions inductive reasoning queries exploit latent semantic structure trained model algebraic operations corresponding vectors. encapsulate operations udfs support following types inductive reasoning queries analogies semantic clustering analogy sequences clustered analogies odd-man-out transferring information meaning subject another. common expressing analogy relationship pair entities source target reason possible target entity target associated another known source entity source example analogy query lawyer client doctor whose answer atient. solve analogy problem form needs token whose meaning vector closest ideal response figure illustrates query performs analogy computation relational variables using analogyudf. query aims customer sales table whose relationship category fresh produce similar category frozen goods analogyudf fetches vectors input variables using cosmul approach returns analogy score vector corresponding input token computed response vector. rows whose variables analogy score greater speciﬁed bound selected returned descending order score. since analogy operation implemented using untyped vectors analogyudf used capture relationships variables diﬀerent types e.g. images text. trait input data. semantic clustering operation wide applications including customer segmentation recommendation etc. figure presents query uses semantic clustering semclusterudf identify customers types inductive reasoning queries analogy sequences clustered analogies implemented combining strategies semantic clustering analogies. analogy sequence query takes input sequence analogy pairs source entity aims identify target entities exhibit relationships source entity input target entities. answer query needs ﬁrst compute centroid vector input target vectors answer following analogy problem source input centroid source using cosmul approach return target entities. unlike analogy sequences clustered analogy operation takes input analogy pairs different entity pairs aims predict pairs shares following relationships input sequence result source entities sourceo share dominant trait input source entities sourcei resultant target entity targeto related corresponding source entity analogy relationship. therefore solve clustered analogy queries ﬁrst perform semantic clustering compute result source entities sourceo result source entity compute target entities using analogy sequence approach. unlike analogy sequences query result clustered analogy query sets source entities associated target entities. items odd-man-out query identiﬁes item semantically diﬀerent remaining items odd-man-out query viewed complementary query semantic clustering. example execution requires context-speciﬁc semantic clustering meaning vectors. speciﬁcally clustering aims partition data clusters member containing remaining data. obvious application odd-man-out query would anomaly detection e.g. identifying fraudalant transaction customer. cognitive olap queries figure presents simple example using semantic similarities context traditional aggregation query. query aims extract maximum sale amount product category sales table merchant similar speciﬁed merchant merchant result collated using values product category. illustrated earlier similarityudf also used identifying customers diﬀerent speciﬁed merchant. either externally trained locally trained model. query easily adapted support aggregation functions avg. query extended support rollup operations aggregated values also exploring integration cognitive capabilities additional operators e.g. between. example value ranges operator computed using similarity query. query associated choices generated similarity inductive reasoning queries. another intriguing extension involves using contextual similarities choose members schema dimension hierarchy aggregation operations like rollup cube. example instead aggregating quarters years quarters semantically similar speciﬁed quarter. cognitive extensions relational data model powerful extensions enabled word vectors. need ability refer constituent tokens columns rows whole rows whole relations. extension declaration clause form token states variable refers token. locate token clause predicates form contains column whole whole relation extension easily express queries asking employee whose address contains token close token dept relation furthermore also extend relational variables form column variables whose names speciﬁed query writing time; bound runtime. variables queries conjunction token variables. enables database querying without explicit schema knowledge useful exploring database. interestingly notation basically syntactic sugar. software translation tool substitute actual table name actual column. then perform query substitution return union results lastly wonder numeric bounds udfs figure determined. short answer bounds application dependent much like hyperparameters machine learning. learns exploring underlying database running experiments. future envision tool guide users select appropriate numeric bound particular query. illustrate unique capabilities cognitive database system discussing scenario queries used gain novel insights multi-modal relational database. scenario consider database national parks across multiple countries links images animals associated national parks images open source image database imagenet populate database. database present results inductive reasoning queries using spark based prototype intel xeon system. prototype implemented scala supports queries written either scala python using spark spark dataframes python pandas sqlite interfaces. although database evaluation fairly simple architecture similar many real life databases e.g. multi-modal patient database text ﬁelds describing patient characteristics image ﬁelds referring associated images insurance claims database text ﬁelds containing claim information image ﬁelds storing supporting pictures figure presents original relational table created user. contains text ﬁelds list paths images provide additional information every image. table image path provide details referred images. create shared word embedding model text images employ automatic generator approach outlined section figure presents workﬂow extracting image features referred images. image database ﬁrst uploaded watson visual recognition system classiﬁcation text description. watson system’s json response parsed text attributes input image extracted. attributes form features images added original table create training version table training table either hidden exposed user. training table converted textual representation build text corpus training word embedding model. sentence text corpus includes original non-image extracted image features resulting multi-modal word embedding model non-image features contribute meaning image features vice versa meaning vectors uniformly represented using vectors dimension figure illustration workﬂow generate text attribute database images using watson visual recognition service using sample image imagenet dataset meaning vectors computed used evaluate semantic relationships values original relational table. example compare national parks serengati sunderbans would similar share multiple image features. noted insight using standard queries original table image information. further many values training database syntactically diﬀerent existing systems fail extract semantic similarities. similar every image user chosen images. images share features input images. query select images lion vulture shark input combinedavgsim identify images similar three images. although input images display animals three diﬀerent classes share common feature three animals carnivorous. computes average vector three input images selects images whose vectors similar computed average vector similarity score higher figure shows three image results andean condor glutton wolverine tyra. although animals diﬀerent classes carnivores feature shared animals input set. ﬁnal example demonstrates external semantic model querying multi-modal database. scenario ﬁrst train word embedding model external knowledge base derived wikipedia. similar model trained database external model assigns dimensional meaning vectors unique tokens wikipedia model select token associated concept hypercarnivore refers class animals whose diet meat. examples hypercarnivores include lions sharks polar bears crocodiles hyenas etc. therefore model hypercarnivore meaning vector related meaning vectors tokens shark crocodiles etc. query employ externally trained model extract images similar concept hypercarnivore. proximityavgforextkb uses external model ﬁnds images database whose classd features related hypercarnivore returns images whose similarity score higher figure shows query result pictures hyenas members hypercarnivore class. example also demonstrates unique capability cognitive databases allows querying database using token present database. case original training databases contain token hypercarnivore. query images figure illustrates analogy sequence query. cases queries formulated using udfs analogyquery analogysequence take values training database input. case analogy query goal images whose classd feature relationship classc feature speciﬁed relationship reptilemonitor lizard. table ﬁrst fetches meaning vectors input parameters uses cosmul approach relational value whose vector maximizes analogy similarity score deﬁned equation query returns corresponding images whose similarity score higher reports descending order similarity score. figure presents output fragment query corresponding images spiny finned fish. analogy sequence query uses operates sequence analogy pairs share source entity diﬀerent target entities converts analogy sequence problem traditional analogy problem ﬁrst computing average vector target entities using cosmult approach. figure presents query analogy sequence output converted single matrix-vector multiplication operation. generalized matrixmatrix multiplication operation enable distance computations sets vectors. ﬁrst identify candidate vectors spatially closer dimensional vector space using either locality sensitive hashing clustering spherical k-means algorithm invoke distance calculations candidate compute precise distances. approach dimension vector locations mapped bit-vector signatures length projecting random planes. given vector spatially closer vectors identiﬁed choosing small hamming distance corresponding signatures. k-means approach centroid information identify candidate spatially close vectors. approaches also accelerated using either simd functions gpus also reduce redundant computations using relational view pre-selects rows table based certain criteria. addition candidate variables known query engine pre-compute pair-wise distances cached results execution udfs later. core nearest-neighbor distance computations namely dotproduct matrix-vector matrix-matrix computations accelerated hardware accelerators on-chip simd using gpus numerical libraries essl openblas provide hardware accelerated matrix computation kernels. further nearest neighbor computations also parallelized either using cpu-based multithreading distributing cluster machines using distributed infrastructure apache spark. language embedding past years number methods introduced obtaining vector representation words language called language embedding. methods range brute force learning various types neural networks log-linear classiﬁers various matrix formulations matrix factorization techniques lately wordvec gained prominence vectors produces appear capture syntactic well semantic properties words. exact mechanism employed wordvec suggestions alternatives subject much research although wordvec gained much prominence many possible methods generating word representing vectors. example glove also builds word embeddings function optimization approach word co-occurrence matrix. vectors associated larger bodies text paragraphs although used hypothetical scenarios demonstrate ideas queries applicable broad class domains. include ﬁnance insurance retail customer care analytics healthcare genomics semantic search documents healthcare informatics human resource management. examples demonstrate several unique capabilities cognitive database systems namely ability build joint cross-modal semantic model multi-modal data transperent integration novel cognitive queries existing query infrastructure using untyped vector representations support contextual semantic queries across multiple data types ability import externally trained semantic model apply enable querying database using token present database. best knowledge none current industrial academic open source database systems support capabilities. cognitive intelligence queries standard analytics queries invoke udfs enable contextual semantic operations relational variables. irrespective kind query core computation involves computing pair-wise similarities vectors used identify nearest furthest neighbors vector. worst case query invoke every combination evaluated turn operate large number vectors. since practice number combinations high critical optimize performance distance computations nearest-neighbor calculations queries. dimensional vector space pair-wise distance between vectors calculated computing cosine distance normalized forms vectors pair-wise distance calculation gets simpliﬁed vector dotproduct even documents. applications paragraph document embedding appear recent work also exploring applying word embeddings capture image semantics applications word embedding word embedding model used wide variety applications beyond nlp. provide general neural framework building vector embaddings entities diﬀerent types vectorial embedding space. common representation used diﬀerent tasks text classiﬁcation link prediction document recommendation etc. youtube recommendation system uses embedding approaches capture user behavior similar embedding-based approaches used recommending news articles discovering topics personalized fashion shopping hope proposed using word embedding supporting analogy queries knowledge bases patent database deepwalk nodevec proposed using word embedding approaches learning neighborhood features nodes network graph. word embedding approaches also used variety semantic applications e.g. embedding triples encoding geo-spatial proximity using latent feature vectors data integration knowledge databases explored relational databases context text capabilities e.g. synonyms practice literature techniques detecting similarity records ﬁelds also explored. semantic similarity database records explored phrase-based ranking applying approach relational data appears indexing searching relational data modeling tuples virtual documents appear eﬀective keyword-based selection relational databases explored system detecting similarity content structure using relational database described related work similarity join appears semantic queries described recently shin described deepdive uses machine learning techniques e.g. markov logic based rules convert input unstructured documents structured knowledge base. proposed cognitive database system distinguished following unique features encoding relational data using word embedding techniques using semantic vectors enable class analytics queries ability make contextual semantic matching unlike traditional value matching supported current queries capturing relationalships across multiple data types including images ability using external knowledge bases. further semantic vectors primarily based database means assume reliance dictionaries thesauri word nets like. vectors generated used vastly enriching querying expressiveness virtually query language. capabilities beyond analytical capabilities present current relational systems. wellknown commercial open source madlib database systems built-in analytics capabilities e.g. spark mllib. apache spark also create deep-learning pipeline invoke external deep-learning infrastructure e.g. tensorflow train model load trained model perform inferencing operations however systems view databases repositories storing input features results analytics deep-learning frameworks. hand cognitive databases word embedding model extract features database entities enhance querying capabilities. systems based statistical relational learning models combine probabilistic graphical models ﬁrst-order logic encode uncertain ﬁrst-order logic rules based known information contrast cognitive database learns information relational data known apriori. paper presented cognitive database innovative relational database system uses power word embedding models enable novel capabilities database systems. word embedding approach uses unsupervised learning generate meaning vectors using database-derived text. vectors capture syntactic well semantic characteristics every database token. vectors enhance database querying capabilities. essence vectors provide another look database almost orthogonal structured relational regime vectors enable dual view data relational meaningful text. thereby introduce explore class queries called cognitive intelligence queries extract information database based part relationships encoded vectors. implementing prototype system apache spark exhibit power queries. current infrastructure enables complex sql-based semantic queries multi-modal databases working accelerating model training nearest neighbor computations using variety approaches developing techniques incremental vector training. believe queries applicable broad class application domains including healthcare bio-informatics document searching retail analysis data integration. incorporating capabilities relational databases. holds great promise innovative applications diﬀerent view data compared today’s database systems. since based concept easy comparisons. example relevant benchmarks except ones used testing language features analogies success cognitive databases mainly evaluated based applications within known domains domains applications currently sphere ai-based systems adaptation capabilities standard feature leading vendor well open source database systems. finally hope work spurs research initiatives exciting emerging area database management systems. baroni dinu kruszewski. dont count predict systematic comparison context-counting context-predicting semantic vectors. proceedings conference association computational linguistics reichart venkatrao pellow pirahesh. data cube relational aggregation operator generalizing group-by cross-tab sub-totals. data mining knowledge discovery grover leskovec. nodevec scalable feature learning networks. proceedings sigkdd international conference knowledge discovery data mining pages wang zhou. spark top-k keyword query relational databases. proceedings sigmod international conference management data beijing china june pages park tajik cafarella mozafari. database learning toward database becomes smarter every time. proceedings international conference management data pages improving topic discovery word embeddings coordinating global local contexts. proceedings sigkdd international conference knowledge discovery data mining pages", "year": 2017}