{"title": "Multi-Stage Classifier Design", "tag": ["cs.CV", "stat.ML"], "abstract": "In many classification systems, sensing modalities have different acquisition costs. It is often {\\it unnecessary} to use every modality to classify a majority of examples. We study a multi-stage system in a prediction time cost reduction setting, where the full data is available for training, but for a test example, measurements in a new modality can be acquired at each stage for an additional cost. We seek decision rules to reduce the average measurement acquisition cost. We formulate an empirical risk minimization problem (ERM) for a multi-stage reject classifier, wherein the stage $k$ classifier either classifies a sample using only the measurements acquired so far or rejects it to the next stage where more attributes can be acquired for a cost. To solve the ERM problem, we show that the optimal reject classifier at each stage is a combination of two binary classifiers, one biased towards positive examples and the other biased towards negative examples. We use this parameterization to construct stage-by-stage global surrogate risk, develop an iterative algorithm in the boosting framework and present convergence and generalization results. We test our work on synthetic, medical and explosives detection datasets. Our results demonstrate that substantial cost reduction without a significant sacrifice in accuracy is achievable.", "text": "abstract many classiﬁcation systems sensing modalities different acquisition costs. often unnecessary every modality classify majority examples. study multi-stage system prediction time cost reduction setting full data available training test example measurements modality acquired stage additional cost. seek decision rules reduce average measurement acquisition cost. formulate empirical risk minimization problem multi-stage reject classiﬁer wherein stage classiﬁer either classiﬁes sample using measurements acquired rejects next stage attributes acquired cost. solve problem show optimal reject classiﬁer stage combination binary classiﬁers biased towards positive examples biased towards negative examples. parameterization construct stage-by-stage global surrogate risk develop iterative algorithm boosting framework present convergence generalization results. test work synthetic medical explosives detection datasets. results demonstrate substantial cost reduction without signiﬁcant sacriﬁce accuracy achievable. many applications including homeland security medical diagnosis decision systems composed ordered sequence stages. stage associated sensor physical sensing modality. typically less informative sensor cheap informative sensor either expensive requires time acquire measurement. practice measurement budget allow modalities used simultaneously making decisions. goal scenarios attempt classify examples cost sensors limit number examples expensive time consuming informative sensor required. fig. multi-stage system consists stages. stage binary classiﬁer reject option. system incurs penalty stage rejects seek measurements. classiﬁer sees ﬁrst sensing modalities making decision. example explosives detection ﬁrst stage infrared imager metal detector used high throughput cost. second stage could slower expensive active millimeter wave scanner. ﬁnal third stage time consuming human inspection. medical applications ﬁrst stages typically non-invasive procedures followed expensive tests ﬁnal stages invasive procedures. sensors ordered stages stage associated sensor measurement sensing modality. multiple stages ordered sequence sensors sensor modalities later stages corresponding expensive time-consuming measurements. many situations often ﬂexibility choosing sensing modality collection possible modalities. cases optimal choice sensing actions also becomes issue. methodology modiﬁed account general setting primarily consider ﬁxed order stages sensing modalities paper. justiﬁed account fact many situations come across consist handful sensors sensing modalities. consequently situations problem choosing sensor ordering justiﬁed since could brute force enumerate optimize different possibilities. reject classiﬁers sequential decision rules either attempt fully classify instance stage reject instance next stage measurements case ambiguity. example explosives detection decision rule ﬁrst stage based scan would attempt detect whether person threat identify explosive type/location case threat. person identiﬁed threat ﬁrst stage unnecessary seek information. similarly medical diagnosis disease diagnosed early stage makes sense begin early treatment rather waiting conclusive tests. information computation note setup partial measurements acquired stage making decision. methods detection cascades full measurement therefore information available every stage. therefore region feature space carved complex regions measurement space equivalently complex features extracted higher costs. contrast partial measurements feature classiﬁer employ agnostic unavailable measurements stage. stage example fig. illustrates advantages scheme alternative scheme ﬁrst acquires measurements sensing modalities refer centralized classiﬁer. reject classiﬁer utilizes stage sensor fraction data achieves performance centralized classiﬁer. fig. line optimal decision using stage modality. blue line optimal using both. curve classiﬁcation error samples rejected point corresponds classifying everything stage blue corresponds rejecting everything classifying using modalities. green partial reject strategy. samples outside green region classiﬁed using ﬁrst modality samples inside region rejected stage classiﬁed using modalities. note blue green error reject strategy stage sensor examples reducing cost factor speciﬁcally assume training examples measurements sensors sensing modalities well ground truth labels available. goal derive sequential reject classiﬁers reduces cost measurement acquisition error prediction phase. show sequential reject classiﬁer problem formulated instance partially observable markov decision process class-speciﬁc probability models different sensor measurements known. case optimal sequential classiﬁer cast solution dynamic program solution sequence stage-wise optimization problems stage problem combination cost current stage cost-to-go function carried later stages. nevertheless class probability models typically unknown; scenarios produce high-dimensional sensor data consequently unlike conventional approaches probability models ﬁrst estimated solve pomdps adopt non-parametric discriminative learning approach. utilize structure pomdp solution empirically approximate value cost-to-go function discrete subset data-space. next instead interpolating parameterizing cost-to-go function learning data formulate empirical discriminative objective utilizes point-wise cost-to-go estimates evaluated training directly learn classiﬁers minimize objective. using decomposition formulate novel multi-stage expected risk minimization problem. solve problem stage ﬁrst factoring cost function classiﬁcation rejection decisions. transform reject decisions binary classiﬁcation problem. speciﬁcally show optimal reject classiﬁer stage combination binary classiﬁers biased towards positive examples biased towards negative examples. disagreement region deﬁnes reject region. approximate empirical risk global surrogates. present iterative solution demonstrate local convergence properties. solution obtained boosting framework. extend well-known margin-based generalization bounds multi-stage setting. tested methods synthetic medical explosives datasets. results demonstrate advantage multistage classiﬁer cost reduction without signiﬁcant sacriﬁce accuracy. active feature acquisition subject paper studied machine learning community early work closely related called prediction time active feature acquisition approach area cost-sensitive learning. goal make sequential decisions whether acquire feature improve prediction accuracy. natural approach formalize problem pomdp. model decision process infer feature dependencies taking acquisition costs account. study strategies optimizing decision trees minimizing acquisition costs. construction usually based purity metric entropy. proposes method acquires attribute increases expected utility. however methods require estimating probability likelihood certain feature value occurs given features collected far. surrogates based classiﬁers regressors employed estimate likelihoods approach requires discrete binary quantized attributes. contrast problem domain deals high dimensional measurements develop discriminative learning approach formulate multi-stage empirical risk optimization problem reduce measurement costs misclassiﬁcation errors. stage solve reject classiﬁcation problem factorizing cost function classiﬁcation rejection decisions. embed rejection decision binary classiﬁcation problem. single stage reject classiﬁers paper also closely related topic reject classiﬁers also investigated. however literature reject classiﬁers primarily considered single stage scenario. bayesian framework introduced chow’s rule classiﬁcation. states given observation reject cost classes reject maximum posteriors class less reject cost maxk=..j context machine learning posterior distributions known decision rule estimated directly. popular approach reject examples small margin. speciﬁcally context support vector machine classiﬁers deﬁne reject region within small distance separating hyperplane embed hinge loss formulation. proposes reject criteria motivated active learning implementation turns computationally impractical. contrast consider multiple stages reject classiﬁers. assume error prone second stage occurs ﬁelds threat detection medical imaging. scenario rejecting margin always meaningful. fig. illustrates thresholding margin reject lead signiﬁcant degradation. usually happens stage measurements complimentary; examples within small margin stage boundary meaningful reject. multiple stages margin based reject classiﬁers considered using svms image classiﬁcation. method take account cost later stages similar myopic method compare experiments section. detection cascades multi-stage sequential reject classiﬁers bears close resemblance detection cascades. much literature cascade design references therein) cascades roughly follow set-up introduced reduce computation cost classiﬁcation. stage cascade binary classiﬁer high detection rate mediocre false alarm rate. stage makes partial decision; either detects instance negative passes next stage. last stage cascade makes full decision namely whether example belongs positive negative class. several fundamental differences detection cascades multistage reject classiﬁers difference system architecture. detection cascades primarily concerned binary classiﬁcation problems. make partial decisions delaying positive decision ﬁnal stage. contrast msrcs make full classiﬁcation decisions stage. conceptually distinction requires fundamentally approach; detection cascades work focus unbalanced problems positives large number negatives; goal stage admit large false positives negligible missed detections. consequently stage associated binary classiﬁcation problem acutely sensitive missed detections. contrast scheme stage composite scheme composed classiﬁer well rejection decision. rejection decision binary classiﬁcation problem. practice msrcs arise important areas medical diagnosis explosives detection argued item performance metric detection cascades tradeoff missed detections ﬁnal stage average computation. msrc’s tradeoff average misclassiﬁcation errors number examples reached later stages reasons difﬁcult directly compare algorithms developed msrcs developed detection cascades. nevertheless goals resulting algorithms similar issues arise cascade design references therein) namely perform joint optimization stages cascade given cost structure different features. cost sensitive methods network intrusion detection systems area sequential decision systems explored. features different computation costs. cost level ruleset learned. goal many cost rules possible. related set-up consider general ensemble base classiﬁers explore minimize ensemble size without sacriﬁcing performance. test phase sample another classiﬁer added ensemble conﬁdence current classiﬁcation low. here similar detection cascades goal reduce computation time. described item important distinction that setting decision based partial information acquired stage. computation driven method stage decides using feature computed full measurement vector. fig. gaussian mixture error rate reject rate complementary measurements. stage uses stage uses dim. myopic strategy thresholding margin classiﬁer method global surrogate; bayesian classiﬁer thresholding margin performs signiﬁcantly worse method. distributed according unknown distribution data point features belongs classes indicated truncated feature vector stage .xk}. space ﬁrst features system stages order stages ﬁxed stage acquires measurement. stage decison reject option either classify example delay decision next stage incur penalty here indicates reject decision. make decision using ﬁrst sensing modalities. last stage terminal standard classiﬁer. deﬁne system risk section digress discriminative setting analyze problem assumption underlying distribution known. hope discover fundamental structure simplify empirical risk formulation next section. proof simplify derivations assume uniform class prior probability however results easily modiﬁed account non-uniform prior. expected conditional risk solved optimally dynamic program recursion note modiﬁed risk functional remarkably similar except modiﬁed reject cost replaces constant stage cost also consider range meaningful. classes random guessing strategy would incur average risk order meaningful option. work contains detailed analysis single stage reject classiﬁer bayesian setting. clear expression express decision regions terms binary classiﬁers observe given reject cost reject region intersection binary decision regions. modify risk function terms agreement disagreement regions classiﬁers namely =y]=y] note arriving expression used =c]=c]. summary section derive optimal pomdp solution decouple multi-stage risk single stage optimization. then binary classiﬁcation setting derive optimal representation reject region classiﬁer terms biased binary decisions section assume probability model longer known cannot estimated high-dimensionality data. instead task multi-stage decision rules based given training here consider binary classiﬁcation setting {+−}. take advantage stage-wise decomposition pomdp solution theorem parametrization reject region theorem formulate empirical version stage risk however requires knowledge costto-go instead trying learn complex function deﬁne point-wise empirical estimate cost-to-go training data observe that standard setting need constrain class decision rules here. constraints minimum risk equal zero achieved ﬁrst stage itself. note stage-wise decomposition signiﬁcantly simpliﬁes erm. objective function minimize empirical version multi-stage risk much difﬁcult stage interdependencies. minimizing indicator loss hard problem. instead take usual approach replace surrogate. introduce algorithm boosting framework based analysis previous section. boosting many possible machine learning approaches used solve boosting easy implement known good performance. global surrogate algorithm sigmoid loss function +exp approximate indicator. similar sigmoid based losses used boosting before subproblem reduces boosting weighted loss stages optimized following order. start last stage make backwards ﬁrst stage. forward pass stage last. forward back passes repeated convergence. algorithm proof simply fact minimizing global smooth cost function coordinate descent vector weak learner weights parametrizing derivation three stage system global cost refer appendix regularization reduce overﬁtting reduce overtraining introduce simple effective regularization. loss parameter introduce multiplicative term cost functionminq exp∑n term limlarge step size weak hypothesis become. also introduces simple stopping criteria abort corresponds situation descent directions found minimize cost function. system composed margin maximizing classiﬁers therefore appropriate derive generalization error bounds based margins. turns employ maximum margin generalization techniques derive error bounds stage version system. stage system consists three boosted binary classiﬁers theorem distribution ×{+−} sample examples chosen independently random according rejected subsample size assume base-classiﬁer spaces ﬁnite probability least random choice training boosted classiﬁers satisfy following bound system error terms error stage error stage. theorem states generalization error bounded empirical margin error training plus term inversely proportinal margins number training samples stage. interesting observation number samples reaches stage depends reject classiﬁer stage. examples make second stage strong generalization. goal demonstrate large fraction data classiﬁed early stage using cheap modality. experiments four real life datasets measurements arising meaningful stages. myopic absolute margin classiﬁer measure conﬁdent classiﬁer example. examples small margin conﬁdence rejected next stage acquire features. approach based reject classiﬁcation know claim optimal classiﬁer threshold posterior. stage obtain binary boosted classiﬁer trained data. threshold margin classiﬁer known given inﬁnite amount training data boosting certain losses approaches likelihood ratio reject region given threshold deﬁned tk}. completely myopic approach rejection take account performance later stages. method similar tefe also uses absolute margin measure rejection. difference myopic strategy boosting classiﬁer used tefe. expected utility/margin expected margin difference measures attribute acquired would useful example. expected utility example large attribute acquired. approach based work train boosted binary classiﬁers data stage given measurement current stage compute expected utility acquiring next measurement rejected next stage utility greater threshold. here denotes possible values take. note approach requires estimating therefore measurement discrete distribution needs performance metric natural performance metric trade system error measurement cost. note utility myopic methods unclear thresholds stage given measurement cost reason compare stages system. stages not-practical would need test every possible every stage stage setting measurement cost proportional fraction examples rejected second stage. algorithm vary reject cost generate system error reject rate plot. margin utility sweep threshold system error stage stage errors. reject rate fraction examples rejected stage require additional measurements. reject rate corresponds higher error rate data classiﬁed ﬁrst stage using less informative measurements. high reject rate performance similar centralized classiﬁer examples classiﬁed stage. experiments stumps weak learners. dataset experiment randomly split data training testing. results evaluated separate test simulations averaged monte-carlo trials. number iterations boosting subproblem global surrogate algorithm number outer loop iterations discrete valued data experiments compare method utility approach consider discrete data. ﬁrst dataset quantized gaussian mixture synthetic data dimension. dimension stage one; dimension stage two. second dataset mammogram mass machine learning repository. used predict severity mammographic mass lesion contains attributes extracted image also evaluation radiologist conﬁdence scale addition true biopsy results. ﬁrst stage features extracted image second stage expert conﬁdence rated discrete scale automatic analysis image cheaper employing opinion radiologist. simulations fig. demonstrate utility performs worse compared fig. comparison global utility quantized gaussian clusters mammogram dataset. reject rate system error. reject rate fraction examples measurements stages. approach outperforms utility possibly need estimate probability likelihoods continuous valued data experiments compare global method myopic method three datasets. pima indians diabetes dataset consists measurements. measurements inexpensive acquire consist simple tests body mass index pedigree. designate ﬁrst stage. measurements constitute second stage require expensive procedures. polyp dataset consists hyper-spectral measurements colon polyps collected colonoscopies attribute measured intensity equally spaced frequencies. finer resolution requires higher photon count proportional acquisition time. ﬁrst stage coarse measurement downsampled frequency bins. second stage full resolution frequency response. using course measurements cheaper acquiring full resolution. threat dataset contains images taken people wearing various explosives devices. imaging done three modalities infrared passive millimeter wave active millimeter images registered. extract many patches images training data. patch carries binary label either contains threat clean. pmmw fastest modalities also less informative. ammw requires raster scanning person slow also useful. fig. three datasets evaluated pima polyps threat. reject rate error rate varying reject cost reject rate fraction examples measurements stages. global myopic compared. global better performance myopic better situations. table performance illustration different datasets datasets sensing modalities. centralized denotes test error obtained modalities. last three columns denotes performance different approaches. performance measured average number examples requiring stage achieve error close centralized. utility approach work last three datasets high-dimensionality issues. note signiﬁcant gains approach competing ones many interesting datasets. fig. global performs better margin cases. threat data margin appears marginally worse global however points curve reject rates less heuristic nature margin cannot construct multistage classiﬁer arbitrary reject rate. goal reach performance centralized classiﬁer utilizing stage sensor small fraction examples. overall results demonstrate beneﬁt multi-stage classiﬁcation rejection rate less small sacriﬁces performance. mammogram data implies half patients diagnoses made solely automatic analysis image without expensive opinion radiologist. pima data similar error achieved without expensive medical procedures. polyps dataset fast resolution measurement enough classify large fraction patience. threat dataset pmmw sufﬁcient decide whether threat present majority instances without requiring person slower ammw scanner. unbalanced false positive false negative penalties medical diagnosis threat detection penalty false positives false negatives equal. easily adapt algorithm account setting. empirical risk modiﬁed include penalty type error type error. experiment fig. demonstrates global algorithms scenario. reject cost compute curve. also compute corresponding average reject rate value highest reject rate corresponds best performance also highest acquisition cost incurred system. note good performance achieved requesting instances measured second stage. fig. stage using global surrogate method. curve corresponds different value reject cost legend displays average reject rate note corresponds centralized system good performance achieved requesting instances measured second stage. three stages lastly demonstrate three stage system apply algorithm three stages threat dataset. note margin unclear generalize multistage scenario deﬁne reject costs different stages. ﬁrst stage second pmmw ammw third. cost acquiring vary costs pmmw stage ammw generate error point corresponds performance particular multistage classiﬁcation strategy. vertical axis fraction examples pmmw measurements used making decision. horizontal axis fraction examples three modalities used. example point ﬁgure correspond system examples pmmw rest data modalities. strategy achieves system error rate note support lies diagonal. reject rates less one. results demonstrate interesting observations. best performance achieved modalities used every example move along vertical lines allow fraction classiﬁed pmmw avoiding ammw together. strategy achieves performance comparable centralized system fig. three stage system. color maps error. point corresponds performance particular multistage classiﬁcation strategy. vertical axis fraction examples pmmw measurements used making decision. horizontal axis fraction examples three modalities used. example point ﬁgure correspond system examples pmmw rest data modalities. strategy achieves system error rate paper propose general framework sequential decision system nonparametric setting. starting basic principles derive bayesian optimal solution. then simplify problem parameterize classiﬁer stage terms binary decisions. formulate problem optimize alternatively minimizing stage time. remarkably subproblems turn weighed binary error minimizations. introduce practical boosting algorithm minimizes global surrogate empirical risk test several datasets. results show advantage formulation heuristic approaches. overall experiments demonstrate multi-stage", "year": 2012}