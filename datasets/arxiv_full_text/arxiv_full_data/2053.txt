{"title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.", "text": "fundamental challenge developing high-impact machine learning technologies balancing need model rich structured domains ability scale data. many important problem areas richly structured large scale social biological networks knowledge graphs images video natural language. paper introduce formalisms modeling structured data show capture rich structure scale data. ﬁrst hingeloss markov random ﬁelds kind probabilistic graphical model generalizes diﬀerent approaches convex inference. unite three approaches randomized algorithms probabilistic graphical models fuzzy logic communities showing three lead inference objective. deﬁne hl-mrfs generalizing uniﬁed objective. second formalism probabilistic soft logic probabilistic programming language makes hl-mrfs easy deﬁne using syntax based ﬁrst-order logic. introduce algorithm inferring most-probable variable assignments much scalable general-purpose convex optimization methods uses message passing take advantage sparse dependency structures. show learn parameters hl-mrfs. learned hl-mrfs accurate analogous discrete models much scalable. together algorithms enable hl-mrfs model rich structured data scales previously possible. many problems machine learning domains rich structured many interdependent elements best modeled jointly. examples include social networks biological networks natural language computer vision sensor networks machine learning subﬁelds statistical relational learning inductive logic programming structured prediction seek represent dependencies data induced relational structure. ever-increasing size available data growing need models highly scalable still able capture rich structure. paper introduce hinge-loss markov random ﬁelds class probabilistic graphical models designed enable scalable modeling rich structured data. hl-mrfs analogous discrete mrfs undirected probabilistic graphical models probability mass log-proportional weighted feature functions. unlike discrete mrfs however hl-mrfs deﬁned continuous variables unit interval. model dependencies among continuous variables linear quadratic hinge functions probability density lost according weighted hinge losses. show hinge-loss features capture many common modeling patterns structured data. designing classes models generally trade scalability expressivity complex types connectivity structure dependencies computationally challenging inference learning become. hl-mrfs address crucial extremes. using hinge-loss functions model dependencies among variables admit highly scalable inference without restrictions connectivity structure hl-mrfs capture wide range useful relationships. reason expressive hinge-loss dependencies core number scalable techniques modeling discrete continuous structured data. motivate hl-mrfs unify three diﬀerent approaches scalable inference structured models randomized algorithms local consistency relaxation discrete markov random ﬁelds deﬁned using boolean logic reasoning continuous information fuzzy logic. show three approaches lead convex programming objective. deﬁne hl-mrfs generalizing uniﬁed inference objective weighted hinge-loss features using weighted features graphical models. since hl-mrfs generalize approaches reason relational data weighted logical knowledge bases retain high level expressivity. show section eﬀective modeling discrete continuous data. also introduce probabilistic soft logic probabilistic programming language makes hl-mrfs easy deﬁne large relational data sets. idea explored classes models markov logic networks discrete mrfs relational dependency networks dependency networks probabilistic relational models bayesian networks. build previous approaches well connection hinge-loss potentials logical clauses deﬁne psl. addition probabilistic rules provides syntax enables users easily apply many common modeling techniques domain range constraints blocking canopy functions aggregate variables deﬁned random variables. next contribution introduce number inference learning algorithms. first examine inference i.e. problem ﬁnding probable assignment unobserved random variables. inference hl-mrfs always convex optimization. although oﬀ-the-shelf optimization toolkit could used methods typically leverage sparse dependency structures common graphical models. introduce consensus-optimization approach inference hl-mrfs showing problem decomposed using alternating direction method multipliers resulting subproblems solved analytically hinge-loss potentials. approach enables hl-mrfs easily scale beyond capabilities oﬀ-the-shelf optimization software sampling-based inference discrete mrfs. show learn hl-mrfs training data using variety methods structured perceptron maximum pseudolikelihood large margin estimation. since structured perceptron large margin estimation rely inference subroutines maximum pseudolikelihood estimation eﬃcient design methods highly scalable hl-mrfs. evaluate core relational learning structured prediction tasks collective classiﬁcation link prediction. show hl-mrfs oﬀer predictive accuracy comparable analogous discrete models scaling much better large data sets. paper brings together expands work scalable models structured data either discrete continuous mixture eﬀectiveness hl-mrfs demonstrated many problems including information extraction automatic knowledge base construction extracting evaluating natural-language arguments high-level computer vision drug discovery predicting drug-drug interactions natural language semantics automobile-traﬃc modeling recommender systems information retrieval predicting attributes trust social networks. ability easily incorporate latent variables hl-mrfs enabled applications including modeling latent topics text predicting student outcomes massive open online courses researchers also studied make hl-mrfs even scalable developing distributed implementations already widely applied indicates hl-mrfs address open need machine learning community. paper organized follows. section ﬁrst consider models structured prediction deﬁned using logical clauses. unify three diﬀerent approaches scalable inference models showing optimize convex objective. generalize objective section deﬁne hl-mrfs. section introduce specifying language giving many examples common usage. next introduce scalable message-passing algorithm inference section many structured domains propositional ﬁrst-order logics useful tools describing intricate dependencies connect unknown variables. however domains usually noisy; dependencies among variables always hold. address this logical semantics incorporated probability distributions create models capture structure uncertainty machine learning tasks. common logic deﬁne feature functions probabilistic model. focus markov random ﬁelds popular class probabilistic graphical models. informally distribution assigns probability mass using scoring function weighted combination feature functions called potentials. logical clauses deﬁne potentials. ﬁrst deﬁne mrfs formally introduce necessary notation deﬁnition vector random variables vector potentials potential assigns conﬁgurations variables real-valued score. also vector real-valued weights. then markov random ﬁeld probability distribution form potentials capture domain behaves assigning higher scores probable conﬁgurations variables. modeler know domain behaves potentials capture might behave learning algorithm weights lead accurate predictions. logic provides excellent formalism deﬁning potentials structured relational domains. sider logical clauses i.e. knowledge base clause disjunction literals literal variable negation drawn variables variable appears indices variables negated written assuming logical knowledge base describing structured domain embed deﬁning potential using corresponding clause assignment variables satisﬁes equal equal preserves structured dependencies described enables much ﬂexible modeling. clauses longer must always hold model express uncertainty diﬀerent possible worlds. weights express strongly model expects corresponding clause hold; higher weight probable true according model. notion embedding weighted logical knowledge bases mrfs appealing one. example markov logic popular formalism induces mrfs weighted ﬁrst-order knowledge bases. given data ﬁrstorder clauses grounded using constants data create propositional clauses propositional clause weight ﬁrst-order clause grounded. weighted ﬁrst-order knowledge base compactly specify entire family mrfs structured machine-learning task. although method easily deﬁning rich structured models wide range problems challenge ﬁnding probable assignment variables i.e. inference np-hard means hope performing tractable inference perform approximately. observe inference deﬁned integer linear program section show convex programming used perform tractable inference mrfs deﬁned weighted knowledge bases. ﬁrst discuss section approach developed goemans williamson views inference instance classic problem relaxes convex program perspective. approach advantage providing strong guarantees quality discrete solutions obtains. however disadvantage general-purpose convex programming toolkits scale well relaxed inference large graphical models section discuss seemingly distinct approach local consistency relaxation complementary advantages disadvantages oﬀers highly scalable message-passing algorithms comes quality guarantees. unite approaches proving solve equivalent optimization problems identical solutions. then section show uniﬁed inference objective also equivalent exact inference knowledge base interpreted using lukasiewicz logic inﬁnite-valued logic reasoning naturally continuous quantities similarity vague fuzzy concepts real-valued data. three interpretations lead inference objective—whether reasoning discrete continuous information—is useful. best knowledge ﬁrst show equivalence. equivalence indicates modeling formalism inference algorithms learning algorithms used reason scalably accurately discrete continuous information structured domains. generalize uniﬁed inference objective section deﬁne hinge-loss mrfs rest paper develop probabilistic programming language algorithms realize goal scalable accurate framework structured data discrete continuous. approach approximating objective relaxation techniques developed randomized algorithms community problem. formally problem boolean assignment variables maximizes total weight satisﬁed clauses knowledge base composed disjunctive clauses annotated nonnegative weights. words objective instance sat. randomized approximation algorithms constructed independently rounding boolean variable true probability then expected weighted satisfaction clause optimizing respect rounding probabilities would give exact solution randomized approach made problem easier goemans williamson showed bound tractable linear program. objective leads expected approximation solution. following method conditional probabilities single boolean assignment achieves least expected score rounding probabilities therefore least solution objective function used obtain them. variable greedily value maximizes expected weight unassigned variables conditioned either possible value previously assigned variables. greedy maximization applied quickly because many models variables participate small fraction clauses making change expectation quick compute variable. speciﬁcally referring deﬁnition assignment needs maximize clauses participates i.e. approximation powerful tractable linear program comes strong guarantees solution quality. however even though tractable generalpurpose convex optimization toolkits scale well large problems. following subsection unify approximation complementary developed probabilistic graphical models community. another approach approximating objective apply relaxation developed markov random ﬁelds called local consistency relaxation approach starts viewing inference equivalent optimization marginal variational formulation inference relaxed optimization ﬁrst-order local polytope vector probability distributions marginal probability state ﬁrst-order local polytope upper bound true objective. much work focused solving ﬁrst-order local consistency relaxation large-scale mrfs discuss section algorithms appealing well-suited sparse dependency structures common mrfs scale large problems. however general solutions fractional guarantees approximation quality tractable discretization fractional solutions. theorem potentials corresponding disjunctive logical clauses associated nonnegative weights ﬁrst-order local consistency relaxation inference equivalent relaxation goemans williamson speciﬁcally partial optimum objective optimum objective vice versa. prove theorem appendix proof analyzes local consistency relaxation derive equivalent compact optimization variable pseudomarginals identical relaxation. theorem signiﬁcant shows rounding guarantees relaxation also apply local consistency relaxation scalable message-passing algorithms developed local consistency relaxation also apply relaxation. previous subsections showed convex program approximate inference discrete logic-based models whether viewed perspective randomized algorithms variational methods. subsection show convex program also used reason naturally continuous information similarity vague fuzzy concepts real-valued data. instead interpreting clauses using boolean logic interpret using lukasiewicz logic extends boolean logic inﬁnite-valued logic propositions take truth values continuous interval extending truth values continuous domain enables represent concepts vague sense often neither completely true completely false. example propositions sensor value high entities similar protein highly expressed captured nuanced manner lukasiewicz logic. also continuous valued represent quantities naturally continuous actual sensor values similarity scores protein expression levels. ability reason continuous values valuable many important applications entirely discrete. identical form relaxed objective therefore deﬁned continuous variables domain logical knowledge base deﬁning potentials interpreted using lukasiewicz logic exact inference identical ﬁnding optimum using uniﬁed relaxed inference objective derived boolean logic previous subsections. result shows equivalence three approaches relaxation local consistency relaxation using lukasiewicz logic. shown speciﬁc family convex programs used reason scalably accurately discrete continuous information. section generalize family deﬁne hinge-loss markov random ﬁelds kind probabilistic graphical model. hl-mrfs retain convexity expressivity convex programs discussed section additionally support even richer space dependencies. begin deﬁne hl-mrfs density functions continuous variables joint domain variables diﬀerent possible interpretations depending application. since generalizing interpretations explored section hl-mrf states viewed rounding probabilities pseudomarginals represent naturally continuous information. generally viewed simply degrees belief conﬁdences rankings possible states; describe discrete continuous mixed domains. application domain typically determines interpretation appropriate. formalisms algorithms described rest paper general respect interpretations. still assuming objective terms deﬁned using weighted knowledge base quickly drop requirement. examine term isolation. observe maximum value unweighted term achieved linear function variables least term satisﬁed whenever occurs. term unsatisﬁed refer distance satisfaction achieving maximum value. also observe rewrite view term relaxed linear constraint easily generalize arbitrary linear constraints. longer require inference objective deﬁned using logical clauses instead term deﬁned using function linear functions capture general dependencies beliefs range values variable take arithmetic relationships among variables. constraint could deﬁned using logical clauses discussed above could deﬁned using knowledge domain. weight indicates important satisfy constraint relative others scaling distance satisfaction. higher weight distance satisfaction penalized. additionally relaxed inference objective admits arbitrary relaxed linear constraints natural also allow hard constraints must satisﬁed times. hard constraints important modeling tools. enable groups variables represent mutually exclusive possibilities multinomial categorical variable functional partial functional relationships. hard constraints also represent background knowledge domain restricting domain regions feasible real world. additionally encode complex model components deﬁning random variable aggregate unobserved variables discuss section think including hard constraints allowing weight take inﬁnite value. again inequality constraints combined represent equality constraint. however introduce inference algorithm hl-mrfs section useful treat hard constraints separately relaxed ones further treat hard inequality constraints separately hard equality constraints. therefore deﬁnition hl-mrfs deﬁne three components separately. objective terms measuring constraint’s distance satisfaction hinge losses. region distance satisfaction angled region distance satisfaction grows linearly away hyperplane loss function useful—as discuss previous section bound expected loss discrete setting among things—but appropriate modeling situations. piecewise-linear loss function makes inference winner take sense preferable fully satisfy highly weighted objective terms completely reducing distance satisfaction terms lower weights. example consider following optimization problem term prefers result indicate ambiguity uncertainty objective terms potentials probabilistic model sometimes preferable result reﬂect conﬂicting preferences. change inference problem smoothly trades satisfying conﬂicting objective terms squaring hinge losses. observe modiﬁed problem mutually exclusive. problems common arise conﬂicting evidence diﬀerent strengths support mutually exclusive possibilities. evidence values could come many sources including base models trained make independent predictions individual random variables domain-specialized similarity therefore complete generalized inference objective allowing either hinge-loss squared hinge-loss functions. users hl-mrfs choice either potential depending appropriate task. formally state full deﬁnition hl-mrfs. deﬁned state solution generalized inference objective proposed previous subsection. state deﬁnition conditional form later convenience deﬁnition fully general since vector conditioning variables empty. deﬁne hl-mrfs placing probability density inputs constrained hinge-loss energy function. note negate hinge-loss energy function states lower energy probable contrast deﬁnition change made later notational convenience. rest paper explore hl-mrfs solve wide range structured machine learning problems. ﬁrst introduce probabilistic programming language makes hl-mrfs easy deﬁne large rich domains. section introduce general-purpose probabilistic programming language probabilistic soft logic allows hl-mrfs easily applied broad range structured machine learning problems deﬁning templates potentials constraints. models structured data often repeated patterns probabilistic dependencies. many examples include strength ties similar people social networks preference triadic closure predicting transitive relationships exactly active constraints functional relationships. often make graphical models easy deﬁne able generalize across diﬀerent data sets repeated dependencies deﬁned using templates. template deﬁnes abstract dependency form potential function constraint along necessary parameters weight potential single value across dependencies deﬁned template. given input data undirected graphical model constructed templates ﬁrst identifying random variables data grounding template introducing potential constraint graphical model subset random variables template applies. program written declarative ﬁrst-order syntax deﬁnes class hl-mrfs parameterized input data. provides natural interface represent hinge-loss potential templates using types rules logical rules arithmetic rules. logical rules based mapping logical clauses hinge-loss potentials introduced section arithmetic rules provide additional syntax deﬁning even wider range hinge-loss potentials hard constraints. subsection deﬁne psl. deﬁnition covers essential functionality supported implementations many extensions possible. syntax describe capture wide range hl-mrfs settings scenarios could motivate development additional syntax make construction diﬀerent kinds hl-mrfs convenient. deﬁnition program rules template hinge-loss potentials hard linear constraints. grounded base ground atoms program induces hl-mrf conditioned speciﬁed observations. constants elements universe discourse. entities attributes. example constant \"person\" denote person constant \"adam\" denote person’s name constant denote person’s age. programs constants written strings double single quotes. constants backslashes escape characters used encode quotes within constants. assumed constants unambiguous i.e. diﬀerent constants refer diﬀerent entities attributes. groups constants represented using variables. deﬁnition predicate relation deﬁned unique identiﬁer positive integer called arity denotes number terms accepts arguments. every predicate program must unique identiﬁer name. refer predicate using identiﬁer arity appended slash. example predicate friends/ binary predicate i.e. taking arguments represents whether constants friends. another example predicate name/ relate person string person’s name. third example predicate enrolledinclass/ relate entities student professor additional attribute subject class. deﬁnition atom predicate combined sequence terms length equal predicate’s arity. sequence called atom’s arguments. atom constants arguments called ground atom. ground atoms basic units reasoning psl. represents unknown observation interest take value example ground atom friends represents whether \"person\" \"person\" friends. atoms ground placeholders sets ground atoms. example atom friends stands ground atoms obtained substituting constants variables already stated deﬁnes templates hinge-loss potentials hard linear constraints grounded data induce hl-mrf. describe data represented provided inputs program. ﬁrst inputs sets predicates closed predicates atoms completely observed open predicates atoms unobserved. third input base ground atoms consideration. atoms must predicate either atoms substituted random variable domain ﬁnal input function maps ground atoms base either observed value symbol indicating unobserved. function valid atoms predicate mapped value. note deﬁnition makes sets redundant sense since derived convenient later explicitly deﬁned. example method specifying psl’s inputs text-based. ﬁrst section text input deﬁnition constants universe grouped types. example universe deﬁnition follows. next section input deﬁnition predicates. predicate includes types constants takes arguments whether closed. example deﬁne predicates advisor-student relationship prediction task follows addition values atoms enrolledinclass predicate could also speciﬁed. ground atom speciﬁed value default observed value predicate closed remain unobserved predicate open. describe text input processed formal inputs first predicate added either based whether annotated tag. then predicate ground atoms predicate added sequence constants arguments created selecting moving also note implementations support predicates atoms deﬁned functionally. predicates thought type closed predicate. observed values deﬁned function arguments. common examples inequality atoms represented shorthand inﬁx operator example following atom value variables replaced diﬀerent constants replaced constant. variables distribution deﬁned rule program applied inputs produces hinge-loss potentials hard linear constraints added hl-mrf. rest subsection describe kinds rules logical rules arithmetic rules. deﬁnition logical rule disjunctive clause literals. logical rules either weighted unweighted. logical rule weighted annotated nonnegative weight optionally power two. therefore formula written implication literal conjunction literals body literal disjunction literals head also valid logical rule equivalent disjunctive clause. kinds logical rules weighted unweighted. weighted logical rule template hinge-loss potential penalizes rule satisﬁed. weighted logical rule begins nonnegative weight optionally ends exponent example weighted logical rule weight induces potentials propagating department membership advisors advisees. unweighted logical rule template hard linear constraint requires rule always satisﬁed. example unweighted logical rule induces hard linear constraints enforcing transitivity friends/ predicate. note period used emphasize rule always enforced disambiguate weighted rules. ground rules rules containing ground atoms. ground rule interpreted either potential hard constraint induced hl-mrf. notational convenience assume without loss generality random variables following description still applies except free variables replaced observations ﬁrst step interpreting ground rule disjunctive clause linear constraint. mapping based uniﬁed inference objective derived section ground rule disjunction literals negated. indices variables correspond atoms negated ground rule expressed disjunctive clause likewise since grounding process uses mapping section logical rules used reason accurately eﬃciently discrete continuous information. convenient method constructing hl-mrfs uniﬁed inference objective weighted logical knowledge bases inference objective. also allow user seamlessly incorporate additional features hl-mrfs squared potentials hard constraints. next introduce even ﬂexible class rules. arithmetic rules general templates hinge-loss potentials hard linear constraints. like logical rules come weighted unweighted variants instead using logical operators arithmetic operators. general arithmetic rule relates linear combinations atoms inequality equality. simple example enforces mutual exclusivity liberal conservative ideologies. make arithmetic rules ﬂexible easy deﬁne additional syntax. ﬁrst generalized deﬁnition atoms substituted sums ground atoms rather single atom. deﬁnition summation atom atom takes terms and/or variables arguments. summation atom represents summations ground atoms obtained substituting individual constants variables summing possible constants variables. ﬁrst argument. note variables used rule i.e. variable rule must unique identiﬁer. summation atoms useful describe dependencies without needing specify number atoms participate. example arithmetic rule deﬁnition ﬁlter clause logical clause deﬁned variable arithmetic rule. logical clause contains atoms predicates appear take arguments constants variables appear arithmetic rule variable deﬁned. filter clauses restrict substitutions variable corresponding arithmetic rule including substitutions clause evaluates true. ﬁlters evaluated using boolean logic. ground atom treated value also supports forms coeﬃcient-deﬁning syntax. ﬁrst form coeﬃcient syntax cardinality function counts number terms substituted variable. cardinality functions enable rules depend number substitutions order scaled correctly averaging. cardinality denoted enclosing variable without pipes. example rule deﬁnes friendliness/ property person social network average strength outgoing friendship links. cases friends/ symmetric extend rule outgoing incoming links follows. second form coeﬃcient syntax built-in coeﬃcient functions. exact supported functions implementation speciﬁc standard functions like maximum minimum included. coeﬃcient functions prepended square brackets instead parentheses distinguish predicates. coeﬃcient functions take either scalars cardinality functions arguments. example following rule matching sets constants requires matched/ atoms minimum sizes sets. note psl’s coeﬃcient syntax also used deﬁne constants example. focused using arithmetic rules deﬁne templates linear constraints also used deﬁne hinge-loss potentials. example following arithmetic rule prefers degree person extroverted exceed average extroversion friends deﬁnition arithmetic rule inequality equality relating linear combinations summation atoms. variable arithmetic rule used once. arithmetic rule annotated ﬁlter clauses subset variables restrict groundings. arithmetic rules either weighted unweighted. arithmetic rule weighted annotated nonnegative weight optionally power two. atoms replaced appropriate summations ground atoms coeﬃcient distributed across summands. leads ground rules arithmetic rule given inputs. arithmetic rule unweighted inequality ground rule algebraically manipulated important question expressivity uses disjunctive clauses positive weights logical rules. logic-based languages support diﬀerent types clauses markov logic networks support clauses conjunctions clauses negative weights. discuss section psl’s logical rules capture general class structural dependencies capable modeling arbitrary probabilistic relationships among boolean variables deﬁned markov logic networks. advantage deﬁnes hl-mrfs much scalable discrete mrfs often accurate show section expressivity tied expressivity problem since class weighted clauses. conditions clauses nonnegative weights disjunctive. ﬁrst consider nonnegativity requirement show actually viewed restriction structure clause. illustrate consider weighted disjunctive clause form clause part generalized problem restrictions weight sign clause structure goal still maximize weights satisﬁed clauses clause could replaced equivalent note clause changed three ways sign weight changed disjunctions replaced conjunctions literals negated. equivalence restriction sign weights subsumed restriction structure clauses. words clauses converted nonnegative weights optimizer might require including conjunctions clauses. also easy verify equation used deﬁne potential discrete replacing potential deﬁned leaves distribution unchanged normalizing partition function. consider requirement clauses disjunctive illustrate conjunctive clauses replaced equivalent disjunctive clauses. idea construct disjunctive clauses assignments variables mapped score constant. simple example replacing conjunction example generalizes procedure encoding boolean disjunctive clauses nonnegative weights. park showed problem discrete bayesian network represented instance sat. distributions bounded factor size problem size polynomial number variables factors distribution. describe boolean represented disjunctive clauses nonnegative weights. given boolean arbitrary potentials deﬁned mappings joint states subsets variables scores created follows. potential original potentials deﬁned disjunctive clauses created. conjunctive clause created corresponding entry potential’s mapping weight equal score assigned weighted potential original mrf. then clauses converted equivalent disjunctive clauses example equations also ﬂipping sign weights negating literals. done entries potentials remains deﬁned disjunctive clauses might negative weights. make weights positive adding suﬃciently large constant weights clauses leaves distribution unchanged normalizing partition function. important note caveats converting arbitrary boolean mrfs mrfs deﬁned using disjunctive clauses nonnegative weights. first number clauses required represent potential original exponential degree potential. practice rarely signiﬁcant limitation since mrfs often contain lowdegree potentials. important point step adding constant weights increases total score state. since bound goemans williamson relative score bound loosened original problem larger constant added weights expected since even approximating np-hard general described general structural dependencies modeled logical rules psl. possible represent arbitrary logical relationships them. process converting general rules psl’s logical rules done automatically made transparent user. elected section deﬁne psl’s logical rules without making conversion automatic make clear underlying formalism. many problems number relations predicted among constants known. binary predicates background knowledge viewed constraints domain range predicate. example might background knowledge entity document exactly label. arithmetic rule express follows. alternatively sometimes ﬁrst argument summed over. example imagine task predicting relationships among students professors. perhaps known student exactly advisor. constraint written follows. finally imagine scenario social networks aligned. goal predict whether pair people network person represented atoms predicate. person aligns person network might align anyone. expressed following arithmetic rules. many variations examples possible. example generalized predicates arguments. additional arguments either ﬁxed summed rule. another example domain range rules incorporate multiple predicates entity participate ﬁxed number relations counted among multiple predicates. many problems require explicitly reasoning similarity rather simply whether entities diﬀerent. example reasoning similarity explored using kernel methods kfoil bases similarity computation relational structure data. continuous variables hl-mrfs make modeling similarity straightforward psl’s support functionally deﬁned predicates makes even easier. example entity resolution task degree entities believed might depend similar names are. rule expressing dependency rule uses similar predicate measure similarity. since functionally deﬁned predicate implemented many diﬀerent possibly domain specialized string similarity functions. similarity function output values range used. potentials deﬁned particular atom equally probable value zero one. often however probable atom value zero unless evidence nonzero value. since atoms typically represent existence entity attribute relation bias promotes sparsity among things inferred exist. further potential prefers atom value least numeric constant reasoning similarities discussed section often also probable atom higher value necessary satisfy potential. accomplish goals simple priors used state atoms values absence evidence overrules priors. prior rule consisting negative literal small weight. example link prediction task imagine preference apply atoms link predicate. prior many tasks number unknowns quickly grow large even modest amounts data. example link prediction task goal predict relations among entities. number possible links grows quadratically number entities handled naively growth could make scaling large data sets diﬃcult problem often handled constructing blocks canopies entities limited subset possible links actually considered. blocking partitions entities links among entities partition element i.e. block considered. alternatively ﬁner grained pruning canopy deﬁned entity entities could possibly link. blocks canopies computed using specialized domain-speciﬁc functions incorporate including atoms bodies rules. since blocks seen special case canopies atom incanopy canopy block not. including incanopy atoms additional conditions bodies logical rules ensure dependencies exist desired entities. another powerful feature ability easily deﬁne aggregates rules deﬁne random variables deterministic functions sets random variables. advantage aggregates used deﬁne dependencies scale magnitude number groundings data. example consider model predicting interests social network. fragment program task follows. rules express belief interests correlated along friendship links social network also certain demographic information predictive speciﬁc interests. question domain expert learning algorithm faces strongly rule weighted relative other. challenge answering question using templates number groundings ﬁrst rule varies person person based number friends groundings second remain constant inconsistent scaling types dependencies makes diﬃcult weights accurately reﬂect relative inﬂuence type dependency across people diﬀerent numbers friends. using aggregate solve problem inconsistent scaling. instead using separate ground rule relate interest friend deﬁne rule grounded person relating average interest across friends person’s interests. fragment approach complex example consider problem determining whether references data refer underlying person. useful feature whether similar sets friends social network. again rule could deﬁned grounded friendship pair would suﬀer scaling issues previous example. instead aggregate directly express similar references’ sets friends are. function measures similarity sets jaccard similarity jaccard similarity nonlinear function meaning cannot used directly without breaking log-concavity hl-mrfs approximate linear function. deﬁne samefriends/ aggregate approximates jaccard similarity deﬁned hl-mrfs language creating them turn algorithms inference learning. ﬁrst task consider maximum posteriori inference problem ﬁnding probable assignment free variables given observations hl-mrfs normalizing function constant exponential maximized minimizing negated argument problem fundamental problem method make predictions weight learning often requires performing inference many times diﬀerent weights here hl-mrfs distinct advantage general discrete models since minimizing convex optimization rather combinatorial one. many oﬀ-the-shelf solutions convex optimization popular interior-point methods worst-case polynomial time complexity number variables potentials constraints although practice perform better worst-case bounds scale well large structured prediction problems therefore introduce algorithm exact inference designed scale large hl-mrfs leveraging sparse connectivity structure potentials hard constraints typical models real-world tasks. potential hard constraint function diﬀerent variables. variables constrained make original problems equivalent. local copy variables used potential function copy used constraint function refer concatenation vectors also introduce characteristic function constraint function constraint satisﬁed inﬁnity not. likewise characteristic function input interval inﬁnity not. drop constraints domain letting range principle instead characteristic functions enforce domain constraints. formulation make computation easier variables correspond problem later decomposed. finally divide problem independent subproblems easier solve using alternating direction method multipliers ﬁrst step form augmented lagrangian function problem. concatenation vectors lagrange describe implement admm block updates updating lagrange multipliers simple step gradient direction updating local copies decomposes potential constraint hl-mrf. although optimization problem convex presence hinge function complicates could solved principle iterative method interior-point method methods would become expensive many admm updates. fortunately reduce problem checking several cases solutions much quickly. optimizer problem correspond three regions solution could region region region check case replacing potential value corresponding region optimizing checking optimizer correct region. check ﬁrst case replacing potential second case replace maximum term inner linear function. optimizer modiﬁed problem found taking gradient objective respect setting gradient equal zero vector solving words optimizer solution equation condition deﬁnes simple system linear equations. coeﬃcient matrix diagonal trivial solve. coeﬃcient matrix symmetric positive deﬁnite system solved cholesky decomposition. optimizer modiﬁed problem i.e. solution equation know solution lies region objective problem whenever αj/ρ modiﬁed term symmetric line therefore reach following third case αj/ρ projection αj/ρ onto hyperplane constraint must active violated optimizers modiﬁed objectives since potential value zero whenever constraint active solving problem reduces projection operation. whether equality inequality constraint solution projection αk+m/ρ feasible deﬁned constraint. equality constraint i.e. optimizer projection y−αk+m/ρ onto hand inequality constraint i.e. cases. first αk+m/ρ solution simply αk+m/ρ. otherwise projection onto update variables solve optimization optimizer state average corresponding local copies added corresponding lagrange multipliers divided step size clipped interval. formally copies local copies corresponding lagrange multiplier then update using algorithm shows complete pseudocode inference. method starts initializing local copies variables appear potential constraint along corresponding lagrange multiplier copy. then convergence iteratively performs updates pseudocode interleaved updates updating lagrange multipliers local copies together subproblem local operations depend variables updated previous iteration. independence reveals another advantage inference algorithm easy parallelize. updates performed parallel results gathered update performed updated broadcast back subproblems. parallelization makes inference algorithm even faster scalable. interesting useful property hl-mrfs always necessary completely materialize distribution order state. consider subset index potentials observe feasible assignment global minimum potential. therefore identify potentials small potentials state perform inference reduced amount time. course identifying hard inference itself iteratively grow starting initial performing inference current adding potentials nonzero values repeating. since lazy inference procedure requires assignment feasible ways handle constraints hl-mrf. include constraints inference problem beginning. strategy ensures feasibility idea lazy grounding also extended constraints improve performance further. check potentials unsatisﬁed i.e. nonzero also check constraints unsatisﬁed i.e. violated. algorithm iteratively grows active potentials active constraints adding unsatisﬁed state hl-mrf deﬁned active potentials constraints also feasible state true hl-mrf. eﬃciency lazy inference improved heuristically adding unsatisﬁed potentials constraints instead adding unsatisﬁed threshold. heuristic decrease computational cost signiﬁcantly although results longer guaranteed correct. bounding resulting error possible important direction future work. section evaluate empirical performance inference algorithm. compare running times mosek commercial convex optimization toolkit uses interior-point methods conﬁrm results yanover ipms scale well large structured-prediction problems show inference algorithm scales much better. fact observe method scales linearly practice number potentials constraints hl-mrf. evaluate scalability generating social networks varying sizes constructing hlmrfs them measuring running time required state. compare algorithm mosek’s ipm. social networks generate designed representative common social-network analysis tasks. generate networks users connected diﬀerent types relationships friendship marriage goal predict political preferences e.g. liberal conservative user. also assume local information user representing features demographic information. plus expected number users zero edges users without edges removed. edge types various parameters represent relationships social networks diﬀerent combinations abundance exclusivity choosing suggested broecheler annotate implement admm java compare mosek encoding entire problem linear program second-order cone program appropriate passing encoded problem java native interface wrapper. experiments performed single machine -core intel core processor ram. optimizer used single thread results averaged runs. ﬁrst evaluate scalability admm solving piecewise-linear problems compare mosek’s interior-point method. figures show results. running time quickly explodes problem size increases. ipm’s average running time largest problem seconds result demonstrates limited scalability interior-point method. contrast admm displays excellent scalability. average running time largest problem seconds. further running time appears grow linearly number potential functions constraints hl-mrf i.e. number subproblems must solved iteration. line best runs sizes coeﬃcient determination combined figure shows admm scales linearly increasing problem size experiment. emphasize implementation admm research code written java commercial package compiled native machine code. evaluate scalability admm solving piecewise-quadratic problem compare mosek. figures show results. again running time interior-point method quickly explodes. test three smallest problems largest took average seconds solve admm scales linearly problem fast quadratic problems linear ones taking average seconds largest problem. advantages ipms great numerical stability accuracy. consensus optimization treats objective terms constraints subproblems often returns solutions optimal feasible moderate precision non-trivially constrained problems although often acceptable quantify infeasibility suboptimality repairing infeasibility measuring resulting total suboptimality. ﬁrst project solutions returned consensus optimization onto feasible region took negligible amount computational time. padmm value objective problem point pipm value objective solution returned ipm. relative error problem /pipm. relative error consistently small; varied section present three weight learning methods hl-mrfs diﬀerent objective function. ﬁrst method approximately maximizes likelihood training data. second method maximizes pseudolikelihood. third method ﬁnds largemargin solution preferring weights discriminate ground truth nearby states. since weights often shared among many potentials deﬁned template groundings rule describe learning algorithms terms templated hl-mrfs. introduce necessary notation hl-mrf templates. expectation distribution deﬁned smoother ascent often helpful divide q-th component gradient number groundings |tq| q-th template experiments. computing expectation intractable common approximation values potentials probable setting current parameters i.e. state. using state makes learning approach structured variant voted perceptron expect best space explored distributions relatively entropy. following voted perceptron take steps ﬁxed length direction gradient average points steps. step outside feasible region projected back continuing. computing pseudolikelihood gradient require joint inference takes time linear size however integral expectation readily admit closed-form antiderivative approximate expectation. variable unconstrained domain integration one-dimensional interval real number line monte carlo integration quickly converges accurate estimate expectation. also apply mple constraints interdependent. example linear equality constraints disjoint groups variables block-sample constrained variables sampling uniformly simplex. types constraints often used represent categorical labels. compute accurate estimates quickly blocks typically low-dimensional. diﬀerent approach learning drops probabilistic interpretation model views hl-mrf inference prediction function. large-margin estimation shifts goal learning producing accurate probabilistic models instead producing accurate predictions. learning task weights separate ground truth nearby states large margin. describe section intuition behind large-margin structured prediction ground-truth state energy lower alternate state large margin. setting output space continuous parameterize margin criterion continuous loss function. valid output state large-margin solution satisfy loss function measures disagreement state training label state common assumption loss function decomposes work distance ˜yi. since expect problems user-speciﬁed parameter. formulation analogous margin-rescaling approach joachims though structured objective natural intuitive number constraints cardinality output space inﬁnite. following approach optimize subject inﬁnite constraint using cutting-plane algorithm greedily grow constraints iteratively adding worst-violated constraint given separation oracle updating subject current constraints. goal cutting-plane approach eﬃciently active constraints solution full objective without enumerate inﬁnite inactive constraints. worst-violated constraint hinge-losses thus adding simply creates augmented hl-mrf. worstviolated constraint computed standard inference loss-augmented hlmrf. however ground truth values interior cause distance-based loss concave require separation oracle solve non-convex objective. case diﬀerence convex functions algorithm local optimum. since concave portion loss-augmented inference objective pivots around value greater ground truth. simply choose initial direction interior labels rounding direction subgradients variables whose solution states interval corresponding subgradient direction convergence. iteratively invoke separation oracle worst-violated constraint. constraint violated violation within numerical tolerance found max-margin solution. otherwise constraint repeat. fact note large-margin criterion always requires slack hlmrfs squared potentials. since squared hinge potential quadratic loss linear always exists small enough distance ground truth absolute distance greater squared distance. cases slack parameter trades peakedness learned quadratic energy function margin criterion. demonstrate ﬂexibility eﬀectiveness learning hl-mrfs test four diverse tasks node labeling link labeling link prediction image completion. experiments represents problem domain best solved structuredprediction approaches dependencies highly structural. experiments show hl-mrfs perform well better canonical approaches. diverse tasks compare number competing methods. node link labeling compare hl-mrfs discrete markov random ﬁelds construct markov logic networks template discrete mrfs using logical rules similarly psl. perform inference discrete mrfs using gibbs sampling approximate states learning using search algorithm maxwalksat link prediction preference prediction task inherently continuous nontrivial encode discrete logic compare bayesian probabilistic matrix factorization finally image completion experimental setup poon domingos compare results report include tests using product networks deep belief networks deep boltzmann machines train hl-mrfs discrete mrfs three learning methods structured perceptron maximum pseudolikelihood estimation large-margin estimation appropriate evaluate statistical signiﬁcance using paired t-test rejection threshold describe hl-mrfs used experiments using rules deﬁne them. investigate diﬀerences linear squared potentials experiments. hl-mrf-l refers model linear potentials hl-mrf-q squared potentials. training mple gradient steps step size average iterates voted perceptron. experimented various settings scores hl-mrfs discrete mrfs sensitive changes. classifying documents links documents—such hyperlinks citations shared authorship—provide extra signal beyond local features individual documents. collectively predicting document classes links tends improve accuracy classify documents citation networks using data cora citeseer scientiﬁc paper repositories. cora data contains papers seven categories directed citation links. citeseer data contains papers categories directed citation links. predicate category/ represent category document cites/ represent citation document another. prediction task given seed documents whose labels observed infer remaining document classes propagating seed information network. runs split data sets training testing partitions seed half set. predict discrete categories hl-mrfs predict category highest predicted value. compare hl-mrfs discrete mrfs task. prediction performed rounds gibbs sampling discarded burn-in. construct using logical rules simply encode tendency class propagate across citations. category following rules direction citation. table lists results experiment. hl-mrfs accurate predictors data sets. variants hl-mrfs also much faster discrete mrfs. table average inference times folds. emerging problem analysis online social networks task inferring level trust individuals. predicting strength trust relationships provide useful information viral marketing recommendation engines internet security. hlmrfs linear potentials applied huang task showing superior results models based sociological theory. reproduce experimental setup using sample signed epinions trust network orginally collected richardson users indicate whether trust distrust users. perform eight-fold cross-validation. fold prediction algorithm observes entire unsigned social network trust ratings. measure prediction accuracy held-out sampled network contains users signed links. links positive negative making sparse prediction task. table average area precision-recall curves social-trust prediction hl-mrfs discrete mrfs. scores statistically equivalent best scoring method metric typed bold. model based social theory structural balance suggests social structures governed system prefers triangles considered balanced. balanced triangles number positive trust relationships; thus considering possible directions links form triad users sixteen logical implications following form. since expect structural implications vary accuracy learning weights rules provides better models. again rules deﬁne hl-mrfs discrete mrfs train using various learning algorithms. inference discrete mrfs perform rounds gibbs sampling ﬁrst burn-in. compute three metrics area receiver operating characteristic curve areas precision-recall curves positive trust negative trust. three metrics hl-mrfs squared potentials score signiﬁcantly higher. diﬀerences among learning methods squared hl-mrfs insigniﬁcant diﬀerences among models statistically signiﬁcant metric. area precision-recall curve positive trust discrete mrfs trained statistically tied best score hl-mrf-l discrete mrfs trained statistically tied best area precision-recall curve negative trust. results listed table though random fold splits same using experimental setup huang also scored precision-recall area negative trust standard trust prediction algorithms eigentrust tidaltrust scored respectively. logical models based structural balance signiﬁcantly accurate hl-mrfs discrete mrfs. addition comparing favorably regard predictive accuracy inference hlmrfs also much faster discrete mrfs. table lists average inference times folds three prediction tasks cora citeseer epinions. illustrates important diﬀerence performing structured prediction convex inference versus sampling discrete prediction space convex inference much faster. preference prediction task inferring user attitudes toward items. problem naturally structured since user’s preferences often interdependent item’s ratings. collaborative ﬁltering task predicting unknown ratings using subset observed ratings. methods task range simple nearest-neighbor classiﬁers complex latent factor models. generally problem instance link prediction since goal predict links indicating preference users content. since preferences ordered rather boolean natural represent continuous variables hl-mrfs higher values indicating greater preference. illustrate versatility hl-mrfs design simple interpretable collaborative ﬁltering model predicting humor preferences. test model jester dataset repository ratings users normalize sample random users rated jokes split train test users. train test matrix sample random observed features remaining ratings treated variables jokes user; predicate likes/ indicates degree preference simrating/ closed predicate measures mean-adjusted cosine similarity observed ratings jokes. also include following rules enforce likes concentrates around observed average rating user item global average atom avgrating takes placeholder constant argument since grounding entire hl-mrf. again three predicates closed computed using averages observed ratings. cases observed ratings taken training data learning test data testing. compare hl-mrf model canonical latent factor model bayesian probabilistic matrix factorization bpmf fully bayesian treatment therefore considered parameter-free; parameter must speciﬁed rank decomposition. based settings used xiong table mean squared errors pixel image completion. hl-mrfs produce accurate completions caltech left-half olivetti faces sumproduct networks produce better completions olivetti bottom-half faces. scores methods reported poon domingos rank decomposition iterations burn iterations sampling. experiments code xiong since bpmf train model allow bpmf training matrix prediction phase. table lists normalized mean squared error normalized mean absolute error averaged random splits. though bpmf produces best scores improvement hl-mrf-l signiﬁcant nmae. digital image completion requires models understand pixels relate other pixels unobserved model infer values parts image observed. construct pixel-grid hl-mrfs image completion. test models using experimental setup poon domingos reconstruct images olivetti face data caltech face category. olivetti data contains images pixels wide tall caltech face category contains examples faces crop center patch done poon figure example results image completion caltech olivetti faces. left right column true face left side predictions hlmrfs spns bottom half predictions hl-mrfs spns. completions downloaded poon domingos bright normalized brightness pixel image north indicates north neighbor ij\". similarly include analogous rules south east west neighbors well pixels mirrored across horizontal vertical axes. setup results rules pixel which image produces rules. train hl-mrfs using step size ﬁrst images data test last ﬁfty. training maximize data log-likelihood uniformly random held-out pixels training image allowing generalization throughout image. table lists results others reported poon domingos sumproduct networks deep boltzmann machines deep belief networks principal component analysis nearest neighbor hl-mrfs produce best mean squared error leftbottom-half settings caltech left-half setting olivetti set. product networks produce lower error olivetti bottom-half faces. reconstructed faces displayed figure shallow pixel-based hl-mrfs produce comparably convincing images sumproduct networks especially left-half setting hl-mrfs learn pixels likely mimic horizontal mirror. neither method particularly good reconstructing bottom half faces qualitative diﬀerence deep shallow hl-mrf completions spns seem hallucinate diﬀerent faces often artifacts hl-mrfs predict blurry shapes roughly pixel intensity observed half face. tendency better match pixel intensity helps hl-mrfs score better quantitatively caltech faces lighting conditions varied olivetti faces. experiments training model takes minutes -core machine predicting takes second image. poon domingos report faster training spns hl-mrfs spns clearly belong class faster models compared dbns dbms take days train modern hardware. researchers artiﬁcial intelligence machine learning long interested predicting interdependent unknowns using structural dependencies. earliest work area inductive logic programming structural dependencies described ﬁrst-order logic. using ﬁrst-order logic several advantages. first capture many types dependencies among variables correlations anti-correlations implications. second compactly specify dependencies hold across many diﬀerent sets propositions using variables wildcards match entities data. features enable construction intuitive general-purpose models easily applicable adapted diﬀerent domains. inference ﬁnds propositions satisfy query consistent relational knowledge base. however limited diﬃculty coping uncertainty. standard approaches model dependencies hold universally dependencies rare real-world data. another broad area research probabilistic methods directly models uncertainty unknowns. probabilistic graphical models family formalisms specifying joint distributions interdependent unknowns graphical structures. graphical structure generally represents conditional independence relationships among random variables. explicitly representing conditional independence relationships allows distribution compactly parametrized. example worst case discrete distribution could represented exponentially large table joint assignments random variables. however describing distribution smaller conditionally independent pieces much compact. similar beneﬁts apply continuous distributions. algorithms probabilistic inference learning also operate conditionally independent pieces described graph structure. therefore straightforward apply wide variety distributions. categories pgms include markov random ﬁelds bayesian networks recently researchers sought combine advantages relational probabilistic approaches creating ﬁeld statistical relational learning techniques build probabilistic models relational data i.e. data composed entities relationships connecting them. relational data often described using relational calculus techniques also equally applicable similar categories data names graph data network data. modeling relational data inherently complicated large number interconnected overlapping structural dependencies typically present. complication motivated directions work. ﬁrst direction algorithmic seeking inference learning methods scale high dimensional models. direction user-oriented and—as growing body evidence shows—supported learning theory seeking formalisms compactly specifying entire groups dependencies model share form parameters. specifying grouped dependencies often form templates domain-speciﬁc language convenient users. often relational data structural dependencies hold without regard identities entities instead induced entity’s class structure relationships entities. therefore many models languages give users ability specify dependencies abstract form ground models speciﬁc data sets based deﬁnitions. addition convenience recent work learning theory says repeated dependencies tied parameters generalizing few—or even one—large structured training example related ﬁeld structured prediction generalizes tasks classiﬁcation regression task predicting structured objects. loss function used learning generalized taskappropriate loss function scores disagreement predictions true structures. often models structured prediction take form energy functions linear parameters. therefore prediction models equivalent inference mrfs. distinct branch learn-to-search methods problem decomposed series one-dimension prediction problems. challenge learn good order predict components structure onedimension prediction problem conditioned useful information. examples learn-to-search methods include incremental structured perceptron searn dagger aggrevate paper focus methods perform joint prediction directly. better understanding diﬀerences relative advantages joint-prediction methods learnto-search methods important direction future work. rest section survey models domain-speciﬁc languages inference methods learning methods encompass many approaches. broad area work—of part—uses ﬁrst-order logic relational formalisms specify templates pgms. probabilistic relational models deﬁne templates terms database schema grounded instances schema create bns. relational dependency networks template using structured query language queries relational schema. markov logic networks ﬁrst-order logic deﬁne boolean mrfs. logical clause ﬁrst-order knowledge base template potentials grounded propositions. whether proposition true boolean random variable potential value corresponding ground clause satisﬁed propositions zero not. clauses either weighted case potential weight clause templated unweighted case must hold universally ilp. ways mlns similar psl. whereas mlns deﬁned boolean variables templating language hl-mrfs deﬁned continuous variables. however continuous variables used model discrete quantities. section information relationships hl-mrfs discrete mrfs section empirical comparisons two. show hl-mrfs scale much better retaining rich expressivity accuracy discrete counterparts. addition hl-mrfs reason directly continuous data. part broad family probabilistic programming languages goals probabilistic programming often overlap. probabilistic programming seeks make constructing probabilistic models easy user separate model speciﬁcation development inference learning algorithms. algorithms developed entire space models covered language easy users experiment including excluding diﬀerent model components. also makes easy existing models beneﬁt improved algorithms. separation model speciﬁcation algorithms also useful reasons. paper emphasize designing algorithms ﬂexible enough support full class hl-mrfs. examples probabilistic programming languages include ibal blog markov logic problog church figaro factorie anglican edward formalisms also proposed probabilistic reasoning continuous domains domains equipped semirings. hybrid markov logic networks discrete continuous variables. addition dependencies discrete variables supported mlns support soft equality constraints variables form deﬁned squared arithmetic rules well hybrid mlns intractable. wang domingos propose random walk algorithm approximate inference. another related formalism aproblog generalizes problog allow clauses annotated elements semiring generalizing problog’s support clauses annotated probabilities. many common inference tasks generalized perspective algebraic model counting pita system probabilistic logic programming also viewe implementing inference various semirings. whether viewed inference without probabilistic semantics searching structured space optimal prediction important diﬃcult task. np-hard general much work focused approximations identifying classes problems tractable. well-studied approximation technique local consistency relaxation inference ﬁrst viewed equivalent optimization realizable expected values potentials called marginal polytope. variables discrete potential indicator subset variables certain state optimization becomes linear program. variable program marginal probability variable particular state variables associated potential particular joint state. marginal polytope marginal probabilities globally consistent. number linear constraints required deﬁne marginal polytope exponential size problem however linear program relaxed order tractable. local consistency relaxation marginal polytope relaxed local polytope marginals variables potential states locally consistent sense marginal potential states sums marginal distributions associated variables. large body work focused solving objective quickly. typically oﬀ-the-shelf convex optimization methods scale well large graphical models structured predictors large branch research investigated highly scalable message-passing algorithms. approach dual decomposition solves problem dual objective. many algorithms coordinate descent trw-s mplp adlp algorithms subgradient-based approaches another approach solving objective uses message-passing algorithms solve problem directly primal form. well-known algorithm ravikumar uses proximal optimization general approach iteratively improves solution searching nearby improvements. authors also provide rounding guarantees relaxed solution integral i.e. relaxation tight allowing algorithm converge faster. another message-passing algorithm solves primal objective uses alternating direction method multipliers optimizes objective binary pairwise mrfs supports addition certain deterministic constraints variables. third example primal message-passing algorithm aplp primal analog adlp. like uses admm optimize objective. approaches approximate inference include tighter linear programming relaxations tighter relaxations enforce local consistency variable subsets larger individual variables makes higher-order local consistency relaxations. mezuman developed techniques special cases higher-order relaxations contains cardinality potentials probability conﬁguration depends number variables particular state. researchers also explored nonlinear convex programming relaxations e.g. ravikumar laﬀerty kumar previous analyses identiﬁed particular subclasses whose local consistency relaxations tight i.e. maximum relaxed program exactly maximum original problem. special classes include graphical models tree-structured dependencies models submodular potential functions models encoding bipartite matching problems nand potentials perfect graph structures researchers also studied performance guarantees subclasses ﬁrst-order local consistency relaxation. kleinberg tardos chekuri considered metric labeling problem. feldman used local consistency relaxation decode binary linear codes. paper examine classic problem sat—ﬁnding joint boolean assignment propositions maximizes weighted clauses satisﬁed—as instance researchers also considered approaches solving study randomized algorithm goemans williamson line work focusing convex programming relaxations obtained stronger rounding guarantees goemans williamson using nonlinear programming e.g. asano williamson references therein. work probabilistic method instead searches discrete solutions directly e.g. mills tsang larrosa choi note approach shang essentially type formulated sat. recent approach blends convex programming discrete search mixed integer programming additionally huynh mooney introduced linear programming relaxation mlns inspired relaxations relaxation general markov logic provides known guarantees quality solutions. finally lifted inference takes advantage symmetries probability distributions reduce amount work required inference. earliest approaches identiﬁed repeated dependency structures pgms avoid repeated computations lifted inference widely applied templates commonly used deﬁne pgms often induce symmetries. various inference techniques discrete mrfs extended lifted approach including belief propagation gibbs sampling approaches lifted convex optimization might extended hl-mrfs. salvo braz kersting kimmig information lifted inference. taskar connected pgms showing train mrfs largemargin estimation generalization large-margin objective binary classiﬁcation used train support vector machines large-margin learning wellstudied approach train structured predictors directly incorporates structured loss function convex upper bound true objective regularized expected risk. learning objective parameters smallest norm linear combination feature functions assign better score training data possible predictions. amount score correct prediction must exceed score predictions scaled using structured loss function. objective therefore encoded norm minimization problem subject many linear constraints possible prediction structured space. structured svms extend large-margin estimation broad class structured predictors admit tractable cutting-plane learning algorithm. algorithm terminate number iterations linear size problem computational challenge large-margin learning structured prediction comes task ﬁnding violated constraint learning objective. accomplished optimizing energy function plus loss function. words task structure best combination favored energy function unfavored loss function. often loss function decomposes components prediction space combined energy function loss function often viewed simply energy function another structured predictor equally challenging easy optimize space structures discrete vectors loss function hamming distance. common large-margin estimation setting parameters predict training data without error. case training data said separable generalizing notion linear separability feature space binary classiﬁcation. solution problem slack variables constraints require training data assigned best score. magnitude slack variables penalized learning objective estimation must trade norm parameters violating constraints. joachims extend formulation slack formulation single slack variable used constraints across training examples eﬃcient. framework large-margin estimation hl-mrfs section repeated inferences required large-margin learning most-violated constraint iteration become computationally expensive. therefore researchers explored speeding learning interleaving inference problem learning problem. cutting-plane formulation discussed above objective equivalently saddle-point problem solution minimum respect parameters maximum respect inference variables. taskar proposed dualizing inner inference problem form joint minimization. problems tight duality i.e. dual problem optimal value primal problem approach leads equivalent convex optimization solved variables words learning most-violated constraint problems simultaneously. solved simultaneously greatly reducing training time. problems non-tight duality gaps e.g. inference general discrete mrfs meshi showed principle applied using approximate inference algorithms like dual decomposition bound primal objective. related problem parameter learning structure learning i.e. identifying accurate dependency structure model. common approach searching space templates pgms. probabilistic relational models friedman learned structures described vocabulary relational schemas. models templated ﬁrst-order-logic-like languages mlns approaches take form rule learning. based rule-learning techniques inductive logic programming series approaches sought learn rules relational data. initially domingos learned rules generating candidates performing beam search identify rules improved weighted pseudolikelihood objective. then mihalkova mooney observed previous approach generated candidate rules without regard data introduced approach used data guide proposal rules relational pathﬁnding. domingos improved ﬁrst performing graph clustering common motifs common subgraphs guide rule proposal. observed modifying rule clause time often stuck poor local optima using motifs reﬁnement operators instead able converge better optima. approaches structure learning search directly grounded pgms including -regularized pseudolikelihood maximization grafting methods extended hl-mrfs psl. paper introduced hl-mrfs class probabilistic graphical models unite generalize several approaches modeling relational structured data boolean logic probabilistic graphical models fuzzy logic. hl-mrfs capture relaxed probabilistic inference boolean logic exact probabilistic inference fuzzy logic making useful models discrete continuous data. hl-mrfs also generalize inference techniques additional expressivity allowing even ﬂexibility. hl-mrfs signiﬁcant addition library machine learning tools embody useful point spectrum models trade scalability expressivity. showed easily applied wide range structured problems machine learning achieve high-quality predictive performance competitive surpassing performance canonical approaches. however models either scale well like discrete mrfs versatile ability capture wide range problems like bayesian probabilistic matrix factorization. also introduced probabilistic programming language hl-mrfs. makes hl-mrfs easy design allowing users encode ideas structural dependencies using intuitive syntax based ﬁrst-order logic. also helps accelerate time-consuming aspect modeling process reﬁning model. contrast types models require specialized inference learning algorithms depending structural dependencies included hl-mrfs encode many types dependencies scale well inference learning algorithms. makes easy quickly remove modify dependencies model rerun inference learning allowing users quickly improve quality models. finally uses ﬁrst-order syntax program actually speciﬁes entire class hlmrfs parameterized particular data grounded. therefore model components model reﬁned data easily applied others. next introduced inference learning algorithms scale large problems. inference algorithm scalable standard tools convex optimization leverages sparsity common dependencies structured prediction. supervised learning algorithms extend standard learning objectives hlmrfs. together combination expressive formalism user-friendly probabilistic programming language highly scalable algorithms enables researchers practitioners easily build large-scale accurate models relational structured data. paper also lays foundation many lines future work. analysis local consistency relaxation hierarchical optimization general proof technique could used derive compact forms objectives. case mrfs deﬁned using logical clauses compact forms simplify analysis could lead greater understanding classes mrfs. another important line work understanding guarantees apply states hl-mrfs. anything said ability approximate inference discrete models beyond models already covered known rounding guarantees? future directions also include developing algorithms hl-mrfs. important direction marginal inference hl-mrfs algorithms sampling them. unlike marginal inference discrete distributions computes marginal probability variable particular state marginal inference hl-mrfs requires ﬁnding marginal probability variable particular range. option well generating samples hl-mrfs extend hit-and-run sampling scheme broecheler getoor method developed continuous constrained mrfs piecewise-linear potentials. also many domains hl-mrfs applied. modeling tools researchers design apply solutions structured prediction problems. acknowledge many people contributed development hl-mrfs psl. contributors include eriq augustine shobeir fakhraei james foulds angelika kimmig stanley london miao lilyana mihalkova dianne o’leary pujara arti ramesh theodoros rekatsinas v.s. subrahmanian. work supported grants iarpa doi/nbc contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors appendix prove equivalence objectives proof analyzes local consistency relaxation derive equivalent compact optimization variable pseudomarginals identical relaxation. since variables boolean refer pseudomarginal simply denote unique setting begin reformulating local consistency relaxation hierarchical optimization ﬁrst variable pseudomarginals factor pseudomarginals structure local polytope pseudomarginals parameterize inner linear programs decompose structure that—given ﬁxed µ—there independent linear program clause rewrite objective straightforward verify objectives equivalent mrfs disjunctive clauses potentials. constraints deﬁning derived constraint constraints deﬁnition ˆφj. omitted redundant make optimization compact replace inner linear program expression gives optimal value setting deriving expression guaranteed exist requires reasoning maximizer problem bounded feasible parameters proof prove lemma karush-kuhn-tucker conditions since problem maximization linear function subject linear constraints conditions necessary suﬃcient optimum writing relevant conditions introduce necessary notation. state need reason variables disagree unsatisﬁed state theorem potentials corresponding disjunctive logical clauses associated nonnegative weights ﬁrst-order local consistency relaxation inference equivalent relaxation goemans williamson speciﬁcally partial optimum objective optimum objective vice versa. proof substituting solution inner optimization lemma local consistency relaxation objective gives projected optimization identical relaxation objective", "year": 2015}