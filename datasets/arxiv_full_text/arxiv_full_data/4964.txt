{"title": "Knowledge Base Completion: Baselines Strike Back", "tag": ["cs.LG", "cs.AI"], "abstract": "Many papers have been published on the knowledge base completion task in the past few years. Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18. This paper shows that the accuracy of almost all models published on the FB15k can be outperformed by an appropriately tuned baseline - our reimplementation of the DistMult model. Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyper-parameter tuning or different training objectives. This should prompt future research to re-consider how the performance of models is evaluated and reported.", "text": "edge graph) speciﬁed triplets head tail entities respectively relation them. entity task predict either tail entity given query predict head entity given task useful test generic ability system reason knowledge base also expanding existing incomplete knowledge bases deducing entries existing ones. extensive amount work published task plain list citations table among distmult simplest. still paper shows even simple model proper hyperparameters training objective evaluated using standard metric hits outperform models evaluated standard datasets suggests huge space improvement hyper-parameter tuning even complex models many ways better suited relational learning e.g. capture directed relations. inspired success word embeddings natural language processing distributional models recently extensively studied. distributional models represent entities sometimes even relations n-dimensional real vectors denote vectors bold font many papers published knowledge base completion task past years. introduce novel architectures relation learning evaluated standard datasets paper shows accuracy almost models published outperformed appropriately tuned baseline reimplementation distmult model. ﬁndings cast doubt claim performance improvements recent models architectural changes opposed hyperparameter tuning different training objectives. prompt future research re-consider performance models evaluated reported. projects wikidata earlier freebase successfully accumulated formidable amount knowledge form entity relation entity triplets. given vast body knowledge would extremely useful teach machines reason knowledge bases. possible test reasoning knowledge base completion true triplets. head queries handled analogically. note softmax normalization suitable ﬁltered protocol since exactly correct triplet guaranteed among candidates. preliminary experiments varied batch size embedding dimensionality number negative samples training regularization parameter learning rate based experiments ﬁxed lr=. decided focus inﬂuence batch size embedding dimension number negative samples. ﬁnal experiments trained several models hyperparameter range train ﬁnal models using adam optimizer also performed limited experiments adagrad adadelta plain sgd. adagrad usually required substantially iterations adam achieve performance. failed obtain competitive performance adadelta plain sgd. validation datasets best hyper-parameter combinations respectively. note tried substantially hyperparameter combinations unlike previous works normalize neither entity relation embeddings. prevent over-ﬁtting stop training hits stop improving validation set. dataset keras based implementation tensorflow backend needed hours converge single geforce gpu. results. besides single models also evaluated performance simple ensemble averages predictions multiple models. technique consistently improves performance machine learning models many domains slightly improved results also case. results experiments together previous results literature shown table distmult proper hyperparameters twice achieves second best score third best score three four commonly reported benchmarks distmult model introduced yang subsequently toutanova chen achieved better empirical results model changing hyper-parameters training procedure using negative-log likelihood softmax instead l-based max-margin ranking loss. trouillon obtained even better empirical result dataset changing distmult’s hyper-parameters. method. evaluation ﬁltered evaluation protocol proposed bordes training validation transform triplet examples tail query head query train model minimizing negative log-likelihood ground truth triplet randomly sampled pool negative triplets ﬁltered protocol rank validation test triplet corrupted triplets appear train valid test dataset formally query correct answer compute rank candidate rain alid sets dataset model together r-gcn+ shows better hits. however distmult performs poorly. even though distmult’s inability model asymmetric relations still allows achieve competitive results hits metrics clearly show limitations. results highlight qualitative differences datasets. interestingly recently published models input outperform models utilize richer features text knowledge base path information. shows possible future improvement. another interesting observation batch size strong inﬂuence ﬁnal performance. larger batch size always lead better results instance hits improved absolute batch size increased figure details. compared previous works trained distmult datasets different training objective yang trouillon optimized margin objective softplus activation function respectively. similarly toutanova chen softmax function however adam optimizer instead rprop simple conclusions work increasing batch size dramatically improves performance distmult raises question whether models would also signiﬁcantly beneﬁt similar hyper-parameter tuning different training objectives; future might better focus metrics less frequently used domain like hits since instance many models achieve similar high hits however even models competitive hits underperform hits case distmult implementation. research focus recently centred ﬁltered scenario decided study. advantage easy evaluate. however scenario trains model expect single correct answer among candidates unrealistic context knowledge bases. hence unstructured transe transh transr ctransr transd lpptransd transparse tatec hole stranse complex proje wlistwise rtranse ptranse gake gaifman hiri r-gcn+ nlfeat teke distmult distmult distmult single distmult ensemble distmult table entity prediction results. denote evaluation metrics mean rank hits mean reciprocal rank respectively. three best results metric bold. additionally best result underlined. ﬁrst group lists models trained knowledge base additional input besides source entity relation. second group shows models path information e.g. consider paths source target entities additional features. models third group trained additional textual data. last group list various implementations distmult model including implementation last lines. since distmult additional features results compared models ﬁrst group. nlfeat abbreviates node+linkfeat model results listed table taken yang table adapted preliminary work small contribution ongoing discussion machine learning community current strong focus state-of-the-art empirical results might sometimes questionable whether achieved better model/algorithm extensive hyper-parameter search. broader discussion light results think ﬁeld would beneﬁt large-scale empirical comparative study different algorithms similar recent study word embedding models references martin abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing lukasz kaiser manjunath kudlur josh levenberg rajat monga sherry moore derek murray shlens benoit steiner ilya sutskever paul tucker vincent vanhoucke vijay vasudevan oriol vinyals pete warden martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous distributed systems kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database proceedstructuring human knowledge. ings sigmod international conference management data. york sigmod pages https//doi.org/./.. antoine bordes nicolas usunier alberto garciaduran jason weston oksana yakhnenko. translating embeddings modeling multirelational data. burges bottou welling ghahramani weinberger editors advances neural information processing systems curran associates inc. pages http//papers.nips.cc/paper/-translatingembeddings-for-modeling-multi-relational-data.pdf. feng minlie huang yang yang xiaoyan zhu. gake graph aware knowledge emproceedings international bedding. conference computational linguistics pages alberto garc´ıa-dur´an antoine bordes nicocomposing relationships usunier. conference empirtranslations. ical methods natural language processing lisbonne portugal pages https//doi.org/./v/d-. alberto garcia-duran antoine bordes nicolas usunier yves grandvalet. combining three-way embeddings models journal link prediction knowledge bases. artiﬁcial intelligence research https//doi.org/./jair.. shizhu kang guoliang zhao. learning represent knowledge graphs gaussian embedding. cikm proceedings international conference information knowledge management pages https//doi.org/./.. guoliang shizhu liheng kang zhao. knowledge graph embedding proceedings dynamic mapping matrix. annual meeting association computational linguistics international joint conference natural language processing pages http//www.aclweb.org/anthology/p-. michael schlichtkrull thomas kipf peter bloem rianne berg ivan titov welling. modeling relational data graph convolutional networks http//arxiv.org/abs/.. omer levy yoav goldberg dagan. improving distributional similarity lessons transactions learned word embeddings. association computational linguistics https//doi.org/./--s-s. yankai zhiyuan maosong yang xuan zhu. learning entity relation embeddings knowledge graph completion. proceedings twenty-ninth aaai conference artiﬁcial intelligence learning pages qiao liuyi jiang minghao zhiguang qin. hierarchical random walk inference knowledge graphs. proceedings international sigir conference research development information retrieval. pages quoc nguyen kairit sirts lizhen mark johnson. stranse novel embedding model entities relationships knowledge bases. proceedings conference north american chapter association computational linguistics human language technologies pages https//doi.org/./v/n. maximilian nickel kevin murphy volker tresp evgeniy gabrilovich. review relational machine learning knowledge ieee graph. https//doi.org/./jproc... martin riedmiller heinrich braun. direct adaptive method faster backpropagation learnneural networks rprop algorithm. ieee international conference ieee pages yelong shen po-sen huang ming-wei chang jianfeng gao. implicit reasonet modeling large-scale structured relationships shared memory. arxiv preprint arxiv. richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. proceedings advances neural information processing systems kristina toutanova danqi chen. observed versus latent features knowledge base text inference. proceedings workshop continuous vector space models compositionality pages th´eo trouillon johannes welbl sebastian riedel eric gaussier guillaume bouchard. complex embeddings simple link predicicml tion. http//arxiv.org/pdf/.v.pdf. zhen wang jianwen zhang jianlin feng zheng chen. knowledge graph embedding translating hyperplanes. aaai conference artiﬁcial intelligence pages zhigang wang juanzi text-enhanced representation learning knowledge graph. proceedings twenty-fifth international joint conference artiﬁcial intelligence. aaai press pages xiao minlie huang xiaoyan zhu. semantic space projection knowledge graph embedding text descriptions. proceedings aaai conference artiﬁcial intelligence. hee-geun yoon hyun-je song seong-bae park se-young park. translation-based knowledge graph embedding preserving logical property relations. naacl pages", "year": 2017}