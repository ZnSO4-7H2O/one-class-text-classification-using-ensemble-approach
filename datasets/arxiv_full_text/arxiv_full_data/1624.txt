{"title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Modeling the structure of coherent texts is a key NLP problem. The task of coherently organizing a given set of sentences has been commonly used to build and evaluate models that understand such structure. We propose an end-to-end unsupervised deep learning approach based on the set-to-sequence framework to address this problem. Our model strongly outperforms prior methods in the order discrimination task and a novel task of ordering abstracts from scientific articles. Furthermore, our work shows that useful text representations can be obtained by learning to order sentences. Visualizing the learned sentence representations shows that the model captures high-level logical structure in paragraphs. Our representations perform comparably to state-of-the-art pre-training methods on sentence similarity and paraphrase detection tasks.", "text": "exploiting unlabelled corpora learn semantic representations data become active area investigation. self-supervised learning typical approach uses information naturally available part data supervisory signals noroozi favaro attempt learn visual representations solving image jigsaw puzzles. sentence ordering considered jigsaw puzzle language domain interesting question whether learn useful textual representations performing task. approach coherence modeling driven recent success capturing semantics using distributed representations modeling sequences using recurrent neural nets rnns dominant approach sequence learning mapping problems. sequence-to-sequence framework variants fueled based approaches range problems language modeling text generation many others. work propose rnn-based approach sentence ordering problem exploits set-to-sequence framework vinyals bengio kudlur wordlevel encoder produces sentence embeddings sentence-level encoder iteratively attends embeddings constructs context representation. initialized representation sentence-level pointer network selects sentences sequentially. widely studied task relevant sentence ordering coherence modeling order discrimination task. given document permuted version task involves identifying coherent ordering. proposed model achieves state-of-the-art performance benchmark datasets task outperforming several classical approaches recent data-driven approaches. addressing challenging task ordering given collection sentences consider novel interesting task ordering sentences abstracts scientiﬁc articles. model strongly outperforms previous work task. visualize learned sentence representations show model captures high-level discourse structure. provide visualizations help understand information sentences model uses identify next sentence. modeling structure coherent texts problem. task coherently organizing given sentences commonly used build evaluate models understand structure. propose end-to-end unsupervised deep learning approach based set-to-sequence framework address problem. model strongly outperforms prior methods order discrimination task novel task ordering abstracts scientiﬁc articles. furthermore work shows useful text representations obtained learning order sentences. visualizing learned sentence representations shows model captures high-level logical structure paragraphs. representations perform comparably state-of-the-art pre-training methods sentence similarity paraphrase detection tasks. modeling structure coherent texts important problem nlp. well-written text particular high-level logical topical structure. actual word sentence choices transitions come together convey purpose text. primary goal build models learn structure arranging given sentences make coherent text. multi-document summarization retrievalbased question answering involve extracting information multiple documents organizing coherent summary. since relative ordering sentences different sources unclear able automatically evaluate particular order essential. barzilay elhadad discuss importance ordering component show ﬁnding acceptable orderings enhance user comprehension. importantly learning order sentences model text coherence. difﬁcult explicitly characterize properties text make coherent. ordering models attempt understand properties learning high-level structure causes sentences appear speciﬁc order human-authored texts. automatic methods evaluating human/machine generated text great importance applications essay scoring text generation copyright association advancement artiﬁcial intelligence rights reserved. finally demonstrate ordering model learns coherence properties text representations useful several downstream tasks including summarization sentence similarity paraphrase detection. summary contributions follows propose end-to-end trainable model based set-to-sequence framework address problem coherently ordering collection sentences. consider novel task understanding structure abstract paragraphs demonstrate state-of-the-art results order discrimination sentence ordering tasks. coherence modeling sentence ordering. coherence modeling sentence ordering approached closely related techniques. many approaches propose measure coherence formulate ordering problem ﬁnding order maximal coherence. recurring themes prior work include linguistic features centering theory local global coherence. local coherence modeled considering properties local windows sentences sentence similarity transition structure. lapata represent sentences vectors linguistic features learn transition probabilities features adjacent sentences. entity-grid model captures local coherence modeling patterns entity distributions. sentences represented syntactic roles entities appearing document. features extracted entity grid used train ranking svm. methods motivated centering theory states nouns entities coherent discourses exhibit certain patterns. global models coherence typically hmms model document structure. content model represents topics particular domain states hmm. state transitions capture possible presentation orderings within domain. words sentence modeled using topic-speciﬁc language model. content model inspired several subsequent work combine strengths local global models. elsner austerweil charniak combine entity grid content model using non-parametric hmm. soricut marcu several models feature functions deﬁne loglinear model assign probability text. louis nenkova model intentional structure documents using syntax features. unlike previous approaches handcrafted features adopt embedding-based approach. local coherence taken account next-sentence prediction component model global dependencies naturally captured rnn. demonstrate model capture logical topical structure several evaluation benchmarks. data-driven approaches. neural approaches gained attention recently. hovy model sentences embeddings derived recurrent neural nets train feed-forward neural network takes input window sentence embeddings outputs probability represents coherence sentence window. coherence evaluation performed sliding window text aggregating score. jurafsky study model larger scale task also sequence-tosequence approach model trained generate next sentence given current sentence vice versa. nguyen joty learn model coherence using convolutional network operates entity-grid representation input document. models limited local nature; experiments show considering larger contexts beneﬁcial. hierarchical rnns document modeling. word-level sentence-level rnns used hierarchical fashion modeling documents prior work. luong jurafsky proposed hierarchical autoencoder generation summarization applications. relevant work similar model considered sentence-level predicts words next sentence given previous sentences word-level predicts word sequence conditioned sentence hidden state. model hierarchical structure similar models takes discriminative approach. combinatorial optimization rnns. vinyals bengio kudlur equip sequence-to-sequence models ability handle input output sets discuss experiments sorting language modeling parsing. called read process write model. read block maps input tokens ﬁxed length vector representation. process block encoder which time-step attends input token embeddings computes attention readout appending current hidden state. write block produces target sequence conditioned representation produced process block. goal show input output orderings matter tasks demonstrated using small scale experiments. work exploits framework address challenging problem modeling logical hierarchical structure text. vinyals fortunato jaitly proposed pointer-networks combinatorial optimization problems output dictionary size depends number input elements. pointer-network decoder sequentially pick next sentence. proposed model inspired human would solve task. first model reads sentences capture meaning general context paragraph. given knowledge model tries pick sentences sequentially till exhaustion. model based read process write framework vinyals bengio kudlur brieﬂy discussed previous section. encoder-decoder terminology common following discussion. figure model overview input sentences represented vectors using sentence encoder. time step model attention weights computed sentence embeddings based current hidden state. encoder uses attention probabilities compute input next time-step decoder uses prediction. scoring function. consider choices scoring function eqs. ﬁrst single hidden layer feed-forward takes inputs learnable parameters). structure similar window network hovy used local window sentences capture context scoring function exploits entire history sentences encoded hidden state score candidates next sentence. also consider bilinear scoring function compared previous scoring function takes generative approach regress next sentence given current hidden state enforcing similar correct next sentence. observed scoring function better sentence representations contrastive sentences. vanilla form found set-to-sequence model tends rely certain word clues perform ordering task. encourage holistic sentence understanding random sentences sentence memory decoder makes classiﬁcation decisions. makes problem challenging decoder since distinguish sentences relevant irrelevant current context identifying correct sentence. model comprised sentence encoder encoder decoder sentence encoder takes input words sentence sequentially computes embedding representation sentence. henceforth refer sentence embedding interchangeably. embeddings given sentences constitute sentence memory available accessed subsequent components. encoder lstm identical originally proposed process block deﬁned time step input lstm computed taking weighted memory elements weights attention probabilities obtained using previous hidden state query iterated ﬁxed number times called read cycles. intuitively model identiﬁes soft input order read sentences. described vinyals bengio kudlur encoder desirable property being invariant order sentence embeddings reside memory. decoder pointer network takes similar form differences lstm takes embedding previous sentence input instead attention readout. training time correct order sentences known used input. test time predicted assignment ˆxt− used instead. attention computation identical encoder interpreted probability correct sentence choice position conditioned previous sentence assignments initial state decoder lstm initialized ﬁnal hidden state encoder. vector zeros. table mean accuracy comparison accidents earthquakes data order discrimination task. reference models entity-grid graph window network sequence-to-sequence respectively. ﬁrst consider order discrmination task widely used literature evaluating coherence models. consider challenging ordering problem coherent order given collection sentences needs determined. demonstrate ordering model learns coherence properties useful summarization. finally show model learns sentence representations useful downstream applications. tasks discussed section train model maximum likelihood objective training data relevant task. used single hidden layer scoring function order discrimination sentence ordering tasks. models trained end-to-end. pretrained dimensional glove word embeddings initialize word vectors. lstms hidden layer size hidden layer size number read cycles encoder model architecture used across experiments. used adam optimizer batch size learning rate learning. model regularized using early stopping. hyperparameters chosen using validation set. order discrimination ordering problem traditionally formulated binary classiﬁcation task given reference paragraph permuted version identify coherent datasets widely used task previous work accidents earthquakes news reports. datasets training test sets include articles approximately permutations article. outperformed model jurafsky earthquakes data window approach jurafsky performs strongly. approach outperforms prior models datasets achieving near perfect performance earthquakes dataset. datasets widely used quite formulaic nature longer challenging. hence turn challenging task ordering given collection sentences make coherent document. sentence ordering task directly address ordering problem. assume availability candidate orderings choose instead good ordering possible permutations sentences. difﬁculty ordering problem depends nature text well length paragraphs considered. evaluation text arbitrary text sources makes difﬁcult interpret results since clear whether attribute observed performance deﬁcient model ambiguity next sentence choices many plausible orderings. text summaries suitable source data task. often exhibit clear ideas little redundancy. speciﬁcally look abstracts conference papers research proposals. data several favorable properties. abstracts usually particular high-level format begin brief introduction description problem proposed approach conclude performance remarks. would allow identify model capture high-level logical structure. second abstracts average length making ordering task accessible. also gives signiﬁcant amount data train test models. nips abstracts. consider abstracts nips papers past years. parsed abstracts paper pdfs obtained abstracts omitting erroneous extracts. dataset split years training respectively validation testing. abstracts. second source abstracts papers anthology network corpus extracted abstracts text parses using simple keyword matching strings ‘abstract’ ‘introduction’. extracts papers published year training year validation years testing. abstracts. also used research award abstracts dataset comprises abstracts diverse scientiﬁc areas contrast previous sources data abstracts also lengthier making dataset challenging. years used training validation testing. capped parses abstracts maximum length sentences. unsuccessful parses parses excessive length discarded. details data provided supplement. following metrics used evaluate performance. accuracy measures often absolute position sentence correctly predicted. kendall’s computed predicted sequence incorrect relative order sequence length. lapata discusses metric reliably correlates human judgements. entity grid. ﬁrst baseline entity grid model barzilay lapata stanford parser brown coherence toolkit derive entity grid representations. ranking trained score correct orderings higher incorrect orderings original work. used permutations document training data. since entity grid provides means feature extraction evaluate model ordering setting follows. choose random permutations document correct order pick order maximum coherence. experimented transitions length entity-grid. seqseq. second baseline consider sequenceto-sequence model trained predict next sentence given current sentence. jurafsky consider similar methods model uni-directional model. methods shown yield sentence embeddings competitive performance several semantic tasks kiros window network. consider window approach hovy jurafsky demonstrated strong performance order discrimination task third baseline. adopt coherence score interpretation considered authors. models consider special embedding vector padded beginning paragraph learned training. vector allows identify initial sentences greedy decoding. decoder. another baseline proposed model without encoder. decoder hidden state initialized zeros. observed using special start symbol baselines helped obtain better performance model. however start symbol help model equipped encoder hidden state initialization alone good enough. place emphasis particular search algorithm work thus beam search coherence score heuristic models. beam size used. decoding sentence candidates already chosen pruned beam. rnns hidden layer size window network used window size hidden layer size initialize models pre-trained glove word embeddings. assess performance model baseline methods table window network performs strongly compared baselines. model better signiﬁcant margin exploiting global context demonstrating global context important task. entity-grid model fairly successful order discrimination task past observe fails discriminate large number candidates. reason could feature representation less sensitive local changes sentence order computational expense obtaining parse trees constructing grids large amount data prohibited experimenting model abstracts data. seqseq model performs worse window network. interestingly jurafsky observe seqseq model outperforms window network order discrimination task wikipedia data. however wikipedia data considered work order magnitude larger datasets considered here could potentially helped generative model. models also expensive inference since involve computing sampling word distributions. table performance comparison semantic similarity paraphrase detection. ﬁrst shows best performing purely supervised methods. last section shows models. abstract. shows model learns high-level structure documents generalizing well unseen text. structure less apparent dataset data diversity longer documents. approaches based barzilay model explicitly capture topics discovering clusters sentences neural approach implicitly discovers structure. sentence ordering summarization section show sentence ordering models learn coherence properties useful summarization. consider variation model model takes sentences several documents input sequentially picks summary sentences predicts special ‘stop’ symbol. distinction model recent work input order sentences assumed unknown making applicable multi-document summarization. train model scratch perform extractive summarization fashion. consider model pre-trained ordering task ﬁne-tuned task. dailymail datasets used experimentation. dailymail pre-training purposes ﬁne-tuning evaluation. labels dailymail used. compare rouge scores models table standard evaluation settings. observe model pre-trained ordering task scores consistently better model trained scratch. results improved using larger news corpora. shows sentence ordering attractive unsupervised objective exploiting large unlabelled learned sentence representations original motivations work question whether learn high-quality sentence representations learning model text coherence. address question trained model large number paragraphs using bookcorpus dataset evaluate quality sentence embeddings derived model evaluation pipeline kiros tasks involve understanding sentence semantics. evaluations performed training classiﬁer embeddings derived model performance indicative quality sentence representations. present comparison semantic relatedness paraphrase detection tasks table results uni-directional versions models discussed fair comparison. similar skip-thought paper train models predicting correct order forward direction another backward direction. numbers shown ordering model obtained concatenating representations models. concatenating representation bag-ofwords representation sentence improves performance. because ordering model choose less attention speciﬁc lexical information focus high-level document structure. hence representations capture complementary semantics. adding features improves performance paper propose method semantic class induction first introduce generative model sentences based dependency trees takes account homonymy model thus seen generalization brown clustering second describe efﬁcient algorithm perform inference learning model third apply proposed method large datasets demonstrate classes induced algorithm improve performance brown clustering task semisupervised supersense tagging named entity recognition representation learning promising technique discovering features allow supervised classiﬁers generalize source domain dataset arbitrary domains present novel formal statement representation learning task argue task computationally intractable general important representation learner able incorporate expert knowledge search helpful features leveraging posterior regularization framework develop architecture incorporating biases representation learning investigate three types biases experiments domain adaptation tasks show biased learners identify signiﬁcantly better sets features unbiased learners resulting relative reduction error tasks respect state-of-the-art representation learning techniques. model several advantages word-level reconstruction objective trained large softmax output layers. limits vocabulary size slows training model achieves comparable performance word reconstruction component. train vocabulary words; results based training time days titan gpu. word inﬂuence attempt understand text-level clues model uses perform ordering task. inspired gradients prediction decisions respect words correct sentence proxy salience word. feed sentences decoder correct order time step compute derivative score correct next sentence respect word embeddings. importance word correctly predicting next sentence deﬁned assume hidden states decoder ﬁxed back-propagate gradients sentence encoder. table shows visualizations abstracts. darker shades correspond higher gradient norms. ﬁrst example model appears using word clues ‘ﬁrst’ ‘second’ ‘third’. similar observation made chen huang second example observe model pays attention phrases present’ argue’ typical abstract texts. also focuses word ‘representation’ appearing ﬁrst sentences. observations link centering theory states entity distributions coherent discourses exhibit certain patterns. model implicitly learns patterns syntax annotations handcrafted features. work investigated challenging problem coherently organizing sentences. rnn-based model performs strongly compared baselines prior work sentence ordering order discrimination tasks. demonstrated captures high-level document structure learns useful sentence representations trained large amounts data. approach ordering problem deviates prior work handcrafted features. however exploiting linguistic features next sentence classiﬁcation potentially improve performance. entity distribution patterns provide useful features named entities treated out-of-vocabulary words. ordering problem studied higher-level discourse units paragraphs sections chapters. material based part upon work supported contract opinions ﬁndings conclusions recommendations expressed authors necessarily reﬂect views ibm. thank umich/ibm sapphire team junhyuk ruben villegas xinchen zhang kibok yuting zhang helpful comments discussions.", "year": 2016}