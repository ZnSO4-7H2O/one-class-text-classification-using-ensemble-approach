{"title": "Document Embedding with Paragraph Vectors", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.", "text": "paragraph vectors recently proposed unsupervised method learning distributed representations pieces texts. work authors showed method learn embedding movie review texts leveraged sentiment analysis. proof concept encouraging rather narrow. consider tasks sentiment analysis provide thorough comparison paragraph vectors document modelling algorithms latent dirichlet allocation evaluate performance method vary dimensionality learned representation. benchmarked models document similarity data sets wikipedia arxiv. observe paragraph vector method performs signiﬁcantly better methods propose simple improvement enhance embedding quality. somewhat surprisingly also show much like word embeddings vector operations paragraph vectors perform useful semantic results. central many language understanding problems question knowledge representation capture essential meaning document machine-understandable format despite much work going area established format perhaps words representations latent dirichlet allocation another widely adopted representation. recent paradigm machine intelligence distributed representation words documents interesting part even though representations less humaninterpretable previous representations seem work well practice. particular mikolov show method paragraph vectors capture many document semantics dense vectors used classifying movie reviews retrieving pages. paper make attempt compare paragraph vectors baselines tasks signiﬁcant practical implications. first benchmark paragraph vectors task wikipedia browsing given wikipedia article nearest articles audience browse next. also test paragraph vectors task ﬁnding related articles arxiv. tasks paragraph vectors allow ﬁnding documents interest simple intuitive vector operations. example japanese equivalence lady gaga.. goal paper beyond benchmarking positive results wikipedia arxiv datasets conﬁrm good representations texts powerful comes language understanding. success tasks shows possible paragraph vectors local non-local browsing large corpora. also show simple effective trick improve paragraph vector. particular observe jointly training word embeddings skip gram model quality paragraph vectors improved. paragraph vector model ﬁrst proposed model inserts memory vector standard language model aims capturing topics document. authors named model distributed memory suggested ﬁgure above paragraph vector concatenated averaged local context word vectors predict next word. prediction task changes word vectors paragraph vector. distributed words model efﬁcient experiments paper focuses implementation paragraph vector. following sections explore paragraph vectors different applications document understanding. quality paragraph vectors. preliminary results also showed training unigrams bigrams improve quality ﬁnal vectors. present range qualitative quantitative results. give examples nearest neighbours wikipedia articles arxiv papers well visualisation space wikipedia articles. also show examples nearest neighbours performing vector operations. quantitative evaluation attempt measure well paragraph vectors represent semantic similarity related articles. constructing triplets triplet consists pair items close item unrelated. publicly available corpora trained paragraph vectors least epochs data hierarchical softmax constructed huffman tree classiﬁer. cosine similarity metric. also applied gibbs sampling iterations varying numbers topics. used values used posterior topic proportions paper hellinger distance compute similarity pairs documents. completeness also include results averaging word embeddings word paper using paragraph vector. finally consider classical words model word represented one-hot vector weighted tf-idf document represented vector comparisons done using cosine similarity. extracted main body text wikipedia articles english site. removed links applied frequency cutoff obtain vocabulary words. trained paragraph vectors wikipedia articles visualized figure using t-sne visualization conﬁrms articles category grouped together. wide range sport descriptions wikipedia explains sports less concentrated. also qualitatively look nearest neighbours wikipedia articles compare paragraph vectors lda. example nearest neighbours wikipedia article machine learning shown table overall paragraph vectors better nearest neighbours lda. table nearest neighbours machine learning. bold face texts articles found unrelated machine learning. hellinger distance cosine distance paragraph vectors work best model. artiﬁcial neural network predictive analytics structured prediction mathematical geophysics supervised learning constrained conditional model sensitivity analysis sxml feature scaling boosting prior probability curse dimensionality scientiﬁc evidence online machine learning n-gram cluster analysis dimensionality reduction functional decomposition bayesian network paragraph vectors artiﬁcial neural network types artiﬁcial neural networks unsupervised learning feature learning predictive analytics pattern recognition statistical classiﬁcation structured prediction training meta learning kernel method supervised learning generalization error overﬁtting multi-task learning generative model computational learning theory inductive bias semi-supervised learning perform vector operations paragraph vectors local non-local browsing wikipedia. table table show results experiments. ﬁrst experiment related articles lady gaga. second experiment japanese equivalence lady gaga. achieved vector operations paragraph vectors word vectors. sets results show paragraph vectors achieve kind analogies like word vectors wikipedia nearest neighbours lady gaga american japanese using paragraph vectors. note ayumi hamasaki famous singers best selling artists japan. also album called poker face quantitatively compare methods constructed datasets triplet evaluation. ﬁrst consists triplets articles knew related domain knowledge. examples deep learning closer machine learning computer network google closer facebook walmart etc. examples hard probably require deep understanding content diego closer angeles jose. second dataset consists triplets articles closer listed category wikipedia last article category sibling category. example articles barack obama closer britney spears china. triplets generated randomly. benchmark document embedding methods words paragraph vector well models capture semantic documents. results reported table table methods also vary number embedding dimensions. results table seen paragraph vectors perform better lda. also peak paragraph vector performance dimensions. paragraph vectors averaging word embeddings perform better model. found tf-idf weighting words inferred topic allocations affect performance. results also joint training word vectors improves ﬁnal quality paragraph vectors. extracted text versions full arxiv papers. case used latest revision available. applied minimum frequency cutoff vocabulary ﬁnal vocabulary words. performed experiments related articles using paragraph vectors. table table show nearest neighbours original paragraph vector paper distributed representations sentences documents current paper. table want bayesian equivalence paragraph vector paper. achieved vector operations paragraph vectors word vectors learned training paragraph vectors. results suggest paragraph vector works well tasks. measure performance different models task picked pairs papers least shared subject unrelated paper chosen random papers shared subjects ﬁrst paper. produced dataset triplets method. results table seen paragraph vectors perform best performing number topics lda. paragraph vectors also less sensitive differences embedding size number topics. also peak paragraph vector performance dimensions. models perform better vector space model. found tf-idf weighting words inferred topic allocations affect performance. evaluating neural word representations tensor-based compositional settings polyglot distributed word representations multilingual lexicon infused phrase embeddings named entity resolution convolutional neural network modelling sentences distributed representations words phrases compositionality convolutional neural networks sentence classiﬁcation simlex- evaluating semantic models similarity estimation exploiting similarities among languages machine translation efﬁcient estimation word representations vector space multilingual distributed representations without word alignment distributed representations sentences documents efﬁcient estimation word representations vector space thumbs sentiment classiﬁcation using machine learning techniques distributed representations words phrases compositionality knet general framework learning word embedding using japanese-spanish thesaurus construction using english pivot multilingual distributed representations without word alignment catching drift probabilistic content models applications table arxiv nearest neighbours distributed representations sentences documents neural bayesian. i.e. bayesian equivalence paragraph vector paper. content modeling using latent permutations simlex- evaluating semantic models similarity estimation probabilistic topic syntax modeling part-of-speech evaluating neural word representations tensor-based compositional settings syntactic topic models training restricted boltzmann machines word observations discrete component analysis resolving lexical ambiguity tensor regression models meaning measuring political sentiment twitter factor-optimal design described results paragraph vectors showing effectively used measuring semantic similarity long pieces texts. experiments show paragraph vectors superior measuring semantic similarity wikipedia articles across sizes paragraph vectors. paragraph vectors also perform lda’s best performing number topics arxiv papers perform consistently relative embedding size. also surprisingly vector operations performed similarly word vectors. provide interesting techniques wide range applications local nonlocal corpus navigation dataset exploration book recommendation reviewer allocation.", "year": 2015}