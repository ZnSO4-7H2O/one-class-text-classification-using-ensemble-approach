{"title": "Exploiting generalization in the subspaces for faster model-based  learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Due to the lack of enough generalization in the state-space, common methods in Reinforcement Learning (RL) suffer from slow learning speed especially in the early learning trials. This paper introduces a model-based method in discrete state-spaces for increasing learning speed in terms of required experience (but not required computational time) by exploiting generalization in the experiences of the subspaces. A subspace is formed by choosing a subset of features in the original state representation (full-space). Generalization and faster learning in a subspace are due to many-to-one mapping of experiences from the full-space to each state in the subspace. Nevertheless, due to inherent perceptual aliasing in the subspaces, the policy suggested by each subspace does not generally converge to the optimal policy. Our approach, called Model Based Learning with Subspaces (MoBLeS), calculates confidence intervals of the estimated Q-values in the full-space and in the subspaces. These confidence intervals are used in the decision making, such that the agent benefits the most from the possible generalization while avoiding from detriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to the optimal policy is theoretically investigated. Additionally, we show through several experiments that MoBLeS improves the learning speed in the early trials.", "text": "abstract—due lack enough generalization statespace common methods reinforcement learning suffer slow learning speed especially early learning trials. paper introduces model-based method discrete statespaces increasing learning speed terms required experience exploiting generalization experiences subspaces. subspace formed choosing subset features original state representation generalization faster learning subspace many-to-one mapping experiences full-space state subspace. nevertheless inherent perceptual aliasing subspaces policy suggested subspace generally converge optimal policy. approach called model based learning subspaces calculates conﬁdence intervals estimated q-values full-space subspaces. conﬁdence intervals used decision making agent beneﬁts possible generalization avoiding detriment perceptual aliasing subspaces. convergence mobles optimal policy theoretically investigated. additionally show several experiments mobles improves learning speed early trials. learning interactions fundamental capabilities intelligent creatures adapt complex environments. intelligent systems beneﬁt variety interactive learning methods among reinforcement learning broadest spectrum usage owes wide range applicability mainly simplicity terms information content required feedback computational load well biological implications cognitive model proof convergence optimal policy discrete representations. nevertheless severely suffers curse dimensionality number interactions agent requires learn linearly related statespace size. results slow learning rate high regret consequently limited usage face limited learning budget well physical systems. inefﬁciency experience generalization discrete state-spaces mainly blamed slow learning. perience generalization method targets employing experience limited number states approximate values larger portion states words generalization means using experiences states update values states called similar states attain optimal policy smaller number interactions environment. challenge generalization experiences ﬁnding similarity across states learning agent aware task model beforehand high level capability discover possible similarities learning. methods proposed enhance generalization capability. methods clustered seven main categories. ensembles learning methods like imitation learning learning sources information excluded categorization. though paper discrete state-spaces following shortly describe seven main categories applied either discrete state-spaces continuous statespaces. ﬁrst category methods generalizes experiences across multiple learning agents. multi-agent learning agents exchange expertise knowledge speed individual learning. methods useful agents different expertise. however applicability inversely correlated heterogeneity among agents. addition methods work isolated nonsocial agents. second category generalization embedded state coding designer similar states share elements state vector approach core generalization continuous state-action values neighboring states naturally similar. nevertheless generalization state coding applied discrete environments. finding hierarchy task generalization reusing learned subtask well structure hierarchy main ideas behind third methods category beneﬁting similarity subtask level targeted. different algorithms developed discover hierarchy parallel learning example). hierarchical methods signiﬁcantly speed learning process converge optimal policy detecting hierarchy hierarchy exists task. nevertheless methods helpful early learning trials since recognizing hierarchy requires interactions environment. alization main idea employing functional space that pair states similarity normalized values experienced actions increases probability similarity q-values less experienced ones. generalization functional space reduces regret early learning trials. however guarantee convergence optimal policy. ﬁfth methods employs attention learning learning percept environment efﬁciently efﬁcient mean paying less cost perception reducing size state-space learn faster. researchers enabled salient features state vector attention learning mechanisms.it also shown human beings employ strategy reduce dimensionality perception space salient features recognized less salient features excluded state deﬁnition experience generalization emerges result multiple-to-one state projection. process gradual generalization shows progressively. attention learning costly helpful state features locally globally redundant. sixth methods efﬁcient state representation implicitly learned. using deep neural networks major example methods example). approach promising especially learn complex problems. however research required beneﬁt face limited learning budget. seventh category group exploited possible generalization subspaces faster learning subspace refers sub-dimension original state representation called full-space. authors showed learning single step task multidimensional full-space accelerated agent simultaneously estimates action-value intervals fullspace subspace fuses estimated values. faster learning possible generalization subspaces obtained multiple-to-one projection full-space subspace. therefore every state subspace beneﬁts experiences action-value estimation compared state full-space. possibility using generalization subspaces monte carlo model-free method multi-step tasks explored. results promising; however method suffers slow learning lack proof convergence. paper extend idea beneﬁting possible generalization subspaces speedup model-based learning multistep tasks especially early learning trials well convergence optimal policy. main contributions paper summarized below explain idea using subspaces framework expediting learning process concrete model. previous works members group idea using subspaces expediting learning process exploited however idea exploited mathematically rigorous manner. work ﬁrst puts forward model using subspaces model-based previous works rest paper organized follows. next section explain problem deﬁnitions used paper. next give introduction modelbased approach afterwards section explain proposed model using subspaces model-based addition model analyze convergence proposed method section. next investigate performance model several environments environments extra sensors. finally ﬁnish paper discussion giving several venues future works. agent lives unknown ﬁnite discrete-time markov decision process discrete state-action deﬁned tuple ﬁnite statespace ﬁnite action space state denotes transition probability transition state state action denotes expected reward state-action discount factor accumulated reward feature variable conveying information environment. assume agent gets information different features multi-dimensional environment denoted feature feature. since assumes fully observable agent feature contains information environment. also state-space cartesian product features called full-space state represented value feature. every non-empty subset full-space called feature subspace call subspace brevity. state subspaces projection state full-space onto subspaces. subspace seen special case tile coding agent’s goal learning optimal policy interaction environment. paper agent gradually builds models task full-space selected subspaces. using models agent approximates optimal state-action q-value interval spaces intelligently integrates values increase learning speed. assume agent know task model knows length reward interval. model task estimated agent experiences learning process. decision-making policy based estimated model best action computed estimated model. example \u0001-greedy policy best action chosen probability random action probability assume agent know distribution immediate rewards. know immediate rewards samples random variables deﬁned ﬁxed interval independent. choose hoeffding inequality distribution-free used compute conﬁdence intervals. hoeffding inequality expressed following equation arbitrary number interval immediate reward state-action sample random variable consider right side called conﬁdence coefﬁcient equal −δr. then value computed following relation approaches conﬁdence interval q-values decision making policies section give overview model estimation q-value estimation conﬁdence interval estimation q-values. assume environment stationary therefore actual transition probabilities ﬁxed. estimated transition probabilities given weissman inequality computing conﬁdence intervals. consider probability vector transition probabilities states taking action elements. probability vector vector positive elements elements one. weissman inequality probability vector expressed following equation vector dimensionality probability vector arbitrary number. consider right-hand side relation equal conﬁdence coefﬁcient transition probabilities. then value equal step calculating conﬁdence interval estimated parameters following ﬁrst explain conﬁdence intervals estimated rewards calculated. then give expression conﬁdence intervals estimated transition probabilities next part. state full-space corresponding state subspaces repeatedly. agent wants decide appropriate action based full-space experiences nearly chooses random action. however agent utilize generalization experiences subspaces speed learning. also problem perceptual aliasing subspaces. environment partially observable perspective subspaces; thus many-to-one mapping state full-space state subspace decisions subspaces suboptimal agent environmental features. example previous maze example assume barriers positions second column. optimal policy positions ﬁrst column adjacent position second column barrier decision going right. therefore making appropriate generalization needs evaluate usefulness respect issue. indeed using subspaces experiences learning process inﬂuential factors proﬁt generalization detriment issue. assume agent select decision suggested fullspace subspaces. early stages learning process agent less experience decision full-space unreliable. agent rely subspace decisions proﬁt generalization detriment issue. gaining experience uncertainty estimated model fullspace becomes less agent choose full-space decision loss issue subspaces increases. obvious task subspace also satisﬁes markov property therefore reinforcement learning tasks subspaces also mdp. proposed approach increasing learning speed policy suggested fullspace combined subspaces’ policies learned mdps. using model-based approach task parameters estimated learning process full-space subspaces accordance environmental feedbacks. reduce effect issue uncertainty suggestion decision-making calculated conﬁdence interval estimated task parameters. scheme proposed method summarized fig. proposed method comprises three main phases performed recursively estimating subspaces fullspace models calculating q-values uncertainties combining decisions subspaces fullspace make decision current state. based q-values uncertainties weight assigned action subspaces full-space. last stage model used integrate acquired choice probabilities choosing appropriate action. call model conﬁdence degree model following subsections proposed learning decision making method explained details. step calculating upper lower bounds q-values upper lower bounds q-values seen optimistic pessimistic expected rewards state-action pairs. denote upper lower bounds respectively. making mistic pessimistic q-values state-action pairs calculated. algorithm illustrates procedure computing algorithm strongly related internal policy evaluation given givan extended value iteration mbie ucrl algorithms difference compute optimistic policy computing conﬁdence interval q-values. methodology generalization agent experiences feature subspaces speed learning process. since subspaces low-dimensional learning needs smaller number experiences compare full-space. fig. illustrates example maze demonstrate generalization subspaces speed learning process. environmental features correspond positions respectively. full-space state-space subspaces agent four actions either going down right left reach goal speciﬁed goal location agent learns action right preferable column learning speed increase. suppose agent already gone state times state times. agent goes state ﬁrst time choose action randomly since full-space experience state. agent relies also subspace experiences choose right action agent ﬁrst column times. this call generalization experiences subspaces. consequently learning process would faster agent relies subspace experiences beginning learning process. generalization agent experiences particularly helpful states full-space rarely visited corresponding states subspaces visited often. example assume agent goes function optimisticvaluep lenr input stores counters estimated probabilities vector estimated rewards vector lenr vector containing length rewards fig. example showing maze. columns rows feature sets subspaces. values feature sets agent four actions either going down right left reach goal speciﬁed goal location agent learns action right preferable column learning speed increase. therefore generalization experiences subspaces increase learning speed. fig. schematic overview proposed framework using subspaces experiences speed learning process. feature agent updates model parameters full-space subspaces using environmental feedbacks actions. accordance estimated model space q-values uncertainties state-action pairs calculated subspaces full-space. finally agent integrates information make decision employing cdm. conﬁdence intervals model uncertainty estimates full-space subspaces. conﬁdence intervals make beneﬁt generalization subspaces taking care issue. conﬁdence intervals estimated full-space subspaces based estimated parameters mdps them. model parameters estimation procedure value estimation already explained subsection iii-a iii-b. since algorithm computationally demanding algorithm episode update optimal expected rewards. computing conﬁdence interval q-values full-space subspaces need calculate according equations space. assume immediate rewards different learning steps speciﬁc state-action full-space interval denoted βfspace]. then fullspace given states subspace note subspaces interval depends immediate reward observation. many-to-one mapping states full-space state subspace. therefore every reward sample transition subspaces different intervals. section explained estimate q-values uncertainties every state-action pairs full-space subspaces. soft decision policy based estimated q-values exploration exploitation. available techniques literature computing policies full-space subspaces separately. meaning subspace full-space considered separate decision policy computed computing ﬁnal policy state-action pair integrate decision policies. integrating decision policies assign coefﬁcient subspace based coefﬁcients ﬁnal policy calculated. ﬁrst discuss calculate coefﬁcients come procedure combining later. coefﬁcients call conﬁdence degrees estimated estimated q-values conﬁdence intervals. conﬁdence interval state-action pair subspace overlap conﬁdence interval corresponding full-space conﬁdence length quantity conﬁdence intervals larger estimated q-values larger uncertainty. length quantity computes effect uncertainty subspace computed following equation |ci| length conﬁdence interval subspace |ci| length conﬁdence interval corresponding full-space state-action. function increasing function determined based experiments. chosen following function simulations. early stages learning process experiences subspaces signiﬁcantly experiences full-space several state-action pairs fullspace state-action pair subspace. therefore uncertainty subspaces less full-space agent rely subspaces decisions. thus conﬁdence degrees higher subspaces decisions length quantity given takes care this. overlap-distance quantity quantity measures consistency experiences subspace corresponding experiences full-space. experience subspace consistent corresponding experiences full-space means issue less problematic state agent rely subspace decision. overlap-distance quantity computed following relation numerator inside function takes account distance estimated q-value subspace corresponding value full-space. denominator shows overlap conﬁdence interval subspace state-action pair corresponding full-space stateproof. limit inﬁnite number experiences based large number estimated transition probability expected reward full-space converges actual values. therefore decision policy fullspace converges optimal policy. also show estimated subspaces converge. therefore conﬁdence intervals converge zero subsequently q-values converges zero. evident converge zero q-values full-space subspace converge different quantities conﬁdence degree subspace converges zero. split whole feature subspace feature complement state full-space represented vector want compute estimated probability transition write state reachable state taking action prove convergence subspace transition probax) doesn’t converge based algorithm computing conﬁdence intervals |ci| remains greater zero. since |ci| converges zero therefore based conclude consequently probability action selection converges action pair small overlap means experiences subspace consistent states full-space subspaces equal overlap distance estimated q-values estimated q-value fullspace different numerator ensures overlapdistance quantity puts weight subspace less distance. overlap-distance quantity handles problem issue subspaces. gaining experience full-space decision becomes reliable agent rely limit estimated q-value fullspace converges point estimated q-values subspaces converge points. length overlap reduces compared interval length. learning process distance estimated q-values subspaces full-space grows. therefore inﬂuence subspaces reduces gradually. state explain decision integration performed based conﬁdence degrees. consider subspace highest conﬁdence degree i.e. maxx∈sj subspaces. ﬁnal unnormalized probability action selection state given value shows choice probability action using policy computed model parameters subspace mdp. value choice probability corresponding full-space mdp. summary proposed method illustrated algorithm following provide proof limit large number experiences conﬁdence degrees subspaces converge zero therefore decision-making purely based full-space decision. therefore elucidates proposed method choosing conﬁdence degrees takes account problem issue subspaces. theorem ﬁnite state-space ﬁnite action space consider decision-making procedure based cdm. number experiences state-actions goes inﬁnity q-values subspaces converge. furthermore q-values subspace corresponding full-space don’t converge number conﬁdence degree subspace converges zero. model based learning explained background. method without considering subspaces. famous learning method selecting best decay rate eligibility trace learning rate subspaces modiﬁed version algorithm uses monte carlo approach slow. hence learning best uncertainties calculated according monte carlo explained model-free learning method uses linear function approximator tile coding here subspace considered tiling. algorithm learning rate learning function approximator. policy agent full-space subspaces \u0001-greedy parameters needed learning process given tables i-iii. models need parameters. model-based approaches meaning mobles need parameter. mobles require mobles needs simulations consider different learning parameters according tables best results reported comparisons. tried different learning rates shown table report best results comparisons. parameter description discount factor greedy policy stopping criterion algorithm algorithms computing optimistic pessimistic q-values conﬁdence coefﬁcient estimated rewards fullspace subspaces conﬁdence coefﬁcient estimated transition probabilities full-space subspaces simulation studies considered experiments sensors another sensors. environments experiments mazes barriers agent learns task reaching goal certain initial point. task ﬁnding average shortest path starting point point. goal ﬁxed starting point beginning episode selected random. environments features feature selected location agent experiments features four extra infrared sensors addition positions agent. later explain meaning extra infrared algorithm conﬁdece degree model decision-making function input vector estimated q-values full-space subspaces vector upper bounds q-values full-space subspaces vector lower bounds q-values full-space subspaces current state full-space policy full-space subspaces algorithm model based learning subspaces function mobles input policy small positive number stopping criterion used three recursive algorithms computing q-values upper bounds q-values lower bounds q-values parameters conﬁdence coefﬁcients. state agent environments position agent. state four actions available agent down right left. agent chooses action moves random direction probability prand moves along intended direction probability −prand. barrier position agent remains current location. immediate rewards obtained agent’s action shown table rewards come truncated mixtures gaussian distributions. knowledge interval lengths rewards number states reachable action used proposed method mobles. explained previously algorithm expensive runs episode. episode q-values updated applying step bellman equations given similar dyna-q learning algorithm small backups algorithm full backup within episode provides signiﬁcant performance beneﬁt. without full backup learning deferred episode. results delaying updating policies therefore slower learning. four different environments used simulations features shown fig. ﬁrst environment obstacle. second third environments room-like structure. fourth environment obstacles arranged environment. fourth environment difﬁcult semirandom obstacles weights decay fast even case using subspaces deteriorate performance proposed method. method mobles-thr last column fig. corresponds method ﬁfth visit state-action learning weights zero meaning switch full-space mode. seen threshold helps improving performance method. shows formulas computing subspaces improved. fig. four different mazes used simulated studies. goal shown environments task goal fewest number steps. episode starts random state environment. figures show simulation results different environments shown fig. left plot fig. plots ﬁrst fig. show average accumulated rewards function number episodes averaged different runs. plots show weights subspaces full-space computed proposed conﬁdence degree model averaged steps episode. large weights subspaces show agent rely decisions. beginning episodes learning weights subspaces large generalization them. simpler environments subspaces informative weights decay slowly harder environment decays fast. problem issue subspaces. clear example effect issue weight decay difference weight decay second third environments. since third environment doors rooms aligned along directions subspaces generalize better problem issue less severe. therefore weights decay slower third part investigate effect adding extra sensors environments previous simulation. environments shown ﬁgures extra sensors four infrared ones shows presence barrier front direction. directions experiments down right left. sensor take values either zero. value shows presence barrier direction sensor zero means barrier. therefore agent total number sensors main sensors four sensors {ir-up ir-right ir-down ir-left}. sensors convey extra information environment. agent learn task without extra sensors incorporating extra sensors expedite learning process. using learning full-space extra sensors redundant agent exploit information conveyed sensors. however simply assume extra sensor subspace procedure learning task. figure shows result proposed method sensors compare sensors learning tiling. ﬁgures extra sensors increases speed learning early stages. sensors generalization sensors i.e. state-space sensors smaller agent learns faster them. environment fig. extra information helpful observe signiﬁcant improvement mobles sensors. fig. result environment obstacle. left plot shows average accumulated reward function number episodes averaged different runs. shaded area shows standard error mean. right plot shows mean weights subspaces full-space mobles method. weights smoothed using rectangular smoothing window length fig. columns left right show results four-rooms environment nine-rooms environment environment semi-random obstacles features. upper shows average accumulated reward lower shows mean weights subspaces full-space mobles method. description plots fig. fig. columns left right show results four-rooms environment nine-rooms environment environment semi-random obstacles features. upper shows average accumulated reward lower shows mean weights subspaces full-space mobles method extra sensors. description plots fig. based approaches reinforcement learning. assume agent prior knowledge task. method attacks curse dimensionality problem deﬁning several mdps different subspaces fullspace addition full-space combining decisions mdps. here curse dimensionality means large amount experiences required learn task. methodology combining decisions balances opposite effects generalization perceptual aliasing subspaces. subspaces better generalization means learning process faster them lower-dimensional. however problem perceptual aliasing partial observable makes decisions cases sub-optimal. state-action pair best selected based conﬁdence degree. conﬁdence degree shows reliability space decision. proposed method computing conﬁdence degrees based estimated qvalues conﬁdence intervals. generalization issue seen comparing estimated q-values conﬁdence intervals subspace full-space explained text. m|s||a| |s||a| number states deﬁned maxs∈s number actions. number parameters subspace difference replaced subspace. therefore dimensionality subspace signiﬁcantly smaller full-space number parameters subsequently computational complexity also smaller. therefore computational complexity method slightly higher model-based approach policy iteration computationally intensive method. convergence method optimal policy proven mathematically. furthermore tested performance mazes ranging simple complex ones. results showed improvement learning speed especially early stages learning process. improving learning speed importance especially agent limited learning budget. environments subspaces informative i.e. generalization subspaces considerable signiﬁcant speed-up using method beginning episodes. even complex environments method works reasonably well defeated pure modelbased method full-space. firouzi ahmadabadi araabi amizadeh mirian siegwart interactive learning continuous multimodal space bayesian approach action-based soft partitioning learning ieee transactions autonomous mental development vol. daee mirian ahmadabadi reward maximization justiﬁes transition sensory selection childhood sensory integration adulthood plos vol. daee development method multimodal sensory integration puterman markov decision processes discrete stochastic dynamic programming. york john wiley sons inc. givan leach dean bounded parameter markov decision ortner auer logarithmic online regret bounds undiscounted reinforcement learning proceedings international conference advances neural information processing systems auer jaksch ortner near-optimal regret bounds reinforcement learning proceedings international conference advances neural information processing systems sutton integrated architectures learning planning reacting based approximating dynamic programming proceedings international conference machine learning mehta tadepalli dietterich automatic discovery transfer maxq hierarchies proceedings international conference machine learning barry kaelbling lozano-p´erez deth* approximate hierarchical solution large markov decision processes proceedings international joint conference articial intelligence interesting feature proposed method easily incorporate extra sensors learning. point state-space sensors redundant therefore exploited original model-based method. however mobles beneﬁt subspaces formed extra sensors increase generalization increase learning speed. simulations environments extra infrared sensors signiﬁcant improvement learning speed. multiple directions future works method works particularly well environments subspaces informative. difﬁcult environments better split environment several sub-environments subgoals called skill acquisition literature then proposed method used sub-environment informative subspaces. framework developed partial observable mdps. think method work well scenarios. example consider sensors high noise. sensors less reliable sensors neglected planning. procedure combining decision making subspaces improved. example fig. including threshold improves learning speed. parametric model like neural networks train parameters sample tasks. objective would parameters minimizing averaged regret tasks. uative feedback nature vol. sutton barto reinforcement learning introduction preprint webpage https//webdocs.cs.ualberta.ca/∼sutton/ book/bookdraftsep.pdf daniel geana gershman leong radulescu wilson reinforcement learning multidimensional environments relies attention mechanisms journal neuroscience vol.", "year": 2017}