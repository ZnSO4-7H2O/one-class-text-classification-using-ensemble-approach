{"title": "Pomegranate: fast and flexible probabilistic modeling in python", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code.", "text": "present pomegranate open source machine learning package probabilistic modeling python. probabilistic modeling encompasses wide range methods explicitly describe uncertainty using probability distributions. three widely used probabilistic models implemented pomegranate general mixture models hidden markov models bayesian networks. primary focus pomegranate abstract away complexities training models deﬁnition. allows users focus specifying correct model application instead limited understanding underlying algorithms. aspect focus involves collection additive sufﬁcient statistics data sets strategy training models. approach trivially enables many useful learning strategies out-of-core learning minibatch learning semi-supervised learning without requiring user consider partition data modify algorithms handle tasks themselves. pomegranate written cython speed calculations releases global interpreter lock allow built-in multithreaded parallelism making competitive with—or outperform—other implementations similar algorithms. paper presents overview design choices pomegranate enabled complex features supported simple code. code available https//github.com/jmschrei/pomegranate python ecosystem becoming increasingly popular processing analysis data. popularity part easy-to-use libraries numpy scipy matplotlib provide fast general purpose functionality. however equally important libraries built provide higher level functionality pandas data analysis scikit-image computer vision theano efﬁcient evaluation mathematical expressions gensim topic modeling natural language processing countless others. naturally many machine learning packages also developed python including implement classic machine learning algorithms scikit-learn mlpy shogun xgboost pomegranate ﬁlls python ecosystem encompasses building probabilistic machine learning models utilize maximum likelihood estimates parameter updates. several packages implement certain probabilistic models style individually hmmlearn hidden markov models libpgm bayesian networks scikit-learn gaussian mixture models naive bayes models. however pomegranate implements wider range probabilistic models modular fashion packages main effects. ﬁrst addition probability distribution pomegranate allows models built using distribution immediately. second improvements aspect pomegranate immediately propagate models would aspect. example support added multivariate gaussian distributions immediately meant models multivariate gaussian emissions could accelerated without additional code. pomegranate currently includes library basic probability distributions naive bayes classiﬁers bayes classiﬁers general mixture models hidden markov models bayesian networks markov chains well implementations factor graphs k-means++/|| used individually primarily serve helpers primary models. several already existing python libraries implement bayesian methods probabilistic modeling. include limited pymc pystan edward pyro emcee bayesian approaches typically represent model parameter probability distribution inherently capturing uncertainty parameter whereas maximum likelihood approaches typically represent model parameter single value. example distinction mixture model either represented probability distributions vector prior probabilities probability distributions probability distributions respective parameters dirichlet distribution representing prior probabilities. ﬁrst representation typically speciﬁes models faster train perform inference with second illustrative type models could build packages implement bayesian methods pymc. representations strengths weaknesses pomegranate implements models falling solely ﬁrst representation. pomegranate designed easy sacriﬁcing computational efﬁciency. models either speciﬁed writing components individually known beforehand learned directly data not. features out-of-core learning parallelization toggled model independently deﬁnition method calls typically simply passing optional parameter. core computational bottlenecks written cython release global interpreter lock enabling multi-threaded parallelism typically python modules cannot take advantage lastly linear algebra operations matrix-matrix multiplications implemented using blas ability toggle present. comparisons computational server intel xeon cores clock speed tesla running centos software used pomegranate scikit-learn v... pomegranate installed using install pomegranate conda install pomegranate platforms. pre-built wheels available windows builds removing sometimes difﬁcult requirement working compiler. pomegranate provides simple consistent implemented models mirrors scikit-learn closely possible. important methods from_samples predict probability method given data optional weights update parameters already initialized model using either maximum-likelihood estimates expectation-maximization appropriate. contrast from_samples method create model directly data manner similar scikit-learn’s method. simple models like single distributions corresponds input data models corresponds initialization step plus call initialization range using k-means mixture models structure learning bayesian networks. predict method returns posterior estimate argmaxm identifying likely component model sample. probability method returns likelihood data given model methods include predict_proba returns probability component sample predict_log_proba returns previous value summarize from_summaries jointly implement learning strategies detailed below. pomegranate supports many learning strategies employed training including outof-core learning massive datasets semi-supervised learning datasets mixture labeled unlabeled data minibatch learning. addition employ multithreaded parallelism data-parallel speedups. features made possible separating collection sufﬁcient statistics data actual parameter update step sufﬁcient statistics smallest numbers needed calculate statistic dataset. example ﬁtting normal distribution data involves calculation mean variance. sufﬁcient statistics mean variance weights out-of-core learning additive nature sufﬁcient statistics means summarize batches data successively sufﬁcient statistics together would sufﬁcient statistics calculated full data set. presents intuitive handle data sets large memory chunking dataset batches memory summarizing successively adding calculated sufﬁcient statistics together afterwards. done passing batch_size parameter training method example model.fit would train pre-initialized model data memory successively summarizing batches size full data seen. summarize from_summaries methods also used independently implement custom out-of-core strategies. minibatch learning natural extension out-of-core strategy minibatch learning parameter update done batches instead full data set. contrast batch methods calculate update using entire dataset stochastic methods typically update using single sample. minibatching speciﬁed passing values batch_size batches_per_epoch parameters using from_summaries batches_per_epoch number batches consider making update. semi-supervised learning semi-supervised learning task ﬁtting model mixture labeled unlabeled data. typically arises situations labeled data sparse unlabeled data plentiful would like make learn informed model. pomegranate supports semi-supervised learning hiddenmarkovmodel bayesclassifier naivebayes models combination mle. models initialized using labeled data. next version used combines sufﬁcient statistics calculated labeled data using sufﬁcient statistics calculated unlabeled data using em-based approach compares favorably scikit-learn. demonstrate generate dataset samples dimensions overlapping gaussian ellipses means respectively standard deviations took pomegranate learn gaussian naive bayes model iterations learn multivariate gaussian bayes classiﬁer full covariance matrix iterations whereas scikit-learn label propagation model kernel converge iterations took kernel neighbors. pomegranate models achieved validation accuracies whereas scikit-learn models better chance. parallelism another beneﬁt additive sufﬁcient statistics presents clear data-parallel parallelize model ﬁtting. simply would divide data several batches calculate sufﬁcient statistics batch locally. sufﬁcient statistics added together back main parameters updated accordingly. implemented dividing data batches running summarize using separate threads running from_summaries threads ﬁnish. typically global interpreter lock python prevents multiple threads running parallel python process. however since computationally intensive aspects written cython released allowing multiple threads once. synthetic data samples dimensions takes seconds train gaussian naive bayes classiﬁer using pomegranate thread seconds threads. comparison takes seconds train gaussian naive bayes classiﬁer using scikit-learn. another synthetic data samples dimensions takes pomegranate learn gaussian mixture model full covariance matrix thread threads threads using gpu. lastly compared speed pomegranate hmmlearn could train state dense gaussian hidden markov model diagonal covariance matrices. synthetic data sequences containing dimensional observations took hmmlearn iterations baum-welch training took pomegranate thread threads threads. pomegranate aims niche python ecosystem exists classic machine learning methods bayesian methods serving implementation ﬂexible probabilistic models. design choices made early building pomegranate allowed great number useful features added later without signiﬁcant effort. clear area improvement future handling missing values many probabilistic models intuitively modify algorithm infer missing values. example trying learn bayesian network dataset missing values identify best structure incomplete dataset infer missing values relearn structure iterating convergence. given prevalence missing data real world extending pomegranate handle missing data efﬁciently priority. would like ﬁrst acknowledge contributors users pomegranate without project would possible. would also like acknowledge adam novak wrote ﬁrst iteration hidden markov model code. lastly would also like acknowledge william noble suggestions guidance development. work partially supported igert grant dge-.", "year": 2017}