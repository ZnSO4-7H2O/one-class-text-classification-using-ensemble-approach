{"title": "Transformation Properties of Learned Visual Representations", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "When a three-dimensional object moves relative to an observer, a change occurs on the observer's image plane and in the visual representation computed by a learned model. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show, using the theory of group representations, that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a striking relationship between irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decorrelated. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the non-commutative 3D rotation group SO(3).", "text": "three-dimensional object moves relative observer change occurs observer’s image plane visual representation computed learned model. starting idea good visual representation transforms linearly scene motions show using theory group representations representation equivalent combination elementary irreducible representations. derive striking relationship between irreducibility statistical dependency structure representation showing restricted conditions irreducible representations decorrelated. partial observability induced perspective projection scene onto image plane motion group linear action space images becomes necessary perform inference latent representation transform linearly. idea demonstrated model rotating norb objects employs latent representation noncommutative rotation group much written invariant representations invariance groups translations rotations projective transformations indeed important object recognition. however general purpose visual representation capable supporting recognition tasks also motion understanding geometrical reasoning invariance enough. instead transformation properties representation crucially important. could understand given representation visual data transforms various rigid non-rigid transformations latent scene would better position build integrated system computes invariant representations well motion relative poses objects. however performing mathematical analysis transformation properties example hidden layer deep neural network motions scene extremely complicated. better approach directly impose good transformation properties representation space learn mapping data representation space transformation properties realized paper study transformation properties distributed representations using tools group representation theory relate transformation properties distributed representation statistical notions decorrelation conditional independence assumption complete observability. partial observability becomes necessary introduce latent variables order obtain representation space good transformation properties. propose number transformation properties good representation have present simple model demonstrates idea modelling rotations objects norb dataset model uses single latent vector coefﬁcients represent images object seen different poses uses latent element rotation group pose. generative neural network model maps transformed latent representation image. unlike previous work learning group representations model assume linear action group input space instead acts linearly latent representation scene. furthermore model ﬁrst learned group model properly deal non-commutative transformations. rest paper organized follows. next section introduce concept group representation core analysis. section three contains main theoretical results dependency structure irreducible representations. followed discussion problems arise partial observability taken account section section presents model training algorithm learning latent group representations followed experiments related work conclusion. start basic assumption learning agent situated space space contains scene. formally represent scene function point space gives list numbers describing example color transparency value material properties etc. section next assume full observability i.e. known entirely. think vector hilbert space sufﬁciently well-behaved functions. following analysis depend particular data representation provides useful intuition ultimately realistic. vector representation scene numerical values would store computer describe approximate depend what scene represented preferred frame reference. transform reference frame element special euclidean group rigid body motions points space transform g−p. transformation leaves invariant euclidean distances angles areas therefore called symmetry euclidean space. symmetry scene transforms notice linear operator. representation hilbert space generically group representation group invertible linear transformations vector space preserves group structure following sense check deﬁned indeed group representation. requirement forms representation sufﬁcient condition vectors describe thing space requires transform space true particular hilbert space construction given above applies generally learned hand-designed vector space representation. fail hold aspects means transform euclidean space lost example rotations axis might equal identity transformation translations might fail commute. hence want representation linear representation special euclidean group modelling perspective would like understand possible ways achieved. observe representation invertible matrix also representation said equivalent result group representation theory tells every unitary representation equivalent sense simple composition basic building blocks called irreducible representations. representation called irreducible nontrivial subspace mapped onto operators shown unitary representations fully reducible means representation equivalent block-diagonal representation whose blocks irreducible. block-diagonal representation said fully reduced. block identiﬁed index denote write block index component corresponding subspace. irreducible representations important representation learning number ways. firstly using irreducible representations computationally efﬁcient using reducible ones. irreducibility also used deﬁne precisely disentangled representation also bengio shown next section representation simple dependency structure certain conditions met. finally easier compute generators ring polynomial invariants irreducible representation used build invariant representations. kazhdan subset generators build invariant shape descriptors. representation learning often seen form generative modelling goal learn latent variable model simple dependency structure latent space. simplest examples goal learn linear model whose latent variables independent alternatively transformation properties center stage learn representation transforms irreducibly symmetry transformations perspective irreducible representations elementary parts observation vectors constructed. given contrasting conceptualizations representation learning interesting investigate transformation properties representation related statistical properties. section show certain conditions irreducible representations decorrelated even conditionally independent. order gain intuition introduce simple model completely observable system symmetry. states system sufﬁciently well-behaved functions circle. observations generated sampling uniformly distributed rotation angle using rotate template observations practice observe discretized functions ﬁnite number coefﬁcients case linear transformation achieves reduction irreducible representations standard fourier transform indeed decorrelate data this observe thus representation rotation group spectral domain. know linear algebra commuting diagonalizable matrices simultaneously diagonalized henriques discussion). hence fully reduced representation diagonal irreducible representations one-dimensional. diagonal elements complex exponentials follows immediately covariance matrix fourier-transformed data diagonal theorem compact group real vector space fully reduced unitary representation furthermore ﬁxed template distributed uniformly covariance matrix vectors diagonal theorem easily generalized template likely slightly weaker theorem proven locally compact groups here. main concern regarding applicability result type groups spaces applies fact reality orbits sampled uniformly. example sample natural images human face likely appear upright position upside down. assumption uniform sampling orbits hold exactly real datasets nevertheless likely irreducible decorrelated representations similar degree data density invariant group consideration. statistical objective decorrelation makes sense working draws underlying distribution images rather impoverished model visual experience. such think decorrelation independence surrogate objectives deeper structural objective irreducibility. observation vectors times respectively unitary representation compact group construct exponential family whose sufﬁcient statistics given matrix elements irreducible unitary representations shown case compact commutative groups invariance l-norm exponent gaussian unitary transformations results posterior exponential family prior furthermore marginal factorize according irreducible representations model irreducible representation gives conditional partial observability reality observe complete scene projected image model function naively could construct representation perspective projection equivariant possible. reason motion bring entirely structures image. classical computer vision solution sought strong assumptions scene geometry assumption scene planar case obtains representation projective group image plane. assumption leads neat formulas real scenes ﬂat. better approach problem partial observability model variability caused linear action low-dimensional group caused action inﬁnite-dimensional group diffeomorphisms scattering representations bruna mallat achieve simultaneous insensitivity translations diffeomorphisms method achieves good performance texture recognition pattern recognition however diffeomorphisms entirely satisfactory model projected motions either invertible deﬁnition projected motions not. furthermore arbitrarily small scene motions bring arbitrarily bright structures image scattering representations lipschitz continuous scene motions. instead learn prior scenes generative model images given scenes perform inference scenes given images. requiring latent scene transforms representation symmetry group bias model towards representing latent properties scene opposed properties image requiring latent scene transforms irreducibly also obtain simple dependency structure latent space section deﬁne simple model demonstrates idea latent group representation concretely. matrix views object instance model views using single latent vector latent transformation view collect matrix order generate ﬁrst compute pass transformed latent scene neural network. conditional given normal distribution centered output generative neural network maps x-space standard normal prior uniform distribution complete graphical model shown ﬁgure regularization zero-mean gaussian prior neural network weights complete joint probability single instance given matrix irreducible representation index complete irreducible unitary representations obtained decomposing called regular representation acting functions sphere case representation space hilbert space square-integrable functions sphere representation deﬁned function decomposed so-called real spherical harmonic functions write representation maps coefﬁcients shown coefﬁcients corresponding expansion rotated function irreducible. furthermore irreducible unitary representations equivalent obtained way. intuitive understanding transformation properties spherical harmonics consider ﬁgure basis functions linearly combined rotation resulting function expressed linear combination basis functions. corresponds representation. experiments detailed section interested action objects could represented functions compact region space. function decomposed using multiple copies irreducible representation done skibbe context rotation invariant shape descriptors. computation representation matrices turn computation transformation block-structure computation breaks large number relatively small matrix multiplies. matrix elements irreducible representations known wigner d-functions. formulae given matrix elements wigner involve numerically unstable sums many elements large coefﬁcients. quite surprisingly given prominence matrices physical theories long history relatively recent paper introduced novel fast method computing representation matrices basis real spherical harmonics authors paper show basis real spherical harmonics rotation speciﬁed zyz-euler angles computed precomputed symmetric orthogonal block matrix exchanges axes represents z-axis rotation takes simple form figure shows matrix corresponding weight three values naively implemented method computational complexity dimension matrix multiplications. however possible apply matrix vector without explicitly constructing using associativity ˆtzx)))). sparse multiplication ˆtzx takes linear time takes quadratic time. practice many copies relatively low-dimensional representations values much smaller dimensionality latent space hence quadratic complexity concern. hard e-step consists partial maximization respect single instance keeping parameters ﬁxed. initialize latent variables state last iteration perform step gradient ascent trained model norb dataset dataset consists objects generic categories four-legged animals human ﬁgures airplanes trucks cars. category contains instances used last training. instance imaged camera elevations azimuths finally lighting conditions instance yielding total images. data made zero mean contrast normalized whitened retaining variance. used neural network hidden layer containing hidden units. group representation determined choice chose number brackets represents multiplier denotes multiplicity. regularization parameters ﬁgure show model able generate reasonable images angles never seen before. model trained images azimuthal degrees model produce images much smaller angles. work related idea transforming auto-encoders capsules hinton transforming auto-encoder consists many capsules learns recognize visual entity predict pose. pose variables thus explicitly represented model linearly pose variables case model. unlike model transforming autoencoder represents scene content probabilities indicates likelihood preferred visual entity present. binary recognition unit used capsule object corresponds orbit model. single latent space shared multiple visual entities generalization makes possible compute metric relations different objects. approach also deal better symmetric objects possible unambiguously estimate pose motion case translational motion edge-like structure known aperture problem instead trying estimate motion anyway model would represent edge vector whose orbit reduced dimensionality compared non-symmetric objects. said fully connected generative network hard-em algorithm used current model suitable dealing large images consider current model proof concept. scalable linear representation learning system could based group-invariant convolutional network mallat generates distributed representation point locally describes scene content transforming covariant manner. problem object recognition static images steadily approaching solved status start looking towards next frontier. central challenges move away idea images i.i.d. draws underlying distribution begin model dynamics visual world. another challenge generalize effectively examples necessitates exploitation symmetries data distribution. problems require take closer look transformation properties learned visual representations. paper theoretically studied consequences assuming linear representation symmetry group observed latent representation space. shown entire class models understood mathematically shown theory specializes case rotation group. furthermore shown uniform sampling orbits geometrical objective learning linear unitary irreducible representation leads decorrelated representations thereby shedding light common learning objective. references anselmi fabio leibo joel rosasco lorenzo mutch tacchetti andrea poggio tomaso. unsupervised learning invariant representations sample complexity magic sensory cortex framework machine learning? technical report center brains minds machines bergstra breuleux bastien lamblin pascanu desjardins turian wardefarley bengio theano math compiler python. proceedings python scientiﬁc computing conference bruna joan mallat st´ephane. invariant scattering convolution networks. ieee transactions pattern analysis machine intelligence august issn ./tpami... bruna joan szlam arthur lecun yann. learning stable group invariant representations convolutional networks. international conference learning representations january kazhdan michael funkhouser thomas rusinkiewicz szymon. rotation invariant spherical harmonic representation shape descriptors. eurographics symposium geometry processing lecun bottou learning methods generic object recognition invariance pose lighting. proceedings ieee computer society conference computer vision pattern recognition cvpr ./cvpr.. michalski vincent memisevic roland konda modeling deep temporal dependencies recurrent grammar cells. advances neural information processing systems pinchon didier hoggan philip rotation matrices real spherical harmonics general rotations atomic orbitals space-ﬁxed axes. journal physics mathematical theoretical february issn skibbe henrik wang qing reisert marco. fast computation spherical fourier harmonic descriptors complete orthonormal basis rotational invariant representation threeieee international workshop digital imaging modeling dimensional objects. conjunction iccv", "year": 2014}