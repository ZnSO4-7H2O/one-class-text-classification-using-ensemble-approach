{"title": "Image-Text Multi-Modal Representation Learning by Adversarial  Backpropagation", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work.", "text": "works require image-text pair information. assumption image-text pair similar meaning embed image-text pair similar points multi-modal space achieve semantic multi-modal representation. pair information always available several situations. image text data usually exist pair paired manually pairing impossible task. category information exist separately image text. also require paired state manually labeled separately. learning multi-modal representation imagetext pair information narrow approach. because training objective focuses adhering image text image-text pair doesn’t care adhering image text semantically similar diﬀerent pair. image text similar multi-modal feature even though semantically similar. addtion resolving every pair relations bottleneck large training dataset. deal problems multi-modal representation learning bring concept ganin’s present novel method image-text multi-modal representation learning. knowledge work ﬁrst approach applying adversarial learning concept multi-modal learning exploiting image-text pair information learn multi-modal feature. category information contrast previous methods using image-text pair information multi-modal embedding. paper show multi-modal feature achieved without image-text pair information method makes similar distribution image text multi-modal feature space methods image-text pair information. show multi-modal feature universal semantic information even though trained category prediction. model end-to-end backpropagation intuitive easily extended multimodal learning work. recently several deep multi-modal learning tasks emerged. image captioning text conditioned image generation object tagging text image search works achieve semantic multi-modal representation crucial part. therefore several works multi-modal representation learning unsupervised image image domain adaptation adversarial backpropagation. adversarial learning concept inspired achieve category discriminative domain invariant feature. extend concept image-text multi-modal representation learning. think image text data covariate shift relation. means image text data semantic information labelling function high level perspective diﬀerent distribution shape. regard multi-modal representation learning process adapting image text distribution distribution retain semantic information time. contrast previous multi-modal representation learning works don’t exploit image-text pair information category information. focus achieving category discriminative domain invariant semantically universal multimodal representation image text. points view multi-modal embedding category predictor domain classiﬁer gradient reversal layer. category predictor achieving discriminative power multi-modal feature. using domain classiﬁer grdient reversal layer makes adversarial relationship embedding network domain classiﬁer achieving domain invariant multi-modal feature. domain invariant means image text distribution multi-modal space. show multi-modal feature distribution well mixed domain means image text multi-modal feature’s distributions multimodal space similar also well distributed t-sne embedding visualization. comparison classiﬁcation performance multi-modal feature uni-modal feature shows exists small information loss within multi-modal embedding process still multi-modal feature category discriminative power even though domain invariant feature after multi-modal embedding. sentence image search result multi-modal feature shows multi-modal feature universal semantic information category information. means within multi-modal-embedding process extracted universal information wordvec vgg-verydeep- removed. paper make following contributions. first design novel image-text multi-modal representation learning method adversarial learning concept. second knowledge ﬁrst work doesn’t exploit image-text pair information multi-modal representation learning. third verify image-text multi-modal feature’s quality various perspectives various methods. approach much generic easily used diﬀerent domain multi-modal representation learning works backpropagation only. several works image-text multi-modal representation learning proposed recent years. speciﬁc tasks little diﬀerent work works’ crucial common part achieving semantic image-text multi-modal representation image text. image feature extraction text feature extraction method diﬀerent work. almost commonly image-text pair information learn image-text semantic relation. many previous approaches ranking loss multimodal embedding. karpathy’s work r-cnn image feature brnn text feature apply ranking loss. approaches vgg-net image feature extracting neural-language-model text feature extracting apply ranking loss triplet ranking loss. generative deep model multi-modal representation learning. methods intentionally miss modality feature generate missed feature modality feature learn relation diﬀerent modalities. therefore also image-text pair information process complicate intuitive. adversarial network concept started concept showed great results several diﬀerent tasks. example dcgan drastically improve generated image quality. text-conditioned dcgan generate related image text. besides image generation approach apply adversarial learning concept domain adaptation ﬁeld gradient reversal layer. domain adaptation pre-trained image classiﬁcation network semantically similar visually diﬀerent domain image target. this category predictor domain classiﬁer adversarial learning network’s feature trained category discriminative domain invariant property. average single feature. represent sentences wordvec embeds word -dimensional semantic space. feature extraction process words sentence converted wordvec vectors -dimensional vector. sentence contains words feature whose covariate shift primary assumption domain adaptation ﬁeld assumes source domain target domain labelling function mathematically diﬀerent distribution form. theoretical work domain adaptation within covariate shift relation source target domain. assume image text also covariate shift relation. assume image text semantic information diﬀerent distribution form. multimodal embedding process adapting distributions retain semantic information time. network structure divided parts feature extraction multi-modal representation learning. former part aims transforming modality signal feature. latter part devised embed feature representation single space. representation visual features pre-trained imagenet. extract image features re-size image image feature sentence feature apply transformations sent images sentences respectively embed features single d-dimensional space. sent embedding image feature activation. since sentence feature -dimensional apply textcnn make possible embedded space. feature embedding network batch-normalization l-normalization respectively. apply dropout fully-connected layers. embedding process regulated components category predictor domain classiﬁer gradient reversal layer similar concept category predictor regulates features multi-modal space multi-modal features discriminative enough classiﬁed valid categories. meanwhile domain classiﬁer gradient reversal layer makes multi-modal features invariant domain. adopt concept layer backward pass reversing gradient values. layer’s input output identity matrix forward pass shown equation backward pass loss network gradient subject shown equation adaptation factor amount domain invariance want figure adversarial multi-modal embedding network structure. arrows mean backpropagation loss. feature extraction part pre-trained large data multi-modal representation learning part trained coco dataset’s scratch. right part ﬁgure t-sne embedding multi-modal feature. domain classiﬁer simple neural network fully-connected layers last sigmoid layer determines domain features multi-modal embedding space. trained discriminates diﬀerence features domains. however since reverses gradient feature embedding networks trained generate features whose domains diﬃcult determined domain classiﬁer. makes adversarial relationship embedding network domain classiﬁer. consequently domain-invariant features generated multi-modal embedding networks. error category predictor domain classiﬁer respectively output last feature embedding layer adaptation factor. adam optimizer training relatively small learning rate that’s empirical diﬃculty generating domain-invariant features regular learning rate. achieve domain invariant feature domain classiﬁer gradient reversal layer properly schedule value positive value. because ﬁrst stage training domain classiﬁer become smart advance adversarial learning process. value increasing domain classifying multi-modal feature become diﬃcult domain classiﬁer become smarter classify correctly. experiment turns proper scheduling important achieve domain invariant feature. exploring many scheduling methods schedule scheme optimal exactly scheduling equation fraction current step training steps. used batch normalization l-normalization normalizing image text feature distribution multi-modal feature layer. experiment without proper normalization seems trained well checking t-sne embedding search result recognize image text feature distribution collapsed achieving domain invariant feature proper normalization process important achieve domain invariant also well distributed multi-modal feature. figure t-sne embedding multi-modal feature image text. trained coco train+validation embedded data samples come coco test set. triplet ranking loss category predictor domain classiﬁer coco test set’s images sentences. result trained triplet ranking loss exploits image-text pair relation. implementing consult wang’s work. fvhglmm sentence representation pre-trained image representation two-branch fully-connected layers multi-modal embedding wang’s did. diﬀerence complex data sampling scheme random data sampling training stage. result trained category predictor domain classiﬁer uses model. image text feature distributions well mixed means image text multi-modal feature’s distributions multi-modal space similar. image text multi-modal features overlapped multimodal space. means semantically similar image text embedded near points multi-modal space. result well mixed domains image-text multi-modal feature distribution. image text overlapped multi-modal space also distributed enough discriminated. means image text similar distribution multi-modal space. think diﬀerence comes diﬀerence training objective. model trained hard classify domain multi-modal feature triplet ranking loss trained adhering image-text pair pushing diﬀerent image-text pair. result means triplet ranking loss adapt image text distribution multi-modal space. result shows model’s training objective suitable learning well mixed domains also well distributed multi-modal feature methods. table category classiﬁcation result coco various modes. image only means vggnet category predictor text only means wordvec textcnn category predictor. image only text only modes don’t include domain classiﬁer gradient reversal layer. image+text multi-modal network model category classiﬁcation precision/recall result shows multi-modal embedding multimodal feature’s category discriminative power decreases little compared multi-modal embedding even though domain invariant feature. means model adapt image text feature distribution multi-modal distribution without large information loss. figure comparison sentence image search result category based sentence image search result. search model care category information universal semantic information even though trained category discriminative multimodal feature. build sentence image search system number coco validation never seen training stage. train images test images simply k-nearest-neighbor search multimodal space computed multi-modal feature. figure shows comparison search result category based search result. sentence query semantic information category label. category based search cannot exploit semantic information search system exploit semantic information sentence query. ﬁgure search system ﬁnds several objects contained category information exists sentence query. ﬁgure search system rightly catches information woman standing trees ﬁeld sentence even though trained predict sentence image training time. means multi-modal embedding process didn’t remove universal information extracted wordvec vgg. also match image text semantically relevant feature multi-modal embedding process. ﬁgure search system thinks similar image query food image category overlapped query’s category. interestingly human’s semantic perspective recognize similar semantic information. figure various search results multi-modal search system. benchmark search system recallk evaluation sentence-to-image image-to-sentence retrieval. this used karpathy’s data split scheme. compare state-of-the-art results model’s performance relatively low. think major reason previous models trained adhering image-text pair pushing diﬀerent image-text pair multimodal space recallk evaluate query’s pair appeared retrieval result. even search result semantically reasonable query’s pair appear retrieval result recallk low. think metric fully appropriate assess search quality. comparison also recallk experiment. proposed novel approach multi-modal representation learning uses adversarial backpropagation concept. method require image-text pair information multi-modal embedding uses category label. contrast almost methods exploit image-text pair information learn semantic relation image text feature. work easily extended multi-modal representation learning method’s future work extending method multi-modal case.", "year": 2016}