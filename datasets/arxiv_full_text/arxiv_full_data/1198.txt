{"title": "Cyclical Learning Rates for Training Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate \"reasonable bounds\" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.", "text": "training. paper demonstrates surprising phenomenon varying learning rate training beneﬁcial overall thus proposes global learning rate vary cyclically within band values instead setting ﬁxed value. addition cyclical learning rate method practically eliminates need tune learning rate achieve near optimal classiﬁcation accuracy. furthermore unlike adaptive learning rates methods require essentially additional computation. potential beneﬁts seen figure shows test data classiﬁcation accuracy cifar- dataset training. baseline reaches ﬁnal accuracy iterations. contrast possible fully train network using method instead tuning within iterations attain accuracy. methodology setting global learning rates training neural networks eliminates need perform numerous experiments best values schedule essentially additional computation. known learning rate important hyper-parameter tune training deep neural networks. paper describes method setting learning rate named cyclical learning rates practically eliminates need experimentally best values schedule global learning rates. instead monotonically decreasing learning rate method lets learning rate cyclically vary reasonable boundary values. training cyclical learning rates instead ﬁxed values achieves improved classiﬁcation accuracy without need tune often fewer iterations. paper also describes simple estimate reasonable bounds linearly increasing learning rate network epochs. addition cyclical learning rates demonstrated cifar- cifar- datasets resnets stochastic depth networks densenets imagenet dataset alexnet googlenet architectures. practical tools everyone trains neural networks. deep neural networks basis state-of-the-art results image recognition object detection face recognition speech recognition machine translation image caption generation driverless technology however training deep neural network difﬁcult global optimization problem. deep neural network typically updated stochastic gradient descent parameters updated loss function learning rate. well known small learning rate make training algorithm converge slowly large learning rate make training algorithm diverge hence must experiment variety learning rates schedules. cyclical learning rates demonstrated resnets stochastic depth networks densenets cifar- cifar- datasets imagenet well-known architectures alexnet googlenet book neural networks tricks trade terriﬁc source practical advice. particular yoshua bengio discusses reasonable ranges learning rates stresses importance tuning learning rate. technical report breuel provides guidance variety hyper-parameters. also numerous websites giving practical suggestions setting learning rates. adaptive learning rates adaptive learning rates considered competitor cyclical learning rates rely local adaptive learning rates place global learning rate experimentation signiﬁcant computational cost possess computational costs used freely. review early work adaptive learning rates found george powell duchi proposed adagrad early adaptive methods estimates learning rates gradients. rmsprop discussed slides geoffrey hinton rmsprop described divide learning rate weight running average magnitudes recent gradients weight. rmsprop fundamental adaptive learning rate method others built schaul discuss adaptive learning rate based diagonal estimation hessian gradients. features method allow automatic method decrease increase learning rate. however paper seems limit idea increasing learning rate non-stationary problems. hand paper demonstrates schedule increasing learning rate universally valuable. zeiler describes adadelta method improves adagrad based ideas limiting squared gradients time limited window making parameter update rule consistent units evaluation relationship update hessian. root mean square statistics variance gradients. dauphin show rmsprop provides biased estimate describe another estimator named esgd unbiased. kingma lei-ba introduce adam designed combine advantages adagrad rmsprop. bache propose exploiting solutions multi-armed bandit problem learning rate selection. summary tutorial adaptive learning rates found recent paper ruder adaptive learning rates fundamentally different policies combined adaptive learning rates shown section addition policies computationally simpler adaptive learning rates. likely similar sgdr method appeared recently. essence learning rate policy comes observation increasing learning rate might short term negative effect achieve longer term beneﬁcial effect. observation leads idea letting learning rate vary within range values rather adopting stepwise ﬁxed exponentially decreasing value. sets minimum maximum boundaries learning rate cyclically varies bounds. experiments numerous functional forms triangular window welch window hann window produced equivalent results adopting triangular window illustrated figure simplest function incorporates idea. rest paper refers triangular learning rate policy. figure triangular learning rate policy. blue lines represent learning rate values changing bounds. input parameter stepsize number iterations half cycle. intuitive understanding methods work comes considering loss function topology. dauphin argue difﬁculty minimizing loss arises saddle points rather poor local minima. saddle points small gradients slow learning process. however increasing learning rate allows rapid traversal saddle point plateaus. practical reason works that following methods section likely optimum learning rate bounds near optimal learning rates used throughout training. curve figure shows result triangular policy cifar-. settings used create curve minimum learning rate maximum also cycle length iterations figure shows accuracy peaks cycle. opt.lr speciﬁed lower learning rate epochcounter number epochs training computed learning rate. policy named triangular described above input parameters deﬁned stepsize code varies learning rate linearly minimum maximum length cycle input parameter stepsize easily computed number iterations epoch. epoch calculated dividing number training images batchsize used. example cifar- training images batchsize epoch iterations. ﬁnal accuracy results actually quite robust cycle length experiments show often good stepsize equal times number iterations epoch. example setting stepsize epoch cifar- training gives slightly better results setting stepsize epoch. furthermore certain elegance rhythm cycles simpliﬁes decision drop learning rates stop current training run. experiments show replacing step constant learning rate least cycles trains network weights running cycles achieve even better performance. also best stop training cycle learning rate minimum value accuracy peaks. simple estimate reasonable minimum maximum boundary values training network epochs. range test; model several epochs letting learning rate increase linearly high values. test enormously valuable whenever facing architecture dataset. triangular learning rate policy provides simple mechanism this. example caffe base minimum value maximum value. stepsize iter number iterations. case learning rate increase linearly minimum value maximum value during short run. next plot accuracy versus learning rate. note learning rate value accuracy starts increase accuracy slows becomes ragged starts fall. learning rates good choices bounds; base ﬁrst value latter value. alternatively rule thumb optimum learning rate usually within factor largest converges base figure shows example making type cifar- dataset using architecture hyper-parameters provided caffe. figure model starts converging right away reasonable base furthermore learning rate accuracy rise gets rough eventually begins drop reasonable whenever starting architecture dataset single range test provides good value good range. compare runs ﬁxed versus range. whichever wins used conﬁdence rest one’s experiments. purpose section demonstrate effectiveness methods standard datasets range architectures. subsections below policies used training cifar- cifar imagenet datasets. three datasets variety architectures demonstrate versatility clr. iterations good setting stepsize section discussed estimate reasonable minimum maximum boundary values learning rate figure needed optimally train network base needed optimally train network. triangular policy shown figure stepsize learning rate bounds shown table running triangular policy parameter setting table shown table obtains test classiﬁcation accuracy iterations triangular policy obtained running standard hyper-parameter settings iterations. table shows adaptive learning rate methods combined ﬁnal accuracy iterations equivalent accuracy obtained without iterations. others necessary iterations obtain similar results. figure shows curves running nesterov method adam method without using adaptive learning rate methods beneﬁts sometimes reduced still valuable sometimes provides beneﬁt essentially cost. residual networks family variations subsequently emerged achieve state-of-the-art results variety tasks. provide comparison experiments original implementations versions three members residual network family original resnet stochastic depth networks recent densenets experiments readily replicated authors papers make torch code available. since three implementation available using torch framework experiments section performed using torch. addition experiment previous section networks also incorporate batch normalization demonstrate value architectures batch normalization. triangular policy derive reducing learning rate accuracy climbs most. test decay policy implemented learning rate starts value linearly reduced base value stepsize number iterations. that learning rate ﬁxed base decay policy base stepsize table shows ﬁnal accuracy providing evidence increasing decreasing learning rate essential beneﬁts method. figure compares learning rate policy caffe range policy using gamma policies. using range policy stop training iteration test accuracy substantially better best test accuracy obtains using learning rate policy. current caffe download contains additional architectures hyper-parameters cifar- particular sigmoid non-linearities batch normalization. figure compares training accuracy using downloaded hyper-parameters ﬁxed learning rate using cyclical learning rate seen figure ﬁnal accuracy ﬁxed learning rate substantially lower cyclical learning rate ﬁnal accuracy clear performance improvement using architecture containing sigmoids batch normalization. experiments carried architectures featuring adaptive learning rate methods clr. table lists ﬁnal accuracy values various adaptive learning rate methods without clr. adaptive methods table invoking respective option caffe. learning rate boundaries given table determined using technique described section lower bound used base ixed policy. densenet+clr table comparison resnets stochastic depth densenets table shows average accuracy runs cifar- cifar- datasets test data training. results datasets three architectures summarized table left column give architecture whether used experiments. columns gives average ﬁnal accuracy runs initial learning rate range used parenthesis reduced training according schedule used original implementation. three architectures original implementation uses initial baseline. accuracy results table right columns average ﬁnal test accuracies runs. stochastic depth implementation slightly different resnet densenet implementation authors split training images training images validation images. however reported results table architecture test accuracies runs. learning rate range used determined range test method cycle length choosen tenth maximum number epochs speciﬁed original implementation. addition accuracy results shown table similar results obtained caffe densenets cifar- using prototxt ﬁles provided authors. average accuracy runs learning rates respectively running within range average accuracy imagenet dataset often used deep learning literature standard comparison. imagenet classiﬁcation challenge provides training images classes giving total labeled training images. caffe website provides architecture hyperparameter ﬁles slightly modiﬁed alexnet downloaded website used baseline. training results reported section weights next estimate reasonable minimum maximum boundaries learning rate figure seen ﬁgure training doesn’t start converging least setting base reasonable. however fair comparison baseline base necessary base triangular triangular policies else majority apparent improvement accuracy smaller learning rate. maximum boundary value training peaks drops learning rate reasonable. comparing range policy policy setting base reasonable case expects average accuracy range policy equal accuracy policy. figure compares results running ixed versus triangular policy alexnet architecture. here peaks iterations multiples produce classiﬁcation accuracy corresponds ixed policy. indeed accuracy peaks cycle triangular policy similar accuracies standard ixed policy implies baseline learning rates quite well shown table ﬁnal accuracies training better accuracies ixed policy. figure shows accuracies range policy oscillate around policy accuracies. advantage range policy accuracy already obtained iteration whereas policy takes iteration reach finally comparison ixed policies table shows ixed triangular policies produce accuracies almost better exponentially decreasing counterparts difference probably tuned gamma. googlenet architecture winning entry imagenet image classiﬁcation competition. szegedy describe architecture detail provide architecture ﬁle. architecture publicly available princeton used following experiments. googlenet paper state learning rate values hyper-parameter solver available baseline hyper-parameters typical situation developing architecture applying network dataset. situation readily handles. instead running numerous experiments optimal learning rates base best guess value ﬁrst step estimate stepsize setting. since architecture uses batchsize epoch equal iterations. hence good settings stepsize would possibly results section based stepsize next step estimate bounds learning rate found range test making epochs learning rate linearly increases ﬁgure shows bounds still model reach convergence. however learning rates cause training converge erratically. triangular range policies base above accuracy peaks learning rate policies correspond learning rate value ixed policies. hence comparisons focus peak accuracies methods. figure compares results running ixed versus triangular policy architecture case peaks cycle triangular policy produce better accuracies ixed policy. ﬁnal accuracy shows improvement network trained triangular policy better accuracy ixed policy. demonstrates triangular policy improves best guess ﬁxed learning rate. results presented paper demonstrate beneﬁts cyclic learning rate methods. short epochs learning rate linearly increases sufﬁcient estimate boundary learning rates policies. policy learning rate cyclically varies bounds sufﬁcient obtain near optimal classiﬁcation results often fewer iterations. policy easy implement unlike adaptive learning rate methods incurs essentially additional computational expense. paper shows cyclic functions learning rate policy provides substantial improvements performance range architectures. addition cyclic nature methods provides guidance times drop learning rate values stop training. factors reduce guesswork setting learning rates make methods practical tools everyone trains neural networks. work explored full range applications cyclic learning rate methods. plan determine equivalent policies work training different architectures recurrent neural networks. furthermore believe theoretical analysis would provide improved understanding methods might lead improvements algorithms.", "year": 2015}