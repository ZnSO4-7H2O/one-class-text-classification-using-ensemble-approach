{"title": "Single-Image Depth Perception in the Wild", "tag": ["cs.CV", "cs.AI"], "abstract": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset \"Depth in the Wild\" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.", "text": "paper studies single-image depth perception wild i.e. recovering depth single image taken unconstrained settings. introduce dataset depth wild consisting images wild annotated relative depth pairs random points. also propose algorithm learns estimate metric depth using annotations relative depth. compared state algorithm simpler performs better. experiments show algorithm combined existing rgb-d data relative depth annotations signiﬁcantly improves single-image depth perception wild. depth single image fundamental problem vision. recent years seen rapid progress thanks data-driven methods particular deep neural networks trained large rgb-d datasets advances broadly impact higher-level tasks. reason many higher-level tasks must operate images wild—images taken constraints cameras locations scenes objects—but rgb-d datasets used train evaluate image-to-depth systems constrained another. current rgb-d datasets collected depth sensors limited range resolution often fail specular transparent objects addition flickr rgb-d images researchers manually capture images. result current rgb-d datasets limited diversity scenes. example depth consists mostly indoor scenes human presence; kitti consists mostly road scenes captured car; maked consists mostly outdoor scenes stanford campus datasets pivotal driving research unclear whether systems trained generalize images wild. possible collect ground-truth depth images wild? using depth sensors unconstrained settings feasible. crowdsourcing seems viable humans good estimating metric depth metric structure general fact metric depth single image fundamentally ambiguous tree behind house slightly bigger away slightly smaller closer—the absolute depth difference house tree cannot uniquely determined. furthermore even cases humans estimate metric depth unclear elicit values them. humans better judging relative depth point closer point often much easier question humans. recent work zoran shows possible learn estimate metric depth using annotations relative depth. although metric depth estimates accurate monotonic transformations well sufﬁciently useful high-level tasks especially occlusion reasoning. seminal results zoran point fronts progress collecting large amount relative depth annotations images wild improving algorithms learn annotations relative depth. paper make contributions fronts. ﬁrst contribution dataset called depth wild consists diverse images annotated randomly sampled points relative depth. sample pair points image minimize redundancy annotation best knowledge ﬁrst large-scale dataset consisting images wild relative depth annotations. demonstrate dataset used evaluation benchmark well training resource second contribution algorithm learning estimate metric depth using annotations relative depth. algorithm signiﬁcantly outperforms zoran also simpler. algorithm zoran ﬁrst learns classiﬁer predict ordinal relation points image. given image classiﬁer repeatedly applied predict ordinal relations sparse point pairs algorithm reconstructs depth predicted ordinal relations solving constrained quadratic optimization enforces additional smoothness constraints reconciles potentially inconsistent ordinal relations. finally algorithm estimates depth pixels assuming constant depth within superpixel. contrast algorithm consists single deep network directly predicts pixel-wise depth network takes entire image input consists off-the-shelf components trained entirely annotations relative depth. novelty approach lies combination ingredients multi-scale deep network produces pixel-wise prediction metric depth loss function using relative depth. experiments show method produces pixel-wise depth accurately ordered outperforming method zoran also state-of-the-art image-to-depth system eigen trained ground-truth metric depth. furthermore combing algorithm dataset existing rgb-d data signiﬁcantly improves single-image depth estimation wild. related work rgb-d datasets prior work constructing rgb-d datasets relied either kinect lidar existing kinect-based datasets limited indoor scenes; existing lidarbased datasets biased towards scenes man-made structures contrast dataset covers much wider variety scenes; easily expanded large-scale crowdsourcing virually umlimited internet images. intrinsic images wild work draws inspiration intrinsic images wild seminal work crowdsources annotations relative reﬂectance unconstrained images. work differs goals well several design decisions. first sample random points instead centers superpixels unlike reﬂectance unreasonable assume constant depth within superpixel. second sample pair points image instead many maximize value human annotations. depth single image image-to-depth long-standing problem large body literature recent convergence deep neural networks rgb-d datasets major advances networks previous works exception trained exclusively using ground-truth metric depth whereas approach uses relative depth. work inspired zoran proposes deep network repeatedly classify pairs points sampled based superpixel segmentation reconstruct per-pixel metric depth solving additional optimization problem. approach different consists single deep network trained end-to-end directly predicts per-pixel metric depth; intermediate classiﬁcation ordinal relations result optimization needed resolve inconsistencies. learning ordinal relations several recent works used ordinal relations intrinsic images wild dataset estimate surface reﬂetance. similar zoran zhou ﬁrst learn deep network classify ordinal relations pairs points make globally consistent energy minimization. narihira learn lightness potential network takes image patch predicts metric reﬂectance center pixel. network applied sparse pixels. although principle lightness potential network applied every pixel produce pixel-wise reﬂectance would quite expensive. making fully convolutional solves partially long lightness potential network downsampling layers case ﬁnal output downsampled accordingly. additional resolution augmentation thus needed. contrast approach completely avoids issues directly outputs pixel-wise estimates. beyond intrinsic images ordinal relations used widely computer vision machine learning including object recognition learning rank gather images flickr. random query keywords sampled english dictionary exclude artiﬁcial images drawings clip arts. collect annotations relative depth present crowd worker image highlighted points which point closer point point hard tell? worker presses respond. many pairs? many pairs points query image? sample image maximizes amount information human annotators. consider extreme—querying possible pairs points image. wasteful pairs points close proximity likely relative depth. words querying pair image less information querying pair image. thus querying pair image cost-effective. pairs? points query given image? simplest would sample random points plane. results severe bias easily exploited algorithm simply classiﬁes lower point image closer depth agree humans time although bias natural makes dataset less useful benchmark. alternative sample points uniformly random horizontal line makes impossible image coordinate cue. another bias algorithm simply classiﬁes point closer center image closer depth agree humans time. leads third approach uniformly sample symmetric points respect center random horizontal line symmetry enforced able simple effective rule based purely image coordinates left point almost equally likely closer right one. ﬁnal dataset consists roughly combination unconstrained pairs symmetric pairs strikes balance need representing natural scene statistics need performance differentiation. protocol results crowdsource annotations using amazon mechanical turk remove spammers insert tasks gold-standard images veriﬁed ourselves reject workers whose accumulative accuracy gold-standard images assign query workers query dataset workers tell relative depth agree other; otherwise query discarded. protocol chance adding wrong answer dataset less measured gold-standard images. processed images obtained valid answers among valid answers unconstrained pairs symmetric pairs. unconstrained pairs takes median seconds worker decide workers agree relative depth time; symmetric pairs numbers numbers suggest symmetric pairs indeed harder. fig. presents examples different kinds queries. learn predict metric depth given annotations relative depth? zoran ﬁrst learn classiﬁer predict ordinal relations centers superpixels reconcile relations recover depth using energy minimization interpolate within superpixel produce per-pixel depth. take simpler approach. idea image-to-depth algorithm would compute function maps image pixel-wise depth. represent function neural network learn end? need ingredients network design outputs resolution input train network annotations relative depth. network design networks output resolution input aplenty including recent designs depth estimation semantic segmentation edge detection common element processing passing information across multiple scales. work variant recently introduced hourglass network used achieve state-of-the-art results human pose estimation consists series figure network design. block represents layer. blocks sharing color identical. sign denotes element-wise addition. block convolution ﬁlter. blocks denote inception module shown figure parameters detailed tab. table parameters type layer network. conv conv sizes ﬁlters used components inception module shown figure.. conv share number input speciﬁed inter dim. convolutions module) downsampling followed series convolutions upsampling interleaved skip connections back features high resolutions. symmetric shape network resembles hourglass hence name. refer reader comparing design related work. purpose particular choice essential various designs mainly differ information different scales dispersed aggregated possible work equally well task. loss function train network using ordinal annotations? need loss function encourages predicted depth agree ground-truth ordinal relations. speciﬁcally consider training image queries location ﬁrst point k-th query location second point k-th query ground-truth depth relation closer equal predicted depth depths point deﬁne loss function essentially ranking loss encourages small difference depths ground-truth relation equality; otherwise encourages large difference. novelty approach novelty lies combination deep network pixelwise prediction ranking loss placed pixel-wise prediction. deep network pixel-wise prediction ranking loss. best knowledge combination proposed before particular estimating depth. training images using superpixel segmentation ground-truth ordinal relations generated comparing ground-truth kinect depth; procedure applied test generate point pairs evaluation training test data zoran table left table ordinal error measures depth. right able metric error measures depth. details metric found versions results eigen using alexnet using vggnet lower better error measures. system zoran network predicts three ordinal relations test pairs equal closer farther report wkdr weighted disagreement rate predicted ordinal relations ground-truth ordinal relations also report wkdr= wkdr= since ground-truth depths almost never exactly same needs relaxed deﬁnition equality. zoran deﬁne points equal depths ratio groundtruth depths within pre-determined range. network predicts equality relation depth difference smaller threshold choice threshold result different values error metrics small pairs predicted unequal error metric equality relations large; pairs predicted equal error metric inequality relations large. choose threshold minimizes maximum three error metrics validation held training set. tab. compares network versus zoran network trained data outperforms three metrics. following also compare state-of-art image-to-depth system eigen trained pixel-wise ground-truth metric depth full depth training compare fairly give network access full depth training set. addition remove limit point pairs training image placed zoran available pairs. results tab. show network achieves superior performance estimating depth ordering. granted comparison entirely fair optimized predicting ordinal relations. comparison still signiﬁcant shows acomputed using implementation based deﬁnition given wkdr stands weighted kinect disagreement rate; weight code released zoran indicates train random subset pairs image train relative depth rival state-of-the-art system estimating depth monotonic transformations. figure. show qualitative results example images used zoran although imperfect recovered metric depth method overall reasonable qualitatively similar state-of-art system trained ground-truth metric depth. metric error measures. network trained relative depth unsurprising well estimating depth ordering. good estimated depth terms metric error? thus evaluate conventional error measures rmse compares absolute depth values ground truths. network trained relative depth know range ground-truth depth values make error measures meaningful normalize depth predicted network mean standard deviation mean depth training set. tab. reports results. metric error measures network still outperforms method zoran addition metric error worse current state-of-the-art comparable earlier methods access ground-truth metric depth. superpixel sampling versus random sampling. compare method zoran train network using point pairs pairs centers superpixels superpixel segmentation necessary? simply train randomly sampled points? answer question train network randomly sampled points. constrain distance points pixels distance similar centers neighboring superpixels. results included tab. using pairs image already achieves comparable performance method zoran using twice four times many pairs improves performance signiﬁcantly outperforms worth noting experiments test pairs still superpixels training random pairs incurs mismatch training testing distributions. still achieve comparable performance despite mismatch. shows method indeed operate without superpixel segmentation. section experiment depth wild dataset. split dataset training images test images report whdr methods tab. state-of-the-art system eigen trained full depth; network trained full depth network pre-trained full depth ﬁne-tuned network trained scratch baseline method uses location query points classify lower point closer guess randomly points height best result achieved pre-training depth ﬁne-tuning diw. training depth work well expected depth indoor scenes. training scratch achieves slightly better performance trained depth despite using much less supervision. pre-training depth ﬁne-tuning leaverages available data achieves best performance. shown fig. quality predicted depth notably better ﬁne-tuning especially outdoor scenes. results suggest promising combine existing rgb-d data crowdsourced annotations advance state-of-the single-image depth estimation. studied single-image depth perception wild recovering depth single image taken unconstrained settings. introduced dataset consisting images wild annotated relative depth proposed algorithm learns estimate metric depth supervised relative depth. shown algorithm outperforms prior algorithm combined existing rgb-d data relative depth annotations signiﬁcantly improves single-image depth perception wild. acknowledgments work partially supported national science foundation grant", "year": 2016}