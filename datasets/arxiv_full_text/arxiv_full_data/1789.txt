{"title": "Automatic Node Selection for Deep Neural Networks using Group Lasso  Regularization", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We examine the effect of the Group Lasso (gLasso) regularizer in selecting the salient nodes of Deep Neural Network (DNN) hidden layers by applying a DNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of gLasso regularization, one for outgoing weight vectors and another for incoming weight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096 nodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment results demonstrate that our DNN training, in which the gLasso regularizer was embedded, successfully selected the hidden layer nodes that are necessary and sufficient for achieving high classification power.", "text": "examine effect group lasso regularizer selecting salient nodes deep neural network hidden layers applying dnn-hmm hybrid speech recognizer talks speech data. test types glasso regularization outgoing weight vectors another incoming weight vectors well sizes dnns hidden layer nodes nodes. furthermore compare glasso regularizers. experiment results demonstrate training glasso regularizer embedded successfully selected hidden layer nodes necessary sufﬁcient achieving high classiﬁcation power. motivated recent rise deep neural networks studied utility speech recognition developing dnnhmm hybrid speech recognizers dnn’s high capability mainly derived deep layer structure massive amount network weight parameters. therefore roughly speaking larger deeper network higher performances are. however large network obviously undesirable viewpoints computational load memory size; also unfavorable viewpoint controlling training robustness unseen data. meet requirement ﬁnding small necessary sufﬁcient structure several approaches reshaped network structure pruned network nodes however methods assumed retraining adapting size-reduced network high discriminative power. therefore need obviously exists developing training methods automatically small sufﬁcient structure. seeking development propose paper training scheme embeds group lasso regularization hidden layer weight vectors. examine feasibility implement large-scale dnn-hmm speech recognizer conduct various evaluation experiments difﬁcult task talks. quite recently glasso-based idea similar reported however independently studied elaborated characteristics scheme large-scale experiments. section discuss relationship work related works including consider standard l-layer layer input layer l-th layer output layer layers hidden layers. network l-th layer consists nodes associated bias coefﬁcient. nodes also fully connected among adjacent layers connection associated weight parameter. weight matrix l-th layers bias vector l-th layer output vector layer activation vector l-th layer activation function. here i-th j-th column element weight parameter j-th node layer i-th node l-th layer; vectors detailed follows respectively aggregated input i-th node l-th layer output j-th node layer bias i-th node l-th layer. figure illustrates example situation node connections between layer l-th layer shown left right sides basically group connections ways outgoing bundle incoming bundle. based groupings group corresponding weights outgoing weight vectors incoming weight vectors here outgoing weight vectors column status corresponds minimum provides high classiﬁcation power selected network nodes. status minimize using standard gradient-based optimization procedure. gradient-based loss minimization gradient terms regularizers play role accelerating minimization. regularizer case gradient given follows example terms clearly indicates effects glasso regularization; weight vector rapidly converges zero norm small. result node outputs less useful minimization disabled early stage loss minimization retained node outputs dominantly used loss minimization. weight vector norm becomes sufﬁciently small loss minimization simply prune corresponding nodes using threshold value node selection procedure summarized follows calculate glassoo norm glassoi norm hidden layer nodes prune nodes whose norm values less shift bias pruned node upper layer nodes especially glassoi case. evaluate proposed training node selection method conducted speech recognition experiments using dnnhmm hybrid speech recognizer talks corpus consists lecture speech data spoken speakers. evaluation experiments divided data following three sets training data speakers validation data speakers testing data speakers. total length training data hours number senone classes vocabulary size represented input speech series acoustic feature vectors based mel-scale frequency cepstrum coefﬁcients dimensions. details input pattern deﬁnitions. front-end part -layer multi-layer perceptron sigmoid activation function post-end part used context-dependent acoustic model -gram language model. initialized part pre-training independently part developed part using boosted training main target training increase senone classiﬁcation accuracy whose categories indicated outputs. however also evaluated training results using word error rate obtained post-end part hybrid recognizer. nearly zero exceeds zero essentially based norm i.e. outputs node layer. situation even remove node large changes retains performances gained node pruning. hand example norm nearly exceeds zero essentially based zero norm situation even prune node large changes performances accordingly remain almost same. however node pruned based weight vector norm bias remains constant. therefore remember shift bias pruned node upper layer nodes. above regardless weight vector grouping control values weight vector norm training precisely selectively minimize norm values also achieve mechanism automatically selects nodes without changes performances trained dnns. node selection naturally expected done salient nodes produce useful outputs increasing dnn’s performances retained. training done concurrently minimize loss function reﬂects dnn’s classiﬁcation errors function reﬂects norm values outgoing/incoming weight vectors unnecessary nodes produce useless outputs automatically disabled corresponding small norm values salient nodes retained. implement training adopt glasso regularization outgoing incoming weight vectors standard training using cross entropy loss. focusing outgoing weight vector case formalize proposed training scheme below. resulting formalization basically common incoming weight vector case. note target layer grouping different types grouping. trainable parameters i.e. loss ˆrgroup respec{{wl bl}l tively glasso regularizers positive constants control effects corresponding regularizers. glasso regularizer speciﬁed next observe characteristics weight vector norm values obtained training. figure shows histogram outgoing weight vector norm values glassoo cases dnn. horizontal axis indicates weight vector norm value vertical axis indicates frequency corresponding norm value. ﬁgure obtained following ﬁndings weight vector norm values glassoo cases clearly separated groups suggesting ease threshold node selection them; weight vector norm values reduced glassoo procedure rather small distinct reduction proves glassoo process successfully reduced norm values selected weight vectors. weight vector norm values case concentrated region implies procedure failed reduce weight vector norm values. executed node selection setting clearly norm values either separate regions generated glasso training. using threshold pruned nodes hidden layers glassoo case nodes glassoi case. performances gained node selection basically gained node selection comparison clearly proves that regardless type weight vector grouping glasso training sorted weight vectors selectively reducing norm values weight vectors allowed node selection simply based thresholding. figure shows pruning certain amount nodes easy l-based dnns setting threshold. comparison removed small-norm nodes produced common l-regularized training case number removed nodes became identical glassoo case similarly case removed nodes l-based network used case. scores scaa wera columns table obtained using size-reduced l-based dnns. much worse gained node selection clearly show cases failed sort weight vectors analyze effects regularizer selection observed changes scores along gradually removing hidden layer nodes nodes increasing order norm values. figure shows scores function gradient-based training minimizing several times along changing hyper-parameters nevertheless simplicity restricted every training repeated twenty epochs whole training samples used once. among different trained statuses selected produced highest value validation data evaluated values selected status testing data measured values testing data using selected hybrid recognizer. addition experiments proposed training associated glasso regularizer also tested training using regularizer weight matrixes bias vectors. then loss replaced additional case elaborate effects proposed training method also tested different sizes nodes every hidden layer another nodes every hidden layer evaluated following four types training glasso regularizer outgoing hidden layer weight vectors glasso regularizer incoming hidden layer weight vectors regularizer outgoing hidden layer weight vectors regularizer incoming hidden layer weight vectors particularly cases conducted common l-regularized training selected different nodes based differences grouping. moreover stated used four cases regularizer bias vectors; also used regularizer input layer weights glassoo case output layer weights glassoi case. table summarize values achieved four training cases glassoo glassoi scab werb represent values obtained node selection; values obtained node selection scaa wera. first focus results scab werb columns. regardless weight vector grouping type regularizer type training achieved accurate classiﬁcation performances less also show glasso procedures achieved performance competitive procedures widely used neural network training. number removed nodes. ﬁgure horizontal axis indicates number removed nodes; scores vertical axis. nodes exist small norm value region scores gradually decrease nodes removed. hand nodes exist small norm value region values remain high later decrease. curves ﬁgure clearly prove procedure corresponds former situation glasso procedure achieves latter desirable situation. introducing glasso’s training scheme stated disposable nodes disabled early training stage. investigate point observed number disposable hidden layer nodes every hidden layer function training epoch. deﬁne disposable node whose norm value smaller figure illustrates observed numbers hidden layers glassoo-based dnn. found following node selection stably realized training progress. occurred comparatively early epochs epochs. dominantly occurred higher hidden layers results figure suggest glasso procedure shows trend mainly reduces weight vector norm values higher hidden layers. scrutinize trend cases pruned disposable nodes using threshold observed number selected nodes every hidden layer. figure illustrates numbers. regardless weight vector grouping size found glasso procedure suppressed weight vector norm values speciﬁcally hidden layers achieved size-reduced reshaped network structure shared tested cases glassoo glassoi dnn. low-rank approximation) replacing small eigenvalues decomposed diagonal matrix zero approach allows parameter reduction without serious degradation original discriminative power network. however reduce number hidden layer nodes therefore probably insufﬁcient achieve desirable amount network parameter reduction especially case using network huge number hidden layer nodes. directly pruning hidden layer nodes norm weight vectors used important function. resembles weight-grouping-based norm proposed training procedure however mechanism reducing norms training procedure implemented restructuring rely additional retraining results. desirable structure genetic algorithm applied high potential generating target structure often needs enormous amount computational resources. compared related studies proposed training embeds glasso regularizer characterized main advantages even additional training conducted easily produce node-pruned dnns without performance degradation computation time almost conventional regularization. motivated glasso procedure deﬁned linear regression approach similar proposed glasso regularizer deﬁned outgoing vector grouping effect evaluated various smallmiddle-sized tasks classify ﬁxed-dimensional patterns. compared work studied paper incoming outgoing vector groupings also elaborated proposed scheme large-scale speech recognition environment. here incoming vector grouping considered ﬁltering function appropriate node selection restructuring convolutional neural networks. investigated feasibility glasso procedure i.e. training associated glasso regularizer hidden layer weight vectors large-scale speech recognition task dnn-hmm hybrid speech recognizer talks task. elaborated nature glasso training applying outgoing incoming weight vector groupings compared glasso procedures regularizer used weight matrixes bias vectors. experiment results clearly demonstrated glasso procedure automatically disabled less useful hidden layer nodes without degradation classiﬁcation performance successfully produced small necessary sufﬁcient structure. ochiai matsuda watanabe kawai katagiri bottleneck linear transformation network adaptation speaker adaptive training-based hybrid dnn-hmm speech recognizer proc. icassp", "year": 2016}