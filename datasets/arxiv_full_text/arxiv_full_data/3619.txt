{"title": "Adversarial Active Learning for Deep Networks: a Margin Based Approach", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We propose a new active learning strategy designed for deep neural networks. The goal is to minimize the number of data annotation queried from an oracle during training. Previous active learning strategies scalable for deep networks were mostly based on uncertain sample selection. In this work, we focus on examples lying close to the decision boundary. Based on theoretical works on margin theory for active learning, we know that such examples may help to considerably decrease the number of annotations. While measuring the exact distance to the decision boundaries is intractable, we propose to rely on adversarial examples. We do not consider anymore them as a threat instead we exploit the information they provide on the distribution of the input space in order to approximate the distance to decision boundaries. We demonstrate empirically that adversarial active queries yield faster convergence of CNNs trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.", "text": "propose active learning strategy designed deep neural networks. goal minimize number data annotation queried oracle training. previous active learning strategies scalable deep networks mostly based uncertain sample selection. work focus examples lying close decision boundary. based theoretical works margin theory active learning know examples help considerably decrease number annotations. measuring exact distance decision boundaries intractable propose rely adversarial examples. consider anymore threat instead exploit information provide distribution input space order approximate distance decision boundaries. demonstrate empirically adversarial active queries yield faster convergence cnns trained mnist shoe-bag quickdraw datasets. efﬁciency deep networks mainly known typical training procedures large datasets. however gathering annotating huge dataset supervised learning prohibit expansion deep networks towards ﬁelds chemistry medicine possible solution build online efﬁcient reduced training rely active learning. active learning family methods seeking optimize automatically training task hand order limit need human annotation. active learning strategies motivated theoretical works demonstrating model perform better using less labeled data data model-crafted also proven efﬁciency wide range machine learning procedures preference rating information user movie recommendation system classifying medical data often requires high cost labeling recently active learning investigated deep networks especially cnns. question scale active learning deep networks raised diverse range topics image classiﬁcation sentiment classiﬁcation dialogue generation works converge common assessment efﬁciency active learning reduce need large labeled training set. transposing directly existing active learning deep networks intuitive. first scaling high dimensional parameters networks turn intractable classic active learning methods optimal experiment design require inverse hessian matrix models iteration would intractable current standard cnns. secondly standard active learning strategy rely uncertainty measure. uncertainty deep networks usually evaluated network’s output however known misleading. indeed discovery adversarial examples demonstrated measuring uncertainty overconﬁdent. adversarial examples inputs modiﬁed small speciﬁc perturbations result unexpected misclassiﬁcation despite strong conﬁdence network predicted class. hand existence adversarial examples somehow discards uncertainty-based selection efﬁcient active learning criterion deep networks. hand magnitude adversarial attacks provide information sample decision boundaries deep network. information relevant active learning known margin-based active learning. generic margin-based active learning assume decision boundaries evolve towards optimal solution training increases. hence samples lying farthest decision boundaries need labeled human expert long current model consistent predictions optimal solution. order reﬁne current model margin-based active learning queries unlabeled samples lying close decision boundary. balcan demonstrated signiﬁcant beneﬁt margin-based approaches reducing human annotations speciﬁc cases obtain exponential improvement human labeling. however requires computing distance sample decision boundaries tractable considering deep networks. although approximate distance considering minimal distance samples different classiﬁcation regions evaluation computationally expensive provides close upper bound real criterion. eventually minimal adversarial perturbation sample provide better upper bound sample decision boundaries. article consider adversarial examples threat rather guidance tool query data. work focuses active selection criterion based sensitiveness unlabeled examples adversarial attacks. speciﬁcally contributions twofold present heuristic margin-based active learning deep networks called deepfool active learning method queries unlabeled samples closest adversarial attacks labels unlabeled sample adversarial counterparts well using twice label. pseudo-labeling comes free without introducing corrupted labels training set. empirically demonstrate dfal labeled data used networks designed achieving higher accuracy random selection. best knowledge ﬁrst active learning method deep networks tested property. describe active learning methods section related work. following section adversarial active learning deep-fool attacks describes method dfal finally experiments demonstrate empirically efﬁciency algorithm three datasets considered recent methods active learning deep networks mnist quick-draw shoe-bag achieve state-of-the-art accuracy three tasks methods much faster previous state-of-the-art approaches. review classic active learning methods applications refer reader burr settles main principle active learning methods lies iteratively building training iterative process alternates training classiﬁer current labeled training convergence model asks oracle label points. points queried pool unlabeled data given heuristic use. several heuristics coexist impossible obtain universal active learning strategy effective given task comes deep learning especially many existing active learning heuristics proven effective. example empirically noticed experiments uncertainty selection uncertainty sampling perform worse passive random selection. since uncertainty selection consists querying annotations unlabeled samples lead predictions lowest conﬁdence cost setup simple. thus used deep networks various tasks ranging sentiment classiﬁcation visual question answering named entity recognition uncertainty selection improved pseudo-labeling method called ceal ceal performs uncertainty selection also adds highly conﬁdent samples increased training set. labels samples queried infered network’s predictions. case deal highly accurate network ceal deﬁnitely improve generalization accuracy. however ceal implies hyperparameters threshold prediction’s conﬁdence. threshold badly tuned corrupt training mistaken labels. uncertainty selection also tailored network ensemble either disagreement models sampling distribution weights recently demonstrated dropout equivalent perform inference posterior distribution weights enabling leverage cost training updating multiple models. thus dropout allows sample ensemble models test time perform dropout query committee bayesian active learning proceeded comparison several active learning heuristics among metrics bald maximizes mutual information predictions model posterior consistently outperforms metrics. deep learning algorithms trained local optimization schemes need several sample time consistent impact training. possible solution select samples scores. sener deﬁne batch active learning problem core selection. minimize population risk model learned small labeled subset. propose upper bound linear combination training error generalization error third term denoted core loss. expressive power cnns authors argue ﬁrst terms negligible. therefore population risk would mainly controlled core loss. core loss consists difference average empirical loss points already labeled average empirical loss entire dataset including unlabeled points. considering labels core loss equivalent computing covering radius network prediction. finally sener used mixed integer programming heuristic minimize best covering radius data. thanks method achieve state-of-the-art performance active learning image classiﬁcation. another direction rarely explored deep networks rely distance decision boundaries namely marginbased active learning. assuming problem separable margin reasonable requirement assumed many popular models perceptron adaboost. positive negative data separable tong demonstrated efﬁciency picking example closest decision boundary exploiting geometric distances relevant active learning intuitive cnns since know beforehand geometrical shape decision boundaries. ﬁrst trial proposed expected-gradientlength strategy consists selecting instances high magnitude gradient. samples impact current model parameter estimates likely modify shape decision boundaries. however computing true gradient given sample intractable without ground-truth label. practice approximate gradient expectation gradients conditioned every possible class assignments. balcan demonstrated signiﬁcant beneﬁt margin-based approaches reducing human annotations. illustrate several margin-based active learning heuristics ﬁgure scenario data underlined green queried. especially ﬁgure describes contribution. original case ﬁgure projection unlabeled sample decision boundary determines whether worth query label depending distance sample boundary. margin-based strategies effective require know compute distance decision boundary. distance intractable naive approximation consists computing instead distance sample interest closest neighboring sample different predicted class. approximating distance sample decision boundary distance sample closest neighboring sample different class coarse computationally expensive. instead propose dfal deep-fool based active learning strategy selects unlabeled samples smallest adversarial perturbation. indeed adversarial attacks originally designed approximate smallest perturbation cross decision boundary. hence binary case distance sample smallest adversarial example better approximates original distance decision boundary aforementioned approximation illustrated ﬁgure binary case label sample added training given network prediction. usually adversarial attacks would allow design perturbation requires also know target label however binary case target class attack obvious. network regularize adversarial examples added training become less sensitive small adversarial perturbations. unlike ceal dfal hyperparameterfree cannot corrupt training basic deﬁnition adversarial attacks know sample adversarial attack share label. multi-class context everything different prior knowledge class closest adversarial region belongs inspired strategy done previously could design many perturbations number classes keep smallest perturbation would time consuming. approach discarded. thus consider available techniques adversarial attacks literature look hardest technique counter since provide information margin cases difﬁcult cases. best knowledge carlini methods among hardest attacks counter. however also requires tune several hyperparameters. thus decided deep-fool algorithm compute adversarial attacks dfal indeed deep-fool iterative procedure alternates local linear approximation classiﬁer around source sample update sample crosses local linear decision. algorithm stops updated source sample becomes effectively adversarial sample regarding initial class source sample. comes dfal deep-fool holds three main advantages hyperparameter free runs fast empirically noticed table competitive state-of-the-art adversarial attacks. moreover dfal theoretically motivated robustness neural networks used robustness explain generalization abilities stochastic algorithms. generalize well long sensitiveness adversarial examples bounded average. explain since deep learning methods majority cases involving stochastic optimization mechanisms common schemes used training phase dropout considered stochastic algorithms. therefore adding samples sensitive small perturbations dfal enforces network increase ensemble robustness generalization abilities. algorithm dfal deepfool active learning require initial labeled training examples require initial unlabeled training examples require hyper-parameters train network require number candidates require nquery number data query require norm used require number data label tested algorithms fully supervised image classiﬁcation three datasets considered recent articles active learning deep learning mnist shoe-bag quick-draw mnist grayscale images digits classes. training test contains respectively samples. shoe-bag dataset created handbags shoes datasets. contains images size training along images testing. quick-draw grayscale images google doodle dataset. downloaded four classes face angel dolphin. lead training samples test size samples. assess efﬁciency method cnns lenet used keras theano although tested methods cnns trained cross-entropy dfal used architectures impaired adversarial attacks. bald select random subset unlabeled training ﬁrst nquery samples expected maximize mutual information model parameters. order sample networks approximate posterior weights also applying dropout test time. ceal select whole unlabeled training ﬁrst nquery samples highest entropy network’s prediction. also label unlabeled samples whose entropy lower given threshold labels queried estimated network’s predictions. core-set select random subset unlabeled training nquery samples cover best training based euclidean distance output last fully connected layer. approximate cover problem follow instructions prescribed initialize selection greedy algorithm iterate mixed integer programming subroutine. also handle robustness prescribed authors. used or-tools reproduce subroutine. average results trials plot accuracy test ﬁgure also index table test accuracy achieved active learning methods ﬁxed size training labeled samples. first interesting observation that independently networks datasets active learning methods originally designed singleton query fail always compete random selection result correlations among queries using score selection. comes method dfal tends convergence faster methods always better random selection independently network dataset hence method robust hyperparameters settings active learning methods considering score selection. diverse conﬁgurations ceal worse uncertainty selection hence selects samples high entropy mistaken predictions adds noise training set. unlike ceal whose probability acquiring extra samples depends efﬁciency network dfal holds constant number extra queries depending number queries. moreover dfal creates artiﬁcial data part pool data. example tables ceal used training mnist shoe-bag dfal used thus dfal allows queries also combined ceal observe dfal always remains three best performing active learning methods. deﬁne methods based test error rate labeled training reaches samples. dfal outperformed really slight percentage either pseudo labeling method core-set since core-set designed batch active learning strategy diminishes correlations among queries. order outperform core-set dfal could extended batch setting approach instead selecting score samples could increase diversity using example submodular heuristics considered dfal active methods achieving best accuracy samples. finally table compares effective number annotations real number data required active learning reach test accuracy training full labeled training set. compare dfal best active learning methods samples. regarding score approaches notice dfal always converges smallest number annotations mnist quick-draw comes shoe-bag dfal remains competitive core-set approach ceal overall less training needed. experiments dfal competitive current state-of-the-art method core-set sometimes outperforming large margin hand method interesting coreset considering computational time. indeed main cons raised core-set optimal solution np-hard problem. overcome issue authors used greedy solution known hold -opt bound. then optimize solution using mixed integer programming subroutine iterate improve coverage. constructing also handle weakness k-center namely robustness assume upper limit number outliers. however using robustness prescribed original paper slows active selection. solution selects batch data time method attributes scores unlabeled sample independently another. hence dfal easily parallelized compute adversarial attacks large pool unlabeled samples. demonstrate computational time method dfal core-set table recorded average runtime selecting queries mnist training samples unlabeled pool size sake fairness compare dfal running time core-set approach without robustness notice runtime performance dfal independent size labeled training set. core-set slows data training set. figure evolution test accuracy trained different labeled training compare efﬁciency dfal core-set built lenet transfered data selected dfal lenet achieve better test accuracy data transfered core-set samples converge similar test accuracy data speciﬁcally designed preliminary experiments problem know advance neither model architecture hyperparameters best suited problem. argue network high capacity likely give high accuracy sufﬁcient enough combined human expertise problem several architectures handcrafted speciﬁc tasks available online. still efﬁciency known typical training procedures large datasets. yanyao shen pointed interesting active learning succeed outperforming classical methods named entity recognition using training introducing lightweight architecture. hence using single predeﬁned model active learning optimize training model well optimized task hand. issue inherent active learning. combining model selection active learning investigated shallow models. main issue raised multiple hypotheses trained parallel beneﬁt labeling different training points. hence active learning strategy effective ﬁxed model less efﬁcient random sampling considering model selection. although combining model selection active learning type model non-trivial deep learning owns speciﬁc property transferability adversarial examples towards wide range architectures lead assume decision borders neural networks trained similar tasks overlap. dfal overcomes limitation. indeed well known adversarial attacks handcrafted speciﬁc network used success networks especially considering cnns. reason raised distance network’s decision borders smaller adversarial perturbations. based argument assume dfal queries useful diverse architectures queried for. strate dfal potential baby task ﬁgure recorded shoe-bag adversarial queries lenet training test accuracy achieved lower adversarial active queries designed transfered training achieves better accuracy random selection also reaching annotated samples also better queries active criteria designed compare test accuracy dfal core-set transfered dataset samples table surprisingly transfered queries core-set perform better random. however almost every case transfered queries dfal outperform core-set random exception concerns transfered queries lenet neither dfal core-set succeed outperforming random believe lenet trained quick-draw smoother decision boundary hyperparameter setting. thus would result queries useful training lenet opposite would true. paper propose heuristic dfal perform margin based active learning cnns approximate projection sample decision boundary smallest adversarial attack. demonstrate empirically adversarial active learning strategy highly efﬁcient cnns trained mnist shoe-bag quick-draw competitive state-of-the-art batch active learning method cnns core-set also outperform core-set runtime performance. thanks transferability adversarial attacks dfal promising approach combining active learning model selection deep networks references al-rfou rami alain guillaume almahairi amjad angermueller christof bahdanau dzmitry ballas nicolas bastien fr´ed´eric bayer justin belikov anatoly belopolsky alexander theano python framework fast computation mathematical expressions. arxiv preprint arxiv. asghar nabiha poupart pascal jiang hang. deep active learning dialogue generation. proceedings joint conference lexical computational semantics carlini nicholas wagner david. adversarial examples easily detected bypassing detection methods. proceedings workshop artiﬁcial intelligence security aisec york acm. isbn ----. ./.. http//doi.acm. org/./.. warren james chen xinyun carlini nicholas song dawn. adversarial example defense ensembles weak defenses strong. usenix workshop offensive technologies vancouver usenix association. https //www.usenix.org/conference/woot/ workshop-program/presentation/he. steven rong jianke michael batch mode active learning application medical image classiﬁcation. proceedings international conference machine learning kapoor ashish grauman kristen urtasun raquel darrell trevor. active learning gaussian processes computer vision object categorization. iccv ieee international conference ieee lewis david gale william sequential algorithm training text classiﬁers. proceedings annual international sigir conference research development information retrieval springer-verlag york inc. moosavi-dezfooli seyed-mohsen fawzi alhussein frossard pascal. deepfool simple accurate method fool deep neural networks. proceedings ieee conference computer vision pattern recognition ozan sener silvio savarese. active learning coninvolutional neural networks core-set approach. ternational conference learning representations https//openreview.net/forum? id=haiuk-rw. accepted poster. mingxuan fuxin joonseok zhou lebanon hongyuan. learning multiplequestion decision trees cold-start recommendation. proceedings sixth international conference search data mining szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. international conference learning representations http//arxiv.org/abs/. zahavy bingyi kang alex sivak jiashi feng huan shie mannor. ensemble robustness generalization stochastic deep learning algorithms https //openreview.net/forum?id=ryutyx-. wang keze zhang dongyu zhang ruimao liang. cost-effective active learning deep image classiﬁcation. ieee transactions circuits systems video technology yanyao shen hyokun zachary lipton yakov kronrod animashree anandkumar. deep active learning named entity recognition. international conference learning representations https //openreview.net/forum?id=rywzaz. accepted poster. zhou shusen chen qingcai wang xiaolong. active deep networks semi-supervised sentiment classiﬁcation. international conference computational linguistics", "year": 2018}