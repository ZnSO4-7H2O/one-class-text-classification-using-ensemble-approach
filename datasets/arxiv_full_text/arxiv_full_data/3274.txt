{"title": "Learning the nonlinear geometry of high-dimensional data: Models and  algorithms", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing \"related\" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering.", "text": "information processing literature includes many models geometry high-dimensional data utilized better performance numerous applications dimensionality reduction data compression denoising classiﬁcation motion segmentation geometric models broadly fall categories namely linear models nonlinear models distinction made within categories depending upon whether models prespeciﬁed learned data focus paper latter case since data-driven learning geometric models known outperform prespeciﬁed geometric models linear models dictate data near lowdimensional subspace hilbert space historically preferred within class data-driven models simplicity. models commonly studied rubrics principal component analysis karhunen–lo`eve transform factor analysis etc. real-world data many applications tend nonlinear. order better capture geometry data applications nonlinear generalizations data-driven linear models remain computationally feasible investigated last decades. popular generalizations nonlinear manifold model manifold model also considered kernel subspace model dictates mapping data higher dimensional hilbert space lies near low-dimensional subspace data-driven learning geometric models case commonly studied moniker kernel another popular generalizations linear models unionmodel dictates data near mixture lowdimensional subspaces ambient hilbert space. data-driven learning model commonly carried rubrics generalized dictionary learning subspace clustering hand data-driven learning uoas model often studied umbrella hybrid linear modeling mixture factor analyzers etc. literature encouraging results reported kernel subspace models context number applications remains room improvement models. canonical model example impose constraint collection subspaces underlying data abstract—modern information processing relies axiom high-dimensional data near low-dimensional geometric structures. paper revisits problem data-driven learning geometric structures puts forth nonlinear geometric models data describing related objects/phenomena. ﬁrst models straddles extremes subspace model union-of-subspaces model termed metric-constrained union-of-subspaces model. second models—suited data drawn mixture nonlinear manifolds—generalizes kernel subspace model termed metric-constrained kernel union-of-subspaces model. main contributions paper regard include following. first motivates formalizes problems mc-uos mckuos learning. second presents algorithms efﬁciently learn mc-uos mc-kuos underlying data interest. third extends algorithms case parts data missing. last least reports outcomes series numerical experiments involving synthetic real data demonstrate superiority proposed geometric models learning algorithms existing approaches literature. experiments also help clarify connections work literature clustering. index terms—data-driven learning kernel methods kernel k-means missing data principal component analysis subspace clustering subspace learning union subspaces. last decade modern signal processing machine learning statistics relying fundamental maxim information processing cope data explosion. maxim states real-world data might high-dimensional hilbert space relevant information within almost always lies near low-dimensional geometric structures embedded hilbert space. knowledge low-dimensional geometric structures improves performance many processing tasks also helps reduce computational communication costs storage requirements etc. work supported part grants ccf- ccf- army research ofﬁce grant wnf-- army research robotics subaward. preliminary versions parts work presented ieee international conference acoustics speech signal processing ieee workshop statistical signal processing ieee international conference acoustics speech signal processing ﬁnal contribution involves carrying series numerical experiments synthetic real data justify heuristics models introduced paper. main focus experiments learning geometry data describing similar phenomenon presence additive white gaussian noise missing entries. case real data demonstrate superiority proposed algorithms focusing tasks denoising data clustering data either complete missing entries. results conﬁrm superiority models comparison number state-of-the-art approaches kernel subspace models conclude discussion pointing work related traditional literature geometry also connections literature learning clustering speciﬁcally mixture components within models treated different clusters within data outputs algorithms automatically lead clusters. alternatively could approach mc-uos/mc-kuos learning problem ﬁrst clustering data learning individual subspaces ambient/feature space. however numerical experiments conﬁrm algorithms perform better heuristic approaches. throughout paper bold lower-case bold upper-case letters represent vectors/sets matrices respectively. i-th element vector/set denoted denotes element matrix m-dimensional zero vector denoted identity matrix denoted given denotes submatrix corresponding rows indexed given sets denotes submatrix corresponding rows columns indexed respectively. finally denote transpose trace operations respectively frobenius norm matrix denoted norm vector represented rest paper organized follows. sec. formally deﬁne metric-constrained union-of-subspaces model mathematically formulate datadriven learning problems studied paper. sec. presents algorithms mc-uos learning presence complete missing data. sec. gives details algorithms learning mc-uos feature space corresponding cases complete missing data. present numerical results sec. followed concluding remarks sec. nterest. hand intuit subspaces describing similar data relation grassmann manifold. lack priori constraint learning subspaces describing similar data potential make different methods learning susceptible errors signal-to-noise ratio outliers missing data etc. another limitation model individual linearity constituent subspaces limits usefulness data drawn nonlinear manifold hand kernel subspace model handle manifold data single kernel subspace requires large dimension capture richness data drawn mixture nonlinear manifolds. goal paper improve state-of-the-art datadriven learning geometric data models complete missing data describing similar phenomenon. particular interested learning models data either mildly highly nonlinear. here informally using terms mildly nonlinear highly nonlinear. heuristically nonlinear data cannot represented mixture linear components deemed highly nonlinear. objective regard overcoming aforementioned limitations model kernel subspace model mildly nonlinear data highly nonlinear data respectively. main contributions introduction novel geometric model termed metric-constrained union-of-subspaces model mildly nonlinear data describing similar phenomenon. similar canonical model mcuos model also dictates data near union lowdimensional subspaces ambient space. distinguishing feature mc-uos model also forces constituent subspaces close according metric deﬁned grassmann manifold. paper formulate mc-uos learning problem particular choice metric derive three novel iterative algorithms solving problem. ﬁrst algorithms operates complete data second deals case unknown number dimension subspaces third carries mc-uos learning presence missing data. main contributions extension mc-uos model highly nonlinear data. model also considered generalization kernel subspace model termed metric-constrained kernel union-ofsubspaces model. mc-kuos model asserts mapping data describing similar phenomenon higher-dimensional hilbert space lies near mixture subspaces feature space additional constraint individual subspaces also close feature space. regard formulate mc-kuos learning problem using kernel trick avoids explicit mapping data feature space. addition derive novel iterative algorithms carry mc-kuos learning presence complete data missing data. denoting additive noise. assume without loss generality throughout paper results training begin assume known priori. later relax assumption extend work sec. iii-b case parameters unknown. goal learn using training data equivalent learning collection subspaces approximate training data also close grassmann manifold here pose goal learning mcuos terms following optimization program denoting projection onto subspace notice ﬁrst term forces learned subspaces close other second term requires simultaneously provide good approximations training data. tuning parameter setup provides compromise subspace closeness approximation error. discussion ﬁnding optimal beyond scope paper cross validation used ranges good values tuning parameters problems worth pointing reformulated uoas model simple extension metric deﬁned addition note mathematically similar related problem studied clustering literature fact straightforward show reduces clustering problem union zero-dimensional afﬁne subspaces. remark mc-uos model learning problem motivated follows. consider facial images individuals varying illumination conditions extended yale dataset figs. generally agreed images individual case regarded lying near -dimensional subspace computed straightforward manner using singular value decomposition subspace distance deﬁned used case identify similarlooking individuals. given noisy training images similar individuals traditional methods learning sparse subspace clustering rely approximation error prone errors. fig. provides numerical validation claim shown good performance noisy images different-looking individuals performance degrades case similar-looking individuals mc-uos learning problem hand able handle cases reliably ﬁrst term penalizes subspaces cluster grassmann manifold. refer reader sec. recall canonical model asserts data mdimensional ambient space represented union subspace here make simpliﬁed assumption subspaces dimension i.e. dim) case subspace corresponds point grassmann manifold denotes s-dimensional subspaces canonical model allows arbitrary points basic premise mc-uos model subspaces underlying similar signals likely form cluster grassmann manifold. order formally capture intuition make distance metric deﬁne mc-uos according metric follows. deﬁnition said constrained respect metric ×gms maxp=p dusp) positive constant metric paper measure distances subspaces based hausdorff distance between vector subspace ﬁrst deﬁned speciﬁcally rm×s rm×s denote orthonormal bases subspaces respectively denotes projection operator onto subspace dpdt easy convince oneself invariant choice orthonormal bases subspaces formally shown metric note directly related concept principal angles subspaces. given subspaces orthonormal bases cosines principal angles cosp) deﬁned ordered singular therefore follows dusp) cosp). conclude discussion mc-uos model noting deﬁnitions metrics grassmann manifold exist literature based different manipulations cosp)’s paper however focus ease computation. ﬁrst geometry learning problem corresponds case high-dimensional data near mc-uos ambient space using qualiﬁer mildly nonlinear data since individual components data modeled linear fashion. terms formal characterization assume access collection noisy training samples rm×n every sample expressed belonging practice however solving directly likely computationally intractable extremely high dimensionality feature space. instead interested solving problem mc-uos learning feature space using kernel trick involves transforming learning problem requires evaluations inner products transformation followed mercer kernel positive semideﬁnite function satisﬁes develop algorithms learn mc-uos feature space without explicit mapping training data feature space. term learning mc-uos feature space using kernel trick metric-constrained kernel union-ofsubspaces learning. similar case mcuos learning consider scenarios paper mckuos learning. ﬁrst scenarios corresponds standard setup dimensions training sample observed second scenario corresponds case missing data dimensions training sample remain unobserved. finally evaluate proposed mc-kuos learning algorithms using metric average approximation error noisy test data clustering performance training data either complete missing entries. conclude pointing mc-kuos learning invariably also leads problem ﬁnding preimages data feature space induced chosen kernel also addressed paper. remark worth noting requires knowledge nonlinear however since rely kernel trick mc-kuos learning framework need access appropriate kernel assumed paper kernel readily available learning best kernel training data interesting extension work beyond scope paper. fig. example illustrating limitations existing methods learning noisy training data. ﬁgure shows examples clean facial images four individuals extended yale dataset bottom shows noisy versions images corresponding ground truth distance subspaces individuals subspaces individuals state-of-the-art learning methods trouble reliably learning underlying subspaces whenever subspaces close other. indeed distance subspaces learned algorithm noisy images individuals case similar-looking individuals paper study variants mc-uos learning problem described ﬁrst variant dimensions training sample observed geometry learning problem exactly given second variant assumed dimensions training sample unobserved requires recharacterization learning problem well posed. defer recharacterization sec. iii-c paper. order quantify performance learning algorithms resort generation noisy test data follows. given noiseless data sample noisy test sample given additive noise im). report metric average approximation error noisy test data using learned subspaces synthetic real data. finally case synthetic data drawn mc-uos also measure performance algorithms terms average normalized subspace distances learned true subspaces. defer formal description metrics sec. describes detail setup experiments. second geometry learning problem corresponds case high-dimensional data drawn mixture nonlinear manifolds ambient space basic premise model case data drawn mixture nonlinear manifolds mapped nonlinear higher-dimensional images data modeled lying near mcuos feature space. order learn model assume access collection training samples rm×n fundamental difference mapped training data algorithm metric-constrained union-of-subspaces learning input training data rm×n number subspaces dimension subspaces parameter initialize random orthonormal bases rm×s}l stopping rule wlii address problem resorting blockcoordinate descent updating time keeping dp’s ﬁxed regard suppose process updating ﬁxed subspace update step. deﬁne containing indices denotes stiefel manifold deﬁned collection orthonormal matrices. note intuitive interpretation. reduces problem ﬁnding subspace closest remaining subspaces collection. reduces problem case learning problem reduces subspace clustering problem studied selecting appropriate straddle extremes subspace closeness data approximation. order solve deﬁne symmetric matrix follows closed-form solution involves eigen decomposition speciﬁcally given ﬁrst eigenvectors associated s-largest eigenvalues. order reduce effects noisy training data begin pre-processing step centers data matrix involves deﬁning mean samples subtracting mean next focus simpliﬁcation optimization problem ﬁrst deﬁne indicator matrix identiﬁes memberships yi’s different subspaces further notice psyi rewritten psyi rm×s denotes orthonormal basis therefore deﬁning collection orthonormal bases rewrite mindw objective function given minimizing simultaneously challenging likely computationally infeasible. instead adopt alternate minimization approach involves iteratively solving alternating following steps minimizing ﬁxed term subspace assignment step; minimizing ﬁxed term subspace update stage. begin alternate minimization start initial block rm×s random orthonormal basis. next carry subspace assignment amounts solving setting wlii order move subspace update step matrix focus optimizing however step requires attention since minimizing entire also lead large-scale optimization problem. distance predeﬁned threshold \u0001min mathematically subspace merging step involves ﬁrst ﬁnding pair subspaces satisﬁes \u0001min. merge setting deﬁned algorithm deﬁning symmetric matrix equal ﬁrst smax eigenvectors associated smaxlargest eigenvalues. finally remove process ﬁnding closest pair subspaces merging repeated normalized subspace distance every pair subspaces becomes greater \u0001min. assume subspace merging move onto step estimation dimension subspaces. ﬁrst estimate dimension subspace denoted selected maximum s’s. many efforts literature estimate dimension subspace; e.g. incomplete list. paper focus method given formulates maximum likelihood estimator because noise level unknown problem simple form. however sensitive noise. therefore ﬁrst apply smoothing process using estimator. involves ﬁrst updating using updated algorithm term metric-constrained unionof-subspaces learning given algorithm terms complexity algorithm iteration notice subspace assignment step requires operations. addition total number operations needed compute iteration finally iteration also requires eigen decompositions matrices complexity. therefore computational complexity micusal iteration given conclude discussion pointing cannot guarantee convergence micusal global optimal solution. however since objective function bounded zero micusal ensures increase iteration follows micusal iterates indeed converge local convergence course function initialization micusal. paper advocate random subspaces initialization study impact random initialization sec. v-a. micusal algorithm described sec. iii-a requires knowledge number subspaces dimension subspaces practice however cannot assume knowledge parameters priori. instead must estimate number dimension subspaces training data themselves. section describe generalization micusal algorithm achieves objective. algorithm term adaptive mc-uos learning requires knowledge loose upper bounds denote lmax smax respectively. amicusal algorithm initializes collection random orthonormal bases basis point stiefel manifold vmsmax. similar case micusal carries subspace assignment subspace update steps iterative fashion. unlike micusal however also greedily remove redundant subspaces collection subspaces {s}lmax subspace assignment step. involves removal signals training data assigned subspace step greedy subspace pruning ensures active subspaces survive subspace update step. amicusal algorithm ﬁnishes iterating subspace assignment subspace pruning subspace update move onto step greedy subspace merging involves merging pairs subspaces close even single subspace dimension used well approximate data represented subspaces individually. step greedily merge pairs closest subspaces long normalized subspace step subspace merging needed lack knowledge true number subspaces underlying model. particular assumption merging threshold \u0001min algorithm satisﬁes \u0001min sec. iii-a propose solve problem making alternating minimization comprises subspace assignment subspace update steps. initialize block rm×s random orthonormal basis. next ﬁxed subspace assignment corresponds solving dpdt orthonormal basis hence choice treat mind∈vms optimization problem grassmann manifold note rewrite |c|. here psωc denotes q-th element order minimize employ incremental gradient descent procedure grassmann manifold performs update respect single component step. speciﬁc ﬁrst compute gradient single cost function move along short geodesic curve gradient direction. instance gradient uσvt update respect performed grouse algorithm step size order converge also reduce step size iteration complete discussion presenting learning algorithm missing data algorithm adaptive mc-uos learning input training data rm×n loose upper bounds lmax smax parameters \u0001min. initialize random orthonormal bases rm×smax}lmax stopping rule update using also section study mc-uos learning case training data missing entries. speciﬁc assume observe entries locations given |ωi| denoted r|ωi|. since impossible compute access complete data quantities psyi shown psωi uniformly random high probability long |ωi| slightly greater here psωi motivated ]ωi) this replace yi−psyi kernel assumption rank) matrix positive deﬁnite. similar algorithm begin centering φ-mapped data feature space pre-processing stage. denote mean φ-mapped images rn×s basis representation matrix orthonormal matrix. also easy verify satisfy matrix subspace instead using explicitly computations sufﬁces mc-kuos learning computations involving carried algorithm robust mc-uos learning input training data {ωi}n number subspaces dimension subspaces parameters initialize random orthonormal bases rm×s}l stopping rule algorithm termed robust mc-uos learning deﬁning maxi |ωi| subspace assignment step rmicusal requires ﬂops iteration computing iteration requires operations. next cost updating respect respect therefore follows computational complexity rmicusal iteration refer reader sec. exact running time rmicusal training data. mc-kuos learning highly nonlinear data section present algorithms solve problem mc-kuos learning rm×n highly nonlinear data. ﬁrst generalize micusal algorithm using kernel trick learn mc-kuos complete data. deal case missing data propose kernel function value estimators solve finally discuss problem ﬁnding pre-images data feature space based mc-kuos model sec. iv-c. involves greedily selecting sample training data step inner products largest setting yi∗. list initialization method algorithm referred greedy kernel initial-orthogonalization procedure based assumption linearly independent compute simultaneously difﬁcult optimization problem minimize update sequentially. unlike mc-uos learning however careful since number samples belong change subspace assignment step. particular ﬁrst need initialize rn×s diagonal entries nonincreasing order. deﬁne bases initialization step ready update sequentially manipulations subproblem expressed ready describe algorithm detail. similar micusal minimize alternating minimizing ﬁxed minimizing ﬁxed begin alternate optimization strategy start initializing orthonormal basis subspace. discussed earlier orthonormal basis represented initializing note linear independent vectors deﬁne s-dimensional subspace. therefore initialize need choose samples training φ-mapped images training samples linearly independent feature space. selection samples based intuition inner products samples subspace feature space typically large initialization procedure setup kernel trick clear cannot apply method sec. iii-c mc-kuos learning. however described sec. iv-a solution mc-kuos learning problem using complete data requires computations inner products regard propose estimate kernel function value using incomplete data mathematically goal proxy function derive proxy function start considering relationship context different kernel functions. ﬁrst consider isotropic kernels form resulting r|ωij|. vector authors deﬁne coherence subspace spanned vector ij]ωij |ωij| high probability. leveraging result give following corollary essentially plugging deﬁnition ij]ωij corollary using simple relationship corollary replace distance term isotropic kernel function m|ωij|ωij provide estimate true value using entries correspond only. example gaussian kernel expyi−yj replace expωij −ωij form need estimate using entries corresponding only. order estimator deﬁne yi◦yj coordinate-wise product means equal ij]ωij r|ωij| respectively. entries following lemma describes deviation estimated inner product lemma algorithm metric-constrained kernel union-ofsubspaces learning input training data rm×n number dimension subspaces kernel function parameter compute kernel matrix initialize {c}l detailed algorithm refer metricconstrained kernel union-of-subspaces learning important thing notice complexity mc-kusal scale dimensionality feature space owing kernel trick. section focus mc-kuos learning case training data missing entries input space. setup similar sec. iii-c. observe locations following resulting observed vector denoted r|ωi|. also assume observed indices signal drawn uniformly random replacement note results derived also translated case sampling without replacement example). given missing data aspect thus section discussed mc-uos learning kernel space complete missing data using kernel trick. suppose given noisy sample noise term belongs subspaces l}). information processing tasks needs ﬁrst representation sample terms learned mc-kuos akin denoising denoised sample feature space projection onto given denoised sample ambient space often need project onto input space many applications termed pre-image reconstruction. section consider problem pre-image reconstruction based mc-kuos model. mathematically problem pre-image reconstruction stated follows. given space closest projection onto learned mcuos involves ﬁrst ﬁnding index easily done using kernel subspace assignment step described solve problem leverage ideas inequalities cannot guaranteed hold even. using this trivially obtain theorem below counterpart theorem polynomial kernels. theorem polynomial kernel degree probability least based discussion above estimate kernel function value using associated proxy function utilizes entries belonging only. thus compute estimated kernel matrix rn×n case missing data. positive deﬁniteness guaranteed case. lieu overall learning process includes kernel subspace assignment kernel subspace update stages. approach robust mc-kuos learning conclude section noting also robustify classical kernel case. words pre-image solution linear combination training data weights ei’s explicitly computed using respective kernel functions. regard described sec. iv-b estimate using entries belonging estimated kernel function value denoted hωi). estimated case. speciﬁc deﬁne containing indices samples yi’s whose u-th entry observed. ei)|ru|/n gaussian kernel eiyi polynomial kernel. conclude section noting methods described also applied case test sample missing entries. section present several experimental results demonstrating effectiveness proposed methods data representation. particular interested learning mc-uos complete/missing noisy training data followed denoising complete noisy test samples using learned geometric structures. case mc-kuos learning evaluate performance algorithms focusing denoising complete noisy test samples clustering complete/missing training data. section examine effectiveness mcuos learning using algorithms complete data experiments compare micusal/amicusal several state-of-the-art learning algorithms blocksparse dictionary design k-subspace clustering sparse subspace clustering robust sparse subspace clustering robust subspace clustering thresholding well principal component analysis case learning algorithms codes provided respective authors. case noisy variant optimization program αz/µz experiments deﬁned parameter varies different experiments. case rssc tuning parameter maxn/) provided. case training data missing entries compare results rmicusal k-grouse grouse describe method pre-image reconstrucproblem minimizing tion using gaussian kernel expyi−yj equivalent maximizing function ﬁrst. whose extremum obtained setting denotes gradient respect toz. express φtφeτ r|nτ|. next deﬁne ndimensional vector χ]cτ χ]in\\cτ in\\cτ assigned means i=χκz yi). setting expz expz using approximation relation φz)))) unique pre-image reconstruction using missing data next consider problem reconstructing pre-image training samples missing entries. easily seen solution pre-image order generate noisy training test data experiments start sets clean training test samples denoted respectively. white gaussian noise samples generate noisy training test samples respectively. following denote variance noise added training test samples respectively. missing data experiments every ﬁxed noise variance create training data different percentages missing values number missing entries signal dimension. reported results based random initializations micusal amicusal algorithms. regard adopt following simple approach mitigate stability issues might arise random initializations. perform multiple random initializations every ﬁxed retain learned mc-uos structure results smallest value objective function also similar approach selecting ﬁnal structures returned k-subspace clustering block-sparse dictionary design difference case replaced approximation error training data. ﬁrst synthetic experiments consider subspaces dimension -dimensional ambient space. subspaces deﬁned orthonormal bases rm×s} follows. start random orthonormal basis rm×s every orth− tsw) every entry rm×s uniformly distributed random number orth denotes orthogonalization process. parameter controls distance subspaces experiments. generating subspaces generate points rs×n matrix whose elements drawn independently identically distribution. here hence stack data matrix {xi}n normalize samples unit norms. test data rm×n produced using foregoing strategy. white gaussian noise different expected noise power xte. speciﬁcally ranges generate times monte carlo simulations noisy data repeated times every ﬁxed xte. therefore results reported correspond average random trials. make collection noisy samples learn union subspaces dimension stack learned orthonormal bases {d}l experiments micusal rmicusal complete missing data experiments respectively. number random initializations used select ﬁnal geometric structure experiments every ﬁxed following metrics performance analysis mcuos learning. since knowledge ground truth represented ground truth orthonormal bases ﬁrst pairs estimated true subspaces fig. comparison mc-uos learning performance synthetic data. show relative errors test signals complete missing data experiments micusal rmicusal. numbers legend indicate percentages missing entries within training data. show relative errors test signals micusal rmicusal using different λ’s. average normalized subspace distances pairs i.e. davg smaller davg indicates better performance mc-uos learning. also learned subspaces close ground truth expected good representation performance test data. good measure regard mean relative reconstruction errors test samples using learned subspaces. speciﬁc training data complete ﬁrst represent every test signal relative reconstruction error respect deﬁned noiseless part hand training data missing entries test signal reconstruction error respect simply calculated x−dτ compare learning methods choose micusal rmicusal. complete data experiments perform subspace dimension yields best denoising result training samples. also subspace dimension grouse corresponding missing data experiments. table summarizes davg’s different learning algorithms complete missing data experiments. seen micusal produces smaller davg’s turn leads smaller relative errors test data; fig. validation claim. mc-uos learning missing data rmicusal also learns better mc-uos that ﬁxed percentage number missing observations training data davg rmicusal much smaller k-grouse rmicusal outperforms k-grouse grouse terms smaller reconstruction errors test data. moreover infer fig. ﬁxed number missing entries increases performance rmicusal degrades less compared kgrouse. also test learning performance complete data subspaces close case learning algorithms including micusal learn subspaces successfully. omit plots space constraints. micusal rmicusal also analyze effect parameter learning performance. implement micusal complete data experiments select rmicusal missing data experiments number missing entries training data signal dimension. results shown fig. fig. table davg’s reconstruction errors test data large micusal rmicusal. learned subspaces close other results poor data representation capability learned d’s. algorithms achieve good performance terms small davg’s relative errors test data. increases further davg relative errors test data also increase. furthermore grows curves relative errors test data micusal rmicusal closer ones k-sub k-grouse respectively. phenomenon coincides discussion sec. iii. finally note micusal rmicusal achieve best performance deviations representation errors test data small falls range. next study effect random initialization subspaces micusal performance calculating standard deviation mean reconstruction errors test data random initializations. mean standard deviations ends σte’s addition gets larger variation results increases slightly hand mean standard deviations k-sub furthermore performance gaps micusal methods larger finally learned mc-uos structure results smallest value objective function always results best denoising performance. suggests micusal always generates best results mostly insensitive choice initial subspaces random initialization. also examine running times rmicusal k-grouse iteration include subspace assignment subspace update stages. subspace implement optimization iterations. experiments carried using matlab intel .ghz ram. fourth table observe rmicusal takes slightly time compared k-grouse rmicusal needs steps updating however advantage rmicusal k-grouse learning better signiﬁcantly outweighs slight increase computational complexity. also number missing entries increases algorithms become faster. reason |ωi| decreases less time needed subspace assignment step computing algorithm experiments city scene data show effectiveness proposed approaches test proposed methods real-world city scene data. first study performance methods francisco city hall image shown fig. generate clean training test data split image left right subimages equal size. extract nonoverlapping image patches left subimage reshape column vectors dimension vectors normalized unit norms used fig. comparison mc-uos learning performance francisco city hall data. show relative errors test signals complete data experiments. show relative errors test signals missing data experiments. numbers legend indicate percentages missing entries within training data. show relative errors test signals rmicusal using different λ’s. signals test signals extracted right subimage. white gaussian noise added separately forming respectively. experiments ranges monte carlo simulations noisy data repeated times results reported correspond average trials. note patch treated single signal here goal learn mc-uos every test patch reliably denoised using learned subspaces. perform amicusal training data parameters lmax smax \u0001min number random initializations used arrive ﬁnal mc-uos structure using amicusal every ﬁxed output amicusal always also perform micusal times. fair comparison also method paper dimension subspace case estimated always note state-of-theart learning algorithms amicusal instead using generated algorithms themselves. reason follows. returned therefore reduces setting. output rssc also coincides algorithm. estimation n/)) sensitive noise data. speciﬁcally estimated always results poorer performance compared case training noise levels. missing data experiments rmicusal k-grouse grouse. fig. fig. describe relative reconstruction errors test samples training data complete. micusal amicusal learn better mc-uos since give rise smaller relative errors test data. further average standard deviation mean relative errors test data around micusal k-sub. inferred fig. fig. rmicusal also yields better data representation performance missing data case. complete missing data experiments ﬁrst amicusal without changing parameters. amicusal always returns subspaces reconstruction errors test data slightly larger distances learned subspaces become larger resulting least \u0001min ﬁxed. however relative errors test data still close ones suggests good choice setting since leads smallest number subspaces best representation performance. also perform rmicusal keeping ﬁxed number missing entries training data signal dimension. show relative errors test data fig. fig. similar results experiments synthetic data observe fact small reconstruction errors test data large subspace closeness metric dominates learning uos. results similar. increases further performance rmicusal gets closer k-grouse. report running time rmicusal k-grouse iteration seventh table perform optimization iterations subspace update step rmicusal k-grouse. experiments rmicusal appears much slower k-grouse. however presented fig. fig. performance rmicusal signiﬁcantly better kgrouse. next repeat experiments complete data experiments using paris city hall image fig. forming perform amicusal using parameters previous experiments. estimated case always always estimated dimension subspace always experiments amicusal state-ofthe-art learning algorithms. returned case. estimated rssc usually reconstruction errors test data close ones reported here. apply using estimated n/)) relative errors test data close results shown here. result subspace. relative reconstruction errors test data different training noise levels shown fig. make conclusion methods obtain small errors thereby outperforming algorithms. average standard deviation mean relative errors test data also smaller micusal compared k-sub fig. comparison mc-uos learning performance extended yale dataset. ﬁrst shows images subject second presents images subject show relative errors test data experiments. experiments face dataset section work extended yale dataset contains cropped images subjects. individual images taken varying illumination conditions. downsample images pixels image vectorized treated signal; thus shown images given subject taken varying illumination conditions well represented -dimensional subspace. ﬁrst focus collection images subjects normalize images unit norms. representative images presented ﬁrst fig. assume images three subjects close mc-uos images subject randomly select half training remaining images belong test samples; therefore white gaussian noise obtain random selection generating repeated times conduct monte carlo simulations noisy data times every ﬁxed xte. experiments value fair comparison dimension subspace apply micusal parameter number random initializations experiments micusal k-sub every ﬁxed again observe micusal outperforms learning methods since results smaller relative errors moreover average standard deviation micusal realizations σte’s smaller k-sub micusal ssc. provide evidence fig. micusal yields better data representation performance setting. average standard deviation mean reconstruction errors test data around micusal k-sub case. section evaluate performance mckuos learning approaches terms following problems image denoising using learned mc-kuos clustering training data points. problems consider usps dataset contains collection -dimensional handwritten digits. authors demonstrated using nonlinear features improve denoising performance dataset. unlike experiments mc-uos learning training data experiments noiseless. denoising experiments assume every noisy test sample experiments experiments image denoising denoising experiments compare result mc-kusal three methods kernel k-means clustering test signal ﬁrst assign cluster whose centroid closest followed kernel method calculate pre-image; kernel number eigenvectors mc-kusal kernel number eigenvectors chosen mins ||ps clean noisy test samples respectively. manner number eigenvectors kpca-oracle different different noise levels σte’s. dimension subspaces mc-kusal kernel k-means clustering kpca-fix number subspaces kernel k-means clustering also equals mc-kusal. case missing training data report results rmc-kusal well rkpca. every ﬁxed test noise level dimension subspace rkpca kpca-oracle. relative reconstruction error denotes pre-image respect noisy test sample experiment gaussian kernel parameter choose digits digit select ﬁrst samples dataset experiments. samples vectorized normalized unit norms. samples randomly choose samples class training remaining samples class testing forming random selection test training samples repeated times cross-validation purposes. perform monte carlo trials noisy test data report mean random trials. fig. comparison mc-kuos learning performance usps dataset perform mckusal note kpca-oracle algorithm ideal case kernel pca. numbers legend indicate percentages missing entries within training data. experiments implement mc-kusal parameters learn mc-uos feature space fig. shows mean relative reconstruction errors test data different methods presence complete training data result mc-kusal comparison methods. observe almost noise levels method produces better results methods. case mc-kusal exception second best methods. caveat practice cannot know beforehand dimension subspace feature space kernel yields best denoising result particular noise level. show denoising performance mc-kusal different fig. observe small usually results good performance relatively small increasing slightly improve denoising performance test data gets small. missing data experiments number missing entries training data signal dimension. parameters rmc-kusal. inferred fig. performance rkpca rmc-kusal comparable noise levels; number missing elements ﬁxed rmc-kusal outperforms rkpca test data small vice versa. section empirically compare clustering performance mc-kusal kernel k-means clustering standard k-means clustering spectral clustering training data complete. case spectral clustering make similarity matrix returned noisy variant optimization program union-of-subspaces model. ﬁrst proposed several efﬁcient iterative approaches learning mcuos ambient space using complete missing data. moreover methods extended case higher-dimensional feature space deal mc-uos learning problem feature space using complete missing data. experiments synthetic real data showed effectiveness algorithms superiority state-of-the-art union-of-subspaces learning algorithms. future work includes estimation number dimension subspaces training data mc-uos learning feature space. |ωij|. here denotes random variables prove bound assumption variables drawn uniformly replacement. means independent value variable replaced i.e. possible values changes therefore mcdiarmid’s inequality implies also present clustering performance rmckusal number missing entries training data signal dimension. compute clustering error mc-kusal/rmc-kusal using ﬁnal kernel subspace assignment labels {li}n following experiments select ﬁrst experiment digits usps dataset every trial randomly choose normalized samples ﬁrst samples digits samples experiments. random selection repeated times. perform mc-kusal rmc-kusal using gaussian kernel expy−y polynomial kernel parameter clustering results listed table clustering error mckusal roughly ones kernel/standard kmeans clustering mc-kusal much better experiments. addition clustering error rmc-kusal increasing function number missing entries gaussian kernel polynomial kernel. experiments using digits apply mckusal rmc-kusal using gaussian kernel expy−y polynomial kernel experiments performed table observe mc-kusal outperforms clustering algorithms reduction clustering error. missing data experiments clustering performance rmc-kusal using gaussian kernel degrades number missing entries data increases. polynomial kernel rmc-kusal increasing number entries missing data result much degradation clustering performance. conclude noting choice kernels experiments agnostic training data. nonetheless datadriven learning kernels active area research sometimes studied rubric multiple kernel learning works leveraged improve performance proposed algorithms careful investigation beyond scope work. bajwa revisiting robustness union-ofsubspaces model data-adaptive learning nonlinear signal models proc. ieee int. conf. acoustics speech signal processing mika sch¨olkopf smola k.-r. m¨uller scholz r¨atsch kernel de-noising feature spaces advances neural information processing systems elad aharon image denoising sparse redundant representations learned dictionaries ieee trans. image process. vol. soltanolkotabi cand`es geometric analysis subspace clustering outliers ann. stat. vol. dyer sankaranarayanan baraniuk greedy feature selection subspace clustering mach. learn. res. vol. m.-h. yang k.-c. kriegman clustering appearances objects varying illumination conditions proc. ieee conf. computer vision pattern recognition wolf shashua kernel principal angles classiﬁcation machines applications image sequence interpretation proc. ieee conf. computer vision pattern recognition bertsekas nonlinear programming. athena scientiﬁc bezdek hathaway convergence alternating optimization neural parallel sci. comput. vol. kokiopoulou chen saad trace optimization eigenproblems dimension reduction methods numer. linear algebra appl. vol. balzano nowak recht online identiﬁcation tracking subspaces highly incomplete information proc. allerton conf. communication control computing waheed bajwa received degree electrical engineering national university sciences technology pakistan degrees electrical engineering university wisconsin-madison respectively. postdoctoral research associate program applied computational mathematics princeton university research scientist department electrical computer engineering duke university currently assistant professor department electrical computer engineering rutgers university. research interests include harmonic analysis high-dimensional statistics machine learning statistical signal processing wireless communications. bajwa three years industry experience including summer position global research niskayuna received best academics gold medal president’s gold medal electrical engineering national university sciences technology morgridge distinguished graduate fellowship university wisconsin-madison army research ofﬁce young investigator award national science foundation career award co-guest edited special issue elsevier physical communication journal compressive sensing communications co-chaired cpsweek workshop signal processing advances sensor networks ieee globalsip symposium sensing statistical inference methods. currently serves publicity publications chair ieee camsap associate editor ieee signal processing letters. tong received degree instrument science engineering shanghai jiao tong university china degree electrical engineering duke university since september working towards degree department electrical computer engineering rutgers state university jersey. research interests include high-dimensional data analysis statistical signal processing image video processing machine learning.", "year": 2014}