{"title": "Combining Representation Learning with Logic for Language Processing", "tag": ["cs.NE", "cs.CL", "cs.LG", "cs.LO"], "abstract": "The current state-of-the-art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient-based optimization. They require little or no hand-crafted features, thus avoiding the need for most preprocessing steps and task-specific assumptions. However, in many cases representation learning requires a large amount of annotated training data to generalize well to unseen data. Such labeled training data is provided by human annotators who often use formal logic as the language for specifying annotations. This thesis investigates different combinations of representation learning methods with logic for reducing the need for annotated training data, and for improving generalization.", "text": "deeply grateful supervisor mentor sebastian riedel. always great source inspiration supportive encouraging matters. particularly amazed trust ﬁrst time freedom gave pursue ambitious ideas contagious optimistic attitude many opportunities presented absolutely could wished better supervisor. would like thank second supervisors thore graepel daniel tarlow feedback well sameer singh whose collaboration guidance made start ph.d. smooth motivating fun. furthermore thank edward grefenstette karl moritz hermann thomas kocisk´y phil blunsom fortunate work internship deepmind summer thankful guidance thomas demeester andreas vlachos pasquale minervini pontus stenetorp isabelle augenstein jason naradowsky time university college london machine reading lab. addition thanks dirk weissenborn many fruitful discussions. would also like thank leser philippe thomas roman klinger preparing well ph.d. studies humboldt-universit¨at berlin. pleasure work brilliant students university college london. thanks michal daniluk luke hewitt eisner vladyslav kolesnyk avishkar bhoopchand nayen pankhania hard work trust. ph.d. life would much without mates matko bosnjak george spithourakis johannes welbl marzieh saeidi ivan sanchez. thanks matko dirk pasquale thomas johannes sebastian feedback thesis. furthermore thank examiners luke dickens charles sutton extremely helpful in-depth comments corrections thesis. thank federation coffee brixton making best coffee world. anguage processing. thanks generous support well funding university college london computer science department able travel conferences wanted attend. greatly indebted thankful parents sabine lutz. sparked interest science importantly ensured fantastic childhood. always felt loved supported well protected many troubles life. furthermore would like thank christa ulrike tillmann hella hans gretel walter unconditional support last years. lastly thanks wonderful women life paula emily. thank paula keeping downs ph.d. love motivation support. thank giving family making feel home wherever are. emily greatest wonder life. current state-of-the-art many natural language processing automated knowledge base completion tasks held representation learning methods learn distributed vector representations symbols gradient-based optimization. require little hand-crafted features thus avoiding need preprocessing steps task-speciﬁc assumptions. however many cases representation learning requires large amount annotated training data generalize well unseen data. labeled training data provided human annotators often formal logic language specifying annotations. thesis investigates different combinations representation learning methods logic reducing need annotated training data improving generalization. introduce mapping function-free ﬁrst-order logic rules loss functions combine neural link prediction models. using method logical prior knowledge directly embedded vector representations predicates constants. method learns accurate predicate representations little training data available time generalizing predicates explicitly stated rules. however method relies grounding ﬁrst-order logic rules scale large rule sets. overcome limitation propose scalable method embedding implications vector space regularizing predicate representations. subsequently explore tighter integration representation learning logical deduction. introduce end-to-end differentiable prover neural network recursively constructed prolog’s backward chaining algorithm. constructed network allows calculate gradient proofs respect symbol representations learn representations proving facts knowledge base. addition incorporating complex ﬁrst-order rules induces interpretable logic programs gradient descent. lastly propose recurrent neural networks conditional encoding neural attention mechanism determining logical relationship natural language sentences. machine learning representation learning particular ubiquitous many applications nowadays. representation learning requires little hand-crafted features thus avoiding need task-speciﬁc assumptions. time requires large amount annotated training data. many important domains lack large training sets instance annotation costly domain expert knowledge generally hard obtain. combination neural symbolic approaches investigated thesis recently regained signiﬁcant attention advances representation learning research certain domains importantly lack success domains. research conducted ph.d. investigated ways training representation learning models using explanations form function-free ﬁrst-order logic rules addition individual training facts. opens possibility taking advantage strong generalization representation learning models still able express domain expert knowledge. hope work particularly useful applying representation learning domains annotated training data scarce empower domain experts train machine learning models providing explanations. function approximation neural networks computation graphs symbols subsymbolic representations backpropagation function-free first-order logic deduction backward chaining inductive logic programming automated knowledge base completion matrix factorization neural link prediction models path-based models matrix factorization embeds ground literals embedding propositional logic embedding first-order logic grounding stochastic grounding training details results discussion training details results discussion restricted embedding space constants zero-shot relation learning relations distant labels computational efﬁciency lifted rule injection asymmetry uniﬁcation module module module proof aggregation neural inductive logic programming training objective neural link prediction auxiliary loss computational optimizations training details independent sentence encoding conditional encoding attention word-by-word attention two-way attention training details results discussion bidirectional conditional encoding generating entailing sentences computation graph backward pass computation graph shown fig. simpliﬁed pseudocode symbolic backward chaining gelder gallaire minker details). full proof tree small knowledge base. knowledge base inference matrix completion true training facts unobserved facts relation representa. tions entity pair representations given training ground atoms matrix factorization learns k-dimensional predicate constant pair representations. here also consider additional ﬁrst-order logic rules seek learn symbol representations predictions comply given rules. weighted scores zero-shot relation learning. weighted various models fraction freebase training facts varied. freebase training facts zero-shot relation learning results presented table module mapping upstream proof state list proof states thereby extending substitution adding nodes computation graph neural network representing proof success. exemplary construction computation graph knowledge base. indices arrows correspond application respective rule. proof states subscripted sequence indices rules applied. underlined proof states aggregated obtain ﬁnal proof success. boxes visualize instantiations modules proofs fail cycle-detection overview different tasks contries dataset visualized nickel left part shows atoms removed task right part illustrates rules used infer location test countries. task facts corresponding blue dotted line removed training set. task additionally facts corresponding green dashed line removed. finally task also facts dash-dotted line removed. attention model rte. compared fig. model represent entire premise cell state instead output context representations later queried attention mechanism also note used. word-by-word attention model rte. compared fig. querying memory multiple times allows model store ﬁne-grained information output vectors processing premise. also note also used. attention visualizations. word-by-word attention visualizations. bidirectional encoding tweet conditioned bidirectional stance predicted using last rules different freebase target relations. implications extracted matrix factorization model manually annotated. premises implications shortest paths entity arguments dependency tree present simpliﬁed version make patterns readable. appendix list annotated rules. relations appear annotated rules omitted evaluation. difference joint signiﬁcant according sign-test weighted reimplementation matrix factorization model compared restricting constant embedding space injecting wordnet rules orginial matrix factorization model riedel denoted riedel-f. vast majority knowledge produced mankind nowadays available digital unstructured form images text. hard algorithms extract meaningful information data resources alone reason issue becoming severe amount unstructured data growing rapidly. recent years remarkable successes processing unstructured data achieved representation learning methods automatically learn abstractions large collections training data. achieved processing input data using artiﬁcial neural networks whose weights adapted training. representation learning lead breakthroughs applications automated knowledge base completion well natural language processing applications like paraphrase detection machine translation image captioning speech recognition sentence summarization name few. representation learning methods achieve remarkable results usually rely large amount annotated training data. moreover since representation learning operates subsymbolic level hard determine obtain certain prediction alone correct systematic errors incorporate domain commonsense knowledge. fact recent general data protection regulation european union introduces right explanation decisions algorithms machine learning models affect users enacted profound implications future development research machine learning algorithms especially nowadays commonly used representation learning methods. moreover many domains interests enough annotated training data renders applying recent representation learning methods difﬁcult. many issues exist purely symbolic approaches. instance given facts ﬁrst-order logic rules prolog obtain answer well proof query furthermore easily incorporate domain knowledge adding rules. however rule-based system generalize questions. instance given apple fruit apples similar oranges would like infer oranges likely also fruits. summarize symbolic rule-based systems interpretable easy modify. need large amounts training data easily incorporate domain knowledge. hand learning subsymbolic representations requires training data. trained models generally opaque hard incorporate domain knowledge. consequently would like develop methods take best worlds. aims thesis investigating combination representation learning ﬁrst-order logic rules reasoning. representation learning methods achieve strong generalization learning subsymbolic vector representations capture similarity even logical relationships directly vector space symbolic representations hand allow formalize domain commonsense knowledge using rules. instance state every human mortal every grandfather father parent. rules often worth many training facts. furthermore using symbolic representations take advantage algorithms multi-hop reasoning like prolog’s backward chaining algorithm backward chaining widely used multi-hop question answering also provides proofs addition answer question. however symbolic reasoning relying complete speciﬁcation background commonsense knowledge logical form. example assume asking grandpa person know grandfather person. explicit rule connecting grandpa grandfather answer. however given large representation learning learn grandpa grandfather mean thing lecturer similar professor becomes relevant want reason structured relations also textual patterns relations problem seek address thesis symbolic logical knowledge combined representation learning make best worlds. speciﬁcally investigate following research questions. contributions thesis makes following core contributions. regularizing representations first-order logic rules introduce method incorporating function-free ﬁrst-order logic rules directly vector representations symbols avoids need symbolic inference. instead symbolic inference regularize symbol representations given rules logical relationships hold implicitly vector space achieved mapping propositional logical rules differentiable loss terms calculate gradient given rule respect symbol representations. given ﬁrst-order logic rule stochastically ground free variables using constants domain resulting loss term propositional rule training objective neural link prediction model automated completion. allows infer relations little training facts mapping logical rules soft rules using algebraic operations long tradition contribution connection representation learning i.e. using rules directly learning better vector representations symbols used improve performance downstream task automated completion. content chapter ﬁrst appeared following publications rockt¨aschel matko bosnjak sameer singh sebastian riedel. low-dimensional embeddings logic. proceedings association computational linguistics workshop semantic parsing rockt¨aschel sameer singh sebastian riedel. injecting logical background knowledge embeddings relation extraction. proceedings north american chapter association computational linguistics human language technologies lifted regularization predicate representations implications subclass ﬁrst-order logic implication rules present scalable method independent size domain constants generalizes unseen constants used broader class training objectives instead relying stochastic grounding implication rules directly regularizers predicate representations. compared method chapter method independent number constants ensures given implication predicates holds possible pair constants test time. method based order embeddings contribution extension task automated completion requires constraining entity representations non-negative. chapter based following publications thomas demeester rockt¨aschel sebastian riedel. regularizing relation representations first-order implications. proceedings north american chapter association computational linguistics workshop automated knowledge base construction thomas demeester rockt¨aschel sebastian riedel. lifted rule injection relation embeddings. proceedings empirical methods natural language processing end-to-end differentiable proving current representation learning neural link prediction models deﬁcits comes complex multi-hop inferences transitive reasoning automated theorem provers hand long tradition computer science provide elegant ways reason symbolic knowledge. chapter propose neural theorem provers end-to-end differentiable theorem provers automated completion based neural networks recursively constructed inspired prolog’s backward chaining algorithm. calculate gradient proof success respect symbol representations allows learn symbol representations directly facts make similarities symbol representations provided rules proofs. addition demonstrate induce interpretable rules predeﬁned structure. three four benchmark method outperforms complex state-of-the-art neural link prediction model. work chapter appeared rockt¨aschel sebastian riedel. learning knowledge base inference neural theorem provers. proceedings north american chapter association computational linguistics workshop automated knowledge base construction rockt¨aschel sebastian riedel. end-to-end differentiable proving. advances neural information processing systems annual conference neural information processing systems recognizing textual entailment recurrent neural networks representation learning models recurrent neural networks used natural language sentences ﬁxed-length vector representations successfully applied various downstream tasks including recognizing textual entailment task determine logical relationship natural language sentences. either approached pipelines hand-crafted features neural network architectures independently sentences ﬁxed-length vector representations. instead encoding sentences independently propose model encodes second sentence conditioned encoding ﬁrst sentence. furthermore apply neural attention mechanism bridge hidden state bottleneck work chapter ﬁrst appeared rockt¨aschel edward grefenstette karl moritz hermann tomas kocisky phil blunsom. reasoning entailment neural attention. proceedings international conference learning representations thesis structure chapter provide background representation learning computation graphs ﬁrst-order logic notation used throughout thesis. furthermore explain task automated completion describe neural link prediction path-based approaches proposed task. chapter introduces method regularizing symbol representations ﬁrst-order logic rules. chapter subsequently focus direct implications predicates subset ﬁrst-order logic rules. class rules provide efﬁcient method directly regularizing predicate representations. chapter introduce recursive construction neural network automated completion based prolog’s backward chaining algorithm. chapter presents based conditional encoding neural attention mechanism. finally chapter concludes thesis discussion limitations open issues future research avenues. chapter introduces core methods used thesis. section explains function approximation neural networks backpropagation. subsequently section introduces function-free ﬁrst-order logic backward chaining algorithm inductive logic programming. finally section discusses prior work automated knowledge base completion linking ﬁrst sections together. function approximation neural networks thesis consider models formulated differentiable functions parameterized task functions i.e. learn parameters training examples input desired output training example. both structured objects. instance could fact world like directedby corresponding target truth score deﬁne loss function measures discrepancy provided output predicted output input given current setting parameters seek parameters minimize discrepancy training set. denote global loss entire training data learning problem thus written note also function since might want measure discrepancy given predicted outputs also regularizer parameters improve generalization. sometimes omit brevity. differentiable functions gradient-based optimization methods computation graphs useful abstraction deﬁning models differentiable functions computation graphs illustrate computations carried model precisely directed acyclic graph nodes represent variables directed edges multiple nodes another node correspond differentiable operation. variables consider scalars vectors matrices generally higher-order tensors. denote scalars lower case letters vectors bold lower case letters matrices bold capital letters higherorder tensors euler script letters variables either inputs outputs parameters model. figure shows simple computation graph calculates σy). denote product vectors. furthermore name intermediate expression figure shows slightly complex computation graph parameters computation graph fact represents logistic regression symbols subsymbolic representations thesis neural networks learn representations symbols. instance symbols words constants predicate names. learn subsymbolic representation symbols mean symbols ﬁxed-dimensional vectors generally tensors. done ﬁrst implementation purpose also consider structured objects tensors like tuples lists. current deep learning libraries theano torch tensorflow come support tuples lists. however brevity leave description here. enumerating symbols assigning number symbol. symbols. denote one-hot vector symbol index everywhere else. figure shows computation graph whose inputs one-hot vectors symbols indices ﬁrst layer one-hot vectors mapped dense vector computation graph corresponds neural link prediction model explain detail section serves illustration symbols mapped vector representations. remainder thesis often omit embedding layer clarity. goal learn symbol representations automatically data. need able calculate gradient output computation graph respect parameters learning data need able calculate gradient loss respect model parameters. assume operations computation graph differentiable recursively apply chain rule calculus. jacobian matrix i.e. matrix partial derivatives here gradient respect note approach generalizes matrices higher-order tensors reshaping vectors gradient calculation back original shape afterwards. backpropagation uses chain rule recursively deﬁne efﬁcient calculation gradients parameters computation graph avoiding recalculation previously calculated expressions. achieved dynamic programming i.e. storing previously calculated gradient expressions reusing later gradient calculations. refer reader goodfellow details. order backpropagation differentiable operation want computation graph need ensure function differentiable respect inputs. example take computation graph depicted fig. example. assume given upstream gradient want compute gradient respect inputs computations carried backpropagation note computation reused calculating ∇yz. gradient entire computation graph respect ∇z∇xz. later computation graphs nodes used multiple downstream computations. nodes receive multiple gradients downstream nodes backpropagation summed calculate gradient computation graph respect variable represented node. chapter backpropagation computing gradient differentiable propositional logic rules respect vector representations symbols develop models combine representation learning ﬁrst-order logic. chapter take construct computation graph possible proofs knowledge base using backward chaining algorithm. allow calculate gradient proofs respect symbol representations induce rules using gradient descent. finally chapter recurrent neural networks i.e. computation graphs dynamically constructed input varying-length input sequences word representations. function-free first-order logic turn brief introduction function-free ﬁrst-order logic extent used subsequent chapters. section follows syntax prolog datalog based lloyd nilsson maluszynski dzeroski syntax start deﬁning atom predicate symbol list terms. lowercase names refer predicate constant symbols uppercase names variables prolog also considers function terms deﬁnes constants function terms zero arity. however thesis work function-free ﬁrst-order logic rules subset logic datalog supports. hence term constant variable. instance grandfatherof atom predicate grandfatherof terms variable constant bart respectively. deﬁne arity predicate number terms takes arguments. thus grandfatherof binary predicate. literal deﬁned negated non-negated atom. ground literal literal variables furthermore consider ﬁrst-order logic rules form body possibly empty conjunction atoms represented list head atom. examples rules table rules atom head called deﬁnite rules. thesis consider deﬁnite rules. variables universally quantiﬁed rule ground rule literals ground. call ground rule empty body fact hence rules table facts. deﬁne symbols containing constant symbols predicate symbols variable symbols call deﬁnite rules like table knowledge base logic program. substitution {x/t /tn} assignment variable symbols terms applying substitution atom replaces occurrences variables respective term deﬁned syntax logic used thesis. assign meaning language need able derive truth value facts. focus proof theory i.e. deriving truth fact facts rules next subsection explain backward chaining algorithm deductive reasoning. used derive atoms atoms applying rules. deduction backward chaining representing knowledge symbolic form appeal automated deduction systems infer facts. instance given logic program table automatically deduce grandfatherof true fact applying rule using facts backward chaining common method automated theorem proving refer reader russell norvig gelder gallaire minker details fig. excerpt pseudocode style functional programming language. particularly making pattern matching check properties arguments passed module. note matches every argument order matters i.e. arguments match line subsequent lines evaluated. denote sets euler script letters lists small capital letters lists lists blackboard bold letters refer prepending element list atom list predicate symbol terms rule seen list atoms thus list lists head list rule head. given goal grandparentof backward chaining ﬁnds substitutions free variables constants facts achieved recursively iterating rules translate goal subgoals attempt prove thereby exploring possible proofs. example could contain following rule applied answers goal proof exploration backward-chaining divided functions called perform depth-ﬁrst search space possible proofs. function attempts prove goal unifying head every rule yielding intermediate substitutions. uniﬁcation iterates pairs symbols lists corresponding atoms want unify updates substitution symbols variable. returns failure non-variable symbols identical atoms different arity. rules uniﬁcation head rule succeeds body substitution passed attempts prove every atom body sequentially ﬁrst applying substitutions subsequently calling repeated recursively uniﬁcation fails atoms proven uniﬁcation grounding facts certain proof-depth exceeded. table shows proof query grandparentof given table using fig. method substitute replaces variables atom corresponding symbol exists substitution variable substitution list. figure shows full proof tree small knowledge base query grandfatherof. numbers arrows correspond application respective rules. visualize recursive calls together proof depth right side. note proofs aborted early uniﬁcation failure. though ﬁrst-order logic used complex multi-hop reasoning drawback symbolic inference generalization beyond explicitly specify facts rules. instance given large would like learn automatically many cases observe fatherof also observe parentof. approached statistical relational learning inductive logic programming particularly neural networks completion discuss remainder chapter. inductive logic programming backward chaining used deduction i.e. inferring facts given rules facts inductive logic programming combines logic programming inductive learning learn logic programs training data. training data include facts also provided logic program system supposed extend. speciﬁcally given facts rules task system regularities form hypotheses unseen facts crucially hypothesis formulated using ﬁrst-order logic. different variants learning tasks focus learning entailment given examples positive facts negative facts system supposed rules positive facts deduced negative facts cannot. many variants systems refer reader muggleton raedt dzeroski overview. prominent systems first order inductive learner greedy algorithm induces rule time constructing body satisﬁes maximum number positive facts minimum number negative facts. chapter construct neural networks proving facts introduce method inducing logic programs using gradient descent learning vector representations symbols. automated knowledge base completion automated completion task inferring facts information contained resources text. important task real-world usually incomplete. instance placeofbirth predicate missing people freebase prominent recent approaches automated completion learn vector representations symbols neural link prediction models. appeal learning subsymbolic representations lies ability capture similarity even implicature directly vector space. compared systems neural link prediction models rely combinatorial search space logic programs instead learn local scoring function based subsymbolic representations using continuous optimization. however comes cost uninterpretable models straightforward ways incorporating logical background knowledge drawbacks seek address thesis. another beneﬁt neural link prediction models inferring whether fact true often amounts efﬁcient algebraic operations makes test-time inference scalable. addition representations symbols compositional. instance compose representation natural language predicate sequence word representations recent years many models automated completion proposed. next sections discuss prominent approaches. high level methods categorized neural link prediction models deﬁne local scoring function truth fact based estimated symbol representations models paths entities predicting relations them. section describe matrix factorization relation extraction model riedel instance simple neural link prediction model. discuss model detail basis develop rule injection methods chapter assume observed entity pair symbols predicate symbols either represent structured binary relations freebase large collaborative knowledge base unstructured open information extraction textual surface patterns collected news articles. examples structured unstructured relations company/founders -co-founder-of- respectively. here -co-founder-of- textual pattern placeholders entities. instance relationship elon musk tesla sentence elon musk co-founder tesla spacex cites foundation trilogy isaac asimov major inﬂuence thinking could expressed ground atom -co-founder-of-. example elon musk appeared ﬁrst textual pattern indicates constant used second argument predicate corresponding pattern. later introduce rule {rs)} observed ground atoms. model riedel maps symbols knowledge base subsymbolic representations i.e. learns dense k-dimensional vector representation every relation entity pair. thus training fact represented vectors respectively. refer embeddings subsymbolic representations vector representations neural representations simply representations clear context. truth estimate fact modeled sigmoid product symbol representations fact expression corresponds computation graph shown fig. vij. score psij measuring compatibility relation entity pair representation interpreted probability fact true conditioned parameters model. would like train symbol representations true ground atoms score close false ground atoms score close zero. results low-rank matrix factorization corresponding generalized principal component analysis representations respectively. factorization depicted fig. known facts shown green task complete matrix cells question mark. equation leads generalization respect unseen facts every relation entity pair represented low-dimensional space information bottleneck lead similar entity pairs represented close distance vector space distributional hypothesis states shall know word company keeps used learning word meaning large collections text applied automated completion could meaning relation entity pair estimated entity pairs relations respectively appear together bayesian personalized ranking common problem automated completion observe negative facts. recommendation literature problem called implicit feedback applied completion would like infer target relation facts know facts know. facts know unknown either true true missing method address issue formulate problem terms ranking loss sample unobserved facts negative facts training. given known fact bayesian personalized ranking samples another entity pair adds soft-constraint relation assumed complete sampled unobserved fact consequently assumed negative. assumption unseen facts necessarily false probability true less known facts. thus sampled unobserved facts lower score known facts. instead working ﬁxed samples resample negative facts every known fact every epoch epoch full iteration known facts denote sample constant pairs leads overall approximate loss strength regularization relation entity pair representations respectively. furthermore relation-dependent implicit weight accounting often sampled training time. resample unobserved fact every time visit observed fact training unobserved facts relations many observed facts likely sampled negative facts relations observed facts. complete computation graph single training example matrix factorization using regularization shown fig. example simple neural link prediction model matrix factorization approach previous section. alternative methods deﬁne score psij different ways different loss functions parameterize relation entity representations differently. instance bordes train projection matrices relation left right-hand argument position respectively. subsequently score fact deﬁned norm difference projected entity argument embeddings note compared matrix factorization model riedel embeds entity-pairs learn individual entity embeddings. similarly transe models score norm difference right entity embedding translation left entity embedding relation embedding contrast models mentioned section rescal optimized using alternating least squares trescal extends rescal entity type constraints freebase relations. element-wise multiplication. model special case rescal constrained diagonal. complex trouillon uses complex vectors representing relations entities. real denote real part imag imaginary part complex vector scoring function deﬁned complex building upon riedel verga developed column-less factorization approach encoding surface form patterns using long short-term memories instead learning noncompositional representation. similarly toutanova uses convolutional neural networks encode surface form patterns. follow-up study verga propose row-less method entity pair representations learned instead computed observed relations thereby generalizing entity pairs test time. methods presented previous section model truth fact local scoring function representations relation entities path-based models score facts based either random walks encoding entire paths vector space path ranking path ranking algorithm learns predict relation entities based logistic regression features collected random walks entities predeﬁned length. extend inference openie surface patterns addition structured relations contained kbs. related approach programming personalized pagerank ﬁrst-order probabilistic logic programming language. uses prolog’s selective linear deﬁnite clause resolution depth-ﬁrst search strategy theorem proving construct graph proofs. instead returning deterministic proofs given query proppr deﬁnes stochastic process graph proofs using pagerank furthermore proppr features head rules whose weights learned data guide stochastic proofs. experiments proppr conducted comparably small contrary neural link prediction models extensions below scaled large real-world shortcoming proppr operating symbols instead vector representations symbols. limits generalization results explosion number paths consider increasing path length. overcome limitation gardner extend include vector representations verbs. verb representations obtained pre-training matrix co-occurrences verbs subject-object tuples collected large dependency-parsing corpus. subsequently representations used clustering relations thus avoiding explosion path features prior work improving generalization. gardner take approach introducing vector space similarity random walk inference thus dealing paths containing unseen surface forms measuring similarity surface forms seen training following relations proportionally similarity. path encoding gardner introduced vector representations representations trained end-to-end task data instead pretrained external corpus. means relation representations cannot adapted training note three limitations work neelakantan first parameter sharing rnns encode different paths different target relations. second aggregation information multiple path encodings. lastly entity information along path relation representations rnn. address ﬁrst issue using single whose parameters shared across paths. address second issue train aggregation function encodings multiple paths connecting entities. finally obtain entity representations alongside relation representations learned vector representations entity’s annotated freebase types. chapter introduce paradigm combining neural link prediction models automated knowledge base completion background knowledge form ﬁrst-order logic rules. investigate simple baselines enforce rules symbolic inference matrix factorization. main contribution novel joint model learns vector representations relations entity pairs using distant supervision ﬁrst-order logic rules rules captured directly vector space symbol representations. symbolic rules differentiable computation graphs representing real-valued losses added training objective existing neural link prediction models. test time inference still efﬁcient local scoring function symbol representations used logical inference needed. present empirical evaluation incorporate automatically mined rules matrix factorization neural link prediction model. experiments demonstrate beneﬁts incorporating logical knowledge freebase relation extraction. speciﬁcally joint factorization distant logic supervision efﬁcient accurate robust noise incorporating logical rules able train relation extractors training facts observed. section introduced matrix factorization method learning representations predicates constant pairs automated completion. section elaborate matrix factorization indeed embeds ground atoms body like parentof. furthermore letf denote rules ﬁrst-order rules later. slight abuse notation also denote depends neural link prediction model. matrix factorization dense vector representations. rescal maps constants training objective riedel used bayesian personalized ranking training objective i.e. encouraged score known true facts higher unknown facts however later model probability rule probability ground atoms scored neural link prediction model need ensure scores interval instead thus negative log-likelihood loss directly maximize probability rules including ground atoms regularization brevity) therefore instead learning rank facts optimize representations assign score close rules model thus seen generalization neural link prediction model rules beyond ground atoms. matrix factorization neural link prediction model embedding ground atoms vector space predicate constant pair representations. next extend ground literals afterwards propositional ﬁrst-order logic rules. using ground literals training objective matrix factorization embedding ground literals learning predicate constant pair representations. words given known ground literals matrix factorization embeds symbols low-dimensional vector space scoring function assigns high probability ground literals. symbols embedded low-dimensional vector space method generalize unknown facts predict probability test time placing similar symbols close distance embedding space. note gained anything matrix factorization explained section equations introducing notation make easier embed complex rules later. embed negated non-negated facts question whether also embed propositional ﬁrst-order logic rules. know propositional logic negation conjunction operators model boolean operator propositional rule. effectively turned symbolic logical operation differentiable operation used learn subsymbolic representations automated completion. differentiable operation conjunction could backpropagate propositional logical expression learn vector representations symbols encode given background knowledge propositional logic. given probability ground atoms product fuzzy logic calculate probability conjunction atoms. however step assume know ground truth probability conjunction atoms. negative log-likelihood loss measure discrepancy predicted probability conjunction ground truth. contribution backpropagating discrepancy propositional rule neural link prediction model scores ground atoms calculate gradient respect vector representations symbols. subsequently update representations using gradient descent thereby encoding ground truth propositional rule directly vector representations symbols. test time predicting score unobserved ground atom done efﬁciently note holds ground atoms propositional logical expression. furthermore propositional logical expression normalized conjunctive normal form. thus eqs. construct differentiable computation graph thus real-valued loss term symbolic expression propositional logic. rule differentiable expression alongside facts optimize symbol representations using gradient descent previously matrix factorization. computation graph allows calculate gradient rule respect symbol representations shown fig. structure bottom part computation graph determined neural link prediction model middle part determined propositional rule. note neural link predictor instead matrix factorization obtaining probability ground atoms. requirement ground atom scores need interval however models case always apply transformation sigmoid. independence assumption equation underlies strong assumption namely probability arguments conjunction conditionally independent given symbol embeddings. already violation assumption however dependent arguments approximation probability conjunction still used gradient updates symbol representations demonstrate empirically conjunction modeled useful improving automated completion. chapter present avoid independence assumption implications. embedding first-order logic grounding backpropagate propositional boolean expressions turn embedding ﬁrst-order logic rules vector space symbol representations. note process chain rules i.e. perform logical inference training test time. instead provided rule used construct loss term optimizing vector representations symbols. training model using gradient descent attempt minimum global loss probability rules high. minimum might attained prediction neural link prediction model scoring ground atoms agrees single rules thereby predicting ground atoms figure given training ground atoms matrix factorization learns k-dimensional predicate constant pair representations. here also consider additional ﬁrst-order logic rules seek learn symbol representations predictions comply given rules. stochastic grounding large domains constants becomes costly optimize would result many expressions added training objective underlying neural link prediction model. rules pairs constants reduce number terms drastically considering pairs constants ctrain appeared together training facts. still ctrain might prohibitively large constant pairs. thus resort heuristic similar sampling constant pairs. given rule obtain ground propositional rule every constant pair least atom rule known training fact substituting free variables constants. addition sample many constant pairs appeared background knowledge form ﬁrst-order rules seen hints used generate additional training data pre-factorization inference ﬁrst perform symbolic logical inference training data using provided rules inferred facts additional training data. example rule additional observed training facts pair constants true fact training data. repeated facts inferred. subsequently matrix factorization extended observed facts. intuition additional training data generated rules provide evidence logical dependencies relations matrix factorization model time allowing factorization generalize unobserved facts deal ambiguity noise data. logical inference performed training factorization model expect learned embeddings encode provided rules. drawback pre-factorization inference rules enforced observed atoms i.e. ﬁrst-order dependencies predicted facts ignored. contrast loss terms rule directly matrix factorization objective thus jointly optimizing embeddings reconstruct known facts well obey provided ﬁrst-order logical background knowledge. however stochastically ground ﬁrst-order rules guarantee given rule indeed hold possible entity pairs test time. next demonstrate approach still useful completion despite limitation chapter introduce method overcomes limitation simple ﬁrst-order implication rules. orthogonal questions evaluating method above. first regularizing symbol embeddings ﬁrst-order logic rules indeed capture logical knowledge vector space used improve completion? second obtain background knowledge form rules useful particular completion task? latter well-studied problem thus focus evaluation ability various approaches beneﬁt rules directly extract training data using simple method. distant supervision evaluation follow procedure riedel evaluating knowledge base completion freebase textual data corpus training matrix consists columns representing freebase relations textual patterns rows training facts belong freebase relations. constant pairs divided train test remove freebase facts test pairs training data. primary evaluation measure mean average precision test relations fmj} test facts relation furthermore ranked list facts relation scored model fact reached. deﬁned precision calculates fraction correctly predicted test facts predicted facts. weighted average precision every relation weighted number true facts respective relation note metric operates ranking facts predicted model take absolute predicted score account. rule extraction annotation simple technique extracting rules matrix factorization model based sanchez ﬁrst matrix factorization complete training data learn symbol representations. training iterate pairs relations freebase relation. every relation pair iterate training atoms evaluate score proxy coverage rule. finally rank rules score manually inspect ﬁlter resulted annotated high-quality rules note rule extraction approach observe relations test constant pairs extracted rules simple ﬁrst-order logic expressions. models experiments access rules except matrix factorization baseline. methods proposed methods injecting logic symbol embeddings prefactorization inference baseline method performs org/parent/child -unit-of-. location/containedby -city-of-. person/nationality -minister-. person/company -executive-. company/founders -co-founder-of-. table rules different freebase target relations. implications extracted matrix factorization model manually annotated. premises implications shortest paths entity arguments dependency tree present simpliﬁed version make patterns readable. appendix list annotated rules. regular matrix factorization propagating logic rules deterministic manner joint optimization maximizes objective combines terms facts ﬁrst-order logic rules. additionally evaluate three baselines. matrix factorization model uses ground atoms learn relation constant representations furthermore consider pure symbolic logical inference since restrict consistent simple rules inference performed efﬁciently. ﬁnal approach post-factorization inference ﬁrst runs matrix factorization performs logical inference known predicted facts. post-inference computationally expensive since premises rules iterate rows matrix assess whether premise predicted true not. since negative training facts follow riedel sampling unobserved ground atoms assume false. rules stochastic grounding described section thus addition loss score training facts loss sampled unobserved ground atoms assume negative well loss terms ground rules. words learn symbol embeddings minimizing includes known unobserved atoms well ground propositional rules sampled using stochastic grounding. addition -regularization symbol representations. minimizing training loss adagrad involve explicit logical inference. instead expect vector space symbol embeddings incorporate given rules. hyperparameters based riedel dimension symbol representations models parameter -regularization initial learning rate adagrad epochs. runtime adagrad update deﬁned single cell matrix thus training data provided ground atom time. matrix factorization adagrad epoch touches observed ground atoms epoch many sampled negative ground atoms. provided rules additionally revisits observed ground atoms appear atom rules thus general rules expensive. however updates ground atoms performed independently thus data needs stored memory. presented models take less minutes train intel core machine. results discussion asses utility injecting logic rules symbol representations present comparison variety benchmarks. first study scenario learning extractors relations freebase alignments i.e. number entity pairs appear textual patterns structured freebase relations zero. measures well different models able generalize logic rules textual patterns section describe experiment number freebase alignments varied order assess effect combining distant supervision background knowledge accuracy predictions. although methods presented target relations insufﬁcient alignments also provide comparison complete distant supervision dataset section zero-shot relation learning start scenario learning extractors relations appear i.e. textual alignments. scenario occurs practice relation added facts connect relation existing relations textual surface forms. accurate extraction relations rely background domain knowledge e.g. form rules identify relevant textual alignments. however time correlations textual patterns utilized improved generalization. simulate setup remove alignments entity person/company location/containedby author/works written person/nationality parent/child person/place birth person/place death neighborhood/neighborhood person/parents company/founders film/directed film/produced pairs freebase relations distant supervision data extracted logic rules background knowledge assess ability different methods recover lost alignments. figure provides detailed results. unsurprisingly matrix factorization performs poorly since predicate representations cannot learned freebase figure weighted various models fraction freebase training facts varied. freebase training facts zero-shot relation learning results presented table relations without observed facts. fact non-zero score matrix factorization random predictions. symbolic logical inference limited number known facts appear premise implications thus performs poorly too. although post-factorization inference able achieve large improvement logical inference explicitly injecting logic rules symbol representations using pre-factorization inference joint optimization gives superior results. finally observe jointly optimizing probability facts rules able best combine logic rules textual patterns accurate zero-shot learning relation extractors. table shows detailed results freebase test relations. except author/works written person/place death jointly optimizing probability facts rules yields superior results. section study scenario learning relations distant supervision alignments i.e. structured freebase relations textual patterns observed entity-pairs. particular observe behavior various methods amount distant supervision varied. methods training data contains different fractions freebase training facts keep textual patterns addition annotated rules. figure summarizes results. performance symbolic logical inference depend amount distant supervision data since take advantage correlations data. matrix factorization make logical rules thus baseline performance using distant supervision. factorization based methods small fraction training data needed achieve around weighted performance thus demonstrating efﬁciently exploiting correlations predicates generalizing unobserved facts. pre-factorization inference however outperform post-factorization inference matrix factorization curve. suggests effective injecting logic symbol representations ground atoms also available. contrast joint model learns symbol representations outperform methods freebase training data interval. beyond seem sufﬁcient freebase facts matrix factorization encode rules thus yielding diminishing returns. although focus work injecting logical rules relations without sufﬁcient alignments knowledge base also present evaluation complete distant supervision data riedel compared riedel al.’s matrix factorization model riedel-f reimplementation achieves lower wmap higher attribute difference different loss function show precision-recall curve fig. demonstrating joint optimization provides beneﬁts existing factorization distant supervision techniques even complete dataset obtains weighted respectively. improvement matrix factorization model explained noting joint model reinforces high-quality annotated rules. figure precision-recall curve demonstrating joint method incorporates annotated rules derived data outperforms existing factorization approaches embeddings knowledge base completion many methods embedding predicates constants based training facts knowledge base completion proposed past work goes learn embeddings follow factual also ﬁrst-order logic knowledge. note method regularizing symbol embeddings rules described chapter generally compatible existing neural link prediction model provides per-atom scores experiments worked matrix factorization neural link prediction model based work able incorporate transitivity rules transe models entities separately instead learning representation every entity pair. logical inference common alternative adding ﬁrst-order logic knowledge trivial perform symbolic logical inference however purely symbolic approaches cannot deal uncertainty inherent natural language generalize poorly. probabilistic inference ameliorate drawbacks symbolic logical inference probabilistic logic based approaches proposed since logical connections relations modeled explicitly approaches generally hard scale large kbs. speciﬁcally approaches based markov logic networks encode logical knowledge dense loopy graphical models making structure learning parameter estimation inference hard scale data. contrast model logical knowledge captured directly symbol representations leading efﬁcient inference test time calculate forward pass neural link prediction model. furthermore symbols embedded low-dimensional vector space natural dealing linguistic ambiguities label errors appear openie textual patterns included predicates automated completion stochastic grounding related locally grounding query programming personalized pagerank difference stochastically grounded rules differentiable terms representation learning training objective whereas proppr grounded rules used stochastic inference without learning symbol representations. weakly supervised learning work also inspired weakly supervised approaches structural constraints source indirect supervision. methods used several tasks semi-supervised information extraction work carlson spirit similar goal using commonsense constraints jointly train multiple information extractors. main difference learning symbol representations allow arbitrarily complex logical rules used regularizers representations. combining symbolic distributed representations number recent approaches combine trainable subsymbolic representations symbolic knowledge. grefenstette describes isomorphism ﬁrstorder logic tensor calculus using full-rank matrices exactly memorize facts. based isomorphism rockt¨aschel combine logic matrix factorization learning low-dimensional symbol embeddings approximately satisfy given rules generalize unobserved facts data. work extends workshop paper proposing simpler formalism without tensor-based logical chang freebase entity types hard constraints tensor factorization objective universal schema relation extraction. contrast approach imposing soft constraints formulated universally quantiﬁed ﬁrst-order rules. lacalle lapata combine ﬁrst-order logic knowledge topic model improve surface pattern clustering relation extraction. since rules specify relations clustered cannot capture variety dependencies embeddings model asymmetry. lewis steedman distributed representations cluster predicates logical inference. again approach expressive learning subsymbolic representations predicates clustering deal asymmetric logical relationships predicates. several studies investigated symbolic representations guide composition symbol representations instead guiding composition using ﬁrst-order logic rules prior domain knowledge form regularizers directly learn better symbol representations. combining symbolic information neural networks long tradition. towell shavlik introduce knowledge-based artiﬁcial neural networks whose topology isomorphic facts inference rules. there facts input units intermediate conclusions hidden units ﬁnal conclusions output units. unlike work learned symbol representations. h¨olldobler hitzler prove every logic program exists recurrent neural network approximates semantics program. theoretical insight unfortunately provide constructing neural network. recently bowman demonstrated neural tensor networks accurately learn natural logic reasoning. method presented chapter also related recently introduced neural equivalence networks eqnets recursively construct neural representations symbolic expressions learn equivalence classes. approach recursively construct neural networks evaluating boolean expressions regularizers learn better symbol representations automated completion. summary chapter introduced method mapping symbolic ﬁrst-order logic rules differentiable terms used regularize symbol representations learned neural link prediction models automated completion. speciﬁcally proposed joint training objective maximizes probability known training facts well propositional rules made continuous replacing logical operators differentiable functions. inspired fuzzy logic contribution backpropagating gradient negative log-likelihood loss propositional rule neural link prediction model scores ground atoms calculate gradient respect vector representations symbols. subsequently update representations using gradient descent thereby encoding ground truth propositional rule directly vector representations symbols. leads efﬁcient predictions test time calculate forward pass neural link prediction model. described stochastic grounding process incorporating ﬁrst-order logic rules. experiments automated completion show proposed method used learn extractors relations little observed textual alignments time beneﬁting correlations textual surface form patterns. method incorporating ﬁrst-order logic rules symbol representations introduced previous chapter relies stochastic grounding. moreover vector representations predicates also representations pairs constants optimized maximize probability provided rules. problematic following reasons. scalability even stochastic grounding incorporating ﬁrst-order logic rules method described dependent size domain constants. example take simple rule ismortal ishuman assume observe ishuman seven billion constants. single rule would already seven billion loss terms training objective generalizability since backpropagate upstream gradients rule predicate representations also representations pairs constants theoretical guarantee rule indeed hold constant pairs observed training. flexibility training loss previous method compatible rank-based losses bayesian personalized ranking instead negative log-likelihood loss results lower performance compared automated knowledge base completion. independence assumption explained section assume probability ground atoms conditionally independent given symbol representaconstants generalizes unseen constant pairs used broader class training objectives assume probability ground atoms conditionally independent given symbol representations. chapter present method satisﬁes desiderata simple implication rules instead general function-free ﬁrst-order logic rules matrix factorization neural link prediction model. however note simple implications commonly used improve automated completion. method propose incorporates implications vector representations predicates maintaining computational efﬁciency modeling training facts. achieved enforcing partial order predicate representations vector space entirely independent number constants domain. involves representations predicates mentioned rules well general rule-independent constraint embedding space constant pairs. example given above require every component implication holds non-negative representation constant. hence method avoids need separate loss terms every ground atom resulting grounding rules. statistical relational learning type approach often referred lifted inference learning deals groups random variables ﬁrst-order level. sense approach lifted form rule injection. allows impose large number rules learning distributed representations predicates constant pairs. furthermore constraints satisﬁed injected rules always hold even unseen inferred ground atoms. addition rely assumption conditional independence ground atoms. ordering symbol representations vector space implications directly captured inspired order embeddings example holds illustrated fig. here fatherof motherof imply parentof since every component ofparentof larger corresponding component infatherof ormotherof. predihomer bart andmarge bart non-negative. thus score fatherof score ofparentof larger long non-negative make sure thatei non-negative representation constant pairs many choices ensuring constant pair representations positive. option initialize constant pair representations non-negative vectors projecting gradient updates make sure stay non-negative. another option apply transforma+ constant pair representations used neural link tion prediction model scoring atoms. instance could relu however choose restrict constant representations even required decided transformation approximately boolean embeddings every constant pair represen\u0001 small positive margin ensure gradient disappear inequality actually satisﬁed. nice property loss compared method presented previous chapter implication holds follow experimental setup previous chapter evaluate corpus again test well presented models incorporate rules alignment textual surface forms freebase relations number freebase training facts increased addition experiment rules automatically extracted wordnet improve automated completion full dataset. incorporating background knowledge wordnet wordnet hypernyms generate rules dataset. iterate surface form patterns dataset attempt replace words pattern hypernyms. resulting surface form contained dataset generate corresponding rule. instance generate rule -official- -diplomat- since patterns contained dataset know wordnet official hypernym diplomat. resulted generated rules subsequently annotated manually yielding high-quality rules listed appendix note rules surface form patterns. thus none rules freebase relation head predicate. although test relations originate freebase still hope improvements transitive effects better surface form representations turn help predict freebase facts. models implemented tensorflow hyperparameters riedel size symbol representations weight regularization adam optimization initial learning rate batch size embeddings initialized sampling values uniformly person/company location/containedby person/nationality author/works written person/place birth parent/child person/place death neighborhood/neighborhood person/parents company/founders sports team/league team owner/teams owned team/arena stadium film/directed broadcast/area served structure/architect composer/compositions person/religion film/produced table weighted reimplementation matrix factorization model compared restricting constant embedding space injecting wordnet rules orginial matrix factorization model riedel denoted riedel-f. results discussion turning injection rules compare model model show restricting constant embedding space regularization effect rather limiting expressiveness model demonstrate model capable zero-shot learning take advantage alignments textual surface forms freebase relations alongside rules show injecting high-quality wordnet rules leads improved predictions full dataset lastly provide details computational efﬁciency lifted rule injection method demonstrate correctly captures asymmetry implication rules restricted embedding space constants incorporating external commonsense knowledge relation representations curious much lose restricting embedding space constant symbols approximately boolean embeddings. surprisingly expressiveness model suffer strong restriction. table restricting constant embedding space yields percentage points higher weighted mean average precision compared real-valued constant embedding space result suggests restriction regularization effect improves generalization. also provide original results matrix factorization model riedel denoted riedelf comparison. different implementation optimization procedure results model compared riedel-f slightly worse section observed injecting implications head freebase relation training facts available infer freebase facts based rules correlations textual surface patterns. here repeat experiment. lifted rule injection model reaches weighted comparable method presented last chapter. experiment initialized predicate representations freebase relations implied rules negative random vectors sampled uniformly reason without negative training facts relations components lifted implication loss. consequently starting high values optimization would impede freedom representations ordered embedding space. demonstrates performs worse joint model chapter still used zero-shot relation learning. figure shows relation extraction performance improves freebase facts added. last chapter measures well proposed models matrix factorization propositionalized rule injection lifted rule injection model make provided implication rules well correlations textual surface form patterns increasing numbers freebase facts. although starts lower performance joint freebase training facts present outperforms joint plain matrix factorization model substantial margin provided freebase facts. indicates that addition much faster joint make better provided rules training facts. attribute able loss ground atoms regularization effect restricting embedding space constants pairs. former compatible rule-injection method approach maximizing expectation propositional rules presented previous chapter. column table show results obtained injecting wordnet rules. compared obtain increase weighted well compared reimplementation matrix factorization model demonstrates imposing partial order based implication rules used incorporate logical commonsense knowledge increase quality information extraction automated completion systems. note evaluation setting guarantees indirect effects rules measured i.e. rules directly implying freebase test relations. consequently increase prediction performance improved predicate embedding space beyond predicates explicitly stated provided rules. example injecting computational efﬁciency lifted rule injection order assess computational efﬁciency proposed method measure time needed training epoch using single .ghz core. measure average epoch using rules using ﬁltered unﬁltered rules respectively. increasing number rules leads increase computation time. furthermore using rules adds overhead computation time needed learning ground atoms. demonstrates lifted rule injection scales well number rules. asymmetry concern incorporating implications vector space vector representation predicates head body simply moving closer together. would violate asymmetry implications. experiments might observe problem testing well model predicts facts freebase relations well predict textual surface form patterns. thus perform following experiment. incorporating wordnet rules form head body select constant pairs observe body training set. implication holds score body atom incorporate rule vector space contrast matrix factorization restricted constant embedding space often high predictions both body head atom. suggests matrix factorization merely captures similarity predicates. contrast injecting implications trained predicate representations indeed yield asymmetric predictions. given high score body ground atom model predicts high score head vice versa. reason also high score body ground atoms fourth rule newspaper daily synonymously used training texts. related work recent research combining rules learned vector representations important developments ﬁeld automated completion. wang demonstrated different types rules incorporated using integer linear programming approach. wang cohen learned embeddings facts ﬁrst-order logic rules using matrix factorization. approaches method presented previous chapter ground ﬁrst-order rules constants domain. limits scalability towards large rule sets large domains constants. formed important motivation lifted rule injection model construction suffer limitation. proposed alternative strategy tackle scalability problem reasoning ﬁltered subset ground atoms. proposed path ranking algorithm capturing long-range interactions entities conjunction modeling pairwise relations. model differs substantially approach consider pairs constants instead separate constants inject provided rules. creating partial order relation embeddings result injecting implication rules model also capture interactions beyond predicates directly mentioned rules demonstrated section injecting rules surface patterns measuring improvement predictions structured freebase relations. combining logic distributed representations also active ﬁeld research outside automated completion. recent advances include work faruqui injected ontological knowledge wordnet word embeddings improve performance downstream tasks. furthermore vendrov proposed enforce partial order embeddings space images phrases. method related order embeddings since deﬁne partial order relation embeddings. extend work automated completion ensure implications hold pairs constants introducing restriction embedding space constant pairs. another important contribution recent work proposed framework injecting rules general neural network architectures jointly training target outputs rule-regularized predictions provided so-called teacher network. although quite different ﬁrst sight work could offer model various neural network architectures integrating proposed lifted loss teacher network. summary presented fast approach incorporating ﬁrst-order implication rules distributed representations predicates automated completion. termed approach lifted rule injection main contribution previous chapter fact avoids costly grounding ﬁrst-order implication rules thus independent size domain constants. construction rules satisﬁed observed unobserved ground atom. presented approach requires restriction embedding space constant pairs. however experiments large-scale real-world show impair expressiveness learned representations. contrary appears beneﬁcial regularization effect. incorporating rules generated wordnet hypernyms model improved matrix factorization baseline completion. especially domains annotation costly small amounts training facts available approach provides leverage external knowledge sources efﬁciently inferring facts. downside lifted rule injection method presented applicable implication rules using matrix factorization neural link prediction model. furthermore unclear regularizing predicate representations pushed without constraining embedding space much. speciﬁcally unclear complex rules transitivity incorporated lifted way. hence exploring direct synthesis representation learning ﬁrst-order logic inference next chapter. current state-of-the-art methods automated knowledge base completion neural link prediction models learn distributed vector representations symbols scoring atoms subsymbolic representations enable models generalize unseen facts encoding similarities vector predicate symbol grandfatherof similar vector symbol grandpaof predicates likely express similar relation. likewise vector constant symbol lisa similar maggie similar relations likely hold constants simple form reasoning based similarities remarkably effective automatically completing large kbs. however practice often important capture complex reasoning patterns involve several inference steps. example father homer homer parent bart would like infer grandfather bart. transitive reasoning inherently hard neural link prediction models learn score facts locally. contrast symbolic theorem provers like prolog enable exactly type multi-hop reasoning. furthermore inductive logic programming builds upon provers learn interpretable rules data exploit reasoning kbs. however symbolic provers lack ability learn subsymbolic representations similarities large limits ability generalize queries similar identical symbols. connection logic machine learning addressed statistical relational learning approaches models traditionally support reasoning subsymbolic representations using subsymbolic representations trained end-to-end training data beltagy neural multi-hop reasoning models address aforementioned limitations extent encoding reasoning chains vector space iteratively reﬁning subsymbolic representations question comparison answers. many ways models operate like basic theorem provers lack crucial ingredients interpretability straightforward ways incorporating domain-speciﬁc knowledge form rules. approach problem inspired recent neural network architectures like neural turing machines memory networks neural stacks/queues neural programmer neural programmer-interpreters hierarchical attentive memory differentiable forth interpreter architectures replace discrete algorithms data structures end-to-end differentiable counterparts operate real-valued vectors. heart approach idea translate concept basic symbolic theorem provers hence combine advantages ability reason vector representations predicates constants. speciﬁcally keep variable binding symbolic compare symbols using subsymbolic vector representations. chapter introduce neural theorem provers end-to-end differentiable provers basic theorems formulated queries prolog’s backward chaining algorithm recipe recursively constructing neural networks capable proving queries using subsymbolic representations. success score proofs differentiable respect vector representations symbols enables learn representations predicates constants ground atoms well parameters function-free ﬁrst-order logic rules predeﬁned structure. ntps learn place representations similar symbols close proximity vector space induce rules given prior assumptions structure logical relationships transitivity. furthermore ntps seamlessly reason provided domain-speciﬁc rules. ntps operate distributed representations symbols single hand-crafted rule leveraged many proofs queries symbols similar representation. finally ntps demonstrate high degree interpretability induce latent rules decode human-readable symbolic rules. figure module mapping upstream proof state list proof states thereby extending substitution adding nodes computation graph neural network representing proof success. contributions threefold present construction ntps inspired prolog’s backward chaining algorithm differentiable uniﬁcation operation using subsymbolic representations propose optimizations architecture joint training neural link prediction model batch proving approximate gradient calculation experimentally show ntps learn representations symbols function-free ﬁrst-order rules predeﬁned structure enabling learn perform multi-hop reasoning benchmark outperform complex state-of-the-art neural link prediction model three four kbs. differentiable prover following describe recursive construction ntps neural networks end-to-end differentiable proving allow calculate gradient proof successes respect vector representations symbols. deﬁne construction ntps terms modules similar dynamic neural module networks module takes inputs discrete objects proof state returns list proof states proof state tuple consisting substitution constructed proof neural network outputs real-valued success score proof. discrete objects substitution used construction neural network network constructed continuous proof success score calculated many different goals training test time. summarize modules instantiated discrete objects substitution set. construct neural network representing proof success score recursively instantiate submodules continue proof. shared signature modules domain controls construction network domain proof states number output proof states. furthermore denote substitution proof state denote neural network calculating proof success. akin pseudocode backward chaining chapter pseudocode style functional programming language deﬁne behavior modules auxiliary functions. uniﬁcation module uniﬁcation atoms e.g. goal want prove rule head central operation backward chaining. non-variable symbols checked equality proof aborted check fails. however want able apply rules even symbols goal head equal similar meaning thus replace symbolic comparison computation measures similarity symbols vector space. module unify updates substitution creates neural network comparing vector representations non-variable symbols sequences terms. signature module domain lists terms. unify takes atoms represented lists terms upstream proof state maps proof state unify iterates list terms atoms compares symbols. symbols variable substitution added substitution set. otherwise vector representations non-variable symbols compared using radial basis function kernel hyperparameter experiments. following pseudocode implements unify. note matches every argument order matters i.e. arguments match line subsequent lines evaluated. dimension vector representations symbols. furthermore fail represents uniﬁcation failure mismatching arity atoms. failure reached abort creation neural network branch proving. addition constrain proofs cycle-free checking whether variable already bound. note simple heuristic prohibits applying non-ground rule twice. sophisticated ways ﬁnding avoiding cycles proof graph rule still applied multiple times leave future work. example assume unifying atoms given upstream proof state latter input atom placeholders predicate constant neural network would output evaluated. furthermore assume grandpaof bart represent indices respective symbols global symbol vocabulary. then proof state constructed unify thus output score neural network high subsymbolic representation input close grandpaof input close bart. however score cannot higher upstream proof success score forward pass neural network note addition extending neural network module also outputs substitution {q/abe} graph creation time used instantiate submodules. furthermore note uniﬁcation applied multiple times proof involves step resulting chained application kernel operations. choice stems property successful proof uniﬁcations successful could also realized multiplication uniﬁcation scores along proof would likely result unstable optimization longer proofs exploding gradients. based unify deﬁne module attempts apply rules signature domain goal atoms domain integers used specifying maximum proof depth neural network. furthermore number possible output proof states goal given structure provided implement denotes rule given head atom list body atoms contrast symbolic method module able grandfatherof rule query involving grandpaof provided subsymbolic representations predicates similar measured kernel unify module. creation neural network dependent also structure goal. instance goal would result different neural network hence different number output proof states ﬁrst lines deﬁne failure proof either upstream uniﬁcation failure passed module maximum proof depth reached line speciﬁes proof success i.e. list subgoals empty maximum proof depth reached. lastly line deﬁnes recursion ﬁrst subgoal proven instantiating module substitutions applied every resulting proof state used proving remaining subgoals instantiating modules. figure exemplary construction computation graph knowledge base. indices arrows correspond application respective rule. proof states subscripted sequence indices rules applied. underlined proof states aggregated obtain ﬁnal proof success. boxes visualize instantiations modules proofs fail cycle-detection example figure illustrates examplary computation graph constructed note constructed training used proving goals structure training test time index input predicate indices input constants. final proof states used proof aggregation underlined. neural inductive logic programming ntps gradient descent instead combinatorial search space rules example done first order inductive learner speciﬁcally using concept learning entailment induce rules prove known ground atoms give high proof success scores sampled unknown ground atoms. representations unknown predicates indices respectively. prior knowledge transitivity three unknown predicates speciﬁed call parameterized rule corresponding predicates unknown representations learned data. rule used proofs training test time given rule. training predicate representations parameterized rules optimized jointly subsymbolic representations. thus model adapt parameterized rules proofs known facts succeed proofs sampled unknown ground atoms fail thereby inducing rules predeﬁned structures like above. inspired wang cohen rule templates conveniently deﬁning structure multiple parameterized rules specifying number parameterized rules instantiated given rule structure rule decoding implicit rule conﬁdence inspection training decode parameterized rule searching closest representations known predicates. given induced rule trained closest representation known predicate every parameterized predicate representation rule formally decode predicate symbol predicates using addition provide users rule conﬁdence taking minimum similarity unknown decoded predicate representations using kernel unify. list predicate representations parameterized rule. conﬁdence rule calculated optimization section present basic training loss ntps training loss neural link prediction models used auxiliary task well various computational optimizations. training objective known facts given usually observe negative facts thus resort sampling corrupted ground atoms done previous work speciﬁcally every obtain corrupted ground atoms sampling ˆj˜i constants. corrupted ground atoms resampled every iteration training denote known corrupted ground atoms together target score negative log-likelihood proof success score loss function parameters given training ground atom target proof success score. note since application training facts ground atoms make proof success score substitution list resulting proof state. prove known facts trivially uniﬁcation themselves resulting parameter updates training hence generalization. therefore training masking calculation uniﬁcation success known ground atom want prove. speciﬁcally uniﬁcation score temporarily hide training fact assume proven facts rules beginning training subsymbolic representations initialized randomly. unifying goal facts consequently noisy success scores early stages training. moreover maximum success score result gradient updates respective subsymbolic representations along maximum proof path take long time ntps learn place similar symbols close vector space make effective rules. speed learning subsymbolic representations train ntps jointly complex complex share subsymbolic representations feasible kernel unify also deﬁned complex vectors. responsible multi-hop reasoning neural link prediction model learns score ground atoms locally. test time used predictions. thus training loss complex seen auxiliary loss subsymbolic representations learned ntps described suffer severe computational limitations since neural network representing possible proofs predeﬁned depth. contrast symbolic backward chaining proof aborted soon uniﬁcation fails differentiable proving uniﬁcation failure atoms whose arity match detect cyclic rule application. propose optimizations speed ntps. first make modern gpus batch processing many proofs parallel second exploit sparseness gradients caused operations used uniﬁcation proof aggregation respectively derive heuristic truncated forward backward pass drastically reduces number proofs considered calculating gradients vectors ones respectively square root taken element-wise. practice partition rules structure batch-unify goals rule heads partition time graphics processing unit furthermore substitution sets bind variables vectors symbol indices instead single symbol indices operations taken goal. gradient approximation ntps allow calculate gradient proof success scores respect subsymbolic representations rule parameters. backpropagating large computation graph give exact gradient computationally infeasible reasonably-sized consider parameterized rule assume given contains facts binary predicates. bound respective representations goal substituted every possible second argument facts proving ﬁrst atom body. moreover substitutions need compare facts proving second atom body rule resulting proof success scores. however note since operator aggregating success different proofs subsymbolic representations proofs receive gradients. overcome computational limitation propose following heuristic. assume unifying ﬁrst atom facts unlikely uniﬁcation successes successes attain maximum proof success unifying remaining atoms body rule facts uniﬁcation ﬁrst atom keep substitutions success scores continue proving these. means partial proofs contribute forward pass stage consequently receive gradients backward pass backpropagation. term heuristic. note cannot guarantee anymore gradient proof success exact gradient large enough close approximation true gradient. experiments consistent previous work carry experiments four benchmark compare complex ntpλ terms area precisionrecall-curve countries mean reciprocal rank hitsm described below. training details including hyperparameters rule templates found section countries countries dataset introduced bouchard testing reasoning capabilities neural link prediction models. consists countries regions subregions facts neighborhood countries figure overview different tasks contries dataset visualized nickel left part shows atoms removed task right part illustrates rules used infer location test countries. task facts corresponding blue dotted line removed training set. task additionally facts corresponding green dashed line removed. finally task also facts dash-dotted line removed. location countries subregions. follow nickel split countries randomly training countries development countries test countries every test country least neighbor training set. subsequently three different task datasets created. tasks goal predict locatedin every test country regions access training atoms varies ground atoms locatedin test country region removed since information subregion test countries still contained task solved using transitivity rule kinship nations umls nations alyawarra kinship uniﬁed medical language system domingos left animals dataset contains unary predicates thus used evaluating multi-hop reasoning. nations contains binary predicates unary predicates constants true facts kinship contains predicates constants true facts umls contains predicates constants true facts. since baseline complex cannot deal unary predicates remove unary atoms nations. split every training facts development facts test facts. evaluation take test fact corrupt ﬁrst second argument possible ways corrupted fact original subsequently predict ranking every test fact corruptions calculate hitsm. training details adam initial learning rate mini-batch size optimization. apply regularization model parameters clip gradient values subsymbolic representations rule parameters initialized using xavier initialization train models epochs repeat every experiment countries corpus times. statistical signiﬁcance tested using independent t-test. models implemented tensorflow maximum proof depth following rule templates number front rule template indicates often parameterized rule given structure instantiated. note rule template speciﬁes predicate representations body shared. results discussion results different model variants benchmark shown table another method inducing rules differentiable automated completion introduced recently yang evaluation setup equivalent protocol however neural link prediction baseline complex already achieves much higher hits results thus focus comparison ntps complex. first note vanilla ntps alone work particularly well compared complex. outperform complex countries nations kinship umls. demonstrates difﬁculty learning subsymbolic representations differentiable prover uniﬁcation alone need auxiliary losses. ntpλ complex auxiliary loss outperforms models majority tasks. difference auc-pr complex ntpλ signiﬁcant countries tasks major advantage ntps inspect induced rules provide interpretable representation model learned. right column table shows examples induced rules ntpλ countries recovered rules needed solving three different tasks. umls induced transitivity rules. relationships particularly hard encode neural link prediction models like complex optimized locally predict score fact. related work combining neural symbolic approaches relational learning reasoning long tradition various proposed architectures past decades review). early proposals neural-symbolic networks limited propositional rules kbann c-ilp neural-symbolic approaches focus ﬁrst-order inference learn subsymbolic vector representations training facts neural prolog clip++ lifted relational neural networks tensorlog logic tensor networks spirit similar ntps need fully ground ﬁrst-order logic rules. however support function terms whereas ntps currently support function-free terms. recent question-answering architectures translate query representations implicitly vector space without explicit rule representations thus easily incorporate domain-speciﬁc knowledge. addition ntps related random walk path encoding models however instead aggregating paths random walks encoding paths predict target predicate reasoning steps ntps explicit uniﬁcation uses subsymbolic representations. allows induce interpretable rules well incorporate prior knowledge either form rules form rule templates deﬁne structure logical relationships expect hold another line work regularizes distributed representations domain-speciﬁc rules approaches learn rules data support restricted subset ﬁrst-order logic. ntps constructed prolog’s backward chaining thus related uniﬁcation neural networks however ntps operate vector representations symbols instead scalar values expressive. ntps learn rules data related systems foil sherlock meta-interpretive learning higher-order dyadic datalog systems operate symbols search discrete space logical rules ntps work subsymbolic representations induce rules using gradient descent. recently yang introduced differentiable rule learning system based tensorlog neural network controller similar lstms method scalable ntps introduced here. however umls kinship baseline already achieved stronger generalization learning subsymbolic representations. still scaling ntps larger competing scalable relational learning methods open problem seek address future work. summary proposed end-to-end differentiable prover automated completion operates subsymbolic representations. used prolog’s backward chaining algorithm recipe recursively constructing neural networks used prove queries speciﬁcally contribution differentiable uniﬁcation operation vector representations symbols construct neural networks. allowed compute gradient proof successes respect vector representations symbols thus enabled train subsymbolic representations end-to-end facts induce function-free ﬁrst-order logic rules using gradient descent. benchmark model outperformed complex state-of-the-art neural link prediction model three four time inducing interpretable rules. ability determine logical relationship natural language sentences integral part machines supposed understand reason language. previous chapters discussed ways combining symbolic logical knowledge subsymbolic representations trained neural networks. ﬁrst steps towards models reason natural language used textual surface form patterns predicates automated knowledge base completion. however automated completion assumed surface patterns atomic generalize unseen patterns compositional representation. chapter using recurrent neural networks learning compositional representations natural language sentences. speciﬁcally tackling task recognizing textual entailment i.e. determining whether natural language sentences contradicting other whether unrelated whether ﬁrst sentence entails second sentence instance sentence girls involved eating contest entails three people stufﬁng faces contradicts three people drinking beer boat. task important since many natural language processing tasks information extraction relation extraction text summarization machine translation rely explicitly implicitly could beneﬁt accurate systems specialized subcomponents negation detection despite success neural networks paraphrase detection end-to-end differentiable neural architectures failed reach acceptable performance lack large high-quality datasets. end-to-end differentiable solution desirable since avoids speciﬁc assumptions underlying language. particular need language features like part-of-speech tags dependency parses. furthermore generic sequence-to-sequence solution allows extend concept capturing entailment across sequential data natural language. recently bowman published stanford natural language inference corpus accompanied long short-term memory baseline achieves accuracy dataset. ﬁrst instance generic neural model without hand-crafted features close accuracy simple lexicalized classiﬁer engineered features rte. explained high quality size snli compared orders magnitude smaller partly synthetic datasets used evaluate systems. bowman al.’s lstm encodes premise hypothesis independently dense ﬁxed-length vectors whose concatenation subsequently used multi-layer perceptron classiﬁcation contrast proposing neural network capable ﬁne-grained comparison pairs words phrases processing hypothesis conditioned premise using neural attention mechanism. contributions threefold present neural model based lstms read sentences determine entailment opposed mapping sentence independently vector space extend model neural word-by-word attention mechanism encourage ﬁne-grained comparison pairs words phrases provide detailed qualitative analysis neural attention benchmark lstm achieves accuracy snli outperforming simple lexicalized classiﬁer tailored percentage points. extension word-by-word neural attention surpasses strong benchmark lstm result percentage points achieving accuracy recognizing entailment snli. parameterized differentiable cell function maps input vector previous state output vector next state simplicity assume input size output size same i.e. applying cell function time-step obtain output vector next state tanh element-wise application hyperbolic function. call fullyconnected cell function modeled dense layer. illustrate computation graph single application fully-connected cell fig. note recurrent application cell function sequence inputs parameters shared time steps. rnns lstm units successfully applied wide range tasks machine translation constituency parsing language modeling recently lstms encompass memory cells store information long period time well three types gates control information cells input gates forget gates output gates given input vector trained biases parameterize gates transformations input. previous chapters denotes element-wise application sigmoid function element-wise multiplication vectors. corresponding computation graph illustrated fig. sequence words representing premise represent hypothesis. both premise hypothesis encoded ﬁxed-dimensional vectors taking last output vector applying function lstm cell function lstm subsequently prediction three classes obtained followed softmax independent sentence encoding straightforward model rte. however questionable efﬁciently entire sentence represented single ﬁxed-dimensional vector. hence next section investigate various neural architectures tailored towards ﬁne-grained comparison premise hypothesis thus require represent entire sentences ﬁxed-sized vectors embedding space. first propose encode hypothesis conditioned representation premise subsequently introduce extension lstm neural attention word-by-word attention finally show attentive models easily used attending ways premise conditioned hypothesis hypothesis conditioned premise models trained using loss predict probability class using differ calculated. contrast learning sentence representations interested neural models read sentences determine entailment thereby comparing pairs words phrases. figure shows high-level structure model. premise read lstm. second lstm different parameters reading delimiter hypothesis memory state initialized last state previous lstm processes hypothesis model illustrated example fig. note premise still encoded ﬁxed-dimensional vector lstm processes hypothesis keep track whether incoming words contradict premise whether entailed whether unrelated. inspired ﬁnite-state automata proposed natural logic inference attentive neural networks recently demonstrated success wide range tasks ranging handwriting synthesis digit classiﬁcation machine translation image captioning speech recognition sentence summarization code summarization geometric reasoning idea allow model attend past output vectors. lstms mitigates cell state bottleneck i.e. fact figure attention model rte. compared fig. model represent entire premise cell state instead output context representations later queried attention mechanism also note used. lstm attention capture entire content premise cell state. instead sufﬁcient output vectors reading premise accumulating representation cell state informs second lstm output vectors premise needs attend determine class. ﬁrst lstm produced reading words premise. furthermore last output vector premise vector ones hypothesis processed lstms. attention mechanism produce vector attention weights weighted representation premise many times words premise transformed hence intermediate attention representation word premise obtained non-linear combination premise’s output vector transformed attention weight word premise result weighted combination values attention model illustrated fig. note model represent entire premise cell state instead output context representations later queried attention mechanism. informally illustrated mapping input phrases output context representations. determining whether sentence entails another desirable check entailment contradiction individual word phrase pairs. encourage behavior employ neural word-by-word attention similar bahdanau hermann rush difference attention generate words obtain sentence pair encoding ﬁne-grained comparison soft-alignment word phrase pairs premise hypothesis. case amounts attending ﬁrst lstm’s output vectors premise second lstm processes hypothesis word time. consequently obtain attention weight-vectors premise output vectors every word hypothesis. modeled follows note number word representations used output context representation known. illustration purposes depicted case information three words contribute output context representation. figure word-by-word attention model rte. compared fig. querying memory multiple times allows model store ﬁne-grained information output vectors processing premise. also note also used. previous section ﬁnal sentence pair representation obtained non-linear combination last attention-weighted representation premise last output vector using word-by-word attention model illustrated fig. compared attention model introduced earlier querying memory multiple times allows model store ﬁne-grained information output vectors processing premise. informally illustrate fewer words contributing output context representations attend premise conditioned hypothesis well attend hypothesis conditioned premise simply swapping sequences. produces sentence pair representations concatenate classiﬁcation. experiments conduct experiments stanford natural language inference corpus corpus orders magnitude larger existing corpora sentences involving compositional knowledge furthermore large part training examples sick generated heuristically examples. contrast sentence pairs snli stem human annotators. size quality snli make suitable resource training neural architectures ones proposed chapter. training details pretrained wordvec vectors word representations keep ﬁxed training. out-of-vocabulary words training randomly initialized uniformly sampling values optimized training. out-of-vocabulary words encountered inference time validation test corpus ﬁxed random vectors. tuning representations words wordvec vectors ensure test time representation stays close unseen similar words contained wordvec. adam optimization ﬁrst momentum coefﬁcient second momentum coefﬁcient every model perform grid search combinations initial learning rate dropout -regularization strength subsequently take best conﬁguration based performance validation evaluate conﬁguration test set. results discussion results snli corpus summarized table total number model parameters including tunable word representations denoted |θ|w+m ensure comparable number parameters bowman al.’s model encodes premise hypothesis independently using lstm also experiments conditional encoding parameters lstms shared opposed using independent lstms. addition compare attentive models benchmark lstms whose hidden sizes chosen least many parameters attentive models since tuning word vectors wordvec embeddings total number parameters |θ|w+m models considerably smaller. also compare models benchmark lexicalized classiﬁer used bowman uses features based bleu score premise hypothesis length difference word overlap unibigrams part-of-speech tags well cross unibigrams. conditional encoding found processing hypothesis conditioned premise instead encoding sentences independently gives improvement percentage points accuracy bowman al.’s lstm. argue information able part model processes premise part processes hypothesis. speciﬁcally model waste capacity encoding hypothesis read hypothesis focused checking words phrases contradiction entailment based semantic representation premise interpretation lstm approximating ﬁnite-state automaton another difference bowman al.’s model using wordvec instead glove word representations importantly ﬁne-tune word embeddings. drop accuracy train test less severe models suggest ﬁne-tuning word embeddings could cause overﬁtting. lstm outperforms simple lexicalized classiﬁer percentage points. best knowledge time publication ﬁrst instance neural end-to-end differentiable model outperforming hand-crafted pipeline textual entailment dataset. attention incorporating attention mechanism observe percentage point improvement single lstm hidden size percentage point increase benchmark model uses lstms conditional encoding attention model produces output vectors summarizing contextual information premise useful attend later reading hypothesis. therefore reading premise model build semantic representation whole premise instead representation helps attending premise’s output vectors processing hypothesis contrast output vectors premise used baseline conditional model. thus models build representation entire premise carry cell state part processes hypothesis—a bottleneck overcome degree using attention. word-by-word attention enabling model attend output vectors premise every word hypothesis yields another percentage point improvement compared attending once. argue explained model able check entailment contradiction individual word phrase pairs demonstrate effect qualitative analysis below. two-way attention allowing model also attend hypothesis based premise improve performance experiments. suspect entailment asymmetric relation. hence using lstm encode hypothesis premise might lead noise training signal. could addressed training different lstms cost doubling number model parameters. qualitative analysis instructive analyze output representations model attending deciding class example. note interpretations based attention weights taken care since model forced solely eqs. rely representations obtained attention contradiction caused single word multiple words interestingly model shows sensitivity context attending yellow color pink color coat. however involved examples longer premises found attention uniformly distributed suggests conditioning attention last output representation limitations multiple words need considered deciding class. word-by-word attention visualizations word-by-word attention depicted fig. found word-by-word attention easily detect hypothesis simply reordering words premise furthermore able resolve synonyms capable matching multi-word expressions single words also noteworthy irrelevant parts premise words capturing little meaning whole uninformative relative clauses correctly neglected determining entailment attention fail example sentences words entirely unrelated cases model seems back attending function words sentence pair representation likely dominated last output vector instead attention-weighted representation methods chapter published rockt¨aschel since many models proposed. roughly classiﬁed sentence encoding models extend independent encoding lstm bowman models related conditional encoding architecture presented section results follow-up works collected leaderboard http//nlp.stanford.edu/projects/snli/. current best result held bidirectional lstm matching aggregation layers introduced wang achieves test accuracy outperforms best independent encoding model percentage points. fact independent encoding models reach performance conditional model word-by-word attention. exceptions recently introduced two-stage bidirectional lstm model neural semantic encoder munkhdalai models presented chapter make little assumptions input data applied domains too. augenstein introduced conditional encoding model determining stance tweet respect target denotes forward reversed sequence rk×k trainable projection matrices trainable forward reverse start state respectively. architecture illustrated fig. achieved second best result semeval task twitter stance detection corpus instead predicting logical relationship sentences kolesnyk used entailment pairs snli corpus learn generate entailed sentence given premise. model encoder-decoder attention used neural machine translation manually annotated test corpus generated sentences model generated correct entailed sentences cases. recursively applying encoder-decoder produced outputs model able generate natural language inference chains wedding party looks happy picture bride groom smiles couple smiling people smiling. summary chapter demonstrated lstms read pairs sequences produce ﬁnal representation simple classiﬁer predicts entailment outperform lstm baseline encoding sequences independently well classiﬁer hand-engineered features. besides contributing conditional model main contribution extend model attention premise provides improvements predictive abilities system. qualitative analysis showed word-by-word attention model able compare word phrase pairs deciding logical relationship sentences. first proposed calculate gradient propositional logic rules respect parameters neural link prediction model stochastically grounding ﬁrst-order logic rules able rules regularizers matrix factorization neural link prediction model automated knowledge base completion. allowed embed background knowledge form logical rules vector space predicate entity pair representations. using method able train relation extractors predicates provided rules little known training facts. chapter identiﬁed various shortcomings stochastic grounding proposed model implication rules used regularize predicate representations. advantages method becomes independent size domain entity pairs guarantee provided rules hold test entity pair. restricting entity pair embedding space non-negative able impose implications partial order predicate representation space similar order embeddings showed empirically restricting entity pair embedding space model generalizes better predicting facts test attribute regularization effect. furthermore showed incorporating implication rules method scales well number rules. investigating ways regularizing symbol representations based rules chapter proposed differentiable prover performs inference symbol representations explicit way. used prolog’s backward chaining algorithm recipe recursively constructing neural networks used prove facts speciﬁcally proposed differentiable uniﬁcation operation symbol representations. constructed neural network allows compute gradient proof success respect symbol representations thus train symbol representation end-to-end proof outcome. furthermore given templates unknown rules predeﬁned structure induce ﬁrst-order logic rules using gradient descent. proposed three optimizations model implemented uniﬁcation multiple symbol representations batch-operation allows make modern graphics processing units efﬁcient proving proposed approximation gradient following proofs used neural link prediction models regularizers prover learn better symbol representations quickly. three four benchmark knowledge bases method outperforms complex state-of-the-art neural link prediction model time inducing interpretable rules. lastly developed neural models recognizing textual entailment i.e. determining logical relationship natural language sentences used long short-term memory encode ﬁrst sentence conditioned representation encoded second sentence using second lstm deciding label sentence pair. furthermore extended model neural word-by-word attention mechanism enables ﬁne-grained comparison word phrase pairs. large corpus models outperform classiﬁer hand-engineered features strong lstm baseline. addition qualitatively analyzed attention model pays words ﬁrst sentence able conﬁrm presence ﬁne-grained reasoning patterns. limitations future work integration neural representations symbolic logic reasoning remains exciting open research area except much systems improving representation learning models taking inspiration formal logic future. demonstrated beneﬁt regularizing symbol representations logical rules automated completion able efﬁciently simple implication rules. future work would interesting general ﬁrst-order logic rules regularizers predicate representations lifted instance informed grounding ﬁrst-order rules. however likely approach regularizing predicate representations using rules theoretical limitations need investigated further. thus believe end-to-end differentiable neural theorem prover introduced thesis ﬁrst proposal towards tight integration symbolic reasoning systems trainable rules symbol representations. major obstacle encountered computational complexity making proof success differentiable calculate gradient respect symbol representations. possible approximate gradient maintaining proofs given query point uniﬁcation query facts necessary. real-world contain millions facts grounding becomes impossible efﬁciently without applying heuristics even using modern gpus. possible future direction could hierarchical attention recent methods reinforcement learning monte carlo tree search used instance learning play chemical synthesis planning speciﬁcally idea would train model learns select promising rules instead trying rules proving goal. orthogonal that ﬂexible individual components end-to-end differentiable provers conceivable. instance uniﬁcation rule selection rule application could modeled parameterized functions thus could used learn optimal behavior data behavior speciﬁed closely following backward chaining algorithm. furthermore constructed prolog’s backward chaining currently support datalog logic programs i.e. function-free ﬁrst-order logic. open question enable support function terms end-to-end differentiable provers. another open research direction extension automated provers handle natural language sentences questions perform multi-hop reasoning natural language sentences. starting point could combination models proposed determining logical relationship natural language sentences differentiable prover. end-to-end differentiable prover introduced thesis used calculate gradient proof success respect symbol representations symbol representations composed encoder trained jointly prover. vision prover directly operates natural language statements explanations avoiding need semantic parsing i.e. parsing text logical form. ntps decompose inference explicit would worthwhile investigate whether obtain interpretable natural language proofs. furthermore would interesting scale methods presented larger units text entire documents. again needs model extensions hierarchical attention ensure computational efﬁciency. addition would worthwhile exploring other structured forms attention forms differentiable memory could help improve performance neural networks differentiable proving. lastly interested applying ntps automated proving mathematical theorems either logical natural language form similar recent work kaliszyk loos mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro gregory corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal j´ozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek gordon murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. corr abs/. http//arxiv.org/abs/.. rami al-rfou guillaume alain amjad almahairi christof angerm¨uller dzmitry bahdanau nicolas ballas fr´ed´eric bastien justin bayer anatoly belikov alexander belopolsky yoshua bengio arnaud bergeron james bergstra valentin bisson josh bleecher snyder nicolas bouchard nicolas boulangerlewandowski xavier bouthillier alexandre br´ebisson olivier breuleux pierre carrier kyunghyun chorowski paul christiano cooijmans marc-alexandre cˆot´e myriam cˆot´e aaron courville yann dauphin olivier delalleau julien demouth guillaume desjardins sander dieleman laurent dinh melanie ducoffe vincent dumoulin samira ebrahimi kahou dumitru erhan ziye orhan firat mathieu germain xavier glorot goodfellow matthew graham aglar g¨ulc¸ehre philippe hamel iban harlouchet jean-philippe heng bal´azs hidasi sina honari arjun jain s´ebastien jean mikhail korobov vivek kulkarni alex lamb pascal lamblin eric larsen c´esar laurent sean simon lefranc¸ois simon lemieux nicholas l´eonard zhouhan jesse livezey cory lorenz jeremiah lowin qianli pierreantoine manzagol olivier mastropietro robert mcgibbon roland memisevic bart merri¨enboer vincent michalski mehdi mirza alberto orlandi christopher joseph razvan pascanu mohammad pezeshki colin raffel daniel renshaw matthew rocklin adriana romero markus roth peter sadowski john salvatier franc¸ois savard schl¨uter john schulman gabriel schwartz iulian vlad serban dmitriy serdyuk samira shabanian ´etienne simon sigurd spieckermann ramana subramanyam jakub sygnowski j´er´emie tanguay gijs tulder joseph turian sebastian urban pascal vincent francesco visin harm vries david warde-farley dustin webb matthew willson kelvin lijun saizheng zhang ying zhang. theano python framework fast computation mathematical expressions. corr abs/. http//arxiv.org/abs/.. miltiadis allamanis peng charles sutton. convolutional attention network extreme summarization source code. proceedings international conference machine learning icml york city june pages http//jmlr.org/ proceedings/papers/v/allamanis.html. miltiadis allamanis pankajan chanthirasegaran pushmeet kohli charles sutton. learning continuous semantic representations symbolic expressions. proceedings international conference machine learning icml sydney australia august pages http//proceedings.mlr.press/v/allamanisa.html. jacob andreas marcus rohrbach trevor darrell klein. learning compose neural networks question answering. naacl conference north american chapter association computational linguistics human language technologies diego california june pages http//aclweb.org/ anthology/n/n/n-.pdf. marcin andrychowicz misha denil sergio gomez colmenarejo matthew hoffman david pfau schaul nando freitas. learning learn advances neural information gradient descent gradient descent. processing systems annual conference neural information processing systems december barcelona spain pages gabor angeli christopher manning. naturalli natural logic inference common sense reasoning. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages http//aclweb.org/anthology/d/d/d.pdf. isabelle augenstein rockt¨aschel andreas vlachos kalina bontcheva. stance detection bidirectional conditional encoding. proceedings conference empirical methods natural language processing emnlp austin texas november pages http//aclweb.org/anthology/d/d/d-.pdf. franz baader bernhard ganter baris sertkaya ulrike sattler. completing description logic knowledge bases using formal concept analysis. ijcai proceedings international joint conference artiﬁcial intelligence hyderabad india january pages http //ijcai.org/proceedings//papers/.pdf. islam beltagy cuong chau gemma boleda garrette katrin raymond mooney. montague meets markov deep semantics probabilistic logical form. proceedings second joint conference lexical computational semantics *sem june atlanta georgia usa. pages http//aclweb.org/anthology/s/ s/s-.pdf. islam beltagy katrin raymond mooney. probabilistic soft logic proceedings annual meeting semantic textual similarity. association computational linguistics june baltimore volume long papers pages http//aclweb.org/anthology/p/p/p-.pdf. islam beltagy stephen roller pengxiang cheng katrin raymond mooney. representing meaning combination logical form vectors. corr abs/. http//arxiv.org/abs/. islam beltagy stephen roller pengxiang cheng katrin raymond mooney. representing meaning combination logical distributional models. computational linguistics kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring human proceedings sigmod international conference knowledge. management data sigmod vancouver canada june pages ./.. http//doi.acm.org/./.. antoine bordes jason weston ronan collobert yoshua bengio. learning structured embeddings knowledge bases. proceedings twenty-fifth aaai conference artiﬁcial intelligence aaai francisco california august http//www.aaai.org/ocs/index. php/aaai/aaai/paper/view/. antoine bordes nicolas usunier alberto garc´ıa-dur´an jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. pages http//papers.nips.cc/paper/-translatingembeddings-for-modeling-multi-relational-data. johan bos. wide-coverage semantic analysis boxer. semantics text processing. step conference proceedings volume research computational semantics pages college publications johan katja markert. recognising textual entailment logical inference. hlt/emnlp human language technology conference conference empirical methods natural language processing proceedings conference october vancouver british columbia canada pages http//aclweb.org/anthology/h/h/h.pdf. matko bosnjak rockt¨aschel jason naradowsky sebastian riedel. programming differentiable forth interpreter. international conference machine learning http//arxiv.org/abs/. guillaume bouchard sameer singh theo trouillon. approximate reasoning capabilities low-rank vector spaces. proceedings aaai spring symposium knowledge representation reasoning integrating symbolic neural approaches samuel bowman gabor angeli christopher potts christopher manning. large annotated corpus learning natural language inference. proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages http//aclweb.org/anthology/d/d/d-.pdf. samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. proceedings annual meeting association computational linguistics august berlin germany volume long papers http//aclweb.org/anthology/p/ p/p-.pdf. richard byrd peihuang jorge nocedal ciyou zhu. limited memory algorithm bound constrained optimization. siam scientiﬁc computing http//dx.doi.org/ kai-wei chang wen-tau bishan yang christopher meek. typed tensor decomposition knowledge bases relation extraction. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages http//aclweb.org/ anthology/d/d/d-.pdf. ming-wei chang lev-arie ratinov roth. guiding semi-supervision constraint-driven learning. proceedings annual meeting association computational linguistics june prague czech republic http//aclweb.org/anthologynew/p/p/p-.pdf. chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun yoshua bengio. attention-based models speech recognition. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/attention-based-models-for-speech-recognition. stephen clark stephen pulman. combining symbolic distributional models meaning. quantum interaction papers aaai spring symposium technical report ss-- stanford california march pages http//www.aaai.org/library/symposia/ spring//ss--.php. coecke mehrnoosh sadrzadeh stephen clark. mathematical foundations compositional distributional model meaning. corr abs/. http//arxiv.org/abs/.. advances neural information processing systems pages http//papers.nips.cc/paper/-a-generalizationof-principal-components-analysis-to-the-exponentialfamily. r´emi coulom. efﬁcient selectivity backup operators monte-carlo tree search. computers games international conference turin italy revised papers pages ./--- http//dx.doi.org/./---_. council european union. position council general data protection regulation. http//www.europarl.europa.eu/sed/doc/news/ document/cons_cons_en.docx april dagan oren glickman bernardo magnini. pascal recognising textual entailment challenge. machine learning challenges evaluating predictive uncertainty visual object classiﬁcation recognizing textual entailment first pascal machine learning challenges workshop mlcw southampton april revised selected papers pages http//dx.doi.org/./_ rajarshi arvind neelakantan david belanger andrew mccallum. chains reasoning entities relations text using recurrent neural networks. conference european chapter association computational linguistics http//arxiv.org/abs/.. oier lopez lacalle mirella lapata. unsupervised relation extraction general domain knowledge. proceedings conference empirical methods natural language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group pages http//aclweb.org/ anthology/d/d/d-.pdf. thomas demeester rockt¨aschel sebastian riedel. lifted rule injection relation embeddings. proceedings conference empirical methods natural language processing emnlp austin texas november pages http//aclweb.org/ anthology/d/d/d-.pdf. liya ding. neural prolog-the concepts construction mechanism. systems cybernetics intelligent systems century. ieee international conference volume pages ieee dong evgeniy gabrilovich geremy heitz wilko horn kevin murphy thomas strohmann shaohua zhang. knowledge vault web-scale approach probabilistic knowledge fusion. sigkdd international conference knowledge discovery data mining york august pages http//doi.acm.org/./.. gregory druck gideon mann andrew mccallum. semi-supervised learning dependency parsers using generalized expectation criteria. proceedings annual meeting association computational linguistics international joint conference natural language processing afnlp august singapore pages http//www.aclweb.org/anthology/p-. john duchi elad hazan yoram singer. adaptive subgradient methods online learning stochastic optimization. journal machine learning research http//dl.acm.org/citation. cfm?id=. manaal faruqui jesse dodge sujay kumar jauhar chris dyer eduard hovy noah smith. retroﬁtting word vectors semantic lexicons. naacl conference north american chapter association computational linguistics human language technologies denver colorado june pages http//aclweb. org/anthology/n/n/n-.pdf. manoel franc¸a gerson zaverucha artur d’avila garcez. fast relational learning using bottom clause propositionalization artiﬁcial neural networks. machine learning ./s---. http//dx.doi.org/./s---. luis antonio gal´arraga nicoleta preda fabian suchanek. mining rules align knowledge bases. proceedings workshop automated knowledge base construction akbccikm francisco california october pages ./.. http//doi.acm.org/./.. herv´e gallaire jack minker editors. logic data bases symposium logic data bases centre d’´etudes recherches toulouse advances data base theory york plemum press. isbn ---x. kuzman ganchev jo˜ao grac¸a jennifer gillenwater taskar. posterior regularization structured latent variable models. journal machine learning rematt gardner partha pratim talukdar bryan kisiel mitchell. improving learning inference large knowledge-base using latent syntactic cues. proceedings conference empirical methods natural language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group pages http//aclweb.org/anthology/d/d/ d-.pdf. matt gardner partha pratim talukdar jayant krishnamurthy mitchell. incorporating vector space similarity random walk inference knowledge bases. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages http//aclweb.org/anthology/d/d/d-.pdf. garrette katrin raymond mooney. integrating logical representations probabilistic information using markov logic. proceedings ninth international conference computational semantics iwcs january oxford http//aclweb.org/anthology/ w/w/w-.pdf. xavier glorot yoshua bengio. understanding difﬁculty training deep proceedings thirteenth international feedforward neural networks. conference artiﬁcial intelligence statistics aistats chia laguna resort sardinia italy pages http //www.jmlr.org/proceedings/papers/v/glorota.html. goodfellow yoshua bengio aaron courville. deep learning. adaptive computation machine learning. press isbn ----. http//www.deeplearningbook.org/. alex graves j¨urgen schmidhuber. framewise phoneme classiﬁcation bidirectional lstm neural network architectures. neural networks ./j.neunet.... http//dx. doi.org/./j.neunet.... edward grefenstette. towards formal distributional semantics simulating logical calculi tensors. proceedings second joint conference lexical computational semantics *sem june atlanta georgia usa. pages http//aclweb.org/anthology/s/s/s.pdf. edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce unbounded memory. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/learning-to-transduce-with-unbounded-memory. quan wang lihong wang wang guo. jointly embedding proceedings conference knowledge graphs logical rules. empirical methods natural language processing emnlp austin texas november pages http//aclweb. org/anthology/d/d/d-.pdf. karl moritz hermann phil blunsom. role syntax vector space models compositional semantics. proceedings annual meeting association computational linguistics august karl moritz hermann tom´as kocisk´y edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines advances neural information processread comprehend. systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/-teachingmachines-to-read-and-comprehend. jochen hipp ulrich g¨untzer gholamreza nakhaeizadeh. algorithms association rule mining general survey comparison. sigkdd explorations ./.. http//doi.acm. org/./.. pascal hitzler steffen h¨olldobler anthony karel seda. logic programs connectionist networks. applied logic j.jal.... http//dx.doi.org/./j.jal.. steffen h¨olldobler. structured connectionist uniﬁcation algorithm. proceedings national conference artiﬁcial intelligence. boston massachusetts july august volumes. pages http//www. aaai.org/library/aaai//aaai-.php. steffen h¨olldobler yvonne kalinke hans-peter st¨orr. approximating semantics logic programs recurrent neural networks. appl. intell. ./a. http//dx.doi.org/ ./a. convolutional language sentences. neural network architectures annual advances information processing systems decemconference neural montreal quebec canada pages zhiting xuezhe zhengzhong eduard hovy eric xing. harnessing deep neural networks logic rules. proceedings annual meeting association computational linguistics august berlin germany volume long papers http //aclweb.org/anthology/p/p/p-.pdf. sergio jim´enez george due˜nas julia baquero alexander gelbukh. unalnlp combining soft cardinality features semantic textual similarity relatedness entailment. proceedings international workshop semantic evaluation semevalcoling dublin ireland august pages http//aclweb.org/anthology/s/s/s.pdf. inferring algorithmic patterns stackadvances neural information processing augmented recurrent nets. systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/-inferringalgorithmic-patterns-with-stack-augmented-recurrentnets. cezary kaliszyk franc¸ois chollet christian szegedy. holstep mainternachine learning dataset higher-order logic theorem proving. tional conference learning representations http //arxiv.org/abs/.. ekaterina komendantskaya. uniﬁcation neural networks uniﬁcation errorcorrection learning. logic journal igpl ./jigpal/jzq. http//dx.doi.org/./jigpal/ jzq. alice julia hockenmaier. illinois-lh denotational distributional approach semantics. proceedings international workshop semantic evaluation semevalcoling dublin ireland august pages http//aclweb.org/anthology/s/ s/s-.pdf. mitchell william cohen. random walk inference learning large scale knowledge base. proceedings conference empirical methods natural language processing emnlp july john mcintyre conference centre edinburgh meeting amarnag subramanya fernando pereira william cohen. reading learned syntactic-semantic inference rules. proceedings joint conference empirical methods natural language processing computational natural language learning emnlp-conll july jeju island korea pages http//www. aclweb.org/anthology/d-. yang chengjie xiaolong wang. learning natural language inference using bidirectional lstm model inner-attention. corr abs/. http//arxiv.org/abs/.. sarah loos geoffrey irving christian szegedy cezary kaliszyk. deep network guided proof search. lpar- international conference logic programming artiﬁcial intelligence reasoning maun botswana pages http//www.easychair.org/ publications/paper/. gideon mann andrew mccallum. generalized expectation criteria semisupervised learning conditional random ﬁelds. proceedings annual meeting association computational linguistics june columbus ohio pages http //www.aclweb.org/anthology/p-. marco marelli luisa bentivogli marco baroni raffaella bernardi stefano menini roberto zamparelli. semeval- task evaluation compositional distributional semantic models full sentences semantic relatedness textual entailment. proceedings international workshop semantic evaluation semevalcoling dublin ireland august pages http//aclweb.org/anthology/s/s/s.pdf. tomas mikolov ilya sutskever chen gregory corrado jeffrey dean. distributed representations words phrases compositionality. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. pages http//papers.nips.cc/paper/distributed-representations-of-words-and-phrases-andtheir-compositionality. jeff mitchell mirella lapata. vector-based models semantic composition. proceedings annual meeting association computational linguistics june columbus ohio pages http//www.aclweb.org/anthology/p-. volodymyr mnih nicolas heess alex graves koray kavukcuoglu. recuradvances neural information processrent models visual attention. systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/-recurrent-modelsof-visual-attention. saif mohammad svetlana kiritchenko parinaz sobhani xiao-dan colin cherry. semeval- task detecting stance tweets. proceedings international workshop semantic evaluation semevalnaaclhlt diego june pages http//aclweb.org/anthology/s/s/s-.pdf. lili zhang jin. natural language inference tree-based convolution heuristic matching. proceedings annual meeting association computational linguistics august berlin germany volume short papers http//aclweb.org/anthology/p/p/p-.pdf. stephen muggleton dianhuan alireza tamaddoni-nezhad. metainterpretive learning higher-order dyadic datalog predicate invention revisited. machine learning arvind neelakantan benjamin roth andrew mccallum. compositional proceedings vector space models knowledge base completion. annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing july beijing china volume long papers pages http//aclweb.org/anthology/p/p/p-.pdf. arvind neelakantan quoc ilya sutskever. neural programmer inducing latent programs gradient descent. international conference learning representations http//arxiv.org/abs/. nemirovski yudin. cezari’s convergence steepest descent method approximating saddle point convex-concave functions. soviet mathematics doklady volume maximilian nickel volker tresp hans-peter kriegel. three-way model collective learning multi-relational data. proceedings international conference machine learning icml bellevue washington june july pages maximilian nickel volker tresp hans-peter kriegel. factorizing yago scalable machine learning linked data. proceedings world wide conference lyon france april pages ./.. http//doi.acm.org/ ./.. maximilian nickel lorenzo rosasco tomaso poggio. holographic embeddings knowledge graphs. proceedings thirtieth aaai conference artiﬁcial intelligence february phoenix arizona usa. pages http//www.aaai.org/ocs/index.php/ aaai/aaai/paper/view/. sebastian pad´o mirella lapata. dependency-based construction semantic space models. computational linguistics coli..... http//dx.doi.org/./coli.. david poole. first-order probabilistic inference. ijcai- proceedings eighteenth international joint conference artiﬁcial intelligence acapulco mexico august pages http//ijcai.org/ proceedings//papers/.pdf. steffen rendle christoph freudenthaler zeno gantner lars schmidt-thieme. bayesian personalized ranking implicit feedback. proceedings twenty-fifth conference uncertainty artiﬁcial intelligence montreal canada june pages https//dslpitt.org/uai/displayarticledetails.jsp? mmnu=&smnu=&article_id=&proceeding_id=. sebastian riedel limin andrew mccallum benjamin marlin. relation extraction matrix factorization universal schemas. human language technologies conference north american chapter association computational linguistics proceedings june westin peachtree plaza hotel atlanta georgia pages http//aclweb. org/anthology/n/n/n-.pdf. rockt¨aschel sebastian riedel. learning knowledge base inference neural theorem provers. proceedings workshop automated knowledge base construction akbcnaacl-hlt diego june pages http//aclweb.org/anthology/w/ w/w-.pdf. injecting logical backnaacl ground knowledge embeddings relation extraction. conference north american chapter association computational linguistics human language technologies denver colorado june pages http//aclweb.org/anthology/n/n/n-.pdf. rockt¨aschel edward grefenstette karl moritz hermann tomas kocisky phil blunsom. reasoning entailment neural attention. international conference learning representations alexander rush sumit chopra jason weston. neural attention model abstractive sentence summarization. proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages http//aclweb. org/anthology/d/d/d-.pdf. ivan sanchez rockt¨aschel sebastian riedel sameer singh. towards extracting faithful descriptive representations latent variable models. aaai spring symposium knowledge representation reasoning stefan schoenmackers oren etzioni daniel weld. scaling textual inference web. conference empirical methods natural language processing emnlp proceedings conference october honolulu hawaii meeting sigdat special interest group pages http//www.aclweb.org/anthology/d. stefan schoenmackers jesse davis oren etzioni daniel weld. learning ﬁrstorder horn clauses text. empirical methods natural language processing http//www.aclweb.org/anthology/ stefan schoenmackers jesse davis oren etzioni daniel weld. learning ﬁrst-order horn clauses text. proceedings conference empirical methods natural language processing emnlp october stata center massachusetts meeting sigdat special interest group pages http //www.aclweb.org/anthology/d-. marwin segler mike preuß mark waller. towards alphachem chemical synthesis planning tree search deep neural network policies. corr abs/. http//arxiv.org/abs/.. luciano seraﬁni artur d’avila garcez. logic tensor networks deep learning proceedings logical reasoning data knowledge. international workshop neural-symbolic learning reasoning co-located joint multi-conference human-level artiﬁcial intelligence york city july http //ceur-ws.org/vol-/nesy_paper.pdf. lokendra shastri. neurally motivated constraints working memory capacity production system parallel processing implications connectionist model based temporal synchrony. proceedings fourteenth annual conference cognitive science society july august cognitive science program indiana university bloomington volume page psychology press yelong shen po-sen huang jianfeng weizhu chen. reasonet learning proceedings workshop stop reading machine comprehension. cognitive computation integrating neural symbolic approaches co-located annual conference neural information processing systems barcelona spain december http //ceur-ws.org/vol-/coconips__paper.pdf. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou vedavyas panneershelvam marc lanctot sander dieleman dominik grewe john nham kalchbrenner ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature /nature. http//dx.doi.org/./nature. sameer singh dustin hillard chris leggetter. minimally-supervised extrachuman language technologies tion entities text advertisements. conference north american chapter association computational linguistics proceedings june angeles california pages http//www.aclweb.org/anthology/n-. richard socher eric huang jeffrey pennington andrew christopher manning. dynamic pooling unfolding recursive autoencoders advances neural information processing paraphrase detection. systems annual conference neural information processing systems proceedings meeting held december granada spain. pages http//papers.nips.cc/paper/dynamic-pooling-and-unfolding-recursive-autoencodersfor-paraphrase-detection. richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. pages http//papers.nips.cc/paper/-reasoning-withneural-tensor-networks-for-knowledge-base-completion. gustav sourek vojtech aschenbrenner filip zelezn´y ondrej kuzelka. lifted relational neural networks. proceedings nips workshop cognitive computation integrating neural symbolic approaches co-located annual conference neural information processing systems montreal canada december http//ceur-ws. org/vol-/coconips__paper_.pdf. sainbayar sukhbaatar arthur szlam jason weston fergus. end-to-end memory networks. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages http// papers.nips.cc/paper/-end-to-end-memory-networks. ilya sutskever oriol vinyals quoc sequence sequence learnadvances neural information processing neural networks. systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/-sequence-tosequence-learning-with-neural-networks. kristina toutanova danqi chen patrick pantel hoifung poon pallavi choudhury michael gamon. representing text joint embedding text knowledge bases. proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages http//aclweb.org/anthology/d/d/ d-.pdf. th´eo trouillon johannes welbl sebastian riedel ´eric gaussier guillaume proceedings bouchard. complex embeddings simple link prediction. international conference machine learning icml york city june pages http //jmlr.org/proceedings/papers/v/trouillon.html. ivan vendrov ryan kiros sanja fidler raquel urtasun. order-embeddings images language. international conference learning representations http//arxiv.org/abs/.. patrick verga david belanger emma strubell benjamin roth andrew mccallum. multilingual relation extraction using compositional universal schema. naacl conference north american chapter association computational linguistics human language technologies diego california june pages http//aclweb.org/anthology/n/n/n-.pdf. patrick verga arvind neelakantan andrew mccallum. generalizing unseen entities entity pairs row-less universal schema. corr abs/. http//arxiv.org/abs/.. oriol vinyals meire fortunato navdeep jaitly. pointer networks. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/pointer-networks. oriol vinyals lukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. grammar foreign language. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages http//papers.nips.cc/paper/-grammaras-a-foreign-language. johanna v¨olker mathias niepert. statistical schema induction. semantic research applications extended semantic conference eswc heraklion crete greece -june proceedings part pages ./---- http//dx.doi. org/./----_. quan wang wang guo. knowledge base completion using embeddings rules. proceedings twenty-fourth international joint conference artiﬁcial intelligence ijcai buenos aires argentina july pages http//ijcai.org/abstract//. joint information extraction reasoning scalable statistical relational learning approach. proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing july beijing china volume long papers pages http//aclweb.org/anthology/p/p/p-.pdf. william yang wang william cohen. learning ﬁrst-order logic embeddings matrix factorization. proceedings twenty-fifth international joint conference artiﬁcial intelligence ijcai york july william yang wang kathryn mazaitis william cohen. programming personalized pagerank locally groundable ﬁrst-order probabilistic logic. international conference information knowledge management cikm’ francisco october november pages ./.. http//doi.acm.org/ ./.. zhuoyu zhao kang zhenyu zhengya guanhua tian. large-scale knowledge base completion inferring grounding network proceedings internasampling selected instances. tional conference information knowledge management cikm melbourne australia october pages ./.. http//doi.acm.org/./ song yang zhongfei zhang yueting zhuang. structured embedding pairwise relations long-range interactions knowlproceedings twenty-ninth aaai conference artiﬁedge base. cial intelligence january austin texas usa. pages http//www.aaai.org/ocs/index.php/aaai/aaai/ paper/view/. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend proceedings tell neural image caption generation visual attention. international conference machine learning icml lille bishan yang wen-tau xiaodong jianfeng deng. embedding entities relations learning inference knowledge bases. international conference learning representations http//arxiv.org/abs/.. wenpeng hinrich sch¨utze. convolutional neural network paraphrase identiﬁcation. naacl conference north american chapter association computational linguistics human language technologies denver colorado june pages http//aclweb.org/anthology/n/n/n-.pdf. luke zettlemoyer michael collins. learning sentences logical form structured classiﬁcation probabilistic categorial grammars. proceedings conference uncertainty artiﬁcial intelligence edinburgh scotland july pages https//dslpitt.org/uai/displayarticledetails.jsp? mmnu=&smnu=&article_id=&proceeding_id=. jiang zhao tiantian lan. ecnu stone birds ensemble heterogenous measures semantic relatedness textual entailproceedings international workshop semantic evalument. ation semevalcoling dublin ireland august pages http//aclweb.org/anthology/s/s/s.pdf.", "year": 2017}