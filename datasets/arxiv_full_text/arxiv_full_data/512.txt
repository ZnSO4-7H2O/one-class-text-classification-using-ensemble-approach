{"title": "Full-Network Embedding in a Multimodal Embedding Pipeline", "tag": ["cs.CV", "cs.CL", "cs.NE"], "abstract": "The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.", "text": "current state-of-the-art image annotation image retrieval tasks obtained deep neural networks combine image representation text representation shared embedding space. paper evaluate impact using full-network embedding setting replacing original image representation competitive multimodal embedding generation scheme. unlike one-layer image embeddings typically used approaches full-network embedding provides multi-scale representation images results richer characterizations. measure inﬂuence full-network embedding evaluate performance three different datasets compare results original multimodal embedding generation scheme using one-layer image embedding rest state-of-the-art. results image annotation image retrieval tasks indicate full-network embedding consistently superior one-layer embedding. results motivate integration fullnetwork embedding multimodal embedding generation scheme something feasible thanks ﬂexibility approach. image annotation task automatically associating input image describing text. image annotation methods emerging technology enabling semantic image indexing search applications. complementary task associating input text ﬁtting image also relevance sort applications. state-of-the-art image annotation methods currently based deep neural representations image embedding text embedding combined unique multimodal embedding space. several techniques merging spaces proposed little effort made ﬁnding appropriate image embeddings used process. fact approaches simply one-layer embedding paper explore impact using full-network embedding generate required image embedding replacing one-layer embedding. integrating multimodal embedding pipeline deﬁned kiros based gated recurrent units neural network text encoding image encoding. unlike one-layer embeddings represents features varying speciﬁcity context visual dataset also discretizes features regularize space alleviate curse dimensionality. particularities result richer visual embedding space reliably mapped common visual-textual embedding space. generic pipeline deﬁned kiros recently outperformed image annotation image search tasks methods speciﬁcally targeting tasks choose test contribution pipeline overall competitive performance expecting conclusion generalize applied solutions tasks assumption would dimmer problem-speciﬁc methodology chosen instead. main goal establish competitiveness image representation used caption related tasks. test suitability approach evaluating performance image annotation image retrieval using three publicly available datasets flickrk flickrk mscoco results obtained pipeline including compared original pipeline kiros using one-layer embedding also methods currently obtaining state-of-the-art results three datasets. last years several solutions proposed problem building common representations images text goal enabling cross-domain search paper builds upon methodology described kiros turn based previous works area neural machine translation work kiros deﬁne vectorized representation input text using rnns. setting word text codiﬁed vector using word dictionary vectors grus. last word vector processed activations grus last time step conveys representation whole input text multimodal embedding space. parallel images processed convolutional neural network pre-trained imagenet extracting activations last fully connected layer used representation images. solve dimensionality matching representations afﬁne transformation applied image representation. similarly approach kiros image annotation image retrieval approaches rely features image representation. current best overall performing model fisher vector although performance competitive image retrieval task. computed respect parameters gaussian mixture model hybrid gaussian-laplacian mixture model images text build using deep neural network features; images features wordvec text features. speciﬁc problem image annotation current state-of-art obtained wordvisualvec model approach uses multimodal embedding space visual space images represented involving deeper text processing. finally largest dataset consider best results certain metrics obtained matchcnn based cnns encode image text. multimodal embedding generator pipeline kiros represents images textual captions space. composed main elements generates image embeddings another generates text embeddings. replace original image embedding generator figure overview proposed multimodal embedding generation pipeline integrated fullnetwork embedding. elements colored orange components modiﬁed neural network training phase. testing inputs provided. generates representation input image processing pre-trained extracting neural activations convolutional fully-connected layers. initial feature extraction process performs dimensionality reduction step convolutional activations applying spatial average pooling convolutional ﬁlter. spatial pooling every feature standardized z-values computed whole image train set. standardization process puts value feature context dataset. point meaning single feature value degree feature value atypically high atypically context dataset. zero marks typical behavior. last step pipeline feature discretitzation process. previously standardized embedding usually large dimensionality entails problems related curse dimensionality. usual approach address issue apply dimensionality reduction methods uses different approach reducing expressiveness discretization features keeping dimensionality. speciﬁcally discretization maps feature values domain indicates unusually value indicates feature average value indicates uncommonly high activation mapping standardized values three categories done deﬁnition constant thresholds. approach integrate multimodal embedding pipeline kiros obtain image representation instead output last layer original model does. encoder architecture processing text used using grus recurrent neural network encode sentences. combine embeddings kiros afﬁne transformation image representation identical fully connected neural network layer. extra layer trained simultaneously grus. elements multimodal pipeline tuned training phase model shown orange figure simple terms training procedure consist optimization pairwise ranking loss correct image-caption pair random pair. assuming correct pair elements closer multimodal space random pair. loss formally deﬁned follows image vector correct caption vector sets random images captions respectively. operator deﬁnes cosine similarity. formulation includes margin term avoid pulling image caption closer distance smaller margin. makes optimization focus distant pairs instead improving ones already close. section evaluate impact using multimodal pipeline image annotation image retrieval tasks. properly measure relevance compare results fn-mme original multimodal pipeline reported kiros additionally deﬁne second baseline using original multimodal pipeline training conﬁguration closer used experiments refer second baseline cnn-mme*. flickrk dataset contains hand-selected images flickr depicting actions events. five correct captions provided image. following provided splits images used train used validation kept testing. flickrk dataset extension flickrk. contains photographs everyday activities events scenes. five correct captions provided image. experiments images used training conform validation kept test. splits ones used kiros karpathy fei-fei mscoco dataset includes images everyday scenes containing common objects natural context. captioning images captions available training images captions available validation. captions test publicly available. previous contributions consider using subset validation validation different subset test. cases subsets composed either images corresponding captions image. experiments consider settings. caption sentences word-tokenized using natural language toolkit python choice word embedding size number grus analyzed obtain range suitable parameters test validation set. total number different words flickrk flickrk mscoco. using words present dataset likely produce overﬁtting problems training examples containing words occur times. overﬁtting problem huge impact performance undesired noise multimodal representation. original setup limited word embedding frequent words using grus. bi-lstm model contrast deﬁnes vocabulary size include words appearing time dataset leading dictionaries size flickrk flickrk mscoco. preliminary experiments validation showed increasing multimodal space dimensionality dictionary length slightly improved performance image retrieval detriment image annotation. however combined performance difference remains rather small using non-extreme parameter values since building model solving tasks kept parameters obtaining highest combined score validation set. flickr datasets word embedding limited frequent words. mscoco dataset larger dictionary considering frequent words. cases grus also dimensionality resultant multimodal embedding space. generating image embedding classical architecture source model pretrained imagenet architecture composed convolutional layers combined pooling layers followed fully connected layers ﬁnal softmax output layer. using results image embedding space dimensions. experiments margin parameter batch size image-caption pairs. within batch every possible alternative image-caption pair used contrasting example. models trained epochs best performing model validation chosen gradient clipping grus threshold adam optimization algorithm learning rate flickr datasets mscoco. evaluate image annotation image retrieval following metrics recallk fraction images correct caption ranked within top-k retrieved results results provided table results obtained mscoco dataset. part shows results using images test bottom shows results using images test. recallk median rank best results shown bold. results original paper. image annotation image retrieval tasks flickrk dataset table shows results proposed fn-mme reported results original model cnn-mme results original model using conﬁguration cnn-mme* current state-of-the-art tables analogous flickrk mscoco datasets. additional results cnn-mme model made publicly available later original authors include mscoco dataset evaluated original paper first consider impact using fne. cases multimodal pipeline proposed kiros obtains equal better results using fne. case originally reported results results made available later original authors experiments using conﬁguration fn-mme comparison consider relevant fn-mme cnn-mme* contain least differences besides image embedding used. particular case fn-mme outperforms cnn-mme* percentual points average flickr datasets roughly points mscoco dataset. particularly image annotation tasks performance fn-mme signiﬁcantly closer state variants model remarkably fn-mme provides best reported results image annotation mscoco dataset. however remark competitive method reported results mscoco. results fn-mme image retrieval tasks signiﬁcantly stateof-the-art. overall competitiveness fn-mme increases dataset size. multimodal pipeline kiros using full-network image embedding results consistently higher performances using one-layer image embedding. results suggest visual representation provided superior current standard construction multimodal embeddings. compared current state-of-the-art results obtained fn-mme signiﬁcantly less competitive problem-speciﬁc methods. since happens models using pipeline results indicate original architecture kiros outperformed general problem-speciﬁc techniques. since compatible multimodal pipelines based embeddings future work paper intend evaluate performance integrated current state-of-the-art image annotation image retrieval boost performance obtained kiros pipeline translates methods combination would likely deﬁne state-of-the-art results tasks. work partially supported joint study agreement ibm/bsc deep learning center agreement spanish government programa severo ochoa spanish ministry science technology tin--p project generalitat catalunya core research evolutional science technology program japan science technology agency", "year": 2017}