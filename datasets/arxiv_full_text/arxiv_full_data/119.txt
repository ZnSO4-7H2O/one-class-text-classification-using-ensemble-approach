{"title": "Neural Networks Compression for Language Modeling", "tag": ["stat.ML", "cs.CL", "cs.LG", "cs.NE", "62M45, 68T50", "I.2.7, I.2.6, I.5.1, I.5.4"], "abstract": "In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank (PTB) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.", "text": "abstract. paper consider several compression techniques language modeling problem based recurrent neural networks known conventional rnns lstm-based networks language modeling characterized either high space complexity substantial inference time. problem especially crucial mobile applications constant interaction remote server inappropriate. using penn treebank dataset compare pruning quantization low-rank factorization tensor train decomposition lstm networks terms model size suitability fast inference. neural network models require space disk memory. also need substantial amount time inference. especially important models devices like mobile phones. several approaches solve problems. based sparse computations. also include pruning advanced methods. general approaches able provide large reduction size trained network model stored disk. however problems models inference. caused high computation time sparse computing. another branch methods uses diﬀerent matrixbased approaches neural networks. thus methods based usage toeplitz-like structured matrices diﬀerent matrix decomposition techniques low-rank decomposition tt-decomposition also proposes type called urnn paper analyze aforementioned approaches. material organized follows. section give overview language modeling methods focus respective neural networks approaches. next describe diﬀerent types compression. section consider simplest methods neural networks compression like pruning quantization. section consider approaches compression neural networks based diﬀerent matrix factorization methods. section deals tt-decomposition. section describes results implementation details. finally section summarize results work. model directly would require calculation general diﬃcult computation steps. common approach features computations ﬁxed value approximate leads widely known -gram models popular approach middle milestone language modeling become recurrent neural networks work area done thomas mikolov -gram models even require space combinatorial explosion neural networks learn representations words sequences without memorizing directly options. approaches language modeling problem based neural networks eﬃcient widely adopted still require space. lstm layer size matrices size moreover usually ﬁrst layer network embedding layer maps word’s vocabulary number vector. need store embedding matrix too. size nvocab nvocab vocabulary size. also output softmax layer number parameters embedding i.e. ×nvocab. experiments reduce embedding size decompose softmax layer well hidden layers. subsection consider maybe eﬀective still useful techniques. described application audio processing image-processing language modeling ﬁeld well described. pruning method reducing number parameters usually majority weight values concentrated near zero. means weights provide valuable contribution ﬁnal output. threshold remove connections weights network. retrain network learn ﬁnal weights remaining sparse connections. quantization method reducing size compressed neural network memory. compressing ﬂoat value eight-bit integer representing closest real number equally-sized intervals within range. pruning quantization common disadvantages since training scratch impossible usage quite laborious. pruning reason mostly lies ineﬃciency sparse computing. quantization store model -bit representation still need -bits computations. means advantages using ram. least tensor processing unit adopted eﬀective -bits computations. lstm mostly complicated formulas. main advantage sizes matrices sizes respectively original matrices size small advantage size multiplication speed. discuss implementation details section tensor train decomposition originally proposed alternative eﬃcient form tensor’s representation tt-decomposition tensor rn×...×nd matrices rrk−×rk tensor elements represented paper author proposed consider input matrix multidimensional tensor apply decomposition matrix size following reshape matrix tensor dimensions size ndmd. finally perform tensor train decomposition tensor. approach successfully applied compress fully connected neural networks developing convolution layer matrix decomposition perform experiments medium large benchmarks. talk language modeling must embedding output layer occupy third total network size. follows necessity reducing sizes too. reduce output layer applying matrix decomposition. describe sizes lstm since useful model practical application. start basic sizes embedding. reduce reduce embedding value chosen suitable degree eﬃcient device implementation. performed several experiments conﬁguration near best. compressed model lstm even smaller lstm better perplexity. results experiments found table trained models using adam optimizer average training time hours geforce titan unfortunately none achieved acceptable quality. best obtained result even worse lstm-- terms size perplexity. article considered several methods neural networks compression language modeling problem. ﬁrst part pruning quantization. shown language modeling diﬀerence applying techniques. second part matrix decomposition methods. shown advantages implement models devices since usually tasks tight restrictions model size structure. point view model lstm nice characteristics. even smaller smallest benchmark demonstrates quality comparable medium-sized benchmarks ptb. acknowledgements. study supported russian federation president grant md-... a.v. savchenko supported laboratory algorithms technologies network analysis national research university higher school economics.", "year": 2017}