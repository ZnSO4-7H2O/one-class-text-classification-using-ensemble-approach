{"title": "A State Space Approach for Piecewise-Linear Recurrent Neural Networks  for Reconstructing Nonlinear Dynamics from Neural Measurements", "tag": ["q-bio.NC", "cs.NE", "q-bio.QM", "stat.ML"], "abstract": "The computational properties of neural systems are often thought to be implemented in terms of their network dynamics. Hence, recovering the system dynamics from experimentally observed neuronal time series, like multiple single-unit (MSU) recordings or neuroimaging data, is an important step toward understanding its computations. Ideally, one would not only seek a state space representation of the dynamics, but would wish to have access to its governing equations for in-depth analysis. Recurrent neural networks (RNNs) are a computationally powerful and dynamically universal formal framework which has been extensively studied from both the computational and the dynamical systems perspective. Here we develop a semi-analytical maximum-likelihood estimation scheme for piecewise-linear RNNs (PLRNNs) within the statistical framework of state space models, which accounts for noise in both the underlying latent dynamics and the observation process. The Expectation-Maximization algorithm is used to infer the latent state distribution, through a global Laplace approximation, and the PLRNN parameters iteratively. After validating the procedure on toy examples, the approach is applied to MSU recordings from the rodent anterior cingulate cortex obtained during performance of a classical working memory task, delayed alternation. A model with 5 states turned out to be sufficient to capture the essential computational dynamics underlying task performance, including stimulus-selective delay activity. The estimated models were rarely multi-stable, but rather were tuned to exhibit slow dynamics in the vicinity of a bifurcation point. In summary, the present work advances a semi-analytical (thus reasonably fast) maximum-likelihood estimation framework for PLRNNs that may enable to recover the relevant dynamics underlying observed neuronal time series, and directly link them to computational properties.", "text": "dept. theoretical neuroscience bernstein center computational neuroscience heidelberg‐ mannheim central institute mental health medical faculty mannheim/ heidelberg university computational cognitive properties neural systems often thought implemented terms network dynamics. hence recovering system dynamics experimentally observed neuronal time series like multiple single‐unit recordings neuroimaging data important step toward understanding computations. ideally would seek state space representation dynamics would wish access governing equations in‐depth analysis. recurrent neural networks computationally powerful dynamically universal formal framework extensively studied computational dynamical systems perspective. develop semi‐analytical maximum‐likelihood estimation scheme piecewise‐linear rnns within statistical framework state space models accounts noise underlying latent dynamics observation process. expectation‐maximization algorithm used infer latent state distribution global laplace approximation plrnn parameters iteratively. validating procedure examples using inference particle filters comparison approach applied multiple single‐unit recordings rodent anterior cingulate cortex obtained performance classical working memory task delayed alternation. model states turned sufficient capture essential computational dynamics underlying task performance including stimulus‐selective delay activity. estimated models rarely multi‐stable however rather tuned exhibit slow dynamics vicinity bifurcation point. summary present work advances semi‐analytical maximum‐likelihood estimation framework plrnns enable recover computationally relevant dynamics underlying observed neuronal time series directly link computational properties. neuronal dynamics mediate physiological anatomical properties neural system computations performs fact seen ‘computational language’ brain. therefore great interest recover experimentally recorded time series like multiple single‐unit neuroimaging data underlying network dynamics ideally even governing equations. trivial enterprise however since neural systems high‐ dimensional come considerable levels intrinsic noise usually partially observable observations corrupted noise measurement preprocessing steps. present article embeds piecewise‐linear recurrent neural networks within state space approach statistical estimation framework deals process observation noise. plrnns computationally dynamically powerful model systems. statistically principled estimation multivariate neuronal time series thus provide access essential features neuronal dynamics like attractor states governing equations computational implications. approach exemplified multiple single‐unit recordings prefrontal cortex working memory. neural dynamics mediate underlying biophysical physiological properties neural system computational cognitive properties hence computational perspective often interested recovering neural network dynamics given brain region neural system experimental measurements. experimentally commonly access noisy recordings relatively small proportion neurons lumped surface signals like local field potentials eeg. inferring computationally relevant underlying dynamics therefore trivial especially since neural system well recorded signals come good deal noise. reconstruction methods nonlinear dynamics nonlinear basis expansions kernel techniques approach problem techniques provide informative lower‐dimensional visualizations population trajectories approximations neural flow field highlight certain salient aspects dynamics return governing equations underlying computations. alternatively state space models statistical framework particularly popular engineering ecology adapted extract lower‐dimensional neural trajectory flows higher‐dimensional recordings state space models link process model unobserved underlying dynamics experimentally observed time series observation equations differentiate process noise observation noise exceptions models assumed linear latent dynamics however. although sufficient yield smoothed trajectories reduced state space representations implies recovered dynamical model powerful enough reproduce range important dynamical computational phenomena nervous system among multi‐stability proposed underlie neural activity working memory networks shown rnns nonlinear activation functions principle approximate dynamical system's trajectory fact dynamical system thus theory powerful enough recover whatever dynamical system underlying experimentally observed time series. piecewise linear activation functions particular popular choice deep learning algorithms considerably simplify derivations within state space framework also producing working memory‐type activity longer delays units transfer function happens coincide bisectrix ease analysis fixed points stability. apply newly derived algorithm multiple single‐ unit recordings prefrontal cortex obtained classical delayed alternation working memory task functions principle constructed properly connecting simple units combined appropriate choice activation thresholds second fixed points plrnn could obtained solving linear equations denote indices units assume respective connectivity matrix columns corresponding units obviously make networks defined moderate size thus computationally feasible explicitly check fixed points stability. γbσwaμξ state path experimentally observed time series {xt}. could instance properly transformed multivariate spike time series neuroimaging data. accomplished expectation‐maximization algorithm iterates state parameter estimation steps developed detail model methods. following first discuss state parameter estimation separately plrnn describing results full algorithm subsequent sections. done along problems higher‐order nonlinear oscillation simple 'working memory' paradigm discrete stimuli retained across temporal interval. finally application validated plrnn algorithm demonstrated multiple single‐unit recordings obtained rats standard working memory task kindly provided james hyman university nevada vegas). latent state distribution explained methods high‐dimensional gaussian mixture number components growing sequence length number latent states semi‐analytical approximate approach developed treats state estimation combinatorial problem first searching mode full distribution contrast e.g. recursive filtering‐smoothing scheme makes local approximations e.g. approach amounts solving high ‐dimensional piecewise linear problem accomplished alternating solving linear equations implied given linear constraints flipping ‐dimensional binary space linear constraints using newton‐type iterations methods). given mode state covariance matrix expectations needed algorithm derived analytically exception approximated problems introduced used assess quality approximations. first problem order‐ limit cycle produced plrnn consisting three recurrently coupled units inputs units parameter settings indicated fig. provided matlab file ‘plrnnoscparam.mat’. limit cycle repeated full cycles corrupted process noise noisy states matrix transformed output matrix observation noise added randomly drawn regression weight matrix state estimation started random initial condition. true estimated states particular problem illustrated fig. indicating tight ransformations samples simulated using bootstrap particle filtering although simulated samples based filtering steps sampling schemes issues own; e.g. analytical sampling estimates tight agreement correlating almost example shown fig. fig. illustrates setup ‘two‐cue working memory task’ chosen later comparability experimental setup. ‐unit plrnn first trained conventional gradient descent produce series unit unit five time steps input occurred unit reverse pattern five time steps input occurred unit stable plrnn reasonable solution problem chosen testing present algorithm instance choosing yields tri‐stable system.) like limit cycle problem before number observations taken equal number latent states process observation noise added system simulated repetitions trial type different noise realizations ‘trial’ started initial condition methods) resulting total series length t== before state estimation started random initial conditions provided correct parameters well observation matrix fig. illustrates correlation true estimated states across trials units fig. shows true estimated states representative cue‐ cue‐ trial respectively. again procedure obtaining maximum a‐posteriori estimate state distribution appears work quite well given true states well would algorithm retrieve parameters plrnn? assess this actual model states simulation runs oscillation working memory task described provided initialization step. based these algorithm first estimated state covariances parameters second step note parameters computed analytically given state distribution provided state covariance matrices required eqn. non‐singular unique solution. hence case misalignment true model parameters come sources estimation based finite‐length noisy realization plrnn process second order moments state distribution still estimated based true state vectors. however appreciated fig. fig. general case states parameters unknown observations given note model stated eqns. over‐specified instance level observations additional variance placed compensated adjusting accordingly following therefore always arbitrarily fixed models observations confirm algorithm finds satisfactory approximations underlying state path state covariances started right parameters vice versa identifies correct parameters provided true states. indeed m‐step since exact increase expected log‐likelihood present state expectancies fixed. however system's piecewise‐defined discrete nature modifying parameters lead constraint violations throw system completely different linear subspace imply decrease likelihood e‐step. thus guaranteed straightforward algorithm converges likelihood would even monotonically increase iteration. examine issue full estimation model times starting different random uniformly distributed initializations parameters. fig. gives maximum likelihood solution across runs correlations true estimated states five state variables model. note estimated true model states order permutation latent state indices together respective columns observation matrix equally consistent data model examined here however partial order information implicitly provided algorithm definition unit‐specific inputs present example true estimated states nicely linearly correlated latent variables regression slopes significantly differed indicating degree freedom scaling states. generally even clear linear relationship single latent state although estimation successful linear transformation usually estimated onto true states. because observation strictly linear linear transformation latent states matrix could essentially reversed level outputs back‐multiplying note piecewise linearity complicates matters however). implies model identifiable linear transformation might serious issue however interested primarily latent dynamics fig. illustrates distribution initial final parameter estimates around true values across runs fig. reveals algorithm clearly improve estimates final estimates seemed largely unbiased next interested kind structure present plrnn approach would retrieve experimental multiple single‐unit recordings obtained rats performing simple well‐examined working memory task namely spatial delayed alternation delay always initiated nose poke animal port located side opposite response levers minimum length spike trains first transformed kernel density estimates convolution gaussian kernel done previously binned resolution. also renders observed data suitable gaussian noise assumptions present observation model models latent states estimated results former reported periods presentation indicated model units three time bins setting external inputs its fig. gives model log‐likelihoods across iterations highest‐likelihood solutions. interestingly single neurons whose responses predicted well estimated model despite large trial‐to‐trial fluctuations others similar trial‐to‐trial fluctuations model captured general trend could potentially suggest trial‐to‐trial fluctuations single neurons could different reasons cases strongly varying single unit responses nevertheless highly predictable least considerable proportion trial‐to‐trial fluctuations must captured deterministic part model’s latent state dynamics hence different initializations states contrast average trend captured neuron’s trial‐to‐trial fluctuations likely represent true intrinsic noise sources model’s deterministic part cannot account for. observation highlights state space models could potentially also provide insights long‐standing questions neurophysiology. fig. shows five trial‐averaged latent states left‐ right‐lever trials highest likelihood solutions. surprisingly first state variables exhibit strong response left right lever respectively. third latent variable appears reflect motor response fourth fifth state variable clearly distinguish left right lever options throughout delay period task sense carrying memory within delay. hence particular data extracted latent states appear summarize quite well salient computational features simple working memory task. insight might gained examining system’s fixed points eigenvalue spectrum. purpose algorithm started different initial conditions maximum absolute eigenvalues drawn relatively uniform distribution within interval clear trend final maximum eigenvalues aggregate around produce models slow dynamics. indeed effectively slow dynamics needed bridge delays true multi‐stability perhaps even physiologically less likely scenario present work semi‐analytical maximum‐likelihood approach estimating piecewise‐ linear recurrent neural networks brain recordings developed. idea models would provide representation neural trajectories computationally relevant dynamical features underlying high‐dimensional experimental time series much lower‐ dimensional latent variable space direct access neural system’s computational properties. specifically estimated reproduce data models allow detailed analysis depth insight system’s computational dynamics e.g. analysis fixed points linear stability directly accessible experimental time series. extensions thereof also frequently applied gain insight neuronal dynamics essential features like attracting states associated different task phases in‐vivo multiple single‐unit recordings unstable periodic orbits extracted relatively low‐ noise slice recordings neuroscience however commonly deals high‐dimensional observations provided current multiple single‐unit neuroimaging techniques addition large variety process measurement noise sources. former include potential thermal noise sources probabilistic behavior single channel gating probabilistic synaptic release fluctuations neuromodulatory background hormone levels large variety uncontrollable external noise sources sensory surfaces including somatosensory visceral feedback within body. measurement noise come direct physical sources like instance instabilities movement tissue surrounding recording electrodes noise properties recording devices themselves mere fact fraction system variables experimentally accessed result preprocessing steps like spike sorting therefore quite different scenario comparatively low‐ dimensional low‐noise situations e.g. laser physics delay‐embedding‐based approaches reconstruction neural dynamics augmented machine learning techniques retrieve least salient features dimensionality high noise levels inherent neural data perhaps lesser extent approaches like delay embeddings directly construct state space observations models pursued statistical state space ramework explicitly incorporate process measurement noise system’s description. also long latent variable space relatively small related observations simple linear equations here high dimensionality observations constitute serious issue estimation. importantly however clear advantage access governing equations themselves allows depth analysis system’s dynamics relation neural computation instance recurrent network models trained past perform behavioral tasks reproduce behavioral data infer dynamical mechanisms potentially underlying working memory context‐dependent decision making although commonly cited cases within statistical framework also approaches somewhat between attempting account observations directly without reference underlying latent variable model differential equations expressed terms nonlinear basis expansions observations estimated strongly regularized regression methods also remains investigated well methods without noise model face high data dimensionality directly transfer neuroscience problems. state space models popular statistical tool many fields science although applications neuroscience recent origin dynamic causal modeling framework advanced human fmri literature infer functional connectivity brain networks dependence task conditions seen state space approach although models usually contain process noise commonly estimated bayesian inference imposes constraints complexity models reasonably dealt framework. neurophysiology smith brown among first suggest state space model multivariate spike count data coupling linear‐ gaussian transition model poisson observations state estimation achieved making locally gaussian approximations similar models variously used subsequently infer local circuit coding properties e.g. biophysical parameters neurons synaptic inputs postsynaptic voltage recordings proposed gaussian process factor analysis retrieving lower‐dimensional smooth latent neural trajectories multiple spike train recordings. gpfa correlation structure among latent variables specified explicitly rather given transition model. buesing discuss regularized forms neural state space models enforce stability macke review different estimation methods models like laplace approximation variational inference methods. models discussed linear latent dynamics however although often sufficient uncover important properties underlying latent processes structures like connectivity synaptic/neuronal parameters obtain lower‐dimensional representations observed process suitable retrieving system dynamics computations linear systems strongly limited repertoire dynamics produce exceptions however current work builds suggested sigmoid‐type activation function coupled poisson spike count outputs used reconstruct latent neural dynamics underlying motor preparation planning non‐human primates. work combined gaussian approximation suggested smith brown extended kalman filter estimation within framework. various approximations conjunction iterative estimation scheme quite prone numerical instabilities accumulating errors however earlier work roweis ghahramani used radial basis function networks partly analytically tracktable approach. nonlinear extensions incorporating quadratic terms proposed well recently state parameter estimation also attempted nonlinear biophysical models approaches usually computationally expensive especially based numerical sampling time pursuing objectives somewhat different targeted theoretical neuroscience. present work plrnns therefore chosen mathematically comparatively tracktable computationally powerful nonlinear recurrent network approach reproduce wide range nonlinear dynamical phenomena given semi‐analytical nature present algorithm runs reasonably fast however mathematical properties certainly require illumination lead algorithmic improvements. although primary focus work develop evaluate state space framework plrnns discussion applicational example chosen here working memory order. working memory generally defined ability actively hold item memory absence guiding external input short‐term reference subsequent choice situations various neural mechanisms proposed underlie cognitive capacity prominently multi‐stable neural networks retain short‐term memory items switching several stimulus‐selective attractor states attractors usually represent fixed points firing rates assemblies recurrently coupled stimulus‐selective cells exhibiting high rates cells coding present stimulus short‐term memory remaining spontaneous low‐rate base level. models inspired physiological observation ‘delay‐active’ cells cells switch high‐rate state delay periods working memory tasks back low‐rate state completion trial similar ‘delay‐active’ latent states observed fig. nakahara doya among first point however that working memory completely sufficient tune system close bifurcation point dynamics becomes slow true multi‐stability required. supported present observation estimated plrnn models fixed points eigenvalues close truly multi‐stable sufficient account maintenance stimulus‐selectivity throughout delay present task experimental observations recently number mechanisms supporting working memory however including sequential activation cell populations synaptic mechanisms discussed. thus neural mechanisms working memory remain active research area previous work estimation state space models expectation‐maximization framework obtaining estimates model parameters underlying latent state path. piecewise‐linear nature model however conditional latent state path density high‐dimensional ‘mixture’ partial gaussians number integrations perform obtain moments scaling tm. although analytically accessible computationally prohibitive almost cases interest. approach therefore focuses computationally reasonably efficient searching mode found good agreement cases. covariances approximated locally around estimate. state estimation linear function obtaining maximizing argument expectancy w.r.t. i.e. piecewise gaussian still take approach maximizing approximation concatenate state variables long column vector unwrap sums across time large block‐banded mtmt matrices indicator vector everywhere except entries indices ...{ first entries rewrite sides important point different quadratic equations depending bits binary vector consequently obtain estimator theory consider different settings solve linear equations implied consistent considered produces largest value equations suggested mathematical programming literature past instance piecewise linear problems recast linear complementarity problem pivoting methods often used solve work well smaller scale settings therefore settled similar simple newton‐type iteration scheme proposed brugnano casulli specifically denote constraints active present scheme initializes random drawing components solution always exists algorithm always terminate finite number steps given certain assumptions provided matrix multiplies states fulfills certain conditions details). usually case present system; although hessian symmetric positive‐definite off‐diagonal elements either larger smaller moreover problem considered here elements hessian depend case on‐diagonal elements newton‐type algorithm outlined always converge exact solution eventually cycle among non‐solution configurations even always increase following three conditions solution encountered; previously probed revisited; constraint violation error defined tolerance level. modifications found algorithm would usually terminate iterations yield approximate solutions constraints still violated elements constraints still violated case full iterations appeared flipping violated constraints often improve overall performance sense yielding higher‐likelihood solutions less numerical problems hence scheme adopted full single corresponding maximum element vector inverted iteration general however resultant slow‐down algorithm always worth performance gains; mixture methods standard results derive reverse chain rule integration elements inverse bivariate ‐covariance matrix. note variable removed first integrand i.e. second term terms would come uni‐ bivariate gaussians univariate gaussian expectancy value respectively. noting this obtains second term starting number different random parameter initializations m‐steps alternated log‐likelihood ratio falls predefined tolerance level preset maximum number allowed iterations exceeded. reasons mentioned results sometimes actually happen log‐likelihood ratio temporarily decreases factor analysis used case iterations continued. derive initial estimates latent states observation parameters although attempted here. implementational details matlab code provided www.zi‐ mannheim.de/en/research/departments‐research‐groups‐institutes/theor‐neuroscience‐e.html validate approximations semi‐analytical procedure developed above bootstrap particle filter given durbin koopman implemented. bootstrap particle filtering state posterior distribution time details experimental task electrophysiological data sets used could found hyman briefly rats alternate left right lever presses skinner obtain food reward dispensed correct choices delay enforced consecutive lever presses. levers located side skinner animals perform nosepoke opposite side lever presses initiating delay period discourage developing external coding strategy animals performing task multiple single units recorded tetrodes implanted bilaterally anterior cingulate cortex present analyses data four rats recorded task selected present exemplary purposes namely clearest single unit traces delay activity observed first place. data consisted simultaneously recorded units units spiking rates retained correct trials trials variable length length including pre‐nosepoke extending delay nosepoke preceding next lever press post‐response phase spike trains convolved gaussian kernels kernel standard deviation individually unit half mean interspike‐interval. note also brings observed series tighter agreement gaussian assumptions observation model finally spike time series binned bins average neural firing rates resulted trials time bins submitted estimation process. indicated section ‘state space model’ trial‐unique initial state mean kkμ durstewitz self‐organizing neural integrator predicts interval times climbing activity. neurosci. wang probabilistic decision making slow reverberation cortical circuits. neuron. izhikevich dynamical systems neuroscience.; press. rabinovich rabinovich huerta varona afraimovich transient cognitive dynamics metastability decision making. plos comput biol. stevens neurotransmitter release central synapses. neuron. pillow shlens chichilnisky simoncelli model‐based spike sorting algorithm removing correlation artifacts multi‐neuron recordings. plos one. balaguer‐ballester lapish seamans durstewitz attractor dynamics cortical populations memory‐guided decision‐making. plos comput biol. lapish balaguer‐ballester seamans phillips durstewitz amphetamine exerts dose‐ dependent changes prefrontal cortex attractor dynamics working memory. neurosci. brunton proctor kutz discovering governing equations data sparse identification nonlinear dynamical systems. proc natl acad wood statistical inference noisy nonlinear ecological dynamic systems. nature. smith brown estimating state‐space model point process observations. neural comput. paninski ahmadian ferreira koyama rahnama vidne look state‐ space models neural data. comput neurosci. paninski vidne depasquale ferreira inferring synaptic inputs given noisy voltage trace sequential monte carlo methods. comput neurosci. pillow shlens paninski sher litke chichilnisky spatio‐temporal correlations visual signalling complete neuronal population. nature. pillow ahmadian paninski model‐based decoding information estimation change‐ point detection techniques multineuron spike trains. neural comput. buesing macke sahani learning stable regularised latent models neural population dynamics. network. latimer yates meister pillow neuronal modeling. single‐trial spike trains parietal cortex reveal discrete steps decision‐making. science. macke buesing sahani estimating state parameters state space models spike trains. chen editor. advanced state space methods neural clinical data. cambridge university press; press. afshar santhanam shenoy extracting dynamical structure embedded neural activity. neural process syst. kemere santhanam afshar meng mixture trajectory models neural decoding goal‐directed movements. neurophysiol. cunningham santhanam shenoy sahani gaussian‐process factor analysis low‐dimensional single‐trial analysis neural population activity. neurophysiol. durbin koopman time series analysis state space methods. oxford statistical science; roweis ghahramani algorithm identification nonlinear dynamical systems. haykin editor. kalman filtering neural networks; amit brunel model global spontaneous activity local structured activity delay periods cerebral cortex. cereb cortex. durstewitz seamans sejnowski neurocomputational models working memory. neurosci. suppl durstewitz implications synaptic biophysics recurrent network dynamics active memory. neural netw. brunel wang effects neuromodulation cortical network model object working memory dominated recurrent inhibition. comput neurosci. working memory. neurosci. funahashi nakamura approximation dynamical systems continuous time recurrent neural networks. neural netw. kimura nakano learning dynamical systems recurrent neural networks orbits. neural netw. chow modeling continuous time dynamical systems input recurrent neural networks. trans circuits syst fundam theory theory appl. lecun bengio hinton deep learning. nature. mnih kavukcuoglu silver rusu veness bellemare human‐level control deep reinforcement learning. nature. hochreiter schmidhuber long short‐term memory. neural comput. hyman whitman emberly woodward seamans action outcome activity state patterns anterior cingulate cortex. cereb cortex. paninski ahmadian ferreira koyama rahnama vidne look state‐ space models neural data. comput neurosci. koyama paninski efficient computation maximum posteriori path parameter estimation integrate‐and‐fire general state‐space models. comput neurosci. brugnano casulli iterative solution piecewise linear systems. siam comput. williams zipser learning algorithm continually running fully recurrent neural networks. neural computat. hertz krogh palmer introduction theory neural computation. addison‐ wesley zhang hyvärinen general linear non‐gaussian state‐space model identifiability identification applications. jmlr workshop conference proceedings models’ dirty little secrets even simple linear gaussian models estimation problems. rep. park bohner macke unlocking neural population non‐stationarity using hierarchical dynamics model advances neural information processing systems twenty‐ninth annual conference neural information processing systems pp.‐. cfj. convergence properties algorithm. statist. boutayeb rafaralahy darouach convergence analysis extended kalman filter used observer nonlinear deterministic discrete‐time systems. ieee trans autom control. durstewitz balaguer‐ballester statistical approaches reconstructing neuro‐cognitive dynamics high‐dimensional neural recordings. neuroforum. shimazaki shinomoto kernel bandwidth optimization spike rate estimation. comp neurosci. latham nirenberg computing stability cortical networks. neural comput. durstewitz seamans beyond bistability biophysics temporal dynamics working memory. neuroscience. strogatz nonlinear dynamics chaos. addison‐wesley publ; durstewitz kelc güntürkün neurocomputational theory dopaminergic modulation working memory functions. neurosci. brunel dynamics sparsely connected networks excitatory inhibitory spiking neurons. comput neurosci. beer parameter space structure continuous‐time recurrent neural networks. neural computation mante sussillo shenoy newsome context‐dependent computation recurrent dynamics prefrontal cortex. nature. hertäg durstewitz brunel analytical approximations firing rate adaptive exponential integrate‐and‐fire neuron presence synaptic noise. front comput neurosci. sussillo barak opening black low‐dimensional dynamics high‐dimensional recurrent neural networks. neural comput.; takens detecting strange attractors turbulence. lecture notes mathematics springer berlin; sauer sauer davies embedology. stat phys. sauer reconstruction dynamical systems interspike intervals. phys lett. francis netoff gluckman schiff periodic orbits language neuronal dynamics. biophys hilleb. channels excitable membranes. sinauer assoc inc; takahashi anzai sakurai approach spike sorting multi‐neuronal activities recorded tetrode‐‐how practical. neurosci res. takahashi anzai sakurai automatic sorting multi‐neuronal activity recorded tetrodes presence overlapping spikes. neurophysiol. kantz schreiber nonlinear time series analysis. cambridge university press; schreiber kantz observing predicting chaotic signals noise much? kravtsov kadtke editors. predictability complex dynamical systems springer ork;. durstewitz gabriel dynamical basis irregular spiking nmda‐driven prefrontal cortex neurons. cereb cortex. zipser kehoe littlewort fuster spiking network model short‐term active memory. neurosci. friston harrison penny dynamic causal modelling. neuroimage. daunizeau stephan friston stochastic dynamic causal modelling fmri data care neural noise? neuroimage. huys paninski smoothing parameter estimation from noisy biophysical recordings. plos comput biol. durstewitz advanced statistical models neuroscience. heidelberg springer; press. stephan kasper harrison daunizeau ouden nonlinear dynamic causal models fmri. neuroimage. toth kostuk meliza margoliash abarbanel dynamical estimation neuron network properties variational methods. biol cybern. network properties path integral monte carlo methods. biol cybern. multistability analysis recurrent neural networks unsaturating piecewise linear transfer functions. neural comput. tang zhang analysis cyclic dynamics networks linear threshold neurons. neural comput. zhang representations continuous attractors recurrent neural networks. ieee trans neural netw. zhang multiperiodicity attractivity delayed recurrent neural networks unsaturating piecewise linear transfer functions. ieee trans neural netw. fuster prefrontal cortex. academic press; fuster unit activity prefrontal cortex delayed‐response performance neuronal correlates transient memory. neurophysiol. funahashi bruce goldman‐rakic mnemonic coding visual space monkey's dorsolateral prefrontal cortex. neurophysiol. miller erickson desimone neural mechanisms visual working memory prefrontal cortex macaque. neurosci. nakahara doya near‐saddle‐node bifurcation behavior dynamics working memory goal‐directed behavior. neural comput. baeg mook‐jung jung dynamics population code working memory prefrontal cortex. neuron. mongillo barak tsodyks synaptic theory working memory. science. fahrmeir tutz multivariate statistical modelling based generalized linear models. springer; eaves \"solving piecewise linear convex equations\" mathematical programming study november; eaves scarf solution systems piecewise linear equations. math oper res. cottle dantzig complementary pivot theory mathemacal programming. linear algebra appl. crisan doucet survey convergence results particle filtering methods practitioners. ieee trans signal process. whitley variance estimation particle filter. arxiv.v hyman balaguer‐ballester durstewitz seamans contextual encoding ensembles medial prefrontal cortex neurons. proc natl acad usa. fig. state parameter estimates nonlinear cycle example. true estimated states periods simulated limit cycle generated state plrnn true parameters provided ‘true states’ refers actual states observations generated. inputs units time steps cycle respectively. true estimated model parameters true states provided. bisectrix lines indicate identity. fig. agreement simulated semi‐analytical solutions state expectancies model fig. across three state variables time steps. here generated using bootstrap particle filter particles. bisectrix lines gray indicate identity. fig. state estimation ‘working memory’ example true parameters provided. setup simulated working memory task stimulus inputs requested outputs across time points working memory trial plrnn units. correlation estimated true states across five state variables time steps. bisectrix black. true estimated states output units principle explicitly designed ‐state plrnn first trained conventional gradient descent perform task yield ‘natural’ less uniform ground truth states parameters. here stable fixed point). matlab file ‘plrnnwmparam.mat’ fig. details parameters. fig. true estimated parameters working memory plrnn true states note provided. top‐left bottom‐right estimates parameter estimates highly accurate although state covariance matrices still estimated well bisectrix lines black indicate identity. fig. full algorithm working memory model state estimates solution. log‐ likelihood function iteration highest‐likelihood initializations. example log‐likelihood although generally increasing always monotonic example true estimated states nicely linearly related although regression slope state estimation case performed inverting single constraint corresponding largest deviation iteration bisectrix lines black indicate identity. fig. full algorithm working memory model. parameter estimates solution fig. true parameters initial final parameter estimates bisectrix lines blue. distributions initial final final reordering states deviations estimated true parameters across runs different initial conditions. final distributions centered around indicating final parameter estimates largely unbiased. note partial information state assignments implicitly provided network unit‐specific inputs hence state reordering produced slight improvements parameter estimates. fig. prediction single unit responses. examples log‐likelihood curves across iterations highest‐likelihood runs ‐state plrnn estimated simultaneously recorded prefrontal neurons working memory task. example unit predicted extremely well estimated plrnn despite considerable trial trial fluctuations example another unit three trials average trend captured plrnn. gray vertical bars indicate times cue/ response. state estimation case performed inverting single constraint corresponding largest deviation iteration fig. example latent states plrnn estimated multiple single‐unit recordings working memory shown trial averages left‐lever right‐lever trials sems computed across trials. dashed vertical lines flank period delay phase used model estimation. note latent variables particular differentiate left right lever responses throughout delay period. fig. initial final distributions maximum eigenvalues associated fixed points plrnns estimated experimental data different initializations parameters including threshold parameters configurations deliberately chosen yield rather uniform distribution absolute eigenvalues", "year": 2016}