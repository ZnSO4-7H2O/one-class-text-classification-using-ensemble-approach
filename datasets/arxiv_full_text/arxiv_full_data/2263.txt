{"title": "Learning K-way D-dimensional Discrete Code For Compact Embedding  Representations", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Embedding methods such as word embedding have become pillars for many applications containing discrete structures. Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying linear transformation based on \"one-hot\" encoding of the discrete symbols. Despite its simplicity, such approach yields number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", each symbol is represented by a $D$-dimensional code, and each of its dimension has a cardinality of $K$. The final symbol embedding vector can be generated by composing the code embedding vectors. To learn the semantically meaningful code, we derive a relaxed discrete optimization technique based on stochastic gradient descent. By adopting the new coding system, the efficiency of parameterization can be significantly improved (from linear to logarithmic), and this can also mitigate the over-fitting problem. In our experiments with language modeling, the number of embedding parameters can be reduced by 97\\% while achieving similar or better performance.", "text": "embedding methods word embedding become pillars many applications containing discrete structures. conventional embedding methods directly associate symbol continuous embedding vector equivalent applying linear transformation based one-hot encoding discrete symbols. despite simplicity approach yields number parameters grows linearly vocabulary size lead overﬁtting. work propose much compact k-way d-dimensional discrete encoding scheme replace one-hot\" encoding. encoding symbol represented d-dimensional code dimension cardinality ﬁnal symbol embedding vector generated composing code embedding vectors. learn semantically meaningful code derive relaxed discrete optimization technique based stochastic gradient descent. adopting coding system efﬁciency parameterization signiﬁcantly improved also mitigate over-ﬁtting problem. experiments language modeling number embedding parameters reduced achieving similar better performance. embedding methods word embedding become pillars many applications learning discrete structures. examples include language modeling machine translation text classiﬁcation knowledge graph social network modeling many others objective embedding module neural networks represent discrete symbol word entity continuous embedding vector seems trivial problem ﬁrst glance directly associate symbol learnable embedding vector done existing work. retrieve embedding vector speciﬁc symbol embedding table lookup operation performed. equivalent following ﬁrst encode symbol one-hot encoding vector generate embedding vector simply multiply one-hot vector embedding matrix rn×d i.e. despite simplicity one-hot encoding based embedding approach several issues. major issue number parameters grows linearly number symbols. becomes challenging millions billions entities database lots symbols observations also exists redundancy parameterization assuming many symbols actually similar other. over-parameterization lead overﬁtting; also requires memory prevents model deployed mobile devices. another issue purely code space utilization perspective one-hot encoding extremely inefﬁcient. code space utilization rate almost zero bits/dimensions code effectively represent symbols. address issues propose novel much compact coding scheme replaces one-hot encoding. proposed approach k-way d-dimensional code represent symbol code dimensions dimension cardinality example concept encoded concept encoded code allocation symbol based data able capture semantics symbols similar codes reﬂect similar meanings. proposed encoding scheme encoding. code system much compact one-hot counterpart. represent symbols size encoding requires increasing small amount easily achieve case still much compact. consider utilization rate encoding times compact one-hot counterpart compactness code translated compactness parametrization. dropping giant embedding matrix rn×d stores symbol embeddings symbol embedding vector generated composing much fewer code embedding vectors. achieved follows ﬁrst embed code sequence vector rd×d apply transformation based neural networks generate ﬁnal symbol embedding. order learn meaningful discrete codes exploit similarities among symbols derive relaxed discrete optimization algorithm based stochastic gradient descent adopting approach reduce number parameters form code embedding size number neural network parameters. validate idea conduct experiments synthetic data well real language modeling task. achieve embedding parameter reduction language modeling task obtain similar better performance. proposed framework symbol associated k-way d-dimensional discrete code. denote symbol symbols cardinality code bits discrete code denoted used. learning mapping function introduced later ﬁxed stored hash table fast lookup. given i-th symbol retrieve code code lookup ﬁnal embedding generated ﬁrst embedding code sequence code embed··· ding vectors apply differentiable transformation function learned well. introduce transformation function next sub-section. rk×d embedding matrix j-th code bit. overall framework illustrated figure order uniquely identify symbol need assign unique code symbol. holds code space fully utilized none symbol change code without affecting symbols. call type code system compact code. optimization problem compact code difﬁcult usually requires approximated combinatorial algorithms graph matching opposite compact code figure conventional symbol embedding based one-hot encoding. proposed encoding scheme. example embedding transformation function used encoding generating symbol embedding code. redundant code system empty code space symbol correspondence changing code symbol affect symbols since random collision probability small makes easier optimize. redundant code achieved slightly increasing size thanks exponential nature relations. hence compact code redundant code function transforms code embedding vectors ﬁnal symbol embedding vector. mentioned above associate embedding matrix rk×d j-th dimension discrete code. enables turn discrete code sequence code embedding vectors applied. work consider types embedding transformation functions. ﬁrst based linear transformation rd×d linear matrix. simple linear nature capacity generated symbol embedding limited. motivates adopt non-linear transformation function based recurrent neural network lstm particular. assuming code embedding dimension lstm hidden dimension formulation given follows. tanh respectively standard sigmoid tanh activation functions. please also noted symbol index ignored simplicity. ﬁnal symbol embedding computed summing lstm outputs code bits code code embedding vectors neural network parameters. dimensions. number parameters neural networks treated constant number symbols since independent provided certain structures presented symbol embeddings. example assume symbol embeddings within \u0001-ball ﬁnite number centroids d-dimensional space require constant achieve \u0001-distance error bound regardless vocabulary size since neural networks memorize ﬁnite centroids. code assignment important parameterization efﬁciency generalization. want learn code allocation function end-to-end data contrast hand-coded one-hot encoding. work assume already given pre-trained embedding vectors thus learn discrete codes based given codes learned re-learn code embedding parameters including transformation function according speciﬁc task. future extend case embeddings available. optimal codes minimize squared loss real embedding vector embedding vector generated code. yields following. differentiable transformation function introduced above. since discrete code cannot directly optimized stochastic gradient descent parameters thus need relaxation order learn effectively sgd. observe code seen concatenation one-hot vector i.e. adjust order update code still non-differentiable. address issue relax one-hot vector continuous vector applying tempering softmax temperature term approximation becomes exact similar techniques applied gumbel-softmax show effects temperature figure learn relaxed code logits perature training. small enough smooth vector linear combination i.e. forward pass instead using tempering softmax output likely smooth straight-through estimator equivalent different temperatures forward backward pass. forward pass used simply take argmax. backward pass pretend larger used. although biased gradient estimator sign gradient still correct. compared using temperatures passes always output one-hot discrete code vanishing gradient problem long backward temperature approaching zero. training procedure summarized algorithm stop_gradient operator prevent gradient back-propagating algorithm epoch code learning straight-through estimator tempering softmax. input symbol embedding {vi} code logits {ˆoi} code embedding matrices transformation section present real synthetic experiments validate proposed approach. ﬁrst experiments based language modeling task. language modeling fundamental task formulated predicting probability sequence words. models based recurrent neural networks word embedding achieve state-ofthe-art results base experiments. widely used english penn treebank dataset used experiments contains words vocabulary size training/validation/test split convention according utilize standard lstm different model sizes trade-off model size accuracy. larger model word embedding size lstm hidden size number smaller model. default used proposed approach. temperature schedule i.e. used train code decay_rate iteration number. ﬁrst train model regularly using conventional embedding approach obtain embedding vectors used learn discrete codes. discrete codes obtained ﬁxed re-train model architecture hyper-parameters code embedding scratch. table shows performance comparisons conventional one-hot word embeddings proposed encoding. presents several variants encoding schemes distinguished combinations discrete code learning model symbol embedding relearning/re-training model. discrete code learning three cases random assignment code learned linear transformation code learned lstm transformation function; latter also utilized symbol embedding re-learning model. firstly observe discrete code learning critical encoding random discrete codes produce much worse performance. secondly observe appropriate code learning test perplexity similar better compared one-hot encoding case saving embedding parameters. table comparisons language modeling ptb. test perplexity embedding size compression rate shown small large model settings. text variants encoding. also vary size affect performance. shown figure small harm performance satisﬁed) suggests redundant code easier learn. order understand effects temperature importance using discrete code output create another experiments based synthetic embedding clusters. generate nodes belong well separated clusters -dimensional space. used mimics k-means clustering problem code represents cluster assignment. squared loss clustering shown figure observed temperature scheduling much effective comparing counterparts. temperature kept constant always percent codes changing loss well converge worse local optimal. smooth continuous code instead discrete code used observe loss ﬁrst decreases increases. temperature small enough behavior mimics discrete code output. inspect learned code pre-trained embedding glove better coverage quality pre-trained language modeling. intentionally vocabulary size model forced collide words. table show learned code based glove vectors demonstrates similar discrete codes learned semantically similar words. idea using efﬁcient coding system dates back information theory error correction code hoffman code however embedding techniques word embedding entity embedding one-hot encoding used along usually large embedding matrix. recent work explores character sub-word based embedding model instead word embedding model yields good results. however cases chars sub-words ﬁxed given priori according language thus semantic meanings attached available data. contrast learn code assignment function data well using ﬁxed length code. compression neural networks risen important topic size parameters large becomes bottleneck deploying model mobile devices. work also seen compress embedding layer neural networks. existing network compression techniques focus layers shared data examples symbols utilized embedding layer time work. lightrnn seen special case proposed code compact code code learning harder expensive. also note similar work encoding embeddings discrete codes conducted parallel ours. paper propose novel k-way d-dimensional discrete encoding scheme replace one-hot\" encoding. adopting coding system efﬁciency parameterization signiﬁcantly improved. furthermore reduction parameters also mitigate over-ﬁtting problem. learn semantically meaningful code derive relaxed discrete optimization technique based sgd. experiments language modeling number free parameters reduced achieving similar better performance. currently working improving on-the-ﬂy code learning along given tasks symbol embeddings given beforehand. ting chen lu-an tang yizhou zhengzhang chen zhang. entity embedding-based anomaly detection heterogeneous categorical events. proceedings twenty-fifth international joint conference artiﬁcial intelligence pages aaai press wenlin chen james wilson stephen tyree kilian weinberger yixin chen. compressing neural networks hashing trick. international conference machine learning pages yoon yacine jernite david sontag alexander rush. character-aware neural language models. proceedings thirtieth aaai conference artiﬁcial intelligence pages aaai press xiang jian yang xiaolin tieyan liu. lightrnn memory computation-efﬁcient recurrent neural networks. advances neural information processing systems pages tomáš mikolov martin karaﬁát lukáš burget ˇcernock`y sanjeev khudanpur. recurrent neural network based language model. eleventh annual conference international speech communication association tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pages jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings conference empirical methods natural language processing pages", "year": 2017}