{"title": "Estimating Continuous Distributions in Bayesian Classifiers", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.", "text": "modeling bayesian problem ables. problem data generated paper abandon normality sumption nonparametric naive bayesian mental results tificial domains comparing density modeling conditional single gaussian; kernel density large reductions artificial kernel bayesian recent years scriptions training alternative learning networks. describe ture bayesian heckerman singh approach. bayesian networks representation sons agnosis noise task. simple compute test cases esti­ mate training rectly factor; equation ignores classes naive bayes treats somewhat differently. modeled single represents attribute class contrast modeled range attribute's view classifier bayesian important assumes ally independent hidden latent attributes tion process. thus depicted bayesian classifier arcs directed observable however convincing kernel estimation finite-sample empirical dard experimental involves data using induced separate dictions accuracy. evaluate classifier bayesian experimental table shows results mains including viation better nificance flexible naive bayes five domains domains cases. domains algorithm erence point comparison. test hypotheses probability pled varying-sized tions order plot learning gorithms. constructed thousand tion used evaluate mercury-barometer/stethoscope training samples independent average standard also quite surprised measuremen nomenon working rithm blood pressure rounded nearest sometimes ical reason suspect sure likely standard paratus significant common domains width would flexible better version assumes single gaussian. experi­ mental studies artificial data suggest approach behaves expected important work remains done scenarios. although results flexible bayesian classifier constitutes promising induction toire probabilistic authors would like thank nils nilsson wray buntine helpful comments pointers relevant literature. graduate research supported part grant n--l- office naval research.", "year": 2013}