{"title": "Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep  Object Recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We propose a general multi-class visual recognition model, termed the Classifier Graph, which aims to generalize and integrate ideas from many of today's successful hierarchical recognition approaches. Our graph-based model has the advantage of enabling rich interactions between classes from different levels of interpretation and abstraction. The proposed multi-class system is efficiently learned using step by step updates. The structure consists of simple logistic linear layers with inputs from features that are automatically selected from a large pool. Each newly learned classifier becomes a potential new feature. Thus, our feature pool can consist both of initial manually designed features as well as learned classifiers from previous steps (graph nodes), each copied many times at different scales and locations. In this manner we can learn and grow both a deep, complex graph of classifiers and a rich pool of features at different levels of abstraction and interpretation. Our proposed graph of classifiers becomes a multi-class system with a recursive structure, suitable for deep detection and recognition of several classes simultaneously.", "text": "abstract propose general multi-class visual recognition model termed classiﬁer graph aims generalize integrate ideas many today’s successful hierarchical recognition approaches. graph-based model advantage enabling rich interactions classes different levels interpretation abstraction. proposed multi-class system efﬁciently learned using step step updates. structure consists simple logistic linear layers inputs features automatically selected large pool. newly learned classiﬁer becomes potential feature. thus feature pool consist initial manually designed features well learned classiﬁers previous steps copied many times different scales locations. manner learn grow deep complex graph classiﬁers rich pool features different levels abstraction interpretation. proposed graph classiﬁers becomes multi-class system recursive structure suitable deep detection recognition several classes simultaneously. overview approach class multiple classiﬁers graph multiple classes learning classiﬁer graph initial feature types classiﬁcation deep detection human loop visual object recognition based relative hierarchical recursive cognitive processes recognition object parts attributes whole objects interactions contextual relationship objects scene. surprise competitive architectures object category recognition today deep hierarchical structure many successful hierarchical approaches including face detector viola jones based classiﬁer cascades object detector felzenszwalb part-based model latent svm’s conditional random fields classiﬁcation trees random forests probabilistic bayesian networks directed acyclic graphs hierarchical hidden markov models methods based feature matching second-order hierarchical spatial constraints even popular nearest neighbor approach matching sift features using ransac geometric veriﬁcation could modeled hierarchical structure ﬁnding correspondences individual features would take place ﬁrst stage processing rigid transformation computation veriﬁcation could implemented linear transformations higher levels processing. proposal performing hierarchical geometric reasoning neural architecture work hinton capsules. hierarchical classiﬁers currently enjoying great practical success development efﬁcient methods deep learning neural networks. considered many real scientiﬁc breakthrough artiﬁcial intelligence deep learning already broadly adopted industry variety applications including object recognition images speech recognition systems based deep learning major machine learning computer vision competitions netﬂix kaggle imagenet challenges review). recent success deep classiﬁcation systems long-term scientiﬁc interest research development strongly motivate work formulating general deep detection recognition network potential overcome many limitations current hierarchical models. overview approach propose general recognition learning strategy based graph structure classiﬁers termed classiﬁer graph aims generalize ideas many previous models. nodes graph individual classiﬁers could type classiﬁer operates dedicated region given location scale relative image bounding box. functions detector certain search area computes returns maximum response area could relatively small local large classiﬁers nodes graph inﬂuence directed edges output could input node classiﬁers viewed role excitatory inhibitory input features providing favorable non-favorable context children. different approaches make conceptual distinction lowlevel features intermediate anonymous classes parts objects properties context simply classiﬁers freely inﬂuence directed links between levels abstraction. free form collectively contextual environment other. ﬂexible graph structure classiﬁers nodes learned scratch several training epochs efﬁcient supervised learning scenario combined natural unsupervised clustering organization training data manner loosely reminiscent cascade correlation node adds single layer graph using logistic linear classiﬁer inputs automatically selected existing pool features pool features ﬁrst initialized manually designed descriptors operate mid-level input randomly sampled many scales locations different instantiating parameters. classiﬁer learned deﬁned speciﬁc location scale certain search area learned becomes potential feature copies many different scales locations different search areas added pool potential features. manner simultaneously grow arbitrarily complex recursive directed graph classiﬁers pool features represent classiﬁers different geometric scales levels abstraction localization uncertainty. graph nodes classiﬁer graph similar ones neural represents logistic linear unit. important difference neural networks ability connect classes levels abstraction using top-down well bottom-up links; note meaning bottom hierarchy conceptual rather physical. explain detail. edges directed input node classiﬁer node motivation parts objects scenes inﬂuence others’ conﬁdence recognition particular class detector could take input outputs classes’ detectors. principle recognizer level could function context input feature recognition class level. predictions higher levels abstraction could function context prediction lower levels probabilistic inﬂuence could also around since chair could expect room. latter example chair becomes context room. two-way relationships happen lower levels part whole classiﬁer graph directed edges formed abstraction levels either direction service shop means expect wheels around weaker still positive inﬂuence wheel might service shop. framework parts objects equal citizens distinguishes part another distinct object scene dedicated region whereas mechanic example different object share pixels car. also parts play objects role similar played objects scene. part nevertheless conceptually four objects scene different classes recognized dedicated classiﬁers composed logistic linear unit relative scale location respect coordinate system image bounding different search area thus proposed system treats parts objects anonymous intermediate classes materials scenes universal classiﬁer nodes free graph-like structure. class multiple classiﬁers proposal somewhat unusual single concept represented using multiple classiﬁers learned early typically focusing low-level pixel/feature inputs others learned late; latter many parents concept classiﬁers clear directed edge part whole different whole part parts inﬂuence existence probability whole presence whole indicates likely presence part. time seems lead chicken problem mutual dependencies which probabilistic inference typically handled using iterative procedures. case explicitly seek feed-forward approach suggests intriguing hypothesis co-existence several classiﬁers class different levels understanding different triggering contextual input features. design choice offers unexpected potential beneﬁt robustness missing features. classiﬁer concept lacks sufﬁcient support another classiﬁer class employing different features could still sufﬁciently sure response. consider example certain object chair represented using classiﬁers classiﬁer based primarily upon features stronger classiﬁer relies global scene recognizer features). clearly chair classiﬁers different failure modes robust different conditions. another example imagine poorer low-res independent person detector could used increase conﬁdence beach. reasoning combined similar weak classiﬁers water sand boats used become conﬁdent indeed beach. beach information trigger powerful classiﬁers better recognizing above person water sand boat beach again. thus allowing multiples classiﬁers concept could simulate iterative inference procedure also move level understanding recognition conﬁdence. type structure expressed naturally classiﬁer graph. idea also establishes interesting connection recent approaches high-level vision tasks using hierarchical inference auto-context another advantage multiple classiﬁers class could better handle large intra-class variability present real-world images people images could appear different sizes shapes resolutions different poses different locations various scenes establishing wide range interactions people objects. many classiﬁers category people could test cases simultaneously. then outputs ensemble person classiﬁers could aggregated ﬁnal answer using variety known methods also mention issue localization. classiﬁer graph predicts existence category certain search area region presence w.r.t. center reference location output returned taking maximum output search window/region directly related pooling convolutional networks maxout units therefore relative central location could multiple classiﬁers class different locations scales different regions presence. believe idea sharing core classiﬁer makes sense consider following examples case face classiﬁer right different left classiﬁers; front wheels different back wheels wheels; person right front needs different classiﬁer different properties distant person barely peripheral vision persons could talk front distant one. sometimes presence certain object anywhere scene need know speciﬁc location unimportant case speciﬁc location crucial example soccer game location goalkeeper crucial penalty important ball half ﬁeld cases distinct could recognized distinct classiﬁers different levels location pose reﬁnement well technical knowledge. thus classiﬁer graph avoids shoehorning multiple facets concept single classiﬁer. believe later classiﬁers likely contain reﬁned versions classiﬁers learned earlier layers condition imposed emerges naturally data merited. graph multiple classes classiﬁcation lower levels abstraction could help classiﬁcation higher levels form part-whole relationships. classes similar levels could inﬂuence establish interactions ideas also suggest multi-class system recognition class achieved recognition many others task superﬁcially appears binary classiﬁcation problem actually multi-class hood. strongly believe since classes interconnected real world also interconnected jointly learned within recognition system. helps recognition semantic understanding scene. fact known since times ancient greeks also conﬁrmed recent psychological research humans much better learning understanding remembering concepts related coherent story stories describe interactions objects relate other spatially temporally well higher levels abstraction fundamental representations intelligent understanding. physical objects well abstract concepts could understood means actual stories communicated verbally experienced real life. thus activations classiﬁers within classiﬁer graph viewed efforts towards explaining complex visual scene form story. co-occurrence interactions object object relationships certain categories usually co-exist presence strong evidence presence other. could context other. classes tend co-occur interact output classiﬁer would useful input feature classiﬁer other. relativity classes co-existence opposition part-whole relationships classes sometimes exist relationship classes. contrast similarity fundamental seeing different classes. white seen opposition black blue yellow cold felt opposition warm. ﬂower ﬂower similarity ﬂowers opposition leaves grass branches part trees gardens. need least parts parts co-exist simultaneously sharing exact space. parts trigger subjective perceptual existence fact different separate entity parts recognized least dedicated classiﬁer. human vision separate existence parts form also suggested patients suffering visual agnosia seem problem seeing understanding meaning seeing objects wholes actually results limited capacity see. brings question isn’t case apparently single uniﬁed conscious visual perception fact simultaneous multi-level process many different subjective realities perceived time objects also learned understood stories give meanings perception triggered behavior speciﬁc context story behind every perceived thing story often triggering seeing existence re-using prior knowledge learned classiﬁers many categories would waste learn classes. classiﬁers could include manually designed features well feature detectors anonymous classes discovered previous learning tasks learning everything scratch every time deal classiﬁcation problem efﬁcient. better appreciate interconnectedness classiﬁcation consider following examples. example imagine countryside horse running grass. although horse clearly reality horse quite basis shape alone conﬁdent object’s identity. however combination several factors color moves size combine convince thing horse. words seeeing horse depends seeing well knowing many things considered inputs subconscious recognition animal. example consider superﬁcially straightforward binary classiﬁcation task recognizing white horse dark background. considered multi-class problem classiﬁer graph? first note even absence external context horse recognized complex interplay various properties overall shape shapes relative conﬁgurations body parts presence distinctive features mane tail. body parts could object classiﬁers shared broadly related animal species; others distinct horses even particular horse since classiﬁers treated identically classiﬁer graph activation patterns classiﬁers enables recognize horse traditional multi-class problems lack positive ﬁring classiﬁers provides crucial information. simultaneous perceptual layers recognition discussed possibility co-existence within structure classiﬁer graph several classiﬁers category part deep multi-category recognition system. graph able recognize simultaneously many different classes sub-classes different levels abstractions context other. classes deﬁned w.r.t. other multiple classiﬁers given class capturing different individual learning experience various training epochs different stages abstraction. representation given class starts simpler context-independent classiﬁers evolves addition complex classiﬁers pool visual input features learning nodes. idea re-using previously learned classiﬁers potential input features also related recent work learning annotations large datasets weakly tagged videos aradhye learned annotations stages stage retaining conﬁdent annotations. employing classiﬁer scores previous stages features current stage enabled system accurately learn annotations form composition even presence label noise. model classiﬁers nodes linear models logistic regression linear support vector machines graph learned node node every graph update automatically select relevant features using boosting logistic linear classiﬁer features selected learning easy. difﬁcult aspects sample organize training order learn multiclass models increasingly sophisticated classiﬁers concept choose relevant features pool initial features previously learned classiﬁers. employ unsupervised clustering boosting strategies order handle issues discussed below. important elements creating growing classiﬁer graph ﬁrst features considered start learning graph; inference method used classiﬁcation given certain classiﬁer graph; learning graph choose positive negative training examples epoch select relevant features pool learn node. initial feature types initial features atomic classiﬁers explained previously trained input nodes. represent ﬁrst level hierarchical structure rooted input sensors. among ﬁrst input features also consider previously trained classiﬁers feature detectors auto-encoders various potentially related classes. order comprehensively capture different visual aspects objects need look various sources information shape color texture occlusion regions boundaries foreground/background segmentation cues. dimension captures different view data list potential features comprehensive redundant discriminative. current work deep nets spends signiﬁcant computation learning features directly pixels. motivation behind learning directly input hand-crafted features optimal. advantage current neural networks systems comes ability learn specialized features cost expensive computation large quantities figure drawing woman’s face. pixel drawing belongs many categories time. example pixel pupil sits also face person. category seen different classiﬁers specialized class. underneath holistic visual experience seeing woman’s face many classiﬁers many levels abstraction simultaneously combined together form contextual environment others recognition. establish interactions classiﬁers objects scene eventually form spatiotemporal stories; section discussion learning spatiotemporal categories video. section want closer attention issue simultaneous category recognition different interpretation levels. consider woman’s face figure play brief mind experiment. imagine looking point pupil left woman’s face. type pixel pupil pixel pixel face pixel woman’s pixel? soon realize pixel belongs categories time many more. simultaneously sits pupil face person. since face image made pixels play different roles time full human-like understanding image. also note pixel question could classiﬁed many classiﬁers simultaneously starting low-res generic reﬁned classiﬁer takes consideration features eye. level could classiﬁers consider geometrical alignments parts face sensitive symmetry harmony general sense beauty. learning classiﬁer graph discussed before output node graph could combination outputs constitute evidence presence another class. existing classiﬁers nodes graph potential features along initial figure schematic description recursive hierarchical face classiﬁer. initial node classiﬁers initial features types linear logistic regression. classiﬁers learned many random copies different locations scales different search areas added pool potential features. manner input features could achieve recursively level abstraction. learning iteration potentially powerful speciﬁc features picked automatically using boosting one-layer classiﬁer learned. process repeated manner leading recursive structure could potentially handle number classes links bottom-up top-down different levels interpretation abstraction. explicit labels used example given clarity presentation. reality intermediate classes sub-classes could discovered learned automatically. example classiﬁer ﬁrst learned. copies added pool different locations scales different search areas face classiﬁer learned classiﬁer given location scale search area could picked automatically added graph together parent nodes relative location search area relative child whole classiﬁer. training data. interesting examples learned features besides common-looking edges corners gabor-like ﬁlters include neural units encode spatial transformations learned features harder guess design manually. pixels without precluding opportunity learning features later stages. thus classiﬁer graphs reject false dichotomy employing traditional engineered features learning features scratch deep architecture. different common trend exclusively learning data argue hybrid approach. nodes classiﬁer graph employ engineered learned features. importantly learned features saved re-used future classiﬁcation tasks instead learn scratch task. classiﬁer graphs thus leverage learned feature detectors auto-encoders previous classiﬁcation tasks designed features sift shape-context proved wide range computer vision problems applications well learning completely features. utilizing engineered features reduce depth deep hierarchical learning reduce training time training data size re-use prior knowledge improve generalization. bootstrapping early nodes classiﬁer graph simple descriptor combined linear classiﬁer eliminate need learn equivalent relatively large three-layer network operates directly system consider among others successful visual features recognition today sift haar wavelets-like features similarity transformation neural units learned also propose local histograms several cells color values similar local histograms gabor ﬁlter responses foreground/background segmentation cues extend idea selecting large pool haarlike features computed many different scales locations include feature pool many classiﬁers feature types different relative scales locations. augment classiﬁer region presence search area similar pooling convolutional nets output returned feature detector maximum response search area. maintain large pool features grown together graph. learned unit node copied many times randomly varied scale location search area added feature pool later use. pool effectively overall graph contains many copies subgraphs learned recursive manner together edges. intriguingly generation copies randomly varied location scale region presence parameters relates reproduction mutation phases genetic programming relatively distant subﬁeld artiﬁcial intelligence space computer programs explored using darwinian-inspired operators reproduction mutation cross-over manipulate blocks code. case cross-over would correspond combining sub-graphs separate classiﬁer nodes even though discuss case creating features random recombination exclude interesting possibility enlarging pool features. moreover many genetic programming approaches represent programs graph-structures reveals another similarity approach classiﬁers represented graphs sub-graphs. classiﬁers’ copies pointers original classiﬁers plus transformation parameters refer copies feature-nodes original learned node termed concept-node contains actual classiﬁer input links weights. classiﬁer graph structure deeply recursive conceptnode class detector calls input parent nodes concept-nodes. recursive calls continue ﬁrst-stage classiﬁers reached. classiﬁcation deep detection feature-node associated classiﬁer center location relative child node dedicated region search area relative center location classical pooling classiﬁer applied every position inside maximum output returned. concept-node node learned relative scale search area concept-nodes always child nodes child nodes effectively concept-nodes created copied added feature pool later selection become parent nodes. node relatively strong geometric relationship child area search small. spectrum node represents completely different location independent object might anywhere scene might cover entire image routine. turn node nchild applies detection procedure parents nodes. then recursively call local detection function every location inside search area also parents return maximum output. parents same ﬁrst ancestor classiﬁer nodes reached avoid redundant processing overlapping pooling areas efﬁcient implementation recursion take advantage dynamic programming caching memoization moving bottom hierarchy saving intermediate results search area along way. first given child node immediately union locations scales search areas initial features concept-nodes called recursive call. starting bottomup initial features ﬁrst classiﬁer called concept-nodes parents returned output locations scales. guarantee single call location scale given concept-node. algorithm also adaptable parallel implementations independent classiﬁers parents ﬁnished applied simultaneously. searching maximum output certain area similar scanning window strategy detecting single best seen object particular region. mentioned above search area found relative node’s children node several children absolute location different them. recursive pooling approach results hierarchical deep detection system also related deep quasi-dense matching strategy recent work large displacement optical hierarchical classiﬁcation approach follows part-based model could arbitrarily deep complex combined large pool features classiﬁers. also suitable detection contextual information mentioned before meant handle several classiﬁers class learning graph non-trivial aspects feature selection choosing appropriate training examples training epoch. feature selection propose novel scheme weakly related combines supervised adaboost re-weighting samples natural unsupervised clustering. epoch certain organization positive training data several potentially overlapping clusters. negative training samples could contain class different positive label clustered. then iteration testing newly added feature detector apply standard adaboost reweighting training samples. next detector selected feature pool best performance separating cluster maximum sample weights negative class. manner take advantage adaboost supervised weighting minimize overall ensemble exponential loss training samples natural unsupervised clustering positively labeled data select diverse feature detectors specialized different views positive training set. interestingly enough observed fruitful collaboration unsupervised clustering data supervised semi-supervised training seemingly unrelated problems graph matching learning graph matching important aspect method chosen clustering distance function used clustering algorithm. clustering training data samples clustering training data could take advantage natural clustering data or/and spatial temporal coherence note clusters need disjoint choose different features parameters clustering could many clusters overlapping elements. since clusters need disjoint propose perform several rounds clustering using different algorithms different descriptors data samples different distance functions could range simple euclidean distance feature space sophisticated distances computed feature matching pyramid match kernel spatial pyramid kernel matching geometric constraints could keep ﬁnal clusters ones high quality estimated internal measures dunn index external ones cluster purity. negative samples also considered clustering cluster purity case would measure percentage positive samples cluster positive majority. also expect spatial temporal coherence natural geometric appearance transformations naturally take place video sequences constitute rich source training data solid basis clustering. moreover user provided keywords freely available videos internet represent weak labels could effectively used grouping together videos common labels. strategy successfully applied automated video annotation large video corpora youtube come back discuss possibility learning video representing spatiotemporal concepts section iteration push next classiﬁer different possible rest ensemble choosing farthest positive cluster maintains classiﬁer diversity even case features/classiﬁers weak. known adaboost typically handle well ensembles strong classiﬁers lack classiﬁer diversity strong classiﬁers good themselves different soft weighting sample help much first divide overall training time several epochs. epoch apply algorithm epoch learns different training based particular clustering training samples regarding training sets epoch several possibilities epoch different subset positive clusters thus obtaining different view class epoch could change label positive class multi-class setting. epoch limit number nodes selected clusterboost stop early even classiﬁcation training perfect. keep graph sparse avoid overﬁtting novel node pool features relatively soon. note sparse networks tend generalize better idea recent successful dropout training partly based figure left face superimposed context-independent part-based classiﬁer. separate classiﬁer left chosen input feature next level face classiﬁer. middle active classiﬁer graph different hierarchical levels. solid arrows indicate bottom-up relationships orange dashed-dotted arrows show top-down lateral relationships. classiﬁers black subgraphs selected input features pool features light blue circles indicate classiﬁers learned initial basic feature types semi-transparent circles indicate local search areas individual classiﬁers. area relative child node. right column pool candidate input features contain basic visual features well simple complex learned classiﬁers full classiﬁer subgraphs. algorithm learning clusterboost current pool features/classiﬁers observation weights initialize clusters positive samples positive samples maximum weights argmaxc ∑i∈c best classiﬁer using feature separates negative data current weights. errk weighted error according training samples. −errk exp= fk)] learn linear logistic classiﬁer current node using outputs input features. update feature pool many modiﬁed copies training epoch. return updated selecting nodes re-learn logistic regression classiﬁer epoch better weights simultaneously obtain probabilistic output epoch node classiﬁer copied together pointer subgraph randomly varying locations scales pooling search areas w.r.t. reference bounding box. many modiﬁed copies added pool features later epoch could input features. manner arbitrarily complex classiﬁers input feature candidates could lead features level abstraction interpretation. feature selected subgraph automatically added graph. subgraph addition expensive pointers subgraph used copy feature. also adding feature pool ﬁrst check whether already graph. similar version already there re-use existing node thus connecting nodes across many levels abstraction. overview overall classiﬁer graph learning scheme presented algorithm evolving distance functions unsupervised clustering training samples strongly depends feature descriptors play important role deﬁning distance learning epoch starts select node train classiﬁer node clusterboost current pool features update graph subgraph make random copies generate features fnew form sampling update feature pool fnew. select training set. back step figure clusterboost select optimal input feature-nodes large pool guided classiﬁcation error measure combines unsupervised clustering supervised weighting training samples. features selected added sequentially. ﬁnal classiﬁer weights recomputed simultaneously supervised learning. many random copies classiﬁer different relative locations scales different pooling areas added feature pool followed improved re-clustering using evolved distance functions consider features added. manner clusterboost repeated epoch epoch large complex classiﬁer graph learned. similarity functions training examples. considered initial manually designed feature types building descriptors used clustering. learn powerful features could expect improve ability perform unsupervised learning starting supervised training epoch propose study ways classiﬁers learned along order better organize training data training epochs. outputs current classiﬁers could used form updated descriptors training images. precisely image could descriptor vector could output classiﬁer image thus classiﬁer feature pool adds element descriptor image. similarity between images function descriptors evolve training epoch next. consequently unsupervised clustering also change. approach could provide natural inter-play supervised unsupervised learning co-evolving many stages training. possible similarity function could consider ratio number co-occurring positive outputs total number positive outputs human loop organization training samples could also performed manually. sketch high-level general strategy possible manual organization training data. classes sequences initially given simpler learning basic shapes centered size-normalized fewer colors less cluttered backgrounds. then complex scenarios follow deformations illumination changes difﬁcult classes still relatively simple. possible focus categories sub-parts ﬁnal classes want learn consistent related context. graph feature pool initialized detectors basic categories relatively deep structure sequences difﬁcult higher-level classes come could ﬁrst think speciﬁc limited world gradually expand generating different views besides unsupervised clustering approach based spatial temporal coherence could also useful cropped images ideal learning bottom-up relationships top-down relationships level scene nearby objects level object/category interest need information surrounding regions contain object. thus contextual top-down lateral relationships desired training images contain object interest inside given ground truth bounding box/region well surrounding related areas objects information given training image could similar format pascal challenge ground truth bounding boxes objects categories present image. figure blue plot natural density data points feature space. natural generative probability discovered unsupervised clustering. clusters expected correlated classiﬁcation error points cluster similar expected receive similar labels classiﬁcation algorithm. since supervised learning take consideration natural clustering data propose modiﬁed robust version adaboost trains weak classiﬁers individual clusters. clusters prioritized based sample weights correlated exponential loss adaboost. figure sampling locations scales search areas cover space uniformly. colored rectangles represent possible search areas given feature pool features. argue granularity sampling need dense local reﬁnements geometric transformations ﬁtting could gaps continuum needed. example human peripheral vision resolution expect able perfectly localize object image periphery without bringing object focus performing extra ﬁne-tuned localization geometric ﬁtting could interpolate discrete figure node learned child node concept node operates entire image thus search area location image center relative scale learned copied many times various locations scales different search areas added pool features potential future parent node feature nodes link pointers original classiﬁer represent. careful inspection ﬁgure reveals recursive nature classiﬁcation graph parent node operates certain scale bounding effectively input image calls classiﬁer pointer passes image. child’s point view image whole image turn child calls parents again call orange classiﬁer process repeats orange circles parents reached ones function directly image input initial feature types. speculate transitions children parents change focus attention feature nodes concept nodes consistent saccades operate human visual system. feature-nodes activated peripheral vision memory system orange concept nodes activated attentional system. tational cost perform inference graph constructed? order provide approximate answers ﬁrst need clarify technical details many copies concept-node need make? isn’t number prohibitively large given want randomly sample locations scales search areas? mean make copy? given node arbitrarily complex graph afford copy many times? start figure estimate many copies node expect make order cover sufﬁciently dense scale location search area parameters’ space. expect potential locations objects image require dense sampling. location scale estimate average different search areas. course actual number locations scales search areas also depend actual category could increased reduced initial training. buildings example large objects images expected found limited range scales locations. birds hand usually small could priori anywhere image. would safe consider different scales average rough estimate maximum copies classiﬁer type. mentioned before need store actual copy original child node subgraph pointer copies would need bytes information encode pointer classiﬁer type location scale search area allow possible visual concepts average different views concept different visual classiﬁer types total feature-nodes pool features. classiﬁer types store maximum input nodes weights incoming edges. initial estimate storage required whole system would range hundreds gigabytes terabytes data. important extension proposed classiﬁer graph would realm video sequences learning recognition spatiotemporal concepts human actions activities human-object interactions generally events involve objects interact time. classiﬁer graph could immediately extended spatiotemporal classes always intelligent vision system learns appearance-based classiﬁers geometric relationships also events take place time. motivation intuition discussed introductory sections apply spatiotemporal domain well. scenes objects parts could statistically related spatial relationships also temporal ones. category could appear within certain time period relative another event certain type. time period relates parent category child time domain equivalent search area region presence discussed spatial realm images. ideas presented previous sections images multiple classes single category overall multi-class recognition system learning classiﬁcation immediately transfer spatiotemporal domain. main ideas behind classiﬁer graph also described introduction limited single moment time. look following example. imagine category eating self-prepared omelette. special case eating omelette might many classiﬁers dedicated concept eating omelette could take advantage temporal context immediate past person eats involved preparing omelette event could dedicated classiﬁer. relationships classiﬁers related eating preparing temporally ordered preparing food must happen eating preparing food certain events happen speciﬁc order gathering necessary ingredients followed cracking eggs bowl beating them adding little salt pepper cooking pan. certain events objects appear certain temporal order certain relative spatial positions others less rigidly linked space time. example frying deﬁnitely take place cracking eggs adding salt could principle happen time. exact relative time differences spatial dependencies might needed classiﬁcation region presence discussed could vary large single point. events objects temporally related weak co-occurrence others strictly ordered time precise relative temporal locations many people make omelettes different ways butter frying others oil; vegetables meats others prefer salt. styles cooking manner cook performs cooking differ person person. many different classiﬁers might learned task preparing omelette depending individual experience cultural context. classiﬁer graph recursive network classiﬁers form deep graph structure. visual system could always reminding classical recurrent neural networks models nodes could triggered previous events maintained short period time others could attention present input. spatiotemporal volume presented working classiﬁer graph memory attention could function together order perform spatiotemporal scans recursive recognition adding third temporal dimension system presented before. related model hierarchical temporal memory system also considers temporal windows classiﬁer co-occurrence establishing relationships discovering recognizing spatiotemporal patterns. would interested study connections models order better understand proposed classiﬁer graph could handle tasks kinds input preferred", "year": 2014}