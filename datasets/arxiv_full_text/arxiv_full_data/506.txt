{"title": "Where to put the Image in an Image Caption Generator", "tag": ["cs.NE", "cs.CL", "cs.CV"], "abstract": "When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. The merge architecture does have practical advantages, as conditioning by merging allows the RNN's hidden state vector to shrink in size by up to four times. Our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the RNN as that yields large, memory-intensive models with few tangible advantages in performance; rather, the multimodal integration should be delayed to a subsequent stage.", "text": "recurrent neural network language model used caption generation image information neural network either directly incorporating conditioning language model ‘injecting’ image features layer following conditioning language model ‘merging’ image features. options attested literature systematic comparison two. paper empirically show especially detrimental performance whether architecture used another. merge architecture practical advantages conditioning merging allows rnn’s hidden state vector shrink size four times. results suggest visual linguistic modalities caption generation need jointly encoded yields large memory-intensive models tangible advantages performance; rather multimodal integration delayed subsequent stage. throughout paper refer textual descriptions images captions although technically caption text complements image extra information available image. speciﬁcally descriptions talk ‘concrete’ ‘conceptual’ image descriptions figure rnn-based neural language models work. legend recurrent neural network; feed forward layer; wordi generated word text; wordstart start token artiﬁcial word placed beginning every sentence order still preﬁx predicting ﬁrst word note state represents preﬁx ‘wordstart’ state represents preﬁx ‘wordstart word’ etc. processing preﬁx passes ﬁnal state statet feedforward layer predicts likely known word next word preﬁx. neural language model typically form recurrent neural network used generate text given sentence preﬁx neural language model predict words likely follow. small modiﬁcation simple model extended image caption generator language model whose predictions conditioned image features. this neural language model must somehow accept input sentence preﬁx also image captioned. raises question stage image information introduced language model? recent work image captioning answered question diﬀerent ways suggesting diﬀerent views relationship image text caption generation task. knowledge however diﬀerent models architectures systematically compared. question image information feature captioning heart broader questions concerning language grounded perceptual information questions addressed cognitive scientists practitioners conditioning injecting versus conditioning merging neural language model conditioned injecting image merging image ‘inject’ architectures image vector injected example treating ‘word’ including part caption preﬁx. trained encode image-language mixture single vector vector used predict next word preﬁx. hand case ‘merge’ architectures image left subnetwork handles caption preﬁx handles purely linguistic information. preﬁx encoded image vector merged preﬁx vector separate ‘multimodal layer’ comes subnetwork. merging done example concatenating vectors together. case trained encode preﬁx mixture handled subsequent feedforward layer. early versus late inclusion image features foregoing description suggests merge architectures tend incorporate image features somewhat late generation process processing whole caption preﬁx. hand inject architectures tend incorporate image features early generation process. inject architectures incorporate image features whole duration generation process. diﬀerent architectures make visual information inﬂuence linguistic choices diﬀerent stages. fixed versus modiﬁable image features word predicted form visual information must available inﬂuence likelihood word. merge architectures typically exact image representation every word output. hand injecting image features allows internal representation image inside hidden state vector changed rnn’s internal updates time step. diﬀerent architectures allow diﬀerent degrees modiﬁcation image features generated word. main contribution paper present systematic comparison diﬀerent ways ‘conditioning’ linguistic choices based visual information carried studying implications caption generator architectures. thus rather seeking results improve state seek determine based exhaustive evaluation inject merge architectures common dataset image features best placed caption generation image retrieval process. scientiﬁc perspective comparison would useful shedding light language grounded vision. images text intermixed throughout process initially kept separate combined multimodal layer? many papers speak rnns ‘generating’ text. case rnns better viewed encoders vectorise linguistic preﬁx next feedforward layer predict next word conditioned image? answers questions would help inform theories caption generation performed. architectures compare provide diﬀerent answers questions. hence important acquire insights relative merits. engineering perspective insights relative performance diﬀerent models could provide rules thumb selecting architecture task image captioning possibly tasks well machine translation. would make easier develop architectures ways perform caption generation. remainder paper structured follows. ﬁrst give overview published caption generators based neural language models focusing particular architectures used. section discusses architectures compare followed description data experiments section results presented discussed section conclude general discussion directions future work. section discuss number recent image caption generation models emphasis image conditions neural language model based distinction inject merge architectures illustrated figure discuss models ﬁrst outline four broad sub-categories architectures identiﬁed literature. section made high-level distinction architectures merge linguistic image features multimodal layer inject image features directly caption preﬁx encoding process. fact distinderived image vector). word vectors caption preﬁx come later. image vector thus treated ﬁrst word preﬁx. requires image vector size word vectors. early binding architecture allows image representation modiﬁed rnn. addition above additional theoretical possibility might refer ‘post-inject’. post-inject architectures would image vector preﬁx rather beginning done pre-inject. would late binding architecture allows minimal modiﬁcation image representation rnn. practice would possible structuring training collection ‘sentence preﬁx next word’ pairs training language model using minibatches individual preﬁxes rather full captions once. attested work actually adopts architecture knowledge; hence shall refer follows. preﬁx either takes separate inputs; word vectors combined image vector single input passed rnn. image vector doesn’t need exactly word need included every word. mixed binding architecture whilst allowing modiﬁcation image representation harder image every time step hidden state vector refreshed original image time. image vector) point. instead image introduced language model preﬁx encoded entirety. late binding architecture modify image representation every time step. init-inject architectures architectures conforming init-inject model treat image vector initial hidden state vector wang combine rnns parallel initialized image. similar architecture init-inject used traditional deep learning machine translation systems source sentence encoded vector used condition language model generate sentence another language. basis system described ﬁrst extract sequence attributes image translate sequence caption. also used attention mechanisms order provide vector representing information whole image whilst parts image attended diﬀerently time step provided par-injection. example initialize centroid image parts attending parts needed. pre-inject architectures pre-inject models treat image though ﬁrst word preﬁx image attributes sometimes used instead image vectors also passing image ﬁrst words instead word using image vector ﬁrst word image attributes second vice versa. like init-inject pre-inject also used provide information injects image attributes. attention-based mechanism init-injects full image attended image par-injected merged. first words image attributes image. either pre-inject made image attributes par-inject made image vice versa. incorporating image every time step. sequence vectors represents sentence topics converted separate sentence conditioning language model using pre-inject. par-inject architectures par-injection inputs image features jointly word caption. common architecture used largest variety implementation. example donahue rnns series better inject image second ﬁrst. parinject image whilst pre-injecting image attributes par-inject attributes image whilst init-injecting image vector. other less common instantiations include par-injecting image ﬁrst word passing words separate resulting hidden state vectors combined image vector many times architecture used order pass diﬀerent representation image every word visual information changes diﬀerent parts sentence generated. example zhou perform element-wise multiplication image vector last generated word’s embedding vector order attend diﬀerent parts image vector. oruganti pass image many times words order diﬀerent image vector every word. chen zitnick simple predict image vector looks like given preﬁx. predicted image used second image representation par-injected together actual image vector. commonly modiﬁed image representations come attention mechanisms rennie inject image input modiﬁed long short term memory network lstm allows inject attended image directly inside input gated expression like init-inject pre-inject par-inject sometimes used provide information whole image attention mechanisms whilst attended image regions merged merge architectures rather combining image features together linguistic features within merge architectures delay combination caption preﬁx vectorised hendricks merge architecture order keep image thus able train part neural network handles images part handles language separately using images sentences separate training sets. tention mechanisms merging diﬀerent image representation every time step. merge well par-inject attended visual regions whilst merge regions whilst par-injecting ﬁxed image representation. literature caption generation provides rich range models comparative evaluations little explicit systematic comparison performance architectures surveyed above represents diﬀerent conditioning prediction language sequences visual information. work tested par-inject preinject vinyals reports pre-inject works better. work compares inject merge architectures concludes merge better inject. however al.’s comparison architectures relatively tangential part overall evaluation based bleu metric answering question architecture best diﬃcult diﬀerent architectures perform diﬀerently diﬀerent evaluation measures shown example wang compared architectures simple rnns lstms. although state systems caption generation inject-type architectures also case complex systems published merge architectures fair conclude inject better merge based survey literature alone. follows present systematic comparison diﬀerent architectures discussed above. perform evaluations using common dataset variety quality metrics covering quality generated captions; linguistic diversity generated captions; networks’ capabilities determine relevant image given caption. section diﬀerent architectures evaluated paper. diagram illustrating main architecture schema basis every tested architecture work shown figure schema based architecture described vinyals without ensemble. architecture chosen simplicity whilst still best performing system mscoco image captioning challenge. figure illustration main architecture schema instantiated four diﬀerent architectures tested paper. legend ‘ff’ fully connected feed forward layer bias; ‘ffimg’ layer projecting image vector ‘ffout’ layer projecting softmax output; layer size vocabulary size dashed arrows used depending whether architecture merge inject. word embeddings word embeddings vectors represent known words prior consist vectors randomly initialised. precompiled vector embeddings wordvec used. instead embeddings trained part neural network order learn best representations words task. recurrent neural network purpose take preﬁx embedded words produce single vector represents sequence. gated recurrent unit used experiments simple reason powerful hidden state vector. contrast lstm state vectors would make architecture comparisons complex presence state vectors raise possibility multiple versions init-inject architecture. using single hidden state vector implement init-inject. image prior training images vectorised using activation values penultimate layer oxfordnet -layer convolutional neural network trained perform object recognition returns -element vector. convolutional neural network inﬂuenced caption generation training. training feed forward layer neural network compresses vector smaller vector. output image caption preﬁx vectorised mixed single vector next step predict next word caption. done passing mixed vector feed-forward layer softmax activation function outputs probability section describes experiments conducted order compare performance diﬀerent architectures described previous section. tensorﬂow used implement neural networks. datasets used experiments version flickrk flickrk mscoco distributed karpathy fei-fei three datasets consist images taken flickr combined seven manually written captions image. provided datasets split training validation test using following number images respectively flickrk flickrk mscoco images already vectorised -element vectors activations layer ‘fc’ oxfordnet -layer convolutional neural network trained object recognition imagenet dataset known vocabulary consists words captions training occur least times. amounts tokens flickrk tokens flickrk tokens mscoco. words used inputs embedded outputs assigned probabilities softmax function. word part vocabulary replaced unknown token. results reliable important best hyperparameters architecture judge performance architectures optimally tuned rather using one-size-ﬁts-all hyperparameter settings might cause architectures under-perform. reason used multi-step process hyperparameter tuning described below. optimized hyperparameters order maximize caption quality flickrk validation using beam search generation method cider objective function. optimal hyperparameters ﬁxed across datasets. also used flickrk hyperparameter tuning cider shown rennie useful metric optimise yielding improvement quality metrics used objective function. language model perplexity validation measured soon epoch results worse perplexity previous epoch training stops. maximum number epochs still used prevent training going long note allowed layer change freely layers init-inject would still require image size size equal pre-inject would still require image size embedding size equal whilst par-inject merge would size restrictions. would make former architectures signiﬁcantly less hyperparameter combinations explore would likely result unfair advantage hyperparameter tuning. following steps followed order tune hyperparameters evaluated training neural network maximum epochs generating captions beam width evaluating captions using cider take best combination found previous steps ﬁnetune using greedy hill climbing record modiﬁed combination. check changing hyperparameter improve performance. previous steps reliable cider scores associated score produced using training generation might coincidentally unusual score ideally would tested hyperparameter combination three times taken mean resulting cider scores. ideally would also tried diﬀerent values maximum number epochs beam width. this however would extremely time consuming. thus apply procedure subset best performing combinations previous steps. ensure subset diverse choosing combinations dissimilar other follows selected combinations take three combinations diﬀerent terms hamming distance. ensure three combinations best combination found previous step. take three combinations selected diﬀerent maximum epochs beam widths them. evaluation measured using average cider score three independent training generation runs. generation metrics metrics quantify quality generated captions measuring degree overlap generated captions test set. mscoco evaluation code measures standard evaluation metrics bleu- rouge-l meteor cider diversity metrics apart measuring caption similarity ground truth also measure diversity vocabulary used generated captions. intended shed light extent captions produced models ‘stereotyped’ extent model re-uses strings case case irrespective input image. limiting case consider caption generator always outputs caption. generator would lowest possible diversity score. order quantify measure percentage known vocabulary words entropy gives measure uniform frequency distributions uniform likely unigram bigram used equal proportion rather using words majority time hence greater variety words used. finally also measure percentage generated captions already exist training estimate extent model evinces ‘parroting’ wholesale caption reuse training set. diversity metrics obtain ceiling estimate computing measures test captions themselves. take ﬁrst caption group human-written captions available image test apply diversity metrics them. retrieval metrics retrieval metrics metrics quantify well architectures perform retrieving correct image test images test given corresponding caption. conditioned language model used retrieval measuring degree relevance image given caption. relevance measured probability whole caption given image different images give diﬀerent probabilities caption. probable caption relevant image. since process takes time proportional number captions multiplied number images pool possible captions consider retrieval excluded captions except ﬁrst group captions available image order reduce evaluation time. mscoco used ﬁrst test images reason similar flickrk flickrk images. three runs experiment three datasets performed. various evaluation measures report mean together standard deviation three runs. initial model weights minibatch selections dropout selections diﬀerent since randomly determined. everything else identical across runs. interesting note that every architecture’s optimal hyperparameters output needs regularized dropout image vector non-linear activation function regularized dropout image input vector must normalized neural network. par-inject seems need help terms regularization even terms beam width whilst small size merge means needs least amount regularization. interesting observation merge architecture much ‘leaner’ overall. terms size needs half par-inject needs quarter init-inject pre-inject require optimal performance. makes sense since merge needs storing linguistic information whilst architectures need additionally store visual information image. using larger merge architecture would likely lead overﬁtting. implication init-inject pre-inject much memoryhungry architectures require large hidden state vectors order function well whilst merge eﬃcient. fact number parameters init. method init. weight range layer size normalize image image activation init. hidden state regularize weights image dropout image proj. dropout embedding dropout dropout minibatch size max. epochs beam width inject architectures par-inject smallest optimal size. probably fact that model image present time steps thereby necessitating less memory allocated ‘remember’ visual information together linguistic information compared early-binding architectures. it’s interesting note par-inject size equal size concatenated image hidden state vector merge architecture. merge’s small size impact performance generated captions compared corpora? metrics reported show considerable variability ranking various architectures depending dataset. example cider scores place init-inject flickrk mscoco merge outperforms measure flickrk. comparing rouge-l meteor cider init-inject seems ranked highest datasets however diﬀerences among architectures small. especially true larger mscoco dataset. thus though init-inject often comes architectures lagging behind wide margin. comes retrieving relevant image caption merge ranked ﬁrst flickrk init-inject flickrk mscoco practically measures well median rank. interestingly sets cases init-inject outperforms architectures merge close second least terms perplexity general picture favour inject models merge evincing marginally greater perplexity datasets. overall however outcomes mirror previous sub-section diﬀerences among architectures seem compelling although init-inject model outperforms merge number instances merge close second. diversity metrics evince dramatic performance diﬀerences. focus proportion generated captions found training mscoco ﬁgure ranges merge par-inject. exception flickrk merge lowest proportion caption reuse overall. results compared preceding sub-sections fact models greatest tendency reuse captions tend perform well corpus-based metrics cider suggests datasets consideration highly stereotyped perhaps signiﬁcant amount redundancy lack variety. similar observation made devlin comparison retrieval-based neural architectures image captioning authors found corpus-based metrics tend give higher scores test instances images similar training instances. neural architectures performed better similar images overall. turning extent architectures training vocabulary picture emerges consistent above. humans used known vocabulary describe test images none evaluated systems used merge architecture tops ranks datasets small margin although unigram bigram entropy highest pre-inject init-inject seeing word training often order learn methodological perspective implies setting even higher frequency threshold words mapped unknown token would feasible would make relatively little diﬀerence results. noted section diﬀerences architectures consideration whether incorporate image features early late. raises possibility diﬀerences degree visual information retained architecture multimodal vector input ‘ffout’ figure information visual linguistic input combined information bottleneck output depends question want answer inject architectures tend ‘forget’ image words input rnn? given rnn’s memory ﬁnite diﬃcult retain information inputs length sequence increases information image might start fading away input sequence gets longer. merge architectures problem visual information kept outside compare original adulterated vectors converge words model implies multimodal vector losing image information would getting inﬂuenced less image preﬁx. figure diﬀerence multimodal vectors time caption held constant image changed. trained model markers show mean absolute diﬀerence multimodal vectors -word caption model original versus randomly chosen image. marker position gives multimodal vector diﬀerence feeding start token whilst marker position gives multimodal vector diﬀerence feeding last word caption. measure distance original adulterated vectors mean absolute diﬀerence take absolute diﬀerence corresponding dimension multimodal vectors take mean diﬀerences. mean absolute diﬀerence avoids giving larger distance larger vectors also intuitive measure diﬀerence vectors. also keeps distance time steps exactly equal merge desirable since merge lose visual information across time steps. experiments used -word captions mscoco test measured mean distance time steps -word captions long enough trend without ending captions create reliable mean repeat procedure times mean images test using random images instance. results shown figure none inject architectures maintained consistent distance original adulterated multimodal vectors. crucially merge architecture also largest distance among architectures demonstrating that architecture words caption exhibit greater dependency image pertain par-inject comes second place terms multimodal vector distance. suggests retains visual information inject architectures though much merge. seems amount retention across time steps changes somewhat unpredictably tends decrease overall means information gets lost time init-inject comes third visual information retention followed pre-inject decrease time. seems that trained results predict generated captions needed long late binding architectures produce better captions retain visual information longer time steps maintaining tighter coupling visual linguistic information. paper presented systematic evaluation number variations architectures image caption generation retrieval. primary focus distinction termed ‘inject’ ‘merge’ architectures. former type model mixes image language information training encode image-preﬁx mixture. contrast merge architectures maintain separation subnetwork encodes linguistic string image vector merging late process prior prediction step. models therefore compatible approaches image caption generation using ‘multimodal’ layer types architectures discussed literature inject architecture popular. little systematic evaluation advantages compared merge. experiments show standard corpus-based metrics cider diﬀerence performance architectures rather small. initinject tends better generation retrieval measures. thus perspective corpus similarity early binding image features models view features modiﬁable appear better alternatives. crucially however also show inject architectures much likely re-generate captions wholesale training data evince less vocabulary variation. hence perspective variation late-binding models treat image features ﬁxed better. part nature available corpora superior performance merge measure suggest that encoding information modalities separately merge architectures might producing less generic stereotyped captions exploiting multimodal resources eﬀectively. experiments visual information retention show that time inject architectures tend loosen coupling visual linguistic features diﬀerence actual adulterated multimodal vectors gets smaller. supports view inject models especially longer captions tend towards generic less image-speciﬁc captions ﬁnding echoes observations devlin extent. case late merging deﬁnition susceptible problem. quires hidden state vector size suﬃcient ‘remember’ caption preﬁxes depends length complexity training captions inject architectures require additional memory also store image information. means merge architectures make better memory. also require less regularization whilst maintaining similar performance architectures. future work hope investigate whether results paper would remain similar experiments repeated applications conditioned neural language models neural machine translation question answering. furthermore keeping language image information separate merge architectures lend potentially greater portability ease training. example possible principle take parameters embedding layers general text language model transfer corresponding layers caption generator. would reduce training time would avoid learning weights embedding weights caption generator scratch. understanding deep learning architectures evolves community goals maximise degree transferability among model components. research paper partially funded endeavour scholarship scheme scholarships part-ﬁnanced european union european social fund operational programme cohesion policy investing human capital create opportunities promote well-being society.", "year": 2017}