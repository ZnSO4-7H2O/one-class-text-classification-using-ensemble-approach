{"title": "Low-Rank Modeling and Its Applications in Image Analysis", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Low-rank modeling generally refers to a class of methods that solve problems by representing variables of interest as low-rank matrices. It has achieved great success in various fields including computer vision, data mining, signal processing and bioinformatics. Recently, much progress has been made in theories, algorithms and applications of low-rank modeling, such as exact low-rank matrix recovery via convex programming and matrix completion applied to collaborative filtering. These advances have brought more and more attentions to this topic. In this paper, we review the recent advance of low-rank modeling, the state-of-the-art algorithms, and related applications in image analysis. We first give an overview to the concept of low-rank modeling and challenging problems in this area. Then, we summarize the models and algorithms for low-rank matrix recovery and illustrate their advantages and limitations with numerical experiments. Next, we introduce a few applications of low-rank modeling in the context of image analysis. Finally, we conclude this paper with some discussions.", "text": "xiaowei zhou hong kong university science technology yang hong kong baptist university hongyu zhao yale university weichuan hong kong university science technology low-rank modeling generally refers class methods solve problems representing variables interest low-rank matrices. achieved great success various ﬁelds including computer vision data mining signal processing bioinformatics. recently much progress made theories algorithms applications low-rank modeling exact low-rank matrix recovery convex programming matrix completion applied collaborative ﬁltering. advances brought attentions topic. paper review recent advance low-rank modeling stateof-the-art algorithms related applications image analysis. ﬁrst give overview concept low-rank modeling challenging problems area. then summarize models algorithms low-rank matrix recovery illustrate advantages limitations numerical experiments. next introduce applications low-rank modeling context image analysis. finally conclude paper discussions. many research ﬁelds data analyzed often high dimensionality brings great challenges data analysis. examples include images computer vision documents natural language processing customers’ records recommender systems genomics data bioinformatics. mathematically represent data point vector denote entire dataset matrix low-dimensionality assumption translated following low-rank assumption rank min. typical example computer vision lambertian reﬂectance d··· correspond images convex lambertian surface various lighting conditions another example signal processing represents vector signal intensities received antenna array time point interested readers referred low-rank examples. corresponds low-rank component corresponds noise error measurements. recovering low-rank structure noisy data becomes centric task many problems. denotes frobenious norm matrix. solving minimization problem interpreted seeking optimal rank-r estimate least-squares sense. according matrix approximation theorem solution given analytically singular value decomposition popular tools data analysis analytical solution computation provable optimality certain assumptions cannot handle difﬁculties real applications. consider following common examples recovery entries. many applications would like recover matrix small number observed entries. typical example that building recommender systems hope make predictions customers’ preferences based information collected far. netflix problem rating customer movie around customers movies dataset entries values since customer rated movies average. problem predict ratings made based current observation. popular solution assume rating matrix low-rank. assumption based fact subgroup customers likely share similar taste ratings movies highly correlated. consequently rank rating matrix bounded number subgroups formed customers. therefore problem turns recovering low-rank matrix entries. problem often called matrix completion recovery gross errors. applications recover lowdimensional subspace corrupted data. example face images person include glasses shadows occlude true appearance. classical assumes independently identically distributed gaussian noise adopts squared differences loss function shown since leastsquares ﬁtting sensitive outliers classical easily corrupted gross errors. example reconstructed face images would include artifacts caused glasses shadows input images recovering subspace low-rank matrix robustly presence outliers become popularly-studied issue. problem often called robust principal component analysis divided categories based approaches modeling low-rank prior. ﬁrst approach minimize rank unknown matrix subject constraints. rank minimization often achieved convex relaxation. call methods rank minimization methods. second approach factorize unknown matrix product factor matrices. rank unknown matrix upper bounded ranks factor matrices. call methods matrix factorization methods. rest paper organized follows. section review rank minimization methods low-rank matrix recovery. shall introduce typical models well corresponding optimization algorithms solve models. section introduce matrix factorization methods low-rank matrix recovery. section synthesized experiments illustrate performances discussed methods. section give brief review applications lowrank modeling image analysis. finally conclude paper discussions section direct approach recovering low-rank matrix minimize rank matrix certain constraints make estimated matrix consistent original data. however rank minimization problem combinatorial known nphard therefore convex relaxation often used make minimization tractable. popular choice replace rank nuclear norm deﬁned using nuclear norm relaxation mainly two-folds. firstly nuclear norm convex. hence feasible compute global optima relaxed problem efﬁciently. secondly nuclear norm proven tightest convex surrogate rank means nuclear norm best approximation rank operator convex functions. moreover analogy using nuclear norm low-rank matrix recovery using ℓ-norm sparse signal recovery well established exact recovery property proven low-rank models using nuclear norm following ﬁrst introduce convex models summarize optimization algorithms ﬁnally introduce nonconvex relaxation methods brieﬂy. cand`es recht theoretically proved solution exactly recover low-rank matrix high probability underlying low-rank matrix satisﬁes incoherence condition locations observed entries unic positive constant matrix size rank. incoherence condition used mathematically characterize difﬁculty recovering underlying low-rank matrix small number sampled entries. informally says singular vectors underlying low-rank matrix sufﬁciently spread uncorrelated standard basis. extreme example underlying low-rank matrix takes entry elsewhere. matrix recovered entry actually sampled. result real applications observed entries noisy equality constraint strict resulting over-ﬁtting therefore following relaxed form often considered matrix completion noise convex programming also used solve rpca. popular method named sparse low-rank decomposition involves decomposition matrix low-rank component sparse component minimizing rank cardinality simultaneously. surprising message that mild assumptions low-rank matrix exactly recovered following convex program named principal component pursuit cardinality respectively. cand`es chandrasekaran analyzed conditions exact recovery. brieﬂy speaking proven underlying low-rank matrix underlying sparse matrix exactly recovered high probability satisﬁes incoherence condition nonzero entries sufﬁciently sparse random spatial distribution. morebasic model extended handle additional scenarios stable considers gaussian noise outlier pursuit incorporates group sparsity matrix recovery compressive measurements stable equality conbased theorem various algorithms developed speciﬁc problems. popular techniques proximal gradient method augmented lagrangian method applicable variety convex problems. method useful solve norm-regularized maximum-likelihood problems model whose energy function comprises differentiable loss nonsmooth regularizer. moreover method often combined nesterov method accelerate convergence examples using method include etc. method closely related alternating direction method multipliers provides powerful framework solve convex problems equality constraints algorithms used belong class. details given following subsections. accelerated proximal gradient method uses nesterov method accelerate convergence instead making quadratic approximation around makes approximation another point linear combination xk−. modiﬁcation give convergence rate please refer details. straightforward equivalent hence softimpute algorithm interpreted ﬁxed step length. fpca algorithm introduced also based continuation technique accelerate convergence. also proposed different implementations matrix completion. tomioka proposed dual augmented lagrangian algorithm matrix completion achieves superlinear convergence. interpreted proximal method descending directions computed augmented lagrangian dual problem. augmented lagrangian method. augmented lagrangian method classical tool minimize convex function equality constraints. example introduce method. named primal minimization dual ascent respectively. primal minimization difﬁcult simultaneously. variable minimize marginal optimization turns nuclear norm regularized proximal problem efﬁciently solved then x-step e-step repeated convergence solve efﬁcient update primal variables iteration instead exactly solving updating dual variable named inexact augmented lagrangian method special case alternating direction method multipliers method summarized algorithm proven straint equivalent original projection operator removed. then applied. minimizing augmented lagrangian turns proximal problem could solved svt. also applied solve nonnegative matrix factorization problem matrix completion σ··· singular values rank minimization intractable. turns nuclear norm tightest convex surrogate. bridge nonconvex cases considered recent literature theoretical analysis recovery properties found besides schatten-p nonconvex surrogate functions rank minimization also studied therefore small small rank. finally problem recovering lowrank matrix converted estimating factor matrices paper discuss representative matrix-factorization methods context low-rank matrix recovery. notice matrix factorization methods recover low-rank matrix. example outputs nonnegative matrix factorization dictionary learning necessarily low-rank. discuss methods here. summary matrix factorization methods please refer straightforward approach solving minimize function alternately ﬁxing one. subproblem estimating turns least-squares problem admits closed-form solution. algorithms type extensively studied many works early computer vision literature recent matrix recovery literature auxiliary variable. step updating solved efﬁciently. additionally lmafit integrates nonlinear successive over-relaxation scheme accelerate convergence alternation. formulation nonconvex empirical results many works demonstrated alternating minimization performed accurately efﬁciently compared convex methods meanwhile theoretical analysis showed alternating minimization succeed conditions similar existing conditions given introduced section lower bounds recovery error using alternating minimization matrix completion analyzed computer vision literature many works adopted higher order algorithms instead alternating least squares solve faster convergence better precision. example buchanan fitzgibbon developed damped newton algorithm solve problem. variables updated based newton algorithm damping factor. however cannot handle large-scale problems infeasibility computing hessian matrix large number variables. interpolate alternating least squares newton algorithm works proposed hybrid algorithms. wiberg algorithm updated least squares updated gauss-newton step iteration. later wiberg algorithm extended damped version achieve better convergence chen proposed algorithm similar wiberg algorithm. difference observation highly incomplete problem likely ill-posed common case collaborative ﬁltering. popular approach addressing issue penalize squared frobenious norms factor matrices method named maximum margin matrix factorization idea similar using squared ℓ-norm ridge regression improve stability parameter estimation. moreover following equality established indicates equivalence mmmf nuclear norm minimization equivalence also studied solve optimization either gradientbased algorithms alternating minimization used rithm named optspace iteratively estimate factor matrices updated gradient descent grassmannian updated least squares. similar theoretical results convex method keshavan provided performance guarantee optspace appropriate incoherence condition. building upon model saad proposed scaled-gradient procedure accelerate convergence algorithm. proposed algorithm named solve following two-factor model updates grassmannian estimates least squares. based model boumal absil developed algorithm named rtrmc optimizes cost function second-order riemannian trust-region method achieve faster convergence. mishra proposed framework optimization riemannian quotient manifolds low-rank matrix factorization. investigated three types matrix factorization full-rank factorization polar factorization subspaceprojection factorization related models respectively. take account invariance class equivalent solutions explored underlying quotient nature search spaces designed class gradient-based trust-region algorithms quotient search spaces. concluded experiments three factorization models different riemmnanian structures almost equivalent terms computational complexity performed favorably compared previous methods lmafit optspace. related works include etc. designed online algorithm solve large-scale problems. numerical experiments showed lrgeomcg performed comparably quotient-space methods matrix completion. recently manifold-optimization toolbox named manopt developed providing ready-to-use algorithms solve optimization problems various manifolds grassmannians ﬁxed-rank manifolds. robust matrix factorization method handling outliers data regarded factorization approach towards rpca. mentioned before sensitivity outliers traditional methods squared loss used penalizes large errors much resulting biased ﬁtting. address issue denotes entry robust loss function. example geman-mcclure function deﬁned adopted solve optimization problem alternating minimization carried updated iteratively solving robust linear regression iterative reweighted least squares. similar idea reweighting data based robust estimators used kanade adopted ℓ-penalty solved problem alternating ℓ-minimization. examples using ℓ-norm robust matrix factorization include kwak proposed maximize ℓ-norm projection data point onto unknown principal directions instead minimizing residue eriksson hengel generalized wiberg algorithm handle case. zheng proposed nuclear-norm regularizer improve convergence solved optimization alm. presented efﬁcient algorithm using group-sparsity regularization established equivalence proposed method rank minimization. many works tried address problem probabilistic point view modeled non-gaussian noise improve robustness. discuss section demand online processing streaming data like video motivated development many incremental algorithms subspace tracking past decades recent works online algorithms incomplete corrupted data developed. grouse example following cost function minimized subspace time data point arrives denotes observed entries iteration ﬁrst solved least squares updated gradient descent grassmannian. algorithm grasta introduced extends grouse handle outliers robust estimation replacing squared loss ℓ-norm solve ℓ-minimization problem iteration admm framework used grasta. shalit developed online algorithm optimization low-rank manifold discussed section improve scalability matrix factorization algorithms parallel frameworks proposed perform matrix factorization parallel computing architectures handle extremely large datasets. recht proposed algorithm named jellyfish large-scale matrix completion. minimizes energy function incremental gradient descent i.e. variables updated following approximate gradient constructed sampling matrix entries. moreover jellyfish adopts block matrix-partitioning scheme special sampling order allow parallel implementation algorithm multiple cores achieve speed-up nearly proportional number cores. gemulla adopted similar strategies stochastic parallel implementation. mackey introduced divide-and-conquer framework matrix factorization. framework ﬁrst divides large input matrix smaller submatrices factorizes submatrices parallel using existing matrix factorization algorithms ﬁnally combines solutions subproblems using random matrix approximation techniques. provided theoretical analysis recovery probability paralleled algorithm compared batch version. obtain marginalized likelihood need integrate prior distribution speciﬁed. ppca adopts zero mean unit covariance gaussian distribution prior. derivation marginal likelihood reads columns rm×r given eigenvectors covariance matrix corresponding largest eigenvalues λ··· diagonal matrix diagonal elements λ··· arbitrary orthogonal matrix. result column space identical subspace spanned principal components derived classical pca. noise variance given inverse variance converges large value inference implies variance small consequently dimension spanned automatically switched off. empirical bayesian approach used also known automatic relevance determination machine learning literature well-known sparse learning model relevance vector machines also adopts similar technique optimize hyperparameters. simple derivation maximum posteriori estimate model turns solution γa/β γb/β. gives probabilistic interpretation mmmf. regularization mmmf corresponds imposing gaussian priors advantage probabilistic modeling regularization parameters need predeﬁned. automatically determined treating variables introducing priors estimating data later salakhutdinov mnih proposed full bayesian method solved model markov chain monte carlo sampling. babacan proposed bayesian method solve rpca. assumes model sparse matrix used model outliers dense matrix used model noise. suppose i.i.d. gaussian noise following probabilistic matrix factorization number columns necessarily determine rank serves upper bound. inference true rank determined automatically hierarchical modeling hyperparameters plays important role automatic determination rank. inference converge extremely large values resulting corresponding columns close zero. automatic switch-off columns driven data determine ﬁnal rank another category methods estimate low-rank matrix explicit rank constraint. make constraint satisﬁed optimization methods often greedy strategy projects intermediate results feasible rank constraint. conceptually methods explicit rank constraint numerically implement low-rank projection step using factorization methods. cial case algorithm named singular value projection proposed solve uses projected gradient descent scheme alternates updating gradient descent projecting intermediate result rank-r matrices. matrix approximation theorem projection done calculating matrix keeping largest singular values. procedure similar proximal gradient method nuclear norm minimization replacing low-rank projection. soft thresholding singular values applied hard thresholding singular values used svp. therefore analogous iterative hard thresholding algorithm sparse coding admira algorithm introduced also intends solve problem instead using hard thresholding admira extends cosamp algorithm compressive sensing matrix case. matching pursuit-like scheme used stepwise select basis vectors reconstruct column space minimize function sparcs algorithm proposed regarded counterpart admira solve robust matrix factorization problem. godec algorithm proposed uses iterative hard threshminimize godec alternates low-rank projection estimate hard thresholding estimate avoid computation godec uses bilateral random projection scheme compute low-rank projection. huge number solvers developed low-rank matrix recovery past decade. table gives inexhaustive list them. section would like numerically illustrate characteristics. paper focus comprehensive comparison test solvers synthesized datasets. note performance algorithm often depends many factors problem size rank underlying low-rank matrix distribution singular values density missing entries outliers noise level even shape matrix. results presented provide brief demonstration under typical settings complete. detailed comparisons refer readers experiment sections algorithm papers report codes produce results presented paper publicly available https//sites.google.com/site/lowrankmodeling/. welcome readers modify codes test algorithms applications. represent algorithm estimate true low-rank matrix respectively. results depend stopping conditions algorithms different algorithms adopt different stopping criteria compare algorithms merely terms relative error ﬁnal estimates. instead plot curve relative error algorithm function time quickly closely algorithm estimate approach ground truth algorithm runs. note curves show convergence rates algorithms since relative distance calculated ground truth instead stationary point cost function. curve averaged randomly-generated instances problem setting. datasets synthesized following low-rank component matrix generated random matrices independently sampled normal distribution then normalized make kx∗k matrix completion locations missing values deﬁned binary matrix entries independently sampled binomial distribution rpca locations outlier entries distribution rithm solving model noisy cases. also tested following matrix factorization methods lmafit optspace grouse lrgeomcg rpca tested convex models noiseless cases spcp noisy cases. addition tested godec prmf comparison. convex programs implemented authors matlab. implemented inexact version adopted varying penalty parameter scheme provided boyd integrated adaptive restart technique introduced practically improve convergence apg. original works partial often used accelerate computation large-scale problems implementation simply used builtsvd function matlab since observed efﬁciency comparable solved original paper here implemented block coordinate descent algorithm solve spcp i.e. alternately updated updated soft thresholding convergence. experience efﬁciency least comparable solve model much simpler implementation. algorithms used matlab packages downloaded authors’ websites. followed default parameter settings original papers tuned reasonably good results speciﬁc problems. algorithms require rank noise level number outlier entries input. simplify comparison provided true values help parameter tuning. initial guess rank-r approximation input matrix using algorithms. figure shows basic case sufﬁcient number entries observed noise exists. values curves decrease small numbers below indicates algorithms recover underlying matrix high precision. convex algorithm comparatively slower since needs perform computation iteration. notice convex model parameter-free recovers low-rank matrix accurately. matrix factorization methods generally accurate fast. manifold-based algorithm lrgeomcg achieves fastest performance followed lmafit. online algorithm grouse also converges accurate solution passing data multiple times. probabilistic method shows competitive performance requires parameter tuning. problem setting figure similar setting figure except rank increased curves figure similar shapes compared figure recovery accuracy algorithms remains high unchanged. however curves factorization-based methods less shift right indicating increase computational time. fig. comparison algorithms matrix completion. curve represents relative distance between algorithm estimate true low-rank matrix function time scale. problem size ﬁxed denote over-sampling ratio true rank noise level respectively. figure shows case gaussian noise added. curves methods converge values larger zero existence random noise. factorization-based methods achieve similar accuracy relative error convex algorithm higher. attributed fact that convex relaxation guarantee optimality optimization introduce bias model. speciﬁcally used solve convex model shrink singular values recovered matrix removing noise components. consequently values recovered matrix shrink towards zero especially noise level large. compensate bias postprocessing techniques could used proven effective figure shows case sampling rate decreased compared figure curves shift right indicating increase computational time means convergence rates algorithms inﬂuenced over-sampling ratio. besides methods still obtain accurate recovery relatively over-sampling ratio. fig. comparison algorithms rpca. curve represents relative distance algorithm estimate true low-rank matrix function time scale. problem size ﬁxed denote proportion outlier entries true rank noise level respectively. convex program achieves high accuracy noiseless cases without knowing true rank. noisy case stable version spcp tested obtain best accuracy shrinkage effect nuclear norm minimization mentioned previous subsection. probabilistic method prmf achieves similar performance pcp. another probabilistic method using variational bayes inference achieves best overall performance terms speed accuracy requires parameter tuning. projection-based method godec performs well ﬁrst cases performance drops difﬁcult cases figure curves ﬂattened high values. many objects interest image analysis modeled low-rank matrices images convex lambertian surface various illuminations dynamic textures changing periodically active contours similar shapes multiple feafig. using rpca remove shadows specularities face images. rows bottom correspond original images low-rank components sparse components respectively. applied algorithm face images person selected images show results. note displayed intensities image scaled face image courtesy extended yale face database ture tracks rigid moving object intuitively lowdimensional subspace models common patterns underlying data. hence recovering low-rank structure critical many applications background subtraction face recognition segmentation. below introduce typical applications based models discussed previous sections. concept dimensionality used face recognition decades since work sirovich kirby applied face images construct face space face image characterized low-dimensional vector later turk pentland introduced eigenface method face recognition. basic steps using eigenfaces face recognition include generating eigenfaces computing ﬁrst eigenvectors matrix composed training images; calculating weight vector input image projecting image onto space spanned eigenfaces; determining whether input image face image person image belongs according projection error weight vector. earliest example using low-rank modeling face recognition. face images real datasets usually corrupted various artifacts shadows specularities occlusions cannot handled classical pca. therefore many approaches based rpca proposed process face images illustrated figure local defects face images could removed sparse component correct description person’s face could obtained low-rank component. procedure improve characterization faces boost performance recognition algorithms fig. using rpca background subtraction. rows bottom correspond input images low-rank components sparse components respectively. applied algorithm subway station dataset dataset surveillance video subway station including moving escalators background. selected frames perform background subtraction frames show results ﬁgure. background subtraction involves modeling background video detecting objects stand background. similar eigenfaces applied model background since work eigenbackground subtraction basic idea underlying background images video captured static camera unchanged except illumination variation. therefore matrix composed vectorized background images naturally modeled low-rank matrix. however training images without foreground objects required generate clean background model traditional methods. estimate background model presence foreground objects rpca desired illustrated algorithm recover background images low-rank component identify foreground objects sparse component. figure gives illustration. notice background includes three moving escalators clearly reconstructed low-rank component. shows appealing capability low-rank modeling background subtraction. achieve better accuracy object detection spatiallycontiguous property foreground pixels modeled integrated rpca using markov random fields smoothing techniques similarly rpca framework used segment point trajectories video groups correspond background foreground respectively segmentation based fact background motion caused camera motion lowdimensional subspace. low-rank representation well known method subspace clustering. subspace clustering data points assumed embedded several lowdimensional subspaces task subspaces membership data point subspaces. popular method spectral clustering clustering achieved partitioning graph whose edge weight represents afﬁnity data points. data point represented linear combination neighbors within subspace coefﬁcient estimated encourages column-wise sparsity outlier term shown that data points several orthogonal subspaces derived block-diagonal intrinsically identiﬁes afﬁnity data points block-diagonal structure indicates clusters data. thus provides favorable afﬁnity matrix perform spectral clustering. similar idea also applied image segmentation binations dictionary atoms. sparse. claimed also low-rank learn discriminative dictionary. intuition that constructed dictionary discriminative signals label represented atoms consequently coefﬁcient matrix blockdiagonal matrix columns ordered class labels. impose structural constraint sparsity rank minimized simultaneously. image alignment refers problem transforming different images coordinate system. peng proposed solve problem rank minimization based assumption batch aligned images form low-rank matrix. parameters transformation estimated solving similarly zhang used model generate transforminvariant low-rank textures difference compared tilt represents single image instead image sequence. assumption tilt rectiﬁed images textures characters codes urban scenes usually symmetric patterns consequently form low-rank matrices. reconstructed low-rank texture used many applications camera calibration reconstruction character recognition etc. low-rank matrix factorization widely used analyze tracks feature points video since seminal work tomasi kanade observation measurement matrix composed feature tracks ranklimited rank depends type camera model denotes image coordinates point frame afﬁne motion matrix frame coordinates point often called measurement matrix motion matrix structure matrix respectively since smallest dimension low-rank factorization measurement matrix solves problem structure motion nonrigid object shape changes frame frame valid frame separately undetermined system. make problem well posed prior knowledge shapes required. low-rank prior widely adopted many state-of-the-art methods nonrigid assumes shapes reconstructed linear combinations limited number basis shapes. exemplar works include another related application motion segmentation feature tracks multiple moving objects instead single rigid object. task segment feature tracks different groups group tracks belong single moving object. discussed above group tracks form rank- subspace. therefore motion segmentation problem formulated subspace clustering i.e. dividing feature tracks multiple clusters cluster forming low-dimensional subspace. detailed introduction subspace clustering please refer popular application matrix completion image restoration. many scenarios desired reconstruct lost corrupted parts image might caused texts logos superposed image. process named image restoration inpainting natural image approximately low-rank problem restoring corrupted pixels formulated matrix completion problem. figure illustration using matrix completion restore image randomly sampled pixels text-occluded image. liang used sophisticated model texture recovered modeled low-rank sparse certain transformed domain. moreover assumed corrupted regions might unknown used sparse error term fig. matrix completion image restoration. recovered image. age. soft-impute algorithm applied. original http//en.wikipedia.org/wiki/filelenna.png. model detect corrupted regions. alleviate shrinkage signals works adopted nonconvex methods rank minimization instead using nuclear norm proposed method video restoration. unreliable pixels video ﬁrst detected labeled missing. then image patches grouped patches group share similar underlying structure approximately form low-rank matrix. finally matrix completion carried patch group restore images. similarly low-rank assumption often used model coherence multiple images noise removal medical image analysis. denoising magnetic resonance images example image sequence usually consists multiple echo images frames dynamic imaging different diffusion-weighted images although images different desired signals images supposed correlated consequently reconstructed several signiﬁcant principal components. remaining components correspond random noise removed. recently candes used operator instead classical achieve robust results image denoising. importantly shown optimal threshold obtained theoretically based stein’s unbiased risk estimate brings great convenience practical applications. active shape model proposed increase robustness deformable models image segmentation. constructs statistical shape space large given shapes constrains candidate shape shape space. active shape model candidate shape represented variations training data vector coefﬁcients represent candidate shape shape space. then determined ﬁtting parametric curve features image. since number columns often small candidate shape conﬁned low-dimensional space. therefore active shape model intrinsically admits low-rank assumption population shapes. moreing upon active shape model image segmentation include alternative approach making low-rank assumption image segmentation impose groupsimilarity constraint multiple shapes nuclear norm minimization require training shapes. image reconstruction based low-rank modeling drawn much attention medical imaging community. idea make temporal coherence dynamic imaging reduce required number sampling. imaging example liang proposed concept partial separability model spatialtemporal image basic model extended integrate sparse properties speciﬁc domains. example image intensity often sparse representation wavelets limited total variation meanwhile temporal component usually periodic bandlimited results sparsity fourier domain low-rank property modeled regionally dependent exploited locally instead using model works proposed reconstruct data nuclear norm minimization. otazo used rpca model separate background dynamic components imaging. lingala jacob developed dictionary learning-based framework dynamic imaging. low-rank modeling also applied reconstruct multi-channel data single k-space data imaging data modalities computed tomography positron emission tomography recent examples low-rank modeling-based applications also include object tracking saliency detection correspondence estimation fac¸ade parsing model fusion depth image enhancement name few. paper introduced concept low-rank modeling reviewed representative low-rank models algorithms applications image analysis. additional reading theories algorithms applications readers referred online documents matrix factorization jungle sparse low-rank approximation wiki updated regular basis. convex programming-based methods low-rank matrix recovery generally achieve stable performance wide range scenarios global optimality optimization. noiseless cases convex programs exactly recover underlying low-rank matrix theoretical guarantee noisy cases nuclear norm minimization shrink true signals compressing noise. compensate shrinkage effect postprocessing steps used works tried alleviate issue going beyond nuclear norm using nonconvex relaxation techniques limitation convex methods requirement repeated computation time consuming unaffordable large-scale problems. many efforts made towards fast computation partial approximate performing without computational efﬁciency still issue many real applications. factorization-based methods widely used real applications mostly computational convenience. inference large matrix reduced estimation smaller factor matrices. moreover cost function matrix factorization often decomposable separate functions data points variables. therefore convenient develop online algorithms real-time processing design distributed algorithms solving large-scale problems. limitation matrix factorization underlying rank needs predeﬁned many models. rank estimation techniques proposed works rank estimation still challenging problem especially noisy cases. probabilistic methods shown great potential simulation real application moreover probabilistic models bayesian treatment often parameter free important real applications. applications low-rank modeling based fact linear correlation often exists among data. prior knowledge used many purposes extracting common patterns removing random noise reducing sampling rates imaging etc. recent advances sparse learning optimization provide powerful frameworks techniques conveniently model low-rank property data develop efﬁcient algorithms. expect applications low-rank modeling near future. balzano nowak recht. online identiﬁcation tracking subspaces highly incomplete information. proceedings annual allerton conference communication control computing buchanan fitzgibbon. damped newton algorithms matrix factorization missing data. proceedings ieee conference computer vision pattern recognition cabral torre costeira bernardino. unifying nuclear norm bilinear factorization approaches low-rank matrix decomposition. proceedings international conference computer vision jiang shen zhao. cine cone beam reconstruction using low-rank matrix factorization algorithm proof-of-princple study. ieee transactions medical imaging christodoulou babacan liang. accelerating cardiovascular imaging exploiting regional low-rank structure group sparsity. proceedings ieee international symposium biomedical imaging cremers. dynamical statistical shape priors level set-based tracking. ieee transactions pattern analysis machine intelligence huang zhang metaxas. background subtraction using rank group sparsity constraints. proceedings european conference computer vision eriksson hengel. efﬁcient computation robust low-rank matrix approximations presence missing data using norm. proceedings ieee conference computer vision pattern recognition georghiades belhumeur kriegman. many illumination cone models face recognition variable lighting pose. ieee transactions pattern analysis machine intelligence haldar liang. spatiotemporal imaging partially separable functions matrix recovery approach. proceedings ieee international symposium biomedical imaging balzano szlam. incremental gradient grassmannian online foreground background separation subsampled video. proceedings ieee conference computer vision pattern recognition jolliffe. principal component analysis. springer verlag kanade. robust norm factorization presence outliers missing data alternative convex programming. proceedings ieee conference computer vision pattern recognition keshavan montanari low-rank matrix completion noisy observations quantitative comparison. proceedings annual allerton conference communication control computing babacan haldar schuff liang. denoising diffusion-weighted magnitude image sequences using rank edge constraints. proceedings ieee international symposium biomedical imaging leventon grimson faugeras. statistical shape inﬂuence geodesic active contours. proceedings ieee conference computer vision pattern recognition mackay. probable networks plausible predictions-a review practical bayesian methods supervised neural networks. network computation neural systems nesterov. gradient methods minimizing composite objective function. technical report universit´e catholique louvain center operations research econometrics nguyen peng liang. spatiotemporal denoising spectroscopic imaging data low-rank approximations. proceedings ieee international symposium biomedical imaging okatani yoshida deguchi. efﬁcient algorithm low-rank matrix factorization missing components performance comparison latest algorithms. proceedings international conference computer vision oliver rosario pentland. bayesian computer vision system modeling human interactions. ieee transactions pattern analysis machine intelligence otazo sodickson cand`es. low-rank+ sparse reconstruction accelerated dynamic seperation background dynamic components. proceedings spie optical engineering applications peng ganesh wright rasl robust alignment sparse low-rank decomposition linearly correlated images. ieee transactions pattern analysis machine intelligence rahmim tang zaidi. four-dimensional image reconstruction strategies dynamic beyond conventional independent frame reconstruction. medical physics salakhutdinov mnih. bayesian probabilistic matrix factorization using markov chain monte carlo. proceedings international conference machine learning shin larson ohliger elad pauly vigneron lustig. calibrationless parallel imaging reconstruction based structured lowrank matrix completion. magnetic resonance medicine porikli ahuja. robust orthonormal subspace learning efﬁcient recovery corrupted low-rank matrices. proceedings ieee conference computer vision pattern recognition h.-y. shum ikeuchi reddy. principal component analysis missing data application polyhedral object modeling. ieee transactions pattern analysis machine intelligence tomasi kanade. shape motion image streams orthography factorization method. international journal computer vision tomioka suzuki sugiyama kashima. fast augmented lagrangian algorithm learning low-rank matrices. proceedings international conference machine learning vidal. subspace clustering. ieee signal processing magzine vidal hartley. motion segmentation missing data using powerfactorization gpca. proceedings ieee conference computer vision pattern recognition wang d.-y. yeung. bayesian robust matrix factorization image video processing. proceedings international conference computer vision wang wang d.-y. yeung. probabilistic approach robust matrix waters sankaranarayanan baraniuk. sparcs recovering lowrank sparse matrices compressive measurements. advances neural information processing systems zhang. solving low-rank factorization model matrix completion nonlinear successive over-relaxation algorithm. mathematical programming computation wright ganesh compressive principal component pursuit. proceedings ieee international symposium information theory xiao chai kanade. closed-form solution non-rigid shape motion zhang z.-h. huang zhang. restricted p-isometry properties nonconvex matrix recovery. ieee transactions information theory zhang ghanem ahuja. low-rank sparse learning robust visual zhang jiang davis. learning structured low-rank representations image classiﬁcation. proceedings ieee conference computer vision pattern recognition zhou yang moving object detection detecting contiguous outliers low-rank representation. ieee transactions pattern analysis machine intelligence zhou wright candes stable principal component pursuit. proceedings ieee international symposium information theory papademetris sinusas duncan. segmentation left ventricle cardiac images using subject-speciﬁc dynamical model. ieee transactions medical imaging", "year": 2014}