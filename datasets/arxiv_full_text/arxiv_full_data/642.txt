{"title": "Hadamard Product for Low-rank Bilinear Pooling", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property.", "text": "byoung-tak zhang school computer science engineering interdisciplinary program cognitive science seoul national university surromind robotics seoul republic korea btzhangbi.snu.ac.kr bilinear models provide rich representations compared linear models. applied various visual tasks object recognition segmentation visual question-answering state-of-the-art performances taking advantage expanded representations. however bilinear representations tend high-dimensional limiting applicability computationally complex tasks. propose low-rank bilinear pooling using hadamard product efﬁcient attention mechanism multimodal learning. show model outperforms compact bilinear pooling visual question-answering tasks state-of-the-art results dataset better parsimonious property. bilinear models provide richer representations linear models. exploit advantage fully-connected layers neural networks replaced bilinear pooling. outer product vectors involved bilinear pooling result this pairwise interactions among given features considered. recently successful application technique used ﬁne-grained visual recognition however bilinear pooling produces high-dimensional feature quadratic expansion constrain model structure computational resources. example outer product feature vectors k-dimensionality produces million-dimensional feature vector. therefore classiﬁcation problems choice number target classes severely constrained number parameters standard linear classiﬁer determined multiplication size high-dimensional feature vector number target classes. compact bilinear pooling reduces quadratic expansion dimensionality orders magnitude retaining performance full bilinear pooling. approximation uses sampling-based computation tensor sketch projection utilizes useful property means projection outer product vectors convolution projected vectors. here proposed projection function randomly sampled parameters algorithm. nevertheless compact bilinear pooling embraces shortcomings. comes sampling approach. compact bilinear pooling relies favorable property provides basis projected features instead original features. calculating exact expectation computationally intractable random parameters ﬁxed training evaluation. practical choice leads second. projected dimension compact bilinear pooling large enough minimize bias ﬁxed parameters. practical choices -dimensional inputs respectively though compacted dimensions reduced ones orders magnitude compared full bilinear pooling high-dimensional features could bottleneck computationally complex models. propose low-rank bilinear pooling using hadamard product commonly used various scientiﬁc computing frameworks tensor operations. proposed method factors three-dimensional weight tensor bilinear pooling three twodimensional weight matrices enforces rank weight tensor low-rank. result input feature vectors linearly projected weight matrices respectively computed hadamard product then followed linear projection using third weight matrix. denotes hadamard example projected vector represented product. also explore non-linearity using non-linear activation functions low-rank bilinear pooling shortcut connections inspired deep residual learning then show becomes simple baseline model one-learning block multimodal residual networks low-rank bilinear model interpretation done. contributions follows first propose low-rank bilinear pooling approximate full bilinear pooling substitute compact bilinear pooling. second multimodal low-rank bilinear attention networks efﬁcient attention mechanism using low-rank bilinear pooling proposed visual question-answering tasks. achieves state-of-the-art performance better parsimonious property. finally ablation studies explore alternative choices e.g. network depth non-linear functions shortcut connections conducted. pirsiavash suggest low-rank bilinear method reduce rank weight matrix less number parameters regularization. rewrite weight matrix rn×d rm×d imposes restriction rank uivt min. based idea rewritten follows uivt denotes column vector ones denotes hadamard product. still need third-order tensors feature vector whose elements {fi}. reduce order weight tensors replace rd×c then redeﬁne rn×d rm×d projected feature vector then low-rank bilinear model equation implemented using linear mappings without biases embedding input vectors hadamard product learn joint representations multiplicative linear mapping bias project joint representations output vector given output dimension. then structure pooling method deep neural networks. discuss possible variations low-rank bilinear pooling based model inspired studies neural networks. equation linear projections bias vectors. result linear models input vectors integrated additive form called full model linear regression statistics applying non-linear activation functions help increase representative capacity model. ﬁrst candidate apply non-linear activation functions right linear mappings input vectors. denotes arbitrary non-linear activation function maps real values ﬁnite interval e.g. sigmoid tanh. inputs come different modalities statistics inputs quite different other result interference. since gradient respect input directly dependent input hadamard product inputs. additional applying activation function hadamard product appropriate since activation functions doubly appear calculating gradients. however applying activation function hadamard product would alternative choice follows note using activation function low-rank bilinear pooling found implementation simple baseline dataset without interpretation low-rank bilinear pooling. however notably studied learning behavior multiplicative integration rnns discussions empirical evidences. apply previous techniques full model non-linear activation linear models inputs nested non-linear activation functions. avoid unfortunate situation shortcut connections explored residual learning shortcut mappings. linear projection shortcut mappings linear mappings. notice formulation generalized form one-block layered though shortcut connections used proposed model explained section section apply low-rank bilinear pooling propose efﬁcient attention mechanism visual question-answering tasks based interpretation previous section. assumed inputs question embedding vector visual feature vectors lattice space. low-rank bilinear pooling attention mechanism attention mechanism uses attention probability distribution lattice space. here using low-rank bilinear pooling deﬁned rg×s rd×g hyperbolic tangent function rn×d rm×d rs×m multiple glimpses explicitly expressed fukui conceptually similar jaderberg softmax function applies vector bias terms omitted simplicity. section conduct experiments select proposed model multimodal low-rank bilinear attention networks experiment controls factors except factor assess effect accuracies. based start assessments initial option shortcut connections called multimodal attention residual networks notice embeddings visual feature better performance based preliminary experiment attribute choice attention mechanism visual features provides capacity learn visual features. hyper-parameters without explicit mention this. dataset used primary dataset data augmentation question-answering annotations visual genome used. validation performed test-dev split model comparison based results test-standard split. comprehensive reviews tasks please refer kaﬂe kanan details preprocessing question vision embedding hyperparameters used experiments described appendix source code experiments available github repository. number learning blocks argue three-block layered shows best performance among four-block layered models taking advantage residual learning. however speculate introduction attention mechanism makes deep networks hard optimize. therefore explore number learning blocks marn attention mechanism using low-rank bilinear pooling. table accuracies experimental model multimodal attention residual networks respect number learning blocks number glimpse position activation functions answer sampling shortcut connections data augmentation using visual genome dataset test-dev split open-ended task. note proposed model multimodal low-rank bilinear attention networks shortcut connections compared marn. model model name size number parameters overall accuracy percentage yes/no numbers others. since fukui report accuracy ensemble model test-standard test-dev results single models included last sector. ﬁgures different precisions rounded. indicates selected model experiment. model non-linearity assess three options applying non-linearity low-rank bilinear pooling vanilla hadamard product equation hadamard product equation answer sampling dataset answers unique persons question visual genome dataset single answer question. since difﬁcult ambiguous questions divided answers probabilistic sampling distribution answers utilized optimize multiple answers. instance found fukui simplify procedure follows |ai| denotes number unique answer multiple answers denotes mode frequent answer denotes secondly frequent answer. deﬁne divided answers least three answers secondly frequent evaluation metric rate divided answers approximately questions divided answers dataset. assume eases difﬁculty convergence without severe degradation performance. shortcut connection contribution shortcut connections residual learning explored based observation competitive performance single-block layered model. since usefulness shortcut connections linked network depth data augmentation data augmentation visual genome question answer annotations explored. visual genome originally provides million visual question answer annotations. aligning valid number question-answering pairs training distinct images. number learning blocks though three-block layered architecture marn shows best performance two-block layered models multiple glimpse models next experiment choose one-block layered model simplicity extend competitive performance number glimpses compared results fukui four-glimpse marn better comparative models. however parsimonious choice two-glimpse marn chosen later experiments. speculate multiple glimpses factors competitive performance based large margin accuracy compared one-glimpse marn non-linearity results conﬁrm activation functions useful improve performances. surprisingly empirical difference options before-hadamard product answer sampling sampled answers result better performance mode answers conﬁrms distribution answers annotators used improve performance. however number multiple answers usually limited cost data collection. shortcut connection though effectively uses shortcut connections improve model performance one-block layered marn shows better performance without shortcut connection. words residual learning used proposed model mlb. seems trade-off introducing attention mechanism residual learning. leave careful study trade-off future work. data augmentation data augmentation using visual genome question answer annotations signiﬁcantly improves performance accuracy test-dev split. especially accuracy others -type answers notably improved data augmentation. comparison single models test-standard shown table overall accuracy model approximately next best model open-ended task vqa. major improvements yes-or-no others type answers. table also report accuracy ensemble model compare ensemble models test-standard places challenge beat previous state-of-the-art margin table test-standard results ensemble models compare state-of-the-art. unpublished entries team names used instead model names. ﬁgures updated challenge. proposes multimodal residual learning hadamard product low-rank bilinear pooling. however utilization low-rank bilinear pooling limited joint residual mapping function multimodal residual learning. higher-order boltzmann machines hadamard product capture interactions input output hidden representations energy function. propose recurrent neural networks using hadamard product integrate multiplicative interactions among hidden representations model. details related works please refer appendix denotes fast fourier transform denotes output dimension inputs random variables. sampled sampled then random variables ﬁxed usage. even dimensions different other used multimodal learning wijk sijkwijk sijk sampled wijk sampled pid} compact bilinear pooling followed fully connected layer r|ω|×d. then method formulated hashing trick share randomly chosen bilinear weights using parameters output value single parameter shared bilinear terms expectation variance comparison method method approximates three-dimensional weight tensor bilinear pooling two-dimensional matrix larger concatenation three two-dimensional matrices low-rank bilinear pooling. ratio number parameters single output total number parameters outputs d/d|ω| /|ω| since method uses three-way factorization. hence parameters allocated bilinear approximation compact bilinear pooling does effectively managing overall parameters guided back-propagation algorithm. uses compact bilinear pooling multimodal tasks needs dimension output reduce bias induced ﬁxed random variables result majority model parameters concentrated last fully connected layer makes fan-out structure. total number parameters highly sensitive number classes approximately mcb+att mcb+att+glove. total number parameters proposed model robust number classes similar role model architecture. suggest low-rank bilinear pooling method replace compact bilinear pooling fan-out structure needs complex computations. low-rank bilinear pooling ﬂexible structure using linear mapping hadamard product better parsimonious property compared compact bilinear pooling. achieve state-of-the-art results dataset using similar architecture fukui replacing compact bilinear pooling low-rank bilinear pooling. believe method could applicable bilinear learning tasks. authors would like thank patrick emaase helpful comments editing. also thankful anonymous reviewers provided comments improve paper. work supported naver labs corp. naver corp. partly korea government part computing resources used study generously shared standigm inc. stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. ieee international conference computer vision wenlin chen james wilson stephen tyree kilian weinberger yixin chen. compressing neural networks hashing trick. international conference machine learning kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing akira fukui dong park daylen yang anna rohrbach trevor darrell marcus rohrbach. multimodal compact bilinear pooling visual question answering visual grounding. arxiv preprint arxiv. ryan kiros yukun ruslan salakhutdinov richard zemel antonio torralba raquel urtasun sanja fidler. skip-thought vectors. advances neural information processing systems ranjay krishna yuke oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li-jia david shamma michael bernstein fei-fei. visual genome connecting language vision using crowdsourced dense image annotations. arxiv preprint arxiv. image question answering using convolutional neural network dynamic parameter prediction. ieee conference computer vision pattern recognition kilian weinberger anirban dasgupta john langford alex smola josh attenberg. feature hashing large scale multitask learning. international conference machine learning peng wang chunhua shen anthony dick anton hengel. anything free-form visual question answering based knowledge external sources. ieee conference computer vision pattern recognition questions k-most frequent answers used. vocabulary size questions used question embedding. based earlier studies word embedding matrix initialized skip-thought vector pre-trained model result question vectors dimensions. hyperparameters used table described table batch size number iterations ﬁxed data augmented models simpliﬁed early stopping used starting k-iteration every iterations avoid exhaustive submissions test-dev evaluation server. rmsprop used optimization. attention lattice size question embedding size channel size extracted visual features joint embedding size number glimpses number candidate answers learning rate learning rate decay factor every iteration dropout rate gradient clipping threshold model schema figure shows schematic diagram denotes hadamard product denotes linear combination visual feature vectors using coefﬁcients output softmax function. softmax function applied vectors output matrix concatenate resulting vectors linear combinations figure schematic diagram mlb. replicate module copies question embedding vector match visual feature vectors. conv modules indicate convolution transform given channel space computationally equivalent linear projection channels. section algorithm multimodal compact bilinear pooling described kind hashing tick given inputs output. random variables uniformly sampled uniformly sampled then count sketch projection function projects intermediate representations deﬁned probability given hence expected number bilinear terms ψiψi since output result circular convolution expected number bilinear terms likewise probability bilinear term allocated probability distribution number bilinear terms follows multinomial distribution whose mean variance linear projection multimodal compact bilinear pooling provides weights bilinear terms shared weight assigned bilinear terms expectation though bilinear term different sign induced hashednets propose method compress neural networks using low-cost hashing function function randomly group portion connections neural networks share single weight. speculate multimodal compact bilinear pooling uses hashing tick reduce number full bilinear weights rate however approximation limited two-way interaction compared three-way factorization method. explicit comparison compact bilinear pooling explicitly substitute compact bilinear pooling low-rank bilinear pooling control everything else means rest model architecture exactly same. according fukui followed signed square root l-normalization dropout linear projection -dimension target dimension. also dropout question embedding vector. note overall architecture multimodal learning same. experimental details referenced implementation fukui test-dev split version gets overall accuracy additionally nonlinearity getting attention distributions increased original using relu overall accuracy still performance parameter matrices number learning blocks however explicit attention mechanism allows lower-level visual features fully-connected layers importantly spatially selective learning. recent state-of-the-art methods variant explicit attention mechanism models note shortcut connections used proposed multimodal low-rank bilinear model. since performance gain stacking multiple layers mlb. leave study residual learning future work leverage excellency bilinear models suggested similar model found study higher-order boltzmann machines suggest factoring method three-way energy function capture correlations among input output hidden representations.", "year": 2016}