{"title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequenceto-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size.", "text": "fandong meng∗ zhengdong zhaopeng hang institute computing technology chinese academy sciences {mengfandong liuqun}ict.ac.cn noah’s huawei technologies {lu.zhengdong tu.zhaopeng hangli.hl}huawei.com propose deepmemory novel deep architecture sequence-to-sequence learning performs task series nonlinear transformations representation input sequence ﬁnal output sequence inspired recently proposed neural turing machine store intermediate representations stacked layers memories read-write operations memories realize nonlinear transformations representations. types transformations designed advance parameters learned data. layer-by-layer transformations deepmemory model complicated relations sequences necessary applications machine translation distant languages. architecture trained normal back-propagation sequenceto-sequence data learning easily scaled large corpus. deepmemory broad enough subsume state-of-the-art neural translation model special case signiﬁcantly improving upon model deeper architecture. remarkably deepmemory purely neural network-based achieve performance comparable traditional phrase-based machine translation system moses small vocabulary modest parameter size. sequence-to-sequence learning fundamental problem natural language processing many important applications machine translation part-of-speech tagging dependency parsing recently signiﬁcant progress development technologies task using purely neural network-based models. without loss generality consider machine translation paper. previous efforts neural machine translation generally fall categories encoder-decoder illustrated left panel figure models type ﬁrst summarize source sentence ﬁxed-length vector encoder typically implemented recurrent neural network convolutional neural network unfold vector target sentence decoder typically implemented attention-model rnnsearch representative represents source sentence sequence vectors simultaneously conducts dynamic alignment gating neural network generation target sentence another illustrated right panel figure empirical comparison approaches indicates attention-model efﬁcient encoder-decoder approach achieve comparable results less parameters training instances superiority efﬁciency comes mainly mechanism dynamic alignment avoids need represent entire source sentence ﬁxed-length vector encoder-decoders attention models reformalized language neural turing machines replacing different forms representations content memories operations basic neural net-controlled read-write actions illustrated left panel figure clear realizing attention mechanism essentially special case reading memory contains representation source sentence. importantly view whole process becomes transforming source sentence putting memory reading memory transform target sentence. architecture intrinsically shallow terms transformations sequence object essentially hidden layer illustrated left panel figure note although inﬁnitely deep depth merely dealing temporal structure within sequence. hand many sequence-to-sequence tasks translation intrinsically complex calls complex powerful transformation mechanism encoder-decoder attention models. reason propose novel deep memory-based architecture named deepmemory sequenceto-sequence learning. shown right panel figure deepmemory carries task series non-linear transformations input sequence different levels intermediate memory-based representations eventually ﬁnal output sequence. deepmemory essentially customized deep version multiple stages operations controlled program choices layers types read/write operations layers tailored particular task. layer-by-layer stacking transformations memory-based representations deepmemory generalizes notion inter-layer nonlinear mapping neural networks therefore introduces powerful deep architecture sequence-to-sequence learning. deepmemory learn representation sequence better suited task layer-by-layer transformations. deep neural network expect stacking relatively simple transformations greatly enhance expressing power efﬁciency deepmemory especially handling translation languages vastly different nature sentences complicated structures. deepmemory naturally subsumes current neural machine translation models special cases importantly accommodates many deeper alternatives modeling power empirically superior current shallow architectures machine translation tasks. although deepmemory initially proposed machine translation adapted tasks require substantial transformations sequences including paraphrasing reasoning semantic parsing also deﬁning layer-by-layer transformations beyond read-write operations proposed design differentiable operations speciﬁc structures task roadmap ﬁrst discuss section read-write operations form non-linear transformation building block deepmemory. section stack transformations together full deepmemory architecture discuss several architectural variations section report empirical study deepmemory chinese-english translation task. start discussing read-write operations pieces memory generalized form nonlinear transformation. illustrated figure transformation non-overlapping memories namely r-memory -memory -memory initially blank. controller operates read-heads values r-memory sent write-head modifying values speciﬁc locations -memory operations completed content r-memory considered transformed written -memory. operations therefore deﬁne transformation representation another pictorially noted right panel figure basic components formally deﬁned below following generic however important modiﬁcations nesting architecture implementation efﬁciency description simplicity. memory memory generally deﬁned matrix potentially inﬁnite size limit ourselves pre-determined matrix memory locations values location. implementation deepmemory always instance-dependent pre-determined algorithm. memories different layers generally different suppose particular instance system reads r-memory writes -memory read/write heads read-head gets values corresponding memory following instructions controller also inﬂuences controller feeding state-machine. deepmemory allows multiple read-heads controller potentially different addressing strategies write-head simply takes instruction controller modiﬁes values speciﬁc locations. controller core controller state machine implemented long short-term memory state time denoted controller determines reading writing time return reading turn takes part updating state. simplicity reading writing allowed time step read-heads allowed. main equations controllers respectively operators dynamics reading writing parameterized deepmemory allowed read memory lower layer write memory higher layer reading memory performed ﬁnishing writing read-write operations transform representation r-memory representation memory design choice read-write speciﬁes inner structure memories -memory. transformation therefore jointly speciﬁed read-write strategies parameters learned supervised fashion memory designed inner structure particular case vector array instance speciﬁc length offers ﬂexibility ﬁxed-length vector representing sequences. representational ﬂexibility particularly advantageous combined proper reading-writing strategy deﬁning nonlinear transformations sequences serve building block deep architecture. addressing reading location-based addressing location-based addressing reading simply notice l-addressing state machine automatically runs clock determined spatial structure r-memory. following clock write-head operates number times. important variant suggested r-memory backwards forward reading pass controller structure parameterized differently. machine translation general attention models discussed computer vision. content-based addressing offers following advantages representation learning hybrid addressing hybrid addressing reading essentially read-heads l-addressing c-addressing. time controller simply concatenates return individual read-heads ﬁnal return worth noting h-addressing tempo state machine determined l-addressing read-head therefore creates -memory number locations writing. shown later haddressing readily extended allow read-heads work different memories. addressing writing location-based addressing l-addressing writing simple. time location -memory updated kept unchanged afterwards. locationcontent-based addressing implemented weights conventional special case l-addressing reading writing actually familiar structure units found stacked layers indeed illustrated ﬁgure right text read-write strategy invoke relatively local dependency based original spatial order r-memory. hard show recover deep model stacking layers read-write operations like this. deep architecture actually partially accounts great performance google neural machine translation model c-addressing however reading writing offers means major reordering units h-addressing spatial structure lower layer memory. paper consider four types transformations induced combinations read write addressing strategies listed pictorially figure notice include combination c-addressing writing since computationally expensive optimize combined c-addressing reading particular read-write strategy still fair amount implementation details speciﬁed omitted space limit. easily design different read/write strategies example particular h-addressing writing. illustrated figure stacking straightforward apply transformation another -memory lower layer r-memory upper layer. entire deep architecture deepmemory diagram figure therefore deﬁned accordingly. basically starts symbol sequence moves sequence word embeddings layers transformation reach ﬁnal intermeidate layer read output layer. since target sequence general different length top-layer memory takes pure c-addressing reading relies built-in mechanism generating lstm stop memory different layers could equipped different read-write strategies even strategy conﬁgurations learned parameters general different. contrast dnns transformations different layers homogeneous sensible architecture design combining nonlinear transformations greatly affect performance model however little known future research needed. cross-layer reading addition generic read-write strategies section also introduce cross-layer reading deepmemory modeling ﬂexibility. words writing layer- deepmemory allows reading layers lower instead layer-− speciﬁcally consider following cases. memory-bundle memory-bundle shown figure concatenates units aligned memories reading regardless addressing strategy. formally location bundle memory layer- layer- would since requires strict alignment tween memories together memory-bundle usually layers created spatial structure origin short-cut unlike memory-bundle short-cut allows reading layers potentially different inner structures using multiple read-heads shown figure example c-addressing read-head memory layer- l-addressing read-head layer- writing memory layer- optimization designed architecture parameters optimized include controller parameters lstm output layer word-embeddings. since reading memory done writing completes feed-forward process described scales memory lower layer memory higher layer forming memory layer controlled corresponding state machine. accordingly optimization correction signal also propagates scales within-layer scale signal back-propagates time controlled corresponding state-machine optimization correction reading writing location memory making c-addressing expensive l-addressing general involves locations memory time optimization done standard back-propagation aiming maximize likelihood target sequence. practice standard stochastic gradient descent mini-batch learning rate controlled adadelta discuss four representative special cases deepmemory arc-i novel deep architectures machine translation. also show current neural machine translation models like rnnsearch described framework deepmemory relatively shallow case. arc-i ﬁrst proposal including variants designed demonstrate effect c-addressing reading intermediate memory layers diagram shown ﬁgure right text. variants employ l-addressing reading memory layer- l-addressing writing layer-. that arc-ihyb writes layer- based h-addressing reading layer- arc-iloc uses l-addressing read layer-. layer- formed together layer- memory-bundle output layer reads predicting target sequence. memory-bundle empirical advantage single layers also used three architectures generating target sequence forming intermediate layers. arc-ii architecture similar arc-ihyb arc-ii designed investigate effect h-addressing reading different layers memory uses strategy arc-ihyb generating memory layer- differs generating layer- arc-ii uses c-addressing reading layer- l-addressing reading layer-. layer- formed together layer- memory-bundle read output layer predicting target sequence. arc-iii intend design study deeper architecture complicated addressing strategy. arc-iii follows arc-ii generate layer- layer- layer-. uses read-heads combined l-addressing write generate layer- readheads consist l-addressing read-head layer- c-addressing readhead memory bundle layer- layer-. generation layer- puts layer- together bigger memory-bundle output layer. arc-iii intermediate layers deepest among four special cases. arc-iv proposal designed study efﬁcacy c-addressing writing forming intermediate representation. employs l-addressing reading memory layer- l-addressing writing layer-. that uses l-addressing reading layer- write layer- c-addressing. c-addressing writing layer- locations layer- randomly initialized. layer- formed bundled layer- reading output layer. relation neural machine translators pointed earlier rnnsearch automatic alignment special case deepmemory shallow architecture. pictorially illustrated figure employs l-addressing reading memory layer- l-addressing writing layer- read output layer generate target sequence. shown figure layer- intermediate layer created nontrivial read-write operations. hand connection deepmemory encoder-decoder architectures less obvious since usually require reading last cell certain layers. speciﬁcally sutskever viewed deepmemory stacking layers l-addressing read-write encoder decoder part actually connected last hidden states lstms corresponding layers. experiments report empirical study applying deepmemory chinese-to-english translation. training data consist sentence pairs extracted corpora chinese words english words respectively. choose nist dataset development nist datasets test sets. case-insensitive -gram nist bleu score evaluation metric sign-test statistical signiﬁcance test. training neural networks limit source target vocabularies frequent words chinese english covering approximately corpora respectively. compare method state-of-the-art models moses open source phrase-based translation system default conﬁguration -gram language model trained target portion training data attention-based model default setting well optimal re-scaling model fair comparison output layer eachdeepmemory variant implemented gated recurrent units deepmemory architectures designed embedding size rnnsearchdefault parameter size less comparable rnnsearchbest. results main results different models given table rnnsearch points behind moses bleu average consistent observations made authors different machine translation tasks remarkably sensible designs deepmemory already achieve performance comparable moses parameters rnnsearchbest parameters. clearly deepmemory architectures yield performance signiﬁcantly better comparable baselines. among them arc-ii outperforms best setting baseline bleu average less parameters. table bleu- scores baselines rnnsearchdefault rnnsearchbest deepmemory architectures phrase-based system indicates results signiﬁcantly better rnnsearchbest. discussion depth detailed comparison between rnnsearch arc-ii arc-iii) quantitatively qualitatively suggests deep architectures essential superior performance deepmemory. although deepest architecture arc-iii bleu behind arcii performance long sentences signiﬁcant better. figure shows bleu scores generated translations test sets respect length source sentences. particular test bleu scores sentences longer merged test clearly sentences length arc-iii yields consistently higher bleu scores arc-ii. observation conﬁrmed observations translation quality consistent intuition deepmemory multiple layers transformation especially good modeling transformations representations essential machine translation relatively complicated sentences. figure bleu scores generated translations merged three test sets respect lengths source sentences. numbers x-axis ﬁgure stand sentences longer corresponding length e.g. source sentences words. c-addressing read comparison arc-ihyb arc-iloc suggests c-addressing reading plays important role learning powerful transformation intermediate representations necessary translation language pairs vastly different syntactical structures. conjecture veriﬁed good performances arc-ii arc-iii c-addressing read-heads intermediate memory layers. however memory layer-+ formed c-addressing read memory layer- serves going later stages performance usually less satisfying. comparison design h-addressing suggests another read-head l-addressing prevent transformation going astray adding tempo memory clearer temporal structure. c-addressing write bleu scores arc-iv lower arc-ii comparable rnnsearchbest suggesting writing c-addressing alone yields reasonable representation. closer look shows although arc-iv performs poorly long sentences fairly well sentences normal length. speciﬁcally source sentences length outperforms rnnsearchbest bleu points. possible explanation particular implementation c-addressing writing arc-iv relies heavily randomly initialized content hard optimize especially structure sentence complex might need guided another write-head smart initialization. cross-layer read another observation cross-layer reading almost always helps. performances arc-i unanimously drop removing memory-bundle short-cut even broadening memory units keep parameter size unchanged. might ﬂexibility gained mixing different addressing modes representations different stages. conclusion propose deepmemory novel architecture sequence-to-sequence learning stimulated recent work neural turing machine neural machine translation deepmemory builds deep architecture processing sequence data basis series transformations induced read-write operations stack memories. architecture signiﬁcantly improves expressive power models sequence-to-sequence learning veriﬁed empirical study benchmark machine translation task. kyunghyun merrienboer bart gulcehre caglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. proceedings emnlp collobert ronan weston jason bottou l´eon karlen michael kavukcuoglu koray kuksa pavel. natural language processing scratch. journal machine learning research koehn philipp hoang hieu birch alexandra callison-burch chris federico marcello bertoldi nicola cowan brooke shen wade moran christine zens richard dyer chris bojar ondrej constantin alexandra herbst evan. moses open source toolkit statistical machine translation. proceedings interactive poster demonstration sessions prague czech republic june luong thang pham hieu manning christopher effective approaches attention-based neural machine translation. proceedings conference empirical methods natural language processing appendix give example translations deepmemory specically arc-ii arciii compare reference translation given rnnsearch. focus long sentences relatively complicated structures.", "year": 2015}