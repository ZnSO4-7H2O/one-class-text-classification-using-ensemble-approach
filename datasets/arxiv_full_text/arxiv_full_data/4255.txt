{"title": "Game of Sketches: Deep Recurrent Models of Pictionary-style Word  Guessing", "tag": ["cs.CV", "cs.AI"], "abstract": "The ability of intelligent agents to play games in human-like fashion is popularly considered a benchmark of progress in Artificial Intelligence. Similarly, performance on multi-disciplinary tasks such as Visual Question Answering (VQA) is considered a marker for gauging progress in Computer Vision. In our work, we bring games and VQA together. Specifically, we introduce the first computational model aimed at Pictionary, the popular word-guessing social game. We first introduce Sketch-QA, an elementary version of Visual Question Answering task. Styled after Pictionary, Sketch-QA uses incrementally accumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves asking a fixed question (\"What object is being drawn?\") and gathering open-ended guess-words from human guessers. We analyze the resulting dataset and present many interesting findings therein. To mimic Pictionary-style guessing, we subsequently propose a deep neural model which generates guess-words in response to temporally evolving human-drawn sketches. Our model even makes human-like mistakes while guessing, thus amplifying the human mimicry factor. We evaluate our model on the large-scale guess-word dataset generated via Sketch-QA task and compare with various baselines. We also conduct a Visual Turing Test to obtain human impressions of the guess-words generated by humans and our model. Experimental results demonstrate the promise of our approach for Pictionary and similarly themed games.", "text": "abstract—the ability intelligent agents play games human-like fashion popularly considered benchmark progress artiﬁcial intelligence. similarly performance multi-disciplinary tasks visual question answering considered marker gauging progress computer vision. work bring games together. speciﬁcally introduce ﬁrst computational model aimed pictionary popular word-guessing social game. ﬁrst introduce sketch-qa elementary version visual question answering task. styled pictionary sketch-qa uses incrementally accumulated sketch stroke sequences visual data. notably sketch-qa involves asking ﬁxed question gathering open-ended guess-words human guessers. analyze resulting dataset present many interesting ﬁndings therein. mimic pictionary-style guessing subsequently propose deep neural model generates guess-words response temporally evolving human-drawn sketches. model even makes human-like mistakes guessing thus amplifying human mimicry factor. evaluate model large-scale guess-word dataset generated sketch-qa task compare various baselines. also conduct visual turing test obtain human impressions guess-words generated humans model. experimental results demonstrate promise approach pictionary similarly themed games. history computer-based modelling human player games backgammon chess important research area. accomplishments wellknown game engines deepblue alphago ability mimic human-like game moves well-accepted proxy gauging progress meanwhile progress visuo-lingual problems visual captioning visual question answering increasingly serving similar purpose computer vision community. developments backdrop explore popular social game pictionarytm. game pictionary brings together predominantly visual linguistic modalities. game uses shufﬂed deck cards guess-words printed them. participants ﬁrst group teams team takes turns. given turn team’s member selects card. he/she attempts draw sketch corresponding word printed card team-mates guess word correctly. rules game forbid verbal communication drawer teammates. thus drawer conveys intended guess-word primarily sketching process. consider scenario depicted figure group people playing pictionary. game ‘social’ robot watching people play. passively sensors record strokes drawn sketching board guess-words uttered drawer’s team members ﬁnally whether last guess correct. observed multiple game rounds robot learns computational models mimic human guesses enable participate game. fig. propose deep recurrent model pictionarystyle word guessing. models enable social robots participate real-life game scenarios shown above. picture credittrisha mittal. ﬁrst collect guess-word data sketch question answering novel pictionary-style guessing task. employ large-scale crowdsourced dataset handdrawn object sketches whose temporal stroke information available starting blank canvas successively strokes object sketch display process human subjects every time stroke added subject provides best-guess object drawn. case existing strokes offer enough clues conﬁdent guess subject requests next stroke drawn. ﬁnal stroke subject informed object category. fig. time-line typical sketch-qa guessing session every time stroke added subject either inputs best-guess word object drawn case existing strokes offer enough clues he/she requests next stroke drawn. ﬁnal stroke subject informed object’s ground-truth category. sketch-qa viewed rudimentary novel form visual question answering approach differs existing work visual content consists sparsely detailed hand-drawn depictions visual content necessarily accumulates time times question what object drawn? answers openended while sufﬁcient sketch strokes accumulate answer’. asking question might seem oversimpliﬁcation vqa. however factors extremely sparse visual detail inaccuracies object depiction arising varying drawing skills humans open-ended nature answers pose unique challenges need addressed order build viable computational models. sketch-qa create crowdsourced dataset paired guess-word sketch-strokes dubbed wordguess- collected guess sequences subjects across sketch object categories. introduce novel computational model word guessing using wordguess- data analyze performance model pictionarystyle on-line guessing conduct visual turing test gather human assessments generated guess-words please visit github.com/val-iisc/sketchguess code dataset related work. begin with shall look procedural details involved creation wordguess- dataset. creating wordguess- dataset sketch object dataset starting point hand-sketched line drawings single objects large-scale tu-berlin sketch dataset dataset contains sketches uniformly spread across object categories sketches obtained crowd-sourced manner providing category name sketchers. aspect dataset collection procedure used tu-berlin dataset aligns draw-usingguess-word-only paradigm pictionary. sketch object temporal order strokes drawn also available. subsequent analysis tu-berlin dataset schneider tuytelaars creation curated subset sketches deemed visually less ambiguous human subjects. experiments curated dataset containing object categories average sketches category. data collection methodology collect guess-word data sketch-qa used webaccessible crowdsourcing portal. registered participants initially shown screen displaying ﬁrst stroke randomly selected sketch object randomly chosen category menu options ‘yes’‘no’ provided. participants felt strokes needed guessing clicked ‘no’ button causing next stroke added. hand clicking ‘yes’ would allow type current best guess object category. wished retain current guess would click ‘no’ causing next stroke added. also propagates recently typed guess-word associates strokes accumulated far. participant instructed provide guesses early possible frequently required. last stroke added ground-truth category revealed participant. participant encouraged guess minimum object sketches. overall obtained guess data participants. pre-processing incomplete guesses instances subjects provided guess attempts initial strokes entered blank guesses subsequently. instances propagated last non-blank guess stroke sequence. multi-word guesses cases subjects provided multi-word phrase-like guesses guesses seem triggered extraneous elements depicted addition target object. fig. here x-axis denotes categories. y-axis denotes number sketches within category multiple guesses. categories shown sorted number sketches elicited multiple guesses. also examined sequences elicited multiple guesses terms object categories belong categories sorted number multi-guess sequences sketches elicited. top- bottom- categories according criteria viewed figure perspective helps understand categories inherently ambiguous terms stroke-level evolution usually drawn humans. another interesting statistic distribution ﬁrst guess location relative length sequence. figure shows distribution ﬁrst guess index locations function sequence length thus value closer implies ﬁrst guess made late sketch sequence. clearly guess location large range across object categories. requirement accurately capture range poses considerable challenge computational models human guessing. obtain category-level perspective computed median ﬁrst-guess location corresponding deviation ﬁrst guess location per-category basis sorted categories median values. resulting plot bottom categories viewed figure perspective helps understand level categories evolve recognizable iconic stroke composition relative original full-stroke reference sketch. thus categories axeenvelopeladder although seemingly simple depicted manner induces doubt guesser consequently delaying induction ﬁrst guess. hand categories cactusstrawberrytelephone tend drawn early initial strokes capture iconic nature either underlying ground-truth category easily recognizable object form different ground-truth. misspelt guesswords address incorrect spellings used enchant spellcheck library default words augmented object category names base dataset spellcheck dictionary. addition manually checked guess-word data remove unintelligible inappropriate words. also removed sequences contain guesses. thus ﬁnally obtain guessword- dataset comprising guesswords distributed across guess sequences categories. important note ﬁnal intermediate guesses could ‘wrong’ either quality drawing human error. deliberately ﬁlter guesses. design choice keeps data realistic ensures computational model opportunity characterize ‘success’ ‘failure’ scenarios pictionary. guess sequence analysis given sketch many guesses typically provided subjects? answer this examine distribution unique guesses sequence. figure shows number guesses large range. expected given large number object categories consider associated diversity depictions. large number subjects provide single guess. arises inherent ambiguity partially rendered sketches conﬁdence subjects place guess. observation also borne hypernyms-parent child ground-truth prediction parent-child relationship wordnet graph. wu-palmer similarity calculates relatedness words using graph-distance based method applied corresponding wordnet synsets. similarity prediction ground-truth least deem correct classiﬁcation. compute average accuracy human guesses progressively relax ‘correct classiﬁcation’ rule combining matching criteria logical-or fashion. average accuracy human guesses viewed table accuracy increases depending extent successive criterion relaxes base ‘exact match’ rule. large increase accuracy sub’ shows pitfall naively using exact matching rule. stage question arises criteria best characterizes human-level accuracy? ultimately groundtruth label consensus agreement among humans. obtain consensus-driven ground-truth performed human agreement study. displayed correctly classiﬁed sketches along labels human subjects. note labelling chosen changes according criteria combination. also human subjects weren’t informed usage criteria combination labelling. instead told labellings provided humans. subject asked provide assessment labelling scale randomly chose sketches correctly classiﬁed criteria combination. sketch collected agreement ratings computed weighted average agreement score. finally computed average weighted scores. ratings indicate syn’ criteria combination agreed upon human subjects characterizing human-level accuracy. determined criteria correct match also contrast human-classiﬁcation performance machinebased state-of-the-art sketch classiﬁer. ﬁnal guess associated full sketch considered guesser’s prediction object underlying sketch. predictions compared ground-truth labels originally provided sketch dataset determine ‘human guesser’ accuracy subsequently compare ‘human guesser’ accuracy machine-based sketch object recognition classiﬁer discuss trends therein final guess-word analysis guessword- data hand ﬁrst question naturally arises accuracy humans ﬁnal full sketches machine-based classiﬁer question straightforward answer compute fraction sketches whose predicted category label exactly groundtruth. however given open-ended nature guess-words ‘exact matching’ approach feasible. even assuming presence universal dictionary approach brittle restrictive. therefore ﬁrst deﬁne series semantic similarity criteria progressively relax correct classiﬁcation criterion ﬁnal sketches. matching criteria correct classiﬁcation exact match predicted guess-word literal match ground-truth category. subset predicted guess-word subset ground-truth vice-versa. criteria lets characterize certain multi-word guesses correct synonyms predicted guess-word synonym ground-truth. synonym determination wordnet synsets prediction ground-truth. hypernyms one-level parents ground-truth predicted guess-word hierarchy induced wordnet graph. table accuracy human guesses various matching criteria indicates matching criteria combined logical-or fashion determine whether predicted guess-word matches ground-truth not. table quantifying suitability matching criteria combination characterizing human-level sketch object recognition accuracy. larger human rating score suitable criteria. section details. comparing human classification performance machine-based classifier contrast human-level performance state-of-the-art sketch classiﬁer ensure fair comparison consider sketches overlap test used evaluate machine classiﬁer. table summarizes prediction combinations classiﬁers. results seem suggest machine classiﬁer ‘wins’ human classiﬁer underlying reason open-ended nature human guesses closed-world setting machine classiﬁer trained. determine whether difference human machine classiﬁers statistically signiﬁcant cohen’s test. essentially cohen’s effect size used indicate standardised difference means ranges suppose given category mean accuracy w.r.t human classiﬁcation criteria similarly corresponding quantities machine classiﬁer examples misclassiﬁcations seen figure although guesses ground-truth categories lexically distant guesses sensible conditioned visual stroke data. calculated cohen’s categories indicated computed average resulting scores. average value indicates signiﬁcant differences classiﬁers according signﬁcance reference tables commonly used determine cohen’s signiﬁcance. general though categories classiﬁer outperforms other. list top- categories classiﬁer outperforms given table distribution correct human guess statistics percategory basis viewed figure category calculate conﬁdence intervals. intervals inform given level certainty whether true accuracy results likely fall range identiﬁed. particular wilson score method calculating conﬁdence intervals employ assume variable interest modeled binomial random variable. computational models describe computational model designed produce human-like guess-word sequences on-line manner. model evaluation split sequences guessword- randomly disjoint sets containing data used training validation testing phases respectively. data preparation suppose sketch composed strokes. cumulative stroke sequence i.e. sequence corresponding guess-words gn}. sketches ﬁrst resized zero-centered. ensure sufﬁcient training data augment sketch data associated guess-words. sketches accumulated stroke sequence ﬁrst morphologically dilated fig. distribution correct predictions across categories sorted median category-level score. x-axis shows categories y-axis stands classiﬁcation rate. subsequent augmentations obtained applying vertical scaling also augment guess-words replacing guess-word plural form synonyms wherever appropriate. data representation penultimate fully-connected layer’s outputs cnns ﬁne-tuned sketches used represent sketch stroke sequence images. guesswords represented using pre-trained word-embeddings. typically human-generated guess sequence contains distinct phases. ﬁrst phase guesses provided subject since accumulated strokes provide insufﬁcient evidence. therefore many initial guesses empty hence corresponding embeddings exist. tackle this guess’ pre-deﬁned non-wordembedding model design strategy model’s objective cumulative stroke sequence target guess-word sequence given choice data representation above model effectively needs sequence sketch features sequence word-embeddings. achieve sequence-tosequence mapping deep recurrent neural network architectural template choice sequential mapping process effective need discriminative sketch representations. ensures focus modelling crucial sequential aspects initiate word-guessing process transition guess-word guessing begun obtain discriminative sketch representations ﬁrst train regressor predict guess-word embedding accumulated stroke image presented important note ignore sequential nature training data process. additionally omit sequence elements corresponding ‘no-guess’ regressor training evaluation. frees regressor additionally model complex many-to-one mapping strokes accumulated ﬁrst guess ‘no-guess’. arrive ﬁnal regressor begin ﬁnetuning pre-trained photo object cnn. minimize impact drastic change domain task undertake series successive ﬁne-tuning steps describe next. learning word-embedding regressor step- ﬁne-tune vgg- object classiﬁcation using sketchy large-scale sketch object dataset -way classiﬁcation corresponding categories present dataset. denote resulting ﬁne-tuned step- weights used initialize vgg- ﬁne-tuned regressing word-embeddings corresponding category names sketchy dataset. speciﬁcally -dimensional wordembeddings provided wordvec model trained -billion google news words choice motivated open-ended nature guess-words sketch-qa consequent need capture semantic similarity ground-truth guess-words rather perform exact matching. loss function w.r.t predicted word embedding ground-truth embedding consider mean squared loss cosine loss hingerank loss length-normalized versions respectively corresponds normalized version randomly chosen category’s word-embedding. value margin convex combination cosine loss hinge-rank loss closs λhloss. predicted embedding deemed ‘correct’ match knearest word-embedding neighbors contains overall found convex combination loss provide best performance. denote resulting regressor step- ﬁne-tuned randomly ordered sketches training data sequences corresponding wordembeddings. repeating grid search convex combination loss found provide best performance validation set. note case hinge-rank loss corresponds word-embedding randomly selected entire word-embedding dictionary. denote ﬁne-tuned regressor fig. architecture deep neural model word guessing. rectangular bars correspond guess-word embeddings. corresponds regressor whose penultimate layer’s outputs used input features lstm model. reﬂects choice modelling guess’ pre-deﬁned non-word embedding. section details. training regressor conﬁgure predict word-embeddings. preliminary evaluation portion training sequences corresponding guess-words. time-step loss determined best regressor. lstm speciﬁc variant. experiments adagrad optimizer early-stopping criterion terminating optimization. evaluation k-nearest neighbor criteria mentioned examine performance determine best conﬁguration compute proportion ‘correct matches’ subsequence validation sequences containing guess-words. baseline also compute sequence-level scores regressor average per-sequence scores across validation sequences. results show regressor performs reasonably well spite overall complexity involved regressing guess-word embeddings however performance noticeably surpassed lstm demonstrating need capture temporal context modelling guess-word transitions. ﬁnal model merge validation training sets re-train best architectural settings determined validation performance retain focus report performance numbers two-phase baseline. complete description baseline architecture related ablative experiments please refer appendix temporal contextual information caused splitting original sequence disjoint sub-sequences possibly reason lower performance two-phase baseline. additionally need integrate sequential information highlighted inferior performance cnnbaseline. also wish point guesses test out-of-vocabulary words i.e. guesses present train validation set. inspite this model achieves high sequence-level accuracy thus making case open-ended word-guessing models. sketch sequences viewed figure visual turing test subjective assessment model also conduct visual turing test. randomly sample sequences test-set. model predictions nearest word-embedding corresponding guess. construct kinds paired sequences corresponds sketch stroke sequence correspond human model generated guess sequences respectively. randomly display stroke-and-guess-word paired sequences human judges judges sequence types. without revealing origin guesses judge prompted produced guesses?. judges entered ratings -point likert scale minimize selection bias scale ordering reversed half subjects sequence ﬁrst compute mode ratings guesser type. determine statistical signiﬁcance ratings additionally analyze rating pairs fig. architecture two-phase baseline. ﬁrst phase used predict location transition word-guessing phase starting transition location second-phase sequentially outputs word-embedding predictions stroke sequence. study distribution ratings human subject-based guesses wordguess- seem clearly identiﬁed frequent rating levels correspond ‘human’. non-trivial frequency ‘machine’ ratings reﬂects ambiguity induced sketches associated guesses also possibility machine equally viable generator. model-generated guesses many could identiﬁed such indicating need sophisticated guessing models. also evident wilcoxon signedrank test indicates signiﬁcant effect guesser type interestingly second-most preferred rating model guesses ‘human’ indicating degree success proposed model. related work beyond obvious entertainment value pictionary involves number social collaborative cognitive aspects studied researchers. attempt neural correlates creativity saggar analyze fmri data participants instructed draw sketches pictionary ‘action’ words approach subjects guess word instead human-elicited text-based responses visual content particularly game-like settings explored object categorization however visual content static accumulate sequentially unlike case. work ullman determining minimally recognizable image conﬁgurations also bears mention. approach complementary sense incrementally stroke content incrementally reduce image content recent times deep architectures sketch recognition found great success. however models trained output single ﬁxed label regardless intra-category variation. contrast model trained actual human guesses naturally exhibits human-like variety responses also model solves much complex temporally-conditioned multiple word-embedding regression problem. another important distinction dataset contains incorrect guesses usually arise ambiguity sketched depictions. ‘errors’ normally considered undesirable deliberately include training phase enable realistic mimicking. turn requires model implicitly capture subtle ﬁne-grained variations sketch quality situation faced existing approaches simply optimize classiﬁcation accuracy. dataset collection procedure similar employed johnson part pictionarystyle game stellasketch. however subject choose object category. also subjects provide guesses stroke sequences existing sketches sketches created real-time. unfortunately stellasketch dataset available publicly study. also pertinent compare task dataset quickdraw large-scale sketch collection initiative google quickdraw task generates dataset object sketches. contrast task sketchqa results dataset human-generated guess words. quickdraw sketch associated single ﬁxed category. sketchqa sketch existing dataset explicitly associated list multiple guess words. sketchqa freedom provided human guessers enables sketches arbitrarily ﬁne-grained labels however quickdraw’s label ﬁxed. finally dataset captures rich sequence guesses response accumulation sketch strokes. therefore used train humanlike guessing models. quickdraw’s dataset lacking human guesses suited purpose. computational model employs long short term memory variant recurrent neural networks lstm-based frameworks utilized tasks involving temporally evolving content video captioning action recognition model needs produce human-like guesses response temporally accumulated content also additional challenge determining long ‘wait’ initiating guessing process. guessing phase begins model typically outputs multiple answers. per-time-step answers even unrelated other. paradigm different setup wherein single answer constitutes output. also output aforementioned approaches softmax distribution words ﬁxed dictionary. contrast regression formulation wherein outputs word-embedding prediction timestep. ensures scalability increase vocabulary better generalization since model outputs predictions constant-dimension vector space. adopt similar regression formulation obtain improved performance image annotation action recognition. since model aims mimic human-like guessing behavior subjective evaluation generated guesses falls within ambit visual turing test however free-form nature guess-words ambiguity arising partial stroke information make task uniquely challenging. discussion conclusion introduced novel guessing task called sketchqa crowd-source pictionary-style open-ended guesses object line sketches drawn. resulting dataset dubbed guessword- contains guess sequences subjects across object categories. also introduced novel computational model produces open-ended guesses analyzed performance guessword- dataset challenging on-line pictionary-style guessing tasks. addition computational model dataset guessword- serve researchers studying human perceptions iconic object depictions. since guesswords paired object depictions data also graphic designers civic planners creation meaningful logos public signage. especially important since incorrectly perceived depictions often result inconvenience mild amusement extreme cases deemed offensive. another potential application domain clinical healthcare. guessword- consists partially drawn objects corresponding guesses across large number categories. data could useful neuro psychiatrists characterize conditions visual agnosia disorder subjects exhibit impaired object recognition capabilities future wish also explore computational models optimal guessing i.e. models guess sketch category early correctly possible. futuristic context mentioned beginning models would help robot contribute productive team-player correctly guessing team-member’s sketch early possible. dataset stroke sequence shown single subject therefore associated single corresponding sequence guesses. shortcoming mitigated future editions sketch-qa. promising approach data collection would digital whiteboards high-quality microphones state-ofthe-art speech recognition software collect realistic paired stroke-and-guess data pictionary games home-like pairs binary classiﬁcation inference repeatedly apply model successive time-steps stopping model outputs second possibility train inference stop unrolling encountered. describe setup model ﬁrst. model model ﬁne-tune vgg- object classiﬁcation model using sketchy proposed model. ﬁne-tuned model used initialize another vgg- model -dimensional bottleneck layer introduced layer. denote model sketch representation feature representations consider possibilitiesapp ﬁne-tuned -way classiﬁcation -dimensional output fullyconnected layer forms feature representation. architecture option modiﬁed -way class prediction additional auxiliary task. choice motivated possibility encoding category-speciﬁc transition location statistics within -dimensional feature representation losses corresponding outputs modiﬁed architecture weighted equally training. loss weighting imbalanced label distributions training feature extraction phase-i encounter imbalance distribution no-guesses guesses mitigate this employ class-based loss weighting binary classiﬁcation task. suppose number no-guess samples number guess samples weights classes computed binary cross-entropy loss computed stand ground-truth prediction respectively no-guess sample otherwise. data thus appropriately accounting relatively smaller number no-guess samples training data. similar procedure also used weighting losses -way auxiliary classiﬁer variant trained. case weights determined per-object category distribution training sequences. experimentally auxiliary task shows better performance ﬁrst rows table lstm setup -dimensional output q-auxiliary per-timestep sketch representation lstm model. capture temporal evolution binary sequences conﬁgure lstm output binary label timestep lstm explored settings would also worthwhile consider sketchqa beyond object names include additional lexical types believe resulting data coupled improved versions computational models could make scenario figure reality day. appendix two-phase baseline model section present architectural design related evaluation experiments two-phase baseline originally mentioned section typically guess sequence contains distinct phases. ﬁrst phase guesses provided subject since accumulated strokes provide insufﬁcient evidence. later stage subject feels conﬁdent enough provide ﬁrst guess. thus location ﬁrst guess starting point second phase. ﬁrst phase offers usable guesswords. therefore rather tackling phases within single model adopt divide-and-conquer approach. design baseline ﬁrst predict phase transition location conditioned location model predicts guess-word representations rest sequence two-phase model model described main paper guess-word generator common component. guess-word generation model already described main paper remainder section focus ﬁrst phase two-phase baseline. consider typical guess sequence suppose ﬁrst phase corresponds initial sub-sequence length second phase corresponds remainder sub-sequence length denoting guess’ guess-word transformed binary sequence times)]. therefore objective phase model correctly predict transition index i.e. table transition location prediction accuracies various phase architectures shown. refers binary output model pre-trained feature extraction. refers model -way auxiliary classiﬁcation. last rows correspond test accuracies best lstm conﬁgurations. ‘loss’ column categorical-cross entropy average sequence loss wseq weighted sequence loss mrnk modiﬁed ranking loss. results shown ‘window width’ sized windows centered ground-truth transition location. rows dotted line show performance best lstm models test sequences. variations number hidden units weight matrices initialized orthogonal matrices gain factor forget gate bias training lstms average sequence loss computed average per-time-step binary cross-entropy losses. loss regularized standard l-weight norm weight-decay parameter optimization adagrad learning rate momentum term gradients clipped training. lstm experiments mini-batch size lstm loss function variants default sequence loss formulation treats time-steps sequence equally. since interested accurate localization transition point explored following modiﬁcations default loss lstm transition weighted loss encourage correct prediction transition location explored weighted version default sequence-level loss. beginning transition location per-timestep losses either side transition weighted exponentially decaying factor e−α]s) time-steps essentially loss transition location weighted losses locations downscaled weights less larger distance transition location smaller weight. tried various values localization accuracy viewed table note weighted loss added original sequence loss actual training. modiﬁed ranking loss want model prevent occurrence premature multiple transitions. incorporate notion ranking loss formulation proposed denote loss time step softmax score ground truth label shall refer detection score. case phase-i model corresponds binary cross-entropy loss. overall loss time step modiﬁed want phase-i model produce monotonically non-decreasing softmax values no-guesses guesses progresses sub-sequence. words transition time i.e. want current detection score less previous detection score. therefore situation ranking loss computed corresponds time step time-step corresponds transition i.e. want detection score previous phase small possible therefore compute ranking loss training convex combination sequence loss ranking loss loss weighting determined grid search λs−r experiments found transition weighted loss provide best performance evaluation inference time accumulated stroke sequence processed sequentially phase-i model outputs marks beginning phase-ii. suppose predicted transition index ground-truth index prediction deemed correct denotes half-width window centered experiments used results indicate q-auxiliary model outperforms best lstm model small margin. addition weighted sequence loss default version plays crucial role latter since default version explicitly optimize transition location. overall large variation sequence lengths transition locations explains performance exact localization. note however performance improves considerably nearby locations considered evaluation inference location predicted phase-i model used starting point phase-ii describe phase-ii model since virtually identical design model described main paper overall results determine overall performance utilize best architectural settings determined validation performance. merge validation training sets re-train best models report performance test set. overall performance measure report items test p-ii fraction correct matches respect subsequence corresponding ground-truth word guesses. words assume accurate localization phase perform phase inference beginning ground-truth location ﬁrst guess. full phase-i model determine transition location. note depending predicted location possible obtain word-embedding predictions ground-truth corresponding time-step corresponds guess’. regarding predictions mismatches compute fraction correct matches full sequence. baseline model outputs best performing per-frame cnns phase phase phase-ii model objective lstm same. case phase-i model. reduction long-range temporal contextual information caused splitting original sequence disjoint sub-sequences possibly another reason lower performance two-phased model. chen lawrence zitnick mind’s recurrent visual representation image caption generation cvpr venugopalan rohrbach donahue mooney darrell saenko sequence sequence-video text cvpr kiros courville salakhutdinov zemel bengio show attend tell neural image caption generation visual attention. icml vol. antol agrawal mitchell batra lawrence zitnick parikh visual question answering iccv eigen fergus predicting depth surface normals semantic labels common multi-scale convolutional architecture proceedings ieee international conference computer vision saxe mcclelland ganguli exact solutions nonlinear dynamics learning deep linear neural networks corr vol. abs/. available http//arxiv.org/abs/. donahue anne hendricks guadarrama rohrbach venugopalan saenko darrell long-term recurrent convolutional networks visual recognition description cvpr", "year": 2018}