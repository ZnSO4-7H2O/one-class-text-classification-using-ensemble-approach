{"title": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision!  In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional.  In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.", "text": "number recent works proposed techniques end-to-end learning communication protocols among cooperative multi-agent populations simultaneously found emergence grounded human-interpretable language protocols developed agents learned without human supervision paper using task talk reference game agents testbed present sequence ‘negative’ results culminating ‘positive’ showing agent-invented languages effective interpretable compositional. essence natural language emerge ‘naturally’ despite semblance ease natural-language-emergence gather recent literature. discuss possible coax invented languages become human-like compositional increasing restrictions agents communicate. fundamental goal artiﬁcial intelligence development goal-driven dialog agents speciﬁcally agents perceive environment communicate humans agents natural language towards goal. historically agents based slot ﬁlling dominant paradigm today neural dialog models trained large quantities data. perhaps somewhat counterintuitively current paradigm treats dialog static supervised learning problem rather interactive agent learning problem naturally speciﬁcally typical pipeline collect large dataset human-human dialog inject machine middle dialog dataset supervise mimic human response. teaches agent correlations symbols convey functional meaning language grounding compositionality aspects planning alternative paradigm long history witnessing recent resurgence situated language learning. number recent works proposed reinforcement learning techniques learn communication protocols agents situated virtual environments completely end-to-end manner perceptual input communication action simultaneously found emergence grounded human-interpretable language communication protocols developed agents without human supervision pretraining simply succeed task. paper study following question conditions lead emergence human-interpretable compositional grounded language? ﬁnding natural language emerge ‘naturally’ multi-agent dialog despite semblance ease natural-languageemergence multi-agent games gather recent literature. speciﬁcally sequence ‘negative’ results culminating ‘positive’ agents always successfully invent communication protocols languages achieve goals near-perfect accuracies invented languages decidedly compositional interpretable ‘natural’; possible coax invented languages become human-like compositional increasing restrictions agents communicate. related work novelty. starting point investigation recent work proposed cooperative reference game agents communication necessary accomplish goal information asymmetry. contribution exhaustive study conditions must present compositional grounded language emerges subtle important differences execution tabular qlearning reinforce generalization novel environments hope ﬁndings shed light interpretability languages invented cooperative multi-agent settings place recent work appropriate context inform fruitful directions future work. testbed cooperative reference game agents q-bot a-bot. game grounded synthetic world objects comprised three attributes color style shape four possible values total objects. fig. shows possible attribute values. task talk plays multiple rounds dialog. start a-bot given object unseen q-bot e.g. side q-bot assigned task consisting attributes e.g. goal q-bot discover attributes hidden object figure task talk testbed study cooperative -player game task talk grounded synthetic world objects shapes colors styles. q-bot assigned task inquire state ordered pair attributes. example gameplay agents q-bot asks questions depending task answered a-bot conditioned hidden instance visible itself. q-bot makes prediction pair attributes dialog a-bot. speciﬁcally q-bot abot exchange utterances ﬁnite vocabularies rounds q-bot speaking ﬁrst. game culminates q-bot guessing pair attribute values e.g. agents rewarded identically based accuracy q-bot prediction. note task talk game setting involves informational asymmetry agents abot sees object q-bot not; similarly q-bot knows task a-bot not. thus two-way communication necessary success. without asymmetry a-bot could simply convey target attributes task without q-bot speak. setting widely studies economics game theory classic lewis signaling game necessitating dialog between agents able ground ﬁnal setting formalize q-bot a-bot agents operating partially observable world optimize policies using deep reinforcement learning. states actions. agent observes input output agent stochastic environment. beginning round q-bot observes state acts utterst token vocabulary similarly a-bot observes history utfigure policy networks q-bot a-bot. round dialog q-bot generates question speaker network conditioned state encoding a-bot encodes conditioned instance encoded instance encoder generates answer q-bot encodes pair a-bot encodes answer updates state encoding sampled q-bot updates state predicts attribute pair prediction lstm round receives reward. terance state emits response last round q-bot takes ﬁnal action predicting pair attribute values solve task. cooperative reward. q-bot a-bot rewarded identically based accuracy q-bot’s prediction receiving positive reward prediction matches ground truth negative reward otherwise. arrive values empirically based speed convergence experiments. policy networks. model q-bot a-bot operating stochastic policies respectively instantiate lstm-based models. lower case characters denote strings upper case denote corresponding vector encoded model. show fig. q-bot modeled three modules speaking listening prediction. task received -dimensional one-hot encoding space possible tasks embedded listener lstm. round speaker network models probability output utterances based state modeled fully-connected layer followed softmax transforms distribution receiving reply a-bot listener lstm updates state processing tokens dialog exchange. ﬁnal round prediction lstm unrolled twice produce q-bot’s prediction based ﬁnal state task before task onehot encoding prediction lstm time steps resulting pair outputs used prediction ˆwg. analogously a-bot modeled combination speaker network listener lstm instance encoder. like q-bot speaker network models probability utterances listener lstm updates given state state based dialog exchanges. instance encoder embeds one-hot attribute vector linear layer concatenates three encodings obtain uniﬁed instance representation. learning policies reinforce. train agents update policy parameters using popular reinforce policy gradient algorithm. note game fully-cooperative assume full observability agent another opting instead treat agent part stochastic environment updating other. derive parameter gradients setup. recall agents take actions utterances attribute prediction objective maximize expected reward agents’ policies though agents receive reward gameplay intermediate actions assigned reward following reinforce algorithm write gradient expectation expectation policy gradients. derive explicitly time step guage solve game near-perfect accuracies invented languages decidedly compositional interpretable ‘natural’ setting language compositional simply amounts ability agents communicate compositional atoms task instance independently. section present series settings progressively restrictive coax agents towards adopting compositional language providing analysis learned languages ‘cheating’ strategies developed along way. tab. summarizes results settings. experiments optimal policies found. setting provide qualitative analysis learned languages report ability generalize unseen instances. object-instances training remaining evaluation. begin simplest setting abot q-bot given arbitrarily large vocabularies. |va| greater number instances learned policy mostly ignores q-bot asks instead a-bot convey instance using pairs symbols across rounds unique instance e.g. token pairs convey shown fig. notice means ‘dialog’ necessary amounts agent codebook maps symbols object instances. essence setting collapsed analog lewis signaling game a-bot signaling complete world state q-bot simply reporting target attributes. examples illustrate behavior setting shown fig. perhaps expected generalization language unseen instances quite poor adopted strategy mapping instances token pairs fails test instances containing novel combinations attributes agents lack agreed-upon code training. seems clear like human communication limited vocabulary standard practice estimate expectations sample averages sampling environment sampling dialog q-bot a-bot culminating prediction q-bot received reward. reinforce update rule intuitive interpretation informative dialog leads positive reward made probable poor exchange leading negative reward pushed implementation details. models implemented using pytorch deep learning framework. represent instances learn dimensional embedding every possible attribute values concatenate three instance attributes obtain ﬁnal instance representation size tokens encoded one-hot vectors embedded dimension vectors. a-bot q-bot learn token embeddings without sharing. listener networks agents implemented lstms hidden layer size dimensions. modules within agent initialized using xavier method episodes two-round dialogs compute policy gradients perform updates according adam optimizer learning rate furthermore gradients clipped faster convergence train episodes next iteration instances misclassiﬁed current network randomly sampling remaining instances. code publicly available. figure overcomplete vocabularies setting owing large vocabulary denote tokens using numbers opposed english alphabet characters shown ﬁgures. a-bot mostly ignores qbot asks instead conveys instance using pairs symbols across rounds unique instance leading highly non-human intuitive non-compositional language. since world attributes possible settings states believe intuitive choice |vq| |va| enough circumvent ‘cheating’ enumeration strategy previous experiment. surprisingly language learned setting decidedly non-compositional also difﬁcult interpret observations follow fig. shows sample dialogs setting. observe q-bot uses ﬁrst round convey task a-bot encoding tasks order-agnostic fashion thus multiple rounds utterance q-bot rendered unnecessary second round inconsistent across instances even task. example symmetric tasks ﬁrst fig. induce ﬁrst token q-bot. given task q-bot ﬁrst round afigure attribute value vocabulary setting show symmetric tasks instance either side illustrate similarities language agents. seen here q-bot maps symmetric tasks order-agnostic fashion uses ﬁrst token convey task information a-bot. needs identify attribute pairs given task. rather ground symbols individual states a-bot follows ‘set partitioning’ strategy i.e. a-bot identiﬁes pair attributes unique combinations round utterances thus symbols reused across tasks describe different attributes ‘set partitioning’ strategy consistent known results game theory nash equilibria ‘cheap talk’ games strategy improved generalization unseen instances able communicate task; however fails unseen attribute value combinations compositional. problem previous setting a-bot’s utterances mean different things based round dialog essentially communication protocol over-parameterized must limit further. first limit a-bot’s vocabulary |va|= reduce number ‘synonyms’ agents learn. second eliminate a-bot’s capability identify different rounds interaction removing abot’s memory. words reset state table emergence compositional grounding language learnt agents. a-bot learns consistent mapping across rounds depending query attribute. token grounding q-bot depends task hand. though compositional q-bot necessarily query attribute order task instead re-arranges accordingly prediction time contains memory. tab. enumerates learnt groundings agents. given mapping predict plausible dialog agents unseen instance task combination. notice possible compositionality emergent language agents. example consider solving task instance fig. q-bot queries across rounds receives answers. examples along grounded meaning tokens shown fig. intuitively consistently grounded compositional language greatest ability generalize among settings explored correctly answering held instances. note errors setting seem largely a-bot giving incorrect answers despite q-bot asking correct questions accomplish task. plausible reason could model approximation error stemming instance encoder test instances unseen novel attribute combinations. demonstrated previous sections even though compositional language optimal policies agents tend learn equally useful forms communication. thus compositional language naturally emerge without explicit need even situations compositionality emerge perhaps interesting analyze process emergence learnt language itself. therefore present study explicitly identiﬁes symbol grounded agents figure example dialogs memoryless a-bot minimal vocabulary setting learnt language consistent grounded denoted token. incorrect predictions unseen instances also shown. notice incorrectly predicted attribute still right category vector time step a-bot longer distinguish rounds another. hypothesize q-bot must ground a-bot’s tokens consistently across rounds able communicate memoryless a-bot. restrictions result learned language grounds individual symbols attributes states. example q-bot learns shape color style. q-bot however learn always utter symbols order task e.g. asking shape ﬁrst notice perfectly valid q-bot later re-arrange attributes task desired order. similarly a-bot learns mappings attribute values attribute query remain consistent regardless round similar learned languages reported recent works closely related solve problem taking away q-bot’s state rather a-bot’s memory. approach taking away task q-bot’s state interpreted q-bot ‘forgetting’ task interacting a-bot. however behavior q-bot remember task during dialog predicting somewhat unnatural compared setting. non-compositional language q-bot insigniﬁcant inconsistent a-bot grounding across rounds poor generalization unseen instances non-compositional language q-bot uses round convey task inconsistent a-bot grounding across rounds poor generalization unseen instances compositional language q-bot uses rounds convey task consistent a-bot grounding across rounds good generalization unseen instances table overview settings explore analyze language learnt agents cooperative game task talk. last columns measure generalization terms prediction accuracy least attribute pair held-out test containing unseen instances. agents–q-bot a-bot–converse other seen traversing dialog tree subtree depicted fig. simply dialog tree enumeration possible dialogs represented form tree levels tree corresponding round interaction. elaborate consider partial dialog tree task shown fig. setting sec. qbot’s ﬁrst token a-bot |va| plausible replies shown -way branch off. general dialog tree task talk contains total |vq||va| leaves levels deep. dialog agents descend land leaves. dialog trees offer interesting alternate view learning problem. goal learning communication agents equivalently seen mapping pairs dialog tree leaves. leaf labeled attribute pair used accomplish prediction task. example solving results dialog →→x→ descend dialog tree along corresponding path assign tuple resulting leaf. case compositional grounded dialog tuples form would mapped leaf labeled successfigure dialog tree memoryless a-bot minimal vocabulary setting shown task every dialog agents results tree traversal beginning root e.g. →→x→ lands top-right leaf. text details. fully solve task. note wildcard style attribute tuple above irrelevant particular task. following section dialog trees explore evolution language learnt agents memoryless a-bot minimal vocabulary setting sec. gain insight languages learned create language evolution plot shown fig. speciﬁcally regular intervals policy learning construct dialog trees. point learning nodes tree befigure evolution language timeline shows groundings learned agents training overlaid accuracy. note q-bot learns encodings tasks early except improvement accuracy strongly correlated groundings learnt. come stay ‘pure’ node identical) point agents learned dialog subsequence. fig. depicts timeline concepts learned various nodes trees training. next describe procedure identify particular ‘concept’ grounded agents language. construction. constructing dialog trees regular intervals identify ‘concepts’ node/leaf using dialog tree completely trained model achieves perfect accuracy train set. concept simply common trend among tuples either assigned leaf contained within subtree node root. illustrate concept right leaf fig. i.e. instances assigned leaf task blue triangles. next given resultant concept node/leaf backtrack time check ﬁrst occurrence tuples satisfy corresponding concept assigned particular node/leaf. words compute earliest time node/leaf ‘pure’ respect ﬁnal learned concept. finally plot leaves/nodes associated concept backtracked time fig. observations. highlight observations fig. below agents ground tasks initially around epoch speciﬁcally q-bot assigns mapped hence q-bot learns ﬁrst token early training procedure around epochs. strong correlation improvement performance agents learn language grounding. particular improvement within span epochs grounding achieved seen fig. conclusion conclusion presented sequence ‘negative’ results culminating ‘positive’ showing invented languages effective decidedly interpretable compositional. goal simply improve understanding interpretability invented languages multiagent dialog place recent work context inform fruitful directions future work.", "year": 2017}