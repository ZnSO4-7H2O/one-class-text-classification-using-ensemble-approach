{"title": "Large-Scale Evolution of Image Classifiers", "tag": ["cs.NE", "cs.AI", "cs.CV", "cs.DC", "I.2.6; I.5.1; I.5.2"], "abstract": "Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.", "text": "neural networks proven effective solving difﬁcult problems designing architectures challenging even image classiﬁcation problems alone. goal minimize human participation employ evolutionary algorithms discover networks automatically. despite signiﬁcant computational requirements show possible evolve models accuracies within range published last year. speciﬁcally employ simple evolutionary techniques unprecedented scales discover models cifar- cifar- datasets starting trivial initial conditions reaching accuracies respectively. this novel intuitive mutation operators navigate large search spaces; stress human participation required evolution starts output fully-trained model. throughout work place special emphasis repeatability results variability outcomes computational requirements. neural networks successfully perform difﬁcult tasks large amounts training data available discovering neural network architectures however remains laborious task. even within speciﬁc problem image classiﬁcation state attained many years focused investigation hundreds researchers simonyan zisserman szegedy huang among many others). therefore surprising recent years techniques automatically discover architectures gaining popularity earliest neuro-discovery methods neuro-evolution despite promising results deep learning community generally perceives evolutionary algorithms incapable matching accuracies hand-designed models paper show possible evolve competitive models today given enough computational power. used slightly-modiﬁed known evolutionary algorithms scaled computation unprecedented levels know. this together novel intuitive mutation operators allowed reach competitive accuracies cifar- dataset. dataset chosen requires large networks reach high accuracies thus presenting computational challenge. also took small ﬁrst step toward generalization evolved networks cifar- dataset. transitioning cifar- cifar- modify aspect parameter algorithm. typical neuro-evolution outcome cifar- test accuracy flops model test accuracy flops. ensembling validation-top models population reaches test accuracy additional training cost. cifar- single experiment resulted test accuracy flops. know accurate results obtained datasets automated discovery methods start trivial initial conditions. throughout study placed special emphasis simplicity algorithm. particular oneshot technique producing fully trained neural network requiring post-processing. also impactful meta-parameters starting poor-performing models table comparison single-model hand-designed architectures. columns indicate test accuracy data-augmented cifar- cifar- datasets respectively. reachable? column denotes whether given handdesigned model lies within search space. entry indicates value reported. indicates result reported huang instead original author. much table based presented huang convolutions algorithm must evolve complex convolutional neural networks navigating fairly unrestricted search space ﬁxed depth arbitrary skip connections numerical parameters restrictions values take. also paid close attention result reporting. namely present variability results addition value account researcher degrees freedom study dependence meta-parameters disclose amount computation necessary reach main results. hopeful explicit discussion computation cost could spark study efﬁcient model search training. studying model performance normalized computational investment allows consideration economic concepts like opportunity cost. neuro-evolution dates back many years originally used evolve weights ﬁxed architecture. stanley miikkulainen showed advantageous simultaneously evolve architecture using neat algorithm. neat three kinds mutations modify weight connection existing nodes insert node splitting existing connection. also mechanism recombining models strategy promote diversity known ﬁtness sharing evolutionary algorithms represent models using encoding convenient purpose— analogous nature’s dna. neat uses direct encoding every node every connection stored dna. alternative paradigm indirect encoding subject much neuro-evolution research example cppn allows evolution repeating features different scales. also rigazio indirect encoding improve convolution ﬁlters initially highly-optimized ﬁxed architecture. research weight evolution still ongoing broader machine learning community defaults back-propagation optimizing neural network weights back-propagation evolution combined stanley structure evolved. algorithm follows alternation architectural mutations weight back-propagation. similarly breuel shafait approach hyper-parameter search. fernando also back-propagation allowing trained weights inherited structural modiﬁcations. studies create neural networks small comparison typical modern architectures used image classiﬁcation focus encoding efﬁciency evolutionary process scale. comes images neuro-evolution results reach computational scale required succeed mnist dataset modern classiﬁers often tested realistic images cifar datasets much challenging. datasets require large models achieve high accuracy. non-evolutionary neuro-discovery methods successful tackling realistic image data. snoek used bayesian optimization tune hyper-parameters ﬁxed-depth architecture reachtable comparison automatically discovered architectures. contain test accuracy dataaugmented cifar- cifar- datasets respectively. entry indicates information reported known zoph quote result similar search space ours well best result. please refer table hand-designed results including state art. discrete params. means parameters picked handful values state time. zoph used reinforcement learning deeper ﬁxed-length architecture. approach neural network—the discoverer—constructs convolutional neural network—the discovered—one layer time. addition tuning layer parameters remove skip connections. this together manual postprocessing gets close state art. baker q-learning also discover network layer time approach number layers decided discoverer. desirable feature would allow system construct shallow deep solutions requirements dataset hand. different datasets would require specially tuning algorithm. comparisons among methods difﬁcult explore different search spaces different initial conditions tangentially also neuro-evolution work lstm structure beyond scope paper. also related work saxena verbeek embed convolutions different parameters species supernetwork many parallel paths. algorithm selects ensembles paths super-network. finally canonical approaches hyper-parameter search grid search example) random search latter better approach builds previous work important differences. explore large model-architecture search spaces starting basic initial conditions avoid priming system information known good strategies speciﬁc dataset hand. encoding different neuro-evolution methods mentioned above simpliﬁed graph transformed full neural network graph training evaluation mutations acting reminiscent neat. however instead single nodes mutation insert whole layers—i.e. tens hundreds nodes time. also allow layers removed evolutionary process simplify architecture addition complexifying layer parameters also mutable prescribe small possible values choose from allow larger search space. ﬁtness sharing. report additional results using recombination part used mutation only. hand back-propagation optimize weights inherited across mutations. together learning rate mutation allows exploration space learning rate schedules yielding fully trained models evolutionary process tables compare approach hand-designed architectures neuro-discovery techniques respectively. lutional network dimensions tensor represent spatial coordinates image third number channels. activation functions applied vertices either batch-normalization rectiﬁed linear units plain linear units. graph’s edges represent identity connections convolutions contain mutable numerical parameters deﬁning convolution’s properties. multiple edges incident vertex spatial scales numbers channels coincide. however vertex must single size number channels activations. inconsistent inputs must resolved. resolution done choosing incoming edges primary one. pick primary edge skip connection. activations coming non-primary edges reshaped zerothorder interpolation case size truncation/padding case number channels addition graph learning-rate value also stored dna. child similar identical parent action mutation. reproduction event worker picks mutation random predetermined set. contains following mutations alter-learning-rate identity reset-weights insert-convolution remove-convolution. alter-stride alter-number-of-channels filter-size add-skip remove-skip speciﬁc mutations chosen similarity actions human designer take improving architecture. clear hybrid evolutionary–hand-design methods future. probabilities mutations tuned way. mutation acts numerical parameter chooses value random around existing value. sampling uniform distributions. example mutation acting convolution output channels automatically search high-performing neural network architectures evolve population models. model—or individual—is trained architecture. model’s accuracy separate validation dataset measure individual’s quality ﬁtness. evolutionary step computer—a worker—chooses individuals random population compares ﬁtnesses. worst pair immediately removed population—it killed. best pair selected parent undergo reproduction. mean worker creates copy parent modiﬁes copy applying mutation described below. refer modiﬁed copy child. worker creates child trains child evaluates validation puts back population. child becomes alive—i.e. free parent. scheme therefore uses repeated pairwise competitions random individuals makes example tournament selection using pairwise comparisons instead whole population operations prevents workers idling ﬁnish early. code detail methods described found supplementary section using strategy search large spaces complex image models requires considerable computation. achieve scale developed massively-parallel lock-free infrastructure. many workers operate asynchronously different computers. communicate directly other. instead shared ﬁle-system population stored. ﬁle-system contains directories represent individuals. operations individuals killing represented atomic renames directory. occasionally worker concurrently modify individual another worker operating case affected worker simply gives tries again. population size individuals unless otherwise stated. number workers always population size. allow long run-times limited amount space dead individuals’ directories frequently garbage-collected. result convolution output channels values within range possible. result models constrained number ﬁlters known work well. true parameters yielding dense search space. case strides applies log-base- value allow activation shapes match easily. principle also upper limit parameters. model depths attainable example. hardware constraints search space unbounded. dense unbounded nature parameters result exploration truly large possible architectures. every evolution experiment begins population simple individuals learning rate performers. initial individual constitutes single-layer model convolutions. conscious choice poor initial conditions forces evolution make discoveries itself. experimenter contributes mostly choice mutations demarcate search space. altogether poor initial conditions large search space limits experimenter’s impact. words prevents experimenter rigging experiment succeed. estimate computation costs identiﬁed basic tensorflow operations used model training validation like convolutions generic matrix multiplications etc. operations estimated theoretical number ﬂoating-point operations required. resulted operation flops valid experiments. individual within evolution experiment compute total flops incurred operations architecture batch examples training validation assign individual cost ftnt fvnv number training validation batches respectively. cost experiment costs individuals. intend flops measurement coarse estimate only. take account input/output data preprocessing graph building memory-copying operations. unaccounted operations take place training step component constant model size therefore expect estimate useful large architectures training validation done cifar- dataset. dataset consists training examples test examples color images labeled common object classes training examples held validation set. remaining examples constitute actual training set. training augmented cifar- dataset number dimensions colors examples cifar- uses classes making much challenging. training done tensorflow using momentum batch size weight decay training runs steps value chosen brief enough individual could trained seconds hours depending model size. loss function cross-entropy. training complete single evaluation validation provides accuracy individual’s ﬁtness. ensembling done majority voting testing evaluation. models used ensemble selected validation accuracy. need architectures trained completion within evolution experiment. happen forced retrain best model possibly having explore hyper-parameters. extra exploration tends depend details model retrained. hand steps enough fully train individual. training large model completion prohibitively slow evolution. resolve dilemma allow children inherit parents’ weights whenever possible. namely layer matching shapes weights preserved. consequently mutations preserve weights preserve none preserve all. example latter ﬁlter-size mutation ﬁlters convolution mutated discarded. avoid over-ﬁtting neither evolutionary algorithm neural network training ever testing set. time refer best model mean model highest validation accuracy. however always report test accuracy. applies choice best individual within experiment also choice best experiment. moreover include experiments managed reproduce unless explicitly noted. statistical analysis fully decided upon before seeing results experiment reported avoid tailoring analysis experimental data want answer following questions simple one-shot evolutionary process start trivial initial conditions yield fully trained models rival hand-designed architectures? variability outcomes parallelizabil algorithm designed iterating cifar- applied without changes cifar- still produce competitive models? used algorithm section perform several experiments. experiment evolves population days typiﬁed example figure ﬁgure also contains examples architectures discovered turn surprisingly simple. evolution attempts skip connections frequently rejects them. sense variability outcomes repeated experiment times. across experiment runs best model validation accuracy testing accuracy experiments reach accuracy close fine differences experiment outcome somewhat distinguishable validation accuracy total amount computation across experiments flops experiment distributed parallel workers figure shows progress experiments detail. control disabled selection mechanism thereby reproducing killing random individuals. form random search compatible infrastructure. probability distributions parameters implicitly determined mutations. control achieves accuracy amount time hardware total amount computation flops. flop count consequence random search generating many small inadequate models train quickly consume roughly constant amounts setup time attempted minimize overhead avoiding unnecessary disk access operations avail much overhead remains spent combination neural network setup data augmentation training step initialization. also partial control weight-inheritance mechanism disabled. also results lower accuracy amount time using flops. shows weight inheritance important process. finally applied neuro-evolution algorithm withchanges meta-parameters cifar-. experiment reached accuracy using flops. attempt datasets. table shows cifar- cifar- results competitive modern handdesigned networks. analysis meta-parameters. observe populations evolve plateau local optimum ﬁtness value optimum varies experiments since experiments reach highest possible value populations getting trapped inferior local optima. entrapment affected important meta-parameters population size number training steps individual. discuss consider relationship local optima. effect population size. larger populations explore space models thoroughly helps reach better optima note particular population size trapped ﬁtness values. intuition gained considering fate super-ﬁt individual i.e. individual architectural mutation reduces ﬁtness case population size super-ﬁt individual wins once every time. ﬁrst produce child mutation away. deﬁnition super-ﬁt therefore child inferior. consequently next round tournament selection super-ﬁt individual competes child wins again. cycle repeats forever population trapped. even sequence mutations would allow escape local optimum sequence never take place. rough argument heuristically suggest population size easily trapped. generally figure empirically demonstrates beneﬁt increase population size. theoretical analyses dependence quite complex assume speciﬁc models population dynamics; often larger populations better handling local optima least beyond size threshold references figure progress evolution experiment. represents individual population. blue dots alive. rest killed. four diagrams show examples discovered architectures. correspond best individual three ancestors. best individual selected validation accuracy. evolution sometimes stacks convolutions without nonlinearity mathematically equivalent single linear operation. unlike typical hand-designed architectures convolutions followed nonlinear function therein). effect number training steps. metaparameter number training steps individual. accuracy increases larger means individual needs undergo fewer identity mutations reach given level training. escaping local optima. might increase population size number steps prevent trapped population forming also free already trapped population. example increasing mutation rate resetting weights population work well quite costly recombination. none results presented used recombination. however explored three forms recombination additional experiments. following tuson ross attempted evolve mutation probability distribution too. this employed recombination strategy child could inherit structure parent mutation probabilities another. goal allow individuals progressed well good mutation choices quickly propagate choices others. separate experiment attempted recombining trained weights parents hope parent learned different concepts training data. third experiment recombined structures child fused architectures parents side-by-side generating wide models fast. none approaches improved recombination-free results study seems warranted. paper shown neuro-evolution capable constructing large accurate networks challenging popular image classiﬁcation benchmarks; neuro-evolution starting trivial initial conditions searching large space; process started needs experimenter participation; process yields fully trained models. completely training models required weight inheritance contrast reinforcement learning evolution provides natural framework weight inheritance mutations constructed guarantee large degree similarity befigure dependence meta-parameters. graphs circle represents result full evolution experiment. vertical axes show test accuracy individual highest validation accuracy experiment. populations evolved total wall-clock time. data points horizontal axis value. left effect population size. economize resources experiments number individual training steps note accuracy increases population size. right effect number training steps individual. note accuracy increases steps. figure escaping local optima experiments. used smaller populations fewer training steps individual make likely population trapped reduce resource usage. represents individual. vertical axis accuracy. example population size escaping local optimum using period increased mutation rate middle bottom example population size escaping local optimum means three consecutive weight resetting events details supplementary section figure repeatability results controls. plot vertical axis wall-time deﬁned test accuracy individual highest validation accuracy became alive inset magniﬁes portion main graph. curves show progress various experiments follows. line shows mean test accuracy across large-scale evolution experiments. shaded area around line width next line represents single experiment weight-inheritance disabled every individual train random weights. lowest curve random-search control. experiments occupied amount type hardware. small amount noise generalization validation test explains lines monotonically increasing. note narrow width area shows high accuracies obtained evolution experiments repeatable. focus reducing computation costs hope future algorithmic hardware improvement allow economical implementation. case evolution would become appealing approach neurodiscovery reasons beyond scope paper. example hits ground running improving arbitrary initial models soon experiment begins. mutations used implement recent advances ﬁeld introduced without restart experiment. furthermore recombination merge improvements developed different individuals even come populations. moreover possible combine neuro-evolution automatic architecture discovery methods. wish thank vincent vanhoucke megan kacholia rajat monga especially jeff dean support valuable input; geoffrey hinton samy bengio thomas breuel mark depristo vishy tirumalashetty martin abadi noam shazeer yoram singer dumitru erhan pierre sermanet xiaoqiang zheng carter vijay vasudevan helpful discussions; thomas breuel andy davis coding contributions; larger google brain team help tensorflow training vision models. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. bayer justin wierstra daan togelius julian schmidhuber j¨urgen. evolving memory cell structures international conference sequence learning. artiﬁcial neural networks springer gruau frederic. genetic synthesis modular neural networks. proceedings international conference genetic algorithms morgan kaufmann publishers inc. kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing humanprolevel performance imagenet classiﬁcation. ceedings ieee international conference computer vision kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv. fernando chrisantha banarse dylan reynolds malcolm besse frederic pfau david jaderberg lanctot marc wierstra daan. convolution evolution differentiable pattern producing networks. proceedings genetic evolutionary computation conference goldberg david richardson genetic algorithms sharing multimodal function optimization. genetic algorithms applications proceedings second international conference genetic algorithms hillsdale lawrence erlbaum krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems miller geoffrey todd peter hegde shailesh designing neural networks using genetic algorithms. proceedings third international conference genetic algorithms morgan kaufmann publishers inc. morse gregory stanley kenneth simple evolutionary optimization rival stochastic gradient descent neural networks. proceedings genetic evolutionary computation conference silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature simmons joseph nelson leif simonsohn uri. false-positive psychology undisclosed ﬂexibility data collection analysis allows presenting anything psychological science signiﬁcant. snoek jasper larochelle hugo adams ryan practical bayesian optimization machine learning algorithms. advances neural information processing systems szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. proceedings going deeper convolutions. ieee conference computer vision pattern recognition weyand tobias kostrikov ilya philbin james. planet-photo geolocation convolutional neural networks. european conference computer vision springer yonghui schuster mike chen zhifeng quoc norouzi mohammad google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. section contains additional implementation details roughly following order section short code snippets illustrate ideas. code intended highly edited clarity. implementation worker runs outer loop responsible selecting pair random individuals population. individual highest ﬁtness usually becomes parent lowest ﬁtness usually killed occasionally either actions carried order keep population size close set-point encoding individual represented serializable class instance containing information except trained weights results paper encoding directed acyclic graph edges represent convolutions vertices represent nonlinearities. sketch class ‘dna_proto‘ protocol buffer used restore state disk. together corresponding ‘to_proto‘ method allow serialization-deserialization mechanism. allows evolving learning rate i.e. exploring space learning rate schedules. self.learning_rate dna_proto.learning_rate parts graph prevented acted upon mutations. following boolean flags control this. self.inputs_mutable vertex_proto.inputs_mutable self.outputs_mutable vertex_proto.outputs_mutable self.properties_mutable vertex_proto.properties_mutable vertex represents block nodes. positive integers computed dynamically in-edges. stands \"scale\" spatial size activations. stands \"depth\" number channels. controls depth output relative input. example input edge depth channels ‘self._depth_factor‘ convolution result output depth channels. multiple-inputs conflicting depth must undergo depth resolution first. self.depth_factor edge_proto.conv.depth_factor control shape convolution filters parameterization ensures filter width height numbers filter_width filter_half_width self.filter_half_width edge_proto.conv.filter_half_width self.filter_half_height edge_proto.conv.filter_half_height controls strides convolution. ˆstride_scale. note conflicting input scales must undergo scale resolution. controls spatial scale output activations relative spatial scale input activations. self.stride_scale edge_proto.conv.stride_scale case depth scale resolution necessary conflicts inputs integer parameters determine inputs takes precedence deciding resolved depth scale. self.depth_precedence edge_proto.depth_precedence mutations instances. mutations restricts space explored somewhat following example mutations. alterlearningratemutation simply randomly modiﬁes attribute many mutations modify structure. mutations insert excise vertex-edge pairs build main convolutional column mutations remove edges handle skip connections. example addedgemutation skip connection random vertices. clarity omitted details vertex targeting mechanism based regular expressions used constrain additional edges placed. mechanism ensured skip connections joined points main convolutional backbone convnet. precedence range used give main backbone precedence skip connections resolving scale depth conﬂicts presence multiple incoming edges vertex. also omitted details attributes edge add. inputs=tensor decay=. center=true scale=true epsilon=self._batch_norm_epsilon activation_fn=none updates_collections=none is_training=self.is_training scope=’batch_norm’) scale_out scale depth_out edge.depth_out stride **edge.stride_scale ‘init_scale‘ used normalize initial weights case multiple incoming edges. weights_initializer slim.variance_scaling_initializer edge.filter_height] stride=stride weights_initializer=weights_initializer weights_regularizer=weights_regularizer biases_initializer=none activation_fn=none scope=’conv’) training evaluation done fairly standard similar tensorﬂow.org tutorials image models. individual’s ﬁtness accuracy held-out validation dataset described main text. parents able pass learned weights children child constructed parent inherits different sets trainable weights embedded tensorflow variable names. child’s weights initialized matching parent inherited provided shape section describes estimate number ﬂoating point operations required entire evolution experiment. obtain total flops flops individual ever constructed. individual’s flops training validation flops. namely individual flops given ftnt fvnv flops training step number training steps flops required evaluate validation batch examples number validation batches. number training steps number validation batches known advance constant throughout experiment. obtained analytically flops required compute operation executed training found analogously. iteration training collect metadata. metadata used determine nodes actually executed well argument shapes. run_meta tf.runmetadata tf.session sess unary math operations square squre root negation element-wise inverse softmax norm; binary element-wise operations addition subtraction multiplication division minimum maximum power squared entrapment local optimum mean general lack exploration search algorithm. encourage exploration increased mutation rate detail carried experiments ﬁrst waited populations converged. reached higher ﬁtnesses others trapped poor local optima. point modiﬁed algorithm slightly instead performing mutation reproduction event performed mutations. evolved increased mutation rate ﬁnally switched back original singlemutation version. -mutation stage populations escape local optimum figure none worse. across populations however escape frequent enough took long propose efﬁcient technique escape optima. interesting direction future work would study elegant methods manage exploration exploitation trade-off large-scale neuro-evolution. identity mutation offers mechanism populations trapped local optima. individuals trained peers happen undergone identity mutations. therefore occur poor architecture become accurate potentially better architectures still need training. extreme case well-trained poor architecture become super-ﬁt individual take population. suspecting scenario performed experiments simultaneously reset weights population plateaued simultaneous reset individuals footing individuals accidentally trained longer unfair advantage. indeed results matched expectation. populations suffer temporary degradation ﬁtness immediately reset individuals need retrain. later however populations reaching higher optima across experiments three successive resets tend cause improvement mention effect merely evidence particular drawback weight inheritance. main results circumvented problem using longer training times larger populations. future work explore efﬁcient solutions.", "year": 2017}