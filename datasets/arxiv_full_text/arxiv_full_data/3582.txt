{"title": "Brenier approach for optimal transportation between a quasi-discrete  measure and a discrete measure", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Correctly estimating the discrepancy between two data distributions has always been an important task in Machine Learning. Recently, Cuturi proposed the Sinkhorn distance which makes use of an approximate Optimal Transport cost between two distributions as a distance to describe distribution discrepancy. Although it has been successfully adopted in various machine learning applications (e.g. in Natural Language Processing and Computer Vision) since then, the Sinkhorn distance also suffers from two unnegligible limitations. The first one is that the Sinkhorn distance only gives an approximation of the real Wasserstein distance, the second one is the `divide by zero' problem which often occurs during matrix scaling when setting the entropy regularization coefficient to a small value. In this paper, we introduce a new Brenier approach for calculating a more accurate Wasserstein distance between two discrete distributions, this approach successfully avoids the two limitations shown above for Sinkhorn distance and gives an alternative way for estimating distribution discrepancy.", "text": "abstract. correctly estimating discrepancy data distributions always important task machine learning. recently cuturi proposed sinkhorn distance makes approximate optimal transport cost distributions distance describe distribution discrepancy. although successfully adopted various machine learning applications since then sinkhorn distance also suﬀers unnegligible limitations. ﬁrst sinkhorn distance gives approximation real wasserstein distance second ‘divide zero’ problem often occurs matrix scaling setting entropy regularization coeﬃcient small value. paper introduce brenier approach calculating accurate wasserstein distance discrete distributions approach successfully avoids limitations shown sinkhorn distance gives alternative estimating distribution discrepancy. machine learning pattern recognition data always work samples. example image classiﬁcation sample image. given ﬁxed type representation images consider space images dimension represents feature image. consider certain images distribution space images. situation easy estimate kind distributions continuous probability distribution. firstly usually access ﬁnite number training samples certain compared large number dimensions image space training samples always enough estimating continuous probability function secondly categories deﬁned semantic concepts highly abstract cannot ensure continuous distributions nature. example therefore machine learning society commonly adopted methods measuring distance distributions usually sample based methods consider distributions discrete measures paper discuss application brenier approach calculating optimal transportation quasi-discrete measure discrete measures approach introduced assumes target measure discrete source measure continuous. estimate brenier function gradient objective energy need estimate graph brenier potential calculate integration source distribution. could done dimensional dimensional spaces reasonable time calculation becomes hard spaces dimensions. therefore considering source measure quasi-discrete distribution wish solve discrete problems machine learning time avoid problem calculation cost higher dimensional spaces. however side eﬀect quasi-discrete measure gets close discrete measure always guaranteed converge solution preserves measures. following part paper ﬁrstly introduce brenier approach optimal transportation quasi-discrete measure discrete measure gradient descent algorithm solving problem. compare brenier approach sinkhorn approach discuss advantages disadvantages. also show possible application brenier approach clustering. theorem brenier variational approach assumes source measure absolutely continuous support source measure convex true therefore introduce piecewise uniform measure compact support could seen approximation probability density function deﬁned small values. probability density uniformly distributed small hypercube volume around source sample total probability mass small hypercube deﬁned probability mass associated center sample minus small value p/ns. apart small hypercubes around source samples probability uniformly distributed total mass rest volume could deﬁned smallest hypercube area contains source samples. measure apply theorem brenier |x−x| exists convex function gradient gives solution monge’s problem unique. convex function called brenier potential solution monge-ampère equation. authors give variational approach solve equation equivalent alexandrov theorem. introduce approach hyperplane forms piecewise linear convex function problem vector polygonal partition {wi}nt source support induced projection measure preserving. authors prove solutions problem critical points following energy function authors prove convex admissible space convex energy moreover unique global minimum interior point gradient induced minimum unique optimal mass transport minimizes total transportation cost furthermore since source measure probability masses concentrated small areas around source samples wish simplify calculation estimating source samples instead possible points therefore deﬁne approximation follows make diﬃcult calculate cell areas. solve problem currently brute force method simply uniformly distribute mass correspond source sample onto cell areas correspond target samples therefore section compare proposed approximate brenier approach sinkhorn approach. diﬀerences listed follows step approximate brenier method condition always holds approaching real number iteration grows. hand sinkhorn method matrix balancing method conditions hold alternatively iterations tend hold algorithm converges. iteration step approximate brenier method current transportation always optimal transportation tns. sinkhorn method solves entropy regularized version problem. therefore optimal solution original problem coeﬃcient regularization term tends zero. however hard achieve tends zeros matrix balancing becomes instable easily face zero denominator error. words entropy regularized problem demands transportation plan sparse since matrix balancing need matrix positive entries. fortunately don’t problem approximate brenier method. transportation learned brenier method tend transport source sample whole target sample hand transportation learned sinkhorn method tend split source sample parts transport group target samples. thing common methods handle abstract distributions i.e. don’t need know number dimensions sample space. sinkhorn need prepare vectors sample weights distance matrix approximate brenier need prepare vectors sample weights inner-product matrix characteristic makes methods ﬂexible diﬀerent kinds applications. sinkhorn method much faster brenier method. approximate brenier method hard converge unless number source samples much larger number target samples. problem sinkhorn method. sinkhorn method constraints choice cost metric currently brenier method works quadratic euclidean distances. simple experimentation compare methods source sample samples gaussian mixture distribution target sample samples. sinkhorn regularization coeﬃcient quadratic euclidean distance cost metric methods resulting sinkhorn distance calculation time seconds; brenier distance calculation time seconds. brenier distance smaller sinkhorn distance meaning transportation learned brenier method better learned sinkhorn method. also performed simple linear programming method solve problem resulting distance calculation time seconds. perform even smaller experiment show maps learned diﬀerent methods consider source samples gaussian mixture distribution target samples. quadratic euclidean distance cost metric. sinkhorn method. results shown following means resulting wasserstein distance transportation represents brenier represents sinkhorn ‘lp’ represents linear programming. results brenier method learns simple elegant optimal transportation map. clustering task given unlabeled sample demanded divide sample clusters samples cluster close samples diﬀerent clusters other. assume given sample source assume cluster centers need learn target case transportation target distribution unknown variables. therefore iterative approach ﬁrstly initialize target samples randomly learn intercepts target samples minimize wasserstein distance given samples cluster centers i.e. minimize since matrix positive elements convex respect input therefore simple gradient descent algorithm learn show simple experiment following given samples gaussian mixture distribution ﬁrstly initialize cluster centers randomly assume distribution mass associated cluster center perform steps gradient descent updating cluster centers step perform approximate brenier method learn current optimal transportation map. step samples transported cluster center considered cluster. ﬁgure show initialized clusters cluster centers ﬁgure illustrate resulting clusters corresponding centers step. steps learned cluster centers nicely located center gaussian distribution. paper show simple application brenier approach approximately solving discrete learning problems. method time consuming continues brenier approach still slower methods like sinkhorn even linear programming. although currently perfect still shining points therefore hope improve method applications machine learning computer vision.", "year": 2018}