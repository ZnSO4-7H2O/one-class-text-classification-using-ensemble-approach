{"title": "Espresso: Efficient Forward Propagation for BCNNs", "tag": ["cs.DC", "cs.CV", "cs.LG", "cs.NE", "62M45", "I.2.6"], "abstract": "There are many applications scenarios for which the computational performance and memory footprint of the prediction phase of Deep Neural Networks (DNNs) needs to be optimized. Binary Neural Networks (BDNNs) have been shown to be an effective way of achieving this objective. In this paper, we show how Convolutional Neural Networks (CNNs) can be implemented using binary representations. Espresso is a compact, yet powerful library written in C/CUDA that features all the functionalities required for the forward propagation of CNNs, in a binary file less than 400KB, without any external dependencies. Although it is mainly designed to take advantage of massive GPU parallelism, Espresso also provides an equivalent CPU implementation for CNNs. Espresso provides special convolutional and dense layers for BCNNs, leveraging bit-packing and bit-wise computations for efficient execution. These techniques provide a speed-up of matrix-multiplication routines, and at the same time, reduce memory usage when storing parameters and activations. We experimentally show that Espresso is significantly faster than existing implementations of optimized binary neural networks ($\\approx$ 2 orders of magnitude). Espresso is released under the Apache 2.0 license and is available at http://github.com/fpeder/espresso.", "text": "many applications scenarios computational performance memory footprint prediction phase deep neural networks need optimized. binary deep neural networks shown effective achieving objective. paper show convolutional neural networks implemented using binary representations. espresso compact powerful library written c/cuda features functionalities required forward propagation cnns binary less without external dependencies. although mainly designed take advantage massive parallelism espresso also provides equivalent implementation cnns. espresso provides special convolutional dense layers bcnns leveraging bit-packing bitwise computations efﬁcient execution. techniques provide speed-up matrix-multiplication routines time reduce memory usage storing parameters activations. experimentally show espresso signiﬁcantly faster existing implementations optimized binary neural networks espresso released apache license available http//github.com/fpeder/espresso. convolutional neural networks revolutionized computer vision pushing task object recognition beyond human capabilities deep neural networks also successfully applied ﬁelds speech recognition automated translation despite achieving impressive classiﬁcation accuracy results dnns require much memory power used effectively embedded low-power devices. many networks consume considerable amount memory. memory remains limited resource mobile platforms making harder usage trained dnns even memory issue dnns remain computationally intensive quickly drain battery. reducing computational load improve energy efﬁciency also enable applications. example processing real-time object classiﬁcation mobile able perform faster predictions frees computational resources spent tasks speech recognition analysis. therefore substantial interest reducing computational memory requirements dnns. efﬁcient deep neural networks achieve target specialized hardware dnns. another strategy reduce network’s memory footprint associated computation hence increasing efﬁciency. solutions preferable implemented software without requiring specialized hardware. research follow software approach focus attention quantized networks. case parameters stored small integers instead single precision ﬂoating point numbers particular consider binary deep neural networks proposed hubara parameters activations -bit integers expense relatively small decrease accuracy bdnns considerably reduce memory usage result faster execution time further note potential hardware implementation bdnns would also cheaper reduced number required fpus. results highly promising currently proof-of-concept implementations binarynets published therefore availability ﬂexible end-to-end framework particular emphasis placed computational efﬁciency enable research bdnns well application practical scenarios. contributions espresso provide optimized framework bdnns capable achieving state-of-the-art run-time performance minimal memory footprint numerical equivalent non-optimized binary counterpart. espresso provides complete optimized framework bdnns supporting dense convolutional layer. current state-ofthe-art optimized bdnns implementations limited fully connected layer serious drawback able optimized state-of-art convolutional bdnns work necessary stepping stone towards optimization training routines paper focus optimization forward-propagation rather back-propagation espresso designed external dependencies. results highly optimized implementation bdnns also substantially simpliﬁes deployment practical applications executing mobile embedded devices. improving performance dnns achieved either hardware software level. hardware level chipsets dedicated execution outperform general-purpose cpus/gpus software level approach design simpler architectures terms overall ﬂoating point operations offer accuracy original model another approach prune weights even entire ﬁlters impact activations simpler model derived. simpliﬁed models compressed weight sharing finally instead removing connections another approach quantize network weights computations executed efﬁciently. quantized networks objective train dnns whose weights signiﬁcantly impact network’s classiﬁcation accuracy. example courbariaux show -bits enough maxout networks efﬁcient multiplications performed ﬁxed-point arithmetic. continuing trend aggressive quantization schemes ternary also studied. binary deep neural networks recently courbariaux showed network binary weights achieve near state-of-the-art results several standard datasets. binary dnns shown perform effectively datasets relatively small images permutation-invariant mnist cifar- svhn recently rastegari show binarized cnns perform well even massive datasets imagenet using binarized versions well-known architectures alexnet resnet- googlenet similarly interesting results achieved binarizing weights activations showed hubara work authors introduce binarynet technique effectively train dnns weights activations constrained binarynet achieves nearly state-of-the-art accuracy training mnist training cifar-. authors also propose binary optimized implementation matrix multiplication result faster performance base-line optimized implementation almost faster theano core contributions namely replace floatingpoint multiply operations xnors bit-counts represent cornerstone build research. espresso provides user necessary tools executing forward-propagation dnns particular emphasis placed convolutional neural networks ubiquitousness computer vision applications. complexity networks cubic size problem less memory efﬁcient computationally intensive traditional machinelearning algorithms. identifying memory computational bottlenecks dnns therefore essential enable practical application. particular primary focus gpu-optimized bdnn architectures refer gpuopt also support equivalent ﬂoatingpoint counterparts heterogeneous architectures refer gpu. implementations espresso feature binary optimizations data encoded single precision ﬂoating point numbers. however still utilize optimized library matrix multiplication. hybrid dnns espresso’s implementations tensors layers come three variants {cpu gpuopt}. cpu-tensor allocated memory processed using sequential code. gpu-tensor allocated main memory processed cuda kernels. espresso provides functions converting tensors layers variant other different variants also interconnected other. consequently espresso enables design hybrid dnns consisting combination {cpu gpuopt} layers. computational bottleneck products dense linear algebra heart deeplearning deep networks viewed composition matrix-matrix matrix-vector elementwise matrix-matrix vector-vector multiplications. implementation dense linear algebra operations relies heavily efﬁcient computation dot-product. execution operator consists floating-point multiply operations. modern architectures ﬂoating-point multiplications executing dominate complexity fmas bdnns address concerns replacing fmas simpler bitwise operations; section technical highlights superior computational performance espresso derives three main technical contributions bit-packing network layers better memory layout management custom optimized cuda kernels. bitpacked layers espresso execute forward operation without need expensive memory re-arrangements employed existing implementations. dynamic memory allocation gpus performance bottleneck espresso implements custom memory allocator pre-allocates memory start-up replaces traditional malloc free system calls. finally matrix multiplications performed cuda kernels adapted bit-packing resort xnors bit-counts. section overview fundamental characteristics bdnns inform basics espresso’s design. bdnns computationally intensive operations replaced xnor bit-count enabling signiﬁcant computational speed-ups. particular xnor simpler machine instruction compared ﬂoating point multiplication therefore achieves much higher throughput many architectures. importantly single xnor step execute multiple -bit wide blocks dot-products increasing overall computational efﬁciency. follows describe network binarized detail compressed memory layout enabling efﬁcient execution dot-products show re-interpret input data allow execution ﬁxed-precision input provide notes regarding training procedure. weights activations hardware level must encoded convention encode amongst many possible choices e.g. stochastic binarization employ following activation function efﬁcient implementation weights bdnn stored bits -bit word. immediate advantage bit-packing drastically reduce memory usage factor. even signiﬁcant advantage ability process multiple values time using registers. particularly useful dot-products bit-packing compute dot-product element vectors using xnor bit-count. furthermore modern computer architectures provide hardware instruction counting number bits given word. assuming binary vectors multiple dot-product equivalent represents bit-shift operator. simple computation becomes building block optimized bdnns binary matrix-matrix matrix-vector operations computed fashion. bdnns require binary input data typically available ﬁrst layer network. however input data usually comes ﬁxed precision format therefore optimized computation dot-products still applied split input data according bit-planes back contribution according corresponding weight. instance hain indicate n-th ﬁxed precision vector corresponding bit-plane obtain training bdnn important note gradient computed binary weights accumulated ﬂoating point precision optimizer needs sufﬁcient precision make reliable update. addition derivative sign function zero almost everywhere cannot used back-propagation. overcome issues straight-through estimator employed back-propagated ﬂoating point argument otherwise. finally training weights clipped avoid large growth ﬂoating point weights would impact binary weights. principal components framework tensors layers network. components organized hierarchy. tensors dimensional matrices used storing inputs weights activations layer processes input tensor produces output tensor network consists concatenation layers. notation indicate channels element. using storing scheme espresso also deﬁnes bit-packed tensors gpuopt implementations following changes increase performance. bit-packing performed according number channels bit-packing done along dimension; bitpacking done along dimension. convolutional layers packing direction enables efﬁcient memory access unrolling/lifting tensor would possible either chosen instead. speciﬁcally layout optimal retrieving pixel neighborhood needed convolution without requiring layout changed. further typically large number ﬁlters used resulting increase tensor dimension direction dimensions progressively shrunk pooling layers. layer types efﬁcient packing direction neurons stored along rows number decreases move toward later stages network. espresso provides following layer types input convolutional pooling dense batch-normalization. layer characterized size tensor parameters output. espresso deﬁnes layer forward function computes output layer given input tensor function applying non-linearity outputs convolutional dense layers. moreover convolutional layer features additional functions pooling unrolling. convolutional layers framework convolutions computed matrix multiplications operation involving high reuse data. computation performed sectioning data amounts cache-friendly resulting implementations attaining close peak computational performance. however order express convolution matrix multiplication need re-organize input memory appropriately. achieved unrolling procedure; figure consists transforming tensor matrix formed unrolling tensor data contained convolution sliding volume. unrolled matrix multiplied ﬁlter matrix. finally result convolution reordered back tensor using lifting procedure. espresso need manually lift convolution result order undo unrolling thanks tensor representation happens automatically zero cost. espresso provides cuda kernels unrolling pooling tensors gpuopt implementations. efﬁcient matrix multiplication matrix-vector multiplications fundamental operations dense layers. architecture openblas library implement operations. gpuand gpuoptarchitectures cuda kernels based magma modiﬁed make compatible binary data representation. kernels matrix multiplication feature register blocking optimization since introduction fermi architectures number registers increased register access latency substantially reduced compared shared-memory; hence caching register-memory level results considerably faster throughput espresso ﬁrst fetches tiles matrices shared-memory process sub-tiles using registers. gpuopt variant modify code replacing blocks single precision multiply operations xnor bit-count using packed tensors. also re-tune kernel block size parameters improving performance reduced size matrices. zero-padding convolutions typical implementations apply tensor convolution same conﬁguration sizes input output tensors matches. achieved zeropadding input tensors convolutional gpuopt layers zero-padding input introduces side-effect making data ternary deal problem treating data binary results convolution corner-cases post-processing. allows leave convolution kernel code computational bottleneck code untouched. corner-cases ﬁxed using highly efﬁcient kernel executes element-wise results convolution correction matrix. correction matrix computed once gpuopt layer loaded simply consists convolution layer’s weights -padded zero-tensor. converting network espresso espresso deﬁned combination layers loaded run-time reading parameters ﬁle. parameters speciﬁes storage format layers well weights. therefore completely speciﬁes layers stored sequentially. training network done binarynet resulting parameters converted espresso format utility script distributed together sources. performance framework evaluated terms average computational time needed perform particular task. execution times averaged experiments obtained machine equipped nvidia geforce intel dual-xeon ghz. mode conﬁgure openblas library matrix multiplication available cores. experimental design perform three quantitative evaluations matrix multiplications dense square matrices size forward-propagations multi-layer perceptron trained mnist dataset forward-propagations convolutional neural network trained cifar- dataset using models datasets compare espresso with author provided optimized implementation binarynet optimized bdnn implemented intel nervana neon framework self-comparison across {cpu gpuopt} binary-optimized implementations convolutional layers publicly available. espresso numerically equivalent binarynet terms classiﬁcation accuracy. therefore evaluation focuses computation speed. public datasets mnist dataset consists instances training instances testing. instance grayscale image depicts digits ranging cifar- dataset consists training instances testing instances color images. images subdivided classes since interest asses real-time performance binary optimized dnns experiment batch-size measure averaged forward time image testing-sets dataset. computing dense matrix multiplication espresso outperforms binarynet factor. much gain attributed optimized kernels register blocking fetching bigger data main memory shared memory kernel increases bandwidth utilization decreasing number memory fetch instructions. -bit packing instead -bit introduces additional performance improvement. -bit kernel achieves memory dram throughput reads writes -bit kernel obtain reads writes. translates resulting speed improvement. evaluate average classiﬁcation execution time mnist dataset trained architecture author-provided sources converted espresso’s format. table espresso achieves consistent speed-up compared binarynet. nervana/neon implementation binary network binarynet derivative affected drawbacks binarynet hence achieves comparable performance. alternatives additional cost running cuda python/theano introduce latency process. table evaluation three variants espresso shows expected outcome gpuopt implementation leading ranking. note able achieve speedup nvidia although device roughly four times throughput xeon binary optimization able increase performance respect implementation. attribute computational gains binary-optimized layers optimized kernels matrix multiplication espresso’s ability perform binary optimization ﬁrst layer. binary optimized layers evident drawback binary-net need binarizing/packing layer’s parameters every time forward method called. case binary optimized networks cost packing parameters closely related cost multiplication itself. therefore reduction bit-packing function calls leads consistent improvement. motivates choice designing speciﬁc layers bit-packing done network loading. optimized kernels binarynet employs bit-packing kernels row-packing column-packing. although binarynet’s pack-by-rows kernel slightly slower pack-by-columns kernel signiﬁcantly slower non-coalesced accesses global memory. additional performance gain achieved swapping matrix-vector favour matrix-matrix multiplication kernels appropriate reason espresso also includes binary-optimized magma kernel. first-layer binary optimization another important advantage offered espresso ability leverage binary optimization ﬁrst layer. since ﬁrst stage network processes nonbinary data binarynet feature binary optimization layer. however input data split constituent bit-planes binary optimization still applied. particular split input vector matrix rows recombine result multiplication weighted sum. experimental results report overall performance boost comparing full binary optimized network ﬁrst layer binary optimized. best knowledge bdnn implementation binary-optimized layers publicly available. self-evaluation implements vggnet-like architecture hubara evaluates across three modalities expected gpuopt implementation achieves signiﬁcantly better performance. unrolling pooling note implementation offers slightly better improvement respect test speed-up. experiment inherent parallelism unrolling pooling higher memory throughput explain behavior. gains marginal still represents computational bottleneck. bit-packing gpuopt implementation results performance gain respect gpu. gains binary optimizations slightly smaller discussed section output convolutional layers signiﬁcantly larger mlp’s dense layers therefore computation bit-packing sign-activation requires computational effort. paper presented espresso highly optimized forward-propagation framework traditional dnns well bcnns supports heterogeneous deployment gpu. binarynet nervana/neon bdnn implementations limited networks framework also supports popular simultaneously outperforming state-of-the-art implementations networks. espresso highly-efﬁcient light-weight self-contained. computation side done though speciﬁcally designed cuda kernels which combined careful handling memory allocation bit-packing allows obtain considerable performance improvements. future work would like training capabilities perform additional performance comparisons larger standard datasets. ahmad abdelfattah azzam haidar stanimire tomov jack dongarra. performance design autotuning batched gemm gpus. international conference high performance computing springer http//icl.cs.utk.edu/magma/ james bergstra olivier breuleux frédéric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math compiler python. proc. python science conf matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems matthieu courbariaux itay hubara daniel soudry el-yaniv yoshua bengio. binarized neural networks training deep neural networks weights activations constrained or-. arxiv preprint arxiv. deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee alex graves abdel-rahman mohamed geoffrey hinton. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. song xingyu huizi jing ardavan pedram mark horowitz william dally. efﬁcient inference engine compressed deep neural network. proceedings international symposium computer architecture ieee press kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine itay hubara matthieu courbariaux daniel soudry el-yaniv yoshua bengio. binarized neural networks. advances neural information processing systems https//github.com/matthieucourbariaux/binarynet. forrest iandola song matthew moskewicz khalid ashraf william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters and< model size. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems rajib nath stanimire tomov jack dongarra. improved magma gemm fermi graphics processing units. international journal high performance computing applications yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. european conference computer vision springer christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition", "year": 2017}