{"title": "Inducing Semantic Representation from Text by Jointly Predicting and  Factorizing Relations", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.", "text": "work propose method integrate recent lines work unsupervised induction shallow semantics factorization relations text knowledge bases. model consists components encoding component semantic role labeling model predicts roles given rich syntactic lexical features; reconstruction component tensor factorization model relies roles predict argument ﬁllers. components estimated jointly minimize errors argument reconstruction induced roles largely correspond roles deﬁned annotated resources. method performs accurate role induction methods english even though unlike previous approaches incorporate prior linguistic knowledge language. shallow representations meaning semantic role labels particular long history linguistics recently emergence large annotated resources propbank framenet automatic semantic role labeling attracted attention semantic role representations encode underlying predicate-argument structure sentences speciﬁcally every predicate sentence identify arguments associate argument underlying semantic role agent patient consider following sentence here police demonstrators batons assigned roles agent patient instrument respectively. semantic roles many potential applications shown beneﬁt example question answering textual entailment among others. scarcity annotated data motivated research unsupervised learning semantic representations existing methods number serious shortcomings. first make strong assumptions example assuming arguments conditionally independent given predicate. second unlike state-of-the-art supervised parsers rely simplistic features sentence. factors lead models being insufﬁciently expressive capture syntax-semantics interface inadequate handling language ambiguity overall introduces upper bound performance. work propose method effective unsupervised estimation feature-rich models semantic roles. demonstrate reconstruction-error objectives shown effective primarily training neural networks well suited inducing feature-rich log-linear models semantics. model consists components log-linear feature rich semantic role labeler tensor-factorization model captures interaction semantic roles argument ﬁllers. method rivals accurate semantic role induction methods english importantly prior knowledge speciﬁc language incorporated feature-rich model whereas clustering counterparts relied language-speciﬁc argument signatures. core approach statistical model encoding interdependence semantic role structure realization sentence. unsupervised learning setting sentences syntactic representations argument positions observable whereas associated semantic roles latent need induced model. crucially good encode roles rather form abstraction. follows refer roles using names though unsupervised setting method latent variable model yield human-interpretable labels them. also focus labeling stage semantic role labeling. identiﬁcation though important problem tackled heuristics unsupervised techniques potentially using supervised classiﬁer trained small amount data. model consists components. ﬁrst component responsible prediction argument tuples based roles predicate. experiments component represent arguments lemmas lexical heads also restrict verbal predicates. intuitively think predicting argument time argument predicted based predicate lemma role assigned argument role-argument pairs learning predict arguments inference algorithm search role assignments simplify prediction task much possible. hypothesis assignments correspond roles accepted linguistic theories hypothesis plausible? primarily semantic representations introduced abstraction capturing crucial properties relation thus representations rather surface linguistic details like argument order syntactic functions crucial modeling sets potential argument tuples. reconstruction component part model. crucially referred ‘searching role assignments simplify argument prediction’ would actually correspond learning another component semantic role labeler predicts roles relying rich sentence features. components estimated jointly minimize errors recovering arguments. role labeler end-product learning used process sentences compared existing methods evaluation. generative modeling learn latent representations. alternative popular neural network community autoencoders instead optimize reconstruction error encoding model feature-rich classiﬁer predicts semantic roles sentence reconstruction model model predicts argument given role given rest arguments roles. idea training linear models reconstruction error previously explored daum´e recently ammar however consider learning factorization models also deal semantics. tensor factorization methods used context modeling knoweldge bases also close spirit. however deal inducing semantics rather factorize existing relations feature vector encoding interactions sentence semantic role representation model used long posterior distributions roles efﬁciently computed approximated. experiments used model factorizes individual arguments reconstruction component predicts argument given semantic roles predicate arguments bilinear softmax model rd×k model parameters partition function ensuring probabilities one. intuitively embeddings encode semantic properties argument example embeddings words demonstrator protestor somewhere near space away word cat. product cprua k-dimensional vector encoding beliefs arguments based argument-role pair turn product cvrj large argument pair semantically compatible predicate small otherwise. intuitively objective corresponds scoring argument tuples according hinting connections tensor factorization methods distributional semantics note also reconstruction model access features sentence forcing roles convey necessary information. however optimizing objective practical exact form reasons marginalization exponential number arguments; partition function requires summation entire potential argument lemmas. existing techniques address challenges. order deal ﬁrst challenge basic mean-ﬁeld approximation instead marginalization substitute posterior distributions tackle second problem computation negative sampling technique test time linear semantic role labeler used inference straightforward. followed lang lapata used conll shared task data previous work unsupervised evaluate model using clustering metrics purity collocation harmonic mean semantic role labeling component relied feature patterns used argument labeling popular supervised role labelers resulted quite large feature space reconstruction component dimensionality embeddings projection dimensionality number negative samples respectively. hyperparameters tuned held-out data chosen among constraining always greater ﬁxed model sensitive parameter deﬁning number roles long large enough. training used uniform random initialization adagrad following baseline simply clusters predicate arguments according dependency relation head. separate cluster allocated frequent relations dataset additional cluster used relations. observed previous work hard baseline beat. also compare previous approaches latent logistic classiﬁcation model agglomerative clustering method graph partitioning approach global role ordering model also report results improved version agglom recently reported lang lapata strongest previous model bayes bayes accurate version bayesian model titov klementiev estimated conll data without relying external data. model outperforms performs best previous models terms interestingly purity collocation balance different model rest systems. fact model induces roles. contrary bayes predicts roles majority frequent predicates though tendency reduces purity scores model also means roles human interpretable. example agents patients clearly identiﬁable model predictions. introduced method inducing feature-rich semantic role labelers unannoated text. approach view semantic role representation encoding latent relation predicate tuple arguments. capture relation probabilistic tensor factorization model. estimation method yields semantic role labeler achieves state-of-the-art results english. f¨urstenau rambow unsupervised induction syntax-semantics lexicon using iterative first joint conference lexical computational semantics-volume main hajiˇc ciaramita johansson kawahara mart´ı m`arquez meyers nivre pad´o ˇstˇep´anek straˇn´ak surdeanu zhang conll- shared task syntactic semantic dependencies multiple languages. conll.", "year": 2014}