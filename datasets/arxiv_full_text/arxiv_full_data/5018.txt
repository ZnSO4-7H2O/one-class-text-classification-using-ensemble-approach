{"title": "Deep Abstract Q-Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.", "text": "desired room locked known location another room domain. agent must navigate several rooms retracing steps door unlock learning navigate individual room challenging learning policy traverse multiple rooms much harder. complete solution presently reach number promising attempts improving long-term planning deep reinforcement learning agents. approaches divided categories intrinsically motivate agent explore portions state-space exhibit form novelty exploit kind abstraction divide learning problem manageable subparts approaches suffer drawbacks. noveltybased approaches indeed encourage exploration. however intrinsic drive toward underexplored states tends interfere agent’s ability form long-term plans. result agent able rooms unable make plan pick unlock door. abstraction-based approaches focus end-to-end learning abstractions resulting sub-policies hindered extremely difﬁcult optimization problem. moreover given lack strong theoretical underpinnings goodness abstraction little external guidance provided optimization scheme. tackle domains long horizons sparse rewards propose following method experimenter provides lightweight abstraction consisting factored high-level states agent. employ formalism abstract markov decision process divide given domain symbolic highlevel representation learning long-term policies pixel-based low-level representation leverage recent successes deep-learning techniques. example high-level representation would current room agent whether agent lowlevel representation would pixel values image. aforementioned factoring decomposes symbolic high-level state collections state-attributes asexamine problem learning planning highdimensional domains long horizons sparse rewards. recent approaches shown great successes many atari domains. however domains long horizons sparse rewards montezuma’s revenge venture remain challenging existing methods. methods using abstraction shown useful tackling long-horizon problems. combine recent techniques deep reinforcement learning existing model-based approaches using expertprovided state abstraction. construct domains elucidate problem long horizons sparse rewards high-dimensional inputs show algorithm significantly outperforms previous methods domains. abstraction-based approach outperforms deep q-networks montezuma’s revenge venture exhibits backtracking behavior absent previous methods. recent advances deep learning enabled training reinforcement learning agents high-dimensional domains. popularly demonstrated mnih research training deep q-networks play various atari games. performance attained mnih spans impressive subset atari library several complicated games remain reach existing techniques including notoriously difﬁcult montezuma’s revenge venture. anomalously difﬁcult domains exhibit sparse reward signals sprawling partially-observable mazes. conﬂuence traits produces difﬁcult games beyond capabilities existing deep techniques solve. spite considerable challenges games closest analogs real-world robotics problems since require agent navigate complex unknown environment manipulate objects achieve long-term goals. example long-horizon problem could domain agent tasked navigating series cluttered rooms visual input. door enter copyright association advancement artiﬁcial intelligence rights reserved. sociated predicate functions manner similar object oriented mdps factoring allows treat actions high-level domain changes attributes predicates rather state-tostate transitions avoiding combinatorial explosion action space number objects increases. example retrieved agent re-learn navigate room room; holding generally change agent navigates. work detail method combining recent techniques deep reinforcement learning existing model-based approaches using expert-provided state abstraction. illustrate advantages method versions room navigation task designed exhibit long horizons sparse reward signals high-dimensional inputs. show experimentally method outperforms deep q-networks competing novelty-based techniques domains. finally apply approach atari montezuma’s revenge venture show outperforms exhibits backtracking behavior absent previous methods. subgoals abstraction common approaches decreasing problem horizons allowing agents efﬁciently learn plan long-horizon domains. earliest reinforcement learning methods using ideas maxq decomposes hierarchy subtasks. subtask accompanied subgoal completed. policy individual subtasks easier compute entire task. additionally maxq constrains choice subtasks depending context parent task. drawback method plans computed recursively requiring transition reward function models self-contained. limitation forces single learning algorithm high-level low-level. approach avoids problem allowing deep reinforcement learning algorithms low-level handle high-dimensional input model-based algorithms high-level create long-term plans guide exploration. temporally extended actions options commonly used approaches decreasing problem horizons bundles reusable segments plans tractable form. learning options highdimensional domains atari games challenging recently performed option-critic option-critic however fails show improvements long-horizon domains montezuma’s revenge venture. work seek learn sub-policies high-level policy. using deep q-learning. h-dqn divided lowlevel controller high-level meta-controller. important note tiers operate different timescales meta-controller specifying long-term manuallyannotated goals controller focus completing short-term. pattern high-level entity providing intrinsic reward low-level agent also explored vezhnevets feudal network. unlike hdqn feudal network rely user-provided goals opting learn low-level worker high-level manager parallel manager supplying vector learned goal-embedding worker. method able achieve higher score montezuma’s revenge previous methods fails explore many rooms novelty-based methods. contrast approach provides abstraction agent allowing leverage existing model-based exploration algorithms rmax enabling agent create long-term plans explore rooms. addition methods rely goal-based form intrinsic motivation work generally motivating agents explore environment. particularly bellemare derive pseudo-count formula approximates naively counting number times state occurs. pseudo-counts generalize well high-dimensional spaces illuminate degree different states explored. using information bellemare able produce reward-bonus encourage learning agents visit underexplored states; method referred intrinsic motivation approach shown explore large portions method able explore signiﬁcantly better still fails execute long-term plans collecting keys unlock doors example collecting ﬁrst agent ends current life rather retracing steps unlocking door allowing retain returning starting location much closer doors. counterintuitive behavior occurs factorization state-space bellemare renders presence agent’s position independent resulting pseudo-counts along path back door still relatively large compared states near key. thus corresponding exploration bonuses backtracking lower remaining near key. therefore environment terminated single life method would never learn leave ﬁrst room. phenomenon illustrated single-life results figure similarly venture agent collected item rooms novelty room encourages remain room instead collecting four items thereby completing level. contrast method allows agent learn different policy collects item after order systematically item explore farther without without dying. schema networks used modelbased object-oriented approach improve transfer across similar atari domains. method however able learn high-dimensional image data provides states actions taken function representing reward incurred transitioning state state taking action function representing probability transitioning taking action terminal states that reached prevent future action. formalism represents environment acted upon agent. agent takes actions receives reward updated state environment. reinforcement-learning problems agents learn policies maximize reward time. success typically measured discounted reward value acting policy given state allow agent learn plan abstract level employ abstract markov decision process formalism presented gopalan amdp hierarchy mdps allowing planning environments various levels abstraction. formally node hierarchy deﬁned augmented tuple mirror standard components deﬁned state projection function maps lower-level states abstract representations one-level hierarchy every represents another augmented base environment action. concrete example consider environment containing four connected rooms. simple two-tiered amdp hierarchy might treat entire rooms abstract states transitioned between. abstract actions performing transitions would mdps hierarchy perform low-level actions goal moving rooms. describe hierarchical system learning agents exhibit long-term plans. approach involves learning coupled agents simultaneously high-level l-agent low-level l-agent. formalism described above deﬁning l-agent’s environment l-agent’s environment also denote state projection function mapping l-states corresponding l-states abstract states actions allow agent plan higher level project ground level states much lower dimensional abstraction agent. l-agent’s abstraction speciﬁed three elements abstract states factored abstraction-attributes represent independent state components; predicate functions used specify dependencies interactions between particular values abstraction-attributes; state projection function ground abstract symbols sets environment states. precisely number factors abstract state number predicate functions provided abstract states. alternatively write emphasize factors write denote predicate functions unfactored domain action taken intent transitioning state state thought symbolically ordered pair since predeﬁned structure variation either state however slight mandates symbolic action. particularly expensive agents acting across multiple levels abstraction need explicitly learn perform symbolic action low-level domain. mitigate learning-cost factorization imposed abstraction-attributes. given state assume independent represent l-action ordered intended attribute changes performing refer representation attribute difference deﬁne formally tuple entries practice seldom case abstract attributes completely independent. allow modeling dependencies certain attributes predicate functions described augment previous notion l-actions independent attributes representing actions tuples attribute differences evaluated predicate functions pl)) interactions agents order agents learn transition abstract states need deﬁne reward function terms abstract states. important note that much like kulkarni l-agent operates different temporal scale l-agent. suppose l-agent state ˜sinit takes action further suppose ˜sgoal intended result applying notice agent operating modiﬁed reward function repisode l-environment still emitting rewards according environment reward function terminal states still triggered l-environment’s terminal denote rewards act= denote whether l-environment terminated denote ﬁnal l-state sterm. termination l-episode quantities returned l-agent provide complete experience tuple ˜sinit previous sections deﬁned semantics amdp hierarchy specify precise learning algorithms used l-agents. indeed reinforcement learning algorithm could used either agents since operates classical mdp. work chose deep reinforcement learning method learner process high-dimensional pixel input model-based algorithm learner exploit long-term planning capabilities. level learner described above every transition states represented amdp. multiple hundred states neighboring states could hundreds thousands amdps. amdp could solved using vanilla would take millions observations train learn since every would learn scratch. avoid high computational cost share parameters except last fully connected layer network between policies. policy different parameters ﬁnal fully connected layer. encourages sharing high-level visual features policies imposes behavior individual l-policy speciﬁed interchangeable ﬁnal-layer parameters. since prone forgetting \u0001-greedy policy dynamically change epsilon based successful amdp measure success amdp periodically evaluating measuring number times policy terminates goal state ˜sgoal. equal percent time amdp succeeds evaluated found allows agent keep exploring actions forgotten well learned forgotten exploiting actions high level learner l-agent tabular r-max learning agent chose reinforcement learning algorithm l-agent constructs long-term plans navigate under-explored states. particularly every action given r-max reward tried number times. chose number ensure random policy could discover possible next abstract states. explore action tried times removed agent’s action-space prevent agent continuing explore heavily explored states. transition dynamics action change subgoal dqns learn tabular model keeps track last attempts action. allows agent adapt quickly changes transition dynamics. transition dynamics amdp build hierarchy on-the-ﬂy. agent begins empty states actions know transition graph every state needs sufﬁciently explored order neighbors. exploration give every state explore action simply amdp goal state. whenever state-state transition discovered amdp action initial state goal state practice limit explore action executed nexplore times. executed nexplore times remove explore action assuming sufﬁciently explored. nexplore experiments. pseudo code detailed algorithm main beneﬁt abstractions shorten reward horizon low-level learner. guiding principal construct abstraction l-states encompass small collections l-states. ensures lagents reasonably experience rewards transitioning neighboring l-states. crucial abstraction close markovian possible transition dynamics state depend history previous states. example imagine four rooms domain room connects rooms reason impassable wall room agent transition side wall side. depending agent entered room transition dynamics room would change. however since high-level learner seen agent transition room would think connected solution would divide room smaller rooms split impassable barrier. experiments abstractions split rooms smaller sectors decrease horizon learners games retain markovian property abstraction. sectors hand-made rooms atari experiments made square grids rooms based coordinates agent. chose particular gridding simple implement approximately markovian across game’s different rooms. experiments assess effectiveness algorithm complex domains involve long horizons sparse rewards high-dimensional inputs. trained agents million frames. mnih every million frames evaluated agents half million frames recording average episode reward evaluation frames. pseudo-count based using mixed monte-carlo return chose double performed well many atari games optimized exploration. agent explored highest number rooms montezuma’s revenge best knowledge. aspects success algorithm required algorithm giving agent multiple lives discussed related work section. therefore also compared agent addition. tested algorithm baselines three different domains. important note provide factorized state projection function predicate functions. however many real world domains natural decompositions low-level state abstract components current room agent room navigation task. domains single-life used implementation pseudo-counts authors unwilling provide source code. implementation able perform level results reported bellemare discovering rooms atari montezuma’s revenge time implementation discovered implementation still explores rooms baseline double discovered rooms. contacted researchers attempted replicate results likewise unable bellemare however kindly provide results montezuma’s revenge venture. compared results averaged trials. limited computing resources experiments single trial. four rooms montezuma’s revenge constructed version room navigation task given series rooms locked doors navigate rooms keys unlock doors reach goal room. domain room discrete grid layout. rooms consist keys doors impassible walls traps episode agent runs state given agent pixel screen current room rescaled converted grayscale. constructed maps rooms four rooms montezuma’s revenge four rooms consists three maze-like rooms goal room consists rooms designed parallel layatari montezuma’s revenge four rooms domain game terminates steps limit number steps. abstraction provided agent consists attributes location agent boolean state keys doors number keys agent had. location agent consists current room sector. used sectors decrease horizon learner four rooms since deadly traps hinder exploration. figure example screen common across four rooms yellow square left represents agent holding green right represents agent’s remaining lives. rooms four rooms blue squares locked doors yellow squares keys unlock doors squares traps result terminal state teal room goal room. entering room gives agent reward results terminal state. sectors provided agent color-coded. results show domains double agent failed learn complete game agent learned consistently solve problems. domain agents fail escape ﬁrst room agent provided life. reﬂects issue pseudocounts described previously image factored makes agent pixels independent result exploration bonuses backtracking doors lower remaining near key. contrast agent able explore rooms also learn complex task collecting unlock ﬁrst room collecting keys different rooms navigating unlock ﬁnal doors goal room emphasize marked difference performance different ways method explores. particularly daqn technique model-based high-level allowing coupled agents quickly generate long-term plans execute low-level. contrast must readjust large portions network’s parameters order change long-term exploration policies. montezuma’s revenge atari montezuma’s revenge atari game similar rooms doors problems series rooms blocked doors keys spread throughgame. also monsters avoid coins give points time-based traps bridges lava pits disappear reappear timer. abstraction similar state-space consisting attributes location agent boolean attribute presence door number keys. location agent consists current room sector. created coarse sectors based agent’s location room gridding room nine equal square regions. prevented sector transitions agent falling avoid entering sector immediately dying falling. example abstraction state figure would room sector keys collected doors unlocked. also tested daqn agent given single life normally agent dies returns location entered room retains keys collected. this valid policy escaping ﬁrst room navigate collect purposefully life agent. allows agent return starting location easily navigate adjacent doors. single life variant agent cannot exploit game mechanic collecting must backtrack starting location unlock doors. comparison illustrates algorithm’s ability learn separate policies different tasks. lives algorithm discover many rooms agent since agent able traverse timing-based traps. traps could traversed random exploration agent never learned anything beyond traps. agent discovered rooms total rooms visited without passing traps. agent underperformed atari montezuma’s revenge timing based traps could easily represented discrete high-level state space. however grant figure average reward four rooms atari single-life atari atari venture domains using following models daqn double four rooms double fail score average reward zero thus overlapping. double data bellemare montezuma’s revenge venture. plots show implementations’ results. performance baselines illustrating agents ability execute learn long-term plans. around million frames agent’s performance greatly decreases. performance drop agent exploring rooms training sub-policies reach rooms. since sub-policies exploitation trained time weights higher network updated train exploration sub-policies exploitation subpolicies forgotten. agent ﬁnishes exploring states would expect agent would revisit exploitation sub-policies relearn them. paper presented novel combining deep reinforcement learning tabular reinforcement learning using daqn. daqn framework generally allows agent explore much farther previous methods domains exploit robust long-term policies. experiments showed daqn agent explores farther high-dimensional domains longhorizons sparse reward competing approaches. illustrates capacity learn execute long-term plans domains succeeding approaches fail. speciﬁcally daqn able learn backtracking behavior characteristic long-term exploration largely absent existing state-of-the-art methods. main drawback approach requirement hand-annotated state-projection function nicely divides state-space. future work hope learn state-projection function. also plan incorporate motivated exploration algorithm learner address difﬁculty time-based traps approach also ability expand hierarchy multiple levels abstraction. problems investigated work single level abstraction allowed agent reason level rooms. however longer horizon domains inter-building navigation many real-world robotics tasks additional levels abstraction would greatly decrease horizon learner thus facilitate efﬁcient learning. agent life method greatly outperforms previous methods agent able escape ﬁrst room also discovered more double agents able escape ﬁrst room onelife setting necessitates backtracking-like behavior successful policy. mentioned before agent incapable backtracking thus cannot perform setting. emphasize inability arises account pseudo-count probabilistic model treating location agent presence independent. property actively discourages agent backtracking. venture atari venture game consists four rooms hallway. every room contains item. agent must navigate hallway rooms avoiding monsters collect items. item collected agent leaves room room becomes locked. abstraction game consisted attributes location agent boolean locked attribute room boolean whether item current room collected location agent consists current room sector. sectors constructed coarse gridding room gridding hallway. example figure agent small pink bottom screen. state abstraction would room sector items collected. sutton precup singh mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence hasselt guez silver deep reinforcement learning double q-learning. aaai vezhnevets osindero schaul heess jaderberg silver kavukcuoglu feudal networks hierarchical reinforcement learning. icml. material based upon work supported national science foundation grant numbers iis- iis- darpa grant numbers wnf--- national aeronautics space administration grant number nnxarg.", "year": 2017}