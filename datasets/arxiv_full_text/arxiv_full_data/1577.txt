{"title": "Decoding-History-Based Adaptive Control of Attention for Neural Machine  Translation", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Attention-based sequence-to-sequence model has proved successful in Neural Machine Translation (NMT). However, the attention without consideration of decoding history, which includes the past information in the decoder and the attention mechanism, often causes much repetition. To address this problem, we propose the decoding-history-based Adaptive Control of Attention (ACA) for the NMT model. ACA learns to control the attention by keeping track of the decoding history and the current information with a memory vector, so that the model can take the translated contents and the current information into consideration. Experiments on Chinese-English translation and the English-Vietnamese translation have demonstrated that our model significantly outperforms the strong baselines. The analysis shows that our model is capable of generating translation with less repetition and higher accuracy. The code will be available at https://github.com/lancopku", "text": "attention-based sequence-to-sequence model proved successful neural machine translation however attention without consideration decoding history includes past information decoder attention mechanism often causes much repetition. address problem propose decoding-history-based adaptive control attention model. learns control attention keeping track decoding history current information memory vector model take translated contents current information consideration. experiments chinese-english translation englishvietnamese translation demonstrated model signiﬁcantly outperforms strong baselines. analysis shows model capable generating translation less repetition higher accuracy. code available https//github.com/lancopku development deep learning neural machine translation demonstrated outstanding effects sequence-to-sequence model commonly-used model nmt. attention mechanism often used seqseq model many cases signiﬁcantly improve performance model. translating decoder builds language model target language semantic coherence attention mechanism obtains source-side information word generation time step. however current source-side information attention mechanism acquires often controversial translated contents attention knowledge translated contents. present typical example over-translation attention-based seqseq model chinese-english translation table example found attention-based seqseq generates phrase russian capital moscow multiple times causing much repetition. killed year cold homeless elderly including many people. gold temperatures moscow capital russia dropped levels last night even locals felt freezing cold. people died result bringing death toll coldness year dead homeless elderly including many drunk. table example translation conventional attentionbased seqseq model nist chinese-english translation task. text highlighted indicates repetition. motivation tackle problem propose decoding-history-based adaptive control attention attention-based seqseq model. mechanism controls output attention based decoding history including past information decoder past alignment information attention mechanism. computation attention requires information memory vector updated based decoding history manipulating decoder output attention vector. help memory attention adaptive translated contents repetition translation reduced. propose decoding-history-based adaptive control attention model tackles conﬂict current attention decoding history generation adaptive translated contents; experiments chinese-english translation english-vietnamese translation show model outperforms strong baselines advantages bleu score bleu score best attention-based seqseq model; figure structure attention-based seqseq seqseq aca. left structure attention-based seqseq model right structure model attention-based seqseq aca. decoder responsible decoding ﬁnal state encoder sequence ym}. ﬁnal encoder state initial state decoder initialized decode step step word embedding time step generates token representing end-ofsentence mark. decoder implement unidirectional lstm. output time step sent feed-forward neural network projected space vocabulary |×dim. time step decoder generates word sampling distribution target vocabulary pvocab where words discrete units words source sequence sent embedding layer become word embeddings input. embedding layer encoder turns embeddings sequence encoder outputs sends ﬁnal hidden state decoder. input word embedding time step minibatch input sequences. lstm consists four gates collectively control information last time step current time step. bidirectional lstm contains structure lstm reads input directions generate sequences hidden memory representation vector decoding time step whose initialization last hidden state encoder also initial state decoder. decoding time step memory updated remove-feed operation. operation based decision decoder output context vector attention mechanism memory observe situation current time step update guide current information. structure recurrent memory presented figure beginning previous memory experiences remove-feed operation. decoder output context vector generate remove gate decide update memory adaptive current decoding feed gate decide update memory information decoder attention remove operation based update information stored memory based decoding attention current time step memory adaptive current decoding. feed operation based elements provide memory information current time step memory store repetition translated contents. next introduce model makes time-sensitive memory improve decoding. update entering next time step memory collaborates decoder output generate gate context vector therefore information attention mechanism controlled information decoding history current state help updated memory. detail operations illustrated below. source text model generates sequence learning process minimize negative log-likelihood generated text reference context sequence target language machine translation summary abstractive summarization mentioned above easy conventional attention-based seqseq models suffer generating incoherent texts conﬂict attention mechanism decoding history. based hypothesis propose decoding-history-based adaptive control attention mechanism tackle problem. instead sending context vector directly output layer time step propose update attention recurrent memory stores information previous decoding time steps information attention mechanism controlled beneﬁcial whole generation. memory updates time step information current decoder output current context vector learn remove unnecessary information store important information time step. moreover responsible restricting information context vector order mitigate conﬂict attention neural language model. objective study build connection attention current time step decoding history implement recurrent memory decoder updating context vector. recurrent memory model responsible controlling information attention mechanism effects attention mechanism connected previous decoding outputs well attention mechanism. moreover memory updated every decoding time step reﬂect development decoding history. gated control context vector rectiﬁed based decoding history current information. memory storing useful information partial translation encourage model translate contents less repeated compared translated contents.even source-side information context vector conﬂict decoding history conﬂict mitigated gate controlled memory. evaluated proposed model nist translation task chinese-english translation provided analysis task. moreover order evaluate performance model low-resource translation also evaluated model iwlst english-vietnamese translation task. chinese-english translation nist translation task trained model sentence pairs extracted ldce ldce ldce hansards portion ldct ldct ldct chinese words english words. following validated model dataset nist translation task tested model nist translation tasks. used frequent words chinese vocabulary english vocabulary. evaluation metric bleu calculated case-insensitive nist bleu score multi-bleu.perl provided moses english-vietnamese translation data translated talks containing training sentence pairs provided iwslt evaluation campaign followed studies huang used preprocessing well validation test set. validation sentences test sentences. english vocabulary words vietnamese vocabulary words. evaluation metric also bleu mentioned above. implement models using pytorch experiments conducted nvidia gpu. size word embedding hidden size batch size adam optimizer train model default setting initialize learning rate based performance development sets -layer lstm encoder -layer lstm decoder. gradient clipping applied norm gradients cannot larger constant experiments. dropout used dropout rate following xiong beam search beam width generate translation evaluation test normalize log-likelihood scores sentence length. chinese-english translation following zhang compare model state-of-the-art systems based implementation results directly reported articles report results baselines moses rnnsearch study english-vietnamese translation following luong manning raffel huang compare model state-of-the-art models present results baseline directly reported studies. table results model baselines chinese-english translation tested nist machine translation tasks bleu score. means studies test models corresponding datasets. advantage bleu score strongest attention-based seqseq bleu score sota model npmt. moreover compared npmt pretrained language model model still better. table shows overall results systems chinese-english translation task. compare model strong baselines results directly reported articles. facilitate fair comparison compare baselines trained training slightly larger training reported articles. many models studies recent years prove strong baselines. results shown translation tasks model clear advantage them bleu score rnnsearch bleu score proves model effective. table shows overall results systems english vietnamese translation. found low-resource translation also bring signiﬁcant improvement attention-based seqseq model order test whether model mitigate problem repetition translation tested repetition nist dataset following evaluated proportion duplicates -gram -gram -gram -gram sentence calculated mean value. found levels translation model less repetition. moreover advantage becomes clearer increase number gram. especially -gram proportion duplicates model almost half model without aca. normal repeating words sentence repeating -gram cases unreasonable. compared model without help seqseq model reduce unreasonable repetition therefore mitigate problem over-translation taking decoding history account. source 在此之前一年 单单手机用户已跃居全球之冠 reference year that number mobile phone users alone already topped world. seqseq+attention year cell phone users cell phone users already world. +aca past year cell phone users leapt highest level world. source 佛莱文在谈及推行再生性能源策略已获致成功时 我们正进入一个新时代。 reference speaking success promoting strategy renewable energies flavin said we’re entering era. seqseq+attention entering era. ente-ring era. +aca speaking success renewable energy strategy fortuyn said entering era. mance seqseq model nmt. still attention mechanism suffers prediction failure therefore number studies proposed improve mechanism also enhanced performance model incorporated previous attention current attention better alignment none based decoding history. besides improving attention mechanism also effective neural networks. gehring cnn-based model greatly improves computation speed. vaswani removed used attention mechanism build model showed outstanding performance. also researches incorporated external knowledge systems also achieved obvious improvement conclusion paper proposes decoding-historybased adaptive control attention model transmit signiﬁcant information decoding history control output attention mechanism adaptively. thus output attention mechanism based decoding history including past information decoder well alignment information attention mechanism. method conﬂict source-side information attention translated contents mitigated. compared attention-based seqseq model model captures correct source information help decoding history translation behaves adaptive past translation. experiments chinese-english translation english-vietnamese translation show model outperforms strong baselines demonstrate effectiveness model. moreover choose nist chinese-english translation dataset test performance model conventional attention-based seqseq model without aca. test bleu scores sentences length shorter increase length performance models decrease model always clear advantage attentionbased seqseq. hypothesis model adapt decoding history improving attention mechanism possible perform better long-length sentence translation. analysis proves model robust translating sentences diverse lengths. table shows translation examples model nist dataset compared translation attention-based seqseq model without reference. obvious translation examples example similar references outperforming model without problems repetition meaning inconsistency. ﬁrst sentence model without generates repetition cell phone users misses semantic unit top. contrary translation closer literal translation faithful expression source. second example requires model reorder translation since name followed adverbial phrase source. complex different structure chinese confused model without generate repetition entering era. model successfully reorders translation putting name adverbial. framework task launched neural machine translation improve focus information encoder bahdanau proposed attention mechanism greatly improved perfor-", "year": 2018}