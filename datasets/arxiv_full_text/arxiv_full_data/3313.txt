{"title": "Texture Modelling with Nested High-order Markov-Gibbs Random Fields", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Currently, Markov-Gibbs random field (MGRF) image models which include high-order interactions are almost always built by modelling responses of a stack of local linear filters. Actual interaction structure is specified implicitly by the filter coefficients. In contrast, we learn an explicit high-order MGRF structure by considering the learning process in terms of general exponential family distributions nested over base models, so that potentials added later can build on previous ones. We relatively rapidly add new features by skipping over the costly optimisation of parameters.  We introduce the use of local binary patterns as features in MGRF texture models, and generalise them by learning offsets to the surrounding pixels. These prove effective as high-order features, and are fast to compute. Several schemes for selecting high-order features by composition or search of a small subclass are compared. Additionally we present a simple modification of the maximum likelihood as a texture modelling-specific objective function which aims to improve generalisation by local windowing of statistics.  The proposed method was experimentally evaluated by learning high-order MGRF models for a broad selection of complex textures and then performing texture synthesis, and succeeded on much of the continuum from stochastic through irregularly structured to near-regular textures. Learning interaction structure is very beneficial for textures with large-scale structure, although those with complex irregular structure still provide difficulties. The texture models were also quantitatively evaluated on two tasks and found to be competitive with other works: grading of synthesised textures by a panel of observers; and comparison against several recent MGRF models by evaluation on a constrained inpainting task.", "text": "currently markov–gibbs random ﬁeld image models include high-order interactions almost always built modelling responses stack local linear ﬁlters. actual interaction structure speciﬁed implicitly ﬁlter coeﬃcients. contrast learn explicit high-order mgrf structure considering learning process terms general exponential family distributions nested base models potentials added later build previous ones. relatively rapidly features skipping costly optimisation parameters. introduce local binary patterns features mgrf texture models generalise learning oﬀsets surrounding pixels. prove eﬀective high-order features fast compute. several schemes selecting high-order features composition search small subclass compared. additionally present simple modiﬁcation maximum likelihood texture modelling-speciﬁc objective function aims improve generalisation local windowing statistics. proposed method experimentally evaluated learning high-order mgrf models broad selection complex textures performing texture synthesis succeeded much continuum stochastic irregularly structured near-regular textures. learning interaction structure beneﬁcial textures large-scale structure although complex irregular structure still provide diﬃculties. texture models also quantitatively evaluated tasks found competitive works grading synthesised textures panel observers; comparison several recent mgrf models evaluation constrained inpainting task. texture modelling central important many computer vision image processing tasks image segmentation inpainting texture classiﬁcation synthesis anomaly detection image recognition. although successful specialised algorithms texture classiﬁcation synthesis segmentation developed generative probabilistic models oﬀer relatively complete models statistics individual textures appealling. applied tasks appearance priors feature extraction needed also interest understanding human vision. generative models must capture features texture signiﬁcant human perception order successful whereas texture features used discrimination need not. prevalent tool image texture modelling markovian undirected graphical models a.k.a. markov random ﬁelds together explicit gibbs probability distribution called herein markov–gibbs random ﬁeld mgrfs particularly popular image analysis involving determination boundaries enforcing smoothness cases markov networks usually sparse directly interacting neighbours variable close high-order mgrf models proposed tasks binary variables eﬃcient maximum posteriori algorithms exist graph cuts however things diﬀerent domain image texture modelling inference needs performed real-valued highly multivalued image variables dense markov networks. networks used paper typically markov blankets containing nearby distant pixels even sampling models proves diﬃcult. mgrfs probabilistic texture models reduce images vector statistics image features assumed suﬃcient describe texture. model completed assigning energy feature vector giving gibbs probability distribution images exp)). figure nesting exponential-family texture models. nesting iteration images meeting existing statistical constraints current model generated sampling model. features/potentials binary patterns selected iteration searching largest deviations empirical marginal distributions target image. provides constraints adds gibbs factor model thereby moving model closer target. recognised necessary expressive models natural images textures higher order interactions image models allow abstracting beyond pixels building upon larger scale image attributes like edges context complex structures captured. addition since regularly tiled textures strong long range correlations nearby tiles natural learn interaction structure speciﬁc texture. still almost unheard computer vision image modelling higher-order structure learned rather hand selected. however selection high-order features poses signiﬁcant problems. cardinality space possible feature functions grows combinatorially order freedom shape support need reduce manage high dimensional input domain. words features parameterised reasonable number parameters. higher-order mgrfs nearly exclusively apply linear ﬁlters feature functions statistics ﬁlter responses means variances correlations histograms forming description vector. texture classiﬁcation many methods extracting useful information high dimensional pixel co-occurrence matrix investigated dimensionality reduced making assumptions images invariant contrast oﬀset changes. however approach seemingly little-applied generative texture models. order tackle problems build texture models model nesting procedure greedily selects features build higher order features composing lower order ones. unlike works attempt provide ﬁxed statistics/texture features distinguish textures origin) rather learn texture-speciﬁc features. potentially provides compact representations still allowing large varied space descriptors. nesting iteration corrects statistical diﬀerences training image textures class given previous model sketched figure contributions paper follows eﬃciently select high-order features nesting models heterogeneous features/potentials coping diﬃculties inference dense mgrf texture models. unlike model nesting used previously learn parameters nesting iteration expensive instead generate images match current statistical constraints equivalent samples ideal model. correct parameter learning delayed afterwards. hidden variables currently popular eases learning inference parameter learning remaining convex theory. extend popular local binary pattern descriptors images learning oﬀsets surrounding pixels highorder ‘binary pattern’ mgrf texture features. quite diﬀerent common high-order linear ﬁltering potts potentials faster compute responses large linear ﬁlters. lbps apparently never used despite enormous popularity image descriptors. experiments texture synthesis using mgrfs histograms suﬃcient statistics provide insight visual features actually captured lbps. compare several families nested texture models utilising diﬀerent high-order features including diﬀerent methods selecting oﬀsets. resulting texture models heterogeneous feature sets composed second-order grey level diﬀerence features th-order features laplacian gaussian gabor ﬁlters. learned long range interactions allows almost-regular textures particular usually ability proposed procedure synthesised well. learn characteristic features across diﬀerent types textures demonstrated texture synthesis across varied greyscale textures also evaluated panel observers along several comparisons. order improve generalisation attempt allow partially inhomogeneous training images variant maximum likelihood estimate used training image split pieces minimum likelihoods pieces rather product maximised. much research texture analysis focused describing textures using distributions responses square linear ﬁlters. mgrf texture models utilising non-trivial ﬁlters introduced frame ﬁlters selected manually-speciﬁed bank. recently learning ﬁlters popularised field-of-experts model model marginal distibution responses ﬁlter hand-picked potentials parameters; original model student-t distributions. therefore interaction structure learned ﬁlter coeﬃcients. extended bimodal uses informative bimodal potentials successfully applied texture modelling heess several state generative texture models built bifoe using various conﬁgurations hidden variables. kivinen williams improved bifoe using gated mrfs investigated convolutional deep belief networks spike-and-slab potential functions. however learning diﬃculties reduce required computation learned-ﬁlter models restricted relatively small ﬁlter sizes. directly capture distant interactions; largest ﬁlter size used mentioned works result ﬁlter-based mgrfs model certain classes textures excellently inherent weaknesses. general survey mrfs covers high order models found recently popular hierarchical ﬁlter-based mrfs merger models especially boltzmann machines models single image patches using independent components analysis overcomplete product-of-experts applied larger images tiling overlapping ﬁlters. today mrfs including latent variables popular often inspired simple complex cells visual cortex strong links convolutional artiﬁcial neural networks. using multiple layers allows application high level computer vision tasks markov property hold object recognition face modelling. many cases integrating latent variables leads completely connected nonmarkovian interaction graph. diverge direction consider direct inclusion higher-order features models various high-order local texture descriptors applied texture classiﬁcation going back least years. applications invariances contrast oﬀset scale rotation deformation given particular weight. non-linear texture features also used mgrf texture models; sivakumar goutsias introduced mgrf texture models used sophisticated multiscale features deﬁned mathematical morphology. many sought directly reduce dimensionality high-order cooccurrence histograms traditional techniques spectral clustering histogram bins vector quantisation gaussian mixture models self-organising maps however dimensionality reduction techniques usually require nearest-neighbour search data points likely makes unsuitably slow feature functions mgrfs. probably popular texture descriptors discrimination lbps compare intensities ring pixels around central form bitvector ≤i≤k. nearly contrastoﬀsetinvariant straight-forwardly extended rotation invariance merging bins extended partial scale invariance using multiple concentric rings. addition proven ability distinguish textures lbps also cheap compute hence investigate texture modelling. nosaka introduced co-occurrence statistics neighbouring lbps rotationally invariant particularly interesting future extension bpbased mgrfs paper rotational invariance. lbps inspired number variants local ternary patterns local radial index also used lbps ltps related non-linear ordinal texture descriptors learnt oﬀsets applied texture classiﬁcation retrieval. although explained within framework mgrfs approaches involve learning complete mgrf model. built higher-order cliques lower-order ones searching cliques interaction graph computed using thresholding rule. restrictive pursue alternative strategies selecting oﬀsets. method selecting features model intuitive theoretically sound greedy sequential structure selection used various authors number details varying. alternates adding features/factors current model candidate according estimate best feature ﬁnding parameters common metric selecting best feature introduce select largest ‘error’ training image model synthesis result paper follows general approach call ‘nesting’ although describe particular ﬂavour. main diﬀerence approach attempt directly generate images matching statistics iteration learning approximate parameters side eﬀect. della pietra treated structure learning mgrfs feature selection problem built higher order features lower order ones. several authors since considered selecting mgrfs features gradually composing together order atomic features starting template-like features conjunctions simple predicates derived directly training data gradually generalising closely related popular method mgrf structure learning sparsity-inducing regularisation forces feature weights exactly zero removed. otherwise proceeding like nesting approach advantages regularisation combats overﬁtting allows removing feature added convex problem. recently chen welling suggested spike-and-slab priors instead regularisation. advantages also lead eﬃcient greedy algorithm optimality guarantee. practical texture synthesis currently dominated algorithms combine pieces source image pieces together well. deﬁned terms match neighbourhoods pieces. region-growing techniques pixel patch image time. algorithms often multiscale however synthesis algorithms property copy large parts source image directly result either explicit part algorithm emergent property algorithms’ search best matching neighbourhoods ‘forced moves’. another rather successful realtime class texture synthesis algorithm based global optimisation image projection onto images certain statistics equal training image particular statistics wavelet-decompositions codebook texture patches primitives also used instead implicitly texture models provide explicit probability distributions less broadly applicable probabilistic models. hence although give good synthesis results algorithms fact attack diﬀerent problem statistical paper does; texture synthesis evaluation. section provides ﬁrst deﬁnitions deﬁnes general exponential distributions conceptual theoretical basis mgrf model nesting. description nesting algorithm follows. probability distribution nowhere-zero represented factorised gibbs factors functions complete subgraphs markov network. paper consider class mgrfs factors described product ﬁxed feature function —identifying certain signal conﬁguration corresponding weight/parameter. restrict scope modelling homogeneous textures repeating gibbs factors cliques across image achieve translation invariance. integers ﬁnite lattice image coordinates list coordinate oﬀsets ﬁxed. order clique family spatially repeated cliques oﬀset pattern given image possible grey levels. order feature function ﬁnite range associated clique family given oﬀset list histogram values collected image vector nesting procedure features added current model selected based disagreement statistics training image model’s expected statistics approximated model samples. model modiﬁed correct errors. statistics question expectations corrections take particularly simple form. feature functions histogram vector concatenation histograms fi+]. general exponential family model probability distribution written following form speciﬁed base model vector-valued feature function parameter vector normalisation factor denotes dot-product. case parameters concatenation per-feature parameter vectors wishes model meeting constraints satisﬁed form expectations epi+] already prior information expressed base model widely known probability distribution matches constraints deviates base model minimum possible amount simple form. general exponential family model given maximum likelihood estimate parameters maxθ pi+. seen parameters achieve desired statistical constraints. technically inﬁnitely distant point parameter space permitted usual deﬁnition gpd. possibility easily avoided slightly smoothing fi+. challenges selecting high-order features structure gradually building features pieces reducing candidate features size exhaustively searched iteration. could reasonably expect conﬁguration pixels characteristically common texture conﬁguration restricted subset pixels would usually also common. hence conjecture practice high-order feature functions picking characteristic interactions found building lower order features. theory general exponential distributions need modify whatever parameters base model have although previous works involving sequential structure learning normally done allows increasing likelihood. however attempt reduce overﬁtting suggest reasonable adjust parameters associated features fi+. dud´ık described coordinate-wise descent algorithm wide class generalised maximum-entropy problems selects best single parameter update iteration. state standard approach updating parameters iteration impractical number features large. however experiments found aggressive solution overﬁtting obtained better results keeping parameters free. given feature functions already selected selector function deﬁned provide candidate features possibly built upon previous features. algorithm proceeds stage-wise sequence selectors providing features certain order type avoid problem comparing across feature types. sequential selection simply selects feature functions among current model largest disagreement training data possible variant separate training validation datasets stop performance validation begins decrease. validation test used earlier work order detect slightly non-homogeneous textures. paper takes idea statistical variations across training data important section discusses selection features pieces training images rather less powerfully looking variation single training validation pair. previous sequential feature selection implementations used distance ‘gain’ gain increase information contained model adding feature either estimated analytically great expense actually comparing every possible extended model also gave correction gain theoretical uncertainty estimated statistics form akaike information criterion penalising model complexity. chain monte carlo sampling mgrf typically vastly larger theoretical estimates uncertainty because sampling converge especially true quality also statistics approximate samples attempt rapidly draw current texture model highly variable. ﬁrst sample drawn) included marginal nearest-neighbour pairwise potentials proceeding selectors. ﬁrst selector returns three pairwise potentials time. frequently visible improvement step another however even samples getting worse indicate model getting worse unstable sampling process. also usually jump class features added decreasing returns thereafter. determining suitable stopping point model learning diﬃcult histogram distances reﬂect visual proximity training image well texture similarity metric improved structural texture similarity might better option. experiments paper instead used ﬁxed iterations selector removes variability imperfect stopping rule large eﬀect. figure shows cliques selected examples detail. procedure described generic could equally well applied learning mgrf models machine learning tasks outside computer vision. reﬁnement speciﬁc texture-modelling including implementation details given below. figure examples nesting procedure. shows pieces original textures ‘text’ subsequent pairs rows illustrate progress feature selection. image shows ‘samples’ nesting iteration approximately matches statistical constraints current model ﬁrst pair shows addition pairwise features second shows addition th-order jagged star features kullback-leibler divergence probability distributions averaged distribution measures discriminability distributions namely average certainty sample ascribed other. symmetric bounded range bits popular suitable comparing empirically estimated distributions unlike often undeﬁned. practice usually provides similar rankings distance. data likelihood base model ﬁxed w.r.t. dropped parameters general exponential family distributions learned without base model. gradient vector hessian matrix log-likelihood easily derived eθ{h} covθ{h} denote respectively expected value covariance matrix image features w.r.t. distribution covariance matrix always nonpositive deﬁnite log-likelihood concave space parameter vectors common property exponential family distributions exactly general intractable. approximation using markov chain monte carlo sampler single-site gibbs sampler highly unreliable energy landscape often deep local minima inescapable reasonable timescales. hessian reasonably easily approximated diagonal matrix given samples model order taylor expansion approximation log-likelihood using eq.s newton step performed inverting hessian numerically stable alternative maximum order likelihood approximation along direction gradient. details. second-order method initial approximation parameters nesting iteration save little time. gradient hessian highly approximate newton’s method takes dangerously large steps take single order step switch stochastic order method follows approximate gradient detailed next section. broadly ways achieve desired invariances designing models enforce invariances learning suitable training data demonstrates them. approaches although latter preliminarily developed here. described model learning terms maximises total probability training image suﬃcient statistics image mean values feature functions image. completely oblivious variations across image reason small parts training image actually probability. particularly true textures composed large randomly distributed textons greatly reduces eﬀective amount training data thus diﬃcult learn kinds mgrf models. place image whole image particular angle oﬀset between textons common average chance. like humans sequential feature selection easily sees patterns noise. already made assumptions texture homogeneous markovian property entire training image sample texture class without foreign inclusions. thus expect every piece training image suﬃcient size also visually recognisable sample texture class much smaller aren’t thus model also recognise them. scale approximately equal twice tesselation oﬀset texture regular related size textons texture composed general similar spatial extent markov blanket. indeed property holds training images used experiments. hence unsatisfactory averages training image considered propose switch away objective. training images. practice simply split single training image pieces size overlap pixels. related maximum oﬀset length interacting pixels search pixels. change optimisation problem assuming time sample mgrf linear number features nesting procedure runs time quadratic number nesting iterations. order keep running times reasonable number nesting steps limited computation nesting iteration minimised. obvious method learn parameters mgrf perform stochastic gradient descent following noisy approximation gradient given sampling model approximate expectation gradient sampling could performed starting initial image performing gibbs sampler sweeps whole image total energy converges. however method slow. mgrfs learn dense makes sampling problematic; gibbs sampling slowly easily gets stuck local minima. alternatively ﬁxed number sweeps performed produce sample still useful synthesising images optimises wrong objective. rather repeatedly sampling models approximate gradient used controllable simulated annealling simultaneously invented produce approximate images matching training statistics much faster possible learning parameters. essentially identical procedure also reinvented name ‘persistent contrastive divergence’ goal parameter learning presently popular method mgrfs. diﬀerence uses small stepsizes started training data rather random image. denotes element-wise hadamard product training image vector current per-parameter step sizes iteration number. robbins-monro sequence used all-ones vector dimension rapidly produce image statistics usually match training statistics fairly well also strong tendency oscillate. reason introduce variant called accelerated controllable simulated annealing uses gain vector adaptive step size instead steps described almeida default parameters initial step size vector uses diﬀerent learning rates diﬀerent parameters dampening oscillations reducing learning rate parameters increasing learning rate seem require adaptive step sizes proved give faster convergence desired statistics steps robust initial step size. finding optimal parameters produce good unguided synthesis results requires tuning eliminate unwanted energy minima approached slowly during mcmc. theory optimal parameters learned mcmc sampling produce samples statistics equal constraints average indistinguishable model features. hence ‘samples’ substituted real samples model. ideally available eﬃcient sampling algorithm singlesite gibbs sampling used popular samplers applicable here. eﬃcient algorithms developed creating images matching desired statistics making modiﬁcations attempting directly move towards naturally converge desired statistics faster starting parameters close mle. nesting iteration ﬁrst found approximation parameters described section four times gibbs sweeps each starting uniform noise images produce four samples size results noisy unreliable reaching desired statistics helps greatly average multiple runs. parameters produced side eﬀect approximate actually diverge nonetheless useful carried forward future iterations. important consideration drawing samples mgrf texture model using mcmc sampler choice initial image. markov chain converges asymptotically model distribution take number steps exponential number pixels ubiquitous deep local minima. however well known mcmc converges slowly highly correlated variables updated independently image models nearby pixels typically highly correlated another making gibbs sampling ineﬃcient prone trapped local minima. common example ‘crystallisation’ regular patterns growing areas meeting without aligning. gibbs sampling regular textures starting noise often leads crystallisation image size several multiples maximum clique size. number ways avoid crystallisation. longer range interactions order strongly enforce global structure. otherwise process pushed towards correct regularity starting initial image template form dots stripes unevenness desired tesselation. tesselation nearregular textures automatically extracted computing co-occurrence statistics diﬀerent oﬀsets form model-based interaction alternatives slowly grow image extending boundaries starting single small ‘seed’ ﬁrst allowed converge starting random piece training image seed. latter used experiments simplicity. texture model used tasks segmentation classiﬁcation better performance could expected appropriate learning sampling validation methods used. example texture synthesis care local mimimum reached sampling starting sample disregard unguided texture synthesis learning model used sweeps acsa growing seed produce images size plus trimmed margins pixels depending feature sizes. hence inpainting shortcut available practice acsa harmful inpainting generalisation required. instead tuned model parameters normal used gibbs sampling inpaint. order reduce space candidate potentials deﬁne families potentials parameterised shape supports unparameterised feature functions. simplest order feature image pixel greylevels trivial grey level co-occurrence feature bins. features proved poor generalisation ability; performed well texture synthesis used inpainting tasks remembered original contrast oﬀset training image rather matching contrast surrounding inpainting frame. previous researchers found histograms pairwise grey level diﬀerence deﬁned fgld bins encodes large majority information pairwise histogram conﬁrmed comparisons features texture synthesis hence used instead features capture pairwise interactions. high-order features investigated binary patterns generalising lbps using learned oﬀsets central pixel. feature function thresholds grey levels pixels clique certain distinguished pixel supposed might suitable describing regions image amount noise causes produce evenly distributed random values. synthesis purposes features must used heterogeneous models paired features break symmetry image inverse experiments found texture contains regions number features would selected nesting procedure otherwise hardly selected. found similar descriptive power statistics additional failure conditions necessary symmetry breaking used further. base model used mgrf ﬁrst order factor model marginal statistics nearest neighbour features oﬀsets exception marginal potentials used inpainting experiment section improved models’ ability match contrast surrounding frame. however unguided synthesis matching original histogram desirable. features constrained clique families pixels distance points. initial candidate always comprised features within circular window. three features added time speed learning. thereafter diﬀerent possibilities considered listed below. ‘combined’ features built characteristic oﬀsets {ri} occurring previously selected features. possible selection oﬀsets choice either using mirrored oﬀsets ri−ri halved oﬀsets ri/−ri/ provided four-oﬀset candidates. features also added time. ‘conjoined’ features built selecting four features symmetric pairs oﬀsets maximal training sample histograms combining single clique family shape examples seen figure even. conﬁgurations include square well circular patterns. considering dense subset rotations radii pixels provided ﬁxed candidates. examples seen figure linear ﬁlters. ﬁxed bank gabor wavelets laplacian gaussians identical used frame except increased number gabor wavelet orientations restricted sizes generative image models often evaluated measuring performance image denoising inpainting model used prior. indirect incomplete method evaluation. instead mainly compare diﬀerent classes models inspection synthesis results. experiments conducted grey-scale digitised photographs natural approximately spatially homogeneous textures sourced several popular databases. textures selected diverse diﬃcult model homogeneous without periodicity features scale longer pixels databases used popular brodatz photoalbum newtex dataset natural textures included meastex framework standardised texture discriminator evaluation; vistex database images collected laboratory computational vision textures subjectively categorised three classes according structure stochastic near-regular irregular image quantised grey levels. exceptions images quantised using contrast-limited adaptive histogram equalization tiles contrast clipping limit adaptive histogram equalization mostly avoids situation image mapped grey levels also lessens shadows gradients hinder recovery long range interactions. value used speeds gibbs sampling. source code experiments freely available website accompanying paper together supplementary material including results large number additional textures. baseline compare synthesis results texture analysis synthesis algorithm portilla simoncelli addition freely available today still successful attempts model texture statistically recently extended colour textures. approach uses iterated projections attempt produce images matching certain ﬁrstsecondorder statistics wavelet responses. hand recent ﬁlter-based texture models figure comparison synthesis results using frame linear ﬁlters models. column original textures; columns models linear ﬁlters linear ﬁlters jag-star features respectively. demonstrated simple highly regular textures short repetition lengths texton scales. unfortunately lack diﬃcult synthesis examples source code previously published approaches hinders comparison although achieved visually better results diﬃcult textures using three layers hundreds ﬁlters deep belief network. theory purpose layers stabilise learning using tiled-convolutional rather fully-convolutional ﬁlters simpler models necessary figures show synthesis results diverse interesting examples modelled using alternative classes features described section training images size. synthesis results diﬀerent models comparison evaluated panel observers. textures synthesised images diﬀerent classes models presented person along original training image tasked placing synthesis results order best worst. observers instructed make rapid decisions rather perform careful inspection. figure synthesis results models diﬀering kinds potentials original textures; second-order glds gld+ ‘combined’ gld+‘conjoined’ gld+jag-star gld+jag-star portilla simoncelli’s results. types options summarised texture category. additionally table reports frequently class model gave best-ranked synthesis result. features outperforming order ones stochastic textures artifact earlier stopping rule usually excluded higher order ones altogether textures. ﬁgures tables seen mgrfs perform well near-regular textures tesselation easily handled stochastic textures apparent dependency actual texture short scale thus natural mgrf. however easy separate performance diﬀerent classes models. stochastic textures method portilla simoncelli usually always dominates apparently signiﬁcantly captures high frequency aspects details not. often able produce nice results irregular textures however often violates ﬁxed tesselation oﬀsets fails represent complex textons. cases simple cheap potentials eﬀective. addition gibbs sampling mgrf adds unwanted high frequency noise. complex irregular textures tiles column figure diﬃcult tables ‘combined’ order seen well irregular textures despite order parameters feature. provides promising evidence piecing together high order features order ones workable technique. however number schemes building higher orders attempted documented here provided results. motivated investigation jag-star features simpler less brittle alternatives. previous independent experiments also favoured ‘combined’ features providing evidence real eﬀect. signﬁcant diﬀerence order jag-star features visible provides evidence large number parameter latter models parameters feature unworkable. results star bp’s close jagstar generally inferior. figure variants jag-star models. rows first training images. second jag-star models. third jag-star models. fourth jag-star models without pairwise potentials table averages standard deviations human rankings texture synthesis results texture category best worst. models except portilla simoncelli include potentials. stochastic near-regular table fraction time image synthesised class texture model ranked best judge possibilities split texture category. models except portilla simoncelli include potentials. also compared eﬀect replacing pairwise features features removing entirely shown figure generally result excluding pairwise potentials great even undetectable higher order features seemingly usually capture statistics expectations. naturally long range features helpful regular textures. diﬀerence features even less though whole texture models base seem figure inpainting examples th-order jag-star model left hand column pair shows worst inpainting result repetitions right hand column best result. block results simple inpainting using gibbs sampling bottom block shows smoothed result averaging consecutive gibbs iterations. smoothed images much higher mssim scores. using texture modelling compared traditional linear ﬁlters too. consider models close used models also including features. either case initial base model used models. still partially captured tiles column augmenting ﬁlters long range potentials prevents failure case linear ﬁlters perform much better pairwise features alone. unexpectedly addition long range potentials synthesised images normally appear distinct composed features similar results across texture types. example speckles ﬁrst texture disappointingly captured indicating deﬁciency ﬁxed ﬁlterbank. however ripples images clearly captured wavelets including portilla simoncelli’s analysis features. additionally table appears frame linear ﬁlters outperform stochastic textures might expected although weaker comprehensive wavelet statistics used hand outperformed textures structure. despite restricting ﬁlter sizes model learning times ﬁlters still minutes average several times models. additionally compared results three recent leading hierarchical mgrf texture models attempted quantitatively evaluate mgrf texture models texture inpainting task. used slightly diﬀerent variant results incomparable.) focusing four highly regular textures small textural feature sizes reasonable compare synthesized pixels inpainted region original ones since nearly aligned. applicable texture types narrow test. task pixel piece brodatz textures provided inpainting frame pixel hole centre. algorithm ﬁlls hole consistent frame diﬀerence ground truth measured using mean structural similarity index mssim found experimentally good measure perceptual distance images. bilinearly scaled allowed small ﬁlters earlier papers. half image used training bottom half testing. since learning procedure stochastic inpainting repeated random inpainting frames test region average mssim score reported. examples inpainted frames shown figure examples texture synthesis using models used inpainting shown figure comparing previously published synthesis results. initially mssim scores across models learnt poor figure reveals reason quantitative results. best previously published models produce smooth synthesis also inpainting results maximising per-pixel posterior marginals rather providing sample texture distribution. especially true published works achieve highest mssim scores hand averaging consecutive gibbs mcmc samples achieve similar scores previously published algorithms although visual quality greatly decreased demonstrated figure mssim scores presented table alongside best previously published results efros leung texture synthesis algorithm baseline. experiment used rather grey levels. comparison ground truth grey levels models using levels gives slight penalty previous works. unknown reason models attempted frequently failed properly inpaint resulting scores texture. full resolution version also unusually diﬃcult synthesize models. markov random ﬁelds long applied image modelling little prior investigation context practical learning interaction structure until recently apparently order mgrfs previously learned. particular model nesting approach applied paper theoretical foundation maximum entropy distributions previous sequential structure learning approaches emphasizes approximation speed using csa. aside this strength model nesting creating heterogeneous models allowing composition feature functions. mixing dissimilar potentials mgrf typically shows immediate improvements. furthermore nearly decades mgrf texture image models based square linear ﬁlters almost exclusively studied superior performance primitive pairwise models preceded them. however shown types high-order features generic binary pattern features introduced paper viable alternatives linear ﬁlters. features several advantages ﬁlters. oﬀset nearly contrast invariant still powerful even order. presented synthesis inpainting experiments showed even feature order suﬃcient capture many important visual aspects textures capable outperforming previously best approaches thanks building features characteristic low-order cliques. relatively orders reduce computation costs ﬁlter coeﬃcients need selected learned. purpose synthesis experiments challenge mainstream highly eﬀective eﬃcient texture synthesis algorithms validate probabilistic models. however still many textures models failed capture especially complex irregular ones well losing details general. order tackle future plan extend models including additional layers feature selectors diverse types statistics particularly co-occurrences outputs feature functions previous levels used several texture models figure comparison synthesis results previously published works eight original brodatz textures; results multi-tm algorithm results -layer results combined conjoin jagstar original images scaled grey levels reversed previous works. image individually renormalised order erase contrast diﬀerences original textures. table results inpainting evaluation; mssim scores ground truth models include pairwise potentials addition higher order ones. results efros leung algorithm taken used patch sizes. used ﬁlters features. stopping rule nesting procedure robust highly variable quality samples would also important feature sets used keep number nesting iterations low. future work also hope improve generalisation ability models invariance small deformations image. closely related selection complex composite features describe powerful abstractions.", "year": 2015}