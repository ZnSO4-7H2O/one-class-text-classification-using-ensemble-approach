{"title": "The observer-assisted method for adjusting hyper-parameters in deep  learning algorithms", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper presents a concept of a novel method for adjusting hyper-parameters in Deep Learning (DL) algorithms. An external agent-observer monitors a performance of a selected Deep Learning algorithm. The observer learns to model the DL algorithm using a series of random experiments. Consequently, it may be used for predicting a response of the DL algorithm in terms of a selected quality measurement to a set of hyper-parameters. This allows to construct an ensemble composed of a series of evaluators which constitute an observer-assisted architecture. The architecture may be used to gradually iterate towards to the best achievable quality score in tiny steps governed by a unit of progress. The algorithm is stopped when the maximum number of steps is reached or no further progress is made.", "text": "paper presents concept novel method adjusting hyper-parameters deep learning algorithms. external agent-observer monitors performance selected deep learning algorithm. observer learns model algorithm using series random experiments. consequently used predicting response algorithm terms selected quality measurement hyper-parameters. allows construct ensemble composed series evaluators constitute observer-assisted architecture. architecture used gradually iterate towards best achievable quality score tiny steps governed unit progress. algorithm stopped maximum number steps reached progress made. hyper-parameters adjusting challenging task addressed many papers important virtually currently used algorithms feature macro parameters shape ﬁnal architecture. turn direct impact performance solutions based algorithms. unfortunately despite fact deep learning algorithms around long time wellestablished procedures hyper-parameters tuning back-propagation model training instead custom techniques grid random heuristic search developed used systems designers. author’s view process adjusting hyper-parameters account data algorithm. turn requires external agent denoted herein ‘observer’. observer learns given hyper-parameters aﬀects performance deep learning algorithm terms chosen quality measurement score accuracy basic idea oﬀset learning process complex algorithm hard control simpler easily adjustable hyper-parameters instance choosing hyper-parameters hierarchical temporal memory time consuming demanding process. replaced using feed-forward neural network model predict right values hyper-parameters number cells columns synapses. observer learns response deep learning algorithm diﬀerent sets hyper-parameters terms selected quality measurement score based information able reason best hyper-parameters. however order able learn relationship hyper-parameters performance range experiments random parameters must conducted. experiments done reliable predictions made observer. worth keeping mind observer models deep learning algorithm. therefore quality model used observer substantial impact hyper-parameters adjusted. component observer algorithm presented fig. series evaluators shown. evaluators provides information regarding value selected hyper-parameter quality score. taking evaluators outputs account decision hyper-parameter updated next step made updater module. evaluator consist mappers ﬁrst mapper unique evaluator responsible hyper-parameter value evaluation. second used predict quality score given hyper-parameters. general mappers implemented kind algorithm linear logistic regression worth keeping mind regression algorithm best evaluated quality score algorithm lifetime; q_best mapping functions evaluating hyper-parameters mapping function evaluating quality score observer-assisted hyper-parameters adjusting algorithm presented alg. assumes predeﬁned values max_iterations max_idle min_contribution well expected quality score q_ex. author paper assumed threshold arbitrarily chosen practice probably cases designer expects quality score q_ex ﬁrst part algorithm hyper-parameters hpn− initialized random values appropriate ranges iterations counter idle iterations counter current quality score value iterations counter used prevent algorithm running inﬁnitely. idle iterations counter used stop adjusting process certain time progress. main work algorithm done inside loop. steps mapper evaluates hyper-parameter value based current hyper-parameters expected quality score. results creation array proposed values hyper-parameters potentially yield better results current set. steps propositions evaluated predicted quality scores stored q_eval array. that single parameter evaluated quality score highest chosen replacement current hyper-parameters best evaluated quality score better previous best remembered future idle iterations counter reset. otherwise iteration counted idle. proposed alg. approaches hyper-parameters adjusting problem single pass i.e. sets expected quality score highest possible value another option reach target quality score multiple passes algorithm basic hyper-parameters adjusting require q_ex ensure q_best hp_best get_random_hyperparams hp_best iterations idle q_best map_q q_best q_ex idle max_idle iterations max_iterations initially q_ex q_init. concurrent passes algorithm q_ex value increased q_step. strategies choosing q_step constant small throughout passes progressively decreased rising number passes. decision starting pass depends algorithm achieving previously expected quality score whether stagnation counter reached upper limit. basic version algorithm selection updated hyper-parameter done using simple criteria evaluated quality score comparison hyper-parameter yielding highest quality score chosen numerical value selection criteria hpidx hyper-parameter henceforth denoted ‘contribution’ expressed however deep-learning algorithm hyper-parameters usually related network structure increase incurs computational cost. using contribution formula could potentially cause huge increase deep-learning algorithm hardware requirements calculation time small gain terms quality score. cases alternative method contribution calculation employed exact mapping done cost function depend factors observed deep-learning algorithm hyper-parameter adjusted hardware used etc. θidx returns equivalent such alg. replaced alg. types hyper-parameters appropriate θidx deﬁnitions. paper introduces concept method used demanding important task deep learning algorithms hyper-parameters adjusting. proposed method based external agent denoted ‘observer’ learns algorithm eﬃciency terms chosen quality score. allows model performance algorithm respect hyper-parameters. author also proposes method incorporation hardware resources consumption process adjusting hyper-parameters. future work author going implement described method conduct series experiments order compare eﬃciency methods", "year": 2016}