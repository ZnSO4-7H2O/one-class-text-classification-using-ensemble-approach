{"title": "A Classification Approach to Word Prediction", "tag": ["cs.CL", "cs.AI", "cs.LG", "I.2.6;I.2.7"], "abstract": "The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these. We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.", "text": "beneﬁcial. particular provide learner rich features combine information available local context along shallow parsing information. time study learning approach speciﬁcally tailored problems potential number features large fairly small number actually participates decision. word prediction experiments perform show signiﬁcant improvements error rate relative traditional restricted features. background inﬂuential problem motivating statistical learning application tasks word selection speech recognition there word classiﬁers derived probabilistic language model estimates probability sentence using bayes rule product conditional probabilities relevant history predicting thus order predict likely word given context global estimation sentence probability derived which turn computed estimating probability word given local context history. estimating terms form done assuming generative probabilistic model typically using markov independence assumptions gives rise estimating conditional probabilities n-grams type features machine learning based classiﬁers maximum entropy models which principle restricted features forms used nevertheless perhaps inﬂuence probabilistic methods eventual goal language model accurately predict value missing word given context. present approach word prediction based learning representation word function words linguistics predicates context. approach raises questions address. first order learn good word representations necessary expressive representation context. present uses external knowledge generate expressive context representations along learning method capable handling large number features generated potentially contribute prediction. second since number words competing prediction large need focus attention smaller subset these. exhibit contribution focus attention mechanism performance word predictor. finally describe large scale experimental study approach presented shown yield signiﬁcant improvements word prediction tasks. task predicting likely word based properties surrounding context archetypical prediction problem natural language processing many tasks necessary determine likely word part-of-speech token given history context. examples include part-of speech tagging word-sense disambiguation speech recognition accent restoration word choice selection machine translation context-sensitive spelling correction identifying discourse markers. approaches problems based n-gram-like modeling. namely learning methods make features conjunctions typically three consecutive words tags order derive predictor. paper show incorporating additional information learning process better classiﬁers language models. eﬀorts directions consists directly adding syntactic information indirectly adding syntactic semantic information similarity models; case n-gram type features used whenever possible cannot used additional information compiled similarity measure used nevertheless eﬀorts direction shown insigniﬁcant improvements believe main reason incorporating information sources needs coupled learning approach suitable studies shown machine learning probabilistic learning methods used make decisions using linear decision surface feature space view feature space consists simple functions original data allow expressive enough representations using simple functional form implies number potential features learning stage needs consider large grow rapidly increasing expressivity features. therefore feasible computational approach needs feature-eﬃcient. needs tolerate large number potential features sense number examples required converge depend mostly number features relevant decision rather number potential features. paper addresses issues mentioned above. presents rich features constructed using information readily available sentence along shallow parsing dependency information. presents learning approach expressive intermediate representation shows yields signiﬁcant improvement word error rate task word prediction. rest paper organized follows. section formalize problem discuss information sources available learning system construct features. section present learning approach based snow learning architecture. section presents experimental study results. section discuss issue deciding candidate words decision. section concludes discusses future work. information sources deﬁnition sentence i-th word. collection predicates sentence information source available sentence representation list predicates example sentence john clock what time i={word subj-verb} interpretation word unary predicate returns value word domain; unary predicate returns value word domain context sentence; subj verb binary predicate returns value words domain second verb sentence ﬁrst subject; returns otherwise. then representation consists predicates non-empty values. e.g. modal part sentence above. subj verb might exist even predicate available e.g. ball given mary. clearly representation contain information available human reading captures however input available computational process discussed rest paper. predicates could generated external mechanism even learned one. issue orthogonal current discussion. generating features goal learn representation word interest. eﬃcient learning methods known today particular used make linear decision surface feature space therefore order learn expressive representations needs compose complex features function information sources available. linear function expressed directly terms expressive enough. deﬁne language allows order learn features generated using deﬁnitions input important features generated applying deﬁnitions diﬀerent given identiﬁcation. presentation assume composition operator along appropriate element written explicitly identiﬁcation features. subtleties deﬁning output representation addressed structured features presented features relations allowed boolean composition operators. cases information list active predicates available. abstract using notion structural information source deﬁned below. allows richer class feature types deﬁned. note deﬁne features used learning process. going deﬁned data driven given deﬁnitions discussed input iss. importance formally deﬁning types fact quantiﬁed. evaluating given sentence might computationally intractable formal deﬁnition would help ﬂesh diﬃculties designing language deﬁne features makes structural information. structural features deﬁned using sis. deﬁning feature naming nodes done relative distinguished node denoted call focus word feature. regardless arity features sometimes denote feature deﬁned respect deﬁnition feature deﬁnitions. collocc restricted conjunctive operator evaluated chain length graph. speciﬁcally chain length sis. then collocation feature respect chain deﬁned example sentence example deﬁne features respect linear structure sentence. word used focus word chain deﬁned respect proximity features deﬁned respect predicate word. example john; clock. collocation features deﬁned respect chain centered focus word deﬁned respect basic features either resulting features include example non-linear structure described feature deﬁnitions make linear structure sentence yield features diﬀerent standard features used literature e.g. n-grams respect word deﬁned colloc appropriate chain. consider given general directed acyclic graph sentence nodes. given distinguished focus word deﬁne chain graph linear structure sentence. since deﬁnitions given above def. def. given chains would apply chain graph. generalization becomes interesting given graph represents involved structure sentence. consider example graph figure described dependency graph sentence edge represent dependency words. feature generation language separate information provided dependency grammar parts. structural information provided left side figure used generate sis. labels edges used predicates part notice authors used structural information used information given labels edges this information produced functional dependency grammar assigns word speciﬁc function structures sentence hierarchically based also generated external rule-based parser learned one. feature fsubj−verb deﬁned collocation chains constructed respect focus word join. moreover deﬁne fsubj−verb active also subj verb deﬁning disjunction collocation features subj-verb subj-aux vrb-verb. features conjunctions words occur focus verb along chains occurs collocations verb. ﬁnal comment feature generation note language presented used deﬁne types features. instantiated data driven given input sentences. large number features created might relevant decision hand; thus process needs followed learning process learn presence many features. experimental investigation done using snow learning system earlier versions snow applied successfully several natural language related tasks. snow task word prediction; representation learned word interest compete evaluation time determine prediction. snow architecture snow architecture sparse network linear units common pre-deﬁned incrementally learned feature space. speciﬁcally tailored learning domains potential number features might large small subset actually relevant decision made. nodes input layer network represent simple relations input sentence used input features. target nodes represent words interest; case studied here word candidates prediction represented target node. input sentence along designated word interest mapped features active representation presented input layer snow propagates target nodes. target nodes linked weighted edges input features. features active example linked target node linear unit corresponding active given example treated autonomously target subnetwork; example labeled treated positive example subnetwork negative example rest target nodes. learning policy on-line mistake-driven; several update rules used within snow. successful update rule variant littlestone’s winnow update rule multiplicative update rule tailored situation input features known priori inﬁnite attribute model mechanism implemented sparse architecture snow. input features allocated data driven input node feature allocated feature active input sentence link exists target node feature active example labeled important properties sparse architecture complexity processing example depends number features active independent total number features observed life time system. important domains total number features large small number active example. chose verb prediction task similar word prediction tasks particular follows paradigm there list confusion sets constructed ﬁrst consists diﬀerent verbs. verb coupled provided occur equally likely corpus. test every occurrence replaced classiﬁcation task predict correct verb. example confusion created verbs make sell data altered follows target subnetworks learned network evaluated decision support mechanism employed selects dominant active target node snow unit winner-take-all mechanism produce ﬁnal prediction. snow available publicly http//lr.cs.uiuc.edu/~cogcomp.html. competing words prior probabilities part speech. further advantage paradigm future experiments choose candidate verbs sub-categorization phonetic transcription etc. order imitate ﬁrst phase language modeling used creating candidates prediction task. moreover pretransformed data provides correct answer easy generate training data; supervision required easy evaluate results assuming appropriate word provided original text. used wall street journal years size corpus words. corpus divided training test. training test data processed parser verbs occur least times corpus chosen. resulted verbs split confusion sets above. after ﬁltering examples verbs sets training examples test examples. linear linear features using linear features deﬁned along linear features predicates subj word collocations subj-verb verb-obj linked focus verb graph structure conjunction linked words. number features generated target verbs around tables columns represent results naive bayes algorithm implemented within snow snow column represents results sparse winnow algorithm within snow. table summarizes results experiments features sets above. baseline experiment uses majority predictor. addition conducted experiment using trigram backoﬀ results conclude using expressive features helps signiﬁcantly reducing wer. however types features learning method handles large number possible features. emphasizes importance learning method. table comparison improvement achieved using similarity methods using methods presented paper. results shown percentage improvement accuracy baseline. table compares method methods similarity measures since could corpus experiments compare ratio improvement wer. baseline studies diﬀerent experiments identical. show improvement best similarity method. furthermore train using examples train using examples. given experience approach data sets conjecture could improved results used many training examples. snow used experiments multi-class predictor representation learned word given evaluation time selected prediction. candidate words called confusion target words. previous experiments generated artiﬁcially subsets size order evaluate performance methods. general however question determining good candidates interesting right. absence good method might choosing verb among larger candidates. would like study eﬀects issue performance method. principle instead working single large confusion might possible split subsets smaller size. process call focus attention would beneﬁcial guarantee that high probability training given training policy every positive example serves negative example targets confusion set. large training might become computationally infeasible. train means training targets together. test means confusion size includes targets. results shown table suggest that terms accuracy signiﬁcant factor confusion size test stage. eﬀect confusion size training minimal note naive bayes algorithm notion negative examples exist therefore regardless size confusion training learns exactly representations. thus column confusion size training makes diﬀerence. application word predictor used might give partial solution problem. example given prediction task context speech recognition phonemes constitute word might known thus suggest generate small confusion used evaluating predictors. tables present results using artiﬁcially simulated speech recognizer using method general phonetic classes. instead transcribing word phoneme word transcribed phoneme classes. speciﬁcally experiments deviate task deﬁnition given above. confusion sets used diﬀerent sizes consist verbs different prior probabilities corpus. sets table simulating speech recognizer word error rate training testing confusion sets determined based phonetic classes simulated speech recognizer. ﬁrst experiment transcription word given broad phonetic groups phonemes belong i.e. nasals fricative etc.. example word transcribed using phonemes transcribe stands plosive vowel. partition results partition verbs several confusions sets. confusion sets consist single word therefore baseline explains high baseline. table simulating speech recognizer word error rate training testing confusion sets determined based phonetic classes simulated speech recognizer. case confusion sets less baseline used explains overall lower baseline. before train means training done targets together train means confusion sets used also training. note case snow used sparse winnow algorithm size confusion training some although small eﬀect. reason training done target words target word representation examples occur used negative examples. smaller confusion used negative examples likely true negative. munoz punyakanok roth zimak. learning approach shallow parsing. emnlp-vlc’ joint sigdat conference empirical methods natural language processing large corpora june. described language allows deﬁne expressive feature types exhibited experimentally advantage using word prediction task. argued success approach hinges combination using large expressive features along learning approach tolerate converges quickly despite large dimensionality data. believe approach would useful disambiguation tasks nlp. also presented preliminary study reduction confusion size eﬀects prediction performance. future work intend study ways determine appropriate confusion makes current task properties.", "year": 2000}