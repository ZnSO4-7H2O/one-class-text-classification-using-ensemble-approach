{"title": "Deep Generalized Canonical Correlation Analysis", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "text": "adrian benton huda khayrallah biman gujral reisinger sheng zhang raman arora center language speech processing johns hopkins university baltimore adrian†hudabgujralreisingerzshengarora† jhu.edu cogsci.jhu.edu †cs.jhu.edu present deep generalized canonical correlation analysis method learning nonlinear transformations arbitrarily many views data resulting transformations maximally informative other. methods nonlinear two-view representation learning linear many-view representation learning exist dgcca ﬁrst cca-style multiview representation learning technique combines ﬂexibility nonlinear representation learning statistical power incorporating information many independent sources views. present dgcca formulation well efﬁcient stochastic optimization algorithm solving learn dgcca representations distinct datasets three downstream tasks phonetic transcription acoustic articulatory measurements recommending hashtags friends dataset twitter users. dgcca representations soundly beat existing methods phonetic transcription hashtag recommendation general perform worse standard linear many-view techniques. multiview representation learning refers settings access many views data train time. views often correspond different modalities independent information examples scene represented series audio image frames social media user characterized messages post friend speech utterance conﬁguration speaker’s tongue. multiview techniques learn representation data captures sources variation common views. multiview representation techniques attractive intuitive reasons. representation able explain many views data likely capture meaningful variation representation good views. also attractive theoretical reasons. example anandkumar show certain classes latent variable models hidden markov models gaussian mixture models latent dirichlet allocation models optimally learned multiview spectral techniques. representations learned many views generalize better since learned representations forced accurately capture variation views time view acts regularizer constraining possible representations learned. methods often based canonical correlation analysis classical statisical technique proposed hotelling spite encouraging theoretical guarantees multiview learning techniques cannot freely model nonlinear relationships arbitrarily many views. either able model variation across many views learn linear mappings shared space simply cannot applied data views using existing techniques based kernel deep present deep generalized canonical correlation analysis unlike previous correlation-based multiview techniques dgcca learns shared representation data arbitrarily many views simultaneously learns nonlinear mappings view shared space. constraint nonlinear mappings views shared space must differentiable. main methodological contribution derivation gradient update generalized canonical correlation analysis objective practical contribution also released implementation dgcca. also evaluate dgcca-learned representations distinct datasets three downstream tasks phonetic transcription aligned speech articulatory data twitter hashtag friend recommendation text network feature views. downstream performance dgcca representations ultimately task-dependent. however clear gains performance dgcca tasks previously shown beneﬁt representation learning views improvement heldout accuracy phonetic transcription. paper organized follows. review prior work section section describe dgcca. empirical results synthetic dataset three downstream tasks presented section section describe differences dgcca non-cca-based multiview learning work conclude future directions section successful techniques multiview representation learning based canonical correlation analysis extension nonlinear many view settings describe section. related multiview learning techniques section canonical correlation analysis statistical method ﬁnds maximally correlated linear projections random vectors fundamental multiview learning technique. given input views covariance matrices respectively cross-covariance matrix ﬁnds directions maximize correlation them technique limitations signiﬁcant extensions first limited learning representations linear transformations data view second leverage input views. deep extension addresses ﬁrst limitation ﬁnding maximally linearly correlated non-linear transformations vectors. passing input views stacked non-linear representations performing outputs. represent network outputs. weights networks trained standard backpropagation maximize objective another extension addresses limitation number views generalized corresponds solving optimization problem equation ﬁnding shared representation different views number data points dimensionality view dimensionality learned representation rdj×n data matrix view. solving gcca requires ﬁnding eigendecomposition matrix scales quadratically sample size leads memory constraints. unlike dcca learn projections transformations views gcca also learns view-independent representation best reconstructs view-speciﬁc representations simultaneously. limitation gcca learn linear transformations view. section present deep gcca multiview representation learning technique beneﬁts expressive power deep neural networks also leverage statistical strength views data unlike deep limited views. fundamentally deep deep gcca different objectives optimization problems immediately clear extend deep views. dgcca learns nonlinear view order maximize correlation learnt representations across views. training dgcca passes input vectors view multiple layers nonlinear transformations backpropagates gradient gcca objective respect network parameters tune view’s network illustrated figure objective train networks reduce gcca reconstruction error among outputs. test time data projected feeding learned network view. formally deﬁne dgcca problem. consider views data rdj×n denote input matrix. network view consists layers. assume simplicity layer view network units ﬁnal layer size output layer view rck×ck− weight matrix layer nonlinear activation function view network. denote output ﬁnal layer rr×n shared representation interested learning. optimization solve dgcca optimization problem using stochastic gradient descent mini-batches. particular estimate gradient dgcca objective problem mini-batch samples mapped network back-propagation update weight matrices j’s. however note dgcca optimization problem constrained optimization problem. immediately clear perform projected gradient descent back-propagation. instead characterize objective function gcca problem optimum compute gradient respect inputs gcca i.e. respect network outputs. gradients back-propagated network update j’s. although relationship dgcca gcca analogous relationship dcca derivation gcca objective gradient respect network output layers non-trivial. main difﬁculty stems fact natural extension correlation objective random variables. instead consider correlations every pair views stack matrix maximize certain matrix norm matrix. gcca suggests optimization problem maximizes correlations shared representation view. since objective well constraints generalized problem different problem immediately obvious extend deep deep gcca. next show sketch gradient derivation full derivation given appendix straightforward show solution gcca problem given solving eigenvalue problem. particular deﬁne roj×oj scaled empirical covariance rn×n corresponding matrix network output projection matrix whitens data; note symmetric idempotent. deﬁne since positive semi-deﬁnite then easy check rows thus minimum thus gradient difference r-dimensional auxiliary representation embedded subspace spanned columns projection actual data onto said subspace intuitively auxiliary representation away view-speciﬁc representation network weights receive large update. computing gradient descent update time complexity largest dimensionality input views. importantly view linear transformation data separates mixture components sense generative structure data could exploited linear model. point reinforced figure shows two-dimensional representation learned applying gcca data figure learned representation completely loses structure data. figure matrix learned applying gcca dgcca data figure contrast failure gcca preserve structure result applying dgcca; case input neural networks three hidden layers units weights randomly initialized. plot representation learned dgcca figure representation mixture components easily separated linear classiﬁer; fact structure largely preserved even projection onto ﬁrst coordinate also illustrative consider view-speciﬁc representations learned dgcca consider outputs neural networks trained maximize gcca objective. plot representations figure view learned nonlinear mapping remarkably well making mixture components linearly separable. recall absolutely direct supervision given mixture component point generated from. training signals available networks reconstruction errors network outputs learned representation section discuss experiments university wisconsin x-ray microbeam database xrmb contains acoustic articulatory recordings well phonemic labels. present phoneme classiﬁcation results acoustic vectors projected using dcca gcca dgcca. acoustic articulatory data views phoneme labels third view gcca dgcca. classiﬁcation k-nearest neighbor classiﬁcation projected result. data train/tune/test split data arora livescu limit experiment runtime subset speakers experiments. cross-speaker experiments using male speaker training splits tuning testing. also perform parameter tuning third view -fold cross validation using single speaker experiments acoustic articulatory measurements views dcca. following pre-processing andrew dimensional feature vectors ﬁrst second view respectively. speaker frames. third view gcca dgcca -dimensional one-hot vectors corresponding labels frame following arora livescu parameters ﬁxed network size regularization ﬁrst views containing three hidden layers sigmoid activation functions. hidden layers acoustic view width layers articulatory view width units. penalty constants used train acoustic articulatory view networks respectively. output layer dimension network dcca dgcca. -fold speaker-dependent experiments performed grid search network sizes covariance matrix regularization third view fold. hyperparameters experiments optimizing networks minibatch stochastic gradient descent step size batch size learning decay momentum. third view neural network penalty addition accuracy examine reconstruction error i.e. objective equation obtained objective gcca dgcca. sharp improvement reconstruction error shows non-linear algorithm better model data. experimental setup dcca under-performs baseline simply running original acoustic view. prior work considered output dcca stacked central frame original acoustic view poor performance absence original features indicates able informative projection original acoustic features based correlation articulatory view within ﬁrst dimensions. highlight improvements dgcca gcca figure presents subset confusion matrices speaker-dependent test data. particular observe large improvements classiﬁcation gcca outperforms dgcca matrices also highlight common misclassiﬁcations dgcca improves upon. instance dgcca rectiﬁes frequent misclassiﬁcation gcca. addition commonly incorrect classiﬁcation phonemes corrected dgcca enables better performance voiceless consonants like vowels classiﬁed almost equal accuracy methods. twitter user hashtag friend recommendation linear multiview techniques effective recommending hashtag friends twitter users experiment views twitter user constructed applying principal component analysis bag-of-words representations tweets posted user mentioned users friends followers well one-hot encodings local friend follower networks. learn evaluate dgcca models identical training development test sets benton evaluate dgcca representations macro precision recall hashtag friend recommendation tasks described there. trained different dgcca model architectures identical architectures across views width hidden output layers view drawn uniformly auxiliary representation width drawn uniformly networks used relus activation functions optimized adam epochs. networks trained twitter users users used tuning estimate heldout reconstruction error model selection. report development test results best performing model downstream task development set. learning rate regularization constants weights chose restrict single hidden layer non-linear activation identical architectures view avoid ﬁshing expedition. dgcca appropriate learning twitter user representations good architecture require little exploration. several points note first dgcca outperforms linear methods hashtag recommendation wide margin terms recall. exciting task shown beneﬁt incorporating views twitter users. results suggest nonlinear transformation input views yield additional gains performance. addition wgcca models sweep every possible weighting views weights wgcca distinct advantage model allowed discriminatively weight views maximize downstream performance. fact dgcca able outperform wgcca hashtag recommendation encouraging since wgcca much freedom discard uninformative views whereas dgcca objective forces networks minimize reconstruction error equally across views. noted benton friend network view useful learning representations friend recommendation unsurprising dgcca applied views cannot compete wgcca representations learned single useful friend network view. strong work outside cca-related methods combine nonlinear representation learning multiple views. kumar elegantly outlines main approaches methods take learn joint representation many views either explicitly maximizing pairwise similarity/correlation views alternately optimizing shared consensus representation view-speciﬁc transformations maximize similarity. models siamese network proposed masci fall former camp minimizing squared error embeddings learned view leading quadratic increase terms loss function size number views increase. rajendran extend correlational neural networks many views avoid quadratic explosion loss function computing correlation view embedding embedding pivot view. although model appropriate tasks multilingual image captioning many datasets clear method choosing pivot view. dgcca objective suffer quadratic increase w.r.t. number views require privileged pivot view since shared representation learned per-view representations. approaches estimate consensus representation multiview spectral clustering approach kumar typically alternating optimization scheme depends strong initialization avoid local optima. gcca objective work builds particularly attractive since admits globally optimal solution view-speciﬁc projections shared representation singular value decomposition single matrix per-view projection matrices. local optima arise dgcca objective also learning nonlinear transformations input views. nonlinear multiview methods often avoid learning nonlinear transformations assuming kernel graph laplacian given present dgcca method non-linear multiview representation learning arbitrary number views. show dgcca clearly outperforms prior work using labels third view successfully exploit multiple views learn user representations useful downstream tasks hashtag recommendation twitter users. date cca-style multiview learning techniques either restricted learning representations views strictly linear transformations input views. work overcomes limitations. performance wgcca suffers compared whitening friend network data ignores fact spectrum decays quickly long tail ﬁrst principal components made large portion variance data also important compare users based components. references animashree anandkumar rong daniel sham kakade matus telgarsky. tensor decompositions learning latent variable models. journal machine learning research galen andrew raman arora jeff bilmes karen livescu. deep canonical correlation analysis. proceedings international conference machine learning raman arora karen livescu. multi-view learning supervision transformed bottleneck features. acoustics speech signal processing ieee international conference ieee harold hotelling. relations sets variates. biometrika kettenring. canonical analysis several sets variables. biometrika diederik kingma jimmy adam method stochastic optimization. arxiv preprint jonathan masci michael bronstein alexander bronstein j¨urgen schmidhuber. multimodal similarity-preserving hashing. ieee transactions pattern analysis machine intelligence janarthanan rajendran mitesh khapra sarath chandar balaraman ravindran. bridge correlational neural networks multilingual multimodal representation learning. arxiv preprint arxiv. abhishek sharma abhishek kumar daume david jacobs. generalized multiview analysis discriminative latent space. computer vision pattern recognition ieee conference ieee weiran wang raman arora karen livescu jeff bilmes. unsupervised learning acoustic features deep canonical correlation analysis. proc. ieee int. conf. acoustics speech sig. proc. weiran wang raman arora karen livescu nathan srebro. stochastic optimization deep nonlinear orthogonal iterations. proceedings annual allerton conference communication control computing john westbury. x-ray microbeam speech production database users handbook. waisman center mental retardation human development university wisconsin madison order train neural networks dgcca need compute gradient gcca objective respect input views. gradient backpropagated input networks derive updates network weights. number data points number views. data matrix representing output neural network i.e. number neurons output layer network. then gcca written following optimization problem dimensionality learned auxiliary representation thus gradient difference r-dimensional auxiliary representation embedded subspace spanned columns projection network outputs onto said subspace intuitively auxiliary representation away view-speciﬁc representation network weights receive large update. methods typically evaluated intrinsically amount correlation captured reconstruction error. measures dependent width shared embeddings viewspeciﬁc output layers necessarily predict downstream performance. although reconstruction error cannot solely relied model selection downstream task found useful signal weed poor models. figure shows reconstruction error hashtag prediction recall initial grid search dgcca hyperparameters. models tuning reconstruction error greater safely ignored variability performance models achieving lower error. since dgcca model high reconstruction error suggests views agree makes sense shared embedding likely noisy whereas relatively lowly reconstruction error suggests transformed views converged stable solution.", "year": 2017}