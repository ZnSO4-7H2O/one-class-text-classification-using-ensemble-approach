{"title": "On the Prior Sensitivity of Thompson Sampling", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The empirically successful Thompson Sampling algorithm for stochastic bandits has drawn much interest in understanding its theoretical properties. One important benefit of the algorithm is that it allows domain knowledge to be conveniently encoded as a prior distribution to balance exploration and exploitation more effectively. While it is generally believed that the algorithm's regret is low (high) when the prior is good (bad), little is known about the exact dependence. In this paper, we fully characterize the algorithm's worst-case dependence of regret on the choice of prior, focusing on a special yet representative case. These results also provide insights into the general sensitivity of the algorithm to the choice of priors. In particular, with $p$ being the prior probability mass of the true reward-generating model, we prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the bad- and good-prior cases, respectively, as well as \\emph{matching} lower bounds. Our proofs rely on the discovery of a fundamental property of Thompson Sampling and make heavy use of martingale theory, both of which appear novel in the literature, to the best of our knowledge.", "text": "abstract. empirically successful thompson sampling algorithm stochastic bandits drawn much interest understanding theoretical properties. important beneﬁt algorithm allows domain knowledge conveniently encoded prior distribution balance exploration exploitation eﬀectively. generally believed algorithm’s regret prior good little known exact dependence. paper ﬁrst step towards answering important question focusing special representative case fully characterize algorithm’s worst-case dependence regret choice prior. corollary results also provide useful insights general sensitivity algorithm choice priors structural assumptions made. particular prior probability mass true upper bounds poorgood-prior cases respectively well matching lower bounds. proofs rely fundamental property thompson sampling make heavy martingale theory appear novel thompson-sampling literature useful studying behavior algorithm. thompson sampling also known probability matching posterior sampling popular strategy solving stochastic bandit problems. important beneﬁt algorithm allows domain knowledge conveniently encoded prior distribution address exploration-exploitation tradeoﬀ eﬀectively. paper focus sensitivity algorithm prior uses. rest section ﬁrst deﬁne bandit setting notation describe thompson sampling; discuss previous works related present paper. receives reward xitt eligible action-selection strategy chooses actions step based past observed rewards xiss; potentially external source randomness. background make following stochastic assumption underlying rewardgenerating mechanism. countable possible reward-generating models. true underlying model rewards i.i.d. random variables taking values drawn known distribution mean course agent knows neither true underlying model optimal action yields highest expected reward. performance agent measured regret incurred always selecting optimal action. precisely frequentist regret eligible action-selection strategy certain reward-generating model deﬁned expectation taken respect rewards i∈at≥ generated according model potential external source randomness. imposes prior distribution natural consider pulls argmaxi∈a concreteness assume distributions )i∈aθ∈θ absolutely continuous respect common measure likelihood functions i)i∈aθ∈θ. posterior distributions computed retwo remarks order. first setup discretized version rather general bandit problems. example k-armed bandit special case cartesian product sets reward distributions arms. another example linear bandits candidate coeﬃcient vectors determine expected reward function. discretization provides convenient useful approximation leads simplicity expositions analysis. abstract formulation analogous expert setting widely studied online-learning literature also recent study thompson sampling experts recently thompson sampling gained interest largely empirical successes furthermore strategy often easy combined complex reward models easy implement asymptotic no-regret results known empirical successes inspired ﬁnite-time analyses deepen understanding strategy. classic k-armed bandits regret bounds comparable widely studied algorithms obtained matching well-known upper bound proved bounds providing interesting insights algorithm assume non-informative priors essentially show thompson sampling comparable regret popular strategies especially based upper conﬁdence bounds. unfortunately bounds show role prior plays performance algorithm. contrast variant thompson sampling proposed bound depends explicitly entropy prior however bound dependence likely sub-optimal. another line work literature focuses bayes regret informative prior. previous work shown that prior twoarmed case -approximation optimal strategy minimizes stochastic regret also shown k-armed case bayes regret always upper bounded prior results later improved prior-dependent bound entropy bound elegantly quantiﬁes terms averaged regret thompson sampling exploits prior distributions tell well thompson sampling works individual problems. indeed analysis bayes regret unclear good prior means theoretical perspective deﬁnition bayes regret essentially assumes prior correctly speciﬁed. extreme case prior point mass bayes regret trivially best knowledge work ﬁrst consider frequentist regret thompson sampling informative prior. speciﬁcally focus understanding ts’s sensitivity choice prior making progress towards better understanding popular bayesian algorithm. shown that strong prior lower bayes regret substantially beneﬁt comes cost true model happens assigned prior frequentist regret large consistent recent result pareto regret frontier ﬁndings suggest thompson sampling -exploring general. techniques like minimonster algorithm necessary modify thompson sampling make less prior-sensitive. open question whether modiﬁed thompson sampling algorithms still take advantage informative prior enjoy small bayes regret. finally analysis makes critical certain martingale property thompson sampling. although martingales applied hypothesis testing example analyzing statistical behavior likelihood ratios martingales analyze behavior posteriors best knowledge. moreover diﬀerent martingale property used authors study bayesian multi-armed bandit problem reward current state expected reward distribution next state play made current state martingale property diﬀerent ours martingales apply reward current state refers inverse posterior probability mass true model naturally expect regret thompson sampling small true reward-generating model given large prior probability mass vice versa. interesting important question understand sensitivity algorithm’s regret prior takes input. take minimalist approach investigate special meaningful case. results fully characterize worst-case dependence ts’s regret prior also provides important insights general case corollary. furthermore analysis appears novel best knowledge making heavy martingale techniques analyze behavior posterior probability. techniques useful studying bandit algorithms. comments order. first goal work solve specialized bandit problems rather understand prior sensitivity seemingly simplistic problems happen nontrivial enough useful constructive proof matching lower bounds. second understand ts’s prior sensitivity without making structural assumptions natural next step work investigate structural robust prior. upper-bound analysis requires following smoothness assumption likelihood functions models note assumption needed upper-bound analysis lower-bound proofs. single application bayes rule change posteriors much analogous bounded gradients rewards online-learning literature. hand small value assumption tends create hard problems thompson sampling since models less distinguishable. therefore assumption trivialize problem. remark upper bounds dependence lower bounds given theorems below. moreover bounds increasing functions smoothness parameter problems small tend harder thompson sampling upper bounds tight universal constant fairly general class hard problems. conjecture dependence artifact proof techniques removed tighter upper bounds problem instances -actions-and--models case. space limit include important novel challenging parts analysis paper. complete proof together simulation results corroborating theoretical ﬁndings given full version large conjecture general upper bounds improved match lower bounds corollary especially small remains open extend proof techniques -actions-and--models case tight general upper bounds. bayesian algorithm worst-case dependence prior thompson sampling logarithmic factors. partly explained fact algorithms designed perform well worst-case scenario. contrary design thompson sampling takes advantage prior information eﬃciently cases especially certain structure model space note paper impose structure thus lower bounds contradict existing results literature non-informative priors small typically large). finally proof techniques thompson sampling literature best knowledge. observation inverse posterior probability true underlying model martingale allows results techniques martingale theory quantify time probability posterior distribution hits certain threshold. then regret thompson sampling analyzed separately hitting times. section study fundamental martingale property thompson sampling implications. results essential proving upper bounds section note similar property holds posterior updates using bayes rule however involve action selection. conditional expectation moreover denote expectation true underlying model i.e. distribution notation similarly deﬁned. furthermore shorthand min{a lemma assume countable true reward-generating model. then stochastic process −)t≥ martingale respect ﬁltration consider -actions-and--models case. constants deﬁne following hitting times hitting probabilities inf{t inf{t martingale property implies following results used repeatedly proofs results. however proven bayes risk always upper therefore must bounded well deﬁned lemma −)t≥ martingale. easy verify stopping times respect ﬁltration follows doob’s optional stopping theorem moreover pt∧τa∧τb hence lebesgue’s dominated convergence theorem eθ−] eθ−] thus section focus -actions-and--models case. present prove results upper bounds frequentist regret thompson sampling. space limitation sketch proof poor-prior case complete proofs including good-prior case appear long version. proofs upper bounds rely several propositions reveal interesting recursions thompson sampling’s regret function prior. although propositions similar analytic techniques diﬀer many important details. space limitation sketch proof proposition technical lemmas propositions developed ready prove ﬁrst upper bound theorem small. second bound large proved similar fashion although details quite diﬀerent work studied important aspect popular thompson sampling strategy stochastic bandits sensitivity prior. focusing special nontrivial problem fully characterized worst-case dependence regret prior goodbad-prior cases matching upper lower bounds. lower bounds also extended general case corollary quantifying inherent sensitivity algorithm prior poor structural assumptions made. results suggest interesting directions future work four outlined here. close upper lower bounds general multiple-model case. conjecture tighter upper bound likely match lower bound corollary second consider prior sensitivity structured stochastic bandits models related certain ways. example discretized version multi-armed bandit problem prior probability mass true model exponentially small uniform prior used strong frequentist regret bound still possible. sensitivity analysis problems provide useful insights guidance applications thompson sampling. thrid remains open whether exists algorithm whose worst-case regret bounds better model. question related recent study pareto regret front conjecture answer negative especially -actions-and--models case. finally interesting consider problem-dependent regret bounds often scale logarithmically", "year": 2015}