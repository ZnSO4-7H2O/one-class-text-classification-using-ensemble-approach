{"title": "Simultaneously Learning Neighborship and Projection Matrix for  Supervised Dimensionality Reduction", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Explicitly or implicitly, most of dimensionality reduction methods need to determine which samples are neighbors and the similarity between the neighbors in the original highdimensional space. The projection matrix is then learned on the assumption that the neighborhood information (e.g., the similarity) is known and fixed prior to learning. However, it is difficult to precisely measure the intrinsic similarity of samples in high-dimensional space because of the curse of dimensionality. Consequently, the neighbors selected according to such similarity might and the projection matrix obtained according to such similarity and neighbors are not optimal in the sense of classification and generalization. To overcome the drawbacks, in this paper we propose to let the similarity and neighbors be variables and model them in low-dimensional space. Both the optimal similarity and projection matrix are obtained by minimizing a unified objective function. Nonnegative and sum-to-one constraints on the similarity are adopted. Instead of empirically setting the regularization parameter, we treat it as a variable to be optimized. It is interesting that the optimal regularization parameter is adaptive to the neighbors in low-dimensional space and has intuitive meaning. Experimental results on the YALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the proposed method.", "text": "abstract—explicitly implicitly dimensionality reduction methods need determine samples neighbors similarity neighbors original highdimensional space. projection matrix learned assumption neighborhood information known ﬁxed prior learning. however difﬁcult precisely measure intrinsic similarity samples high-dimensional space curse dimensionality. consequently neighbors selected according similarity might projection matrix obtained according similarity neighbors optimal sense classiﬁcation generalization. overcome drawbacks paper propose similarity neighbors variables model low-dimensional space. optimal similarity projection matrix obtained minimizing uniﬁed objective function. nonnegative sum-to-one constraints similarity adopted. instead empirically setting regularization parameter treat variable optimized. interesting optimal regularization parameter adaptive neighbors low-dimensional space intuitive meaning. experimental results yale coil mnist datasets demonstrate effectiveness proposed method. computer vision system high-dimensional. known curse dimensionality occurs number training samples class smaller dimension samples. hand high dimension data gives arise overﬁtting problem limits generalization ability system. hand high dimension data leads efﬁciency classifying image. therefore dimensionality reduction fundamental task many applications computer vision pattern recognition. data proper criterion constraints. lowdimensional representation achieved projection matrix whose number columns smaller dimension input data. learn projection matrix required almost methods relationship highdimensional training samples known computed. relationship information includes samples neighbors similarity pair samples. example classical predeﬁned number neighbors selected according euclidian distance high-dimensional space similarity pair samples computed using exponential function. supervised algorithm lfda computes neighbors similarity class-wise manner. note classical methods lfda process selecting neighbors computing similarity independently process learning projection matrix. argue neighbors high-dimensional space necessarily neighbors underlying lowdimensional space similarity obtained highdimensional space hence capture intrinsic similarity. example shown fig. original high-dimensional space feature stands lightness feature stands length. assume lightness feature unstable true fig. nearest neighbor two-dimensional space projected onto one-dimensional space transformed respectively. one-dimensional space nearest neighbor instead ||a′ c′|| ||a′ b′||. respectively. one-dimensional space nearest neighbor many dimensionality reduction methods methods divided supervised unsupervised semi-supervised methods point view whether class labels utilized. proposed method belongs supervised category. according similarity samples obtained used dimensionality reduction methods divided categories methods label-oriented similarity methods feature-oriented similarity method differs existing methods point view similarity samples section mainly review methods label-oriented similarity methods feature-oriented similarity. note beyond scope paper several classical kinds dimensionality reduction methods manifoldbased methods tensor-based methods probabilistic methods covariance based methods non-negative methods sparseness low-rank based methods dimensionality reduction method label-oriented similarity similarity samples depends labels. generally pairs samples share similarity. supervised method pairs samples class similarity similarity class either equal unequal similarity another class. representative supervised methods variants unsupervised method pairs samples whole training similarity. representative unsupervised methods variants pca. suppose training samples basis vector optimal solution used dimensionality reduction. learns optimal basis vector training based least squares reconstruction criterion equivalently maximum variance criterion axis one-dimensional space samples correctly classiﬁed classiﬁer nearest neighbor. nearest neighbor two-dimensional space feature lightness discriminative three samples transformed one-dimensional space spanned vertical axis. speciﬁcally transformed respectively. one-dimensional space nearest neighbor instead ||a′ c′|| ||a′ b′||. example demonstrates neighbors obtained high-dimensional space might correct computing neighbors proper low-dimensional space might better purpose classiﬁcation. inspired example shown fig. similarity computed high-dimensional space directly used similarity low-dimensional space. similarity ﬁxed vary low-dimensional representation. based insight propose objective function similarity projection matrix mapping high-dimensional space low-dimensional space unknown variables. summary novelties contributions paper follows. formulate similarity pair samples projection matrix variables found. traditional methods projection matrix expressed variable whereas similarity ﬁxed computed original high-dimensional space. jointing optimizing similarity projection matrix expected method able yields optimal solutions. therefore proposed similarity classiﬁcation-oriented whereas existing similarity feature-oriented. method proposed similarity satisﬁes sum-toone constraint non-negative constraint. similarities sample samples equals one. thus proposed non-negative similarity satisﬁes properties probability. within class condition makes sample neighbor sample. theoretical analysis shows optimal similarity function projection matrix. proposed uniﬁed objective function regularization parameter similarity norm penalty term. penalty term makes similarity sparse extent. samples neighbors sample fraction samples neighbors sample. instead empirically setting regularization parameter treat variable optimized. theoretical analysis shows regularization parameter related squared distances neighbors low-dimensional space. optimal regularization parameter also function projection matrix. remainder paper organized follows section related work discussed. proposed slnp algorithm described section experimental results given section summarizing concluding label-oriented similarity samples completely determined labels samples. therefore labeloriented similarity irrelevant features samples. however values feature vectors important measuring similarity samples. generally speaking feature-oriented similarity superior label-oriented similarity class labels also features used computing similarity. representative feature-oriented methods include lfda solde jglda seen similarity function difference feature vector feature vector therefore similarity called featureoriented. similarity also depends parameter usually empirically chosen. effect weighting difference feature-oriented similarity ensure that close original high-dimensional space low-dimensional representations close well point view graph embedding implies similarities equal pair samples training set. label-oriented similarity also interpreted samples neighbors sample difference similarities. training samples divided different classes class labels class label sample denoted number samples class aims ﬁnding optimal basis vector maximizes rayleigh coefﬁcient equivalently minimizes inverse rayleigh coefﬁcient training samples divided different classes class consists samples. subscripts index class sample class respectively. sake notation simplicity assumed note proposed theory algorithm work also d-dimensional representation d-dimensional sample obtained similarity similarity sijk denote similarity between sample sample class similarities class form symmetric similarity matrix j-th column vector stands similarities j-th sample class k-th element sijk. similarity matrices classes form similarity tensor rc×n traditional methods pre-deﬁned similarity according class labels values samples xik. method similarity sijk variable satisﬁes properties probability non-negative constraints sum-to-one constraints guarantee similarity sijk probability. effect whitening constraints letting features total training samples equal variance. total scatter matrix respectively. feature-oriented similarity local resulting similarity lfda makes lfda capable dealing multimodal class composed samples several separate clusters. similarities expressed optimization problem lfda form analysis section shows existing methods employ ﬁxed similarities learning projection vectors computation similarities prior independent computation projection vectors. shown fig. traditional label-oriented similarity featureoriented similarity categorized ﬁxed similarity. paper propose variable similarity learning better projection vectors. proposed variable similarity varies projection vector classiﬁcation-oriented. variable similarity projection vector formulated uniﬁed objective function proper constraints similarity projection vector. effect regularization term similarities sparse. effect line intuition small number samples similar sample neighboring samples small region sample. regularization parameters form matrix rc×ni entry i-th regularization parameters corresponding class denote transpose column vector rni×. call regularization matrix. task optimal similarity tensor rc×n projection matrix rd×d regularization parameter propose alternative algorithm seek optimal variables turn. three-order tensor contains similarity matrices similarity matrix consists similarities class entry sijk. similarity matrix independent similarity matrices optimal matrix individually calculated. optimization problem becomes computation high bounds order guarantee similarity sijk low-dimensional samples nearest neighbors low-dimensional sample distance dijk ||wt xik|| used determining neighbors without loss generality assume distances ascent order dijm consequently sijk sijk algorithm training algorithm proposed slnp method input classes training samples xcnc rd×. number neighbors. number iterations. output projection matrix rd×d similarity tensor call proposed method slnp training algorithm optimal projection matrix given algorithm test stage low-dimensional representation test sample obtained classiﬁers trained low-dimensional version training samples. type classiﬁers adopted. emphasis contribution dimensionality classical classiﬁer nearest neighbor employed evaluation proposed slnp method. implies regularization parameter related squared distances neighbors lowdimensional space. regularization parameter increases distances low-dimensional space. low-dimensional distances neighbors large give large penalty similarity. therefore method regularization parameter adaptive neighbors low-dimensional space intuitive meaning. left right rest face images denoted -dimensional subspace similarities ﬁrst sample rest samples computed. prior iteration similarities equal iteration proceeds similarities change. last iteration similarity largest similarity among similarities samples moreover last iteration similarity smallest similarity among similarities samples comparing images image similar illumination condition image whereas image quite different computed similarities consistent intuition. summary following phenomena observed. image similar appearance largest similarity reference image image quite different appearance smallest similarity reference image. though similarities reference image training images different different large belong class. compare learned similarity traditional similarity used lpp. similarity method closely related projection matrix similarity irrelevant projection matrix. noted many variants above-mentioned methods proposed. despite success methods break basic frameworks classical flda lsda sense ﬁnding neighbors computing similarities original highdimensional space. note also almost methods employ pre-reduce dimension highdimensional data order avoid singularity problem speed training process. method also follows strategy. rd×dp projection matrix pca. wsln ca×d projection matrix slnp learning transformed samples caxij ﬁnal projection matrix cawsln number features extracted relatively large number dsln features extracted features relatively small. parameters dsln experimentally determined. extended yale face database contains images human subjects poses illumination conditions examples face images shown fig. experiments image size normalized pixels. original image dimensional space. convergence. investigate convergence proposed images subject randomly selected training images. dsln fig. shows objective function varies iteration number convergence achieved iteration number therefore proposed method good convergence property. property learned similarities. fig. shows similarities change iteration. fig. face images belong class class label face image rectangle denoted vector robustness number neighbors. algorithm parameters sijk learned automatically whereas number neighbors manually set. therefore worth investigating whether proposed method sensitive number neighbors. comparison methods dsln experimentally determined. plot fig. recognition rate changes recognition rate increases fast become stable slightly decreases used method compare methods. table gives recognition rates different methods different number samples class training. recognition rates increase importantly proposed slnp method achieves best performance. face vector taken reference. face vectors decently sorted according similarities reference fig. shows sorted results proposed similarities employed fig. shows results corresponding traditional similarities. observed method able give reasonable sorting results. example similar fig. similar fig. attached shadow nose whereas attached shadow exists similarity optimal low-dimensional space method capable ﬁlling semantic gap. traditional similarities employed following phenomena observed. traditional similarity inferior proposed sense capturing semantic similarity. though images belong class difference similarity large. example whereas property learned regularization parameter. tells regularization parameter function distances neighbors low-dimensional space. optimal regularization parameter obtained iteratively applying different sample class corresponds different regularization parameter intuitively understand regularization parameter compute average regularization parameter fig. shows images class class class corresponding average regularization parameters respectively. order average regularization parameters order relationship explained follows. intrinsic variation class largest variation class least. regularization parameter sensitive intrinsic variations samples. rates cases. superiority slnp remarkable recognition rates slnp whereas recognition rates lsda lfda respectively. lsnp outperforms lsda lfda respectively. coil- database consists objects images class objects placed motorized turntable rotated degrees every degrees time. experiment image down-sampled size pixels. examples images shown fig. number neighbors different number samples class used training rest samples used test. parameters dsln corresponding different given table iii. randomly samples images dataset. images normalized pixels. ﬁrst investigate fig. recognition rate changes number neighbors dsln specially results best recognition performance. therefore used following experiments. however noted differences recognition rates signiﬁcant. performance insensitive belhumeur hespanha kriegman eigenfaces fisherfaces recognition using class speciﬁc linear projection ieee trans. pattern anal. mach. intell. vol. jul. georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee trans. pattern anal. mach. intell. vol. jun. yang zhang locality-constrained label embedding dictionary learning algorithm image classiﬁcation ieee trans. neural netw. learning syst. vol. methods also given. generally speaking advantage proposed slnp methods becomes signiﬁcant number samples class small. recognition rates slnp lfda lsda respectively. recognition rates slnp lfda lsda respectively slnp outperforms lfda outperforms lfda paper presented supervised dimensionality reduction. letting similarity neighbors depend projection matrix proposed objective function consists similarity data term similarity norm penalty term imposed nonnegative sum-to-one constraints similarity. alternative algorithm developed optimal similarities projection matrix regularization parameter lagrangian multiplier. theoretical analysis showed optimal similarities regularization parameter lagrangian multiplier functions distances lowdimensional space spanned projection matrix.", "year": 2017}