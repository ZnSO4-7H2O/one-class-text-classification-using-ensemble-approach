{"title": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces", "tag": ["cs.AI", "cs.LG"], "abstract": "Graph models are relevant in many fields, such as distributed computing, intelligent tutoring systems or social network analysis. In many cases, such models need to take changes in the graph structure into account, i.e. a varying number of nodes or edges. Predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics, e.g. with respect to user behaviour. However, predictive techniques in the past have almost exclusively focused on single edges or nodes. In this contribution, we attempt to predict the future state of a graph as a whole. We propose to phrase time series prediction as a regression problem and apply dissimilarity- or kernel-based regression techniques, such as 1-nearest neighbor, kernel regression and Gaussian process regression, which can be applied to graphs via graph kernels. The output of the regression is a point embedded in a pseudo-Euclidean space, which can be analyzed using subsequent dissimilarity- or kernel-based processing methods. We discuss strategies to speed up Gaussian Processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems. We find that simple regression methods, such as kernel regression, are sufficient to capture the dynamics in the theoretical models, but that Gaussian process regression significantly improves the prediction error for real-world data.", "text": "graph models relevant many ﬁelds distributed computing intelligent tutoring systems social network analysis. many cases models need take changes graph structure account i.e. varying number nodes edges. predicting changes within graphs expected yield important insight respect underlying dynamics e.g. respect user behaviour. however predictive techniques past almost exclusively focused single edges nodes. contribution attempt predict future state graph whole. propose phrase time series prediction regression problem apply dissimilaritykernel-based regression techniques -nearest neighbor kernel regression gaussian process regression applied graphs graph kernels. output regression point embedded pseudo-euclidean space analyzed using subsequent dissimilaritykernel-based processing methods. discuss strategies speed gaussian processes regression cubic linear time evaluate approach well-established theoretical models graph evolution well real data sets domain intelligent tutoring systems. simple regression methods kernel regression suﬃcient capture dynamics theoretical models gaussian process regression signiﬁcantly improves prediction error real-world data. model connections entities graphs oftentimes method choice e.g. model traﬃc connections cities data lines ∗funding grant number citec center excellence computing nodes communication people social networks structure student’s solution learning task intelligent tutoring system examples nodes well connections change signiﬁcantly time. example traﬃc graphs traﬃc load changes signiﬁcantly course making optimal routing timedependent problem distributed computing distribution computing load communication machines crucially depends availability speed connections current load machines changes time social networks communication networks users enter network users leave interactions users change rapidly intelligent tutoring systems students change solution time closer correct solution cases would beneﬁcial predict next state graph question provides opportunity intervene negative outcomes occur example re-routing traﬃc providing additional bandwidth required provide helpful hints students. traditionally predicting future development based knowledge past topic time series prediction wide-ranging applications physics sociology medicine engineering ﬁnance ﬁelds however classic models time series prediction arima narx kalman ﬁlters recurrent networks reservoir models focus vectorial data representations equipped handle time series graphs accordingly past work predicting changes graphs focused simpler sub-problems phrased classic problems e.g. predicting overload energy network predicting appearance single edges social network contribution develop approach address time series prediction problem graphs frame regression problem structured data input output. approach steps first represent graphs pairwise kernel values well-researched scientiﬁc literature representation embeds discrete graphs smooth kernel space. second within space apply similaritykernel-based regression methods nearest neighbor regression kernel regression gaussian processes predict next position kernel space given current position. note provide graph corresponds predicted point kernel space. indeed identifying corresponding graph primal space kernel pre-image problem general hard solve however show data point still analyzed subsequent kerneldissimilarity based methods. underlying dynamics kernel space captured simple regression scheme -nearest neighbor kernel regression complex models gaussian processes required. however suﬀer relatively high computational complexity kernel matrix inversion. fortunately deisenroth suggested simple strategy permit predictions linear time namely distributing prediction multiple gaussian processes handles constant-sized subset data further prepost-processing kernel data e.g. eigenvalue correction usually requires quadratic cubic time. added complexity avoided using well-known nyström approximation investigated contributions work first provide integrative overview seemingly disparate threads research related timevarying graphs. second provide scheme time series prediction dissimilarity kernel spaces. scheme compatible explicit vectorial embeddings provided graph kernels require representation. third discuss predictive result point implicit kernel feature space analyzed using subsequent kerneldissimilarity-based methods. fourth provide eﬃcient realization prediction pipeline gaussian processes linear time. finally evaluate proposed approaches theoretical practical data sets. start investigation covering related work introducing notation describe dynamics graph data. based work develop proposed approach time series prediction kernel dissimilarity spaces. also discuss speedup techniques provide predictions linear time. finally evaluate approach empirically four data sets well-established theoretical models graph dynamics namely barabasi-albert model conway’s game life well real-world data sets java programs predict next step program development theoretical models even simple regression schemes suﬃcient capture underlying dynamics java data sets applying gaussian processes signiﬁcantly reduces prediction error. dynamically changing graphs relevant many diﬀerent ﬁelds trafﬁc distributed computing social networks intelligent tutoring systems breadth ﬁeld focus relatively general concepts applied wide variety domains. begin formalisms model dynamics graphs namely time-varying graphs sequential dynamical systems turn towards research question predicting dynamics graphs. mainly addressed domain social networks umbrella link prediction well models graph growth finally preparation approach discuss graph kernels dissimilarities well prior work kernel-based approaches time series prediction. models graph dynamics time-varying graphs time-varying graphs introduced casteigts colleagues eﬀort integrate diﬀerent notations found ﬁelds delay-tolerant networks opportunistic-mobility networks social networks authors note that domains graph topology changes time inﬂuence changes severe model terms system anomalies. rather dynamics regarded integral part nature system revisit slightly varied version notation developed work. ﬁgure show example time-varying graph modeling connectivity simple public transportation graph course day. example nodes model stations edges model train connections stations. night nodes present edges lines active yet. early morning lines become active others remain inactive. finally mid-day lines scheduled active disturbance e.g. construction work station closed adjacent connections become unavailable. note example nodes stations assumed known advance. nodes known advance e.g. long-term public transportation model stations built future deﬁnition time-varying graphs extended allow inﬁnite node restriction ﬁnite number nodes present time. also relevant modelling social networks student solutions intelligent tutoring systems introduction nodes happens frequently. using notion presence function many interesting concepts static graph theory generalized dynamic version. example neighborhood node time deﬁned nodes edge exists similarly path time nodes deﬁned sequence edges nodes called connected time path exists time further deﬁne temporal subgraph graph time graph nodes edges present time note assumed discrete time deﬁnition time-varying graph. justiﬁed following consideration graph embedded continuous time changes graph take form value changes node edge presence function happen discretely presence function take values call points continuous time discrete change occurs events. assuming ﬁnitely many events write events lifetime graph ascending sequence accordingly changes graph fully described sequence temporal subgraphs therefore even timevarying graphs deﬁned continuous time fully described considering discrete lifetime sequential dynamical systems sequential dynamical systems introduced barret reidys mortvart generalize cellular automata arbitrary graphical structures essence assign binary state node static graph state updated according function maps current states node neighbors next state node induces discrete dynamical system graphs interestingly sdss related time-varying graphs interpreting binary state node time value presence function such sequential dynamical systems provide compact model node presence function time-varying graph. furthermore graph dynamics governed known future predicted simply simulating dynamic system. unfortunately underlying unknown technique applied knowledge learning schemes exists date infer underlying data. therefore predictive methods required. accordance classical time series prediction describe time series prediction graphs predicting next temporal subgraph given sequence temporal subgraphs time-varying graph. knowledge exists approach addresses problem general form. however speciﬁc subproblems addressed literature. link prediction realm social network analysis liben-nowell kleinberg formulated link prediction problem stated given sequence temporal subgraphs time-varying graph edges added graph next time step i.e. edges example given past collaborations scientiﬁc community predict collaborations future? simplest approach address challenge compute similarity index nodes rank non-existing edges according similarity index nodes predict edges index certain threshold typical similarity indices include number common neighbors time jaccard index time adar index time recent approach train classiﬁer predicts value edge presence function edges using vectorial feature representation edge time features include similarity indices discussed survey zhou list maximum-likelihood approaches stochastic models probabilistic relational models link prediction growth models seminal paper barabási albert described simple model incrementally grow undirected graph node node small fully connected seed graph since then many models graph growth emerged notably stochastic block models latent space models stochastic block models assign node block model probability edge nodes dependent respective blocks latent space models embed nodes underlying latent space model probability edge depending distance space classes models used link prediction well graph generation. further trained pre-observed data order provide accurate models data. however none discussed models addresses question time series prediction general deletion nodes edges label changes covered models. instead predicting next graph sequence directly consider indirect approaches base prediction pairwise dissimilarities simple example consider -nearest-neighbor apkernels respective successors gt+. confronted graph simply predict graph traditionally deﬁned minimum number edit operations required permitted edit operations include node insertion node transform deletion edge insertion edge deletion substitution labels nodes edges problem generalization classic string tree edit distance deﬁned minimum number operations required transform string another tree another respectively unfortunately string edit distance tree edit distance eﬃciently computed respectively computing exact graph edit distance np-hard however many approximation schemes exist e.g. relying self-organizing maps gaussian mixture models graph kernels binary linear programming particularly simple approximation scheme order nodes graph sequence apply standard string edit distance measure sequences experiments real-world java data rely kernel approximated graph distance suggested note labeled graphs desire assign non-uniform costs substituting labels. indeed labels semantically related others implying lower edit cost. learning edit costs data topic structure metric learning mainly investigated string edit distances however string edit distance applied substitute graph edits results apply graphs well graph kernels complementary view graph edit distances graph kernels represent similarity graphs instead dissimilarity. formal requirement graph kernel implicitly explicitly maps graph vectorial feature representation dot-product putes pairwise kernel values graphs graph feature vectors computing kernel least hard graph isomorphy problem thus eﬃcient graph kernels rely non-injective feature embedding still expressive enough capture important diﬀerences graphs particularly popular class graph kernels walk kernels decompose kernel graphs kernels paths taken graphs recently advances made using decompositions graphs constituent parts instead parts overall kernel constructed kernel values parts guaranteed preserve kernel property according convolutional kernel principle experiments artiﬁcial data apply shortest-path-length kernel suggested borgwardt colleagues compares lengths shortest paths graphs construct overall graph kernel suﬃciently expressive simple cases quantitative measures distance similarity supports time series prediction -nearest neighbor fashion stated above. however would like apply sophisticated prediction methods well. turn kernel-based methods. idea apply kernel-based methods time series prediction new. popular example support vector regression wide-ranging applications ﬁnance business environmental research engineering another example gaussian processes predict chemical processes motion data physics data graph kernel used provides explicit vectorial embedding graphs methods readily applied time series prediction graph data. however work generalize approach kernels implicit vectorial embedding available. enables broader range graph distances kernels graph edit distance approximation java program data. relying notation time-varying graphs introduced above describe time series graphs sequence temporal subgraphs time series prediction problem predicting next graph series gt+. ﬁrst transform problem regression problem suggested sapankevych colleagues learn function maps past states time series successor state relying distancekernel-based approaches address problem computing pairwise distancekernel values graph sequences length reis convolutional kernel principle easily construct distances kernels graph sequences distances kernels graphs. however simplicity assume remainder paper simple graph distances kernels suﬃcient. equivalent markov assumption assume next temporal subgraph depends temporal subgraph conditionally independent temporal subgraphs gt−. note markov assumption quite common related literature. example sequential dynamical systems make assumption link prediction methods graph growth models note framework generalizes nonmarkovian case replacing graph distance kernel sequence-of-graph distance kernel using everything else as-is. -nearest neighbor kernel regression gaussian process regression. revisit basic research dissimilarities similarities kernels leverage insights derive time series prediction purely latent spaces without referring explicit vectorial form. finally discuss speed gaussian process prediction cubic linear time. section introduce three non-parametric regression techniques standard form assuming vectorial input output data. explain methods applied structured input output data. assume training data tuples yi)} available desired output input further assume dissimilarity kernel input space available wherever needed. kernel regression kernel regression ﬁrst proposed nadaraya watson seen generalization -nearest neighbor smooth predictive function using kernel instead dissimilarity predictive function given similar training data point prediction degenerates. another limitation kernel regression exact reproduction training data possible i.e. achieve both training data reproduction well smooth predictive function turn gaussian process regression. gaussian process regression assume output points realization multivariate random variable gaussian distribution model extends several ways. first encode prior knowledge regarding output points mean prior distribution denoted respectively. second cover gaussian noise training output points within model. noise assume mean standard deviation n-dimensional identity matrix multivariate gaussian probability density function mean covariance matrix note assumed distribution takes outputs argument single point. posterior distribution obtained marginalization note posterior distribution again gaussian distribution. distribution mean corresponds point maximum probability density deﬁne predictive function predictive mean posterior distribution point note predictive mean becomes prior mean zero vector i.e. test data point similar training data point. methods described relies either dissimilarity kernel note strictly deﬁned concepts now. indeed dissimilarities well similarities inherently ill-deﬁned concepts. dissimilarity function form decreases input arguments sense related conversely similarity function increases input arguments sense related many cases rigorous criteria apply well violated practice hand kernels strictly deﬁned inner products. require kernel apply prediction method dissimilarity similarity available require conversion method obtain corresponding kernel. ﬁrst step note simple convert dissimilarity similarity simply apply monotonously decreasing function. context work apply radial basis function transformation further ﬁnite data points transform symmetric similarity kernel. symmetric matrix pairwise similarities data points entries symmetric similarity matrix kernel matrix positive semi-deﬁnite positive semi-deﬁniteness enforced applying eigenvalue decomposition removing negative eigenvalues matrix e.g. setting zero taking absolute value. resulting matrix applied obtain kernel matrix note eigenvalue decomposition similarity matrix cubic time-complexity number data points. however linear-time approximations recently discovered based nyström method therefore work assume kernel obtained needed. introducing three regression techniques assumed data point vector algebraic operations like scalar multiplication well matrix-vector multiplication vector addition permitted. case graphs operations well-deﬁned. however rely prior work regarding vectorial embeddings dissimilarities similarities deﬁne regression latent space. particular idea approach time series prediction structured data apply time series prediction implicit euclidean space without need explicit embedding obvious prediction successor closest training data point. graph itself providing function mapping directly graphs inputs graphs outputs. situation less clear gpr. here approach express predictive output linear combination known data points permits processing show later. particular proof resulting linear combinations convex least aﬃne. linear convex aﬃne combinations deﬁned follows deﬁnition vectors vector space real numbers. αi·xi called linear combination numbers linear coeﬃcients. linear combination called aﬃne. additionally proof prediction provided aﬃne combination requires additional assumptions. first require certain natural prior namely absence knowledge best prediction further observe training inputs outputs time series prediction special form training data presented form sequences implies point predecessor another point also successor anpoint vice versa theorem training data provided terms sequences successor eﬀect reformulate predictive function mapping test data point coeﬃcients aﬃne combination represent actual predictive result euclidean space note coeﬃcients provide primal space representation data i.e. know graph corresponds particular aﬃne combination looks like. indeed graph kernels generally injective might multiple graphs correspond aﬃne combination. finding graph called kernel pre-image problem hard solve even vectorial data poses challenge respect processing interpret data point explicit representation fortunately still address many classical questions data analysis representation relying already existing implicit embedding simply extend embedding predicted point aﬃne combination follows using extended embedding simply apply dissimilaritykernel-based method predicted point. dissimilarities includes relational learning vector quantization classiﬁcation relational neural clustering kernels includes non-parametric regression techniques discussed here also techniques like kernel vector quantization support vector machines classiﬁcation kernel variants kmeans neural clustering therefore achieved intend dissimilarity measure graphs start computing matrix pairwise dissimilarities training data. required symmetrize matrix setting diagonal zero. implicitly step embeds training data pseudo-euclidean space pairwise pseudo-euclidean distances. transform matrix similarity matrix using example radial basis function transformation. transform kernel matrix eigenvalue correction implicitly step embeds data euclidean space pairwise kernel values represent inner products training data. linear time predictions regression involves inversion matrix resulting complexity. variety eﬃcient approximation schemes exist recently robust bayesian committee machine introduced particularly fast accurate approximation rbcm approach distribute examples disjoint sets based e.g. clustering input data space. sets separate regression used yielding predictive distributions distributions combined ﬁnal predictive distribution µrbcm here prior variance prior prediction metaparameter introduced model. weights seen measure predictive power single experts. suggested authors diﬀerential entropy given note case. approach results linear-time complexity size single cluster considered constant matrix constant size inverted. challenges remain apply rbcm productively within proposed pipeline. first remains show predictive mean rbcm still form aﬃne combination. second require dissimilaritykernel-based clustering scheme runs linear time ensure overall linear-time complexity. regarding ﬁrst issue show regards second issue apply relational neural clusters data points using voronoi-tesselation respect prototypes. ensure roughly constant-sized clusters supply method number prototypes proportional data size. further ensure clustering takes linear time. such relational neural requires quadratic time training. however sped training prototype positions constant-sized subset data assigning remaining data points closest prototype. computing distance data points prototypes requires pairwise distances data points data points training subset approach illustrated ﬁgure relational neural places prototypes data thereby distributes data points disjoint clusters successor relations depicted arrows. cluster separate gaussian process trained. test data point provides separate predictive gaussian distribution given terms means variance predictive distributions merged overall predictive distribution mean equation variance equation note overall predictive distribution similar prediction circle-cluster test data point closer cluster thus predictive variance circle-cluster lower giving higher weight merge process. experimental evaluation apply pipeline introduced previous section four data sets theoretical models real-world data sets java programs. cases evaluate root mean square error prediction method leave-oneout-crossvalidation sequences data set. denote current test trajectory tj}j=...m predicted aﬃne coeﬃcients point matrix squared pairwise dissimilarities accordingly rmse fold following form evaluate four regression models namely -nearest neighbor kernel regression gaussian process regression robust bayesian committee machine well identity function baseline i.e. predict current point next point. optimized hyper parameters methods using random search random trials. trial evaluated rmse nested leave-oneout-crossvalidation training sequences chose parameters corresponded lowest rmse. average dissimilarity data set. drew uniform distribution range theoretical data sets ﬁxed java data sets avoid need eigenvalue correction random trial. drew exponential distribution range theoretical applied hyper-parameter selection runtime overhead clustering negligible need rely linear-time speedup described could compute clustering whole training data set. experimental hypotheses prediction methods yield lower rmse compared baseline rbcm outperform rbcm signiﬁcantly worse compared evaluate signiﬁcance wilcoxon signed rank test. barabási-albert model simple stochastic model graph growth undirected graphs hyper-parameters growth process starts fully connected initial graph nodes adds nodes one. newly added node connected existing nodes degt) shown edge distribution resulting growth model scaleconway’s game life john conway’s game life simple -dimensional cellular automaton model. nodes ordered regular dimensional grid connected eight neighbors grid. figure standard patterns used game life-data except block glider pattern. unique states patterns shown. note state glider equals state rotation. note conway’s game life turing-complete evolution general unpredictable without computing every single step according rules created trajectories initializing grid standard patterns random position namely blinker beacon toad block glider block glider ﬁrst four patterns simple oscillators period glider inﬁnitely moving structure period block glider chaotic structure converges block four glider steps. system time steps resulting graphs overall. every step activated cells random simulating observational noise. data representation theoretical data sets explicit feature embedding inspired shortest-path-kernel borgwardt colleagues using floyd-warshall algorithm compute shortest paths graph compute histogram lengths shortest paths feature dissimilarity euclidean distance feature vectors normalize average figure example graph associated matrix shortest paths returned floydwarshall algorithm histogram path lengths used feature representation approach. note self-distances ignored. table mean rmse runtime across cross validation trials theoretical data sets methods standard deviation shown brackets. runtime entries shorter runtime milliseconds. best value column highlighted bold print. distance data set. transformed distance kernel radial basis function transformation. eigenvalue correction required. rmse runtimes three data sets shown table expected rbcm outperform identity-baseline supporting outperforms baseline barabási-albert data also results lend support rbcm outperforms data sets however rbcm signiﬁcantly better barabási-albert data indicating simple data sets theoretical ones might already provide suﬃcient predictive quality. finally observe signiﬁcant diﬀerence rbcm expected interestingly data sets rbcm slower compared probably constant overhead maintaining multiple models. real-world java data sets consist programs diﬀerent problems beginner’s programming courses. motivation time series prediction data help students achieve correct solution intelligent tutoring system students incrementally work program might stuck know proceed. then would like predict likely next state program given trajectories students already correctly solved problem. scheme inspired prior work intelligent tutoring systems hint factory barnes colleagues data sets consist ﬁnal working versions programs simulate incremental growth. ﬁrst represented programs graphs abstract syntax tree recursively removed last semantically important node program empty. then reversed order resulting sequence achieve growing program. detail following data sets minipalindrome data consists java programs realizing eight diﬀerent strategies recognize palindromic input abstract syntax trees programs contain nodes average. programs come eight diﬀerent variations described simulation resulted data points. sorting benchmark data java sorting programs taken implementing sorting algorithms namely bubblesort insertionsort abstract syntax trees contain nodes average. simulation resulted data points. table mean rmse runtime across cross validation trials java data sets methods standard deviation shown brackets. runtime entries shorter runtime seconds. best value column highlighted bold print. then computed sequence alignment distance resulting node sequences similar method described robles-kelly hancock particular used aﬃne sequence alignment learned node dissimilarity suggested transformed dissimilarity similarity radial basis function transformation obtained kernel clip eigenvalue correction show rmses runtimes data sets table contrary theoretical data sets before observe strong diﬀerences predictive models baseline well rbcm well supports interestingly rbcm appears achieve better results compared might case additional smoothing provided averaging operation cluster-wise results. result supports finally observe rbcm times faster compared gpr. results indicate possible achieve time series prediction kernel dissimilarity spaces particular graphs. experiments even simple predictive models outperform baseline staying are. real-world data could improve predictive error signiﬁcantly applying complex predictive model namely robust bayesian committee machine indicates trade-oﬀ terms model choice simpler models faster case -nearest neighbor result easier interpret given graph. however case real-world data likely complex predictive model required accurately describe underlying dynamics kernel space. fortunately runtime overhead constant factor idea apply time series prediction euclidean space representing graph output point space. even though know graph corresponding predicted location latent space shown point analyzed using subsequent dissimilaritykernel-based methods dissimilarities kernel values respect predicted point still calculated. however challenges research remain. first usual hyperparameter optimization techniques gaussian processes depend vectorial data representation thus necessarily applicable proposed pipeline. therefore alternative hyperparameter selection techniques required. second theoretic empirical results regarding number data required make accurate predictions still lacking novel approach. finally applications predicted point primal space required need prediction form graph example feedback provision intelligent tutoring systems predicting precise structure sensor network e.g. predictive maintenance reﬁnery. cases solve pre-image problem finding original point maps aﬃne combination pseudo-euclidean space. problem proved particularly challenging literature could proﬁt consideration", "year": 2017}