{"title": "KL-based Control of the Learning Schedule for Surrogate Black-Box  Optimization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper investigates the control of an ML component within the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box optimization. The known CMA-ES weakness is its sample complexity, the number of evaluations of the objective function needed to approximate the global optimum. This weakness is commonly addressed through surrogate optimization, learning an estimate of the objective function a.k.a. surrogate model, and replacing most evaluations of the true objective function with the (inexpensive) evaluation of the surrogate model. This paper presents a principled control of the learning schedule (when to relearn the surrogate model), based on the Kullback-Leibler divergence of the current search distribution and the training distribution of the former surrogate model. The experimental validation of the proposed approach shows significant performance gains on a comprehensive set of ill-conditioned benchmark problems, compared to the best state of the art including the quasi-Newton high-precision BFGS method.", "text": "paper investigates control component within covariance matrix adaptation evolution strategy devoted black-box optimization. known cma-es weakness sample complexity number evaluations objective function needed approximate global optimum. weakness commonly addressed surrogate optimization learning estimate objective function a.k.a. surrogate model replacing evaluations true objective function evaluation surrogate model. paper presents principled control learning schedule based kullback-leibler divergence current search distribution training distribution former surrogate model. experimental validation proposed approach shows signiﬁcant performance gains comprehensive ill-conditioned benchmark problems compared best state including quasi-newton high-precision bfgs method. noted requirements machine learning algorithms might rather diﬀerent depending whether algorithms used isolation components computational systems. beyond usual criteria consistency convergence speed components enforce stability controllability properties. speciﬁcally component never cause catastrophic event system calls component. concrete example controllability-enforcing strategy contrarily usage stopping criterion support vector machine algorithm deﬁned terms number quadratic programming iterations rather terms accuracy since convergence global optimum might slow circumstances counterpart strategy might reduce predictive performance learned model cases thus hindering performance whole computational system. thus becomes advisable component takes charge control hyper-parameters even schedule calls automatic hyperparameter tuning algorithm general case however shows critical task requiring sufﬁcient empirical evidence. paper focuses embedding learning component within distribution-based optimization algorithm visibility algorithms particularly industrial applications explained robustness w.r.t. noise multi-modality objective function contrast classical optimization methods quasi-newton methods price robustness lack regularity assumption objective function leads large empirical sample complexity i.e. number evaluations objective function needed approximate global optima. another drawback usually large number hyper-parameters tuned algorithms reach good performances. shall however restrict remainder paper covariance matrix adaptation evolution strategy known almost parameterless distribution-based black-box optimization algorithm. property commonly attributed cma-es invariance properties w.r.t. monotonous transformations objective function linear transformations initial representation instance space high sample complexity commonly prevents distribution-based optimization algorithms used expensive optimization problems single objective evaluation might require several hours so-called surrogate optimization algorithms survey) address limitation coupling black-box optimization learning surrogate models local approximations objective function replacing evaluations true objective function evaluation surrogate function issue surrogate-based optimization control learning module paper integrated coupling distributionbased optimization rank-based learning algorithms called kl-acm-es presented. contribution compared state analyze learning schedule respect drift sample distribution formally surrogate model trained given sample distribution distribution iteratively modiﬁed along optimization. relearn surrogate model depends fast error rate surrogate model increases sample distribution moves away training distribution. mild assumptions shown error rate increase bounded respect kullback-leibler divergence training current distribution yielding principled learning schedule. merit approach empirically demonstrated shows signiﬁcant performance gains comprehensive ill-conditioned benchmark problems compared best state including quasi-newton high-precision bfgs method. paper organized follows. sake self-containedness section presents covariance matrix adaptation brieﬂy reviews related work. section gives overview proposed klacm-es algorithm discusses notion drifting error rate. experimental validation proposed approach reported discussed section section concludes paper. so-called -cma-es maintains gaussian distribution iteratively used generate samples updated based best samples ones. formally samples time drawn current gaussian distribution tively center gaussian distribution perturbation step size covariance matrix. next distribution center weighted best samples denoting i-th best sample ones next covariance matrix updated using local information search direction given global inforσt mation stored so-called evolution path distribution center positive learning rates strictly increasing scalar function hand self-adaptation covariance matrix makes cma-es invariant w.r.t. orthogonal transformations search space interestingly cma-es interpreted information-geometric optimization framework achieves natural gradient ascent space parametric distributions sample space using kullbackleibler divergence distance among distributions. shown basic -cma-es variant particular case parametric distribution space gaussian distributions since late many learning algorithms used within surrogate-based optimization ranging neural nets gaussian processes using particular expected improvement selection criterion. surrogate cma-es algorithm similar approach s∗acm-es interleaves optimization procedures ﬁrst regards optimization objective function assisted second regards optimization learning hyperorthogonal transformations search space enforced using radial basis function kernel involves inverse covariance matrix adapted cma-es. formally rank-based surrogate model learned using kernel deﬁned main strength s∗acm-es achieve simultaneous optimization objective function together learning hyper-parameters thus requiring user initially deﬁne range values adjusting online minimize drift error rate. worth noting automatic tuning hyper-parameters critical general particularly dealing small sample sizes. fact hyper-parameter tuning found eﬀective considered setting seems explained component s∗acm-es actually considers sequence learning problems deﬁned successive distributions receives feedback epoch choice made previous epoch. signiﬁcant weakness however s∗acmes learning schedule deﬁned terms number cma-es iterations epoch. however drift error rate rather depend fast optimization distribution modiﬁed. remark core proposed algorithm. proposed kl-acm-es algorithm presented section diﬀers s∗acm-es regarding control learning schedule decision relearning surrogate model. proposed criterion based kullback-leibler divergence distribution used generate training distribution current working distribution cma-es. worth noting since kullbackleibler divergence depends parameterization criterion intrinsic could used distribution-based black-box optimization algorithms. denote following surrogate model idea retrain surrogate model whenever tion error bounded depending error w.r.t. w.r.t. respectively noted errpθ′ bounded square erradmissible high probability. practice however noted well known bound often quite loose even case convex σ-admissible instance using hinge loss function proposed inversely proportional constant q-dependent terms vanish large however discussed section context application algorithms driven optimization goal. particular number training samples kept small possible. empirical alternative update klthresh investigated inspired trustregion paradigm classical optimization. context work ranking-svm uniform loss stability property hence q-dependent terms small enough possible given parameter erradmissible deﬁne threshold klthresh that provided divergence remains klthresh sought threshold klthresh ﬁnally interpreted terms trust regions classical heuristic optimization methods often proceed associating region search space quadratic surrogate model approximating objective function region. region referred trust region assessed ratio expected improvement measured surrogate model improvement true objective. depending ratio trust region expanded restricted. proposed kl-based control learning schedule viewed principled adaptively control dynamics trust region three diﬀerences. firstly trust region deﬁned terms distributions search space trust region deﬁned distributions trust region assessed posteriori empirical error surrogate model ﬁrst distribution outside trust region. thirdly assessment exploited adjust radius next trust region klthresh. idation ﬁrstly involves s∗acm-es default parameters. cma-es variant used within s∗acmes kl-acm-es state bipopactive cma-es algorithm comparative validation also involves quasi-newton bfgs method. indeed bfgs suﬀers known numerical problems ill-conditioned problems extensive discussion). limitation overcome considering instead -decimal digit precision arithmetic version bfgs referred pbfgs included high-precision arithmetic package arprec algorithms compared twentyfour -dimensional noiseless thirty noisy benchmark problems diﬀerent known characteristics separable non-separable unimodal multi-modal illconditioned deceptive functions without weak global structure. problem uniformly generated orthogonal transformations considered. problem algorithm independent runs launched. performance algorithm reported median optimum value number evaluations empirical cumulative distribution success solving sets similar functions algorithms initialized section presents experimental validation proposed kl-acm-es algorithm compared various cma-es algorithms including s∗acm-es quasi newton bfgs algorithm bbob noiseless noisy benchmark suite figure comparative performance kl-acm-es compared high-precision bfgs cma-es variants -dimensional rosenbrock function fros ros. medium number function evaluations reach target objective value computed corresponding shown. markers show objective value reached given number function evaluations. kl-acm-es outperforms s∗acm-es pbfgs rosenbrock function merits invariance w.r.t. monotonous transformations objective function shine pbfgs behavior signiﬁcantly degraded compared fros slowing convergence factor ros. quite contrary cma-es variants including s∗acmes kl-acm-es behave exactly three functions construction. fig. displays overall results bbob benchmark. similar functions grouped thus distinguishing cases separable moderately diﬃcult ill-conditioned multi-modal weakly structured multimodal objective functions finally shown quasinewton methods interpreted approximations bayesian linear regression varying prior assumptions; prospective research direction replace linear regression ordinal regression-based ranking gaussian processes order derive version bfgs invariant w.r.t. monotonous", "year": 2013}