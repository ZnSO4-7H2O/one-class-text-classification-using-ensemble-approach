{"title": "An Infinite Hidden Markov Model With Similarity-Biased Transitions", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME", "G.3; I.2.6"], "abstract": "We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between \"nearby\" states. This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pair-wise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a \"harmonic parsing\" task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.", "text": "weights drawn iteratively breaking stick remaining weight according beta distribution. parameter known concentration parameter governs quickly weights tend decay large corresponding slow decay hence weights needed given cumulative weight reached. stick-breaking process denoted grifﬁths engen mccloskey. thus discrete probability measure weights locations deﬁned i.i.d.∼ shortcoming prior transition matrix fact source destination states special element corresponds self-transition. hdphmm however self-transitions likely priori transitions state. sticky hdp-hmm addresses issue adding extra mass location base measure generates replaced alternative approach treats self-transitions special hidden semi-markov model wherein state duration distributions modeled separately ordinary selftransitions ruled out. however describe generalization hierarchical dirichlet process hidden markov model able encode prior information state transitions likely between nearby states. accomplished deﬁning similarity function state space scaling transition probabilities pairwise similarities thereby inducing correlations among transition distributions. present augmented data representation model markov jump process which jump attempts fail probability success proportional similarity source destination states. augmentation restores conditional conjugacy admits simple gibbs sampler. evaluate model inference method speaker diarization task harmonic parsing task using fourpart chorale data well several synthetic datasets achieving favorable comparisons existing models. hierarchical dirichlet process hidden markov model bayesian model time series data generalizes conventional hidden markov model allow countably inﬁnite state space. hierarchical structure ensures that despite inﬁnite state space common destination states reachable positive probability source state. hdp-hmm characterized following generative process. oberlin college oberlin university arizona tucson usa. correspondence colin reimer dawson <cdawsonoberlin.edu>. code http//colindawson.net/hdp-hmm-lt. paper structured follows section deﬁne model. section develop gibbs sampling algorithm based augmented data representation call markov jump process failed transitions section test versions model speaker diarization task speakers inter-dependent another fourpart chorale corpus demonstrating performance improvements state-of-the-art models local transitions common data. using sythetic data hdp-hmm show variant learn similarity bias data support finally section conclude discuss relationships hdp-hmm-lt existing variants. code additional details available http//colindawson.net/hdp-hmm-lt/ wish transition model concept transition nearby state transitions states likely priori extent nearby similarity space. order accomplish this ﬁrst consider alternative construction transition distributions based normalized gamma process representation measure assigning sets contain shown normalization constant positive ﬁnite almost surely βkδθk. draw stickbreaking process draw i.i.d. sequence base measure draw i.i.d. sequence random measures {gj} process deﬁnes hierarchical dirichlet process associated hidden states inﬁnite matrix entry mass associated models ability privilege self-transitions contain notion similarity pairs states identical cases transition matrix integrated prior probability transitioning state depends top-level stick weight associated state identity parameters previous state main contributions paper generalization hdp-hmm call hdp-hmm local transitions allows geometric structure deﬁned latent state space nearby states priori likely transitions them simple gibbs sampling algorithm model. property introduced elementwise rescaling renormalizing transition matrix. versions similarity structure illustrated case states similar extent emission distributions similar. another similarity structure inferred separately. cases give augmented data representations restore conditional conjugacy thus allow simple gibbs sampling algorithm used inference. rescaling renormalization approach similar used hdp-hmm-lt used paisley deﬁne discrete inﬁnite logistic normal model instance correlated random measure setting topic modeling. there however contexts mixture components distinct sets notion temporal dependence. developed based directly diln model. paisley employ variational approximations whereas present gibbs sampler converges asymptotically true posterior. discuss additional differences between model diln-hmm sec. class application useful incorporate notion locality occurs latent state sequence consists several parallel chains global state changes incrementally increments independent across chains. factorial hmms commonly used setting ignores dependence among chains hence poorly combinations states much probable suggested chain-wise dynamics. another setting property useful notion state geometry licenses syllogisms e.g. frequently leads frequently leads sensible infer lead well. property arguably total number visits state since products terms posterior longer however conditional conjugacy restored data-augmentation process natural interpretation described next. section deﬁne stochastic process call markov jump process failed transitions obtain hdp-hmm-lt marginalizing variables. reinstating auxiliary variables obtain simple gibbs sampling algorithm full mjp-ft used sample marginal posterior variables used hdp-hmm-lt. deﬁned last section. consider continuous-time markov process states suppose process makes jump state time next jump state occurs time πjj) independent ˜ut. note formulation unlike standard formulations markov jump processes assuming self-jumps possible. observe jump sequence holding times ordinary markov chain transition matrix row-proportional observe jumps directly instead observation generated jump distribution depends state jumped ordinary whose transition matrix obtained normalizing hdp-hmm. modify process follows. suppose jump attempt state state probability failing case transition occurs observation generated. assuming independent failures rates successful failed jumps πjjφjj πjj) respectively. probability ﬁrst successful jump state proportional rate successful jump attempts πjjφjj. conditioned holding time independent distributed exp. tzt=j period failed attempts jump state poisson)) independent. data augmentation bears conceptual similarity geometrically distributed auxiliary variables introduced hdp-hsmm transitions likely local. accomplish associating latent state location space introducing similarity function scaling element example might wish deﬁne divergence function exp{−dj transitions less likely farther apart states are. setting obtain standard hdp-hmm. diln-hmm employs similar rescaling transition probabilities exponentiated gaussian process following scaling function must positive semi-deﬁnite particular symmetric whereas hdp-hmm-lt need take values moreover diln-hmm allow scales tied state parameters hence encode independent notion similarity. letting replace almost surely last inequality carries original hdp. prior means unnormalized transition distributions proportional αβφj distribution latent state sequence given fact hdp-hmm-lt yields factorial limit ﬁxing uniform probability dynamics controlled entirely transition matrix chain setting exp−dj asymmetric yields nonparametric extensions factorial inﬁnite factorial hidden markov model inﬁnite factorial dynamic model developed recent years making indian buffet process state prior. would conceptually straightforward combine state prior similarity bias model provided chosen similarity function uniformly bounded space inﬁnite length binary vectors exponentiated negative hamming distance since number differences draws ﬁnite probability yields reasonable similarity metric. develop gibbs sampling algorithm based mjpft representation described sec. augmenting data duration variables failed jump attempt count matrix well additional auxiliary variables deﬁne below. representation transition matrix represented directly deterministic function unscaled transition rate matrix similarity matrix full variables partitioned blocks represents auxiliary variables introduced below represents emission parameters represents additional parameters free parameters similarity function hyperparameters emission distribution. restore conditional conjugacy. however differences ﬁrst measure many steps chain would remained state markovian dynamics whereas represents putative continuous holding times transition second allows restoration zeroed entry whereas allows work unnormalized entries avoiding need restore zeroed entries hsmm-lt incorporating {uj} {qjj} augmented data simpliﬁes likelihood yielding note local transition property hdphmm-lt combined sticky property sticky hdp-hmm nongeometric duration distributions hdp-hsmm additional prior weight self-transitions. former case changes inference needed; simply extra mass shape parameter gamma prior employ auxiliary variable method used distinguish sticky regular self-transitions. semi-markov case diagonal elements zero allow observations emitted i.i.d. according state-speciﬁc duration distribution sample latent state sequence using suitable semi-markov message passing algorithm inference matrix affected since diagonal elements assumed unlike original representation hdp-hsmm dataaugmentation needed durations already account normalization setting local transition property desirable case latent states encode multiple hidden features time vector categories. problems often modeled using factorial hmms pppppp since conditioning transition matrix sample entire sequence jointly forwardbackward algorithm ordinary hmm. since sampling labels jointly step requires computation iteration bottleneck inference algorithm reasonably large done this sample forward distributions. also possible employ variant beam sampling speed iteration cost slower mixing variant here. depending application locations depend emission parameters sampling conditional unchanged hdp-hmm. general-purpose method sampling sampling dependent case dependence form emission model speciﬁc instances illustrated experiments below. sampling enable joint sampling employ weak limit approximation approximating stick-breaking process using ﬁnite dirichlet distribution components larger expect need. product-of-gammas form integrate analytically obtain marginal likelihood integer ranging unsigned stirling number ﬁrst kind. normalizing constant distribution cancels ratio gamma functions likelihood letting posterior dirichlet whose mass parameter parameter space hidden states associated prior similarity function applicationspeciﬁc; consider cases. ﬁrst speakerdiarization task state consists ﬁnite ddimensional binary vector whose entries indicate speakers currently speaking. experiment state vectors determine pairwise similarities partially determine emission distributions lineargaussian model. second experiment data consists bach chorales latent states thought harmonic contexts. there components states govern similarities modeled independent emission distributions categorical distributions four-voice chords. sampling since identiﬁed inﬂuencing transition matrix emission distributions state sequence observation matrix used update. independent beta-bernoulli priors coordinate gibbs sample coordinate conditioned others coordinatewise prior means {µd} sample turn conditioned details appendix sampling conditioned sampled bayesian linear regression. column multivariate normal prior columns posteriori independent multivariate normals. experiments reported here compared directly ground truth value ground truth signal matrix constrain diagonal inverse gamma priors variances resulting conjugate updates. results attempted infer binary speaker matrices using models binary-state factorial individual binary speaker sequences modeled independent ordinary hdp-hmm without local transitions latent states binary vectors sticky hdphmm hdp-hmm-lt model model combines sticky properties. models concentration noise precision parameters given gamma priors. given unif prior. sticky models ratio evaluated models iteration using lenge. underlying signal consisted speaker channels recorded time steps resulting signal matrix denoted mapped microphone channels weight matrix speakers grouped conversational groups speakers within conversation took turns speaking task naively possible states however conversational grouping speaker conversation speaking given time state space constrained states possible number speakers conversation turn within conversation consisted single sentence turn orders within conversation randomly generated random pauses distributed inserted sentences. every time speaker turn sentence drawn randomly sentences uttered speaker data. conversations continued signal down-sampled length ’on’ portions speaker’s signal normalized amplitudes mean standard deviation additional column added speaker signal representing background noise. resulting sigtrix thus weight matrix denoted matrix following gael valera weights drawn independently unif distribution independent noise added entry observation matrix. model latent states d-dimensional binary vectors whose entry indicates whether speaker speaking. locations identiﬁed binary vectors laplacian similarity function hamming distance expj emission model lineargaussian data weight matrix signal matrix whose experiments discussed here assume independent assumption easily relaxed appropriate. ﬁnite-length binary vector states possible states ﬁnite seem nonparametric model unnecessary. however reasonably large likely possible states vanishingly unlikely would like encourage selection sparse states. moreover could state emission parameters different transition dynamics. next describe addifigure score inferred relative ground truth binary speaker matrices cocktail party data evaluated every gibbs iteration ﬁrst aggregating across runs model. middle inferred sticky-lt models gibbs iteration averaged runs. bottom number states used model training set. error bands conﬁdence interval mean iteration. hamming distance inferred ground truth state matrices score. also plot inferred decay rate number states used sticky-lt models. results models fig. fig. plot ground truth state matrix average state matrix averaged runs post-burn-in iterations. sticky-lt models outperform others regular sticky model exhibits small advantage vanilla hdp-hmm. converge nonnegligible value suggesting local transition structure explains data well. models also states non-lt models perhaps owing fact weaker transition prior non-lt model likely explain nearby similar observations single persisting state whereas model places higher probability transitioning state similar latent vector. generated data directly ordinary hdp-hmm used cocktail experiment sanity check examine performance model absence similarity bias. results fig. parameter large model worse performance non-lt model data; however parameter settles near zero model learns local transitions probable. hdp-hmm-lt ordinary hdp-hmm. model make entirely inferences non-lt model however; particular concentration parameter larger. figure binary speaker matrices cocktail data time horizontal axis speaker vertical axis. white black ground truth matrix followed inferred speaker matrix sticky hdp-hmmlt hdp-hmm-lt binary factorial sticky-hdp-hmm vanilla hdp-hmm. inferred matrices averaged runs gibbs iterations each ﬁrst iterations discarded burn-in. extent trade sparsity transition matrix achieved either beginning sparse rate matrix prior rescaling beginning less sparse rate matrix becomes sparser rescaling test version hdp-hmm-lt model components latent state governing similarity unrelated emission distributions used model unsupervised grammar learning corpus bach chorales. data corpus four-voice major chorales j.s. bach music randomly selected training used test evaluate surprisal trained models. chorales transposed c-major distinct four-voice chord encoded single integer. total distinct chord types chord tokens chorales types tokens training chorales chord types unique test set. modiﬁcations model inference since chords encoded integers emission distribution state cat. symmetric dirichlet prior resulting conjugate updates conditioned latent state sequence figure score inferred relative ground truth binary speaker matrices synthetic data generated vanilla hdp-hmm model. middle learned similarity parameter model gibbs iteration averaged runs. bottom number states used model training set. error bands conﬁdence interval mean iteration. ﬁrst iterations omitted. experiment locations independent priors. gaussian similarity function exp{−λdj euclidean distance. since latent states continuous hamiltonian monte carlo update update simultaneously conditioned results gibbs chains iterations using hdp-hmm-lt sticky-hdp-hmm-lt hdphmm sticky-hdp-hmm models training chorales modeled conditionally independent another. evaluated marginal likelihood test chorales every iteration. training test likelihoods fig. although model achieve close training data generalization performance better suggesting vanilla hdp-hmm overﬁtting. perhaps counterintuitive since model ﬂexible might expected prone overﬁtting. however similarity bias induces greater information sharing across parameters hierarchical model instead entry transition matrix being informed mainly transitions directly involving corresponding states informed extent transitions inform similarity structure. hdp-hmm allowing state space geometry represented similarity kernel making transitions nearby pairs states likely priori. introducing augmented data representation call markov jump process failed transitions obtain gibbs sampling algorithm simpliﬁes inference ordinary hdphmm. multiple latent chains interdependent speaker diarization hdp-hmm-lt model combines hdp-hmm’s capacity discover small joint states factorial hmm’s ability encode property transitions involve small number chains. hdp-hmm-lt outperforms both well outperforming sticky-hdp-hmm speaker diarization task speakers form conversational groups. despite addition similarity kernel hdp-hmm-lt able suppress local transition prior data support achieving identical performance hdphmm data generated directly latter. local transition property particularly clear transitions occur different times different latent features binary vector-valued states cocktail party setting model used state space equipped suitable similarity kernel. similarities need deﬁned terms emission parameters; state locations represented inferred separately demonstrate using bach chorale data. there model achieves better predictive performance held-out test ordinary hdp-hmm overﬁts training property acts encourage concise harmonic representation chord contexts arranged bidirectional functional relationships. focused ﬁxed-dimension binary vectors cocktail party synthetic data experiments would straightforward property model nonparametric latent states ifhmm inﬁnite factorial dynamic model indian buffet process state prior. valera isabel ruiz francisco svensson lennart perez-cruz fernando. inﬁnite factorial dynamical model. advances neural information processing systems gael jurgen saatci yunus whye ghahramani zoubin. beam sampling inﬁnite proceedings inhidden markov model. ternational conference machine learning jinsong leung henry. hidden markov models discrete inﬁnite logistic normal distribution priors. information fusion international conference ieee appendix concerns derivation augmented data representation referred markov jump process failed transitions appendix ﬁlls details gibbs sampling steps sample rescaled used hdp-hmm-lt. appendix gives derivation updates binary state vectors version hdp-hmm-lt used cocktail party experiment. finally appendix gives details hamiltonian monte carlo update version model used bach chorale experiment. gain stronger intuition well simplify posterior inference re-casting hdp-hmm-lt continuous time markov jump process attempts jump state another fail failure probability increases function distance states. suppose process state jumps state made rate πjj. deﬁnes continuous-time markov process off-diagonal elements transition rate matrix diagonal elements addition self-jumps allowed occur rate πjj. observe jumps durations jumps ordinary markov chain whose transition matrix obtained appropriately normalizing observe jumps themselves instead observation generated jump distribution depends state jumped ordinary hmm. modify process follows. suppose jump attempt state state chance failing increasing function distance states. particular success probability then rate successful jumps πjjφjj corresponding rate unsuccessful jump attempts πjj). this denote total number jump attempts unit interval time spent state since assuming process markovian total number attempts poisson) distributed. conditioned successful index jumps indicates state visited process given process state discretized time standard property markov processes probability ﬁrst successful jump state proportional rate successful attempts πjjφjj. indicate time elapsed t−th successful jump standard property sums i.i.d. exponential distributions gamma distribution shape equal number variates rate equal rate individual exponentials. moreover since ˜qjt poisson distributed total number failed attempts total duration integer ranging qjj. note recurrence relation could compute coefﬁcients explicitly; however typically simpler computationally efﬁcient sample distribution simulating number occupied tables chinese restaurant process customers enumerate probabilities. simply draw assignments customers tables according chinese restaurant process number distinct tables realized; assign ﬁrst customer table setting then customers assigned assign customer table probability αβj/) case increment existing table probability case increment mjj. principle distribution binary vectors suppose simplicity factored independent coordinate-wise bernoulli variates. bernoulli parameter coordinate. states parameters previous version model binary state vector similarities emission distribution depended. here deﬁne latent locations locations independent emission distributions inference informed solely transitions. since continuous locations hamlitonian monte carlo sample jointly. variation metropolis-hastings algorithm designed efﬁciently explore highdimensional continuous distribution adopting proposal distribution incorporates auxiliary momentum variable make likely proposals useful directions improve mixing compared naive movement.", "year": 2017}