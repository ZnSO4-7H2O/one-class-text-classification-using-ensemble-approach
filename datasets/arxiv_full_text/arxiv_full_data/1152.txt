{"title": "An Empirical Evaluation of Current Convolutional Architectures' Ability  to Manage Nuisance Location and Scale Variability", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We conduct an empirical study to test the ability of Convolutional Neural Networks (CNNs) to reduce the effects of nuisance transformations of the input data, such as location, scale and aspect ratio. We isolate factors by adopting a common convolutional architecture either deployed globally on the image to compute class posterior distributions, or restricted locally to compute class conditional distributions given location, scale and aspect ratios of bounding boxes determined by proposal heuristics. In theory, averaging the latter should yield inferior performance compared to proper marginalization. Yet empirical evidence suggests the converse, leading us to conclude that - at the current level of complexity of convolutional architectures and scale of the data sets used to train them - CNNs are not very effective at marginalizing nuisance variability. We also quantify the effects of context on the overall classification task and its impact on the performance of CNNs, and propose improved sampling techniques for heuristic proposal schemes that improve end-to-end performance to state-of-the-art levels. We test our hypothesis on a classification task using the ImageNet Challenge benchmark and on a wide-baseline matching task using the Oxford and Fischer's datasets.", "text": "conduct empirical study test ability convolutional neural networks reduce effects nuisance transformations input data location scale aspect ratio. isolate factors adopting common convolutional architecture either deployed globally image compute class posterior distributions restricted locally compute class conditional distributions given location scale aspect ratios bounding boxes determined proposal heuristics. theory averaging latter yield inferior performance compared proper marginalization. empirical evidence suggests converse leading conclude current level complexity convolutional architectures scale data sets used train cnns effective marginalizing nuisance variability. also quantify effects context overall classiﬁcation task impact performance cnns propose improved sampling techniques heuristic proposal schemes improve end-to-end performance state-of-the-art levels. test hypothesis classiﬁcation task using imagenet challenge benchmark wide-baseline matching task using oxford fischer’s datasets. convolutional neural networks de-facto paragon detecting presence objects scene portrayed image. cnns described approximately invariant nuisance transformations planar translation virtue architecture virtue approximation properties that given sufﬁcient parameters transformed training data could principle yield discriminants insensitive nuisance transformations data represented training set. addition planar translation object detector must manage variability scaling occlusion. nuisances elements transformation group e.g. location-scale group case position scale aspect ratio object’s support. fact convolutional architectures appear effective classifying images containing given object regardless position scale aspect ratio suggests network effectively manage nuisance variability. however quest performance benchmark datasets researchers away letting manage nuisance variability. instead image ﬁrst pre-processed yield proposals subsets image domain tested presence given class proposal mechanisms remove nuisance variability position scale aspect ratio leaving category classify resulting bounding number classes trained with. differently rather computing posterior distribution nuisance transformations automatically marginalized used compute conditional distribution classes given data sample element approximates nuisance transbounding box. think conditional distribution class given image deﬁned class posterior marginalized respect nuisance group nuisances known class-conditionals nuisance order approximate weighted tested proposal determined refcomputes erence frame approximation then explicit marginalization different uniform weights) computes approach therefore average lower bound proper marginalization fact would outperform direct computation worth investigating empirically. goal formation represented bounding box. nuisance found maximum-likelihood selecting bounding yields highest probability class goal class regardless transformation nuisance approximately marginalized averaging conditional distributions respect estimation nuisance transformations. effective computing marginals respect nuisance variability would beneﬁt conditioning averaging respect nuisance samples. direct corollary data processing inequality proposals subsets whole image theory less informative even accounting resolution/sampling artifacts fortiori performance decrease conditioning mechanism representative nuisance distribution case proposal schemes produce bounding boxes based adaptively downsampling coarse discretization location-scale group class posteriors conditioned bounding boxes discard image outside limiting ability network leverage side information context. converse true i.e. averaging conditional distributions restricted proposal regions outperform operating entire image would bring question ability marginalize nuisances translation scaling else dpi. paper test hypothesis aiming answer question effective current cnns reduce effects nuisance transformations input data location scaling? best knowledge never done literature despite keen interest understanding properties cnns following empirical success. cognizant dangers drawing sure conclusions empirical evaluations especially involve myriad parameters exploit training sets exhibit biases. sect. describe testing protocol uses recognized existing modules keep factors constant testing hypothesis. ﬁrst show baseline singlemodel top- error imagenet classiﬁcation slightly decreases performance constrained ground-truth bounding boxes seem surprising ﬁrst would appear violate theorem however note restriction bounding boxes condition location-scale group also visibility image outside bounding ignored. thus slight decrease performance measures loss discarding context ignoring image beyond bounding box. true bounding boxes -pixel show that conditioned ground-truth-with-context indeed decrease error expected fig. show classiﬁcation performance function size whole image alexnet yields lowest top- errors imagenet validation models. also indicates context effectively leveraged current architectures limited relatively small neighborhood object interest. second contribution concerns proper sampling nuisance group. interpret restricted bounding function maps samples location-scale group class-conditional distributions proposal mechanism down-samples group classical sampling theory teaches retain value function samples local average process known anti-aliasing. also table show simple uniform averaging samples isotropic scale group reduces error respectively. unintuitive expects averaging conditional densities would produce less discriminative classiﬁers line recent developments concerning domain-size pooling test effect anti-aliasing absent knowledge ground truth object location follow methodology evaluation protocol develop domain-size pooled test benchmark classiﬁcation wide-baseline correspondence regions selected generic low-level detector third contribution show procedure improves baseline mean standard benchmark datasets fourth contribution goes towards answering question forth preamble consider popular baselines perform stateof-the-art imagenet classiﬁcation challenge introduce novel sampling pruning methods well adaptively weighted marginalization based inverse r´enyi entropy. averaging conditional class posteriors obtained various sampling schemes improve overall performance would imply implicit marginalization performed inferior obtained sampling group averaging resulting class conditionals. indeed observation e.g. achieve overall performance compared using whole image table alexnet’s vgg’s top- error imagenet classiﬁcation challenge ground-truth localization provided compared applying model entire image. ground truth various sizes isotropically anisotropically. show averaging class posteriors performs applying network concentric domain sizes around ground truth. ﬁfth contribution actually provide method performs state imagenet classiﬁcation challenge using single model. table provide various results time complexity. achieve top- classiﬁcation error alexnet compared error tested regularly sampled crops corresponds relative error reduction respectively. data augmentation techniques scale jittering ensemble several models could deployed along method. literature cnns role computer vision rapidly evolving. attempts understand inner workings cnns conducted along theoretical analysis aimed characterizing representational properties. intense interest sparked surprising performance cnns computer vision benchmarks many couple proposal scheme cnn. work relates vast body work refer reader references papers describe benchmarks adopt namely bilen also explore idea introducing proposals classiﬁcation. however approach leverages signiﬁcantly larger number candidates focuses sophisticated classiﬁers post-normalization class posteriors. investigation targets selecting small subset discriminative candidates among generic object proposals building popular models. trivialize location scaling? first test hypothesis eliminating nuisances location scaling providing bounding object interest improve classiﬁcation accuracy. given restricting network operate bounding prevents leveraging context outside alexnet pretrained models provided matconvnet open source library test top- top- classiﬁcation errors imagenet classiﬁcation challenge validation consists images salient class annotated priori human. however imagenet classes appear many images confound classiﬁer. test classiﬁer various settings ﬁrst feeding entire image letting classiﬁer manage nuisances. test ground-truth annotated bounding concentric regions include isotropic anisotropic expansion ground-truth region. observe similar behavior also consistent models. alexnet table using object’s groundtruth support performs slightly worse using whole image. object region -pixel top- classiﬁcation error decreases fast. however trade-off context clutter. providing much context diminishing returns. fig. show errors vary function size around object interest. performance starts dropping size. padding gives top- error alexnet opposed respectively classifying whole image. latter reduces top- error alexnet lower single domain size suggests explicitly marginalizing samples beneﬁcial. next test whether improvement stands using object proposals. introducing object proposals. deploy proposal algorithm generate object regions within image. edge boxes provide good trade-off between recall speed first decide number proposals provide satisfactory cover majority objects present dataset. single image search highest intersection union overlap groundtruth region proposed sample turn evaluate network’s performance overlapping sample. repeat process various number proposals small subset validation ﬁnally choose provides satisfactory trade-off between classiﬁcation performance computational cost. domain-size pooling regular crops. investigate inﬂuence domain-size pooling test time stand-alone technique additional proposals ﬁnal method described algorithm deploy domain-size aggregation network’s class posterior sizes uniformly sampled range normalized size original image. parameter search choose original horizontally ﬂipped area gives samples total. finally standard data augmentation techniques literature. customary image isotropically rescaled predeﬁned size predetermined selection crops extracted pruning samples. continuing sample patches within image diminishing return terms discriminability including background patches noisy class posterior distribution. adopt informationtheoretic criterion ﬁlter samples subsequent approximate marginalization. proposal evaluate network take normalized softmax output ilsvrc classiﬁcation. output non-negative numbers interpret vector probability distribution discrete space classes compute r´enyi entropy figure top- top- classiﬁcation errors imagenet function size alexnet architecture. size corresponds ground-truth bounding refers whole image. relatively small around ground truth provides best trade-off between informative context clutter. finally apply domain size average pooling class posterior domain sizes concentric ground truth. added declared size either dimensions along minimum dimension uniformly sampled range respectively. figure visualizing different sampling strategies. upper left object proposals. generic proposals using edge boxes upper right concentric domain sizes centered center image. below regular crops conjecture discriminative class distributions tend peaky less ambiguity among classes therefore lower entropy. fig. show selecting subset image patches whose class posterior lower entropy improves classiﬁcation performance. extract candidate object proposals evaluate network original candidates horizontal ﬂips. keep small subset whose posterior distribution lowest entropy. r´enyi entropy relatively small powers found encourages selecting regions highly-conﬁdent candidate object. parameter increases entropy increasingly determined events highest probability. larger would effective images single object case images ilsvrc. finally introduce weighted average selected support sample weight posterior. uniform weights weights proportional inverse entropy posterior latter expected perform better naturally gives higher weight discriminative samples. comparisons. compare various sampling inference strategies alexnet models. classiﬁcation results table refer validation ilsvrc except last demonstrates results test set. rows show performance popular multi-crop methods compare strategies involve concentric domain sizes object proposals introduce prior encouraging largest proposals among ones standard setting would give. instead directly extracting example proposals generate keep largest ones figure show top- error function number proposals average produce ﬁnal posterior. samples generated algorithm classiﬁed alexnet. blue curve corresponds selecting samples lowest-entropy posteriors. compare method simple strategies random selection ranking largest-size highest conﬁdence proposals. random sample selection times visualize estimated conﬁdence intervals error-bars. empirically discriminative power classiﬁer increases samples selected least entropy criterion. extracting crops order preserve aspect ratio single image rescale minimum dimension proposals extracted original image resolution rescaled anisotropically model’s receptive ﬁeld. additionally multi-crop algorithms resize image different scales sample patches ﬁxed size densely image. szegedy scales crops scale yields object proposals. extract several object proposals image keep largest ones). among choose proposals whose class posterior lowest r´enyi entropy parameter hyper-parameter search choose concentric domain sizes around center sizes uniformly extracted normalized range corresponds whole image crops. regular crops; e.g. scales conditionals either uniform equals inverse entropy posterior table top- top- errors imagenet classiﬁcation challenge. rows include common data augmentation strategies literature next three rows concentric domain sizes uniformly sampled range normalized size original image finally last seven rows introduce adaptive sampling consists data-driven object proposal algorithm entropy criterion select discriminative samples based extracted class posterior distribution. last shows results test set. eval stands number samples evaluated method number samples eventually element-wise averaged produce single vector class conﬁdences. previous top-reported regular sampling results shown bold. results presented table indicate expected scale jittering test time improves classiﬁcation performance -crop -crop strategies. additionally -crop strategy better -crop strategy models. results bold lowest errors achieved speciﬁc single models using regular crops. present methods observe using alexnet network concentric domain sizes outperforms multi-crop algorithms even evaluates averages patches. furthermore combining common crops achieves best results networks even without using -scale jittering. interpretation improvements concentric samples serve natural prior majority ilsvrc images speciﬁcally model trained without scale jittering training appears ﬁrst area table pre-trained models alexnet publicly available matconvnet toolbox simonyan evaluation crops scales report top- error imagenet validation. contrast implementation produces attributed using different pre-trained model initial weights sampled zero-mean gaussian distribution standard deviation might also minor differences training process. following introduce adaptive sampling mechanism algorithm reduce top- error alexnet respectively. perspective krizhevsky report top- error combine models. improve performance single model. relative improvement deployed instances alexnet compared data-augmentation methods used respectively. shows results marginalization weighted based entropy methods rows uniform weights last show results ilsvrc test server top-performing method regular concentric crops assume objects occupy image appear near center. known bias imagenet dataset. analyze effect adaptive sampling calculate intersection union error objects regular concentric crops show fig. performance various methods function error. improvement using adaptive sampling regular concenhead-to-head comparisons. detected scale mser dsp-cnn samples domain sizes within neighborhood around computes responses samples averages posteriors. deployed deep network unsupervised convolutional network proposed trained surrogate labels unlabeled dataset objective invariant several transformations commonly observed images captured different viewpoints. opposed networkclassiﬁers task correspondence network purely region descriptor whose last layers representations. fig. show comparison dsp-cnn oxford dataset cnn’s layer representation mser dsp-cnn simply averages layer’s responses domain sizes. sizes uniformly sampled neighborhood. improvement based matching mean average precision. fischer’s dataset includes pairs images extreme transformations oxford dataset. types transformations include zooming blurring lighting change rotation perspective nonlinear transformations. fig. table show comparisons dsp-cnn layer- layer- representations demonstrate relative improvement. domain sizes. parameters selected cross-validation. table show comparisons baselines using data dsp-sift parameter search concatenating layers achieve state performance shown fig. observing though high dimensionality method compared local descriptors. time complexity. table show number evaluated samples subset actually averaged extract single class posterior vector. sequential time needed method linear number evaluated patches eval. experiments matconvnet library parallelize load testing done batches patches. report time proﬁle method table entries cover boxes methods evaluated together. extracting proposals major bottleneck using efﬁcient algorithm edge boxes rows report results faster version edge boxes leverage edge sharpening decision tree. overall compared -crop strategy object proposal scheme introduces marginal computational overhead. test effect domain-size pooling correspondence tasks convolutional architecture done sift using datasets protocols illustrated fig. domain sizes centered around detector. expect averaging increase discriminability detected regions turn matching ability similar beneﬁts last rows table figure head head comparison dsp-cnn oxford fischer’s datasets. layer- features unsupervised network used descriptors. dsp-cnn outperforms counterpart terms matching respectively. right dsp-cnn performs comparably state-of-the-art dsp-sift descriptor given inherent high-dimensionality layers perform dimensionality reduction principal component analysis investigate affects matching performance. table show performance compressed layer- layer- representations dimensions concatenation. modest performance loss compressed features outperform single-scale features large margin. empirical analysis indicates cnns designed invariant nuisance variability small planar translations virtue convolutional architecture local spatial pooling learned manage global translation distance shape variability means large annotated datasets practice less effective naive theory counterproductive practice sampling averaging conditionals based ad-hoc choice bounding boxes corresponding planar translation scale aspect ratio. taken caveats first shown statement empirically choices network architectures trained particular datasets unlikely representative complexity visual scenes speciﬁc choice parameters made respective authors classiﬁer evaluation protocol. test hypothesis fairest possible setting kept choices constant comparing trained theory marginalize nuisances thus described applied bounding boxes provided proposal mechanism. address addition answering question posed introduction along shown framing marginalization nuisance variables averaging sub-sampling marginal distributions leverage concepts classical sampling theory anti-alias overall classiﬁer leads performance improvement categorization measured imagenet benchmark correspondence measured oxford fischer’s matching benchmarks. course like universal approximator principle capture geometry discriminant surface learning away nuisance variability given sufﬁcient resources terms layers number ﬁlters number training samples. abstract sense indeed marginalize nuisance variability. analysis conducted show that level complexity imposed current architectures training less effectively ad-hoc averaging proposal distributions. leaves researchers choice investing effort design proposal mechanisms subtracting duties category downstream invest effort scaling size efﬁciency learning algorithms general cnns render need proposal scheme moot.", "year": 2015}