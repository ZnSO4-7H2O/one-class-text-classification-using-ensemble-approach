{"title": "Relational Neural Expectation Maximization: Unsupervised Discovery of  Objects and their Interactions", "tag": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely \\emph{unsupervised} fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects.", "text": "common-sense physical reasoning essential ingredient intelligent agent operating real-world. example used simulate environment infer state parts world currently unobserved. order match real-world conditions causal knowledge must learned without access supervised data. address problem present novel method learns discover objects model physical interactions visual images purely unsupervised fashion. incorporates prior knowledge compositional nature human perception factor interactions object-pairs learn efﬁciently. videos bouncing balls show superior modelling capabilities method compared unsupervised neural approaches incorporate prior knowledge. demonstrate ability handle occlusion show extrapolate learned knowledge scenes different numbers objects. humans rely common-sense physical reasoning solve many everyday physics-related tasks example enables foresee consequences actions infer state parts world currently unobserved. causal understanding essential ingredient intelligent agent operate within world. common-sense physical reasoning facilitated discovery representation objects serve primitives compositional system. allow humans decompose complex visual scene distinct parts describe relations reason dynamics well consequences interactions successful machine learning approaches common-sense physical reasoning incorporate prior knowledge design. maintain explicit object representations allow general physical dynamics learned object pairs compositional manner however approaches learning supervised relies object-representations external sources typically unavailable real-world scenarios. methods suffer lack compositionality representational level objects. prevents end-to-end neural approaches efﬁciently learning functions operate multiple entities generalize human-like lake santoro perez work propose relational n-em novel approach common-sense physical reasoning learns physical interactions objects visual images purely unsupervised fashion. core neural expectation maximization method allows discovery compositional object-representations unable model interactions objects. therefore endow n-em relational mechanism inspired previous work enabling factor interactions object-pairs learn efﬁciently generalize visual scenes varying number objects without re-training. goal learn common-sense physical reasoning purely unsupervised fashion directly visual observations. argued order solve problem need exploit compositional structure visual scene. conventional unsupervised representation learning approaches gans goodfellow learn single distributed representation superimposes information input without imposing structure regarding objects low-level primitives. monolithic representations factorize physical interactions pairs objects therefore lack essential inductive bias learn efﬁciently. hence require alternative approach discover objects representations primitives visual scene unsupervised fashion. approach neural expectation maximization learns separate distributed representation object described terms features iterative process perceptual grouping representation learning. compositional nature representations enable formulate relational n-em novel unsupervised approach common-sense physical reasoning combines n-em interaction function models relations objects efﬁciently neural expectation maximization differentiable clustering method learns representation visual scene composed primitive object representations. representations adhere many useful properties symbolic representation objects therefore used primitives compositional system described format contain information object visual scene correspond together form representation visual scene composed objects learned unsupervised therefore serves starting point approach. goal n-em group pixels input belong object capture information efﬁciently distributed representation object. high-level idea access family distributions formalize objective inference mixture distributions. using expectation maximization compute maximum likelihood estimate parameters mixture obtain grouping pixels object corresponding representation. reality access n-em learns instead parameterizing mixture neural network back-propagating iterations unrolled generalized procedure. following greff model image spatial mixture components parameterized vectors neural network used transform representations parameters separate pixel-wise distributions. binary latent variables encodes unknown true pixel assignments pixel generated component full likelihood given figure illustration different computational aspects r-nem applied sequence images bouncing balls. note representations level correspond previous time-step. different colors correspond different cluster components .the right side shows computational overview υr-nem function computes pair-wise interactions object representations. learned statistical model images given object representations compute object representations given image maximizing marginalization complicates process thus generalized maximize following lowerbound instead iteration generalized consists steps e-step computes estimate posterior probability distribution latent variables given θold previous iteration. yields soft-assignment pixels components based accurately model generalized m-step updates θold taking gradient ascent step using previously computed soft-assignments θnew unrolled computational graph generalized steps differentiable provides means train implement statistical model images given object representations. using back-propagation time williams train minimize following loss intra-cluster term identical credits component accurately representing pixels assigned inter-cluster term ensures representation captures information pixels assigned powerful variant n-em obtained substituting generalized m-step recurrent neural network hidden state case entirety consists recurrent encoder-decoder architecture receives input step. learning objective prone trivial solutions case overcapacity could prevent network modelling statistical regularities data correspond objects. adding noise input image reducing dimensionality guide learning avert this. moreover case rnn-em evaluate following time-step encourage learning object representations corresponding dynamics. intuitive interpretation using denoising next-step prediction part training objective guide network learn essential properties objects case correspond gestalt principles prägnanz common fate rnn-em able capture dynamics individual objects parametrized recurrent connection operates object representation across consecutive time-steps. however relations interactions take place objects captured way. order overcome shortcoming propose relational n-em adds relational structure recurrence model interactions objects without violating properties learned object representations. consider generalized form standard rnn-em dynamics equation computes object representation time function object representations previous time-step interaction function weight matrices sigmoid activation function input recurrent model time υrnn-em dynamics model coincides standard update rule thereby recovering original rnn-em formulation. inductive bias incorporated reﬂects modeling assumptions interactions objects environment therefore nature θk’s interdependence. incorporates assumption interaction takes place objects θk’s fully independent recover υrnn-em. hand assume interactions among objects take place assume little structure interdependence θk’s forfeit useful properties compositionality. example longer extrapolate learned knowledge environments fewer objects lose overall data efﬁciency instead make efﬁcient compositionality among learned object representations incorporate general guiding constraints inﬂuence another constrain capture interdependence θk’s compositional manner enables physical dynamics learned efﬁciently allow learned dynamics extrapolated variable number objects. propose parametrized interaction function υr-nem incorporates modeling assumptions updates based pairwise effects objects concatenation operator corresponds multi-layer perceptron. first transformed using obtain enables information relevant object dynamics made explicit representation. next pair concatenated processed computes shared embedding encodes interaction object object notice clear separation focus object context object previous work compute effect object object attention coefﬁcient encodes whether interaction object object takes place. attention coefﬁcients help select relevant context objects seen ﬂexible unsupervised replacement distance based heuristic used previous work finally compute total effect θi=k weighted effects multiplied attention coefﬁcient. visual overview υr-nem seen right side figure machine learning approaches common-sense physical reasoning roughly divided groups symbolic approaches approaches perform state-to-state prediction. former group performs inference parameters symbolic physics engine restricts synthetic environments. latter group employs machine learning methods make state-to-state predictions often describing state system compact object-descriptions either used input system training purposes incorporating information objects methods achieved excellent generalization simulation capabilities. purely unsupervised approaches state-to-state prediction visual inputs state-descriptions rival capabilities. method purely unsupervised state-to-state prediction method operates pixel space taking ﬁrst step towards unsupervised learning common-sense reasoning real-world environments. proposed interaction function υr-nem seen type message passing neural network incorporates variant neighborhood attention light recent work seen permutation equivariant function. r-nem relies n-em discover compositional object representation visual inputs. closely related approach n-em framework utilizes similar mechanism perform inference group representations addition performs inference group assignments. recent work combined recurrent ladder network obtain powerful model applied sequential data. however lack single compact representation captures information group makes compositional treatment physical interactions difﬁcult. unsupervised approaches rely attention group together parts visual scene corresponding objects approaches suffer similar problem sequential nature prevents coherent object representation take shape. related work also taken steps towards combining learnability neural networks compositionality symbolic programs modeling physics playing games learning algorithms visual understanding natural language processing section evaluate r-nem three different physical reasoning tasks vary dynamical visual complexity bouncing balls variable mass bouncing balls invisible curtain arcade learning environment compare r-nem unsupervised neural methods incorporate inductive biases reﬂecting real-world dynamics show indeed beneﬁcial. experiments adam default parameters train validation test sequences early stopping patience epochs. encembeff used unique single layer neural network rectiﬁed linear units. used two-layer neural network tanh units followed single sigmoid unit. detailed overview experimental setup found appendix figure r-nem applied sequence bouncing balls. column corresponds time-step coincides step. time-step r-nem computes representations according input added noise group reconstruction produced predicts state environment next time-step. attention coefﬁcients visualized overlaying colored reconstruction context object white reconstruction focus object based prediction accuracy e-step computes soft-assignments visualized coloring pixel according figure performance method bouncing balls task. method trained dataset balls evaluated test balls test-set balls losses reported relative loss baseline dataset always predicts current frame. score used evaluate degree compositionality achieved. figure left three sequences time-steps ground-truth r-nem last time-steps sequences produced r-nem simulated. right loss entire test-set time-steps. bouncing balls study physical reasoning capabilities r-nem bouncing balls task standard environment evaluate physical reasoning capabilities exhibits visual complexity complex non-linear physical dynamics. train r-nem sequences binary images time-steps contain four bouncing balls different masses corresponding radii. balls initialized random initial positions masses velocities. balls bounce elastically image window. qualitative evaluation figure presents qualitative evaluation r-nem bouncing balls task. time-steps observed pixels belong balls grouped together assigned unique component background divided among components indicates representation component produces group reconstruction indeed contain information unique object together θk’s yield compositional object representation scene. total reconstruction displays accurate reconstruction input sequence next time-step indicating r-nem learned model dynamics bouncing balls. comparison compare modelling capabilities r-nem lstm rnn-em terms binomial cross-entropy loss predicted image ground-truth image last frame well relational takes account objects currently take part collision. unless speciﬁed test-set sequences containing four balls observe r-nem produces markedly lower losses compared methods moreover order validate component captures single ball report adjusted rand index score soft-assignments ground-truth assignment pixels objects. left column plot r-nem achieves score meaning roughly cases ball modeled single component. suggests compositional object representation achieved sequences. together observations line qualitative evaluation validate incorporating real world priors greatly beneﬁcial υr-nem enables interactions modelled accurately compared rnn-em terms relational bce. similar greff increasing number components training increases quality grouping r-nem figure addition observe loss reduced further matches hypothesis compositional object representations greatly beneﬁcial modelling physical interactions. extrapolating learned knowledge test-set sequences containing balls evaluate ability method extrapolate learned knowledge physical interactions four balls environments balls. evaluating r-nem rnn-em test-set order accommodate increased number objects. seen middle plot figure r-nem greatly outperforms methods. notice that since report loss relative baseline roughly factor increased complexity task. perfect extrapolation learned knowledge would therefore amount change relative performance. contrast observe worse performance lstm evaluated dataset extra balls. suggests gating mechanism lstm allowed learn sophisticated overly specialized solution sequences four balls generalize dataset balls. r-nem rnn-em scale markedly better dataset lstm. although similarly suffers lesser extend type overﬁtting likely inability learn reasonable solution sequences four balls begin with. hence conclude superior extrapolation capabilities rnn-em r-nem inherent ability factor scene terms permutation invariant object representations attention insight role attention mechanism gained visualizing attention coefﬁcients done figure component draw reconstruction colored according color component correspond colored balls indicate whether component took information component account computing state observed attention coefﬁcient becomes non-zero whenever collision takes place colored ball lights following time-steps. attention mechanism learned r-nem thus assumes role distance-based heuristic previous work matching intuitions mechanism would best utilized. quantitative evaluation attention mechanism obtained comparing r-nem variant incorporate attention figure shows methods perform equally well regular test r-nem performs worse extrapolating learned knowledge likely reason behavior range changes thus extrapolating environment balls total exceed previous boundaries impede learned dynamics. simulation scene accurately modelled r-nem approximately simulate dynamics recursive application figure compare simulation capabilities r-nem rnn-em bouncing balls environment. left shows r-nem sequence normal steps followed simulation steps well ground-truth sequence. last frame sequence clearly observed r-nem managed accurately simulate environment. ball approximately correct place shape ball preserved. balls simulated hand deviate substantially ground-truth position size increased. general r-nem produces mostly accurate simulations whereas consistently fails. interestingly found cases r-nem frequently fails single component models ball. right side figure summarizes loss time-steps across entire test-set. although crude measure simulation performance still observe r-nem consistently outperforms rnn-em rnn. hidden factors occlusion abundant real world ability handle hidden factors crucial physical reasoning system. therefore evaluate capability r-nem handle occlusion using variant bouncing balls contain invisible curtain. figure shows r-nem accurately models sequence maintain object states even confronted occlusion. example note step blue ball completely occluded collide orange ball. step ball accurately predicted re-appear bottom curtain opposed left side curtain. demonstrates r-nem notion object permanence implies understands scene level beyond pixels assigns persistence identity objects. figure r-nem applied sequence bouncing balls invisible curtain. ground truth sequence displayed followed prediction r-nem soft-assignments pixels components r-nem models objects well interactions even object completely occluded subset steps shown. figure r-nem accurately models sequence frames obtained agent playing space invaders. group longer corresponds object instead assumes role high-level entities engage similar movement patterns. space invaders test performance r-nem visually challenging environment train sequences binarized images time-steps game-play space invaders arcade learning environment also feed action agent interaction function. figure conﬁrms r-nem able accurately model environment even though visual complexity increased. notice visual scenes comprise large numbers primitive objects behave similarly. since trained r-nem four components unable group pixels according individual objects forced consider different grouping. r-nem assigns different groups every column aliens together spaceship three large shields. groupings seem based movement degree coincides semantic roles environment. examples also found r-nem frequently assigns different groups every column aliens three large shields. individual bullets space ship less frequently grouped separately action-noise environment small size bullets current resolution makes less predictable. argued ability discover describe scene terms objects provides essential ingredient common-sense physical reasoning. supported converging evidence cognitive science developmental psychology intuitive physics reasoning capabilities built upon ability perceive objects interactions fact young infants already exhibit ability even suggest innate bias towards compositionality inspired observations proposed r-nem method incorporates inductive biases existence objects interactions implemented clustering objective interaction function respectively. speciﬁc nature objects dynamics interactions learned efﬁciently purely visual observations. experiments r-nem indeed captures dynamics various environments accurately methods exhibits improved generalization environments different numbers objects. used approximate simulator environment predict movement collisions objects even completely occluded. demonstrates notion object permanence aligns evidence young infants seem infer occluded objects move connected paths continue maintain objectspeciﬁc properties moreover young infants also appear expect objects interact come contact analogous behaviour r-nem attend objects collision imminent. summary believe method presents important step towards learning human-like model world completely unsupervised fashion. current limitations approach revolve around grouping prediction. aspects scene humans group together typically varies function task mind. perceive stack chairs whole goal move another room individual chairs goal count number chairs stack. order facilitate dynamic grouping would need incorporate top-down feedback agent grouping procedure deviate built-in inductive biases. another limitation approach need incentivize r-nem produce useful groupings injecting noise reducing capacity. former prevent small regularities input detected. finally interaction e-step among groups makes difﬁcult increase number components without causing harmful training instabilities. multitude interactions objectives r-nem sometimes challenging train. terms prediction implicitly assumed objects environment behave according rules inferred. poses challenge objects deform manner difﬁcult predict however practice masking input helps component quickly adapting representation unforeseen behaviour across consecutive time steps. perhaps severe limitation r-nem second loss term outer training objective hinders modelling complex varying backgrounds background group would predict pixel prior every group. argue ability engage common-sense physical reasoning beneﬁts intelligent agent needs operate physical environment provides exciting future research opportunities. future work intend investigate top-down feedback agent could incorporated r-nem facilitate dynamic groupings also compositional representations produced r-nem beneﬁt reinforcement learner example learn modular policy easily generalizes novel combinations known objects. interactions controller model world posed schmidhuber constitute research directions. authors wish thank grifﬁths anonymous reviewers helpful comments constructive feedback. research supported swiss national science foundation grant project input zeno karl schindler foundation summerschool grant. chang would like thank christiane born sarah craver cinzia daldini misti program supporting stay switzerland. grateful nvidia corporation donating dgx- part pioneers research award donating minsky machine. references pulkit agrawal ashvin nair pieter abbeel jitendra malik sergey levine. learning poke poking experiential learning intuitive physics. advances neural information processing systems peter battaglia razvan pascanu matthew danilo jimenez rezende others. interaction networks learning objects relations physics. advances neural information processing systems matko bošnjak rocktäschel jason naradowsky sebastian riedel. programming differentiable forth interpreter. proceedings international conference machine learning duan marcin andrychowicz bradly stadie jonathan jonas schneider ilya sutskever pieter abbeel wojciech zaremba. one-shot imitation learning. advances neural information processing systems eslami nicolas heess theophane weber yuval tassa david szepesvari geoffrey hinton attend infer repeat fast scene understanding generative models. advances neural information processing systems felix gers jürgen schmidhuber fred cummins. learning forget continual prediction lstm. artiﬁcial neural networks icann ninth international conference volume goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems klaus greff antti rasmus mathias berglund tele harri valpola juergen schmidhuber. tagger deep unsupervised perceptual grouping. advances neural information processing systems radek grzeszczuk demetri terzopoulos geoffrey hinton. neuroanimator fast neural network emulation control physics-based models. proceedings annual conference computer graphics interactive techniques john hummel keith holyoak collin green leonidas doumas derek devnich aniket kittur donald kalar. solution binding problem compositional connectionism. compositional connectionism cognitive science papers aaai fall symposium levy gayler justin johnson bharath hariharan laurens maaten judy hoffman fei-fei lawrence zitnick ross girshick. inferring executing programs visual reasoning. arxiv preprint arxiv. kansky silver david mély mohamed eldawy miguel lázaro-gredilla xinghua nimrod dorfman szymon sidor scott phoenix dileep george. schema networks zero-shot transfer generative causal model intuitive physics. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. yuko munakata james mcclelland mark johnson robert siegler. rethinking infant knowledge toward adaptive process account successes failures object permanence tasks. psychological review adam santoro david raposo david barrett mateusz malinowski razvan pascanu peter battaglia timothy lillicrap. simple neural network module relational reasoning. arxiv preprint arxiv. jürgen schmidhuber. learning think algorithmic information theory novel combinations reinforcement learning controllers recurrent neural world models. arxiv preprint arxiv. ronald williams. complexity exact gradient computation algorithms recurrent neural networks. technical report technical report technical report nu-ccs-- boston northeastern university college computer science jiajun ilker yildirim joseph bill freeman josh tenenbaum. galileo perceiving physical object properties integrating physics engine deep learning. advances neural information processing systems kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio. show attend tell neural image caption generation visual attention. international conference machine learning experiments train networks using adam default parameters batch size train validation test inputs. quality learned groupings evaluated computing adjusted rand index respect ground truth ignoring background overlap regions early stopping validation loss improved epochs. bouncing balls data similar previous work modiﬁcations. data consists sequences binary images time-steps balls randomly sampled types ball times heavier times larger radius other. balls initialized random initial positions velocities. balls bounce elastically image window. conv. elu. stride layer norm conv. elu. stride layer norm conv. elu. stride layer norm fully connected. elu. layer norm recurrent. sigmoid. layer norm output fully connected. relu. layer norm fully connected. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. sigmoid instead using transposed convolutions ﬁrst reshape image using default nearest-neighbour interpolation followed normal convolution order avoid frequency artifacts note layer norm recurrent connection. timestep feed input network input added bitﬂip noise consistent earlier work r-nem trained next-step prediction objective prior pixel data bernoulli distribution prevent conﬂicting gradient updates back-propagating gradients interaction function υr-nemnetwork structured follows fully connected. relu. layer norm fully connected. relu. layer norm fully connected. relu. layer norm fully connected. tanh. layer norm fully connected. sigmoid. comparison extrapolation comparison experiment r-nem rnn-em trained following insights greff extrapolation task adjusted number components test time comparing rnn-em used υrnn-em. comparing used υrnn-em yielding standard recurrent autoencoder receives time-step used pre-trained produce dataset sequences time-steps. receives stack four frames input recorded every ﬁrst frame stack. frames ﬁrst pre-processed mnih thresholded obtain binary images. since images used different encoder decoder given conv. elu. stride layer norm conv. elu. stride layer norm conv. elu. stride layer norm conv. elu. stride layer norm fully connected. elu. layer norm recurrent. sigmoid. layer norm output fully connected. relu. layer norm fully connected. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. sigmoid used architecture υr-nem difference time-step concatenated embedding action produced agent hidden state. used single layer units relu activation function compute embedding.", "year": 2018}