{"title": "Video Description using Bidirectional Recurrent Neural Networks", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.", "text": "abstract. although traditionally used machine translation ﬁeld encoder-decoder framework recently applied generation video image descriptions. combination convolutional recurrent neural networks models proven outperform previous state obtaining accurate video descriptions. work propose pushing model introducing contributions encoding stage. first producing richer image representations combining object location information convolutional neural networks second introducing bidirectional recurrent neural networks capturing forward backward temporal relationships input frames. automatic generation image descriptions recent trend computer vision represents interesting diﬃcult task. possible dramatic advances convolutional neural network models allowed outperform state-of-the-art algorithms many computer vision problems object recognition object detection activity recognition etc. generating descriptions videos represents even challenging task could lead multiple applications imagine amount data generated youtube video description would help categorize them provide eﬃcient retrieval mechanism serve summarization tool blind people. however problem video description generation several properties make specially diﬃcult. besides signiﬁcant amount image information analyze videos variable number images described sentences diﬀerent length. furthermore descriptions videos high-level summaries necessarily expressed terms objects actions scenes observed images. many open research questions ﬁeld requiring deep video understanding. eﬃciently extract important elements images deﬁne local global spatio-temporal information determine salient content worth describe generate ﬁnal video description. speciﬁc questions need attention computer vision machine translation natural language understanding communities order solved. work propose enrich state-of-the-art architecture using bidirectional neural networks modeling relationships temporal directions. furthermore test inclusion supplementary features help detect contextual information scene video takes place. although problem video captioning recently appeared thanks learning capabilities oﬀered deep learning techniques general pipeline adopted works resembles traditional encoder-decoder methodology used machine translation main diﬀerence that encoder step instead generating compact representation source language sentence generate representation images belonging video. aims automatically translate text speech source target language. within last decades prevailing approach statistical application connectionist models area drawn much attention researchers last years. moreover approach recently proposed so-called neural machine translation translation process carried means large recurrent neural network systems rely encoder-decoder framework encoder produces compact representation input sentence source language decoder takes representation generates corresponding target language sentence. rnns usually make gated units popular long short-term memory order cope long-term relationships. recent reintroduction deep learning computer vision ﬁeld cnns allowed obtain richer image representations compared traditional hand-crafted ones. networks demonstrated powerful tool extract feature representations several kinds computer vision problems like objects scenes recognition. thanks cnns ability serve knowledge transfer mechanisms also usually used feature extractors. majority works devoted generate textual descriptions single images also follow encoder-decoder architecture. encoding stage apply combination lstm describing input image. decoding stage lstm charge receiving image information generating word word ﬁnal description image problem video captioning similar. seminal works applied methodologies inspired classical nevertheless recent works following encoder-decoder approach obtained state-of-the-art performances present methodology natural language video description makes deeper structures double-way analysis input video. propose base architecture introduced contributions twofold. first produce richer image representations combining complementary cnns detecting objects contextual information input images. second introduce bidirectional lstm network encoding stage ability learn forward backward long-term relationships input sequence. moreover make code public. overview proposal depicted fig. propose encoder-decoder approach consisting four stages using cnns lstms describing images modeling temporal relationship respectively. second considering need describe actions performed consecutive frames apply blstm capturing temporal relationships complementary information taking look action forward backward manner. third output vectors forward backward lstm models previous step concatenated together output image soft attention model decoder. model decides parts input video focus emitting next word considering description generated far. given video description problem encoding stage need properly characterize video understanding kind objects structures appear images modeling relationships actions along time. tackling ﬁrst part problem several kinds pretrained cnns used describing images distinguished diﬀerent architectures diﬀerent datasets used training. although extended comparison combinations models could used applying characterization propose combining object context-related information. purpose googlenet architecture separately trained datasets objects scenes combination kinds data inform objects appearing surroundings ideal problem hand. given video cnns generate sequence d-dimensional feature vectors fig. forward layer lstm unit encoder. output depends previous hidden state current feature vector video extracted input output forget gates module amount information ﬂows across unit. solve second problem blstm processes sequence generating sequence vectors. blstm networks composed independent lstm layers namely forward backward. layers analogue latter processes input sequence reversed time. lstm networks have addition classical hidden state memory state. memory state. hidden state controlled output gate current memory state depends updated memory state previous memory state respectively modulated forget input gates obtained applying logistic non-linear function input previous hidden state. lstm gate associated weight matrices accounting input previous hidden state. matrices must estimated training set. figure shows illustration lstm unit. architecture applies backward layer dependencies next time-step previous one. since forward backward layers independent diﬀerent weight matrices estimate. decoder lstm network acts language model conditioned information provided encoder. network equipped attention mechanism soft alignment model implemented single-layered perceptron helps decoder know look generating output word. given sequence generated encoder decoding time-step attention mechanism weights feature vectors combines decoder lstm deﬁned similarly forward layer encoder takes account previously generated word context vector attention mechanism addition previous hidden state. last word representafig. decoder lstm unit. output depends previous hidden state word embedding previously generated word context vector provided attention mechanism probability distribution vocabulary output words deﬁned hidden state means softmax function. function represents conditional probability word given input video history dataset composed open domain clips collected youtube annotated using crowd sourcing platform. video variable number captions written diﬀerent users. used splits made separating dataset videos training validation remaining testing. training clips captions treated separately accounting total training samples. order evaluate compare results diﬀerent models used standardized coco-caption evaluation package provides several metrics text description comparison. used three main metrics presented tests used batch size learning rate automatically adadelta method authors reported applied frame subsampling picking image every frames reducing computational load. parameters network randomly initialized. evaluation validation performed every updates. learning process stopped reported error increased evaluations. conﬁguration best model respect bleu measure validation selected. table report results best models test set. ﬁrst correspond result obtained system object features conﬁgurations reported analyzing obtained results clear improvement trend derived applying blstm temporal inference mechanism. blstm addition using objects features allows improve result metrics obtaining beneﬁt bleu points. adding scenes-related features also slightly improves result although remarkable blstm improvement. combination objects+scenes+blstm oﬀers best cider performance nevertheless result slightly objects+blstm metrics. behaviour probably signiﬁcant increase number parameters learn. investigated whether reduction number parameters reducing size features larger datasets could lead improvements. conclusion presented methodology natural language video description takes proﬁt bidirectional analysis input sequence. architecture ability learn forward backward longterm relationships input images. additionally complementary object scene-related image features proved obtain richer video representation. improvements allowed method outperform state-of-the-art results problem hand. results suggest deep structures help transfer knowledge input sequence frames output natural language caption. hence next step take must delve application deeper modeling structures cnns multi-layered lstm networks. acknowledgments. work partially founded tin--c prometeoii// travel grant miprcv network. radeva partially supported icrea academia grant. acknowledge nvidia donation used work.", "year": 2016}