{"title": "Deep Learning for Multi-label Classification", "tag": ["cs.LG", "cs.AI"], "abstract": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature", "text": "learning classiﬁcation. however relatively little work multi-label literature considered approach. using instance data construct model makes implicit assumption labels originate data recovered directly usually however labels feature variables originate particular abstract concepts. example generally think image labelled beach pixel-data vector beach-like rather image meets criteria abstract idea beach ideally then feature would include variables grainy surface sand pebbles adjacent body water. hence highly desirable recover hidden dependencies structure original concepts behind learning task. good representation dependencies make problem easier learn. restricted boltzmann machine learns layer hidden features unsupervised fashion. hidden layer capture complex dependencies structure input space represent compactly methods detail paper using rbms offer interesting beneﬁts multilabel classiﬁcation variety domains many classiﬁcation paradigms previously relatively uncompetitive multi-label learning often obtain much higher predictive performance become competitive thus offer respective advantages context better posterior-probability estimates lower memory consumption faster performance easier implementation incremental learning. output feature space updated incrementally. makes incremental learning feasible also means cost savings magniﬁed batchlearners need retrained intervals data. model built using unlabeled examples typically obtained much cheaply labelled examples; especially multi-label contexts since examples assigned multiple labels. also stack several rbms create varieties deep belief networks look approaches using dbns. ﬁrst approach learn ﬁnal layer together labels existing multi-label classiﬁer. second approach back-propagation ﬁne-tune weights neural network discriminative prediction augment second multi-label predictive layer. abstract—in multi-label classiﬁcation main focus develop ways learning underlying dependencies labels take advantage classiﬁcation time. developing better feature-space representations predominantly employed reduce complexity e.g. eliminating non-helpful feature attributes input space prior training. important task since many multilabel methods typically create many different copies views input data transform considerable memory saved taking advantage redundancy. paper show proper development feature space make labels less interdependent easier model predict inference time. task deep learning approach restricted boltzmann machines. present deep network that empirical evaluation outperforms number competitive methods literature. multi-label classiﬁcation supervised learning problem instance associated multiple labels. opposed traditional task single-label classiﬁcation instance associated single class label. multi-label context receiving increased attention applicable wide variety domains including text audio data still images video bioinformatics references therein. well-known approach multi-label classiﬁcation simply train independent classiﬁer label. usually known literature binary relevance transformation e.g. essentially multi-label problem transformed binary problem label off-the-shelf binary classiﬁer applied problems individually. practically multi-label literature identiﬁes method limited fact dependencies labels explicitly modelled proposes algorithms take dependencies account. date many successful multi-label algorithms obtained so-called problem transformation methods example methods make many copies feature space memory highest performing methods also ensembles example support vector machines decision trees probabilistic methods boosting competitive methods large part literature could beneﬁt tremendously concise representations feature space relatively much singe-label context; initial investment reducing number feature variables multi-label problem much likely offer considerable speed-ups framework carry empirical evaluation many different methods literature collection real-world datasets diverse domains results indicate beneﬁts style learning multi-label classiﬁcation. multi-label datasets classiﬁcation methods rapidly become numerous recent years classiﬁcation performance steadily improved. overview well known inﬂuential work area provided binary relevance approach obtain high predictive performance model dependencies labels. number methods improved predictive performance methods model label dependence. well-known alternative label powerset method transforms multi-label problem singlelabel problem single class powerset values label dependencies modelled directly predictive performance greater computational complexity high practical applications. complexity issue addressed works former presents rakel ensemble method selects subsets labels uses learn subproblems. classiﬁer chain approach received recent attention example method employs classiﬁer label like classiﬁers independent. rather classiﬁer predicts binary relevance label given input space plus predictions previous classiﬁers another type binary-classiﬁcation approach pairwise transformation method binary model trained pair labels. predictions result naturally pairwise preferences multi-label prediction methods adapted make multi-label predictions example methods performs well several domains although application easily prohibitive many datasets quadratic complexity. alternative problem transformation algorithm adaptation speciﬁc single-label method adapted directly multi-label classiﬁcation. mlknn k-nearest neighbours method adapted multi-label learning voting labels found neighbours. iblr related method also incorporates second layer logistic regression. bpmll back-propagation neural network adapted multi-label classiﬁcation multiple binary outputs label variables. methods. clustering-based supervised approach used obtain label-speciﬁc features label. advantages method reduced label-relevances trained separately example methods case meta technique easily applied independently preprocessing learning techniques describe paper. redundancy eliminated learning space method taking random subsets training space across ensemble. work centers fact standard approach considers full input space label even though subset variables relevant particular label. compressive sensing techniques also used literature reducing complexity multi-label data taking advantage label sparsity methods mainly motivated reducing algorithm’s running-time reducing number feature variables input space rather learning modelling dependencies them. examples featurespace reduction multi-label classiﬁcation reviewed authors fully-connected network closely related boltzmann machine multi-label classiﬁcation using gibbs sampling inference. network model dependencies label space prediction rather improve feature space. since fully connected network tractable problems relatively small number labels. follows input domain possible feature values. instance represented vector feature values output domain possible labels. instance associated subset labels typically represented binary vector i.e. label associated instance otherwise. example; example. boltzmann machine type fully-connected neural network used discover underlying regularities training data many features involved type network tractable restricted boltzmann machine setting units fully connected layers unconnected within layers. learns layer hidden feature variables original feature variables training hidden variables provide compact representation underlying patterns structure input. fact capture input space regions whereas standard clustering requires parameters examples capture much complexity. used single-label classiﬁcation ﬁnal output layer typically softmax function following section outline approach creating dbns suitable multi-label classiﬁcation. ideally would produce hidden variables correspond directly label variables thus could recover label vector directly given input vector; i.e. deterministically mappable unfortunately seldom case abstract hidden variables need correspond directly labels. however expect hidden layer data closely related labels original data thus makes sense feature space classify instances. hence using hidden space created would expect multi-label classiﬁer obtain better performance simply using hidden representation instance input feature space associating labels create training train multi-label classiﬁer dataset. evaluate test instance feed obtain upper layer acquire prediction thus test instance. take approaches. since sub-optimality produced greedy learning necessarily harmful many discriminative supervised methods treat ﬁnal hidden layer variables feature input variables train off-the-shelf multi-label model predict neural network bpmll except create layers initialize weights using rbms. later show methods performs much better. employ back propagation ﬁne-tune network supervised fashion example number epochs training instance propagated forward network output prediction ˆyi. errors propagated backward network updating weights initialisation rbms fewer epochs required would usually typical back propagation approaches possible depth form including additional classiﬁcation layer. multi-label context previously done basic method second trained outputs ﬁrst related technique neural network context often called skip layer used e.g. case allow generic classiﬁers. helps discriminative power taking account dependencies label space. note also experimented models instance space label space together generatively multi-label setting complicates inference since possible tried using gibbs sampling could obtain competitive results model multi-label setting compared approaches however seems like interesting direction intend follow idea future work. carry empirical evaluation gauge effectiveness efﬁciency rbms dbns number different multi-label classiﬁcation scenarios using different learning algorithms wide collection databases. implemented methods meka framework; open-source java-based framework number important benchmark multi-label methods. framework rbms easily used wide variety multi-label schemes. source code implementations made available part meka framework. selected commonly-used datasets variety domains listed table along basic statistics them. datasets vary considerably respect type data dimensions music instances music associated emotions; scene images belong categories; yeast proteins associated multiple biological functions genbase gene sequences. medical enron reuters text datasets text documents associated categories. datasets described greater detail ﬁrst compare performance introducing blindly trained reducing input dimension three common paradigms multi-label classiﬁcation test improvements proposed feature extraction algorithm. would improve performance multi-label classiﬁcation paradigms extracted features relevant better describing task hand neutral negative features extracted blindly correspond relevant features assigning labels. several parameters need ﬁne-tuned three-fold cross validation them. considered number hidden units learning rate momentum used weight costs epochs throughout. svm-based classiﬁers signiﬁcant difference processed features compared using data directly kernel compensate preprocessing done rbm. case almost results comparable except scene medical which respectively eccr outperform. remark linear logistic regression good nonlinear cases seams using features reduces input dimension makes classiﬁcation problem easier linear classiﬁer performs well state-of-the-art nonlinear classiﬁer. figure show accuracy seven data bases eccr multi-label classiﬁer classiﬁer function number hidden units rbm. plot seen enough features using comparable using clear medical number features little would needed increase number extracted features achieved performance does. finally table show accuracy svmbased classiﬁer scene dataset tested combinations learning rate momentum number hidden units ﬁxed accuracy case combination learning rate momentum better indicates sufﬁcient number hidden units learning quite robust overly sensitive hyperparameter settings. ensemble classiﬁer chains competitive method uses chain rule improve prediction potential label. unclear best ordering ensemble labels randomly ordered realization table report accuracy deﬁned report performance multi-label classiﬁers table eccr respectively denote accuracy rbm-generated features original input space. used different classiﬁers nonlinear logistic regression trained default parameters weka. seen logistic regression classiﬁer achieved accuracy generated features signiﬁcantly better music scene enron reuters datasets underperforms medical dataset comparable yeast genbase datasets. reduces dimensionality input space classiﬁer also makes features suitable linear classiﬁers allows interpreting there variety multi-label evaluation measures used multi-label experiments literature; provides overview popular. accuracy provides good balance gauge overall predictive performance multi-label methods pairwise classiﬁcation implemented pairwise approach namely four-class pairwise classiﬁer build models learn classes label pair dividing votes individual labels using threshold classiﬁcation time. overall obtains better predictive performance pairwise methods create decision boundaries labels example especially svms. report accuracy table using hyper parameters make results comparable across multi-label classiﬁcation paradigms reported table iib. conclusions similar paradigms. linear classiﬁer signiﬁcantly better generated features original input space nonlinear classiﬁer versatile enough provide accurate predictions without generated features. fortunately linear classiﬁer generated features quite close svm-based classiﬁer allows interpret features contribute label hence provide intuitive interpretations features hard interpretation nonlinear mapping. results paradigm similar ones reported previous section. logistic regression generated features lend accurate predictions compared unprocessed features baseline classiﬁer comparable results achieved nonlinear classiﬁer. processing features might need rely nonlinear classiﬁer. using generated features help hurt either terms accuracy nonlinear mapping shantanu godbole sunita sarawagi. discriminative methods multi-labeled classiﬁcation. pakdd eighth paciﬁc-asia conference knowledge discovery data mining pages springer yuhong suicheng multi-label classiﬁcation using conditional dependency networks. ijcai international conference artiﬁcial intelligence pages ijcai/aaai jesse read bernhard pfahringer geoff holmes. multi-label classiﬁcation using ensembles pruned sets. icdm eighth ieee international conference data mining pages ieee jesse read bernhard pfahringer geoff holmes eibe frank. classiﬁer chains multi-label classiﬁcation. ecml european conference machine learning pages springer leander schietgat celine vens struyf hendrik blockeel dragi kocev saso dzeroski. predicting gene function using hierarchical multi-label decision tree ensembles. bioinformatics eduardo sontag. feedforward nets interpolation classiﬁcation. newton spolar everton alvares cherman maria carolina monard huei diana lee. comparison multi-label feature selection methods using problem transformation approach. electronic notes theoretical computer science proceedings {xxxviii} latin american conference informatics farbound hsuan-tien lin. multi-label classiﬁcation workshop proceedings grigorios tsoumakas ioannis vlahavas. random k-labelsets ensemble method multilabel classiﬁcation. ecml european conference machine learning pages springer rong jelena tesic john smith. model-shared subspace boosting multi-label classiﬁcation. sigkdd international conference knowledge discovery data mining pages julio zaragoza luis enrique sucar eduardo morales concha bielza pedro larra˜naga. bayesian chain classiﬁers multidimensional classiﬁcation. ijcai’ international joint conference artiﬁcial intelligence pages min-ling zhang zhang. multi-label learning exploiting label dependency. sigkdd international conference knowledge discovery data mining pages york acm. min-ling zhang zhi-hua zhou. multilabel neural networks applications functional genomics text categorization. ieee transactions knowledge data engineering table compare accuracy proposed dbms structures previously proposed methods. also added mlknn bpmll iblr table either best classiﬁer close best give sense features generated second layer improve ﬁrst layer. example database eccr good enough compared almost good performance databases also improved structure seems amenable multi-label classiﬁcation competitive proposed paradigms literature. empirical evaluation variety multi-label datasets shows selection high-performing multi-label methods literature improved upon using rbm-processed feature space. labels become easier model training time predict inference time. obtained improvement percentage points accuracy using original feature space directly. study showed important improvements obtained multi-label classiﬁcation respect scalability predictive performance using deep learning area multi-label classiﬁcation. result recommend multi-labellers focus feature modelling rather solely modelling dependencies output labels. multi-label models achieved best predictive performance overall compared seven competing methods multi-label literature. weiwei cheng krzysztof dembczy´nski eyke h¨ullermeier. bayes optimal multilabel classiﬁcation probabilistic classiﬁer chains. icml’ international conference machine learning haifa israel june omnipress. johannes f¨urnkranz eyke h¨ullermeier eneldo loza menc´ıa klaus brinker. multilabel classiﬁcation calibrated label ranking. machine learning november", "year": 2014}