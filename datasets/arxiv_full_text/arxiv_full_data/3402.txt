{"title": "iCaRL: Incremental Classifier and Representation Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.", "text": "figure class-incremental learning algorithm learns continuously sequential data stream classes occur. time learner able perform multi-class classiﬁcation classes observed far. ﬁrst criteria express essence classincremental learning. third criterion prevents trivial algorithms storing training examples retraining ordinary multi-class classiﬁer whenever data becomes available. interestingly despite vast progress image classiﬁcation made last decades single satisfactory class-incremental learning algorithm days. existing multi-class techniques simply violate handle ﬁxed number classes and/or need training data available time. naively could overcome training classiﬁers class-incremental data streams e.g. using stochastic gradient descent optimization. this however cause classiﬁcation accuracy quickly deteriorate effect known literature catastrophic forgetting catastrophic interference existing techniques fulﬁll properties principally limited situations ﬁxed data representation. cannot extended deep architectures learn classiﬁers feature representations time major open problem road artiﬁcial intelligence development incrementally learning systems learn concepts time stream data. work introduce training strategy icarl allows learning classincremental training data small number classes present time classes added progressively. icarl learns strong classiﬁers data representation simultaneously. distinguishes earlier works fundamentally limited ﬁxed data representations therefore incompatible deep learning architectures. show experiments cifar- imagenet ilsvrc data icarl learn many classes incrementally long period time strategies quickly fail. natural vision systems inherently incremental visual information continuously incorporated existing knowledge preserved. example child visiting learn many animals without forgetting home. contrast artiﬁcial object recognition systems trained batch setting object classes known advance training data classes accessed time arbitrary order. ﬁeld computer vision moves closer towards artiﬁcial intelligence becomes apparent ﬂexible strategies required handle large-scale dynamic properties real-world object categorization situations. least visual object classiﬁcation system able incrementally learn classes training data becomes available. call scenario class-incremental learning. work introduce icarl practical strategy simultaneously learning classiﬁers feature representation class-incremental setting. based careful analysis shortcomings existing approaches introduce three main components combination allow icarl fulﬁll criteria forth above. three components classiﬁcation nearest-mean-of-exemplars rule prioritized exemplar selection based herding representation learning using knowledge distillation explain details steps section subsequently context previous work section section report experiments cifar imagenet datasets show icarl able classincrementally learn long periods time methods quickly fail. finally conclude section discussion remaining limitations future work. section describe icarl’s main components explain combination allows true classincremental learning. section explains underlying architecture gives high-level overview training classiﬁcation steps. sections provides algorithmic details explains design choices. observed class icarl ensures total number exemplar images never exceeds ﬁxed parameter algorithm describes mean-of-exemplars classiﬁer used classify images classes observed section detailed explanation. training. icarl processes batches classes time using incremental learning strategy. every time data classes available icarl calls update routine routine adjusts icarl’s internal knowledge based additional information available observations also icarl learns existence classes. architecture. hood icarl makes convolutional neural network interpret network trainable feature extractor followed single classiﬁcation layer many sigmoid output nodes classes observed feature vectors l-normalized results operation feature vectors e.g. averages also re-normalized write explicitly avoid cluttered notation. denote parameters network split ﬁxed number parameters feature extraction part variable number weight vectors. denote latter following sections convention denotes number classes observed far. resulting netin principle icarl strategy largely architecture agnostic could feature metric learning strategies. here discuss context cnns avoid overly general notation. resource usage. incremental nature icarl need priori information many classes occur theory– unlimited amount time. time runtime memory requirement size feature extraction parameters storage exemplar images many weight vectors classes observed. knowledge allows assign resources depending application scenario. upper bound number classes known simply pre-allocate space many weight vectors required remaining available memory store exemplars. without upper limit would actually grow number weight vectors time decrease size exemplar accordingly. clearly least exemplar image weight vector required classes learned ultimately ﬁnite number classes learned unless allows possibility resources runtime algorithm. note icarl handle increase resources on-the-ﬂy without retraining simply discard exemplars unless forced memory limitations. icarl uses nearest-mean-of-exemplars classiﬁcation strategy. predict label image computes prototype vector class observed average |py| feature vector exemplars class also computes feature vector image classiﬁed assigns class label similar prototype background. nearest-mean-of-exemplars classiﬁcation rule overcomes major problems incremental learning setting seen contrasting possibilities multi-class classiﬁcation. usual classiﬁcation rule neural network would argmaxy=...t network output deﬁned alternatively softmax output layer. argmaxy argmaxy network’s prediction rule equivalent linear classiﬁer non-linear feature weight vectors class-incremental setting problematic weight vectors decoupled feature extraction routine whenever changes must updated well. otherwise network outputs change uncontrollably observable catastrophic forgetting. contrast nearestmean-of-exemplars rule decoupled weight vectors. class-prototypes automatically change whenever feature representation changes making classiﬁer robust changes feature representation. choice average vector prototype inspired nearest-class-mean classiﬁer incremental learning ﬁxed feature representation. classincremental setting cannot make true class mean since training data would stored order recompute quantity representation change. instead average ﬂexible number exemplars chosen provide good approximation class mean. note that work normalized feature vectors equation written equivalently argmaxy therefore also interpret classiﬁcation step classiﬁcation weight vector decoupled data representation changes consistently exemplar set. algorithm lists steps incrementally improving feature representation. first icarl constructs augmented training consisting currently available training examples together stored exemplars. next current network evaluated example resulting network outputs previous classes stored finally network parameters updated minimizing loss function image encourages network output correct class indicator classes classes reproduce scores stored previous step background. representation learning step resembles ordinary network ﬁnetuning starting previously learned network weights minimizes loss function training set. consequence standard end-to-end learning methods used backpropagation mini-batches also recent improvements dropout adaptive stepsize selection batch normalization well potential future improvements. modiﬁcations plain ﬁnetuning preventing least mitigating catastrophic forgetting. first training augmented. consists training examples also stored exemplars. ensured least information data distribution previous classes enters training process. note step important exemplars stored images feature representation would become outdated time. second loss function augmented well. besides standard classiﬁcation loss encourages improvements feature representation allow classifying newly observed classes well also contains distillation loss ensures discriminative information learned previously lost learning step. whenever icarl encounters classes adjusts exemplar set. classes treated equally this i.e. classes observed total number exemplars stored icarl exemplars class. ensured available memory budget exemplars always used full extent never exceeded. routines responsible exemplar management select exemplars classes reduce sizes exemplar sets previous classes. algorithm describes exemplar selection step. exemplars selected stored iteratively target number met. step iteration example current training added exemplar namely causes average feature vector exemplars best approximate average feature vector training examples. thus exemplar really prioritized list. order elements matters exemplars earlier list important. procedure removing exemplars speciﬁed algorithm particularly simple reduce number exemplars discards exemplars keeping examples background. exemplar management routines designed objectives mind initial exemplar approximate class mean vector well possible remove exemplars time algorithm’s runtime without violating property. latter property challenging actual class mean vector available algorithm anymore removal procedure called. therefore adopt data-independent removal strategy removing elements ﬁxed order starting make responsibility exemplar construction routine make sure desired approximation properties fulﬁlled even removal procedure called later times. prioritized construction logical consequence condition ensures average feature vector subset exemplars starting ﬁrst good approximation mean vector. prioritized construction used herding create representative samples distribution. also shown iterative selection requires fewer samples achieve high approximation quality than e.g. random subsampling. contrast potential methods exemplar selection designed objectives guaranteed provide good approximation quality number prototypes. overall icarl’s steps exemplar selection reduction exactly incremental learning setting selection step required class once ﬁrst observed training data available. later times reduction step called need access earlier training data. icarl builds insights multiple earlier attempts address class-incremental learning. section describe important ones structuring hand learning techniques ﬁxed data representations hand techniques also learn data representation classical connectionists well recent deep learning approaches. learning ﬁxed data representation. data representation ﬁxed main challenge classincremental learning design classiﬁer architecture accommodate classes time training process without requiring access training data seen far. simplest process type could nearest neighbor classiﬁer would require storing training data learning process therefore qualify class-incremental procedure deﬁnition. mensink observed nearest class mean classiﬁer property. represents class prototype vector average feature vector examples observed class far. vector computed incrementally data stream need store training examples. example classiﬁed assigning class label prototype similar example’s feature vector respect metric also learned data. despite simplicity shown work well robust standard parametric classiﬁers incremental learning setting ncm’s main shortcoming cannot easily extended situation nonlinear data representation learned together classiﬁers prevents class mean vectors computable incremental way. icarl adopt idea prototype-based classiﬁcation. however prototypes average features vectors examples speciﬁcally chosen subset allows keep small memory footprint perform necessary updates constant computational effort. class-incremental learning criteria i)–iii) introduced section partially kuzborskij showed loss accuracy avoided adding classes existing linear multi-class classiﬁer long classiﬁers retrained least small amount data classes. chen divvala introduced systems autonomously retrieve images resources identiﬁes relations them incrementally learn object classiﬁers. royer lampert adapt classiﬁers time-varying data stream method cannot handle newly appearing classes pentina show learning multiple tasks sequentially beneﬁcial choosing order data tasks available time. wechsler scheirer well bendale boult aimed related distinct problem open recognition test examples might come classes training examples seen far. polikar introduced ensemble based approach handle increasing number classes needs training data classes occur repeatedly. zero-shot learning proposed lampert classify examples previously unseen classes include training step those. representation learning. recent success neural networks large parts attributed ability learn classiﬁers also suitable data representations least standard batch setting. first attempts learn data representations incremental fashion already found classic neural network literature e.g. particular late mccloskey described problem catastrophic forgetting i.e. phenomenon training neural network data causes overwrite learned previous data. however classical works mainly context connectionist memory networks classiﬁers networks used small shallow today’s standards. generally existing algorithms architectural changes unable prevent catastrophic forgetting example moe-helgesen al.’s survey classical goodfellow al.’s modern architectures except speciﬁc settings kirkpatrick al.’s major achievement early connectionist works however identiﬁed main strategies catastrophic forgetting addressed freezing parts network weights time growing network order preserve ability learn rehearsal i.e. continuously stimulating network recent also earlier data. works mainly followed freeze/grow strategy however requires allocating resources network time therefore violates principle iii) deﬁnition class-incremental learning. example xiao learn tree-structured model grows incrementally classes observed. context multi-task reinforcement learning rusu propose growing networks extending layer horizontally. icarl adopt principle rehearsal update model parameters learning representation training data currently available classes also exemplars earlier classes available anyway required prototypebased classiﬁcation rule. additionally icarl also uses distillation prevent information network deteriorates much time. hinton originally proposed distillation transfer information different neural networks icarl within single network different time points. principle recently proposed hoiem name learning without forgetting incrementally train single network learning multiple tasks e.g. multiple object recognition datasets. main difference class-incremental multi-class situation lies prediction step multi-class learner pick classiﬁer predicts correctly observed classes. multi-task leaner make multiple classiﬁers evaluated data dataset. section propose protocol evaluating incremental learning methods compare icarl’s classiﬁcation accuracy alternative methods also report experiments shed light icarl’s working mechanisms isolating effect individual components benchmark protocol. agreed upon benchmark protocol evaluation class-incremental learning methods exist. therefore propose following evaluation procedure given multi-class classiﬁcation dataset classes arranged ﬁxed random order. method trained class-incremental available training data. batch classes resulting classiﬁer evaluated test part data dataset considering classes already trained. note that even though test data used once overﬁtting occur testing results revealed algorithms. result evaluation curves classiﬁcation accuracies batch classes. single number preferable report average accuracies called average incremental accuracy. task image classiﬁcation introduce instantiations protocol. icifar- benchmark cifar- data train classes batches classes time. evaluation measure standard multi-class accuracy test set. dataset manageable size benchmark times different class orders reports averages standard deviations results. iilsvrc benchmark imagenet ilsvrc dataset settings using subset classes trained batches using classes processed batches evaluation measure top- accuracy part dataset. icarl implementation. icifar- rely theano package train -layers resnet allowing icarl store exemplars. training step consists epochs. learning rate starts divided epochs iilsvrc maximal number exemplars tensorﬂow framework train -layers resnet epochs class batch. learning rate starts divided epochs methods train network using standard backpropagation minibatches size weight decay parameter note learning rates might appear large purpose worked well likely binary cross-entropy network layer. smaller rates might required multi-class softmax layer. source code data available http//www.github.com/srebuffi/icarl. main experiments studies classiﬁcation accuracy different methods class-incremental conditions. besides icarl implemented tested three alternative class-incremental methods. finetuning learns ordinary multi-class network without taking measures prevent catastrophic forgetting. also interpreted learning multi-class classiﬁer incoming classes ﬁnetuning previously learned multiclass classiﬁcation network. fixed representation also learns multi-class classiﬁcation network prevents catastrophic forgetting. freezes feature representation ﬁrst batch classes processed weights classiﬁcation layer corresponding classes processed. subsequent batches classes weights vectors classes trained. finally figure experimental results class-incremental training icifar- iilsvrc reported multi-class accuracies across classes observed certain time point. icarl clearly outperforms methods setting. fixing data representation trained ﬁrst batch performs worse distillation-based lwf.mc except iilsvrc-full. finetuning network without preventing catastrophic forgetting achieves worst results. comparison network trained data available achieves multi-class accuracy. also compare network classiﬁer attempts preventing catastrophic forgetting using distillation loss learning like icarl does exemplar set. classiﬁcation uses network output values themselves. essentially learning without forgetting approach applied multi-class classiﬁcation denote lwf.mc. figure shows results. icarl clearly outperforms methods incremental setting among methods distillation-based network training always second best except iilsvrcfull better representation ﬁrst batch classes. finetuning always achieves worst results conﬁrming catastrophic forgetting indeed major problem class-incremental learning. figure confusion matrices different method icifar- better visibility). icarl’s predictions distributed close uniformly classes whereas lwf.mc tends predict classes recent batches frequently. classiﬁer ﬁxed representation bias towards classes ﬁrst batch network trained ﬁnetuning predicts exclusively classes labels last batch. -class classiﬁer icifar- training using batches classes time characteristic patterns icarl’s confusion matrix looks homogeneous classes terms diagonal entries well off-diagonal entries shows icarl intrinsic bias towards contrast this confusion matrices classes show inhomogeneous patterns distillation-based training many non-zero entries towards right i.e. recently learned classes. even extreme effect ﬁnetuning predicted class labels come last batch classes network trained with. ﬁnetuned network simply forgot earlier classes even exist. ﬁxed representation shows opposite pattern prefers output classes ﬁrst batch classes trained confusion matrices iilsvrc show patterns found appendix. first analyze exactly icarl improves plain ﬁnetuning-based training differs three aspects mean-of-exemplars classiﬁcation rule exemplars representation learning distillation loss. ﬁrst learns representation icarl uses network’s outputs directly classiﬁcation mean-of-exemplar classiﬁer. second uses exemplars classiﬁcation distillation loss training. third uses neither distillation loss exemplars classiﬁcation makes exemplars representation learning. comparison also include lwf.mc again uses distillation exemplars all. table summarizes results average classiﬁcation accuracies steps incremental training. hybrid setups mostly achieve results icarl lwf.mc showing indeed icarl’s components contribute substantially good performance. particular comparison icarl hybrid shows mean-of-exemplar classiﬁers particularly advantageous smaller batch sizes i.e. updates representation performed. comparing icarl hybrid sees small class batch sizes distillation even hurt classiﬁcation accuracy compared using prototypes. larger batch sizes fewer updates distillation loss clearly advantageous. finally comparing result hybrid lwf.mc clearly shows effectiveness exemplars preventing catastrophic forgetting. second experiments study much accuracy lost using means-of-exemplars classiﬁcation prototypes instead nearest-class-mean rule. latter unmodiﬁed icarl learn representation classify images class-means recomputed representation update using current feature extractor. note requires storing training data would qualify class-incremental method. results table show minor differences icarl conﬁrming icarl reliably identiﬁes representative exemplars. figure illustrates effect different memory budgets comparing icarl hybrid classiﬁer table classiﬁer table data representation icarl differ classiﬁcation rules. method beneﬁt larger memory budget showing icarl’s representation learning step indeed beneﬁts prototypes. given enough prototypes icarl’s mean-of-exemplars classiﬁer performs similarly classiﬁer classifying network outputs competitive. introduced icarl strategy class-incremental learning learns classiﬁers feature representation simultaneously. icarl’s three main components nearest-mean-of-exemplars classiﬁer robust changes data representation needing store small number exemplars class herdingbased step prioritized exemplar selection representation learning step uses exemplars combination distillation avoid catastrophic forgetting. experiments cifar- imagenet ilsvrc data show icarl able learn incrementally long period time methods fail quickly. main reason icarl’s strong classiﬁcation results exemplar images. intuitive being able rely stored exemplars addition network parameters could beneﬁcial nevertheless important observation pronounced effect class-incremental setting. therefore hypothesize also architectures able beneﬁt using combination network parameters exemplars especially given fact many thousands images stored memory requirements comparable sizes current deep networks. particular icarl’s perforﬁcation solved. mance still lower systems achieve trained batch setting i.e. training examples classes available time. future work plan analyze reasons detail goal closing remaining performance gap. also plan study related scenarios classiﬁer cannot store training data form e.g. privacy reasons. possible direction would encode feature characteristics earlier tasks implicitly autoencoder recently proposed rannen triki acknowledgments. work parts funded european research council european union’s seventh framework programme /erc grant agreement life-long learning visual scene understanding tesla cards used research donated nvidia corporation.", "year": 2016}