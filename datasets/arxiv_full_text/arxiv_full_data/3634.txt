{"title": "Constrained Deep Learning using Conditional Gradient and Applications in  Computer Vision", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "A number of results have recently demonstrated the benefits of incorporating various constraints when training deep architectures in vision and machine learning. The advantages range from guarantees for statistical generalization to better accuracy to compression. But support for general constraints within widely used libraries remains scarce and their broader deployment within many applications that can benefit from them remains under-explored. Part of the reason is that Stochastic gradient descent (SGD), the workhorse for training deep neural networks, does not natively deal with constraints with global scope very well. In this paper, we revisit a classical first order scheme from numerical optimization, Conditional Gradients (CG), that has, thus far had limited applicability in training deep models. We show via rigorous analysis how various constraints can be naturally handled by modifications of this algorithm. We provide convergence guarantees and show a suite of immediate benefits that are possible -- from training ResNets with fewer layers but better accuracy simply by substituting in our version of CG to faster training of GANs with 50% fewer epochs in image inpainting applications to provably better generalization guarantees using efficiently implementable forms of recently proposed regularizers.", "text": "number results recently demonstrated beneﬁts incorporating various constraints training deep architectures vision machine learning. advantages range guarantees statistical generalization better accuracy compression. support general constraints within widely used libraries remains scarce broader deployment within many applications beneﬁt remains under-explored. part reason stochastic gradient descent workhorse training deep neural networks natively deal constraints global scope well. paper revisit classical ﬁrst order scheme numerical optimization conditional gradients thus limited applicability training deep models. show rigorous analysis various constraints naturally handled modiﬁcations algorithm. provide convergence guarantees show suite immediate beneﬁts possible training resnets fewer layers better accuracy simply substituting version faster training gans fewer epochs image inpainting applications provably better generalization guarantees using efﬁciently implementable forms recently proposed regularizers. w×···×wl denotes cartesian product weight matrices network layers seek learn data sampled underlying distribution here thought features data denotes labels variable parameterizes function desire learn predicts labels given features whose accuracy measured using loss function unsupervised setting notion labels hence common practice inject prior knowledge inductive bias loss simplicity speciﬁcation intentionally transparent activation function layers speciﬁc network architecture. common instantiations task non-convex results last years demonstrate reasonable minimizers generalize well test data found variants. recent results also explored interplay overparameterization network degrees freedom issues related global optimality haeffele vidal janzamin soudry carmon regularizers. independent network architecture choose deploy given task practitioner often want impose additional constraints regularizers pertinent application domain interest. fact task speciﬁc constraints improve behavioral performance neural networks computational statistical perspective long history dating back least platt barr zhang constantinides ideas revisited recently rudd motivated generalization convergence simply strategy compression cheng however using constraints types architectures common modern computer vision problems still actively researched various groups. example mikolov demonstrated training recurrent networks accelerated constraining part recurrent matrix close identity. sparsity low-rank encouraging constraints shown promise number settings interesting paper pathak showed linear constraints output layer improves accuracy semantic image segmentation task. m´arquez-neila showed hard constraints output layer yield competitive results human pose estimation task oktay used anatomical constraints cardiac image analysis. discussion suggests results demonstrating value speciﬁc constraints speciﬁc problems development still nascent stage. therefore surprising existing software libraries apis deep learning vision machine learning offer little support constraints. example keras offers support simple bound constraintscharles optimization schemes. aside issue constraints moment discuss choice optimization schemes today. little doubt algorithms dominate landscape problems vision machine learning. instead evaluating loss gradient full training simply computes gradient parameters using training examples. mitigates cost running back propagation full training data comes various theoretical guarantees well hardt raginsky reader notice part reason constraints intensively explored broader range problems interplay constraints algorithm m´arquez-neila regularizers local constraints easily handled within others require great deal care adversely affect convergence practical runtime bengio also broad range constraints unlikely work well based theoretical results known today remains open question optimization johnson zhang defazio note algorithms standard remained constant focus research community since offer many theoretical advantages also easily translated practice dauphin grubb bagnell include adaptive sub-gradient methods adagrad duchi rmsprop algorithm dauphin addresses issue illconditioning deep networks normalized form various adaptive schemes learning rate adjustments zeiler utilizing momentum method kingma however reader notice methods impose constraints local fashion since computational cost imposing global constraints using sgd-based methods becomes extremely high pathak explore issue depth next section. question constraints needed statistical learning models vision machine learning equivalently restated terms need regularization setting learning models notion well known precisely stated shortly. recall regularization schemes form another nearly back study ﬁtting models observations data wahba mundle broadly speaking schemes divided related categories algebraic statistical ﬁrst category refer problems otherwise possible difﬁcult solve also known ill-posed problems tikhonov example without introducing additional piece information possible solve linear system equations number observations less number degrees freedom second category regularization explaining data using simple hypotheses rather complex ones example minimum description length principle rissanen underlying rationale that complex hypotheses less likely accurate unobserved samples words generalizability models diminish complexity model increases ﬁxed dataset size. equivalently need data train complex models. recent developments theoretical side showed imposing simple global constraints parameter space deep networks effective analyzing learning theoretic properties including sample complexity generalization error neyshabur hence seek solve suitable regularization function ﬁxed usually assume simple example sense gradient computed efﬁciently. using lagrangian interpretation problem following constrained formulation note loss function convex problems equivalent sense given exists optimal solutions problems coincide practice chosen standard statistical procedures cross validation. finding pareto optimal solutions hand loss function nonconvex typically case formulation powerful ﬁxed might solutions whereas solution problem obtained section boyd vandenberghe turns easier understand phenomenon lens multiobjective optimization. multiobjective optimization care taken even deﬁne notion optimality feasible points depending problem. among various notions optimality argue pareto optimality suited goal. essence overﬁtting effectively. practice many algorithms pareto optimal solutions problem dominates speciﬁcally formulation falls category scalarization technique whereas \u0001-constrained technique. well known problem nonconvex \u0001-constrained technique yields pareto optimal solutions whereas scalarization technique algorithms used solve different. contributions show many interesting global constraints interest vision/machine learning reformulations enforced using classical optimization technique deployed much training deep learning models. analyze theoretical aspects proposal detail. application side speciﬁcally explore analyze performance conditional gradient algorithm speciﬁc focus training deep models constrained formulation shown progressively cases loss accuracy training time scenarios procedure shines offers signiﬁcant beneﬁts performance runtime generalization. experiments indicate that less -parameters improve resnet accuracy gans trained nearly third computational time achieving better qualitative performance image inpainting task. setup stage development ﬁrst discuss broad strategies used solve problems form shown first natural extension gradient descent also known projected used intuitively take gradient step compute point closest feasible deﬁned regularization function. hence iteration requires solution following optimization problem so-called projection operator frobenius norm gradient step size learning rate. practice compute using training samples running backpropagation algorithm. note objective function e∼dl smooth probability distribution commonly referred stochastic smoothing. hence descriptions assume derivative well deﬁned ghadimi furthermore constraints simply standard method broadly used literature. essentially requires optimizing quadratic function feasible set. main bottleneck imposing constraints complexity solving even though many admit efﬁcient procedure theory using applications training deep models challenge since complicated easily amenable implementation taylor frerix wong kolter. natural question whether methods faster following sense solve simpler problems iteration also impose constraints effectively? assertive answer provided scheme falls second general category ﬁrst order methods conditional gradient algorithm reddi frank wolfe guarantee convergence update mild conditions case problems form much simpler form hence suitable training deep learning models. additional bonus algorithms also offer nice space complexity guarantees also tangibly attainable practice making promising choice constrained training deep models. order study promise algorithm training deep models important speciﬁcally understand exactly algorithm behaves regularization constraints commonly used vision machine learning. section describe broad basket generic constraints broadly used community arranged hierarchy sorts ranging cases scheme perfect expected yield wide-ranging improvements situations performance satisfactory additional technical development warranted. example -norm often used induce sparsity collins kohli nuclear norm used induce rank regularization often compression and/or speed-up reasons barone know constraints imposed using likely work well? order analyze qualitative nature constraints suitable algorithm categorize constraints three main categories based update computationally learning-wise compare update constraints. categorize constraints category updates take similar form algebraically. reason call category excellent easy transfer empirical knowledge obtained unconstrained setting speciﬁcally learning dropout rates regime want impose additional constraints. case quantiﬁable improvements terms computation learning. easy update rules essentially take amount calculation easily done performing backpropagation step. actual change existing implementation minimal automatically offer important advantage notably scale invariance several recent papers found advantageous computationally theoretically lacoste-julien jaggi nuclear norm. hand nuclear norm situation quite different. known projection algorithms require computing iteration full singular value decomposition case deep learning methods becomes restrictive recht contrast requires computing top- singular vector done easily efﬁciently power method jaggi hence case number edges network near-quadratic speed i.e. making practically implementable golub loan large scale settings encountered vision. furthermore interesting observe rank running iterations implies need store vectors instead whole matrix making viable solution deployment small form factor devices memory constraints howard hence case obtain strong practical impact algorithms immediately. main takeaway that since projections computationally expensive projected viable option practice. earlier algorithms always least efﬁcient updates general constraint imposed using algorithm also imposed algorithm faster. hence generic constraints deﬁned category constraints empirical knowledge cannot easily transferred pgd. classical norms fall category example ball done linear time using gradient clipping boyd vandenberghe evaluate step constraint corresponds iteration update edge network. might necessarily common knowledge high dropout rate leads underﬁtting words network tends need longer training time srivastava similarly update step algorithm takes following form case update uses sign gradient magnitude all. cases issue information gradients used standard form algorithm making efﬁcient practical purposes. interestingly even though update rules extreme ways using gradient information fact group norm type penalty model trade-off. recent work shows efﬁcient procedures solve corresponding updates well space reasons ideas described supplement garber hazan class regularization norms nicely fall either categories used several problems vision total variation norm. norm widely used denoising algorithms promote smoothness estimated sharp image chambolle lions norm image deﬁned certain type norm discrete gradient ﬁeld ∇ji) i.e. note corresponds classical anisotropic isotropic norm respectively. motivated idea deﬁne norm feed forward deep network. norm name suggests captures notion balanced networks shown make network stable neyshabur incidence matrix network rows indexed nodes columns indexed edges column contains exactly nonzero entries rows corresponding starting node ending node respectively. also consider weight matrix network vector indexed order columns then norm deep neural network turns trivial solve requires special schemes fadili peyr´e runtime complexity number nodes impractical deep learning applications vision. contrast iterations require special form maximum computation done efﬁciently goldfarb harchaoui lemma \u0001-approximate step computed time proof show problem equivalent solving dual speciﬁc linear program. efﬁciently accomplished using johnson zhang space reasons full proof supplement. remark discussion suggests conceptually category constraints incorporated immensely beneﬁt methods. however unlike category constraints requires specialized implementations solve subproblems currently available popular libraries. additional work needed broad utilization possible. covered constraints already vision/machine learning recently attempts m´arquez-neila made utilize deep networks. review notion regularization introduced recently roots primarily deep learning neyshabur ﬁrst deﬁnition explain properties type constraint captures. denotes paths corresponds node input layer corresponds edge node layer i-th layer lies path vout output layer. therefore path norm measures norm possible paths network output layer. need path norm? basic properties relu scaling invariant following multiplying weights incoming edges node positive constant dividing outgoing edges node change hence update scheme scaling invariant signiﬁcantly increase figure performance resnet- cifar dataset increases training error loss value test error start decrease simultaneously. note sufﬁciently large training test errors start signiﬁcantly faster wide range stepsizes showing stable training speed. furthermore authors neyshabur showed path regularization converges optimal solutions generalize better compared usual updates apart computational beneﬁts clear statistical generalization advantages too. incorporate path norm constraint? recall remark feasible bounded step well deﬁned. unfortunately case path norm. this consider simple line graph weights case path path norm constraint clearly unbounded. further aware efﬁcient procedure compute projection higher dimensions since known efﬁcient separation oracle. interestingly take advantage fact feasible bounded. intuition generalized update layer time describe precisely. path-cg algorithm order simplify presentation assume biases noting procedure easily extended case individual bias every node. layer vectorized weight matrix layer want update usual corresponds gradient. number nodes j-th layers respectively. edge layers compute intuitively computes norm paths pass edge excluding weight efﬁciently done using dynamic programming time number layers. consequently computation path norm also satisﬁes runtime neyshabur details. observe path norm constraint layers ﬁxed reduces solving following problem remark starting point chosen simply randomly assigning weights normal distribution mean complexity path-cg discussion full algorithm given algorithm main computational complexity path-cg comes computing matrix layer described earlier done backpropagation. hence complexity algorithm running iterations essentially size mini-batch. scale invariance path-cg note algorithms satisfy much general property called afﬁne invariance jaggi implies also scale invariant. scale invariance makes algorithm efﬁcient since avoids exploring functions compute value. present experimental results three different case studies support basic premise theoretical ﬁndings earlier sections constraints easily handled algorithm context deep learning preserving empirical performance models. ﬁrst experiments designed show simple/generic constraints easily incorporated existing deep learning models faster training times better accuracy reducing -layers using resnet architecture. second experiments evaluate pathcg algorithm. goal show path-cg much stable path-sgd algorithm neyshabur implying lower generalization error model. third experiments show gans trained faster using algorithm training tends stable. validate this test performance image inpainting application. since algorithm maintains solution convex combination previous iterates hence decrease effect random initialization training scheme consists phases burn-in phase algorithm constant stepsize; decay phase stepsize decaying according makes sure effect randomness initialization diminished. epoch burn-in phase hence conclude algorithm guaranteed converge stationary pointlacoste-julien figure performance path-cg -layer fully connected network four datasets observe across datasets path-cg much faster last column shows stable respect path norm. start problem image classiﬁcation detection localization. tasks best performing architectures variants deep residual networks purposes analyze performance algorithm used shallower variant resnet namely resnet- architecture trained cifar krizhevsky dataset. resnet- consists residual blocks fully connected input output layers. residual block consists convolution relu batch normalization layers details. cifar dataset contains color images size different categories/labels. hence network contains approximately parameters. make discussion clear present results case total frobenius norm network parameters constrained less trained using algorithm. effect parameters step sizes model iterations figure plots essentially show chosen enough accuracy close accuracy resnet- many parameters interesting property immediate practical case study goal compare path-cg path-sgd algorithm neyshabur terms accuracy stability algorithm. considered image classiﬁcation problem path norm constraint network varying before. train simple feed-forward network consists fully-connected hidden layers units each followed output layer nodes. used relu nonlinearity activation function cross entropy loss neyshabur details. performed experiments standard datasets image classiﬁcation mnist lecun cifar krizhevsky ﬁnally color images house numbers svhn dataset netzer al.. figure shows result achieve accuracy path-sgd. path-cg main advantage path-sgd results supplement pathcg stable path norm path-sgd algorithm increases rapidly. shows path-sgd effectively regularize path norm whereas path-cg keeps path norm less expected. finally illustrate ability framework exciting recent application image inpainting using generative adversarial networks brieﬂy explain overall experimental setup. gans using game theoretic notions deﬁned system neural networks called generator discriminator competing zero-sum game arora image inpainting/completion performed using following steps amos train standard normal image generation task trained generator tune noise gives best output figure hence basic hypothesis generator trained well follow-up task image inpainting beneﬁts automatically. train dc-gan faster better image inpainting used state dc-gan architecture experiments impose frobenius norm constraint parameters discriminator avoid mode collapse issues trained using algorithm. order verify performance algorithm used standard face image datasets celeba conducted experiments trained celeba dataset test dataset vice-versa. results shown figure tuning found generator generates high quality images trained images comparison original dc-gan epochs ﬁgure shows images completed using trained dc-gan look realistic. results indicate trained dc-gan qualitatively performs good better standard dc-gan. experiments showing stability model varying supplement. takeaway gans trained faster change accuracy. conclusions main emphasis work provide evidence supporting three distinct related threads global constraints relevant context training deep models vision machine learning; lack support global constraints existing libraries like keras tensorﬂow abadi complex interplay constraints shown side-stepped great extent using constraints easily incorporated negligible small changes existing implementations. provide range empirical results three different case studies support claims conjecture broad variety problems immediately beneﬁt viewing lens conditional gradient algorithms. analysis experiments suggest concrete ways realize performance improvements generalization runtime substituting schemes certain classes deep learning models. tensorﬂow code experiments made available github.", "year": 2018}