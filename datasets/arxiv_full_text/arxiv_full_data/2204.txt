{"title": "Deep reinforcement learning from human preferences", "tag": ["stat.ML", "cs.AI", "cs.HC", "cs.LG"], "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.", "text": "sophisticated reinforcement learning systems interact usefully real-world environments need communicate complex goals systems. work explore goals deﬁned terms human preferences pairs trajectory segments. show approach effectively solve complex tasks without access reward function including atari games simulated robot locomotion providing feedback less agent’s interactions environment. reduces cost human oversight enough practically applied state-of-the-art systems. demonstrate ﬂexibility approach show successfully train complex novel behaviors hour human time. behaviors environments considerably complex previously learned human feedback. recent success scaling reinforcement learning large problems driven domains well-speciﬁed reward function unfortunately many tasks involve goals complex poorly-deﬁned hard specify. overcoming limitation would greatly expand possible impact deep could increase reach machine learning broadly. example suppose wanted reinforcement learning train robot clean table scramble egg. it’s clear construct suitable reward function need function robot’s sensors. could design simple reward function approximately captures intended behavior often result behavior optimizes reward function without actually satisfying preferences. difﬁculty underlies recent concerns misalignment values objectives systems could successfully communicate actual objectives agents would signiﬁcant step towards addressing concerns. demonstrations desired task extract reward function using inverse reinforcement learning reward function used train agent reinforcement learning. directly imitation learning clone demonstrated behavior. however approaches directly applicable behaviors difﬁcult humans demonstrate alternative approach allow human provide feedback system’s current behavior feedback deﬁne task. principle within paradigm reinforcement learning using human feedback directly reward function prohibitively expensive systems require hundreds thousands hours experience. order practically train deep systems human feedback need decrease amount feedback required several orders magnitude. approach learn reward function human feedback optimize reward function. basic approach considered previously confront challenges involved scaling modern deep demonstrate complex behaviors learned human feedback. summary desire solution sequential decision problems without well-speciﬁed reward function algorithm reward function human’s preferences simultaneously training policy optimize current predicted reward function human compare short video clips agent’s behavior rather supply absolute numerical score. found comparisons easier humans provide domains equally useful learning human preferences. comparing short video clips nearly fast comparing individual states show resulting comparisons signiﬁcantly helpful. moreover show collecting feedback online improves system’s performance prevents exploiting weaknesses learned reward function. experiments take place domains atari games arcade learning environment robotics tasks physics simulator mujoco show small amount feedback non-expert human ranging ﬁfteen minutes hours sufﬁces learn original tasks even reward function observable. consider novel behaviors domain performing backﬂip driving trafﬁc. show algorithm learn behaviors hour feedback—even though unclear hand-engineer reward function would incentivize them. figure schematic illustration approach reward predictor trained asynchronously comparisons trajectory segments agent maximizes predicted reward. long line work studies reinforcement learning human ratings rankings including akrour pilarski akrour wilson sugiyama wirth fürnkranz daniel asri wang wirth lines research considers general problem reinforcement learning preferences rather absolute reward values optimizing using human preferences settings reinforcement learning algorithm follows basic approach akrour akrour consider continuous domains four degrees freedom small discrete domains assume reward linear expectations hand-coded features. instead consider physics tasks dozens degrees freedom atari tasks hand-engineered features; complexity environments force different algorithms reward models cope different algorithmic tradeoffs. notable difference akrour akrour elicit preferences whole trajectories rather short clips. although gather orders magnitude comparisons experiments require less order magnitude human time. differences focus changing training procedure cope nonlinear reward models modern deep example using asynchronous training ensembling. approach feedback elicitation closely follows wilson however wilson assumes reward function distance unknown target policy reward function using bayesian inference rather performing produce trajectories using estimate target policy. experiments involve synthetic human feedback drawn bayesian model perform experiments feedback gathered non-expert users. clear methods wilson extended complex tasks work real human feedback. macglashan pilarski knox stone knox perform experiments involving reinforcement learning actual human feedback although algorithmic approach less similar. macglashan pilarski learning occurs episodes human trainer provides feedback. appears infeasible domains like atari games thousands hours experience required learn high-quality policy would prohibitively expensive even simplest tasks consider. tamer also learn reward function however consider much simpler settings desired policy learned relatively quickly. work could also seen speciﬁc instance cooperative inverse reinforcement learning framework framework considers two-player game human robot interacting environment purpose maximizing human’s reward function. setting human allowed interact game stating preferences. compared prior work contribution scale human feedback deep reinforcement learning learn much complex behaviors. recent trend scaling reward learning methods large deep learning systems example inverse imitation learning semi-supervised skill generalization bootstrapping demonstrations consider agent interacting environment sequence steps; time agent receives observation environment sends action environment. traditional reinforcement learning environment would also supply reward agent’s goal would maximize discounted rewards. instead assuming environment produces reward signal assume human overseer express preferences trajectory segments. trajectory segment sequence observations actions write indicate human preferred trajectory segment trajectory segment informally goal agent produce trajectories preferred human making queries possible human. precisely evaluate algorithms’ behavior ways human’s preferences generated reward function agent ought receive high total reward according know reward function evaluate agent quantitatively. ideally agent achieve reward nearly high using optimize qualitative sometimes reward function quantitatively evaluate behavior cases qualitatively evaluate well agent satisﬁes human’s preferences. paper start goal expressed natural language human evaluate agent’s behavior based well fulﬁlls goal present videos agents attempting fulﬁll goal. model based trajectory segment comparisons similar trajectory preference queries used wilson except don’t assume reset system arbitrary state segments generally begin different states. complicates interpretation human comparisons show algorithm overcomes difﬁculty even human raters understanding algorithm. policy interacts environment produce trajectories parameters updated traditional reinforcement learning algorithm order maximize predicted rewards processes asynchronously trajectories ﬂowing process process human comparisons ﬂowing process process parameters ﬂowing process process following subsections provide details processes. using compute rewards left traditional reinforcement learning problem. solve problem using algorithm appropriate domain. subtlety reward function non-stationary leads prefer methods robust changes reward function. focus policy gradient methods applied successfully problems paper advantage actor-critic play atari games trust region policy optimization perform simulated robotics tasks. here assume reward function observation action. experiments atari environments instead assume reward function preceding observations. general partially observable environment could instead consider reward functions depend whole sequence observations model reward function recurrent neural network. wilson also assumes ability sample reasonable initial states. work high dimensional state spaces random states reachable intended policy inhabits low-dimensional manifold. case used parameter settings found work well traditional tasks. hyperparameter adjusted entropy bonus trpo. trpo relies trust region ensure adequate exploration lead inadequate exploration reward function changing. normalized rewards produced zero mean constant standard deviation. typical preprocessing step particularly appropriate since position rewards underdetermined learning problem. human overseer given visualization trajectory segments form short movie clips. experiments clips seconds long. human indicates segment prefer segments equally good unable compare segments. segments distribution indicating segment user preferred. human selects segment preferable puts mass choice. human marks segments equally preferable uniform. finally human marks segments incomparable comparison included database. interpret reward function estimate preference-predictor view latent factor explaining human’s judgments assume human’s probability preferring segment depends exponentially value latent reward summed length clip follows bradley-terry model estimating score functions pairwise preferences specialization luce-shephard choice rule preferences trajectory segments. understood equating rewards preference ranking scale analogous famous ranking system developed chess difference points chess players estimates probability player defeating game chess difference predicted reward trajectory segments estimates probability chosen human. actual algorithm incorporates number modiﬁcations basic approach early experiments discovered helpful analyzed section fraction data held used validation predictor. regularization adjust regularization coefﬁcient keep validation loss times training loss. domains also apply dropout regularization. rather applying softmax directly described equation assume chance human responds uniformly random. conceptually adjustment needed human raters constant probability making error doesn’t decay difference reward difference becomes extreme. equation discounting could interpreted modeling human indifferent things happen trajectory segment. using explicit discounting inferring human’s discount function would also reasonable choices. decide query preferences based approximation uncertainty reward function estimator similar daniel sample large number pairs trajectory segments length reward predictor ensemble predict segment preferred pair select trajectories predictions highest variance across ensemble members. crude approximation ablation experiments section show tasks actually impairs performance. ideally would want query based expected value information query leave future work explore direction further. ﬁrst experiments attempt solve range benchmark tasks deep without observing true reward. instead agent learns goal task asking human trajectory segments better. goal solve task reasonable amount time using queries possible. experiments feedback provided contractors given sentence description task asked compare several hundred several thousand pairs trajectory segments task trajectory segment seconds long. contractors responded average query seconds experiments involving real human feedback required minutes hours human time. comparison also experiments using synthetic oracle whose preferences trajectories exactly reﬂect reward underlying task. agent queries comparison instead sending query human immediately reply indicating preference whichever trajectory segment actually receives higher reward underlying task. also compare baseline training using real reward. outperform rather nearly well without access reward information instead relying much scarcer feedback. nevertheless note feedback real humans potential outperform human feedback might provide better-shaped reward. describe details experiments appendix including model architectures modiﬁcations environment algorithms used optimize policy. ﬁrst tasks consider eight simulated robotics tasks implemented mujoco included openai made small modiﬁcations tasks order avoid encoding information task environment reward functions tasks linear functions distances positions velocities quadratic function features. included simple cartpole task comparison since representative complexity tasks studied prior work. figure shows results training agent queries human rater compared learning synthetic queries well learning real reward. case atari games sparse rewards relatively common clips zero reward case oracle outputs indifference. considered clips rather individual states ties never made large majority data. moreover ties still provide signiﬁcant information reward predictor long common. figure results mujoco simulated robotics measured tasks’ true reward. compare method using real human feedback method using synthetic feedback provided oracle reinforcement learning using true reward function curves average runs except real human feedback single point average reward consecutive batches. reacher cheetah feedback provided author time constraints. tasks feedback provided contractors unfamiliar environments algorithm. irregular progress hopper contractor deviating typical labeling schedule. labels able nearly match reinforcement learning tasks. training learned reward functions tends less stable higher variance comparable mean performance. surprisingly labels algorithm performs slightly better simply given true reward perhaps learned reward function slightly better shaped—the reward learning procedure assigns positive rewards behaviors typically followed high reward. real human feedback typically slightly less effective synthetic feedback; depending task human feedback ranged half efﬁcient ground truth feedback equally efﬁcient. task human feedback signiﬁcantly outperformed synthetic feedback apparently asked humans prefer trajectories robot standing upright proved useful reward shaping. second tasks consider seven atari games arcade learning environment games presented mnih figure shows results training agent queries human rater compared learning synthetic queries well learning real reward. method difﬁculty matching challenging environments nevertheless displays substantial learning matches even exceeds some. speciﬁcally beamrider pong synthetic labels match come close even labels. seaquest qbert synthetic feedback eventually performs near level learns slowly. spaceinvaders breakout synthetic feedback never matches nevertheless agent improves substantially often passing ﬁrst level spaceinvaders reaching score breakout enough labels. figure results atari games measured tasks’ true reward. compare method using real human feedback method using synthetic feedback provided oracle reinforcement learning using true reward function curves average runs except real human feedback single point average reward consecutive frames. games real human feedback performs similar slightly worse synthetic feedback number labels often comparably synthetic feedback fewer labels. human error labeling inconsistency different contractors labeling uneven rate labeling contractors cause labels overly concentrated narrow parts state space. latter problems could potentially addressed future improvements pipeline outsourcing labels. qbert method fails learn beat ﬁrst level real human feedback; short clips qbert confusing difﬁcult evaluate. finally enduro difﬁcult learn difﬁculty successfully passing cars random exploration correspondingly difﬁcult learn synthetic labels human labelers tend reward progress towards passing cars essentially shaping reward thus outperforming game experiments traditional tasks help understand whether method effective ultimate purpose human interaction solve tasks reward function available. using parameters previous experiments show algorithm learn novel complex behaviors. demonstrate hopper robot performing sequence backﬂips behavior trained using queries less hour. agent learns consistently perform backﬂip land upright repeat. figure performance algorithm mujoco tasks removing various components described section section graphs averaged runs using synthetic labels each. keeping alongside cars enduro. trained roughly queries million frames interaction environment; agent learns stay almost exactly even moving cars substantial fraction episode although gets confused changes background. remove regularization dropout robotics tasks only trajectory segments length rather ﬁtting using comparisons consider oracle provides true total reward trajectory segment total rewards using mean squared error results presented figure mujoco figure atari. particular interest poor performance ofﬂine reward predictor training; nonstationarity occupancy distribution predictor captures part true reward maximizing partial reward lead bizarre behavior undesirable measured true reward instance pong ofﬂine training sometimes leads agent avoid losing points score points; result extremely long volleys repeat sequence events inﬁnitum type behavior demonstrates general human feedback needs intertwined learning rather provided statically. main motivation eliciting comparisons rather absolute scores found much easier humans provide consistent comparisons consistent absolute scores especially continuous control tasks qualitative tasks section nevertheless seems important understand using comparisons affects performance. continuous control tasks found predicting comparisons worked much better predicting scores. likely scale rewards varies substantially complicates regression problem smoothed signiﬁcantly need predict comparisons. atari tasks clipped rewards effectively predicted sign avoiding difﬁculties tasks comparisons targets signiﬁcantly different performance neither consistently outperformed other. also observed large performance differences using single frames rather clips. order obtain results using single frames would need collected signiﬁcantly comparisons. general discovered asking humans compare longer clips signiﬁcantly helpful clip signiﬁcantly less helpful frame. found short clips took human raters understand situation longer clips evaluation time roughly linear function clip length. tried choose shortest clip length evaluation time linear. atari environments also found often easier compare longer clips provide context single frames. agent-environment interactions often radically cheaper human interaction. show learning separate reward model using supervised learning possible reduce interaction complexity roughly orders magnitude. show meaningfully train deep agents human preferences also already hitting diminishing returns sample-complexity improvements cost compute already comparable cost non-expert feedback. although large literature preference elicitation reinforcement learning unknown reward functions provide ﬁrst evidence techniques economically scaled state-of-the-art reinforcement learning systems. represents step towards practical applications deep complex real-world tasks. future work able improve efﬁciency learning human preferences expand range tasks applied. long would desirable make learning task human preferences difﬁcult learning programmatic reward signal ensuring powerful systems applied service complex human values rather low-complexity goals. thank olivier pietquin bilal piot laurent orseau pedro ortega victoria krakovna owain evans andrej karpathy igor mordatch jack clark reading drafts paper. thank tyler adkisson mandy beri jessica richards heather tran contractors providing data used train agents. finally thank openai deepmind providing supportive research environment supporting encouraging collaboration. riad akrour marc schoenauer michèle sebag. april active preference learning-based reinforcement learning. joint european conference machine learning knowledge discovery databases pages marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research eric brochu tyson brochu nando freitas. bayesian interactive optimization approach procedural animation design. proceedings siggraph/eurographics symposium computer animation pages eurographics association atari experiments using virtual machine cpus nvidia costs ~/month gce. training takes compute cost training labels corresponds roughly hours human labour minimum wage totals layla asri bilal piot matthieu geist romain laroche olivier pietquin. score-based inverse reinforcement learning. international conference autonomous agents multiagent systems pages chelsea finn tianhe justin pieter abbeel sergey levine. generalizing skills semi-supervised reinforcement learning. international conference learning representations johannes fürnkranz eyke hüllermeier weiwei cheng sang-hyeun park. preference-based reinforcement learning formal framework policy iteration algorithm. machine learning dylan hadﬁeld-menell stuart russell pieter abbeel anca dragan. cooperative inverse reinforcement learning. advances neural information processing systems pages todd hester matej vecerik olivier pietquin marc lanctot schaul bilal piot andrew sendonaris gabriel dulac-arnold osband john agapiou joel leibo audrunas gruslys. learning demonstrations real world reinforcement learning. arxiv preprint arxiv. bradley knox peter stone. learning non-myopically human-generated reward. jihie jeffrey nichols pedro szekely editors pages isbn ----. http//doi.acm.org/./. james macglashan mark robert loftin peng david roberts matthew taylor michael littman. interactive learning policy-dependent human feedback. arxiv preprint arxiv. machwe parmee. introducing machine learning within interactive evolutionary design environment. proceedings design international design conference dubrovnik croatia volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning pages patrick pilarski michael dawson thomas degris farbod fahimi jason carey richard sutton. online human training myoelectric prosthesis controller actor-critic reinforcement learning. international conference rehabilitation robotics pages john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. international conference machine learning pages jimmy secretan nicholas beato david ambrosio adelein rodriguez adam campbell kenneth stanley. picbreeder evolving pictures collaboratively online. conference human factors computing systems pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot sander dieleman dominik grewe john nham kalchbrenner ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature patrikk sørensen jeppeh olsen sebastian risi. breeding diversity super mario behaviors interactive evolution. computational intelligence games ieee conference pages ieee aaron wilson alan fern prasad tadepalli. bayesian approach policy learning trajectory preference queries. advances neural information processing systems pages christian wirth johannes fürnkranz. preference-based reinforcement learning preliminary ecml/pkdd workshop reinforcement learning generalized feedback many environments termination conditions depend behavior agent ending episode agent dies falls over. found termination conditions encode information task even reward function observable. avoid subtle source supervision could potentially confound attempts learn human preferences only removed variable-length episodes versions robotics tasks episode ends certain parameters outside prescribed range replaced termination conditions penalty encourages parameters remain range atari games send life loss episode signals agent effectively converting environment single continuous episode. providing synthetic oracle feedback replace episode ends penalty games except pong; agent must learn penalty. removing variable length episodes leaves agent information encoded environment itself; human feedback provides guidance ought beginning training compare number trajectory segments drawn rollouts untrained policy. atari domain also pretrain reward predictor epochs beginning training reduce likelihood irreversibly learning policy based untrained predictor. rest training labels rate decaying inversely number timesteps; twice many timesteps elapsed answer half many queries unit time. details schedule described section. label annealing allows balance importance good predictor start need adapt predictor agent learns encounters states. training real human feedback attempt similarly anneal label rate although practice approximate contractors give feedback uneven rates. except otherwise stated ensemble predictors draw factor clip pair candidates ultimately present human presented clips selected maximum variance different predictors described section openai continuous control tasks penalize large torques. torques directly visible human supervisor reward functions good representatives human preferences trajectories removed them. simulated robotics tasks optimize policies using trust region policy optimization discount rate reward predictor twolayer neural network hidden units each using leaky relus nonlinearities. compare trajectory segments last seconds varies timesteps depending task. normalize reward predictions standard deviation learning reward predictor entropy bonus tasks except swimmer entropy bonus noted section entropy bonus helps incentivize increased exploration needed deal changing reward function. collect comparisons randomly initialized policy network beginning training rate labeling frames reward functions second degree polynomials input features concerned tasks could take simpler approach learning reward function. however using ﬂexible architecture allows immediately generalize tasks reward function simple described section atari agents trained using standard environment wrappers used mnih no-ops beginning episode max-pooling adjacent frames stacking frames frameskip life loss ending episode rewards clipped atari games include visual display score theory could used trivially infer reward. since want focus instead inferring reward complex dynamics happening game replace score area constant black background seven games. beamrider additionally blank enemy ship count enduro blank speedometer. atari tasks optimize policies using algorithm synchronous form policy architecture described mnih standard settings hyperparameters entropy bonus learning rate decayed linearly reach zero million timesteps steps update parallel workers discount rate policy gradient using adam reward predictor images inputs stack frames total input tensor. input convolutional layers size strides ﬁlters leaky relu nonlinearities followed fully connected layer size scalar output. convolutional layers batch norm dropout prevent predictor overﬁtting. addition regularization adapative scheme described section since reward predictor ultimately used compare sums timesteps scale arbitrary normalize standard deviation compare trajectory segments timesteps collect comparisons randomly initialized policy network beginning training rate labeling frames training decreased every frames roughly proportional predictor trained asynchronously agent hardware typically processes label timesteps. maintain buffer last labels loop buffer continuously; ensure predictor gives enough weight labels total number labels becomes large. ablation studies figure pretraining labels rather target beamrider curve averaged runs rather sign slot spreadsheet. appropriate url’s give you’ll repeatedly presented video clips controlling virtual robot. look clips select better things happen. decide events actually witness clip. here’s guide constitutes good behavior speciﬁc domain cheetah robot move right fast possible. ﬁrst priority robot standing upright failing center robot high possible. robots upright neither breaker whichever moving faster right. pendulum pendulum pointing approximately ties pendulum fallen can’t tells side screen. pendulum hasn’t fallen down that’s better unable pendulum. clips look click tie. don’t understand what’s going clip hard evaluate click can’t tell. speed feedback using arrow keys left right select clips can’t tell. error saying we’re clips. what’s occasionally server clips give you’ll error message. normal wait minute refresh page. don’t clips couple minutes please ping slack. need start right time listed spreadsheet? starting minutes listed time ﬁne. providing feedback play game minutes sense works. it’s often hard tell game looking short clips especially you’ve never played before. play game online minutes. you’ll need press click game reset button start game. timer minutes explore game works. sign slot spreadsheet. appropriate url’s give you’ll repeatedly presented video clips playing game. look clips select better things happen. example left clip shows shooting enemy ship right clip shows shot enemy ship better things happen left clip thus left clip better. decide actions actually witness clip. don’t worry agent situation focus happens clip itself. clips look click tie. don’t understand what’s going clip hard evaluate click can’t tell. minimize responding can’t tell unless truly confused. speed feedback using arrow keys left right select clips can’t tell. error saying we’re clips. what’s occasionally server clips give you’ll error message. normal wait minute refresh page. don’t clips couple minutes please ping slack. agent already dead clip starts compare clip getting killed performance clip neither good bad. treat purely average play. it’s possible contains frame dying it’s deﬁnitely bad. need start right time listed spreadsheet? starting minutes listed time ﬁne.", "year": 2017}