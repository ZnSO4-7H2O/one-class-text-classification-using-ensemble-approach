{"title": "Coupled Ensembles of Neural Networks", "tag": ["cs.CV", "stat.ML"], "abstract": "We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters without losing performance or to significantly improve the performance with the same level of performance. The use of branches brings an additional form of regularization. In addition to the split into parallel branches, we propose a tighter coupling of these branches by placing the \"fuse (averaging) layer\" before the Log-Likelihood and SoftMax layers during training. This gives another significant performance improvement, the tighter coupling favouring the learning of better representations, even at the level of the individual branches. We refer to this branched architecture as \"coupled ensembles\". The approach is very generic and can be applied with almost any DCNN architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same budget, DenseNet-BC has error rate of 3.46%, 17.18%, and 1.8% respectively. With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.", "text": "investigate paper architecture deep convolutional networks. building existing state models propose reconﬁguration model parameters several parallel branches global network level branch standalone cnn. show arrangement efﬁcient signiﬁcantly reduce number parameters without losing performance signiﬁcantly improve performance level performance. branches brings additional form regularization. addition split parallel branches propose tighter coupling branches placing fuse layer log-likelihood softmax layers training. gives another signiﬁcant performance improvement tighter coupling favouring learning better representations even level individual branches. refer branched architecture coupled ensembles. approach generic applied almost dcnn architecture. coupled ensembles densenet-bc parameter budget obtain error rates respectively cifar- cifar- svhn tasks. budget densenet-bc error rate respectively. ensembles coupled ensembles densenet-bc networks total parameters obtain error rates respectively tasks. design early convolutional architectures involved choices hyper-parameters ﬁlter size number ﬁlters layer padding since introduction vggnet design moved towards following template ﬁxed ﬁlter size features maps down-sample half input resolution either maxpool strided convolutions double number computed feature maps following down-sampling operation. philosophy used state models resnet densenet last architectures extended template include skip-connections non-contiguous layers. work extends template adding another element refer coupled ensembling. set-up network decomposed several branches branch functionally similar complete proposed template achieves performance comparable state models signiﬁcantly lower parameter count. proposed modiﬁcation paper make following contributions show given parameter budjet splitting large network ensemble smaller parallel branches type jointly training performs better par; ﬁnal softmax layer used prediction step show ensemble fusion works better averaging done layer done after; ﬁnal log-likelihood layer used training step show ensemble fusion branches works better fusion done layer done after; combining elements signiﬁcantly improved performance and/or signiﬁcantly reduce parameter count state-of-the-art neural network architectures cifar svhn data sets. show multi-branch networks ensembled higher level still producing signiﬁcant performance gain. paper organised follows section discuss related work; section introduce concept coupled ensembles motivation behind idea; section present evaluation proposed approach compare state art; conclude discuss future work section multi-column architectures. network architecture propose strong similarities cires¸an’s neural networks committees multi-column deep neural network type ensemble networks committee members columns correspond basic block instances however coupled ensemble networks differ following train single model composed sub-networks train member column separately. consider coupled ensemble smaller networks improving performance network given budget parameters entire model. contrary improving utilising multiple models ﬁxed size therefore multiplying overall size consider placing averaging layer completely network also log-likelihood layer training soft-max layer training and/or prediction; used preprocessing branches considered different preprocessing blocks different members different subsets columns; consider also future. multi-branch architectures. multi-branch architectures successful several vision applications recently modiﬁcations proposed architectures using concept grouped convolutions order factorize spatial feature extraction channel features. modiﬁcations additionally advocate template building blocks stacked together form complete model. modiﬁcation level building blocks corresponding base architectures resnet inception respectively. contrast propose generic modiﬁcation structure global model level. includes template speciﬁc architecture basic block speciﬁed basic block replicated parallel branches form ﬁnal composite model. improve performance architectures shake-shake regularization proposes stochastic mixture branches achieved good results cifar datasets. however number epochs required convergence much higher compared base model. additionally technique seems depend batch size. contrast apply method using exact hyper-parameters used underlying cnn. zhao investigate usage parallel paths resnet connecting layers allow information exchange paths. however requires modiﬁcation local level residual blocks. contrast method generic rearrangement given architecture’s parameters introduce additional choices. additionally empirically conﬁrm proposed conﬁguration leads efﬁcient usage parameters. neural network ensembles. ensembling reliable technique increase performance models task. presence several local minima multiple trainings exact neural network architecture reach different distribution errors per-class basis. hence combining outputs lead improved performance overall task. observed early commonly used obtaining results classiﬁcation challenges despite increase training prediction cost. proposed model architecture ensemble independent networks given single model made parallel branches trained. similar spirit residual block resnet resnext inception module inception done full network level. would like emphasize arranging given budget parameters parallel branches leads increase performance additionally classical ensembling approach still applied fusion independently trained coupled ensemble networks produces signiﬁcant performance improvement following discussion deﬁne terms branch proposed model comprises several branches. branch takes input data point produces score vector corresponding target classes. current design cnns referred single-branch. number branches denoted operation used combine parallel branches make model. experiments branches combined taking average predicted probabilities target classes. section explores different choices operations fuse layer. consider classiﬁcation task individual samples always associated exactly class labelled ﬁnite set. case cifar svhn ilsvrc tasks. theory work tasks well consider neural network models whose last layer outputs score vector dimension number target classes. usually implemented linear layer referred fully connected layer. layer followed softmax layer produce probability distribution target classes. training followed loss layer example negative log-likelihood case recent network architectures image classiﬁcation differences among related present last layer. agnostic internal setup resulting basic block always takes image input produces vector values output parametrized tensor case ensemble predictions fusion usually done computing individual predictions separately model instances averaging them. instances trained seperately. functionally equivalent predicting super-network including instances parallel branches ﬁnal averaging layer top. super-networks generally implemented branch instances often already correspond maximum memory capacity gpus. remaining layer operation implemented separately. alternatively possible place averaging layer last layer basic block instances layer factorized. though networks actually layer last layer need linear layer long produces value target label. refer output basic block similarly proposed method easily adapted multi-label classiﬁcation simply replacing layer variant appropriate multi-label classiﬁcation. again refer output setup model composed parallel branches branch produces score vector target categories. explore three options combine vectors training activation average. probability average likelihood average transformation multiple branches combining branch activations averaging probabilites predict target categories leads performance improvement lower parameter count experiments parameter vector composite branched model concatenation parameter vectors basic blocks parameters basic blocks fuse layer contain parameters. model instances really need share architecture. three training versions considered depending upon whether fuse layer placed layer. instances trained simultaneously trough single loss function parameter vector obtained single training phase. practice ﬁrst third versions layers replaced single cross-entropy layer numerically stable. similarly second intermediate case logsoftmax layer used instead regular layer arithmetic averaging equivalent replacing arithmetic average geometric regular layers. training done four different ways three coupled ways correspoding fuse layer plus basic blocks trained separately independent trainings. regardless training basic blocks performed possible coupled ways making ‘coupled prediction’ also possible individual basic blocks making ‘individual predictions’. combinations expected consistent equally efﬁcient implemented evaluated shown section evaluate proposed architecture cifar svhn data sets. cifar- cifar- consist training images test images distributed categories respectively. svhn consists training images easy training images testing images distributed categories. image datasets size pixels. cifar- cifar- svhn input image normalised subtracting mean image dividing standard deviation. training cifar datasets standard data augmentation used comprises random horizontal ﬂips random crops. svhn data augmentation used. however dropout ratio applied case densenet training svhn. testing done normalising input training. error rates given percentages correspond average computed last epochs cifar last epochs svhn. measure conservative used densenet authors densenet-bc amos kolter pytorch implementation used. execution times measured using single nvidia optimal micro-batch. experiments section done cifar- data basic block densenet-bc depth growth rate experiments section consider conﬁguration baseline reference point. natural point comparison proposed branched architecture ensemble independent models. rows table present results cases respectively. shows error rate obtained averaging predictions identical models trained separately. even though total number trainable parameters involved exactly same jointly trained branched conﬁguration gives much lower test error next point comparison single branch model comprising similar number parameters multi branch conﬁguration. choice single branch models done increasing keeping constant increasing increasing keeping constant. last three rows table show error multi branch model considerably lower compared single branch model observations show arranging given budget parameters parallel branches efﬁcient terms parameters compared large single branch multiple independent trainings. section analyse relation number branches model performance. section compare performance proposed branched model different choices positioning fuse layer experiments carried evaluate best training prediction fusion combinations. consider branched models trained following conditions training fusion layer layer layer. table shows performance differently trained systems different prediction conﬁgurations individual average performance trained instances performance ensemble system fusion layer layer. note table includes models parameters obtained using different training methods make following observations avg. training separate predictions work well. expected since similar average reached quite unrelated instances. avg. training avg. prediction works better still good non-linearity layer distorts average. indeed consistent avg. training avg. prediction works quite well though yield best performance. avg. prediction works least well often signiﬁcantly better avg. prediction whatever training choice explained fact layer compresses values probabilities close values remain spread transmit information layer even different training conditions. average error rate basic blocks trained jointly coupled ensembles fusion signiﬁcantly lower error rate individual instances trained separately. indicates coupling forces learn complementary features group also better learn individually. averaging probabilities forces network continuously update branches consistent other. provides stronger gradient signal. additionally training loss remains higher compared single branch models serving regularizer. error gradient back-propagated fuse layer table coupled ensembles densenet-bcs versus single model comparable complexity study training prediction fusion combinations. bottom performance given top- error rate cifar- data standard data augmentation. columns indicate densenet-bc hyperparameter values basic block. column indicates number branches. column avg. indicates type fuse layer training none separate trainings fusion layers respectively column individual gives performance individual basic blocks evaluated separately; columns give performance fusion prediction done layers respectively. last three columns give total number parameters model duration training iteration prediction time test image supplementary material section average standard deviations computed independent trainings ensemble combinations except based avg. training signiﬁcantly better single network comparable size depth. global network size parameters error rate decreases best single instance combination instances four instances). section investigate optimal number branches given model parameter budget. evaluate cifar- densenet-bc basic block parameter budget equal parameters indeed optimal number instances likely depend upon network architecture upon parameter budget upon data gives least reference. investigated larger models results table table different number branches ﬁxed parameter count. models trained cifar- standard data augmentation. table caption meaning column labels. applicable fuse layer avg. average standard deviation trials different seeds; huang reports supplementary material section table shows performance different conﬁgurations branches depth growth rate difﬁculty densenet-bc parameter counts strongly quantiﬁed according values additionally value coupled ensemble version. even critical moderate size models like targeted here. selected model conﬁgurations parameters target making fair comparison. models slightly parameters interpolation done possibly accurate comparisons. make following observations considered case optimal number branches parameters error rates decreases regular densenet-bc model using branches yields signiﬁcant performance gain classical case even original performance reported densenet-bc using branches performs signiﬁcantly less well. slightly varying hyper-parameters around optimal value lead signiﬁcant performance drop showing coupled ensemble approach densenet-bc architecture quite robust relatively choices. gain performance comes expense increased training prediction times even though model size change. smaller values prevents good parallelism efﬁciency. increase relatively smaller bigger networks. experiment done validation random split cifar training could draw conclusions there; predict combination best test set. combination appears slightly better difference probably statistically signiﬁcant. evaluated coupled ensemble network approach networks various sizes. used huang densenet-bc architecture basic block since current state close time started experiments. used amos kolter pytorch densenet-bc implementation multi-branch single-branch experiments. also evaluated approach using resnet pre-activation basic block check coupled ensemble approach works well architectures. table reports upper part results obtained current best systems results obtained coupled ensembles approach lower part. results presented table correspond training single possibly network. even ensembles considered always coupled described section trained single global network. level ensembling involving multiple trainings considered section results presented cifar data standard data augmentation svhn using extra training data. system resnet resnet stochastic depth resnet stochastic depth resnet pre-activation resnet pre-activation densenet densenet-bc densenet-bc densenet-bc shake-shake model s-s-i shake-shake model s-e-i snapshot ensemble densenet- snapshot ensemble densenet- snapshot ensemble densenet- sgdr wrn-- loshchilov hutter sgdr wrn-- snapshots resnext- resnext- dfn-mr zhao dfn-mr zhao igc-lm zhang igc-lm zhang resnet pre-activation resnet pre-activation resnet pre-activation densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc densenet-bc resnet pre-act architecture ensemble versions branches leads signiﬁcantly better performance single branch versions comparable higher number parameters. regarding densenet-bc architecture considered different network sizes roughly following multiples powers ranging parameters extremes corresponding error rates available huang chose values depth growth rate points interpolated according scale much possible. experiments showed trade-off critical given overall parameter count. also case choosing number branches depth growth rate given overall parameter count budget long conﬁgurations experimented single-branch multi-branch versions model additionally largest model tried branches. single branch version extreme network sizes obtained error rates signiﬁcantly lower reported huang checked torch implementation used pytorch used equivalent. difference fact used conservative measure error rate statistical differences different initializations and/or non-deterministic computations still coupled ensemble version leads signiﬁcantly better performance network sizes even compared densenet-bc’s reported performance. larger models coupled densenet-bcs perform better current state implementations aware time publication work. shake-shake s-s-i model performs slightly better cifar also compare performance coupled ensembles model architectures ‘learnt’ meta learning scenario. results presented supplementary material section coupled ensemble approach limited size network memory training time training takes. hardware access possible much beyond m-parameter scale. going further resorted classical ensembling approach based independent trainings. interesting question whether could still signiﬁcantly improve performance since classical approach generally plateaus quite small number models coupled ensemble approach already include several. instance sgdr snapshots approach signiﬁcant improvement models much improvement models multiple times training quite costly models large instead ensembled four large coupled ensemble models trained four values results shown table obtained signiﬁcant gain fusing models quite small fusion three four them. best knowledge ensembles coupled ensemble networks outperform state implementations including ensemble-based ones time publication work. table classiﬁcation error comparison state multiple model trainings. system svhn params sgdr wrn-- runs snapshots sgdr wrn-- runs snapshots densenet-bc ensemble ensembles densenet-bc ensemble ensembles densenet-bc ensemble ensembles figure shows given parameter count coupled ensemble networks ensembles coupled ensemble networks perform signiﬁcantly better parameter budgets. approach shown blue diamonds. figure comparison parameter usage different methods. results cifar- cifar- svhn tasks. densenet-bc single-branch conﬁguration; ours proposed multiple branch conﬁguration using densenet-bc basic block single training parameters ensembles coupled ensembles beyond; sgdr-wrn snapshot ensembles sgdr wide resnets parameters ensembles snapshot ensembles beyond; other architectures mentioned tables proposed approach consists replacing single deep convolutional network number basic blocks resemble standalone models. intermediate score vectors produced basics blocks coupled fuse layer. training time done taking arithmetic average log-probabilites targets. test time score vectors averaged following output score vector. aspects leads signiﬁcant performance improvement single branch conﬁguration. improvement comes cost small increase training prediction times. proposed approach leads best performance given parameter budget seen tables ﬁgure increase training prediction times mostly sequential processing branches forward backward passes. smaller size branches makes data parallelism gpus less efﬁcient. effect pronounced larger models. could solved ways. first data dependency branches possible extend data parallelism branches restoring initial level parallelism. done implementing parallel implementation multiples convolutions time. second alternatively multiple gpus used possible spread branches gpus. currently evaluated coupled ensemble approach relatively small data sets. therefore plan conduct experiments imagenet check whether work equally well large collections. work partially supported labex persyval-lab experiments presented paper partly carried using grid’ test-bed supported scientiﬁc interest group hosted inria including cnrs renater several universities well organizations cires¸an meier gambardella schmidhuber convolutional neural proceedings innetwork committees handwritten character classiﬁcation. ternational conference document analysis recognition icdar pages washington usa. krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei imagenet large scale visual recognition challenge. international journal computer vision szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages figure shows common structure test train versions networks used basic blocks. figure shows possible place averaging layer last layer basic block instances layer factorized. model instances need share architecture. figure shows possible place averaging layer last layer equivalent geometric mean values) layer layer. reuse basic blocks groups original form much possible efﬁciency ensuring meaningful comparisons. available pytorch implementations. branches deﬁned parameter vector containing parameters original implementation. global network deﬁned parameter vector concatenation parameter vectors. training done coupled mode prediction done separate mode vice-versa dedicated script used splitting vector ones vice-versa. coupled networks train versions test version global parameter vector used split deﬁning basic block functions. combine four possible training conditions three possible prediction conditions even though consistent equally efﬁcient. overall network architecture determined global hyper-parameter specifying train versus test mode; global hyper-parameter specifying number branches; global hyper-parameter specifying layer layer placed average denotes number categories. element referenced averaging activations branch. average denotes averaging across branches logsoftmax operation applied branch activation vector separately. hence average scoresc average score vector translated version average score vector. also arithmetic average logsoftmax values equivalent geometric average softmax values. holds inference interested maximum value. attempting compare relative performance different methods face issue reproducibility experiments statistical signiﬁcance observed difference between performance measures. even experiment identiﬁed following sources variation performance measure underlying framework implementation made experiments torch random seed network initialization. cudnn non-determinism training associative operations default fast non-deterministic. observed results varies even tool seed. practice observed variation important changing seed. fluctuations associated computed moving average standard deviation batch normalization ﬂuctuations observed even training learning rate momentum weight decay last epochs training level inﬂuence default value hyper-parameters. regardless implementation numerical determinism batch norm moving average epoch sampling questions still expect dispersion evaluation measure according choice random initialization since different random seeds likely lead different local minima. generally considered local minima obtained properly designed trained neural networks similar performance observe relatively small dispersion conﬁrming hypothesis. dispersion small negligible complicates comparisons methods since differences measures lower dispersions likely nonsigniﬁcant. classical statistical signiﬁcance tests help much since differences statistically signiﬁcant sense observed models obtained different seeds everything else kept equal. experiments reported section gives estimation dispersion particular case moderate scale model. generally cannot afford large number trials larger models. tried quantify relative importance different effects particular case densenetbc cifar table shows results obtained experiment four groups three rows. tried four combinations corresponding torch versus pytorch seed versus different seeds. conﬁguration used performance measure error rate model computed last epoch average error rate models computed last epochs error rate model lowest error rate epochs. cases present minimum median maximum mean±standard deviation measures corresponding identical runs additionally case average error rate models computed last epochs present root mean square standard deviation ﬂuctuations last epochs make following observations seem signiﬁcant difference torch pytorch implementations; seem signiﬁcant difference using seed using different seeds; dispersion observed using seed implies exactly reproduce results; seem signiﬁcant difference means measures computed single last epoch means measures computed last epochs; standard deviation measures computed runs slightly consistently smaller measures computed last epochs computed single last epoch; difference best worst measures; expected since averaging measure last epochs reduces ﬂuctuations moving average standard deviation computed batch normalization possibly random ﬂuctuations ﬁnal learning steps; mean measures computed runs signiﬁcantly lower measure taken best epoch computed either single last epoch last epochs. expected since minimum always average. however presenting measure involves using test data selecting best model. following observations propose method ensuring best reproducibility fairest comparisons. choosing measure minimum error rate models computed training seems neither realistic good practice since know model best without looking results like tuning test set. even though necessarily unfair system comparison measures done condition systems introduce bias absolute performance estimation. using error rate last iteration last iteration seem make difference mean standard deviation smaller latter therefore preferred single experiment conducted. also checked using last epochs make much difference value different used critical. cifar experiments reported paper used average error rate models obtained last epochs robust conservative. case svhn experiments slightly different since much smaller number much bigger epochs; used last iterations case. results) likely general. table also shows results coupled ensemble network comparable size coupled ensemble network four times bigger. similar observations made additionally observe range standard deviations smaller. might averaging already made branches leading reduction variance. though requires conﬁrmation larger scales coupled ensemble networks might lead better stable performance.", "year": 2017}