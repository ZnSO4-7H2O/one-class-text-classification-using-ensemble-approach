{"title": "Scaling Up Estimation of Distribution Algorithms For Continuous  Optimization", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Since Estimation of Distribution Algorithms (EDA) were proposed, many attempts have been made to improve EDAs' performance in the context of global optimization. So far, the studies or applications of multivariate probabilistic model based continuous EDAs are still restricted to rather low dimensional problems (smaller than 100D). Traditional EDAs have difficulties in solving higher dimensional problems because of the curse of dimensionality and their rapidly increasing computational cost. However, scaling up continuous EDAs for higher dimensional optimization is still necessary, which is supported by the distinctive feature of EDAs: Because a probabilistic model is explicitly estimated, from the learnt model one can discover useful properties or features of the problem. Besides obtaining a good solution, understanding of the problem structure can be of great benefit, especially for black box optimization. We propose a novel EDA framework with Model Complexity Control (EDA-MCC) to scale up EDAs. By using Weakly dependent variable Identification (WI) and Subspace Modeling (SM), EDA-MCC shows significantly better performance than traditional EDAs on high dimensional problems. Moreover, the computational cost and the requirement of large population sizes can be reduced in EDA-MCC. In addition to being able to find a good solution, EDA-MCC can also produce a useful problem structure characterization. EDA-MCC is the first successful instance of multivariate model based EDAs that can be effectively applied a general class of up to 500D problems. It also outperforms some newly developed algorithms designed specifically for large scale optimization. In order to understand the strength and weakness of EDA-MCC, we have carried out extensive computational studies of EDA-MCC. Our results have revealed when EDA-MCC is likely to outperform others on what kind of benchmark functions.", "text": "model presents extracted global statistical information search space. uses model guidance reproduction better solutions. actually underlying model presenting sampling mechanism. traditional underlying model usually implicitly expressed evolutionary operators. model explicitly presented algorithm classiﬁed instance eda. edas proposed originally combinatorial optimization. research edas extended discrete domain continuous optimization much progress made. paper focus edas single objective continuous optimization domain. many studies continuous done last decade. general major branches continuous edas. based gaussian distribution model widely used intensively studied another major branch based histogram models however existing studies common problem performance validated dimensional problems performance higher dimensional problems rarely studied. following sections reason researchers simply ignored issue continuous edas difﬁculties high dimensional search space. relying learning model samples edas suffer well-known curse dimensionality considering multi-dependency structure variables solve non-separable problems effectively traditional edas’ fast increasing computational cost also makes impractical real-world applications. paper propose novel framework model complexity control named eda-mcc scale continuous optimization. adopting weakly dependent variable identiﬁcation subspace modeling eda-mcc restrict model complexity necessary level make eda-mcc less suffer curse dimensionality. furthermore also suppress increasing demand population size reduce overall computational cost terms time. experimental comparisons well-known benchmark functions validate effectiveness efﬁciency edamcc. eda-mcc signiﬁcant advantages traditional edas solving high dimensional nonseparable problems local optima terms solution quality computational cost. signiﬁcant difference eda-mcc traditional edas model complexity penalization abstract—since estimation distribution algorithms proposed many attempts made improve edas’ performance context global optimization. studies applications multivariate probabilistic model based continuous edas still restricted rather dimensional problems traditional edas difﬁculties solving higher dimensional problems curse dimensionality rapidly increasing computational cost. however scaling continuous edas higher dimensional optimization still necessary supported distinctive feature edas probabilistic model explicitly estimated learnt model discover useful properties features problem. besides obtaining good solution understanding problem structure great beneﬁt especially black optimization. propose novel framework model complexity control scale edas. using weakly dependent variable identiﬁcation subspace modeling eda-mcc shows signiﬁcantly better performance traditional edas high dimensional problems. moreover computational cost requirement large population sizes reduced eda-mcc. addition able good solution eda-mcc also produce useful problem structure characterization. eda-mcc ﬁrst successful instance multivariate model based edas effectively applied general class problems. also outperforms newly developed algorithms designed speciﬁcally large scale optimization. order understand strength weakness eda-mcc carried extensive computational studies eda-mcc. results revealed eda-mcc likely outperform others kind benchmark functions. intensively studied context global optimization. compared traditional evolutionary algorithms genetic algorithms neither crossover mutation operator eda. instead explicitly builds probabilistic model promising solutions search space. solutions sampled dong research china beijing china. part work done candidate laboratory complex systems intelligence science institute automation chinese academy sciences beijing china traditional appropriate high dimensional optimization still strive scale motivation based distinctive advantage applying compared users discover identify useful properties/features problem learnt probabilistic model. since model explicitly built always possible observe learnt model structure parameters. simple univariate model based edas interdependencies among variables simply ignored possible reveal deeper level information represents problem structure variable dependencies. however multivariate model based edas potentials. eda-mcc multi-dependency adopted degree model complexity explicitly controlled. eda-mcc ﬁrst attempt scaling multivariate model based high dimensional continuous optimization. clear difference eda-mcc previously developed edas model complexity penalization strategy shown following sections. remainder paper organized follows. section analyze difﬁculties traditional edas high dimensional problems especially gaussian based edas. section present eda-mcc gaussian model adopted. difference eda-mcc traditional edas model complexity penalization also discussed. experimental studies problems given section section dependence eda-mcc parameters investigated. section random partitioning based compared clustering based advantage random partitioning high dimensional optimization veriﬁed discussed. problem property characterization ability eda-mcc shown section vii. section viii analyze respective mutual effects ﬁnal conclusions drawn section along future work. primary difference different edas probabilistic model used. adopting gaussian distribution model fig. form normal density deﬁned mean vector covariance matrix earliest proposed gaussian based edas based simple univariate gaussian umdag pbilc edas variables regarded independent other. simplicity models makes easy implement algorithms characterized level computational cost. also simplicity difﬁculties solving problems whose variables strong interdependencies. remedy this several edas based multivariate gaussian initialize population generating individuals randomly. repeat stopping criterion met. select individuals estimate probability density function sample number individuals combine create proposed emnaglobal normal idea egna emnaglobaladopts conventional maximum likelihood estimated multivariate gaussian distribution represented normal idea egna obtaining maximum likelihood estimation graphical factorization bayesian factorization constructed usually local search greedy search. constructing graphical factorization introduces additional computation along maximum likelihood estimation computational time solution sampling procedure reduced. hand want sample solutions conventional multivariate gaussian distribution emnaglobal decomposing must since edas essentially based multivariate gaussian distribution performances similar. least signiﬁcant superiority another reported later extensions edas proposed improve poor explorative ability eeda ct-avs-idea sdr-avsidea edas scale according criterion maximum likelihood estimation. comparative study different covariance matrix scaling strategies found besides single gaussian based edas edas adopting gaussian mixture distribution proposed solving multimodal problems. hybrid continuous optimization algorithms using gaussian based edas also proposed. interestingly previous studies shown although gaussian models cannot always offer accurate estimation true distribution promising solutions nevertheless offer useful information guiding global search many unimodal some multimodal problems. satisfactory explanation phenomenon presented literature. interesting future study multimodal problem easy hard given single gaussian based e.g. using recently proposed analytical approach however except univariate gaussian based edas existing studies multivariate gaussian based edas restricted dimensional problems edas based univariate histogram based multivariate histogram histogram models ﬂexible gaussian models convenience describe arbitrary multimodality. however considering multiple variable dependencies full interdependency required number bins increase exponentially problem size makes multivariate histogram models hard applied high dimensional problems practice. although efforts made improve scalability multivariate histogram model based edas existing results edas also restricted dimensional problems even lower multivariate gaussian based edas. best knowledge three attempts studying continuous large scale problems univariate model based lseda-gl proposed wang application umdag egna logistic regression regularizers large small microarray classiﬁcation problem proposed bielza study parallel implementation egnaee sphere function proposed mendiburu however attempts limitations. lseda-gl adopts univariate model mixed gaussian l´evy distribution. discussed before lacks ability describe reﬂect problem structure. hand multivariate utilized parameter optimizer logistic regression model parameters trained constrained maximum likelihood. parameters constrained certain intervals effectively regularizing model. however general performance multivariate broader types high-dimensional problems still unknown. study focuses parallel multivariate eda’s performance terms speed execution time solution quality test function involved experiment. word open important question expect promising performance multivariate model based edas high dimensional optimization problems? since edas completely rely probabilistic models built ﬁnite data samples must suffer well-known curse dimensionality ﬂexible complex model data requires yield reliable estimation sustain enough good performance. according curse dimensionality theory amount data sustain given spatial density increases exponentially dimensionality search space. adversely impact method based spatial density unless data follows certain simple distributions. obviously latter condition always satisﬁed practice. population size grow fast problem size grows sustain good performance. since tries learn global statistical information sampled data sufﬁciently large also requires large population size level selection pressure needs maintained. course demand increasing population size different levels models different levels complexity. simple univariate model based edas solving dimensional problem estimates dimensional distributions independently. population size large enough estimating distributions ﬁnding good enough solution necessarily grow grows. however multivariate models degrees freedom make usually require larger population sizes validated experiments. dimensions problems high traditional edas complex multivariate models become inapplicable since large population size consume considerable computational resources. urgent need techniques reduce required computational resources without affecting precisions learning probabilistic models. since previous results show gaussian models less affected curse dimensionality histogram models reasonable usually gaussian models less degrees freedom histogram models single gaussian models less degrees freedom gaussian mixture models following sections focus using single multivariate gaussian models scale eda. univariate gaussian models also involved analysis experiments. however noticed conclusions generalized restricted gaussian models. although previous research shown single gaussian model based edas perform well many unimodal multimodal problems still known limitations effect curse dimensionality. speciﬁcally edas using maximum likelihood estimated gaussian supposed poor explorative ability. theoretical proved maximal analysis umdag distance mean population move across search space bounded algorithm guaranteed converge since population variance converges zero. although theoretical analysis developed similar results multivariate gaussian based edas using maximum likelihood estimation also observed experimental studies improve explorative ability several gaussian based edas covariance matrix scaling thus proposed. effectiveness techniques high dimensional search space still lacks validation. besides curse dimensionality computational cost also restrict application high dimensional optimization. exclude ﬁtness evaluation model building subsequent solution sampling steps determine overall computational cost also related model complexity. general univariate model based edas level computational cost. however applied high dimensional problems even population size sufﬁciently large multivariate edas difﬁculties terms rapidly increasing computational cost steps. even problems whose ﬁtness function evaluation time-consuming multivariate model based edas’ overall runtime become unacceptable practice. concentrate computational cost brought model within generation. give analytical computational complexity terms data access representative edas different model complexities univariate gaussian based umdag multivariate gaussian based emnaglobal suppose current model built selected individuals last generation. denotes population size denotes number selected individuals usually computational complexities emnaglobal shown table detailed umdag steps computation please appendix mentioned above univariate model sufﬁcient solving problem necessarily need grow grows. table shows univariate model based edas umdag overall computational cost grows linearly although model’s simplicity restricts performance computational cost grows mildly. hand multivariate gaussian based edas emnaglobal overall cost grows much faster. although reported necessary grows approximately normal idea practice usually true overall computational cost typical multivariate gaussian based thus grows least following experimental studies illustrative comparisons time made. univariate gaussian based edas shares model structure differ model parameters updated. edas share level computational complexity. however different multivariate gaussian based edas different computational complexity. mentioned above emnaglobalestimates model maximum likelihood estimation sampling solutions decomposition covariance matrix. normal idea egna build graphical factorization maximum likelihood estimation parameters factorization sample solutions traversing graphical structure. maximum likelihood estimation step three exactly same thus share computational complexity step. latter steps emnaglobal’s computational complexity easy analyze since decomposing covariance matrix constantly costs cubic time problem size. whereas graphical factorization normal idea egna obtained several different structure search algorithms whose computational complexity relevant speciﬁc algorithms used current state data. obtaining structure normal idea conditional variances factorization computed inverse covariance matrix costs computational complexity decomposing covariance matrix. normal idea’s computational complexity deﬁnitely higher emnaglobal. egna parameters gaussian network computed different manner making analytical calculation computational cost difﬁcult. previous literature egna offer analytical results computational complexity either. also considering fact multivariate gaussian based edas covariance matrix scaling additional computation choose emnaglobalas representative multivariate gaussian based edas analyze computational complexity. analysis recalling differences performance computational complexity univariate gaussian multivariate gaussian easily related gaussian model complexity. roughly speaking univariate gaussian simple structure cheap computational cost difﬁculty solve non-separable problems. multivariate gaussian complex structure thus expensive computational cost solve non-separable problems effectively. explicitly control model complexity according criterion combine advantages together. propose novel control gaussian model complexity steps weakly dependent variable identiﬁcation subspace modeling resulting algorithm called eda-mcc corr linear correlation coefﬁcient between standard deviations respectively according deﬁnition correlation coefﬁcient cannot exceed absolute value. thus correlation coefﬁcients also seen normalized covariances. suppose evolution process multivariate gaussian based generation correlation coefﬁcients nearly zeros means observed linear dependencies variables actually weak distribution model learn much different univariate gaussian model. exhibited behavior generation differ much univariate gaussian either. case switching current model univariate gaussian signiﬁcantly reduce computational complexity requirement population size holding nearly performance. inspired fact ﬁrstly identify approximately independent variables apply simple univariate model them. call strategy weakly dependent variable identiﬁcation weakly dependent/correlated variables identiﬁed ﬁrst calculating global correlation matrix pick variables whose absolute values correlation coefﬁcients variables larger threshold variables formally deﬁned |corr| performing still leave rest variables multivariate model. words still consider variables fully dependent other. contrast weakly dependent regard variables strongly dependent. strongly dependent variables deﬁned note global correlation matrix purpose identifying need large amount samples estimating reliable global covariance matrix purpose guiding search even though computing correlation matrix essentially difference computing covariance matrix. precision covariance matrix direct impact inﬂuencing sampling procedure thus inﬂuencing algorithm’s behavior require sufﬁciently large amount data. whereas correlation matrix coarse learning identifying weakly dependent variables precision longer plays leading role determine algorithm’s performance. later loose requirement sample size also helps reduce computational cost. mcorr denote sample size constructing global correlation matrix main depicted fig. term weakly dependent/correlated strictly deﬁned term statistics domain. whether variable classiﬁed determined correlation matrix hand user speciﬁed parameter correlation matrix reﬂects observed information search space different values reﬂect user’s conﬁdence univariate model. larger probable variables optimized univariate model. less computational cost smaller population size required. note nongaussian model based edas weakly dependent identical weakly correlated. apply edas identiﬁcation method needs re-deﬁned. course imagine ways deﬁning weakly/strongly dependent variables. instance classify variables weakly strongly dependent considering correlation function optimized. idea separating weakly dependent variables strongly dependent ones context interesting worth consideration future. however typically done implementations deﬁnition weak/strong dependency restricted variables model reﬂect correlation variable function value. suppose limited population size still large samples give reliable estimation multivariate gaussian model. obtain better performance project points several subspaces dimensional search space build model sample solutions subspaces. impractical increase building subspace models using combination approximate global estimation another choice. call subspace modeling whose shown fig. subset group variables corresponds subspace. samples projected ⌈|s|/c⌉ subspaces build multivariate model subspace. capacity indicates maximum size subspace. represents extent trust samples give reliable estimation. dividing variables several subspaces projecting samples lower dimensional subspaces considers local dependencies among variables belonging fig. demonstrations gaussian distributions different correlation coefﬁcients. contours denote gaussian densities. every sub-ﬁgure variables standard deviation equals correlation coefﬁcient equals covariance. subspace density samples subspace increase. technique probably offers feasible alleviating growth population size respect growing problem dimension validated experimental results later sections. construct according randomly partition ⌈|s|/c⌉ non-intersected subsets s⌈|s|/c⌉. user speciﬁed parameter deﬁning size subset estimate multivariate model subset based selected individuals. randomly partitioning variables different subsets regarded independently. multivariate gaussian model subspace combination subspace gaussian models seen approximation global gaussian estimation global mean vector still identical combination subspace models global covariance matrix approximated block diagonal matrix whose main diagonal blocks subspace covariance matrices. fig. shows example. variables kept together within group. means size current beyond capability global multivariate model samples estimate according user’s experience. therefore make concession explicitly eliminating dependencies variables keeping rest. state later performed every generation thus random partition ﬁxed evolution. variables different subsets current generation always chance grouped subset keep interactions next generations. similar strategy also proposed sampling individual using model variables sampled subspace models belong concatenate sampled variables evaluation newly sampled individual traditional edas. random subspace partitioning method proposed simple straightforward one. experiments show although simplest method indeed signiﬁcantly improve edas’ performance high dimensional problems. course sophisticated subspace partitioning methods developed needed. example divide several clusters variables according correlation coefﬁcients treat cluster subspace. however clustering still disadvantage suffers curse dimensionality. given ﬁnite sample size cannot expect good clustering high dimensional space. later section comparison random subspace partitioning clustering-based conducted. experiments provide evidence simple random partitioning performs signiﬁcantly better clustering-based partitioning high dimensional problems. incorporating within framework explicitly control model complexity. helps reduce model complexity necessary level reduces model complexity according population size applied. denote subset vector denote realizations variables performing ﬁnal joint several approaches controlling/penalizing model complexity edas also proposed previous studies. instance egnaee uses edge exclusion test control structure complexity gaussian network uses metric local search decide structure normal idea uses metric penalize complexity normal factorization however signiﬁcant differences eda-mcc previous approaches fig. shows typical results model structure applying previous approaches wi+sm. using previous approaches still probable model structure connected graph although dependencies removed. means variables still within multivariate model. thus curse dimensionality computational complexity issue still strongly restrict algorithm’s performance higher dimensional problems. grows performance keep deteriorating computational cost rapidly increase. consistent fact rare results algorithms higher dimensional problems reported. hand wi+sm explicitly partitions variables several separated groups. different small models applied subsets experiments prove wi+sm signiﬁcantly slow performance deterioration increasing speed commotional cost grows. fig. demonstration model structures applying traditional approaches wi+sm respectively. circle represents variable directed edges represent dependency. univariate variable multivariate variables instance assign univariate gaussian assign multivariate gaussian based main novel algorithm model complexity control given fig. discussed above purpose coarse learning mcorr need large sample mcorr individuals selected individuals calculate correlation matrix duplicate samples cannot contribute correlation estimation sampling without replacement. initialize population generating individuals randomly. repeat stopping criterion met. select individuals randomly sample mcorr individuals selected individuals without replacement. mcorr individuals calculate comparison computational complexity edac emnaglobalare shown table demcc umdag tails computation please refer appendix mcorr number generations eda-mcc’s computational complexity always complexities univariate gaussian multivariate one. besides eda-mcc requires smaller computational cost reduced. model variables eeda model mentioned section subset eeda multivariate gaussian based using covariance matrix scaling. performing maximum likelihood estimation eeda scales covariance matrix resetting minimum eigenvalue maximum eigenvalue. eeda regards direction eigenvector minimum eigenvalue corresponds approximation ﬁtness function’s gradient. previous studies shown enlarging variance along direction eeda better explorative ability emnaglobaland require smaller population size. since covariance matrix scaling done eeda roughly level computational complexity previous approaches trying precisely learn global structure data fact impractical high dimensional space. also involve complicated computation make computational complexity edas become even higher. hand wi+sm global structure roughly learnt. since hard perform good global learning high dimensional space wi+sm tries perform good learning divided subspaces give better approximated global estimation. fortunately controlling parameters explicit physical implications interpreted easily. introduce additional time consuming computation eda. even help reduce eda’s computational complexity problem size goes large. also imagine global structure successfully learnt conditions wi+sm outperform traditional approaches. discussion controlling parameters conducted section compared previous approaches wi+sm offers ﬂexibility introducing different search strategies edas. instance form univariate models multivariate models applied subsets respectively. different models different subsets also implemeted. offers opportunities develop edas hybrid algorithms. paper discuss application gaussian models. involved algorithms four algorithms involved experimental comparisons umdag emnaglobal eeda eda-mcc. extensions previous analysis computational complexity select umdag representative univariate gaussian based edas emnaglobalas representative multivariate gaussian based edas. based maximum likelihood estimation. since also many theoretical studies experimental comparisons real-world applications edas made taking edas comparisons make sense. eeda included representative multivariate gaussian based edas using covariance matrix scaling. seen extension emnaglobal makes easy implement based implementation emnaglobal. furthermore fair comparisons algorithm’s behaviors computation time emnaglobaland eeda model variables made. eda-mcc apply umdag eeda model subset implementation yield fair comparisons umdag emnaglobaland eeda. order compare time fairly implement algorithms visual within template framework. algorithms share basic data structures algorithm utility functions numerical computation library. differ model building solution sampling modules. test functions test functions listed table iii. selected classical benchmark functions special session functions minimization problems. details functions including shifted global optima transformation matrices etc. omitted here. readers test functions contains several comparison pairs whether algorithm sensitive shifted rotated function landscape. functions also classiﬁed groups common parameter settings real-world applications usually limitation maximal number ﬁtness evaluations algorithm paramec ters varied. traditional edas umdag emnaglobaland eeda besides representing selection pressure parameter population size given ﬁxed larger offer better learning reduce number generations meantime vice versa small people aware tradeoff population size number generations understand balance factors even vary problem problem signiﬁcant inﬂuence performance eda. however best knowledge still common experience setting suitable achieving promising performance given ﬁxed fes. studies edas investigations emphasize setting population size. instead population size always apply four choices aiming releasing promising performance every every problem. four-population-size tests given problem corresponding dimensionality compare average best solution values obtained every population size every problem select best population size ﬁnal decision algorithm problem given problem size. moreover algorithms tests thus respectively. algorithms initialized uniform random initialization within search space. elitist approach used algorithms i.e. best individual survived next generation together newly sampled individuals constitute generation. settings widely used studying edas previously publications. test function problem sizes also illustrate edas’ requirements population size achieve best performance. according i.e. maximal dimensional problem. algorithms terminated exceed limit. single test result averaged independent runs. experiments executed computer ram. parameters eda-mcc experiments eda-mcc mcorr regard mcorr points enough calculate correlation coefﬁcients pair variables popular threshold deﬁne weakly correlated context statistics. experience also observed sensitive value example small value result empty i.e. variables regarded strongly correlated other makes null operation. large lead i.e. discards eda-mcc degrades umdag dependencies among variables. release power eda-mcc most must optimal given problem parameters. different problems parameters lead different optimal value mentioned above reﬂects user’s conﬁdence univariate model. reasonable analysis effects constant moderate value experiments. demonstrate beneﬁt whereas value beneﬁts give problem independent issue. later section different values inﬂuence eda-mcc tested shown. practice settings determined according user’s preference. normal cases larger applied also larger vice versa. large enough give reliable estimation entire dimensional space implies fully trust global estimation rather approximating combination subspace models. time also afford required computational complexity. hand smaller reduce computational complexity. users weigh pros cons parameters mcorr explicit physical implications. values either bounded determined guidance pre-determined parameters user’s preference. easy parameters applying eda-mcc problem. inﬂuence different investigated later section record difference best ﬁtness algorithm known global optimum i.e. tests. values always nonnegative minimization problems. smaller better performance algorithm implies. mean values standard deviations algorithm test shown table reported smaller consider multiple results among four-populationsize tests report shows fastest convergence. table shows corresponding population sizes used algorithms separable unimodal problems separable unimodal structures facilitate univariate model based edas solving problems although always case. experiments show that eda-mcc perform well. howcase umdag ever emnaglobal relies global multivariate estimation exhibits signiﬁcant performance degradation. eeda also performs well better explorative ability emnaglobal good umdag eda-mcc overall eda-mcc shows best performance among multivariate model based edas statistical signiﬁcance performs well umdag also note emnaglobaland eeda perform worse global optimum shifted away center search space. regarding time required population sizes although time algorithm correspond different population sizes thus different number generations reﬂect time needed exert algorithm’s best performance. umdag costs least time whereas emnaglobaland eeda cost most. eda-mcc’s time grows faster umdag slower emnaglobaland eeda. since easy umdag model population size grows mildly. however population sizes needed emnaglobaland eeda keeps high levels. edamcc’s requirement large population size clearly relaxed wi+sm. requires much smaller population size simultaneously shows signiﬁcant better performance. non-separable problems local optima group functions either unimodal local optima implies problems clear inner structures. non-separable properties pose signiﬁcant difﬁculties umdag fails perform best test. hand eda-mcc performs signiﬁcantly best nearly tests except tests emnaglobalshows worst performance eeda performs generally umdag eda-mcc. note shifted versions respectively. original unshifted versions although umdag eeda performs signiﬁcantly worse edamcc absolute performance bad. however global optima shifted away performance become much worse. emnaglobalhas similar issue absolute performance always worst. among algorithms eda-mcc shows robust performance respect shifts global optima. time cost algorithms similar results previous group functions eda-mcc’s time grows much slower emnaglobaland eeda. although umdag costs least time performance always worse eda-mcc problems. eda-mcc also usually needs smallest population sizes among except also next group optimal population size eda-mcc eeda sometimes ﬂuctuate grows. explained since better explorative ability beneﬁt large population size also large number generations resulted applying small emnaglobalwhich population size. however umdag completely relies maximum likelihood estimation optimal population sizes usually keep increasing. group relatively hard problems algorithm gives good absolute performance. best knowledge following tests known algorithms good solutions problems eda-mcc fact best general. among problems global optimum bounds domain requires explorative ability among test functions. test eeda performs best since global guidance gradient relatively good estimation obtained. however eda-mcc explicitly partitions search space search along approximated global gradient effective eeda. problem size grows eda-mcc outperforms eeda signiﬁcant better solution. conﬁrms effectiveness using combination subspace models approximate global estimation high dimensional space precise global estimation hard obtain approximating global estimation combination subspace models performs better. verify effectiveness combination subspace models extend experiments compare eeda eda-mcc. experimental settings solution quality comparison. results divided groups according problem properties. cell contains mean standard deviation runs. value regard zero. best result above. comparison shown table fig. grows even larger performance combination subspace models signiﬁcantly better poor global model. eda-mcc ﬁnds signiﬁcantly better solutions also scales larger problems better i.e. much slower increase time larger problems. cannot perform well eda-mcc computational cost always much lower. wonder whether bigger time budget would lead superior performances edaumdag mcc. fig. plot averaged evolutionary curves runs algorithms tests give answer. evolutionary curves umdag quickly become algorithm proceeds. implies fact even given time umdag cannot better solution converges suboptimal one. mcc. functions large population sizes perform even group worse. implies failure umdag functions primarily model simplicity either larger population size longer time lead better performance. word group non-separable functions edac fails tests performs signiﬁcantly best. umdag model simplicity. emnaglobaland eeda cannot perform well high dimensional tests either. multimodal problems many local optima functions quite large number local optima lead complicated function landscape make problem hard solve. problems using sample size estimated multivariate model cannot reliable previous group problems. results coincide intuition. although separable results show easy solve multivariate gaussian based edas. previous study shown small population size applied emnaglobaland eeda cannot perform well eeda even perform worse emnaglobal. huge number local optima misleads multivariate search covariance matrix scaling. umdag performs best emnaglobalthe second function. eeda eda-mcc adopting covariance matrix scaling fail. applying rotation makes non-separable. even global optimum shifted compared results surprisingly umdag still outperforms others whereas emnaglobalbecomes much worse. eeda eda-mcc approximately hold solution quality. intuitively non-separable problem hard group. results representative functions summarized table vii. observe larger population size help umdag obtain better results experiments. speciﬁc results using become little better still always much worse edac using large population sizes results averaged runs. results using also directly included table comparison. function value result compared eda-mcc result using nonparametric mann-whitney umdag however results reveal high dimensional even much harder multivariate gaussian model. expanded multimodal function umdag performs best. seems complicated problem structure group functions poses similar difﬁculties eda-mcc simple algorithms like umdag good enough problems. time comparisons group functions similar previous results eda-mcc’s time emnaglobal. since eda-mcc always umdag based wi+sm cannot perform well optimal population size also becomes large. failure eda-mcc success analyze failure umdag eda-mcc success umdag additional experiments presented here. generally speaking experiments concern characteristics edas closely related performance functions. goal intrinsic reasons prevent eda-mcc performing well them. ﬁrst characteristic take account model complexity eda. speciﬁc problem multivariate gaussian necessarily outperform univariate gaussian eda. failures several multivariate gaussian edas success univariate gaussian probably imply using high dependency degree functions longer effective. intuition validated experiments failures eda-mcc functions likely attribute failures high dependency degree novel techniques adopted eda-mcc. therefore test explicitly controlling dependency degree changing value i.e. original settings note edamcc perform exactly umdag restricts multivariate dependencies minimal degree dependencies variables considered. also tests happens dimensions. note tests essentially identical since variables included. another characteristic inﬂuence performance base multivariate model also indicates algorithm building probabilistic model. adopts maximum likelihood estimation umdag emnaglobalmodel similar umdag model maximum likelihood estimation. umdag promising performance three functions indicate maximum likelihood estimation efﬁcient covariance matrix scaling three functions. therefore replace eeda model emnaglobalmodel eda-mcc framework test effect base model. crossing settings base multivariate model subspace size four candidate implementations compared umdag eda-mcc eeda model eda-mcc eeda model eda-mcc emnaglobalmodel eda-mcc emnaglobalmodel still implementation four population sizes applied test. best result among four-population-size tests selected ﬁnal result. comparison including results umdag summarized table viii. experiments observe tests statistical signiﬁcant difference among candidate algorithms three problems. eda-mcc good umdag tests switching different degrees multi-dependencies help eda-mcc achieve performance promising umdag matter base model eeda model emna model. implies three functions computational resources limited utilizing multi-dependencies among variables effective strategy. speciﬁc long considering multi-dependencies even minimal degree search misled huge number local optima. increases effect becomes serious. nevertheless changing eeda model emnaglobalmodel help better solutions although results always good umdag implies functions large radical covariance matrix scaling easily misled complicated function landscape. however conservative maximum likelihood estimation perform better. covariance matrix scaling strategy effective small. course discussions restricted predeﬁned population sizes maximal fes. since edamcc perform good umdag dimensional tests guess extremely large population size sufﬁciently large budget eda-mcc potential come even outperform umdag considering fast increasing number local optima fast increasing complexity function landscape grows eda-mcc’s requirement population size outperform umdag also increase tremendously. also explained effect curse dimensionality. therefore facing problems many local optima maybe computationally expensive apply multivariate search strategy expect good performance. case cheap simple univariate model based algorithm umdag better choice given limited computational resources. discovered experiments compared traditional edas eda-mcc shows remarkable effectiveness efﬁciency high dimensional non-separable problems local optima. simple separable problems eda-mcc comparable umdag problems many local optima work well case eda-mcc offers partial simple umdag solution three problems proposed beginning section eda-mcc’s computational cost usually lower traditional multivariate gaussian based edas; edamcc’s increasing speed time cost also much slower. high dimensional space sparse data only global estimation longer precise eda-mcc effective. however function landscape huge number local optima edamcc well traditional multivariate gaussian model based edas fail. case simple univariate gaussian based edas effective efﬁcient. success eda-mcc mean escape curse dimensionality. eda-mcc suffer less controlling model complexity necessary level. using ﬁxed ﬁnite population size eda-mcc edas relying learning deﬁnitely fail extremely high dimensional search space. note although eda-mcc better performance traditional edas none candidate algorithms performs well enough ﬁnding high quality solution. hand problems really hard solve edas using current experimental settings. hand effective efﬁcient search strategies large scale optimization still designed investigated. enlarge problem size compare eda-mcc traditional edas several optimization algorithms designed large scale optimizac mimicg tion. involved traditional edas include umdag also gaussian model based continuous mimicg whose model complexity umdag multivariate gaussian based edas. variable dependency mimicg chain-shaped structure bivariate conditional gaussian densities. however multivariate gaussian based edas emnaglobal eeda egna included time benchmark functions long acceptable. recently yang proposed cooperative coevolution framework variable grouping adaptive weighting large scale optimization problems. algorithm named decc-g uses differential evolution base algorithm framework proposed. decc-g also adopts variable partitioning strategy within cooperative coevolution framework decc-g activating variables group variables ﬁxed. evaluation currently activated variables calculated context ﬁxed variables. whereas eda-mcc although variables also grouped several subsets optimizations simultaneous synchronized. eda-mcc instance cooperative coevolution. deccg compared three algorithms sansde fepcc decc-o several functions shows outstanding performance terms mean best solution values compared algorithms. compare eda-mcc results reported another algorithm sep-cma-es recently proposed hansen also included comparison. original cma-es incapable handling problems several hundreds dimensions sep-cma-es developed using diagonal covariance matrix gaussian model keeping original covariance matrix adaptation. several recent studies investigated performance high dimensional problems larger although sep-cma-es uses diagonal covariance model estimations matrix well umdag different. major difference sep-cma-es relies cumulation information gathered evolution path model covariance matrix heuristic-based thus requires small population size. however typical like umdag estimates covariance matrix based samples current generation maximum likelihood estimation learning-based manner thus usually requires much larger population size sepcma-es. seen later experiments could lead different performance. recommended parameter settings sep-cma-es conduct comparison population size selected size initial standard deviation identical third search interval initial search point center search space. implementation sep-cma-es derived implementation cmaes. following maximal .e+. results averaged independent runs. population size decc-g subcomponent dimension tests. parameters sansde fepcc decc-o please refer umdag mimicg population size selected size adopted. implementation eda-mcc keeps unchanged experiments using umdag model eeda model subset population size selected size mcorr tests. small solving problem consequently test whether better performance obtained keeping selection pressure. test give small population sizes high conﬁdence dimensional subspace still trust estimated subspace models. result simplest separable eda-mcc umdag decc-o decc-g sep-cma-es perform well. second group non-separable functions eda-mcc sep-cma-es show stable good performance. interestingly although sep-cma-es adopts diagonal covariance matrix performs generally well non-separable functions also reported ronsenbrock functions signiﬁcantly outperforms eda-mcc. whereas eda-mcc signiﬁcantly outperforms sep-cma-es eda-mcc sep-cma-es reach global optimum although sep-cma-es little better average performance signiﬁcant difference eda-mcc’s. compare decc-g eda-mcc decc-g performs better edamcc. decc-g rather sensitive shifted global optimum shifted eda-mcc performs well holding almost solution quality whereas decc-g becomes much worse. similar situations happen shifted rotated version performance eda-mcc sensitive shifted rotated function landscape decc-g. last group functions analyzed above umdag clear advantage effectively solve huge number local optima general. decc-o umdag performs much better others. consistent previous observations. deccoptimize function variable time within cooperative coevolution framework behaviors similar extent. therefore umdag effective functions huge number local optima f-f. exception decc-o fails explained sensitiveness shifted global optimum. sep-cma-es although also uses univariate model performance worse umdag might partly small population size covariance matrix estimated sep-cmaes. observations also extent consistent previous analysis simple univariate model standard conservative maximum likelihood estimation efﬁcient high dimensional problems many local optima. fails perform best problem. suffering effect curse dimensionality neither effective umdag problems simple univariate model already handle good eda-mcc non-separable problems clear structure. results validate analysis difﬁculties traditional edas high dimensional problems. generally speaking eda-mcc relatively small population size shows robust performance problems especially non-separable problems local optima. performs statistically better sansde decc-o umdag although decc-g also performs generally well sensitiveness shifted global significance level shown markers marker implies significant difference. results fepcc reported thus also leave blank. two-tailed friedman test demonstrates algorithms interesting considering univariate nature gaussian model. could topic worthy study future work. eda-mcc ﬁrst successful application multivariate model based general class problems since continuous edas proposed. moreover compared show signiﬁcant superiority eda-mcc umdag functions implies advantage using probabilistic models statistical learning optimization. also note tune parameters edamcc. potential performance even better realworld high dimensional problems. section investigate dependence eda-mcc newly introduced parameters experiments. separable function non-separable function selected test functions demonstration. different settings tested functions problem size population size selected size adopted previous experiments eda-mcc kept ﬁxed following tests i.e. performance comparison combinations summarized tables x-xi. note current implementation eda-mcc uses eeda model subsets even adopting large long empty eda-mcc’s performance still variable dependencies distance umdag over-eliminated large according deﬁnition covariance matrix scaling performance become unstable since gradient easily wrongly approximated. generally speaking separable problems different much impact eda-mcc’s performance. non-separable different change best performance much except combining small large make easily become empty undoubtedly hazardous performance non-separable problems. large harmful solving non-separable problems although cost longer time analyzed before. however small similar effect large dependencies variables over-eliminated. since partition random considering non-separability makes covariance matrix scaling fail together small conclude large obviously hazardous nonseparable problems. besides small recommended either brings similar negative effect large subspace modeling clustering variables? eda-mcc randomly partition subspaces whether sophisticated partitioning applied e.g. partition subspaces clustering variables based strength interdependencies. intuitively method work well sample size large enough compared problem size grows large limited sample size available performance good random partition since learning method including unsupervised clustering affected curse dimensionality. section replace previous eda-mcc greedy clustering like method named smgc compare eda-mcc. resulting algorithm called eda-mcc-gc details sm-gc shown fig. short sm-gc partitions subspaces following steps first pair variables whose absolute correlation largest among ones picked initial cluster. implies pair variables strongly dependent among all. variable outside cluster selected added cluster condition correlation variables cluster strongest. operation iterates cluster reaches maximal size strongly dependent variable found perspective cluster. cluster refers partitioned subspace. then dependencies cluster rest variables eliminated. outer loop keeps generating subspaces greedy manner variables partitioned strongly dependent variables left. clustering still variables left univariate model applied variables since regarded weakly dependent algorithm. compare eda-mcc-gc previous eda-mcc three representative functions algorithms compared tests. population sizes parameters eda-mcc-gc used eda-mcc previous experiments. results parameters used summarized table xii. tests signiﬁcant difference eda-mcc-gc eda-mcc. however tests small sample size applied edamcc performs signiﬁcantly better eda-mcc-gc. veriﬁes previous intuition applied high dimensional optimization problems limited population size partitioning subspaces based clustering might effective random partition. though illustrative experiments cannot exclude possibility delicate clustering approach might outperform random partition speciﬁc high dimensional optimization problems clustering motivation scaling edas regard solving problem major advantage using traditional gain feedback problem properties observing probabilistic model learnt. learnt structure estimated parameters model reﬂect underlying properties problem. addition ﬁnding solution ability characterize problem properties. however advantage deeply investigated. recent study discrete model used represent interactions protein conformations probability models. still rare study done continuous models characterize structure optimization problem. eda-mcc able give analysis observing model structure obtained wi+sm. experiments eda-mcc also record results procedure every generation test. analyzing results give in-depth analysis problem properties characterization ability eda-mcc. record number strongly dependent variables i.e. elements curves average strong runs evolution thus plotted. variables partitioned also plotted matrix corresponds variable. column corresponds generation. element column ranging indicates many runs partitioned variable generation runs. examining matrix rows relatively hard human eyes additional experiments eda-mcc. results experiments even harder read omit here. tests based settings previous experiments. relatively small easier examine graphic results changing trends grows. purpose comparing average strong matrix ﬁgure clearly transform column indicates number generations number evaluations following ﬁgures. horizontal axis average strong graph converted eval well. limited page length report results although results seemed solo effect actually plays important role guarantee effectiveness mutual effects shown later. fig. separable strong remains level. grows level strong also becomes higher. interpreted effects data sparsity higher dimensional space. ﬁxed experiments number variables become smaller search space enlarges eda-mcc capture correlations actually exist variables. relatively level strong consistent separability function. furthermore grey levels matrices nearly uniform means variables observed play identical roles contributing ﬁtness function value. also consistent function expression. fig. shows eda-mcc correctly recognizes problem structures shifted rosenbrock variable dependency problem chain-like structure ﬁrst variable determines second second determines third ﬁrst identiﬁes last pair variables quickly realizes ﬁrst pair variables important. structural information problem clearly precisely identiﬁed. experiments shown eda-mcc signiﬁcantly outperforms others shifted rotated high conditioned elliptic fig. shows always helps eda-mcc recognize problem structure. results clearly show variables constantly identiﬁed strongly dependent evolution fig. results sphere. curves average strong plotted upper row. corresponding matrices plotted lower row. darker element times variable partitioned speciﬁc eval runs. fig. results shifted rosenbrock. curves average strong plotted upper row. corresponding matrices plotted lower row. darker element times variable partitioned speciﬁc eval runs. element whose value found matrix partly represents extent original variables impact function value. roughly speaking indicates effect onto thus onto ﬁnal function value. non-linear hard analyze exact impact variable. since mainly impacts function value instead analyze column partly indicate impact onto thus onto ﬁnal function value give rough analysis. plot curves coefﬁcient sub-ﬁgures ﬁrst column fig. sub-ﬁgures second column show fig. results shifted rotated high conditioned elliptic. curves average strong plotted upper row. corresponding matrices plotted lower row. darker element times variable partitioned speciﬁc eval runs. absolute value matrix abs. absolute value positive negative coefﬁcients variable inﬂuence function value. sub-ﬁgures third column show column denoted abs. compare experimental results shown last column fig. stretch widths make size. directly fig. large domination becomes weak coefﬁcients etc. approach coefﬁcient therefore difference rough analysis experimental results also becomes larger. however four tests always evidence successfully recognizes problem structure variables impacting function value correctly identiﬁed dark rows fig. shows results shifted rotated rastrigin performs results also help explain umdag well problem eda-mcc fails. examining results rastrigin results similar fig. since separable results reasonable. analyzed above inefﬁciency covariance matrix scaling function huge number local optima eda-mcc cannot perform well. however non-separable still fails recognize problem structure sample size less enough considering huge number local optima. information gather looks like separable problem useful interdependencies learnt observation. result eda-mcc perform well either. algorithms characterization ability describe problems’ underlying structural information always remarkable. regard valuable aspect eda-mcc. however huge number local optima eda-mcc still limitation. also noticed current implementation eda-mcc haven’t tried every possible univariate model multivariate model gaussian models used. therefore even eda-mcc correctly characterizes problem properties every possible effort utilize information. explain cases eda-mcc cannot outperform algorithms even correct problem structure characterization. admit results restricted within capability gaussian models. thing needs addressed solving realworld problem practice user want able eda-mcc multiple runs obtain problem’s structural information. however problem provide sufﬁcient information. case recommended allow eda-mcc restarts aggregate information collected multiple trials generate matrix. section analyze roles interactions. besides implementation edamcc wi+sm also implement only version only version. compare versions eda-mcc test functions analyze respective roles. save space report comparisons selected functions including here. parameters only only exactly respective settings previous eda-mcc experiments. test population sizes fig. explanations results coefﬁcients shown ﬁrst column. second column demonstrates abs. third column shows column denoted abs. experimental results shown last column directly adopted fig. last columns similar especially dimensional tests. solution results shown table xiii. wi+sm performs best usually ﬁnds order-ofmagnitude better solutions only only. only applies several multivariate models variables ways dealing actually weakly dependent variables efﬁcient. therefore fails perform best function except simplest hand only perform slightly better wi+sm wi+sm much worse others. times reported fig. although only cannot solutions comparable quality time cost usually acceptable comparable wi+sm. whereas only cost much time. generally speaking wi+sm shows much robust performance moderate time cost only only. also interesting only perform slightly better wi+sm implies contribute functions. consistent previous conclusions section iv-c subspace partitioning changing help solve functions. without only even performs little better. necessary e.g. only fail. investigate interaction terms eda-mcc’s ability characterization problem structure plot results only fig. demonstrations. results only functions similar either fig. results shifted rotated rastrigin. curves average strong plotted upper row. corresponding matrices plotted lower row. darker element times variable partitioned speciﬁc eval runs. fig. results procedure only results similar results eda-mcc fig. contributes nothing solving characterizing problem. result quite different fig. implies effects functions strong clear variable interdependencies. also procedure. based samples drawn unprecise global model also becomes useless eventually variables partitioned also makes modeling sampling global multivariate model becomes slower costs longer time. hand unnecessary only still characterize problem structure properly ﬁnds solutions better quality. functions. functions strong variable interdependencies like without precision global multivariate model fast deteriorates search proceeds. affects solution quality note emnaglobal usually practice means population size usually larger problem size thus overall complexity sampling measured primarily step step ignored. overall complexity precision search model thus helps effectively recognize problem structure. hand helps properly apply different search strategies weakly dependent strongly dependent variables good solutions effectively. obviously success edamcc terms problem structure characterization ability robust performance high dimensional optimization problems based combination paper ﬁrst analyze difﬁculties traditional continuous edas high dimensional search space. curse dimensionality given ﬁnite population size performance traditional edas fast deteriorates problem size grows large. computational cost also increases fast using multivariate model nonseparable problems. improve performance reduce computational cost high dimensional optimization novel multivariate model complexity control proposed. adopting weakly dependent variable identiﬁcation subspace modeling eda-mcc shows signiﬁcantly better performance traditional edas high dimensional non-separable problems local optima. computational cost requirement large population size also significantly reduced eda-mcc. besides eda-mcc exhibits remarkable problem property characterization ability. solving problem eda-mcc solution also give users feedbacks variable dependency structures problem. ability valuable obtaining solution. especially useful facing black optimization problem. based extracted problem structural information efﬁcient algorithms designed speciﬁcally give better solutions. limitations eda-mcc also analyzed. first dimensional search space available population size usually large enough offer good global model estimation eda-mcc effective traditional edas. advantage eda-mcc traditional edas appears high dimensional space given population size fails give reliable global model estimation. second facing high dimensional non-separable problems huge number local optima eda-mcc effective efﬁcient simple univariate gaussian eda. note current discussions implementation eda-mcc still restricted gaussian models. different base univariate multivariate models gaussian still tested analyzed. moreover smarter self-adaptive setting still interesting issue left future work. computation using premises section give one-generation computational complexity edamcc. univariate gaussian models multivariate gaussian models. zhenyu yang yang tapabrata wang insightful comments help improve paper alexander mendiburu kindly provided source codes mimicg egna. work partially supported epsrc grant china scholarship council scholarship weishan dong support visit university birmingham part work done. continuous iterated density estimation evolutionary algorithms within idea framework proceedings optimization building using probabilistic models obupm workshop gecco- tsutsui pelikan goldberg evolutionary algorithm using marginal histogram models continuous domain proceedings optimization building using probabilistic models obupm workshop gecco- yuan gallagher playing continuous spaces analysis extension population-based incremental learning proceedings ieee congress evolutionary computation vol. poˇs´ık distribution tree-building real-valued evolutionary algorithm parallel problem solving nature ppsn viii ding zhou optimizing continuous problems using estimation distribution algorithm based histogram model proceedings conference simulated evolution learning ding zhou reducing computational complexity estimating multivariate histogram-based probabilistic model proceedings ieee congress evolutionary computation ding zhou histogram-based estimation distribution algorithm competent method continuous optimization journal computer science technology vol. ding zhou zhang marginal probability distribution estimation characteristic space covariance-matrix proceedings ieee congress evolutionary computation suganthan hansen liang chen auger tiwari problem deﬁnitions evaluation criteria special session real-parameter optimization nanyang technological university singapore http//www.ntu.edu.sg/home/epnsugan tech. rep. santana larra˜naga lozano protein folding simpliﬁed models estimation distribution algorithms ieee transactions evolutionary computation vol. bosman thierens advancing continuous ideas mixture distributions factorization selection metrics proceedings optimization building using probabilistic models obupm workshop gecco- bosman thierens numerical optimization real-valued estimation-of-distribution algorithms scalable optimization probabilistic modeling algorithms applications wang restart univariate estimation distribution algorithm sampling mixed gaussian l´evy probability distribution proceedings ieee congress evolutionary computation bielza robles larra˜naga estimation distribution algorithms logistic regression regularizers microarray classiﬁers methods information medicine vol. implementation edas based probabilistic graphical models ieee transactions evolutionary computation vol. gonz´alez lozano larra˜naga mathematical modelling umdac algorithm tournament selection. behaviour linear quadratic functions international journal approximate reasoning vol. zhang dong huang approach model based localization recognition vehicles proceedings international workshop visual surveillance ieee conference computer vision pattern recognition cvpr’", "year": 2011}