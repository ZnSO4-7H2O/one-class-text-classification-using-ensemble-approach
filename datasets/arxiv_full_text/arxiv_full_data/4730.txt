{"title": "Separation of Concerns in Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, we propose a framework for solving a single-agent task by using multiple agents, each focusing on different aspects of the task. This approach has two main advantages: 1) it allows for training specialized agents on different parts of the task, and 2) it provides a new way to transfer knowledge, by transferring trained agents. Our framework generalizes the traditional hierarchical decomposition, in which, at any moment in time, a single agent has control until it has solved its particular subtask. We illustrate our framework with empirical experiments on two domains.", "text": "paper propose framework solving single-agent task using multiple agents focusing different aspects task. approach main advantages allows training specialized agents different parts task provides transfer knowledge transferring trained agents. framework generalizes traditional hierarchical decomposition which moment time single agent control solved particular subtask. illustrate framework empirical experiments domains. marvin minsky’s society mind theory postulates behaviour result single cognitive agent rather result society individually simple interacting processes called agents power approach lies specialization different agents different representations different learning processes larger scale society whole validates approach technological achievements result many cooperating specialized agents. study minsky’s idea specialized agents context reinforcement learning goal learn policy agent interacting initially unknown environment using feedback form positive negative rewards. speciﬁcally evaluate setting single-agent task solved using multiple agents agent different reward function. insight behind work difference performance objective speciﬁes type behaviour desired learning objective feedback signal modiﬁes agent’s behaviour. single reward function often takes roles. however roles always combine well single function becomes clear domains sparse rewards learning prohibitively slow. intrinsic motivation aims address this adding domainspeciﬁc intrinsic reward signal reward coming environment. typically intrinsic reward function potential-based maintains optimality resulting policy. system motivated similar ideas intrinsic motivation relax typical assumptions allow general application. speciﬁcally relax assumptions along three dimensions optimality policy reasonable policy challenging problems; allow learning objective yields good performance performance metric; consider multiple agents different learning objective. work also related options hierarchical learning fact approaches viewed implementing special type system agents organized hierarchical way; actions agents higher hierarchy selectors agents directly hierarchical decomposition especially useful sparse-reward problems clear sub-goals identiﬁed. however general multi-agent approach consider applied relevant cases well demonstrate paper. main challenges multi-agent system achieve stable independent learning distinguish different ways single-agent task decomposed multi-agent system identify conﬁguration sufﬁcient conditions stable learning. besides analysis perform experiments domains demonstrate value decomposing task using separation concerns. rest paper organized follows. start motivating example illustrate concept separating concerns covering basic background mdps discuss stability conditions different types conﬁgurations provide empirical results illustrate beneﬁts capabilities separating concerns discussing related work conclude motivate idea separating concerns consider fruit collection task figure goal collect fruits quickly possible. agent aims maximize return expected discounted rewards giving agent reward fruits eaten using optimal policy guaranteed minimal number steps fruits. grid size squares fruits statespace large value state-space size enormous. large state-spaces necessarily make problem hard; using deep reinforcement learning task often mapped low-dimensional representation accurately represent optimal value function. problem above however instance travelling salesman problem known np-complete makes highly unlikely low-dimensional representation found accurately represent optimal value function. reward problem sparse makes problem np-complete. adding potential-based intrinsic reward function make reward less sparse make problem easier because maintains optimality solution hence task remains np-complete. task made easier adding domain knowledge form modiﬁed figure fruit collection example. robot collect fruits quickly possible actions directional movements shown plus no-op action. robot receives reward fruits collected; otherwise reward fruit positions drawn randomly start episode. consider learning objective gives reward eating fruit combination small ﬁnding low-dimensional representation becomes easier fruits away minimal impact value function ignored. potential issue nearby fruits gone agent might know hand large could used ignore fruits away ﬁnding good low-dimensional representation becomes much challenging. alternatively consider fruit gets assigned speciﬁc agent whose learning objective estimate optimal action-value function eating fruit aggregator making ﬁnal action selection. agent sees reward assigned fruit gets eaten otherwise. state-space agent ignore fruits irrelevant value function. therefore example single state-space size gets replaced state-spaces consisting states. moreover agents learn parallel using off-policy learning. hence learning problem becomes much easier. well multi-agent approach performs respect performance objective depends aggregator. aggregator could example voting scheme select action based summed action-values select action according agent highest action-value. last form action selection would result greedy behaviour agent always going fruit closest correlates well performance metric. domains however might require different aggregator. sult different options option giving policy speciﬁc fruit. options would actions higher-level agent would evaluate based high-level reward function. state-space higher-level agent however would still state-space learning problem would reduced. problems formalized markov decision processes described -tuples form consisting states; actions; transition probability function giving state action probability transition state next step; reward function giving expected reward transition discount factor specifying future rewards weighted respect immediate reward. goal maximize return actions taken discrete time steps according policy deﬁnes action selection probability conditioned state. policy corresponding action-value function gives expected value return conditioned state action section discuss several agent conﬁgurations decompose tasks different ways. conﬁguration discuss sufﬁcient conditions stable learning deﬁne below. given learning method converges optimal policy single-agent task applying method independently agents model overall policy model converges ﬁxed point. moreover ﬁxed point depends model particular learning algorithm used. general decompose single-agent task using agents shown figure time step agent i×ci choses action environment actions communication actions also allow agents communication actions environment actions. environment actions agents aggregator function maps action input space agent based communication actions previous time steps updated state space. general agent partially observable full state space communication actions. formally state space agent projection onto subspace interpretation equation agents except agent stationary policy task agent becomes markov. note trivially holds agent partially observable assumption equation holds deﬁne agent independent agent policy agent affect transition dynamics agent way. formally extend deﬁnitions µ−i−j stationary policies assigns policy agent except agent m−i−j space sets. then agent independent agent receives reward agenthor ruithor; vertical agent deﬁned similarly vertical direction. stability equation fact agents fully independent follows trivially agents converge independent other. hence stable parallel learning occurs. dependency graph acyclic agents depend agents agents fully independent. example consider fruit catching task shown figure abody aarm. consider decomposition ‘body agent’ ‘arm agent’. body agent controls abody observing receiving reward bodyhor ruithor. agent controls aarm observes receives reward fruit caught. case body agent fully independent agent depends body agent. stability acyclic graph contains fully independent agents whose policies converge independent agents. policies converged agents depend independent agents converge agents converged. hence also case stable parallel occurs. agent dependent agent independent show dependency relations agents using dependency graph based relations distinguish three different subclasses discuss below. figure falling fruit example. robot catch falling fruit green basket receive reward basket attached main body move relative body. robot move main body along horizontal axis using abody {left no-op right}. independent that robot move basket left right body using aarm {left no-op right}. reward baskethor ruithor. condition still holds agents depend upon other. stability setting guarantee stable parallel learning learning agent causes environment non-stationary agent vice versa. possible approach non-parallel learning grouped coordinate descent involves iteratively learning policy agent freezing policies others rotating policy learns convergence occurs approach provide convergence case however requires agents reward function. said single iteration grouped coordinate descent gives well-deﬁned ﬁxed point. since made statements close ﬁxed point optimal policy good ﬁxed point ﬁxed points. ﬁxed point depend strongly initial policies order agents updated. aside common approach pre-training lowlevel agent ﬁxed policy freezing weights training high-level policy using pre-trained agent instance general update strategy. ensemble learning consists using large amount weak learners order build strong learner. applied idea weak learners abandoned sake performance inability framing problem smaller problems. example wiering hasselt combination strong algorithms policy voting value function averaging build even stronger algorithm. enables ensemble learning weak leaners local state space local reward deﬁnitions outline below. ensemble setting agents train policies action space basis local state space local reward function contrary section send actions aggregator instead inform aggregator preferences lat. aggregator selects action based preferences agents. aggregator deﬁned used well many others majority voting rank voting q-value generalized means maximizer etc. agents trained offpolicy based actions taken aggregator since controller system. apply conﬁguration section stability given ﬁxed strategy aggregator stable section evaluate models section empirically. evaluate model action aggregation game catch focussing particular communication. furthermore evaluate model ensemble game inspired pac-man show beats state-of-the-art algorithms. ﬁrst example compare agent model game catch. catch simple pixel-based game introduced mnih standard game consists screen binary pixels goal catch ball dropped random location screen paddle moves along bottom screen. available actions left no-op right. agent receives reward catching ball ball falls screen otherwise. first compare performance model action aggregation agent. model consists high-level low-level agent. high-level agent communication actions communicate desired action low-level agent {left no-op right}. low-level agent environmental actions lat. furthermore high-level agent uses large discount factor access full screen whereas low-level agent small discount factor uses bounding pixels around paddle. high-level agent receives reward ball caught otherwise; low-level agent receives reward plus small positive reward taking action suggested high-level agent. high-level agent takes actions every time steps whereas lowlevel agent takes actions every time step. cyclic dependency agents. hence guarantee stable parallel learning. however found practice parallel learning worked well task. agent high-level agent used identical convolutional neural network. reduced state size low-level agent requires small dense network. full implementation details appendix graphs figure show results comparison. model learns signiﬁcantly faster agent every tested conﬁguration. particular domain agent fails learn anything signiﬁcant considered training period epochs. contrast already converges epochs. reason better performance model two-fold lowlevel agent learn quickly small state space high-level agent experiences less sparse reward reduced action selection frequency. show importance co-operation low-level high-level agent performed additional experiment varied communication reward additional reward low-level agent receives following request high-level agent. results shown figure communication reward high performance drops quickly. interestingly reason performance drop different cases. illustrated figure shows typical play different communication rewards. communication reward low-level agent ignores requests high-level agent misses balls dropped relatively away paddle communication reward high low-level agent ignore environment reward always follow suggestion high-level agent. high-level agent action-selection frequency paddle tends overshoot ball communication reward correctly ball caught almost always showing ideal low-level agent acts neither fully independent fully dependent. highlight effect high-level agent’s communication frequency tested several action-selection frequency settings domain. results shown figure communication frequent learning speed goes down relative action selections reward appears sparse making learning harder. hand infrequent asymptotic performance reduced high-level agent enough control low-level agent move approximately right position. figure left comparing different action selection intervals high-level agent system catch. right effect penalizing communication high-level agent ﬁnal performance system catch. communication probability shows fraction time steps high-level agent sends communication action. figure pac-boy game. pac-boy receives reward eating fruit gets reward bumping ghosts move randomly maze episode ends fruits eaten steps whichever comes ﬁrst. added ‘no-op’ action communication action high-level agent affect reward function low-level agent way. furthermore give high-level agent small penalty choosing communication action no-op action. action-selection frequency high-level agent figure shows results different values communication penalty. system learn maintain near optimal performance without need constant communication. pac-boy contains potential fruit positions. fruit distribution randomized. speciﬁcally start episode probability position fruit. episode fruits remain ﬁxed eaten pac-boy. state game consists positions pac-boy fruits ghosts resulting hence ﬂat-agent implemented without using function approximation. setup separate concerns follows assign agent possible fruit location. agent sees reward fruit assigned position gets eaten. state space consists pac-boy’s position resulting states. addition assign agent ghost. agent receives reward pac-boy bumps assigned ghost. state space consists pac-boy’s position ghost’s position resulting states. fruit agent active fruit assigned position. average fruits average number agents small state spaces agents tabular representation. train agent parallel off-policy learning using q-learning. aggregator function sums q-values action qsum uses \u0001-greedy action selection respect summed values. interestingly q-table ghost-agents exactly same. hence beneﬁt intra-task transfer learning sharing q-table ghostagents results ghost-agents learning twice fast. baselines ﬁrst baseline agent uses exact input features model. speciﬁcally state agent model encoded one-hot vector vectors concatenated resulting binary feature vector size active features time step. vector used linear function approximation q-learning. consider deep baselines. ﬁrst standard algorithm reward clipping. second combined order handle large magnitudes reward input dqn-clipped dqn-scaled -channel binary image channel shape game grid represents positions following features walls ghosts fruits pacboy. complete implementation details supplementary document. figure shows learning speed model compared baselines described above. upper-bound line shows maximum average score obtained. converges policy level controller speciﬁes goal low-level controller. goal accomplished top-level controller selects goal low-level controller. system trained phases ﬁrst phase low-level controller trained different goals; second phase high-level low-level controllers trained parallel. heess also system high-level low-level controller high-level controller continuously sends modulation signal low-level controller affecting policy. type hierarchical learning viewed special case system agents organized hierarchical way. work conjugate mdps also closely related. here several agents coordinate produce action. whole network trained using policy gradient. difference work approach viewed fully-cooperative multi-agent system whereas consider non-cooperative setting. foerster used framework communicating agents based deep neural networks solve various complex tasks. like work conjugate mdps considered cooperative multi-agent setting. allowing deﬁne different rewards agents wider range expressivity. believe pac-boy experiment good illustration powerful system made non-cooperative agents. demonstrated giving agents reward function depends communication actions agents made listen requests agents different degrees. well listens depends speciﬁc reward function. general agents made fully ignore agents fully controlled agents something between makes trade-off between following request another agent ignoring moreover showed agent retains level independence cases yield best overperformance. furthermore demonstrated model convincingly beat state-of-art methods challenging domain. model common closest related work—intrinsic motivation hierarchical learning—that uses domain-speciﬁc knowledge improve performance. long line work aims learn close optimal whereas baselines fall considerably short. linear baseline must handle massive state space absolutely reductions thus takes considerably longer converge. dqn-clipped dqn-scaled converge similar ﬁnal performances policies differ dqnscaled much wearier high negative reward obtained eaten ghosts thus takes much time fruit. order evaluate soc’s capability knowledgetransfer tested different forms pre-training. specifically compared following pre-trained ghost agents pre-trained fruit agents pretrained fruit ghost agents. perform pre-training using random behaviour policy. pre-training agents transferred full game remaining agents trained. seen figure knowledge transfer results clear boost beginning compared original implementation. knowledge especially context options. although signiﬁcant progress made remains challenging problem. purposes domain knowledge obstacle. scale applied speciﬁc real-world systems example complex dialogue systems environments. context using domain knowledge achieve good performance otherwise intractable domain acceptable. article illustrated speciﬁc settings called action aggregation ensemble believe soc’s expressive power wider settings still discovered evaluated. bezdek hathaway howard wilson windham local convergence analysis grouped variable version coordinate descent. journal optimization theory applications foerster assael freitas whiteson learning communicate deep multi-agent reinforcement learning. proceedings advances neural information processing systems mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou kumaran king wierstra legg hassabis human-level control deep reinforcement learning. nature order speed learning take advantage smaller domains tuned parameters originally reported mnih based rough search domain. speciﬁcally reduced replay memory size target network update frequency number annealing steps exploration. coarse search learning rates sampled catch pac-boy. pop-art learning rate versus network used low-level agent low-level agent uses bounding require full convolution network.", "year": 2016}