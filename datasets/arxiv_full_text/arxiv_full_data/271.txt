{"title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.", "text": "work introduce abstract syntax networks extension standard encoder-decoder framework utilizing modular decoder whose submodels composed natively generate asts top-down manner. decoding process given input follows dytasks like code generation semantic parsing require mapping unstructured inputs well-formed executable outputs. introduce abstract syntax networks modeling framework problems. outputs represented abstract syntax trees constructed decoder dynamically-determined modular structure paralleling structure output tree. benchmark hearthstone dataset code generation model obtains bleu exact match accuracy compared previous state-ofthe-art values furthermore perform competitively atis jobs semantic parsing datasets task-speciﬁc engineering. sequence-to-sequence models proven effective tasks using encoder-decoder frameworks exploit sequential structure input output side. approaches account much richer structural constraints outputs—including well-formedness well-typedness executability. wellformedness case particular interest since readily enforced representing outputs abstract syntax trees approach seen much lighter weight namically chosen mutual recursion modules structure tree produced mirrors call graph recursion. implement process using decoder model built many submodels associated speciﬁc construct grammar invoked construct needed output tree. common neural approaches structured prediction decoder proceeds greedily accesses ﬁxed encoding also attention-based representation input model signiﬁcantly outperforms previous architectures code generation obtains competitive state-of-the-art results suite semantic parsing benchmarks. hearthstone dataset code generation achieve token bleu score exact match accuracy greatly improving previous best results bleu exact match ﬂexibility asns makes readily applicable tasks minimal adaptation. illustrate point suite semantic parsing experiments. jobs dataset improve previous state-of-the-art achieving exact match accuracy compared previous record likewise perform competitively atis datasets matching exceeding exact match reported dong lapata though quite reaching records held best previous semantic parsing approaches encoder-decoder architectures without attention applied successfully sequence prediction tasks like machine translation tree prediction tasks like constituency parsing latter case work focused making task look like sequence-tosequence prediction either ﬂattening output tree representing sequence construction decisions work differs recursive top-down generation procedure. neural modeling code including prediction problems longer history. allamanis maddison tarlow proposed modeling code neural language model generating concrete syntax trees left-ﬁrst depth-ﬁrst order focusing metrics like perplexity applications like code snippet retrieval. recently shin attacked problem using grammar-based variational autoencoder top-down generation similar instead. meanwhile separate line work focused problem program induction input-output pairs prediction framework similar spirit doubly-recurrent decoder network introduced alvarez-melis jaakkola propagates information tree using vertical lstm siblings using horizontal lstm. model differs using separate module grammar construct learning separate vertical updates siblings labels require siblings jointly present; however horizontal lstm nodes variable numbers children. differences models reﬂect design decisions also differences data—since asts labeled nodes labeled edges come additional structure model exploits. apart ours best results codegeneration task associated hearthstone dataset based sequence-tosequence approach problem abstract syntax networks greatly improve results. previously andreas introduced neural module networks visual question answering modules corresponding linguistic substructures within input query. primary purpose modules nmns compute deep features images style convolutional neural networks features ﬁnal decision layer. contrast modules describe here modules make decisions generate modules call next figure fragments abstract syntax tree corresponding example code figure blue boxes represent composite nodes expand constructor prescribed named children. orange boxes represent primitive nodes corresponding values written underneath. solid black squares correspond constructor ﬁelds sequential cardinality body class deﬁnition arguments function call abstract syntax trees model makes abstract syntax description language framework represents code fragments trees typed nodes. primitive types correspond atomic values like integers identiﬁers. accordingly primitive nodes annotated primitive type value type—for instance figure identifier node storing \"create minion\" represents function name. composite types correspond language constructs like expressions statements. type collection constructors speciﬁes particular language construct node type represents. figure shows constructors statement expression types. associated language constructs include function class deﬁnitions return statements binary operations function calls. composite types enter syntax trees composite nodes annotated composite type choice constructor specifying node expands. root node figure example composite node type stmt represents class deﬁnition therefore uses classdef constructor. figure hand root uses call constructor represents function call. children speciﬁed named typed ﬁelds constructor cardinalities singular optional sequential. default ﬁelds singular cardinality meaning correspond exactly child. instance classdef constructor singular name ﬁeld type identifier. fields optional cardinality associ. encoder component input encoded using component-speciﬁc bidirectional lstm. results forward backward token encodings later used attention mechc anism. obtain encoding input whole decoder initialization concatenate ﬁnal forward backward encodings component single vector apply linear projection. decoder modules decoder decomposes several classes modules construct grammar discuss turn. throughout denote current vertical lstm state represent generic feedforward neural network. lstm updates hidden state input notated lstm. composite type modules composite type corresponding module whose role select among constructors type. figure exhibits composite type module receives vertical lstm state input applies feedforward network softmax output layer choose constructor constructor modules constructor corresponding module whose role compute intermediate vertical lstm state ﬁelds whenever chosen composite node ated zero children ﬁelds sequential cardinality associated zero children—these designated using sufﬁxes grammar respectively. fields sequential cardinality often used represent statement blocks body ﬁeld classdef functiondef constructors. grammars needed semantic parsing easily given asdl speciﬁcations well using primitive types represent variables predicates atoms composite types standard logical building blocks like lambdas counting figure shows resulting λ-calculus trees look like. asdl grammars λ-calculus prolog-style logical forms quite compact figures appendix show. represent inputs collections named components consists sequence tokens. case semantic parsing inputs single component containing query sentence. case hearthstone card’s name description represented sequences characters tokens respectively categorical attributes represented single-token sequences. hearthstone restrict input output vocabularies values occur training set. model uses encoder-decoder architecture hierarchical attention. idea behind approach structure decoder collection mutually recursive modules. modules correspond elements grammar composed together manner mirrors structure tree generated. vertical lstm state passed module module propagate information decoding process. encoder uses bidirectional lstms embed component feedforward network combine them. componenttoken-level attention applied input step decoding process. constructor ﬁeld modules ﬁeld constructor corresponding module whose role determine number children associated ﬁeld propagate updated vertical lstm state them. case ﬁelds singular cardinality decision update vacuous exactly child always generated. hence modules forward ﬁeld vertical lstm state unchanged child corresponding case sequential ﬁelds horizontal lstm employed child decisions state updates. refer figure illustration recurrent process. initialized transformation vertical state wfvuf horizontal lstm iteratively generation stops process terminates represented solid black circle figure otherwise process continues represented white circle figure case horizontal state sui− combined vertical state attention-based context vector using feedforward network update obtain joint context-dependent encoding ﬁeld position current state either vertical lstm state isolation concatentation vertical lstm state either horizontal lstm state character lstm state submodule computes attention using separate matrix separate attention score qcomp certain decision points require attention highlighted description above; however ﬁnal implementation made attention available decoder decision points. supervised attention datasets consider partial total copying input tokens primitive nodes quite common. rather providing explicit copying mechanism instead generate alignments possible deﬁne tokens attention given primitive node concentrated. matches found corresponding tokens taken whole input. loss term encourages ﬁnal attention weights concentrated speciﬁed subset. formally matched subset componenttoken pairs loss term associated supervision would alignments generated using exact string match heuristic also included limited normalization primarily splitting special characters undoing camel case lemmatization semantic parsing datasets. primitive type modules primitive type corresponding module whose role select among values within domain type. figure presents example simplest form selection process value obtained closed list softmax layer applied incoming vertical lstm state string-valued types open class however. deal these allow generation closed list previously seen values figure synthesis values. synthesis delegated character-level lstm language model part role primitive module open class types choose whether synthesize value not. training allow model character lstm unknown strings include probability binary decision loss order ensure model learns generate character lstm. decoding process decoding process proceeds mutual recursion constituting modules syntactic structure output tree mirrors call graph generation procedure. step active decoder module either makes generation decision propagates state tree both. construct composite node given type decoder calls appropriate composite type module obtain constructor associated module. module invoked obtain updated vertical lstm states constructor’s ﬁelds corresponding constructor ﬁeld modules invoked advance process children. attention following standard practice sequence-tosequence models compute bilinear attention score qraw token input using decoder’s current state token’s encoding semantic parsing data three semantic parsing datasets jobs atis. three consist natural language queries paired logical representation denotations. jobs consists pairs prolog-style logical representations atis consist pairs respectively λ-calculus logical forms. training-test split zettlemoyer collins jobs standard training-development-test split atis. preprocessed versions datasets made available dong lapata text input lowercased stemmed using nltk matching entities appearing input-output pair replaced numbered abstract identiﬁers type. evaluation compute accuracies using tree exact match evaluation. following publicly released code dong lapata canonicalize order children within conjunction disjunction nodes avoid spurious errors otherwise perform transformations comparison. code generation data hearthstone dataset introduced ling consists cards paired implementations open-source hearthbreaker engine. trainingdevelopment-test split identical ling split sizes respectively. textual components contain card’s name description function categorical ones contain numerical attributes enumerated attributes name card represented sequence characters evaluation direct comparison results ling evaluate predicted code based exact match token-level bleu relative reference implementations library. additionally compute node-based precision recall scores predicted trees compared reference code asts. formally scores obtained deﬁning intersection predicted gold trees largest common tree preﬁx. settings experiment feedforward lstm hidden dimensions value. select dimension smaller jobs datasets larger atis hearthstone datasets. dimensionality used inputs encoder cases. apply dropout non-recurrent connections vertical horizontal lstms selecting noise ratio parameters randomly initialized using glorot initialization perform passes data jobs experiments passes atis hearthstone experiments. early stopping based exact match used semantic parsing experiments performance evaluated training jobs development atis. parameters hearthstone experiments selected based development bleu scores. order promote generalization ties broken cases preference toward higher dropout ratios lower dimensionalities order. system implemented python using dynet neural network library adam optimizer default settings optimization batch size semantic parsing experiments batch size hearthstone experiments. table results hearthstone task. supatt refers system supervised attention mentioned section refers system ling nearest neighbor baseline nearest follows ling though performs somewhat better; nonzero exact match number stems spurious repetition data. state-of-the-art accuracy jobs dataset number improves supervised attention added. atis datasets respectively exceed match results dong lapata however fall short previous best results respectively obtained wang difference partially attributable typing information rich lexicons previous semantic parsing approaches hearthstone dataset improve signiﬁcantly initial results ling across evaluation metrics shown table stringent exact match metric improve tokenlevel bleu improve supervised attention added obtain additional increase several points scale achieving peak results accuracy bleu. figure many cards moderately complex descriptions implementation follows functional style seems suit modeling strategy usually leading correct predictions. error analysis discussion examples figures show classes hearthstone dataset share great deal common structure. result simplest cases figure generating code simply matter matching overall structure plugging correct values initializer places. cases system generally predicts correct code figure cards nontrivial logic expressed imperative style challenging system. example prediction comes close gold code misses important statement addition making minor errors. gold code; predicted code. exception instances strings incorrectly transduced. introducing dedicated copying mechanism like used ling specialized machinery string transduction alleviate latter problem. next simplest category card-code pairs consists card’s logic mostly implemented nested function calls. figure illustrates typical case card’s effect triggered game event trigger effect described arguments effect constructor. system usually also performs well instances like these apart idiosyncratic errors take form underovergeneration simply substitution incorrect predicates. cards whose code includes complex logic expressed imperative style figure pose greatest challenge system. factors like variable naming nontrivial control interleaving code predictable description code required conventions library combine make code cards difﬁcult generate. instances system nonetheless able synthesize close approximation. however complex cases predictions deviate signiﬁcantly correct implementation. addition speciﬁc errors system makes larger issues remain unresolved. existing evaluation metrics approximate actual metric functional equivalence. modiﬁcations bleu tree exact match canonicalize code—for example anonymizing variables—may prove meaningful. direct evaluation functional equivalence course impossible general practically challenging even hearthstone dataset requires integrating game engine. existing work also attempt enforce semantic coherence output. long-distance semantic dependencies occurrences single variable example particular modeled. well-typedness executability. overcoming evaluation modeling issues remains important open problem. asns provide modular encoder-decoder architecture readily accommodate variety tasks structured output spaces. particularly applicable presence recursive decompositions provide simple decoding process closely parallels inherent structure outputs. results demonstrate promise tree prediction tasks believe application general output structures interesting avenue future work. miltiadis allamanis daniel tarlow andrew gordon wei. bimodal modelling proceedings source code natural language. international conference machine learning icml lille france july pages david alvarez-melis tommi jaakkola. tree-structured decoding doubly-recurrent neural networks. proceedings international conference learning representations jacob andreas marcus rohrbach trevor darrell klein. neural module networks. proceedings ieee conference computer vision pattern recognition oral. yoshua bengio r´ejean ducharme pascal vincent christian janvin. neural probabilistic language model. mach. learn. res. http//dl.acm.org/citation.cfm?id=.. danqi chen christopher manning. fast accurate dependency parser using neuproceedings conral networks. ference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group acl. pages http//aclweb.org/anthology/d/d/d-.pdf. span-based constituency parsing structure-label system provably optimal dynamic oracles. proceedings conference empirical methods natural language processing emnlp austin texas november pages grammars. naacl conference north american chapter association computational linguistics human language technologies diego california june pages xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. proceedings international conference artiﬁcial intelligence statistics society artiﬁcial intelligence statistics. kwiatkowski eunsol choi yoav artzi luke zettlemoyer. scaling semantic parsers on-the-ﬂy ontology matching. proceedings conference empirical methods natural language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group acl. pages percy liang michael jordan klein. learning programs hierarchical bayesian approceedings international proach. conference machine learning june haifa israel. pages wang ling phil blunsom edward grefenstette karl moritz hermann tom´as kocisk´y fumin wang andrew senior. latent predictor networks code generation. proceedings annual meeting association computational linguistics august berlin germany volume long papers. chris maddison daniel tarlow. structured generative models natural source code. proceedings international conference machine learning icml beijing china june pages aditya krishna menon omer tamuz sumit gulwani butler lampson adam kalai. machine learning framework programming exproceedings international ample. conference machine learning icml atlanta june pages graham neubig chris dyer yoav goldberg austin matthews waleed ammar antonios anastasopoulos miguel ballesteros david chiang daniel clothiaux trevor cohn kevin manaal faruqui cynthia garrette yangfeng lingpeng kong adhiguna kuncoro gaurav kumar chaitanya malaviya paul michel yusuke ana-maria popescu oren etzioni henry kautz. towards theory natural language interproceedings infaces databases. ternational conference intelligent user interfaces. pages richard shin alexander alemi geoffrey irving oriol vinyals. tree-structured variaproceedings intertional autoencoder. national conference learning representations oriol vinyals lukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. advances grammar foreign language. neural information processing systems annual conference neural information processing systems december montreal quebec canada. pages daniel wang andrew appel jeff korn christopher serra. zephyr abstract syntax description language. proceedings conference domain-speciﬁc languages conference domain-speciﬁc languages usenix association berkeley dsl’ pages luke zettlemoyer michael collins. learning sentences logical form structured classiﬁcation probabilistic categorial grammars. proceedings conference uncertainty artiﬁcial intelligence edinburgh scotland july pages luke zettlemoyer michael collins. online learning relaxed grammars parsing proceedings joint logical form. conference empirical methods natural language processing computational natural language learning (emnlp-conll-. pages zhao liang huang. type-driven incremental semantic parsing polymorphism. naacl conference north american chapter association computational linguistics human language technologies denver colorado june pages", "year": 2017}