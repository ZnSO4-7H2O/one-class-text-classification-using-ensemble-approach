{"title": "Acceleration of Deep Neural Network Training with Resistive Cross-Point  Devices", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We identify the RPU device and system specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30,000X compared to state-of-the-art microprocessors while providing power efficiency of 84,000 GigaOps/s/W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisted of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration and analysis of multimodal sensory data flows from massive number of IoT (Internet of Things) sensors.", "text": "neural networks demonstrated significant commercial success last years performance exceeding sophisticated prior methods speech object recognition however improvement demonstrated billion connections trained cluster machines three days training dnns relies general backpropagation algorithm intrinsically local parallel various hardware approaches accelerate training exploiting locality parallelism explored different level success starting early current developments fpga specially designed asic acceleration possible fully utilizing locality parallelism algorithm. fully connected layer maps neurons neurons significant acceleration node connecting nodes together massive systolic array whole instead usual time complexity problem reduced therefore constant time independent array size. however addressable problem size limited number ovel nano-electronic device concepts based non-volatile memory technologies phase change memory resistive random access memory explored recently implementing neural networks learning rule inspired spike-timing-dependent plasticity observed biological systems recently implementation acceleration training using backpropagation algorithm considered reported acceleration factors ranging even significant reduction power area. bottom-up approach using previously developed memory technologies looks applications high on/off ratio digital bit-wise storage asymmetrical reset operations becoming limitations acceleration training non-ideal device characteristics potentially compensated proper design peripheral circuits whole system partially cost significantly increased operational time contrast propose up-down approach ultimate acceleration training achieved design system performed crossbar array two-terminal resistive devices proposed years forward cycle stored conductance values crossbar array form matrix whereas input vector transmitted voltage pulses input rows. backward cycle weight storage processing first propose significantly simplify multiplication operation using stochastic computing techniques shown using stochastic streams multiplication operation reduced simple operation fig. illustrates stochastic update rule numbers encoded neurons translated stochastic streams using stochastic translators sent crossbar array device changes conductance slightly bits coincide. scheme write update rule follows length stochastic stream output strs used update cycle change weight value single coincidence event random sequence. probabilities equal unity controlled respectively gain factor str. voltage pulses positive negative amplitudes sent corresponding strs rows columns respectively. opposed floating point number encoded binary stream conductance value change significantly single pulse applied device column. however pulses coincide device sees full voltage conductance change nonzero amount parameter proportional network trained standard mnist training dataset examples images handwritten digits using cross-entropy objective function backpropagation algorithm pixel values pixel image given inputs sigmoid softmax activation functions used deviation mean ratio scales /√).. increasing stochastic stream length would decrease error turn would increase update time. order find acceptable range different values setting order match learning rates used baseline model discussed above. shown fig. small sufficient voltages shown fig. values correspond saturating type nonlinear response response linear typically considered memristor values corresponds rectifying type non-linear response. shown fig. algorithm fails converge linear response however non-linearity factor enough arious materials physical mechanisms device concepts analyzed view potential implementation cross-bar arrays neural network training technologies initially developed storage class memory applications. clear beforehand however whether price lengthening training time violates time complexity. order determine device specifications required achieve ultimate acceleration time complexity arbitrarily small thus continuously covering allowed conductance values. determine largest acceptable single coincidence event produce significant error penalty parameter scanned parameters fixed shown fig. large convergence poor since controls standard deviation stochastic update rule smaller results approaching baseline model. smaller produces error penalty epoch small weight range absolute value weights kept certain bound varied parameters identical fig. taken assure results mostly defined choice weight range. model weights single coincidence event value used coincidence event assumed standard deviation varied average value seen algorithm deviation mean value reach acceptable error penalty. incremental conductance change single coincidence event introduced. case used device sampled gaussian distribution beginning training models fig. average value used standard deviation varied positive negative pulses illustrated figs. determine much asymmetry changes algorithm tolerate considered uniform throughout whole device array. model fig. fixed varied weaker value. similarly fig. shows analogous results fixed varied. results show values standard deviation /∆%k parameters training coincidence event. models assume average value standard deviation /∆%k needs less mean value achieve ultimate acceleration training backpropagation algorithm array size approached time complexity operation enforced. case overall acceleration proportional favors large arrays. general design array peripheral circuits circuitry. typical intermediate metal levels scaled cmos technology thickness width corresponding typical line resistance pq%r s/to parasitic capacitance uq%r vw/to. assuming reasonable clock frequency pulses used update cycle allowing delay pulse width longest line length zq%r assuming reasonable line spacing results array devices. since conductance values devices positive assume pair identical device arrays used encode positive negative weight values. weight value proportional difference conductance values stored corresponding devices located identical positions pair arrays. minimize resulting total area ’\\]]\\^ array size full update cycle performed using pulses completed order estimate average device resistance l_r‘ar assume voltage drop transmission line defined lq%r/l_r‘ar lq%r total line resistance equal pq%rzq%r. contribution output resistance line drivers total line resistance minimized proper circuit design. array size average device resistance therefore l_r‘ar using resistance value assuming operating voltage power dissipation pair arrays estimated d\\]]\\^ scheme used input vectors represented fixed amplitude pulses tunable duration. pulse duration multiple proportional value input vector. currents measurement time er\\f current readout circuits illustrated fig. positive restricted results exceedingly large error shown curve fig. make design bounds. bounds softmax sigmoid restricted total penalty fig. operation voltage range input corresponding resolution voltage step required respectively. numbers op-amp) exceed conductance on/off ratio device. equation assumes devices capacitor large voltage swing. however given bounds imposed transformation devices. since shown above acceptable bound enough number replaced assuming signal feeding exceed l_r‘ar choice integrating capacitor dictated integration time er\\f on/off ratio fig. presents estimates acceptable noise levels various on/off ratios devices integration times er\\f. noise level corresponds input referred noise op-amp calculated using standard noise analysis integrator-based circuits er\\f taken er\\f chosen respectively corresponding level acceptable input referred noise shown curve fig. derived n/√xy. corresponding capacitance also calculated using thermal noise shot noise supply voltage noise etc. thermal noise pair arrays devices estimated n/√xy leaves n/√xy noise sources. depending exact physical implementation device type non-linear contribution adc. assuming er\\f forward backward cycles adcs operating resolution bqozr/ru required. state-of-the-art sar-adc provide performance occupy area consume power results total area total power array adcs. area much number weight updates second single tile estimated srptuer/ given duration update cycle array size. translates power efficiency srptuer// area efficiency srptuer//oo tile throughput forward backward cycles estimated srp/ given forward cycle power area efficiencies srp// srp//oo times less communication bandwidth power single core nearby cache state-of-the-art on-chip coherent provide sufficient communication bandwidth distant tiles. ompute resources needed sustain time complexity single tile estimated assuming cycle time numbers generated columns rows. support parallel operation tiles compute resources need scaled thus limiting number tiles active given time keep total power envelop chip example single core power achieve w.d/ might sufficient corresponding power efficiency design point would srp//. views four research groups. ieee signal processing magazine neural networks. nips recognition. iclr conference machine learning errors. nature networks on-chip learning. ieee transactions neural networks neuron-unit architecture. ieee journal solid-state circuits shan dang deep image scaling image recognition. arxiv. numerical precision. arxiv. supercomputer. annual ieee/acm international symposium microarchitecture doi./micro.. technologies comput. syst. nanotechnology systems tolerance device variation. advanced materials neuroscience indiveri linares-barranco legenstein deligeorgis prodromakis integration nanoscale memristor synapses neuromorphic computing architectures. nanotechnology spike timing synaptic strength postsynaptic cell type. journal neuroscience official journal society neuroscience procedia computer science using phase-change memory synaptic weight element. electron devices meeting ieee international doi./iedm.. acceleration memristor-based neural network.", "year": 2016}