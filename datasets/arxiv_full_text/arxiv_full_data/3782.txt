{"title": "Multilinear Subspace Clustering", "tag": ["cs.IT", "cs.CV", "cs.LG", "math.IT", "stat.ML"], "abstract": "In this paper we present a new model and an algorithm for unsupervised clustering of 2-D data such as images. We assume that the data comes from a union of multilinear subspaces (UOMS) model, which is a specific structured case of the much studied union of subspaces (UOS) model. For segmentation under this model, we develop Multilinear Subspace Clustering (MSC) algorithm and evaluate its performance on the YaleB and Olivietti image data sets. We show that MSC is highly competitive with existing algorithms employing the UOS model in terms of clustering performance while enjoying improvement in computational complexity.", "text": "paper present model algorithm unsupervised clustering data images. assume data comes union multilinear subspaces model speciﬁc structured case much studied union subspaces model. segmentation model develop multilinear subspace clustering algorithm evaluate performance yaleb olivietti image data sets. show highly competitive existing algorithms employing model terms clustering performance enjoying improvement computational complexity. clustering algorithms seek detect disjoint clouds data. however high-dimensional statistics data become sparse types methods trouble dealing noise. fact completely approach geometry clustering recently made headway analysis high-dimensional data sets. called subspace clustering approach assumes data come subspaces offset angles rather clouds offset gaps called union subspaces model applications included detection tightly correlated gene clusters genomics patient-speciﬁc seizure detection data image segmentation subspace clustering methods must embed data however high-dimensional data sets subspace clustering applied initial structure data vector rather matrix tensor examples include auditory temporal modulation features image patches data sliding-window approach seek develop clustering method incorporates geometric innovation subspace clustering without vectorizing higher-order arrays. this formulate algebraic generative model data along methods inference. subspace clustering problem multilinear variant mathematically subspace clustering problem described follows. given points ...n suppose point element subspaces. problem decide membership points. simplicity treat known. order take advantage patterns two-way data modify assumptions subspace clustering problem. rather modeling data union subspaces assume come union tensor products subspaces given subspaces suppose columns form basis likewise tensor product {a|a uyvt} dim× matrix. words matrices space conﬁned refer model union multilinear subspaces model call multilinear subspace clustering problem. note tensorsubspace tensor space subspaces tensor space tensor subspace therefore assuming tensor-subspace structure clusters uoms model. algorithm data generation points clusters latent dimension ambient dimension given rd×d repeat times draw draw random length-d vector compute datum ukyn algorithm uoms data generation points clusters latent dimension dvdu ambient dimension dvdu given rdu×du repeat times draw draw random matrix compute datum ukynvt uoms cluster closely related separable covariance models also d-pca. extensions idea -d... data shown equivalent hosvd tucker decompositions useful dimensionality reduction image ensembles multilinear subspace models used machine learning paper study extension models considering union structured subspaces. input rd×n holding data vectors number clusters thershold procedure normalize compute adjacency matrix highest values zero cos− perform normalized spectral clustering output vector clustering labels input rd×n holding data vectors number clusters procedure solve mince s.t. form |c|⊤ apply spectral clustering output vector clustering labels clustering model many algorithms exploiting model clustering focus general methods form afﬁnity matrix among data points followed spectral clustering ﬁrst called thresholded subspace clustering introduced provably reliable robust subspace clustering algorithm involves constructing weighted graph nodes represent data points edges represent connectivity points. inner product data points used edge weight idea points subspace generally higher inner product points different subspaces. symmetric adjacency matrix graph thresholded setting highest weights zero order ﬁlter noise. second method called sparse subspace clustering involves expressing point linear combination points dataset. algorithm ﬁnds sparsest possible linear representation data point terms data points achieved minimizing ℓnorm idea points used come subspace point question. weighted graph formed adjacency matrix found using sparse representations point. algorithms taken respectively detailed algorithms case two-way data data points would collection matrices columns come union subspaces rows come union subspaces take advantage fact subspaces method would cluster columns rows separately; however expensive solution. instead randomly select single column single matrix cluster these. stack random columns side side form matrix xcols transpose stack random rows side side form matrix xrows. column matrices comes data matrix perform clustering algorithm xrows xcols separately pause obtaining symmetric adjacency matrix case. repeat process trials ending adjacency matrices combine possible ways. possible combination methods detailed subsequetly. combining adjacency matrices thought condensing multiple graph realizations obtain single weighted graph representing clustering problem. condensed graph perform spectral clustering achieve segmentation original points. algorithm outlines steps described above. thresholding matrices together followed thresholding setting highest edges zero. possible choice threshold method would average number data points cluster number known minus filtering quantile quantile method involves choosing parameter taking l-th highest weight edge adjacency matrices. choice poses obstacle given value optimal graphs. projection project individual adjacency matrix’s columns onto leading singular vectors adding instances. however fact matrix projected onto leading singular vectors sharing information graph realizations could lead loss quality. algorithmic complexity data points dimension algorithmic complexity therefore comparing vectorized -way data using data matrix form data points matrices size dcdr. iteration form matrices size column space space respectively. since perform matrices algorithmic complexity iteration approximately projection method computationally expensive using randomized computational cost incurred. therefore iterations compared tsc. therefore pick cheaper. number trials obviously leads possible conclusion better choice large data realistic data smaller dimensions. computational complexity algorithm large becomes prohibitive compared well msc. images size pixels images different people. parameters vary number trials method combining graph realizations. numbers clusters ranging compare results obtained using ssc. measure success based clustering error measures fraction misclassiﬁed points deﬁned ﬁrst test various methods condensing graph detailed section constant number trials clustering error clusters using four methods. results depicted table show method projecting columns adjacency matrices onto leading eigenvectors adding produces smallest clustering error far. therefore method future results shown paper. using project-ﬁrst method combining graph realizations next test effect varying number trials used msc. goal number independent number data points small enough remains efﬁcient possible minimizing clustering error. table note clustering error begins plateau trials. randomly selecting columns rows matrix ideally different ﬁber iteration. setting number trials number ﬁbers make sense want ﬁber once. following subsection discuss using trials seems increasing trials begins less effect performance. code taken requires parameter previously optimized vectorized data. therefore vary three inputs achieve best results. ﬁrst parameters allow algorithm reject outliers allow afﬁne subspaces respectively. outlier parameter true code dismiss outlying entries afﬁne parameter true code broaden scope include afﬁne subspaces. numerical results seen table interestingly best results come allowing outlier rejection disallowing afﬁne subspaces. note slightly counterintuitive matrix single ﬁbers entire data point iteration. therefore might think since many ﬁbers connection other random ﬁbers would thrown outliers really would want entire outlying data points single ﬁbers. important avenue future research. overall performance comparisons ﬁrst compare vectorized data using iteration average clustering errors given clusters respectively. full data varying number trials seen table trials number decrease error begins plateau obtain average clustering errors trials clusters respectively data. lowering number trials even still obtain better clustering errors using numbers clusters comparable clustering errors clusters. face datum large cannot compare algorithm using ssc. vectorizing data yields points dimension large algorithm given optimization problem constructing sparse representation point. therefore immediate advantage impractical large data. report clustering errors using iteration seen previous subsection. optimized results clusters respectively comparable obtained using data. results algorithm shown table look results algorithms olivetti faces data found includes images size pixels different subjects. test perform average results runs chooses random subjects takes images each. results shown table paper presented model namely uoms model presented algorithm unsupervised clustering model. showed resulting algorithm competitive existing methods computationally efﬁcient. future work investigate deal outliers drawing rows columns data entire data itself. animportant avenue develop systematic method provable guarantees combining various graph realizations. well known subspace clustering performance depends distribution data. therefore relative importance different instances characterized perform weighted graph combination. also extend method automatic clustering data sets action videos. haiping k.n. plataniotis a.n. venetsanopoulos multilinear principal component analysis tensor objects recognition pattern recognition icpr international conference vol. dutta waltz k.m. ramasamy gross salleb-aouissi diab pooleery c.a. schevon emerson patient-speciﬁc seizure detection intra-cranial using high dimensional clustering machine learning applications ninth international conference allen yang john wright shankar sastry unsupervised segmentation natural images lossy data compression tech. rep. ucb/eecs- eecs department university california berkeley yannis panagakis constantine kotropoulos elastic subspace clustering applied pop/rock music structure analysis pattern recognition letters vol. jian yang zhang a.f. frangi jing-yu yang two-dimensional approach appearancebased face representation recognition pattern analysis machine intelligence ieee transactions vol. multilinear subspace analysis image ensembles computer vision pattern recognition proceedings. ieee computer society conference june vol. ii–– vol..", "year": 2015}