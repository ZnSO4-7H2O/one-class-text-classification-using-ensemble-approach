{"title": "Decision Forests, Convolutional Networks and the Models in-Between", "tag": ["cs.CV", "cs.AI"], "abstract": "This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.", "text": "paper investigates connections state classiﬁers decision forests convolutional neural networks decision forests computationally efﬁcient thanks conditional computation property cnns achieve state accuracy thanks representation learning capabilities. present systematic analysis fuse conditional computation representation learning achieve continuum hybrid models different ratios accuracy efﬁciency. call family hybrid models conditional networks. conditional networks thought decision trees augmented data transformation operators cnns block-diagonal sparse weight matrices explicit data routing functions. experimental validation performed common task image classiﬁcation cifar imagenet datasets. compared state cnns hybrid models yield accuracy fraction compute cost much smaller number parameters. machine learning enjoyed much success recent years academic commercial scenarios. learning approaches gained particular attention random forests used e.g. microsoft kinect deep neural networks used speech recognition image classiﬁcation among applications. decision trees characterized routed behavior conditioned learned routing function data sent either child another. conditional computation means test time small fraction nodes visited thus achieving high efﬁciency. convolutional neural networks repeatedly transform input several non-linear transformations. typically layer units need perform computation. cnns achieve state-of-theart accuracy many tasks decision trees potential efﬁcient. paper investigates connections popular models highlighting differences similarities theory practice. related work. decision forests introduced efﬁcient models classiﬁcation regression. forests extended density estimation manifold learning semi-supervised learning decision jungle variant replaces trees dags reduce memory consumption. general decision trees neural networks perceived different models. however work demonstrates decision tree represented two-layer perceptron special pattern sparsity weight matrices. recent papers addressed issue mixing properties trees convolutional networks together. example tworouted architecture stump googlenet another example tree-like architecture. work combines multiple expert cnns manually designed architecture. component trained speciﬁc task using part-speciﬁc loss. contrast investigate training single tree-shaped model minimizing global training loss. model various branches explicitly trained recognize parts work cascade classiﬁers trained different level recognition difﬁculty. model consider tree-based architectures. finally work achieves state classiﬁcation accuracy replacing fully-connected layers forest. model least expensive origmatrix shows joint correlations activations layers perceptron trained imagenet classiﬁcation task. ﬁnal layers deep model reduced number features classes visualization. thanks relus correlation matrix many zero-valued elements distributed unstructured way. reordering rows columns reveals underlying noisy blockdiagonal pattern operation corresponds ﬁnding groups layer- features highly active certain subsets classes thus darker blocks fig. correspond three super-classes zeroing off-diagonal elements corresponds removing connections between corresponding unit pairs. yields sparse architecture fig. selected subsets layer- features sent corresponding subsets layer- units; thus giving rise data routing. shown imposing block-diagonal pattern sparsity joint activation correlation neural network corresponds equipping network tree-like routed architecture. next section formalize intuition show beneﬁts sparse architectures. section introduces conditional networks model comparison trees cnns discusses efﬁciency training. clarity ﬁrst introduce compact graphical notation representating trees cnns. representing cnns. figure shows conventional represent units connected edges notation shown fig. symbol denotes popular nonlinear transformation consecutive layers linear projection matrix denoted indicates non-linear function case cnns function could also incorporate e.g. max-pooling drop-out. deep cnns long concatenations structure fig. representing trees dags. graphical language also represent trees dags usually tree data moved node another untransformed. notation achieved identity matrix additionally selecting subset features longer vector achieved non-square matrix element equal everywhere else. identity selection transforms special instances linear projections. figure block-diagonal correlation activations data routing. example -layer preceptron relu activations. portion ‘vgg’ model trained correlation matrix shows unstructured acagenet. tivation correlation unit pairs. reordering units reveals noisy block-diagonal structure. zeroing-out offdiagonal elements equivalent removing connections unit pairs. corrsponds sparser routed perceptron inal since convolutional layers split different branches. contributions. contributions paper follows show dag-based architectures rich hierarchical structure produce classiﬁcation accuracy state much lower compute memory requirements. demonstrate conditional networks still differentiable despite presence explicit data routing functions. iii) show conditional networks used fuse output ensembles data driven yielding higher accuracy ﬁxed compute. validation task image-level classiﬁcation cifar imagenet datasets. seminal work demonstrated introducing rectiﬁed linear unit activations allows deep cnns trained effectively. given scalar input relu activation max. thus type nonlinearity switches large number feature responses within cnn. relu activations induce data-dependent sparsity; sparsity tend much structure enforcing special type structured sparsity basis efﬁciency gain attained conditional networks. illustrate concept example. output exemplar multi-layer perceptron fig. computed given trained look average correlation activations pairs units successive layers training data. example figure compact graphical notation neural networks. data transformation indicated projection matrix followed non-linearity bias term shown homogeneous coordinates. figure generic conditional network. conditional networks fuse efﬁcient data routing accurate data transformation single model. vector concatenations denoted selection matrices routing explicit hard successive operations applied ever smaller subsets incoming data associated compute savings. next show implicit conditional networks also yield efﬁciency. efﬁciency implicit routed networks. figures compares standard -routed architecture. total numbers ﬁlters layer ﬁxed number multiplications necessary ﬁrst convolution ckxkyw size feature kernel size architectures. however routing depth second ﬁlters different architectures. therefore conventional cost second convolution ckxkyw half cost standard cnn. increased efﬁciency fact shallower kernels convolved shallower feature maps. simultaneous processing parallel routes yield additional time savings. implicitly-routed conditional networks trained standard back-propagation algorithm selection functions become extra parameters optimize over gradients derived straightforwardly. show explicitly-routed networks also trained using back-propagation. need compute partial derivatives respect router’s parameters illustrate using small network fig. subscripts index layers superscripts index operator trees present cnns data routing. routers send incoming data selected sub-branch enable conditional computation. routers represented perceptrons though choices possible. general router outputs real-valued weights used select single best route multiple routes send data fractionally children computational efﬁciency efﬁciency explicit data routing. split nodes explicit routers data conditionally sent children according output routing function implicit routers data unconditionally selectively sent children using figure training network’s routers back-propagation. conditional network used illustrate train router’s parameters gradient descent back-propagation. data transformation operations cnns rich dag-shaped architectures trainable data routing functions. next show efﬁciency advantages branched models comparative experiments. conditional networks generalize decision trees dags cnns thus could used tasks successful. compare models anpopular task image-level classiﬁcation. explore effect different branched architectures joint measure classiﬁcation accuracy test-time compute cost iii) model size. conditional sparsiﬁcation perceptron begin experiment designed illustrate potential advandages using explicit routes within neural network. take perceptron train imagenet classes scale relighting augmentation turn perceptron small tree routes additional compact perceptron router router trained minimize overall classiﬁcation loss interpolating trees cnns. given test image apply convolutional layers beginning tree. apply router outputs soft-max normalized treated probabilities deciding route/s send image send image highest probability route could send multiple routes e.g. probable ones. reproduce behaviour tree. corresponds left-most point curves setting corresponds sending image routes. latter reproduces behaviour nearly cost different values correspond different points along error-cost curves. figure computational efﬁciency implicit conditional networks. standard two-routed architecture explicit routers. larger boxes denote feature maps smaller ones ﬁlters. branching depth second kernels changes architectures. reduction kernel size yields fewer computations thus higher efﬁciency branched network. {{pj} denoting parameters network ground-truth assignments output units. deﬁne energy single training data point though extension full dataset trivial outer summation. network’s forward mapping output router. general routing weights continuous multiple routes time. update rule matrix whose j-th number routes. equation shows inﬂuence soft routing weights backpropagated gradients route. thus explicit routers trained part overall back-propagation procedure. since trees dags special instances conditional networks recipe training back-propagation figure conditional sparsiﬁcation single-layer perceptron. take deep model turn last fully connected layer tree routes top--error test-timecost curves conditional networks trained different values test-time cost computed number ﬂoating point operations image hardwareindependent. strong sub-linear shape curves indicates gain trade-off accuracy efﬁciency. dynamic accuracy-efﬁciency trade-off. ability select desired accuracy-efﬁciency operating point runtime allows e.g. better battery management mobile applications. contrast corresponds single point accuracy-efﬁciency space pronounced sub-linear behaviour curves suggests increase efﬁciency considerably little accuracy reduction care amount computation? modern parallel architectures yield high classiﬁcation accuracy little time. parallelism increasing efﬁciency. focus reducing total amount computations maintaining high accuracy. computation affects power consumption huge practical importance mobile applications well cloud services next extend conditional processing also expensive convolutional layers deep cnn. comparing various architectures imagenet figure conditional network used imagenet experiments employs implicit data routing convolutional layers yield higher compute efﬁciency corresponding unrouted deep small lines indicate groups feature maps implemented caffe. validation images. base experiments network current best models also based speciﬁcally focus model deep relatively memory efﬁcient single nvidia gpu). global max-pooling. found using global maxpooling last convolutional layer effective reducing number parameters maintaining accuracy. trained network pooling achieved lower top- error baseline network decrease number parameters designing efﬁcient conditional architecture. designed conditional network fig. starting unrouted vgg-gmp splitting convolutional layers dag-like routed architecture. hypothesis ﬁlter need applied small number channels input feature map. data routing implemented ﬁlter groups thus n-th convolutional level ﬁlters vgg-gmp divided groups. group depends results previous ﬁlters. feature maps last convolutional layer concatenated together globally max-pooled single-routed fully-connected layers remain vgg-gmp. training. trained architecture fig. scratch parameters except using initialization learning schedule initial learning rate learning rate iteration weight decay respectively validation accuracy levelled learning rate decreased factor twice. architecture took twice many epochs train figure comparing different network architectures imagenet. top- error function test-time compute model size various networks validated imagenet dataset. view. error compute cost. error model size. vgg-gmp reduces model size signiﬁcantly. conditional networks yield points closest origin corresponding best accuracy-efﬁciency trade-off. conditional architecture second closest origin. thanks higher efﬁciency took roughly time. results accuracy compute size. order compare different network architectures fairly possible training augmentation aside supported caffe similarly report test-time accuracy based centrecropped images without potentially expensive data oversampling. reduces overall accuracy constitutes fairer test teasing effects different architectures. applying oversampling networks produced similar accuracy improvement models without changing ranking. figure shows top- error function test-time compute cost model size various architectures. compute cost measured number multiplyaccumulate operations. chose measure efﬁciency directly related theoretical complexity testing algorithm machine/implementation independent. later also show parallel implementation measure efﬁciency correlates well measured timings gpu. model size deﬁned total number parameters relates memory efﬁciency. larger model sizes tend yield overﬁtting architectures closest axes origin accurate efﬁcient. conditional network fig. corresponds achieves top- error bright green circle fig. identical network based upon. however conditional network requires less half compute almost one-ﬁfth parameters. conditional architecture second closest origin googlenet obtain efﬁciency sending data different branches network. although highly branched tree structures still figure correlation predicted layer-wise test-time compute costs actual measured timings conditional architecture fig. three histograms normalized comparison. thought special instances conditional networks. googlenet achieves best results joint three-way metric probably thanks multiple intermediate training losses learnt low-dimensional embeddings iii) better tuning architecture speciﬁc image dataset. finally best accuracy achieved even efﬁcient model uses ﬂops thus falls outside plot. fewer operations correspond faster execution? figure reports layer-wise comparison predicted test-time compute cost actual measured timings network architecture fig. strong correlation number ﬂoating-point operations actual measured times. case correlation slightly less strong data moving overheads. conﬁrms that indeed fewer operations correspond faster execution roughly ratio. discussed section extra speed comes parameters number nodes layer conditional network. fig. shows resulting architecture. turns layers. fair comparison bayesian optimization architecture too. reduce complexity unrouted nin- network learning reduction i.e. maximize number per-layer ﬁlters. forig/i{i forig number ﬁlters layer networks trained parameters except using initialization learning schedule initial learning rate learning rate iteration weight decay respectively training epochs validation accuracy changed iterations. split original training training images validation images. remaining images used testing. results accuracy compute size. fig. shows test errors respect test-time cost model size multiple architectrues. diamonds denote unrouted networks circles denote conditional networks. original shown samples unrouted ﬁlterreduced versions explored bayesian optimization shown pink. sample conditional variants shown grey circles. green circle denotes conditional architecture close origin space conditional networks proposed optimization distributed near surface either error size compute cost them. conditional samples average closer origin unrouted counterparts. accuracy best conditional network almost identical model times faster times smaller. difference cnns conditional networks latter include data routers. explicitly-routed architecture create ensemble cnns data traverses selected component cnns thus saving computation. example branched network fig. applied ilsvrc image classiﬁcation task. network routes deep cnn. here googlenet basis component route although architectures used. generalizing straightforward. routes different compute cost arising differing degrees test-time oversampling. oversampling ﬁrst route oversampling second route. router determines figure automatically learned conditional architecture image classiﬁcation cifar. structure parameters conditional network learned automatically bayesian optimization. best viewed screen. fact branched architectures successive layers need convolutions smaller shorter kernels ever smaller feature maps. architectures tested experiments implemented caffe framework enjoy levels parallelism parallel matrix multiplications data parallelism thanks mini-batches. although highly-branched conditional networks could theory beneﬁt model parallelism feature implemented caffe validate hybrid model task classifying images cifar dataset. dataset contains images classes typically divided training images test images. take state network network model reference build conditional version time optimal conditional architecture constructed automatically using bayesian search parametrized family architectures. designing family conditional networks. model large number ﬁlters ﬁrst convolutional layer representing sizable amount overall compute. build variant prepends layer ﬁlters model. variant complex routed allows split larger layers many routes increase efﬁciency. changing number routes level nin- model generate whole family possible conditional architectures. learning optimal network architecture. next search parametrized space routed architectures using bayesian optimization optimization maximized size-normalized accuracy figure comparing network architectures cifar. classiﬁcation error function test-time compute model size various networks validated cifar dataset. view. error compute cost. error model size. automatically-optimized conditional architecture times faster times smaller accuracy. figure explicit data routing conditional ensembles. explicitly-routed conditional network mixes existing deep cnns learned data-dependent fashion. image sent route router trained together rest network back-propagation predict accuracy route image. router deep based cnn; allows computation reuse extra efﬁciency. test time trade made predicted accuracy computational cost. figure shows resulting error-cost curve. costs including cost applying router taken consideration here. given trained conditional network dynamic multi-way data routing generate curve error-compute space. point curve shows top- error validation given compute cost amortized average validation set. dashed line corresponds trivial error compute trade-off could made selecting base network random probability chosen achieve required average compute cost. fact green curve lies signiﬁcantly straight line conﬁrms much improved trade-off achieved conditional network. operating point indicated green circle achieve nearly accuracy oversampled googlenet less half compute cost. conventional ensemble would incur higher cost since routes used images. figure error-accuracy results conditional ensembles cnns. error-accuracy results googlenet base networks shown purple. dynamic error-cost curve conditional ensemble green. green circle achieve accuracy accurate googlenet half cost. paper investigated similarities differences decision trees/forests convolutional networks. introduce hybrid model thought trees augmented representation learning capabilities cnns augmented explicit data routers rich branched architecture. experiments image classiﬁcation shown highly branched architectures yield improved accuracyefﬁciency trade-off compared trees cnns. desired accuracy-efﬁciency ratio selected time without need train network. finally shown explicit routers improve efﬁciency ensembles cnns without loss accuracy. hope ﬁndings help pave systematic exploration efﬁcient architectures deep learning scale.", "year": 2016}