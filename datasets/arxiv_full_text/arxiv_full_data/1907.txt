{"title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "tag": ["cs.CL", "cs.CV", "cs.LG"], "abstract": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is \"yes\", and otherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on the high-level semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.", "text": "figure address problem answering binary questions images. eliminate strong language priors shadow role detailed visual understanding visual question answering abstract scenes collect balanced dataset containing pairs complementary scenes scenes opposite answers question visually similar possible. view task answering binary questions visual veriﬁcation task convert question tuple concisely summarizes visual concept present result answer question otherwise approach attends relevant portions image verifying presence visual concept. sentence describing given image possible state results relatively coarse understanding image exploiting statistical biases captured standard language models. example giraffes usually found grass next tree coco dataset images this generic caption giraffe standing grass next tree applicable images containing giraffe dataset. machine conﬁdently generate caption recognizing giraffe without recognizing grass tree standing next general captions borrowed nearest neighbor images result surprisingly high performance complex compositional structure language makes problems intersection vision language challenging. language also provides strong prior result good superﬁcial performance without underlying models truly understanding visual content. hinder progress pushing state computer vision aspects multi-modal paper address binary visual question answering abstract scenes. formulate problem visual veriﬁcation concepts inquired questions. speciﬁcally convert question tuple concisely summarizes visual concept detected image. concept found image answer question otherwise abstract scenes play roles allow focus highlevel semantics task opposed low-level recognition problems perhaps importantly provide modality balance dataset language priors controlled role vision essential. particular collect ﬁne-grained pairs scenes every question answer question scene exact question. indeed language priors alone perform better chance balanced dataset. moreover proposed approach matches performance state-of-the-art approach unbalanced dataset outperforms balanced dataset. introduction problems intersection vision language increasingly drawing attention. witnessing move beyond classical bucketed recognition paradigm rich compositional tasks involving natural language. problems concerning vision language proven surprisingly easy take relatively simple techniques. consider image captioning involves generating recent task involving vision language visual question answering system takes image free-form natural language question image input produces natural language answer output unlike image captioning answering questions requires ability identify speciﬁc details image several recently proposed datasets real images e.g. well abstract scenes latter allows research semantic reasoning without ﬁrst requiring development highly accurate detectors. even task however simple prior give right answer surprisingly high percentage time. example dataset common sport answer tennis correct answer questions starting what sport similarly white alone correct answer questions starting what color the. alhalf questions datatset answered correctly neural network ignores image completely uses question alone relying systematic regularities kinds questions asked answers tend have. true even binary questions answer either asleep? room?. would think withconsidering image evidence answers would equally plausible. turns answer binary questions correctly simply answering binary questions. moreover language-only neural network correctly answer binary questions without even looking image. also discussed dataset bias effects give false impression system making progress towards goal understanding images correctly. ideally want language pose challenges involving visual understanding rich semantics allowing systems away ignoring visual information. similar ideas propose unbias dataset would force machine learning algorithms exploit image information order improve scores instead simply learning game test. involves equal number answers test whole also ensuring particular question unbiased system reason believe withbringing visual information question answered paper focus binary questions reasons. first unlike open-ended questions binary questions relevant semantic information available question alone. thus answering binary questions naturally viewed visual veriﬁcation concepts inquired question second binary questions easier evaluate open-ended questions. although approach visual veriﬁcation applicable real images choose abstract images test abstract scene images allow focus high-level semantic reasoning. also allow balance dataset making changes images something would difﬁcult impossible real images. main contributions follows balance existing abstract binary dataset creating complementary scenes questions answer scene answer another closely related scene. show languageapproach performs signiﬁcantly worse balanced dataset. propose approach summarizes content question tuple form concisely describes visual concept whose existence veriﬁed scene. answer question verifying tuple depicted scene present results training testing balanced unbalanced datasets. related work visual question answering. recent work proposed several datasets methods promote research task visual question answering ranging constrained settings freeform natural language questions answers example proposes system generate binary questions templates using ﬁxed vocabulary objects attributes relationships objects. studied joint parsing videos corresponding text answer queries videos. studied synthetic human-generated questions restriction answers limited colors object categories sets categories. number recent papers proposed neural network models composing lstms cnns introduced large-scale dataset free-form open-ended along several natural models. uses crowdsourced workers answer questions visual content asked visually-impaired users. data augmentation. classical data augmentation techniques widely used past years provide high capacity models additional data learn from. transformations designed change label distribution training data. work augment dataset explicitly change label distribution. human subjects collect additional scenes every question dataset equal number ‘yes’ ‘no’ answers sense approach viewed semantic data augmentation. several classiﬁcation datasets imagenet balanced. infeasible task real images heavytail concepts captured language. motivates abstract scenes. visual abstraction language. number works used abstract scenes focus high-level semantics study connection modalities language including automatically describing abstract scenes generating abstract scenes depict description capturing common sense learning models ﬁne-grained interactions people learning semantic importance visual features works also taken advantage visual abstraction control distribution data example collects equal number examples verb/preposition combinations multiple scenes depict exact sentence/caption. similarly balance dataset making sure question dataset scene another scene extent possible. visual veriﬁcation. reason plausibility commonsense assertions gathering visual evidence real images abstract scenes contrast focus visually-grounded image-speciﬁc questions like picture riding elephant?. also reasons relations objects maps relations visual features. take input description automatically generate scene compatible tuples description plausible scene. case single tuple want verify exists given image goal answering free form yes/no question image. visual attention involves searching attending relevant image regions. uses alignment/attention image caption generation. input image describe entire image local regions phrases sentences. address different problem visual question answering. given image text input. want align parts question regions image extract detailed visual features regions image referred text. dataset abstract scenes abstract library. clipart library contains paperdoll human models spanning genders races ages different expressions. limbs adjustable allow continuous pose variations. addition humans library contains objects animals various poses. library contains different scene types indoor scenes containing indoor objects e.g. desk table etc. outdoor scenes contain outdoor objects e.g. pond tree etc. different scene types indicated different background scenes. abstract dataset consists abstract scenes questions scene train/val/test splits k/k/k scenes respectively. results total train validation test questions. question human-provided ground-truth answers. questions categorized types ‘yes/no’ ‘number’ ‘other’. paper focus ‘yes/no’ questions gives dataset questions train questions. since test annotations publicly available possible number ‘yes/no’ type questions test set. binary questions unbalanced test random subset training questions unbalanced validation rest training questions unbalanced train set. balance abstract dataset posing counterfactual task given abstract scene binary question would scene looked like answer binary question different? posing counterfactual questions obtaining corresponding scenes nearly impossible real images abstract scenes allow perform reasoning. conducted following mechanical turk study given abstract scene associated question dataset subjects modify clipart scene answer changes ‘yes’ ‘no’ example question cloud covering sun? worker move ‘sun’ open space scene change answer ‘yes’ ‘no’. snapshot interface shown fig. workers modify scene little possible. encourage minimal changes complementary scenes thought hard-negatives/positives learn subtle differences visual signal relevant answering questions. signal used additional supervision training models leverage explanations provided annotator addition labels. complementary scenes also thought analogous good pedagogical techniques learner taught concepts changing thing time contrasting full instructions interface found supp. material. note pairs lend easy creation complementary scenes existing clipart library. instance question raining? answer needs changed ‘no’ ‘yes’ possible create ‘rain’ current clipart library. fortunately scenes make small minority dataset keep balanced train test comparable unbalanced ones terms size collect complementary scenes ∼half respective splits train test set. since turkers indicated scenes could modiﬁed change answer limited clipart library complementary scenes them. total complementary scenes train complementary scenes test resulting balanced train containing samples balanced test containing samples. split balanced samples balanced train validation purposes. examples balanced dataset shown fig. fig. publicly released evaluation script experiments. evaluation metric uses ground-truth answers question compute performance. consistent dataset collected answers human subjects using complementary scenes balanced test set. compare degree balance unbalanced balanced datasets. pairs unbalanced test corresponding complementary scene balanced test corresponding complementary scenes. note dataset balanced either scenes could modiﬁed answers questions common answer human annotated answers questions match intended answer person creating complementary scene either inter-human disagreement worker succeed creating good scene. present overview approach describing step detail following subsections. answer binary questions images propose two-step approach language parsing question parsed tuple visual veriﬁcation verify whether tuple present image not. language parsing step summarizes binary question tuple form refers primary object relation secondary object e.g. binary question room? goal extract tuple form <cat room>. tuples need arguments present. instance asleep <dog asleep primary argument always present. since focus binary questions extracted tuple captures entire visual concept veriﬁed image. concept depicted image answer otherwise answer extract tuples questions align arguments objects image extract text image features ﬁnally learn model reason consistency tuple image tuple extraction section describe extract tuples questions. existing work studied problem however approaches catered towards statements directly applicable questions. give overview method details found supp. material. parsing stanford parser parse question. word assigned entity e.g. nominal subject direct object etc. remove characters letters digits parsing. summarizing intermediate step ﬁrst convert question summary converting tuple. first remove stop words determiners auxillary verbs full list stop words provided supp. material. next following common practice remove words nominal subject passive nominal subject example woman couch petting dog? parsed woman couch petting dog?. summary question expressed extracting tuple extracted summary question next split arguments. ideally would like noun phrases relation verb phrase preposition verb form example <dog room> <woman couch petting dog>. thus apply hunpos part speech tagger assign words appropriate arguments tuple. supp. material details. order extract visual features describe objects scene referred need align image. extract tuples binary questions training data. among three arguments contain noun phrases. determine objects referred arguments follow idea compute mutual information word occurrence object occurrence consider arguments occur least twice training set. test time given image tuple corresponding binary question object image highest mutual information considered referred primary object similarly instance object category image assign random instance. note questions ground-truth answer ‘no’ possible actually refers object present image cases object images aligned p/s. however since category label aligned object feature model learn handle cases i.e. learn question mentions ‘cat’ aligned clipart object category ‘table’ answer ‘no’. found simple mutual information based alignment approach surprisingly well. also found fig. shows examples clipart objects three words/phrases highest mutual information. extracted tuples aligned clipart objects image compute score indicating strength visual evidence concept inquired question. scoring function measures compatibility text image features model ensemble similar models– q-model tuple-model whose common architecture inspired recently proposed approach speciﬁcally model takes inputs along different branch. models image features different language features. q-model encodes sequential nature question feeding lstm using hidden representation language embedding tuple-model focuses important words question uses concatenation wordvec embeddings language features. consist word average corresponding wordvec embeddings. -dimensional feature vector passed fully-connected layer followed tanh non-linearity layer create dense language embedding. image represented rich semantic features described sec. binary model converts image features -dim inner-product layer followed tanh layer. inner-product layer learns visual features onto space text features. image text features common space point-wise multiplied resulting -dim fused language+image representation. fused vector passed fully-connected layers multi-layered perceptron ﬁnally outputs -way softmax score answers ‘yes’ ‘no’. predictions q-model tuple-model multiplied obtain ﬁnal prediction. models learned separately end-to-end cross-enptropy loss. implementation uses keras learning performed batch-size dropout probability model trained till validation loss plateaus. test time given question image features perform visual veriﬁcation simply performing forward pass network. features approach. visual features describe objects image referred arguments interactions context scene within objects present. particular feature vector scene dimensions composed dimensions primary object secondary object encoding object category instance absolute location modeled gmms pose expression gender skin color dimensions relative location primary secondary objects dimensions encoding object categories instances present scene around compare model several strong baselines including language-only models well state-of-the-art method. prior predicting common answer training test questions. common answer unbalanced balanced set. blind-q+tuple language-only baseline similar architecture approach except model accepts language input utilize visual information. comparing approach blind-q+tuple quantiﬁes extent model succeeded leveraging image answer questions correctly. sota q+tuple+h-img model similar architecture approach except uses holistic image features describe entire scene layout instead focusing speciﬁc regions scene determined model analogous state-ofthe-art models presented except applied abstract scenes. holistic features include bag-of-words clipart objects occurrence human expressions human poses human poses refer clusters obtained clustering human pose vectors locations global angles deformable parts human body) training set. extract -dim holistic features complete scene four quadrants concatenate together create -dim vector. holistic image features similar decaf features real images good capturing present where attend different parts image based questions capturing intricate interactions objects. comparing model sota q+tuple+h-img quantiﬁes improvement performance attending speciﬁc regions image dictated question being asked explicitly capturing interactions relevant objects scene. words quantify improvement performance obtained pushing deeper understanding image generic global image descriptors. thus name model q+tuple+aimg attention. draw following inferences vision helps. observe models utilize visual information tend perform better blind model trained balanced dataset. lack strong language priors balanced dataset forces models focus visual understanding. attending speciﬁc regions important. trained balanced visual understanding critical proposed model q+tuple+a-img focuses speciﬁc region scene outperforms baselines large margin. bias exploited. expected performance models trained unbalanced dataset better balanced dataset models learn language biases training unbalanced dataset also present unbalanced test set. also evaluate models trained train splits unbalanced balanced datasets testing balanced test set. results summarized table observations experiment training balanced better. clear table language+vision models trained balanced data perform better models trained unbalanced data. models trained balanced data learn extract visual information answer question correctly since longer able exploit figure qualitative results approach. show input questions complementary scenes subtle perturbations other along tuples extracted approach objects scenes model chooses attend answering question. primary object shown secondary object blue. language biases training set. models trained unbalanced blindsided learning strong language priors available test time. blind models perform close chance. expected trained unbalanced dataset blind model’s performance signiﬁcantly lower balanced dataset unbalanced note accuracy higher binary classiﬁcation accuracy accuracy provides partial credit inter-human disagreement ground-truth answers. attention helps. trained balanced dataset model q+tuple+a-img able outperform baselines signiﬁcant margin. speciﬁcally model gives improvement performance relative state-of-the-art model showing attending relevant regions describing detail helps also seen sec. role balancing. clear improvements reasoning vision addition language. note addition lack language bias visual reasoning also harder balanced dataset pairs scenes ﬁne-grained differences opposite answers question. model really needs understand subtle details scene answer questions correctly. clearly room improvement hope balanced dataset encourage future work detailed understanding visual semantics towards goal accurately answering questions images. classifying pair complementary scenes. experiment even harder setting test point consists pair complementary scenes associated question. recall construction answer question image pair other. test point considered correct model able predict answers correctly. since language-only models utilize textual information question ignoring image therefore predict answer scenes accuracy zero setting. results baselines model trained balanced unbalanced datasets shown table observe model trained balanced dataset performs best. again note create pair-level test consider pairs answers opposites. removed scenes workers unable create complementary scenes ﬁnite clipart library well scenes majority answer workers agree intended answer creator scene. work involves three steps tuple extraction tuple object alignment question answering. conduct analyses three stages determine importance three stages. manually inspected random subset questions found tuple extraction accurate time. given perfect tuple extraction alignment step correct time. given perfect tuple extraction alignment approach achieves accuracy compared imperfect tuple extraction alignment. thus accuracy lost imperfect tuple extraction alignment. conducted ablation study analyze importance kinds language features– lstm question wordvec tuple. blind models trained tested unbalanced datasets found combination performs better individual methods. speciﬁcally q+tuple achieves accuracy compared fig. shows qualitative results approach. show question complementary scenes opposite answers. even though pairs scenes opposite ground truth answers questions visually similar model successfully predicts correct answers scenes. further model learned attend regions scene seem correspond regions relevant answering question hand. ability predict different answers scenes subtle perturbations demonstrates visual understanding. idea balancing dataset generalized real images. instance mturk workers images different answers given question. advantage clipart lets make complementary scenes ﬁne-grained forcing models learn subtle differences visual information. differences complementary real images coarser therefore easier visual models. overall trade-off between clipart real images. clipart easier low-level recognition tasks difﬁcult balanced dataset introduce ﬁne-grained semantic differences. real difﬁcult low-level recognition tasks easier balanced dataset coarse semantic differences. paper take step towards ai-complete task visual question answering. speciﬁcally tackle problem answering binary questions images. balance existing abstract binary dataset augmenting dataset complementary scenes nearly questions balanced dataset answer scene answer another closely related scene. approach perform well balanced dataset must understand image. make balanced dataset publicly available. propose approach extracts concise summary question tuple form identiﬁes region scene focus veriﬁes existence visual concept described question tuple answer question. approach outperforms language prior baseline state-of-the-art approach large margin balanced dataset. also present qualitative results showing approach attends relevant parts scene order answer question. acknowledgements. work supported part paul allen family foundation award d.p. ictas virginia tech awards d.b. d.p. google faculty research awards d.p. d.b. army research awards d.p. d.b. national science foundation career award d.b. army research ofﬁce award d.b. ofﬁce naval research grant d.b. authors would like thank larry zitnick facebook research lucy vanderwende microsoft research claire bonial army research useful discussions. section analyze language priors present abstract dataset this implemented n-gram baseline model predict answers. implementation used training model extracts n-grams start questions training remembers common answer n-gram. test time model extracts ngram beginning test question predicts common answer corresponding extracted ngram question. case extracted n-gram present model’s n-gram memory model predicts answer found model able achieve accuracy substantially high compared prior baseline accuracy qualitatively frequently occurring n-gram balanced test little girl accuracy answer yes. similarly n-grams young accuracies respectively answer yes. cases bias towards answering instance n-gram leaves accuracy predicting questions occurs example clearly demonstrates humans tend leaves trees trees images them. cases bias dataset limited clipart library. instance predicted answer always correct n-grams door open? raining? clipart library open doors rain. similarly n-gram daytime? gets accuracy answer yes. believe lstms known perform well remembering sequences able exploit dataset bias remembering common answer n-grams. moreover language model gets even higher accuracy n-gram baseline probably learn remember meaningful words question instead ﬁrst words. figure example complementary scenes balanced dataset. pair left scene dataset right scene modiﬁed version created workers answer given question. abstract library consists scene typesindoor outdoor scenes. scene type indicated different background shown fig. consists clipart objects respective setting. instance indoor scenes contain indoor objects pillow ﬁreplace etc. outdoor scenes contain outdoor objects eagle bike football etc. clipart objects categorized types human paperdoll human models spanning genders different races different ages including babies. human model take available expressions. limbs adjustable allow continuous pose variations. human models present scene types. animal animal models indoor scenes outdoor scenes. keep scenes realistic wild animals deer raccoon eagle etc. available create indoor scenes. animal models shown fig. large object large objects present indoor scenes large objects outdoor scenes subset large objects shown fig. small object small objects present indoor scenes small objects outdoor scenes subset small objects shown fig. full instructions amazon mechanical turk interface collect complementary scenes seen fig. make task clear also show good examples workers. finally interface shown fig. complementary scenes balanced dataset shown fig. tuple would <cat ground> answers ideally opposite. problem negation hard deal even research often ignored negative sentences rarely spoken humans. example training dataset less binary questions contain word isn’t aren’t doesn’t don’t didn’t wasn’t weren’t shouldn’t couldn’t wouldn’t. primary object features secondary object features relative location features category category object belongs human animal large object small object instance attribute facing left right absolute locations modeled gaussian mixture model components different depths separately human features composed gender skin color pose expressions animal features pose occurrence relative location feature modeled another composed different components. scene level features encode object categories instances objects present scene consist parts category instance ﬁrst step parsed processed question sec. using stanford parser resulting word question assigned grammatical entity nominal subject adverb modiﬁer etc. follow steps extracted summary usually starts entity nominal subject passive nominal subject parsing result ﬁrst check nsubj nsubjpass exists. would like words entity nsubj nsubjpass nouns pronouns. therefore tagger whole sentence speciﬁcally hunpostagger word assigned nsubj nsubjpass tagged noun drop words otherwise search nouns pronouns nearby words. word entity nsubj tagged pronoun would like keep meaningful pronouns etc. rather this etc. created stop word list pronouns therefore word nsubj nsubjpass entity tagged pronoun present stop list keep question fragments there. example given question children having good time? parser assigns word children nsubj hunpos tagger tags noun. drop words output children good time. example given question playing football? word assigned entity nsubj stanford parser tagged pronoun hunpos cases neither nsubj nsubjpass question fall following three cases starting entity root starting entity nmod starting entity nummod. directly look ﬁrst noun pronoun drop words example given question leaves trees? nsubj nsubjpass word leaves tagged root ﬁrst noun. drop words leaves output leaves trees. fragments sentence check entity word delete words whose entity entity list example given binary question girl pushing stool? parsing result stanford parser girl pushing stool first search nsubj word girl. tagger tags means pronoun drop words results fragments girl pushing stool. lastly check words entities entity list delete word therefore extracted summary girl pushing stool. extracted summary ﬁrst look words entity nsubj nsubjpass. exists split words beginning example extracted summary girl pushing stool word girl entity nsubj make girl. cases would like noun phrase instead noun word example summary lady couch would like lady lady couch petting would like split lady couch later case summary common structure subject word preposition word noun word usually noun words categories used real object category refer objects clipart library example desk chair etc. location category example middle right etc. therefore created lists shown below real object list location list word tagged nsubj nsubjpass stanford parser tagged noun tagger check next word entity next word belonging either object word list location word list. lastly check summary group words till otherwise assign nsubj nsubjpass example extracted summary lady couch petting entity nsubj corresponds word lady next word tagged followed word couch object list. moreover check word couch last word summary. assign lady couch drop words included extracted summary look nouns remaining summary. locate noun words kept example extracted summary lady couch petting assign lady couch drop summary thus left petting summary. petting noun move next word. noun. stop make everything case dog. dog. cases words modify nouns example pretty girl beautiful ﬂowers etc.. cases would like adjectives included nouns look adjectives tagged tagger noun this keep otherwise keep nouns example summary scooters facing opposite directions scooters drop summary leaves facing opposite directions. opposite adjective directions noun. keep opposite directions minor special case occurrence phrases front having with. phrases qualitatively noticed cause issues. cases front tagged nouns confuses system. detect front having with skip move next word. splitting argument since already split simply assign everything else left summary example extracted summary lady couch petting lady couch thus petting. used stanford parser work. better understanding sec. list stanford parser entities reference. nsubj nominal subject nominal subject noun phrase syntactic subject clause. nsubjpass passive nominal subject passive nominal subject noun phrase syntactic subject passive clause. dobj direct object direct object verb phrase noun phrase object verb. root root root grammatical relation points root sentence. xcomp open clausal complement open clausal complement verb adjective predicative clausal complement without subject. advmod adverb modiﬁer adverb modiﬁer word adverb adverb-headed phrase serves modify meaning word. ccomp clausal complement clausal complement verb adjective dependent clause internal subject functions like object verb adjective. advcl adverbial clause modiﬁer adverbial clause modiﬁer verb phrase sentence clause modifying verb depend dependency labeled system unable determine precise dependency relation words. amod adjectival modiﬁer adjectival modiﬁer noun phrase adjectival phrase serves modify meaning coordination coordination relation element conjunct coordinating conjunction word conjunct.", "year": 2015}