{"title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest.  Unfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. To improve topic interpretability, we introduce Graph-Sparse LDA, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). In our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. Graph-Sparse LDA recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance.", "text": "originally designed model text topic modeling become powerful tool uncovering latent structure domains including medicine ﬁnance vision. goals model vary depending application cases discovered topics used prediction downstream task. cases content topic intrinsic scientiﬁc interest. unfortunately even using modern sparse techniques discovered topics often difﬁcult interpret high dimensionality underlying space. improve topic interpretability introduce graph-sparse hierarchical topic model leverages knowledge relationships words model topics summarized latent concept-words underlying graph explain observed words. graph-sparse recovers sparse interpretable summaries real-world biomedical datasets matching state-of-the-art prediction performance. probabilistic topic models originally developed discover latent structure unorganized text corpora models generalized provide powerful ﬂexible framework uncovering structure variety domains including medicine ﬁnance vision. popular latent dirichlet allocation model topics distributions words vocabulary documents summarized mixture topics contain. here word anything counted document observation. applied diverse applications ﬁnding scientiﬁc topics articles classifying images recognizing human actions modeling objective varies depending application. cases topic models used provide compact summaries documents used downstream tasks prediction classiﬁcation recognition. situations content topics independent interest. example clinician want understand certain topic within patient’s data correlated mortality geneticist meanwhile wish topics discovered publicly available datasets formulate next hypothesis tested expensive laboratory study. kinds applications present unique challenges opportunities topic modeling. standard formulation topics distributions words vocabulary. vocabulary typically assumed unstructured i.e. words assumed priori relationship. sparse topic models offer partial solution problem enforcing constraint many word probabilities given topic zero. unfortunately vocabularies large still hundreds words non-zero probabilities. enforcing sparsity alone therefore sufﬁcient induce interpretable topics. work propose strategy achieving interpretability exploiting structured vocabularies exist many specialized domains. controlled structured vocabularies encode known relationships tokens comprising vocabulary. example diseases organized billing hierarchies clinical concepts related directed acyclic graphs examples well. keywords biomedical figure simpliﬁed section icd-cm diagnostic hierarchy. here epilepsy might good conceptword summarize speciﬁc forms epilepsy descendants. knowing patient epilepsy also explain instances central nervous system disorder even disease. figure example tree structure every node represents vocabulary word. concept-word explain instances descendants ancestors e.g. node concept word matrix would non-zero values descendants ancestors marked brown. publications organized hierarchy known mesh searching mesh terms standard practice biomedical literature retrieval tasks. genes organized pathways interaction networks. structures often summarize large bodies scientiﬁc research human thought; great deal effort gone construction. structured vocabularies necessarily imperfect important property deﬁnition represent domain experts codify knowledge thus provide window might create models experts meaningfully interpret. designed understood humans structured relationships provide form information unique learned ontology. unfortunately existing topic modeling machinery equipped capitalize controlled structured vocabularies. therefore propose model graph-sparse exploits dag-structured vocabularies induce interpretable topics still summarize data well. approach appropriate documents come annotated structured vocabulary terms e.g. biomedical articles mesh headers genes known interactions species known taxonomies. graph-sparse introduces additional layer hierarchy standard model instead topics distributions observed words topics distributions concept-words generate observed words using noise process informed structure vocabulary using structure vocabulary guide induced sparsity recover topics interpretable domain experts. demonstrate graph-sparse real-world applications. ﬁrst collection diagnoses patients autism spectrum disorder. diagnosis hierarchy recover clinically relevant subtypes described small concepts. second corpus biomedical abstracts annotated hierarchicallystructured medical subject headings here graph-sparse identiﬁes meaningful concise groupings mesh terms biomedical literature retrieval tasks. cases topic models found graph-sparse better predictive performance state-of-the-art sparse topic model providing much sparser topic descriptions. efﬁciently sample model introduce novel inference procedure prefers moves along manifolds constant likelihood identify sparse solutions. paper data documents modeled using words representation common topic models. data consist counts words vocabulary documents. standard model posits following generative process words comprising document number topics. rows matrix document-speciﬁc distributions topics matrix represents topic’s distribution words. notation refers encode topic word document assigned word document since words assigned independently identically matrix often word occurs document sufﬁcient statistic words win. bayesian nonparametric model graph-sparse builds upon recent nonparametric extension latent compound dirichlet allocation addition allowing unbounded number topics lida introduces sparsity document-topic matrix topic-word matrix using three-parameter indian buffet process. prior expresses preference describing document topics topic words. extend lida assuming words document belong structured vocabulary known relationships form tree nearby groups terms—as deﬁned respect graph structure—are associated speciﬁc phenomena. example biomedical ontology nodes sub-tree correspond particular virus different sub-tree describe speciﬁc drug treatment used treat hiv. papers investigating anti-retrovirals treatment would tend terms drawn sub-trees. intuitively would like uncover sub-trees concepts underpinning topic. using concept-words summarize words topic natural many scenarios structured vocabularies often speciﬁc inconsistently annotated. example trial annotated term antiviral agents child anti-retroviral agents. thus generative modeling perspective nearby words vocabulary thought produced core concept. model posits topic made sparse concept-words explain words ancestors descendants formally replace previous generative process following process introduces ˜win concept word behind observed word element-wise hadamard product indian buffet process standard model document-topic matrix represents distribution topics document. however masked according document-speciﬁc vector matrix drawn concentration parameter thus ¯bnk topic nonzero probability document otherwise. similarly topic-concept matrix binary topic-concept mask matrix represent topic matrix sparsity pattern except represent relationship topics concept-words. priors document-topic topic-concept matrices follow lida concept-word matrix describes distributions words concept. form ontology determines sparsity pattern notation refer binary vector length concept-word descendant ancestor observed word otherwise. illustrate sparsity constraints figure dark-shaded concept nodes explain themselves words ancestors descendants. brown green nodes ancestor observed words shared concept word. intuitively concept-word matrix viewed allowing variation process assigning terms documents behalf domain experts. example document antiretroviral agents annotator describe document key-word nearby vocabulary antiviral agents rather speciﬁc term. similarly primary care physician using hierarchy figure note patient epilepsy since expert neurological disorders specialist might bill speciﬁc term convulsive epilepsy intractable. generally concept-word matrix thought describing neighborhood words could covered concept. introducing additional layer describe blocked-gibbs procedure sampling well additional metropolis-hastings procedure helps sampler move toward sparser topic-concept word matrices speciﬁcally proposal distribution designed prefer proposals overall likelihood signiﬁcantly change. knowledge mcmc uses moves result near-constant likelihood encourage large changes prior novel approach. ﬁrst describe resample instantiated parameters graph-sparse model describe sample topics. blocked gibbs sampling procedure relies ﬁrst sampling intermediate assignment tensors. ﬁrst counts often word assigned topic document second counts often observed word assigned concept word topic tensors sampled follows count tensors probability observed word belongs topic given marginalizes potential concept-words. thus multinomial distribution allocate counts across topics mult indicate tensor slice. updating probability generating concept word given observed word topic given ˜ww. thus sample count tensor using multinomial note given topic assignment observed word need know document came determine distribution concept-word assignments. thus never need consider four-way {document topic concept-word word} count tensor inference. recall modeling objectives identify small interpretable concept-words topic. placed sparsity-inducing prior gibbs sampling procedure computationally straightforward often give desired sparsity fast enough. mixing slow time counts assigned topic across documents. many documents reaching zero counts unlikely thus sampler slow sparsify topic-concept word matrix introduce procedure encourage moves topic concept-word matrix directions greater sparsity joint moves given proposal distribution acceptance ratio procedure given allow moves toward greater sparsity proposal uses core ideas. first form ontology propose intelligent split-merge moves second attempt make move keeps likelihood constant possible proposing thus prior terms larger inﬂuence move. form follows choose random topic concept word denote concept words descendants probability psplit sample random vector dirichlet create graph ontology merge move corresponding moving mass single node. denotes frobenius norm constraints must simplex respect ontology optimization solved quadratic program linear constraints. sample practice generally needs proposal according large order propose appropriately conservative moves. demonstrate ability graph-sparse model interpretable predictive topics example real-world examples biomedical domains. case compare model state-of-theart bayesian nonparametric topic modeling approach lida focus lida subsumes popular sparse topic models focused topic model sparse topic model proposed model generalization lida. samplers iterations. topic matrix product initialized using tensor decomposition factored using alternating minimization sparse enforced simplex ontology constraints. initialization procedures reduced burn-in time. finally random data-set held compute predictive log-likelihoods. demonstration problem ﬁrst considered problem -word vocabulary arranged binary tree three underlying topics single concept matrix uniformly distributed probability mass ancestors concept word probability mass concept word’s descendants initialization problem randomly generated document-topic matrix comprising documents. figures show difference held-out test likelihoods ﬁnal samples independent instantiations problem. difference held-out test likelihoods skewed positive implying graphsparse makes somewhat better predictions lida. importantly graph-sparse also recovers much sparser matrix seen ﬁgure note course graph-sparse additional layer structure allows sparse topic concept-word matrix lida access ontology information important point incorporating available controlled structured vocabulary model solution similar better predictive performance state-of-the-art models additional beneﬁt much interpretable structure. patterns co-occurring diagnoses autism spectrum disorder autism spectrum disorder complex heterogenous disease often accompanied many co-occurring conditions epilepsy intellectual disability. consider patients different diagnoses datum corresponds number times patient received diagnosis ﬁrst years life. diagnoses organized tree-structured hierarchy known icd-cm diagnoses higher hierarchy less speciﬁc clinicians encode diagnosis level hierarchy including less speciﬁc ones. figure shows difference held-out test log-likelihoods graph-sparse sparse divided overall mean held-out log-likelihood models burn-in. three domains predictive performance graph-sparse within percent lida. second shows number non-zero dimensions topic-concept word topic-word graph-sparse lida models respectively. results shown independent instantiations problem independent mcmc runs autism systematic review problems. figure shows difference test log-likelihood graph-sparse lida independent runs divided overall mean test-likelihood value. less pronounced example graph-sparse still slightly better predictive performance—certainly current state-of-the-art topic modeling. however ontology allows much sparser topics seen figure application topics correspond possible subtypes asd. able concisely summarize ﬁrst step toward using output model future clinical research. finally table shows example topic recovered graph-sparse corresponding topic discovered lida. corresponding topic lida similar diagnoses using hierarchy allows graph-sparse summarize probability mass topic concept words rather words. topic—which shows connection severe form intellectual disability epilepsy—as well topics matched recently published clinical results subtypes medical subject headings biomedical literature national library medicine maintains controlled structured vocabulary medical subject headings terms hierarchical terms near root general tree. example cardiovascular diseases subsumes heart diseases turn parent heart aneurysm. mesh terms useful searching biomedical literature. example conducting systematic review looks summarize totality published evidence pertaining precise clinical question. identifying evidence literature time-consuming expensive tedious endeavor; computational methods table sample discovered topic using graph-sparse data compared lida. graph-sparse required concepts summarize probability mass topic lida required lida show diagnoses associated topic sample diagnoses summarized shown concept words. including epilepsy unspeciﬁed without mention intractable epilepsy localization-related epilepsy epileptic syndromes generalized convulsive epilepsy withmention intractable epilepsy localization-related epilepsy epileptic syndromes generalized convulsive epilepsy intractable epilepsy epilepsy unspeciﬁed intractable epilepsy infantile spasms without mention intractable epilepsy convulsions convulsions conditions anomaly unspeciﬁed chromosome intellectual disability including epilepsy unspeciﬁed without mention intractable epilepsy generalized convulsive epilepsy intractable epilepsy brain condition quadriplegia hemiplegia unspeciﬁed affecting dominant side migraine without aura intractable migraine flaccid hemiplegia flaccid hemiplegia hemiparesis affecting unspeciﬁed side metabolic encephalopathy... reducing labor involved process therefore investigated mesh terms helpful annotations facilitating literature screening systematic reviews help researchers undertaking review quickly decide articles relevant query not. however mesh terms manually assigned articles small group annotators. thus inherent variability speciﬁcity terms assigned articles. variability make leveraging terms difﬁcult. graph-sparse provides means identifying latent concepts deﬁne distributions terms nearby mesh structure. interpretable sparse topics provide concise summaries biomedical documents thus easing evidence retrieval process overburdened physicians. consider dataset documents annotated unique mesh terms screened systematic review effects calcium-channel blocker drugs ﬁgure test log-likelihood graph-sparse data lida producing much sparser summary concept-words here concepts found graph-sparse correspond sets mesh terms might help researchers rapidly identify studies reporting results trials investigating ccb’s—without make sense topic comprising hundreds unique mesh terms. table shows concept-words sample topic discovered graph-sparse compared similar topic discovered lida. graph-sparse gives topic mass double-blind trials ccbs; knowing relative prevalence article topic would clearly help researcher looking reports randomized controlled trials ccbs. contrast words related concept ccbs divided among terms lida. lida terms drug therapy combination mibefradil also present graph-sparse table sample discovered topic using graph-sparse mesh data studies comprising calcium channels systematic review compared lida. superscripts denote term found different locations mesh structure; collapse appear sequentially topic. space constraints show discovered topics. graph-sparse captures concepts double-blind trial calcium channel blockers topic exactly researchers looking summarize systematic review. adrenergic beta-antagonists drug therapy combination calcium channel blockers felodipine atenololm benzazepines mibefradil angina pectoris myocardial ischemia atrial flutter much lower probability concept summarizes instances. note professional systematic reviewer conﬁrmed concise topics found graph-sparse would useful facilitating evidence retrieval tasks found lida. topic models gained wide popularity ﬂexible framework uncovering latent structure corpora. existing topic models typically assumed observed words unstructured. contrast considered scenarios words drawn known underlying structure prior work interpretable topic models focused various notions coherence. introduced idea intrusion detection hypothesized coherent interpretable topic would human annotator would able identify inserted intruder word among words topic; automated process. contrary expecation found interpretability negatively correlated test likelihood. developed measures topic coherence strongly correlated human annotations topic quality. however evaluations works still focus words topic contrast approach sacriﬁce predictive quality using ontological structure provides compact summary describes words quality particularly valuable kinds scenarios described annotation disagreement diagnostic slosh result large number words non-trivial probabilities. human-provided structure induce interpretability also distinguishes graph-sparse hierarchical tree structured topic models structure typically learned. example nested chinese restaurant process learn hierarchies topics subtopics speciﬁc parents. expand idea nonparametric markov model allows subtopic multiple parents. develop inference techniques sparse versions tree-structured topic models. learned hierarchies also used capture correlations topics models learned hiearchical structure allows various kinds statistical sharing topics. however topic still distribution large vocabulary interpretation task complicated requiring human inspect hierarchy topics structure. among fully unsupervised approaches closest work super-word concept modeling uses nested beta process describe document sparse super-words concepts associated sparse words. known auxiliary information words encoded feature vector used encourage discourage words part concept. difference approach graph structure guide formation concepts maintains interpretability removing need concept sparse words. graph-structured relationships also result much simpler inference procedure. applied increase interpretability expert-deﬁned hierarchies used topic models contexts. early work used hierarchies word-sense disambiguation n-gram tuples. idea later incorporated topic modeling context work used hierarchical structure partial supervision improve topic-modeling output scenarios words come controlled vocabularies others not. consider representing content website summaries hierarchical model. approach exploits ontological structure jointly modeling word ontology term generation. showed model improved existing approaches respect perplexity. dirichlet forest priors enforce expert-provided must topic cannot topic constraints words. finally propose hierarchically supervised model hierarchy document labels speciﬁcally treat categories ‘labels’ model assignment documents regression models. model stipulates node assigned category parents thus capturing hierarchical structure. contrast works focus prediction tasks graph-sparse uses ontology probabilistic rather enforced manner obtain sparse topics extant controlled vocabularies. note word generation model much general approaches. considered scenarios ontological structure allows concept-word generate words descendants ancestors. however imagine concept-word generate nearby observed word deﬁnition nearby entirely model-designer difference allows much ﬂexibility modeling underlying structure tree collection neighborhoods. time formulation results gibbs sampling procedure simpler many hierarchical models. topic models revolutionized prediction classiﬁcation models many domains many scientists attempting uncover structure data. applications however prediction enough scientists wish able understand structure order posit theories. time structured knowledge-bases often exist scientiﬁc domains; information dense resources capture wealth expertise. paper proposed model exploits resources achieve stated identifying interpretable topics. speciﬁcally described novel bayesian nonparametric model graph-sparse leverages existing controlled vocabulary structures induce interpretable topics. bayesian nonparametric aspect model allows discover number topics dataset. leveraging ontological knowledge allows uncover sparse sets concept words provide succinct interpretable topic summaries maintain ability explain large number observed words. combination representational power efﬁcient inference procedure allowed realize topic interpretability still matching state-of-the-art predictive performance. focused controlled vocabularies biomedical domain approach could generally applied text corpora using standard hierarchies wordnet general domains using hierarchies could eliminate need basic pre-processing stemming. model relatively straightforward implement expect useful variety topic factor-discovery applications observed dimensions human-understandable relationships.", "year": 2014}