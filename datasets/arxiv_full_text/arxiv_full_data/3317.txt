{"title": "Deep Mean Maps", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "The use of distributions and high-level features from deep architecture has become commonplace in modern computer vision. Both of these methodologies have separately achieved a great deal of success in many computer vision tasks. However, there has been little work attempting to leverage the power of these to methodologies jointly. To this end, this paper presents the Deep Mean Maps (DMMs) framework, a novel family of methods to non-parametrically represent distributions of features in convolutional neural network models.  DMMs are able to both classify images using the distribution of top-level features, and to tune the top-level features for performing this task. We show how to implement DMMs using a special mean map layer composed of typical CNN operations, making both forward and backward propagation simple.  We illustrate the efficacy of DMMs at analyzing distributional patterns in image data in a synthetic data experiment. We also show that we extending existing deep architectures with DMMs improves the performance of existing CNNs on several challenging real-world datasets.", "text": "togram features words representations words local features histogram gradients dense sift features recent developments extended distributionbased methods beyond histograms nonparametric continuous domains methods adept providing robust representations give holistic aggregate view image helpful complex classiﬁcation tasks. deep architectures compose many layers computational neural units recently achieved tremendous amount success computer vision tasks replacing hand-designed features learned convolutional ﬁlters. fact deep architectures often convolutional neural networks almost become de-facto standard datasets recent success believed depth hierarchical nature deep cnns lead top-level convolutional features exhibit semantically meaningful representations typically last levels features concatenated fully-connected layers obtain ﬁnal classiﬁcation result. would informative study distributions high-level features image supervised tasks rather treating extracted features simply vector. paper address lack application distributions deep architectures development deep mean layer provide fast scalable featurization top-level convolutional features representative distributions nonparametric fashion. combination distribution-based deep architecture approaches seen initial success ad-hoc method extracting features distributions ﬁxed pre-trained high-level features however work jointly learns high-level features uses distributions supervised learning tasks. present deep mean maps scalable nonparametric fashion. distributions high-level features deep architecture become commonplace modern computer vision. methodologies separately achieved great deal success many computer vision tasks. however little work attempting leverage power methodologies jointly. paper presents deep mean maps framework novel family methods non-parametrically represent distributions features convolutional neural network models. dmms able classify images using distribution top-level features tune top-level features performing task. show implement dmms using special mean layer composed typical operations making forward backward propagation simple. illustrate efﬁcacy dmms analyzing distributional patterns image data synthetic data experiment. also show extending existing deep architectures dmms improves performance existing cnns several challenging real-world datasets. modern computer vision seen extensive separate machine learning methodologies deep architectures learning distributions. however research begun consider joint methods. paper present deep mean maps novel framework jointly learn distributions high-level features deep architectures. show dmms easy implement existing deep learning infrastructure able improve results existing architectures various datasets. main tool learn distributions high-level features mean embeddings detail below. show mean embeddings along random features implemented using typical layers scalable learning distributions high-level features vision tasks. mean embeddings distributions serve embed distributions reproducing kernel hilbert space allowing apply many familiar kernel methods svms datasets distributions kernel trick. formally distribution rkhs induced kernel deﬁned course calculation necessitates population mean ex∼p available real-world data. instead typically computes ﬁnite iid∼ sample estimate given sample {xj}n tions top-level features discriminant learning. show deep mean maps implemented typical machinery making forward backward propagation tractable learning effective features discriminant models based distributions. outline rest paper structured follows. first detail framework section study behavior synthetic problem illustrate efﬁcacy several real-world datasets section section discusses relationship method gives concluding remarks. focus deep architectures convolutional networks brieﬂy discuss below; however framework extends architectures. show treat distributions high-level features convolutional networks inputs learning tasks mean maps furthermore show learn supervised tasks using mean maps manner scales large datasets random fourier features lastly demonstrate incorporate mean maps convolutional network perform supervised tasks whilst tuning network’s highlevel features purpose. deep architectures learning high-level descriptive features extensively explored last decade believed deep architectures present many modern models instrumental learning features supervised unsupervised tasks. high-level features extracted variety different deep architectures focus deep convolution neural networks cnns used great success many vision tasks. inspired study visual cortices cnns successive applications locally-receptive convolution subsampling nonlinear transformation normalization layers. localized nature cnns enjoy reduction parameters general neural networks exploit spatial structures present image data. furthermore modern non-linear units relus efﬁcient implementations large datasets modern cnns able employ layers achieve state performance high accuracy spatial structure deep models make outputs top-level convolution layers excellent high-level features image classiﬁcation. thus scaling working approximate primal space. below expand learn high-level features whose distributions used supervised tasks mean embeddings random features. given cnns proven successful feature extractors classiﬁcation tasks natural look study distributions said features across spatial dimensions. many vision tasks scene classiﬁcation distribution ﬁlter values highly informative since expect presence features across various locations relevant supervised tasks. fact achieved state-of-the-art performance whilst considering ﬁxed pre-trained simple parametric approximations distribution high-level features ad-hoc summary statistics. similar vein uses ﬁnal global average pooling layer convolution features sums spatial information robust spatial translations generalizes better. view global average pooling representing distribution features mean; method allows richer representations distributions. illustrate include layer computes mean embedding convolution ﬁlters order learn informative features distributions discriminate supervised tasks. suppose convolution layer produces tensor rm×h×w input image forward network number convolutional ﬁlters spatial dimensions super pixels wish consider distributions vectors ﬁlter values cijl ci)t calculate term mean layer. mean layer calculates ˆµxi propagates forward network allowing classify images distributions high-level features tuned distributional representation. show mean layer represented terms standard mechanisms. speciﬁcally mean layer performs scaled convolutions biased uniformly random offsets cosine nonlinearity layer global-average pooling. ˆµxi computed mean vectors rd×h×w tensor random feature embeddings unfortunately computation gram matrix pairwise kernel evaluations application kernel methods dataset {xi}n scale intractable modern image datasets containing millions images. mitigate this mean embedding random features estimate kernel evaluations approximate ﬁnite primal space. shift-invariant kernel iid∼ spectral distribution thus approximate kernel evaluation using ﬁnite dimensional dot-product even characteristic kernels induced inﬁnite feature like kernel. kernel figure mean layer. consider top-level features resulting several convolution/pooling layers. compute mean embedding ﬁlter vectors perform following take convolution frequencies {ωd}d take element-wise cosines; global average pooling compute d-length vector. layer image batch mean embedding top-level convolution features ˆµxi computed. vectors network fully connected fashion perform learning w.r.t. loss. note linear operations vectors ˆµtxi interpreted rkhs inner-product element moreover straightforward perform back-propagation layer calculating derivatives w.r.t. inputs parameters. demonstrate effectiveness approach image classiﬁcation problems. experiments implemented caffe deep learning framework code made publicly available. generate data using bank texture images found colored brodatz texture database class represents distribution patches generated images texture bank. consider dataset classes generated random. speciﬁcally class corresponds mixture dirichlet distributions mixture component support m-dimensional simplex given dirichlet mixture class generate image follows pick random location patch uniformly random draw pick random patch image {pj}m patch πjpj overlay chosen location repeat iid∼ unif drawn training time {bd}d held ﬁxed throughout. note dropped constant since factored linear operations network. compute vectors mean perform hence clear mean layer efﬁcient implement train terms existing operations layers. note practice learn kernel used mean learning frequencies {ωd}d consider ﬁxed kernel drawing {ωd}d iid∼ kernel). furthermore learn scale ﬁxed kernel; e.g. learn bandwidth kernel drawing {ωd}d iid∼ parameterizing cosine layer terms used simple network following structure convolution ﬁlters kernel sized stride relu layer; pooling stride appended mean layer network also compared appending hidden layers nodes relu non-linearity directly classifying pooled features network trained sgd. look test accuracies aforementioned three networks subset training images interesting note even instances class able almost perfectly distinguish classes outperforms networks large margin. built mechanisms learn distributions images features thus sample efﬁcient fashion. figure accuracy time synthetic data experiment. blue curves correspond accuracies network mean layer. green curves correspond network feeds max-pooled ﬁlters directly classiﬁer. lastly curves correspond network uses hidden layers classify. solid crossed lines indicate training respectively. section illustrate effectiveness deep mean maps several real-world scene classiﬁcation datasets. shall begin network trained imagenet dataset ﬁne-tune dataset. flickr style ﬁrst dataset consider known flickr style assembled contains images style categories deﬁned user groups bright noir long exposure geometric romantic. standard train-test split divide training images trainvalidation split model selection. figure shows image class. wikipaintings dataset also assembled contains images genres including impressionism baroque surrealism cubism. category contains least images. divide dataset randomly training validation test images. figure shows image class. places places- dataset contains training validation test images scene categories including runway resort trench kasbah dorm room. test labels public; submissions must scored server authors figure shows sample images. consider minor variation model known caffenet googlenet model ﬁne-tune network consistent learning rate schedule architecture pick model occasional snapshots highest validation accuracy. frequencies) following ways replacing replacing fully connected layers mean layer top-level features look classify solely mean embedding level features. forking furthermore rather sending toplevel features hidden mean layers replication learn sets distinct top-level-features hidden mean layer respectively. also consider following variants model dropout dropout layer mean embedding. encourage network avoid relying frequency random embedding thereby faithfully replicating kernel. best model variant picked according validation score nearly always outperform equivalent base model test flickr style wikipaintings. hence dmms distributions top-level features able improve existing deep-architectures. furthermore ﬂexible nature mean layer allows extend existing networks many different variations dmms seem generalize well empirically. thus simple validation approach effective choosing extend network’s architecture dmms. previously mentioned deep architectures learning high-level features explored myriad work high-level features extracted variety different deep architectures focus deep convolution neural networks modern cnns able employ layers without unsupervised pretraining thanks reduced parameterization recent advents training methods nonlinear units large datasets figure illustration method extending existing deep models. top-level refers feature last convolutions layer. replacing append mean layer top-level convolutional features. replicating copy top-level convolutional features feed copy mean layer passing copy classiﬁcation layers. forking make distinct top-level convolutional features mean layer classiﬁcation layers. distribution-based approaches often histograms representations features dense-sift moreover efforts extended distribution-based methods nonparametric continuous domains work explores distributions high-level deep architecture features ad-hoc features distributions ﬁxed pre-trained high-level features contrast paper learns high-level features uses distributions scalable nonparametric fashion mean maps. mean embeddings served practical ﬂexible method learn using datasets sample sets drawn distributions furthermore random features allowed scalable learning distributions mean embeddings also used extensively comparison two-sample testing distributions recent research also explored mean maps deep architectures building fast sampling mechanisms conducting two-sample tests generated sample points original data work differs using mean maps loss function rather architecture itself. mean embeddings random features allow mean layer non-parametrically represent distributions top-level features whilst still scaling large image datasets. also inner-products mean embeddings network interpretable rkhs inner products distributional embeddings allowing build strong theoretical foundation furthermore showed mean layer implemented using typical operations making forward backward propagation simple dmms. moreover illustrated aptitude mean layer learning distributions visual features discrimination synthetic data experiment. even instances mean layer allowed network quickly learn distinguish visual distributional patterns sample efﬁcient manner. lastly showed dmms used extend several existing state-ofthe-art deep-architectures improve performance various challenging real-world datasets. indeed mean layer prove ﬂexible capable extending networks variety ways; propensity generalize well simple choose extension method straightforward validation approach. thus clear framework successfully build myriad state-of-the-art deep architectures available.", "year": 2015}