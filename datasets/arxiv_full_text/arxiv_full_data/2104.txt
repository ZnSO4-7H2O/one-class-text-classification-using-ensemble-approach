{"title": "Practical Kernel-Based Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML", "68T05 (Primary), 93E35, 90C40, 93E20, 49L20 (Secondary)", "I.2.8; I.2.6; G.3"], "abstract": "Kernel-based reinforcement learning (KBRL) stands out among reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which is statistically consistent and converges to a unique solution. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation that takes into account both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also illustrate the potential of our algorithm in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data.", "text": "kernel-based reinforcement learning stands among approximate reinforcement learning algorithms strong theoretical guarantees. casting learning problem local kernel approximation kbrl provides computing decision policy statistically consistent converges unique solution. unfortunately model constructed kbrl grows number sample transitions resulting computational cost precludes application large-scale on-line domains. paper introduce algorithm turns kbrl practical reinforcement learning tool. kernel-based stochastic factorization builds simple idea transition probability matrix represented product stochastic matrices swap factors multiplication obtain another transition matrix potentially much smaller original retains fundamental properties precursor. kbsf exploits insight compress information contained kbrl’s model approximator ﬁxed size. makes possible build approximation takes account diﬃculty problem associated computational cost. kbsf’s computational complexity linear number sample transitions best without discarding data. moreover algorithm’s simple mechanics allow fully incremental implementation makes amount memory used independent number sample transitions. result kernel-based reinforcement learning algorithm applied large-scale problems oﬀ-line on-line regimes. derive upper bounds distance value functions computed kbrl kbsf using data. also prove possible control magnitude variables appearing bounds means that given enough computational resources make kbsf’s value function close desired value function would computed kbrl using sample transitions. potential algorithm demonstrated extensive empirical study kbsf applied diﬃcult tasks based real-world data. kbsf solve problems never solved before also signiﬁcantly outperforms state-of-the-art reinforcement learning algorithms tasks studied. †parts material presented technical report appeared papers published neural information processing systems conference current manuscript substantial extension aforementioned works. reinforcement learning provides conceptual framework potential materialize long-sought goal artiﬁcial intelligence construction situated agents learn behave direct interaction environment endeavor come without challenges; among them extrapolating ﬁeld’s basic machinery large-scale domains particularly persistent obstacle. long recognized virtually real-world application reinforcement learning must involve form approximation. given mature stage supervised-learning theory considering multitude approximation techniques available today realization come across particularly worrisome issue ﬁrst glance. however well known sequential nature reinforcement learning problem renders incorporation function approximators non-trivial despite diﬃculties last decades collective eﬀort reinforcement learning community given rise many reliable approximate algorithms among them ormoneit sen’s kernel-based reinforcement learning stands reasons. first unlike approximation schemes kbrl always converges unique solution. second kbrl consistent statistical sense meaning adding data always improves quality resulting policy eventually leads optimal performance. unfortunately good theoretical properties kbrl come price since model constructed algorithm grows number sample transitions cost computing decision policy quickly becomes prohibitive data become available. computational burden severely limits applicability kbrl. help explain spite nice theoretical guarantees kernel-based learning widely adopted practical reinforcement learning tool. paper presents algorithm potentially change situation. kernelbased stochastic factorization builds simple idea transition probability matrix represented product stochastic matrices swap factors multiplication obtain another transition matrix potentially much smaller original retains fundamental properties precursor kbsf exploits insight compress information contained kbrl’s model approximator ﬁxed size. words kbsf builds model whose size independent number sample transitions serves approximation model would constructed kbrl. since size model becomes parameter algorithm kbsf essentially detaches structure kbrl’s approximator conﬁguration. extra ﬂexibility makes possible build approximation takes account diﬃculty problem computational cost ﬁnding policy using constructed model. kbsf’s computational complexity linear number sample transitions best without throwing data away. moreover show paper amount memory used algorithm independent number sample transitions. together properties make possible apply kbsf large-scale problems oﬀ-line on-line regimes. illustrate possibility practice present extensive empirical study kbsf applied diﬃcult control tasks based real-world data never solved before. kbsf outperforms least-squares policy iteration ﬁtted q-iteration several oﬀ-line problems sarsa diﬃcult on-line task. also show kbsf sound algorithm theoretical point view. specifically derive results bounding distance value function computed algorithm computed kbrl using data. also prove possible control magnitude variables appearing bounds means make diﬀerence kbsf’s kbrl’s solutions arbitrarily small. start paper presenting background material section then section introduce stochastic-factorization trick insight underlying development algorithm. kbsf presented section section divided parts theoretical practical. section present theoretical results showing diﬀerence kbsf’s kbrl’s value functions bounded also diﬀerence controlled. section brings experiments kbsf four reinforcement-learning problems single double pole-balancing drug schedule domain epilepsy suppression task. section introduce incremental version algorithm applied on-line problems. section follows structure section theoretical results followed experiments. speciﬁcally section extend results section on-line scenario section present experiments triple pole-balancing helicopter tasks. section discuss impact deviating theoretical assumptions kbsf’s performance also present practical guide conﬁgure algorithm solve reinforcement learning problem. section summarize related works situate kbsf context kernel-based learning. finally section present main conclusions regarding current research discuss possibilities future work. consider standard framework reinforcement learning agent interacts environment tries maximize amount reward collected long interaction agent environment happens action ﬁnite sets called state action spaces respectively. execution action state moves agent state action must selected transition certain probability occurrence associated reward goal agent policy mapping states actions maximizes expected eled markov decision process tuple describe dynamics task hand. action deﬁnes next-state distribution upon taking action state reward received transition given |ra)| rmax state es∼p a{ra)}. interaction agent environment modeled natural searching optimal policy resort dynamic programming central theory dynamic-programming concept value function. value state policy denoted expected return agent receive following eπ{r|s es∼p a{ra) γes∼p π)}. policies. particular policy considered least good another policy goal dynamic programming optimal policy performs worse other. well known always exists throughout paper conventional matrix notations interchangeably depending context. using latter vectors denoted small boldface letters matrices denoted capital boldface letters. policy a|s| time polynomial number states actions r|s| r|s|×|a|. deﬁne operator r|s|×|a| r|s| maxj also given deﬁne r|s| r|s|×|a| ijvj bellman operator given fundamental result dynamic programming states that starting expression gives optimal t-step value function vector approaches point optimal t-step policy obtained selecting contrast dynamic programming reinforcement learning assumed unknown agent must learn policy based transitions sampled environment. process learning decision policy based ﬁxed sample transitions call batch reinforcement learning. hand online reinforcement learning computation decision policy takes place concomitantly collection data discussed using dynamic programming compute optimal value function time polynomial number sample transitions however since application bellman operator computational cost procedure easily become prohibitive practice. thus kbrl leads dilemma hand wants much data possible describe dynamics task hand number transitions small enough allow numerical solution resulting model. following sections describe practical approach weight relative importance conﬂicting objectives. mathematical construct explored before. example cohen rothblum brieﬂy discuss special case nonnegative matrix factorization cutler breiman focus slightly modiﬁed versions stochastic factorization statistical data analysis. however paper focus useful property type factorization recently noted order stochastic factorization. case elements probabilities transitions states artiﬁcial states speciﬁcally elements interpreted probabilities transitions original states artiﬁcial states rows seen probabilities transitions opposite direction. interpretation dihkhj probabilities associated two-step transitions state artiﬁcial state back state words accumulated probability possible paths stopover artiﬁcial states ¯sh. following similar reasoning diﬃcult swapping factors stochastic factorization switching obtains transition probabilities artiﬁcial states rm×m compact version figure illustrates stochasticity follows immediately property perhaps surprising fact matrix shares fundamental characteristics original matrix speciﬁcally possible show that recurrent class corresponding class period given simple assumptions factorization irreducible irreducible regular regular refer insight stochastic-factorization trick given stochastic factorization transition matrix swapping factors factorization yields another transition matrix potentially much smaller original retains basic topology properties former latter comes almost inevitably. motivation would course save computational resources example barreto fragoso shown possible recover stationary distribution linear transformation corresponding distribution paper stochastic-factorization trick reduce computational cost kbrl. strategy summarize information contained kbrl’s model ﬁxed size. idea using stochastic factorization reduce dynamic programming’s computational requirements straightforward given factorizations transition matrices apply trick obtain reduced solved place original one. general scenario would independent factorization daka action however current work focus particular case obviously order apply stochastic-factorization trick ﬁrst compute matrices involved factorization. unfortunately procedure computationally demanding exceeding number operations necessary used fact maxai since d¯ra dkad ˇpad stochastic matrix satisﬁes sorg singh’s deﬁnition soft homomorphism paper). applying theorem authors know note bound made tighter replace right-hand side right-hand side however replacement would result less intelligible bound cannot computed practice. needless subsequent developments depend proposition also valid tighter version bound. appendix derive another bound distance measured factorization exact recover computable version sorg singh’s bound soft homomorphisms. hand deterministic—that nonzero elements expression reduces whitt’s classical result regarding state aggregation dynamic programming. finally exact deterministic factorizations righthand side reduces zero. also makes sense since case stochasticfactorization trick gives rise exact homomorphism proposition elucidates basic mechanism stochasticfactorization trick reduce number states possible exploit result computation optimization problem d¯ra∞ objective minimize function section presented kbrl approximation framework reinforcement learning whose main drawback high computational complexity. section discussed stochastic-factorization trick principle useful reduce long circumvents computational burden imposed calculation matrices involved process. show leverage components produce algorithm called kernel-based stochastic factorization overcomes computational limitations. transitions ending states want apply stochastic-factorization trick kbrl’s mdp. assuming matrices structure computing look sub-matrices corresponding nonzero columns call matrices rm×na rna×m. strategy kbsf matrices elements order conclude construction kbsf’s deﬁne vectors expected rewards ¯ra. shown expression reward functions kbrl’s ˆra) depend ending state recalling interpretation rows transition probabilities representative states original ones illustrated figure clear discussed section kbrl’s approximation scheme interpreted derivation ﬁnite mdp. case sample transitions deﬁne ﬁnite state space model’s transition reward functions. means state space dynamics kbrl’s model inexorably linked except maybe degenerate cases changing also changes other. deﬁning representative states kbsf decouples mdp’s structure particular instantiation. note that representative states diﬀerent sets sample transitions give rise diﬀerent models. conversely transitions generate diﬀerent mdps depending representative states deﬁned. step step description kbsf given algorithm kbsf simple understand implement. works follows ﬁrst built described above. then action-value function determined dynamic programming algorithm. finally kbsf returns approximation ˆv∗—the optimal value function kbrl’s mdp—computed ¯q∗. based shown algorithm point kbsf’s mechanics fact matrices never actually computed instead directly solve containing states only. results eﬃcient algorithm requires operations bits build reduced version kbrl’s maxa reduced model constructed kbsf’s computational cost becomes function only. particular cost solving dynamic programming becomes polynomial instead applinote that practice kbsf’s computational requirements reduced even enforces kernels ¯κ¯τ sparse. particular given ﬁxed instead computing ¯k¯τ evaluate kernel pre-speciﬁed neighborhood only. assuming ¯k¯τ clear computation matrices performed kbsf follows reasoning underlying computation kbrl’s matrices ˆpa; particular however look matrix things slightly diﬀerent here probability transition representative state given ¯κ¯τ computation involves itself. strictly adhere kbrl’s logic computing transition probabilities representative states probability transitioning upon executing state knew transition action function occurred. case would matrix action section provide interpretation approximation computed kbsf supports deﬁnition matrix start looking kbrl constructs matrices ˆpa. shown figure action state associated stochastic vector evaluated since dealing continuous state space possible compute analogous vector focusing nonzero entries clearly full knowledge function ˆpsa allows exact computation kbrl’s transition matrix ˆpa. suppose know ˆpsa want compute suppose approximation function points given training composed pairs possible approximation ¯psa evaluated points case ¯κ¯τ function orders representative states according distance given state nearest representative state deﬁne dist dist assuming ﬁxed sets sample transitions show that that maxai dist order guarantee show that need following lemmas proved appendix according distance ﬁxed state partition subsets control relative magnitude corresponding kernels’s sums adjusting parameter based lemmas present main result section also proved appendix proposition tells that regardless speciﬁc reinforcement learning problem hand distances sampled states respective nearest representative states small enough make kbsf’s approximation kbrl’s value function accurate desired setting suﬃciently small value small maximum distance maxai dist depends particular choice kernel sets sample transitions here deliberately refrained making assumptions order present proposition general form. note ﬁxed number representative states imposes minimum possible value maxai dist value small enough decreasing actually hurt approximation. optimal value case context-dependent. positive side statement note that even maxai dist might possible make setting appropriately. therefore rather practical guide conﬁgure kbsf proposition seen theoretical argument showing kbsf sound algorithm sense limit recovers kbrl’s solution. present series computational experiments designed illustrate behavior kbsf variety challenging domains. start simple problem puddle world show kbsf indeed capable compressing information contained kbrl’s model. move diﬃcult tasks compare kbsf state-of-the-art reinforcement-learning algorithms. start classical control tasks single double pole-balancing. next study medically-related problems based real data drug schedule epilepsy-suppression domains. problems considered paper continuous state space ﬁnite number actions modeled discounted tasks. algorithms’s results correspond performance greedy decision policy derived ﬁnal value function computed. cases decision policies evaluated challenging test states tasks cannot easily solved. details experiments given appendix order show kbsf indeed capable summarizing information contained kbrl’s model puddle world task puddle world simple two-dimensional problem objective reach goal region avoiding puddles along way. implemented task exactly described sutton except used discount factor evaluated decision policies pre-deﬁned test states surrounding puddles experiment carried follows ﬁrst collected sample transitions using random exploration policy case kbrl sample transitions deﬁned model used approximate value function. order deﬁne kbsf’s model states grouped k-means algorithm clusters representative state placed center resulting cluster kernels’s widths varied results reported represent best performance algorithms runs; picked combination parameters generated maximum average return. following convention refer speciﬁc instances method ﬁrst number enclosed parentheses algorithm’s name number sample transitions used approximation second size model used approximate value function. note kbrl coincide. kbrl using models order magnitude smaller. indicates kbsf summarizing well information contained data. depending values compression represent signiﬁcant reduction consumption computational resources. example replacing kbrl kbsf obtain decrease approximately number operations performed policy shown figure figures vary observe figure kbrl kbsf similar performances improve increases. however since kbsf using model ﬁxed size computational cost depends linearly whereas kbrl’s cost grows roughly. explains huge diﬀerence algorithms’s times shown figure evaluate kbsf compares modern reinforcement learning algorithms diﬃcult tasks. ﬁrst contrast method lagoudakis parr’s least-squares policy iteration algorithm besides popularity lspi natural candidate comparison three reasons also builds approximator ﬁxed figure results puddle-world task averaged runs. algorithms evaluated test states distributed region state space surrounding puddles shadowed regions represent conﬁdence intervals. compare performance lspi kbsf pole balancing task. pole balancing long history benchmark problem represents rich class unstable systems objective problem apply forces wheeled cart moving along limited track order keep poles hinged cart falling over. several variations task diﬀerent levels diﬃculty; among them balancing poles side side particularly hard paper compare lspi kbsf singletwo-poles versions problem. implemented tasks using realistic simulator described gomez refer reader appendix details problems’s conﬁguration. experiments carried described previous section sample transitions collected random policy clustered k-means algorithm. versions pole-balancing task lspi used data approximation architectures kbsf. make comparison lspi fair possible ﬁxed width kbsf’s kernel also policy iteration used decision policy mdps constructed kbsf algorithm maximum iterations limit used lspi. figure shows results lspi kbsf single double pole-balancing tasks. call attention fact version problems used significantly harder commonly-used variants decision policies evaluated single state close origin. probably reason lspi achieves success rate single pole-balancing task shown figure contrast kbsf’s decision policies able balance pole attempts average using representative states. results kbsf double pole-balancing task still impressive. wieland rightly points version problem considerably diﬃcult single pole variant previous attempts apply reinforcement-learning techniques domain resulted disappointing performance shown figure kbsf able achieve success rate number perspective recall test states quite challenging poles inclined falling opposite directions. good performance kbsf comes relatively computational cost. conservative estimate reveals that kbrl computer used experiments would wait months results. kbsf delivers decision policy less minutes. kbsf’s computational cost also compares well lspi shown figures lspi’s policyevaluation step involves update solution linear system equations figure results pole-balancing tasks function number representative states averaged runs. values correspond fraction episodes initiated test states pole could balanced steps test sets regular grids deﬁned hypercube centered origin covering state-space axes dimension shadowed regions represent conﬁdence intervals. compare kbsf ﬁtted q-iteration algorithm fitted q-iteration conceptually simple method also builds approximation based solely sample transitions. adopt algorithm ensemble trees generated geurts al.’s extratrees algorithm. refer resulting method fqit. chose fqit comparisons shown excellent performance benchmark real-world reinforcement-learning tasks experiments reported paper used fqit ensembles trees. detailed appendix besides number trees fqit three main parameters. among them minimum number elements required split node construction trees denoted ηmin particularly strong eﬀect algorithm’s performance computational cost. thus experiments ﬁxed fqit’s parameters reasonable values—selected based preliminary experiments—and varied ηmin. respective instances tree-based approach referred fqit. compare fqit kbsf important medical problem refer drug schedule domain typical treatments drug cocktails containing types medication reverse transcriptase inhibitors protease inhibitors despite success drug cocktails maintaining viral loads several complications associated longterm use. attracted interest scientiﬁc community problem optimizing drug-scheduling strategies. strategy receiving attention recently structured treatment interruption patients undergo alternate cycles without drugs. although many successful treatments reported literature consensus regarding exact protocol followed scheduling treatments seen sequential decision problem actions correspond types cocktail administered patient simplify problem’s formulation assumed drugs administered ﬁxed amounts reducing actions four possible combinations drugs none only only both. goal minimize viral load using little drugs possible. following ernst performed experiments using model describes interaction immune system hiv. model developed adams identiﬁed validated based real clinical data. resulting reinforcement learning task -dimensional continuous state space whose variables describe overall patient’s condition. formulated problem exactly proposed ernst strategy used generate data also followed protocol proposed authors brieﬂy explain. starting batch sample transitions generated random policy algorithm ﬁrst computed initial approximation problem’s optimal value function. based approximation .-greedy policy used collect second batch transitions varied fqit’s parameter ηmin experiments kbsf ﬁxed varied discussed beginning section possible reduce kbsf’s computational cost sparse kernels. experiments drug schedule task computed largest values largest values ¯k¯τ since current experiments number sample transitions ﬁxed refer particular instances algorithm simply kbsf. figure shows results obtained fqit kbsf drug schedule task. shown figure fqit’s performance improves ηmin decreased expected. contrast increasing number representative states strong impact quality kbsf’s solutions overall performance kbsf drug schedule task nearly impressive previous problems. example even using representative states corresponds sixth sampled states kbsf unable reproduce performance fqit ηmin hand look figure clear diﬀerence algorithms’s performance counterbalanced substantial diﬀerence associated computational costs. illustration note kbsf times faster fqti times faster fqti. diﬀerence algorithms’s times expected since iteration fqit involves construction ensemble trees requiring least operations improvement current decision policy note fact fqit uses ensemble trees blessing curse. hand reduces variance approximation hand also increases algorithm’s computational cost given fqit’s kbsf’s time complexities wonder latter also beneﬁt averaging several models. order verify hypothesis implemented simple model-averaging strategy kbsf trained several agents independently using algorithm sample transitions together single committee. order increase variability within committee agents instead using k-means determine representative states simply selected uniformly random sampled states figure results drug schedule task averaged runs. policies evaluated days starting state representing patient’s unhealthy state shadowed regions represent conﬁdence intervals. claim approach described best model-averaging strategy used kbsf. however seems suﬃcient boost algorithm’s performance considerably shown figure note kbsf already performs comparably fqti using agents committee. number increased expected return kbsf’s agents considerably larger best fqit’s agent small overlap conﬁdence intervals associated algorithms’s results. good performance kbsf still impressive look figure shows even using committee agents algorithm faster fqit. concluding mention that overall experience fqit conﬁrms ernst al.’s report stable easy-to-conﬁgure method usually delivers good solutions. fact given algorithm’s ease problem hand solved oﬀ-line using moderate number sample transitions fqit good alternative. hand on-line problems oﬀ-line problems involving large number sample transitions fqit’s computational cost prohibitive practice. section discuss experiment computational demand eﬀectively precludes algorithm. conclude empirical evaluation kbsf using learn neuro-stimulation policy treatment epilepsy. shown electrical stimulation speciﬁc structures neural system ﬁxed frequencies eﬀectively suppress occurrence seizures unfortunately vitro neurostimulation experiments suggest ﬁxed-frequency pulses equally eﬀective across epileptic systems. moreover long term treatment potentially damage patients’s neural tissues. therefore desirable develop neuro-stimulation policies replace ﬁxed-stimulation regime adaptive scheme. search eﬃcient neuro-stimulation strategies seen reinforcement learning problem. study using generative model developed bush based real data collected epileptic hippocampus slices. model shown reproduce seizure pattern original dynamical system later validated deployment learned treatment policy real brain slice associated decision problem ﬁve-dimensional continuous state space highly non-linear dynamics. time step agent must choose whether apply electrical pulse. goal suppress seizures much possible minimizing total amount stimulation needed experiments performed described section single batch sample transitions collected policy selects actions uniformly random. speciﬁcally random policy used collect trajectories length resulting total sample transitions. baseline comparisons already mentioned ﬁxed-frequency stimulation policies usually adopted vitro clinical studcompare kbsf lspi fqit. task lspi kbsf sparse kernels computed kernels -nearest neighbors given state modiﬁcation made possible representative states kbsf. since lspi reduction computational cost signiﬁcant ﬁxed keep time within reasonable bounds. again kbsf lspi used approximation architectures representative states deﬁned k-means algorithm. ﬁxed varied tree-based method improved smaller values ηmin expected increase computational cost. thus order give overall characterization fqit’s performance report results obtained extreme values ηmin. figure shows results epilepsy-suppression task. order obtain diﬀerent compromises problem’s conﬂicting objectives varied relative magnitude penalties associated occurrence seizures application electrical pulse speciﬁcally appears plots subscripts next algorithms’s names. shown figure lspi’s policies seem prioritize reduction stimulation expense higher seizure occurrence clearly sub-optimal clinical point view. fqit also performs poorly solutions representing advance ﬁxed-frequency stimulation strategies. contrast fqti kbsf able generate decision policies superior policy eﬃcient stimulation regime known date clinical literature however shown figure kbsf able least times faster tree-based method. clear previous section characteristic kbsf sets apart methods demand terms computational resources. speciﬁcally time memory complexities algorithm linear number sample transitions terms number operations performed algorithm best without discarding transitions. however terms memory usage possible even better. section show build kbsf’s approximation incrementally without ever access entire sample transitions once. besides reducing memory complexity algorithm modiﬁcation additional advantage making kbsf suitable on-line reinforcement learning. batch version kbsf described section matrices vectors determined using transitions corresponding sets undesirable consequences. first construction requires amount memory although signiﬁcant improvement kbrl’s memory figure results epilepsy-suppression problem averaged runs. decision policies evaluated episodes transitions starting ﬁxed test states drawn uniformly random. dependence impractical. second batch version kbsf incorporate data model recompute multiplication actions sample transitions available. even ignore issue memory usage clearly ineﬃcient terms computation. follows present incremental version kbsf circumvents important limitations since computed based only discard sample transitions computing that keep variables stored vectors resulting variables modest memory overhead. note apply ideas recursively splitting sets subsets smaller size. thus fully incremental computing kbsf’s requires almost extra memory. algorithm shows step-by-step description update based sample transitions. using method update model kbsf’s space complexity drops since amount memory used kbsf independent process arbitrary number sample transitions instead assuming partition ﬁxed data consider generated based policy learned kbsf using transitions thus algorithm provides ﬂexible framework integrating learning planning within kbsf. speciﬁcally algorithm cycle learning model problem based sample transitions using model derive policy resorting policy collect data. algorithm shows possible implementation framework. order distinguish batch counterpart call incremental version algorithm ikbsf. ikbsf updates model value function ﬁxed intervals respectively. recover batch version kbsf; fully on-line method stores sample transitions. algorithm also allows inclusion representative states model using algorithm easy given representative state ¯sm+ suﬃces ¯pm+j ¯pjm+ then following applications update rules dynamics naturally reﬂect existence state ¯sm+. note inclusion representative states destroy information already model. allows ikbsf reﬁne approximation needed. think several ways detecting need representative states. simple strategy based proposition impose maximum distance allowed sampled state nearest representative state dist. thus anytime agent encounters state added model sm+. section report experiments ikbsf using approach. that though discuss theoretical properties incremental version algorithm. discussed ikbsf need store sample transitions build approximation. however computation requires tuples available. situations feasible keep transitions order compute however want ikbsf full extend need computing without using sample transitions. upon reaching state time step ikbsf selects action performed based section address issue deriving upper bound diﬀerence action-value function would computed kbrl using transitions processed ikbsf time step order derive bound assume ikbsf uses ﬁxed ¯s—meaning representative states added model —and never stops reﬁning model every iteration start showing following lemma proved appendix lemma ﬁnite mdps. then lemma provides upper bound diﬀerence action-value functions mdps state space action space discount factor strategy result bound error introduced application stochastic-factorization trick context ikbsf. time step ikbsf model built based transitions observed thus far. shown beginning section exactly matches model would computed batch kbsf using data representative states. thus think matrices available iteration ikbsf result stochastic-factorization trick applied matrices although ikbsf explicitly compute matrices serve solid theoretical ground build result proposition suppose ikbsf executed ﬁxed representative states using matrices vector computed algorithm iteration then state encountered ikbsf time step ˆqt− ˜qt| ˆqt− deﬁned proposition’s statement optimal actionvalue function strategy bound term right-hand side since model constructed kbrl using data seen ikbsf time step state correspond states mdp. thus therefore applying lemma proposition shows that time step error action-value function computed ikbsf bounded quality level stochasticity stochastic factorization implicitly computed algorithm. term accounts possibility ikbsf computed optimal value function model step either update algorithm done completion note restriction strictly look empirical performance incremental version kbsf. following structure section start puddle world task show ikbsf indeed able match performance batch kbsf without storing sample transitions. next exploit scalability ikbsf solve diﬃcult control tasks triple pole-balancing helicopter hovering. also compare ikbsf’s performance reinforcement learning algorithms. puddle world problem proof concept ﬁrst experiment show ikbsf able recover model would computed batch counterpart. order applied algorithm puddle-world task using random policy select actions. figure shows result experiment vary parameters note case corresponds batch version kbsf whose results puddle world shown figure expected performance kbsf policies improves gradually algorithm goes sample transitions general intensity improvement proportional amount data processed. important performance decision policies sample transitions processed essentially values conﬁrms ikbsf used instrument circumvent kbsf’s memory demand. thus batch sample transitions available memory possible split data chunks smaller sizes still value-function approximation would computed entire data processed once. shown figure small computational overhead associated strategy figure results puddle-world task averaged runs. kbsf used representative states evenly distributed state space sample transitions collected random policy. agents tested sets states surrounding puddles figures show batch version kbsf able satisfactorily solve double pole-balancing task. order show scalability incremental version algorithm section raise adding third pole problem. perform simulations using parameters usually adopted two-pole problem extra pole length mass longer pole results diﬃcult control problem -dimensional state space experiments kbsf two-pole task used representative states sample transitions collected random policy. start experiment triple pole-balancing using exactly conﬁguration ikbsf reﬁne model incorporating sample transitions update rules also ikbsf grow model necessary. speciﬁcally representative state added on-line every time agent encounters sample state ¯sj) corresponds setting maximum allowed took average hour minutes process times less data. shown figure ikbsf’s ability process large number sample transitions allows algorithm achieve success rate approximately similar performance batch kbsf two-pole version problem good performance ikbsf triple pole-balancing task especially impressive recall decision policies evaluated test states representing possible directions inclination three poles. order achieve level performance kbsf approximately memory would necessary even using sparse kernels whereas ikbsf used less memory. argue comparison fqit kbsf fair since latter used times amount data used former. thus figures show results fqit using batches transitions—exactly number transitions processed ikbsf. cannot compare ikbsf fqit computational cost tree-based approach prohibitively large look instances algorithm opposite trends. surprisingly extra sample transitions actually made performance fqit worse. hand fqit performs signiﬁcantly better using data though still well figure results triple pole-balancing task function number sample transitions averaged runs. values correspond fraction episodes initiated test states poles could balanced steps test sets regular grids cells deﬁned hypercube centered origin covering state-space axes dimension shadowed regions represent conﬁdence intervals. conclude observe figure number representative states grows function number sample transitions processed kbsf. expected beginning learning process grows fast reﬂecting fact relevant regions state space visited yet. data come number representative states starts stabilize. previous sections showed ikbsf used circumvent inherent memory limitations batch learning. show algorithm performs fully on-line regime. that focus challenging reinforcement learning task goal control autonomous helicopter. helicopters unique control capabilities speed ﬂight in-place hovering make indispensable instruments many contexts. ﬂexibility comes price though widely recognized helicopter signiﬁcantly harder control ﬁxed-wing aircraft part diﬃculty complex dynamics helicopter non-linear noisy asymmetric also counterintuitive aspects additional complication controlling autonomous helicopter fact wrong action easily lead crash dangerous expensive. thus usual practice ﬁrst develop model helicopter’s dynamics model design controller model constructed abbeel based data collected actual ﬂights xcell tempest helicopter resulting reinforcement learning problem -dimensional state space whose variables represent aircraft’s position orientation corresponding version task considered goal keep helicopter hovering close possible ﬁxed position. episodes start target location time step agent receives negative reward proportional distance current state desired position. tail rotor’s thrust exerts sideways force helicopter aircraft cannot held stationary zero-cost state even absence wind. episode ends helicopter leaves hover regime state’s variables exceeds pre-speciﬁed thresholds. helicopter controlled -dimensional continuous vector whose variables represent longitudinal cyclic pitch latitudinal cyclic pitch tail rotor collective pitch main rotor collective pitch. adjusting value variables pilot rotate helicopter around axes control thrust generated main rotor. since kbsf designed deal ﬁnite number actions discretized using values dimension resulting possible actions. details discretization process given below. compare ikbsf sarsa algorithm using tile coding value function approximation applied sarsa learning rate tilings containing tiles each. except parameters adjusted preliminary experiments order improve performance sarsa agent. also deﬁned action-space discretization based sarsa’s performance. particular instead partitioning dimension equally-sized intervals spread break points unevenly along axis order maximize agent’s return. result process described appendix interaction sarsa agent helicopter hovering task dictated \u0001-greedy policy. initially every transitions value decreased ikbsf agent collected sample transitions using exploration regime. based ﬁrst batch transitions representative states determined k-means algorithm. representative states added ikbsf’s model that. value function model updated ﬁxed intervals transitions. ﬁxed figure shows results obtained sarsa kbsf helicopter hovering task. note figure average episode length increases abruptly points value decreased. true sarsa kbsf. also since number steps executed episode increases time interval abrupt changes decreases length expected. finally observe performance agents stabilizes around episodes probably point almost exploration taking place anymore. compare kbsf sarsa clear former signiﬁcantly outperforms latter. speciﬁcally cut-point episodes kbsf agent executes approximately times number steps performed sarsa agent crashing. looking figures argue ﬁrst nothing surprising here model-based algorithm kbsf sample eﬃcient sarsa also considerably slower notice though diﬀerence times sarsa kbsf shown figure part consequence good performance latter since kbsf able control helicopter larger number steps corresponding episodes obviously take longer. better measure algorithms’s computational cost seen figure shows average time taken method perform transition. observe kbsf’s computing time peaks points model value function updated. beginning kbsf’s changes considerably result value function updates take longer. data come iteration value function computed previous round). point kbsf’s computational cost step slightly higher sarsa’s even though former computes model environment latter directly updates value function approximation. conclude note objective section exclusively show kbsf outperform well-known on-line algorithm compatible computational cost. therefore focused comparison algorithms rather obtaining best possible performance task. also important mention diﬃcult versions helicopter task addressed literature usually using domain knowledge conﬁguration algorithms guide collection data since focus evaluating on-line performance kbsf addressed problem purest form without using prior information help algorithms solve task. execution experiments observed several interesting facts kbsf immediate conceptual deﬁnition. section share lessons learned reader. start discussing impact deviating theoretical assumptions performance algorithm. present general guidelines conﬁgure kbsf solve reinforcement learning problems. theoretical guarantees regarding kbrl’s solution assume initial states uniformly sampled somewhat restrictive precludes collection data direct interaction environment. ormoneit conjectured sampling states uniform distribution strictly necessary indeed later ormoneit glynn relaxed assumption case kbrl applied average-reward mdp. case required exploration policy used collect data chooses actions positive probability. described sections computational experiments collected data \u0001greedy policy good performance kbsf corroborates ormoneit also make assumptions regarding smoothness reward function transition kernel continuous unfortunately assumptions usually veriﬁable practice. empirically observed kbsf indeed performs better problems smooth dynamics—loosely speaking problems small perturbation results small perturbation pole balancing task. problems rougher dynamics like epilepsysuppression task still possible good results kbsf case necessary representative states narrower kernels result problems type kbsf less eﬀective reducing kbrl’s computational cost. performance kbsf depends crucially deﬁnition representative states looking expression ideally states would rows matrices would form convex hull containing rows corresponding however easy states exist. even exist ﬁnding trivial problem. instead insisting ﬁnding representative states allow exact representation matrices sounds realistic content oneself approximate solution problem. proposition suggests reasonable strategy deﬁne representative states control magnitude maxai dist maximum distance sampled state nearest representative state. based observation experiments clustered states used clusters’s centers representative states. despite simplicity strategy usually results good performance shown sections course approaches possible. simplest technique perhaps select representative states random sampled states shown section strategy seems work reasonably well adopted together model averaging. another alternative resort quantization approaches k-means among them promising method beygelzimer al.’s cover tree since directly tries minimize maxai dist easily updated on-line another possibility mixture gaussians sampled states deﬁnition representative states also seen opportunity incorporate prior knowledge domain interest approximation model. example knows regions state space important others information used allocate representative states regions. similar reasoning applies tasks level accuracy required decision policy varies across state space. regardless exactly representative states deﬁned using ikbsf always ones on-line necessary course best combination values depends speciﬁc problem hand particular choice kernels. give general advice parameters based theory practice. since parameter used kbrl decrease number sample transitions admissible rate analogously proposition strategy usually facilitates conﬁguration kbsf rescale data variables approximately magnitude—which corresponds using weighted norm computation kernels. using strategy able obtain good results kbsf problems performing coarse search space parameters varied order magnitude alternatively deﬁne neighborhood used compute explained appendix experiments computed ¯k¯τ closest sampled states closest representative states using approach possible conﬁguring kbsf suﬃciently large values adjust advantage adjusting intuitive directly conﬁguring experiments compared kbsf kbrl lspi ﬁtted q-iteration sarsa terms computational cost terms quality resulting decision policies. section situate algorithm broader context approximate reinforcement learning. approximation reinforcement learning important topic generated huge body literature. broad overview subject refer reader books sutton barto bertsekas tsitsiklis szepesv´ari narrow attention kernel-based approximation techniques. figure impact kernels’s widths performance kbsf kbrl. results puddle-world task averaged runs. errors around mean correspond conﬁdence intervals. figure details. ings literature. side kernel smoothing techniques like kbrl kbsf local kernels essentially device implement smooth instance-based approximation side methods reproducing kernels implicitly represent inner product high-dimensional state space although frameworks give rise approximators similar structures rest diﬀerent theoretical foundations. since reproducing-kernels methods less directly related kbsf describe brieﬂy. discuss kernel smoothing approaches detail. basic idea reproducing-kernel methods apply kernel trick context reinforcement learning roughly speaking approximation problem rewritten terms inner products only replaced properly-deﬁned kernel. modiﬁcation corresponds mapping problem high-dimensional feature space resulting expressiveness function approximator. perhaps natural applying kernel trick context reinforcement learning kernelize formulation value-function approximation problem another alternative approximate dynamics using kernel-based regression method following slightly diﬀerent line work bhat propose kernelize linear programming formulation dynamic programming. however method directly applicable reinforcement learning since based assumption full knowledge mdp. weaker assumption suppose reward function known focus approximation transition function. approach taken grunewalder propose embed conditional distributions deﬁning transitions hilbert space induced reproducing kernel. turn attention kernel-smoothing techniques closely related kbrl kbsf. kroemer peters propose apply kernel density estimation problem policy evaluation. call method non-parametric dynamic programming kbrl compute value function ﬁxed policy many similarities npdp also important diﬀerences. like kbrl npdp statistically consistent. unlike kbrl assumes ﬁnite action space directly approximates conditional density functions a|s) npdp assumes continuous models joint density kroemer peters showed value function npdp nadaraya-watson kernel regression form. surprisingly also form kbrl’s solution policy evaluated cases coeﬃcients kernel-based approximation derived value function approximate mdp. diﬀerence transition matrices computed algorithm. shown transition probabilities kbrl’s model given kernel values themselves. contrast computation element ndpd’s transition matrix requires integration continuous state space practice done numerical integration techniques computationally demanding directly compared npdp kbrl algorithms build model whose number states dictated number sample transitions neither method explicitly attempts keep small. since case application bellman operator methods suitable problems large number transitions required applicable on-line reinforcement learning. however kernel-smoothing methods avoid computational issue either keeping small executing number operations grows linearly algorithms directly comparable kbsf. ﬁrst attempts adapt kbrl on-line scenario jong stone instead collecting batch sample transitions learning process starts authors propose grow incrementally based exploration policy derived kbrl’s current model. avoid running dynamic-programming algorithm completion transitions computationally feasible jong stone resort moore atkeson’s prioritized sweeping method propagate changes value function every time model modiﬁed. idea exploiting interpretation kbrl derivation ﬁnite order tabular exploration methods insightful. however clear whether smart exploration suﬃcient overcome computational diﬃculties arising fact size underlying model inexorably linked number sample transitions. example even using sparse kernels experiments jong stone following line work jong stone later proposed guide kbrl’s exploration state space using brafman tennenholtz’s r-max algorithm. paper authors address issue kbrl’s scalability aggressively. first show combine approach dietterich’s max-q algorithm allowing decomposition kbrl’s hierarchy simpler models. potentially reduce computational burden ﬁnding policy strategy transfer user responsibility identifying useful decomposition task. practical approach combine kbrl stable form value-function approximation. that jong stone suggest gordon’s averagers. shown appendix setting corresponds particular case kbsf representative states selected among sampled states noted that even using temporal abstraction function proximation jong stone’s approach requires recomputing kbrl’s transition probabilities sample infeasible reasonably large problems. kveton theocharous propose practical algorithm reduce kbrl’s computational cost. method closely resembles batch version kbsf. algorithm kveton theocharous’s method deﬁnes representative states give rise reduced mdp. main diﬀerence construction models that instead computing similarity measure sampled state representative states algorithm associates single ¯sj—which comes computing hard aggregation state space aggregation corresponds matrix single nonzero element row. fact possible rewrite kveton theocharous’s algorithm using kbsf’s formalism. case elements would deﬁned normalized kernel induced inﬁnitely narrow kernel whose value greater zero gives closest representative state easy make matrix computed kbsf close desired hard aggregation setting suﬃciently small value practically simply plug place algorithm exactly recover kveton theocharous’s method. note though that computation would actually replacing deviating kbrl’s framework. note representative rows matrix computed kbsf states sampled states would coincide subset rows corresponding kbrl’s matrix however property lost uses instead paper presented kbsf reinforcement learning algorithm results application stochastic-factorization trick kbrl. kbsf summarizes information contained kbrl’s model ﬁxed size. algorithm decouples structure model conﬁguration. makes possible build approximation accounts diﬃculty problem computational resources available. main strengths kbsf simplicity. shown paper uncomplicated mechanics unfolded update rules allow fully incremental version algorithm. makes amount memory used kbsf independent number sample transitions. therefore lines code reinforcement-learning algorithm applied large-scale problems oﬀ-line on-line regimes. kbsf also sound method theoretical point view. discussed distance value function computed algorithm computed kbrl bounded factors quality level stochasticity underlying stochastic factorization. showed factors made arbitrarily small implies that theory make kbsf’s solution close kbrl’s solution desired. theoretical guarantees always translate practical methods either because built upon unrealistic assumptions account procedural diﬃculties arise practice. ensure case algorithm presented extensive empirical study kbsf successfully applied diﬀerent problems quite challenging. also presented general guidelines conﬁgure kbsf solve reinforcement learning problem. reasons listed above believe kbsf potential becoming valuable resource solution reinforcement learning problems. subject exhausted. several possibilities future research brieﬂy discuss. algorithmic perspective perhaps pressing demand prinincidentally also opens cipled methods select representative states. possibility automated procedure kernel’s widths based solely data. taking idea further think distinct associated regarding integration kbsf broader context subject deserves investigation possibility building approximation based multiple models. model averaging inherently linked kbsf principle used virtually reinforcement learning algorithm. however kbsf’s computational cost makes particularly amenable technique. since algorithm orders magnitude faster method whose complexity iteration function number sample transitions aﬀord compute several approximations still paper emphasized role kbsf technique reduce kbrl’s computational cost. however equally important whether algorithm provides beneﬁts statistical point view. ormoneit showed that general number sample transitions needed kbrl achieve certain approximation accuracy grows exponentially dimension state space. methods avoid exponential dependency explore sort regularity problem’s structure—paraphrasing authors break curse dimensionality incorporating prior knowledge approximation think kbsf cast strategy particular deﬁnition representative states interpreted practical mechanism incorporate knowledge approximation. whether impact algorithm’s sample complexity interesting question future investigation. conclude noting kbsf represents particular stochasticfactorization trick exploited context reinforcement learning. principle algorithm builds model based sample transitions resort trick leverage data. basic idea remains same instead estimating transition probabilities every pair states focuses small representative states whose values propagated throughout state space based notion similarity. believe general framework potentially materialized multitude useful reinforcement learning algorithms. given denote smallest satisﬁes assumption implies function positive eventually decay exponentially. note assume greater zero everywhere order guarantee ¯κ¯τ well deﬁned value straightforward generalize results case ﬁnite support ensuring that given sets sample transitions representative states that ¯k¯τ assumption know ¯φ/¯τ ¯φ/¯τ function exploits fact decreasing maximum allowed value according factors. ﬁrst diﬀerence magnitude dist dist. easy understand. suppose want make smaller given threshold. much farther value ¯φ/¯τ considerably smaller value ¯φ/¯τ even large. hand diﬀerence magnitude dist dist small decrease ensure suﬃciently small. therefore upper bound decreases |dist dist|. second factor inﬂuences upper bound relative sizes sets again hard observe expressions diﬀer coeﬃcient multiplying righthand side inequalities. min. then deﬁned apply lemma guaranmake holds. finally minz minaij δaij minz minaij θaij guarantee true implies also true section derived upper bound approximation error introduced application stochastic-factorization trick. section introduce another bound diﬀerent properties. first bound less applicable depends quantities usually unavailable practical situation bright side unlike bound presented proposition bound valid norm. also draws interesting connection important class approximators known averagers norm vector proof. bellman operators given ¯¯∆. note γpav e¯¯ra γelav column thus since stochastic think gordon’s averagers given resort theorem author conclude therefore ˇ∆v. derived upper bound depends ﬁxed points ﬁxed point unique ﬁxed point since latter deﬁned bound essentially function factorization terms expected. notice bound valid norm ﬁxed point ¯¯pa¯¯v∗ consequence fact transient therefore ¯¯q∗. also since e¯¯qa∗ e¯¯ra γelae¯¯v∗ γpae¯¯v∗ know ¯¯q∗ ∆e¯¯v∗. putting results together obtain γ∆e¯¯v∗ e¯¯v∗ lemma puddle world puddle-world task implemented described sutton task modeled discounted problem transitions associated zero reward except leading goal resulted reward ending inside puddles lead penalty pole balancing implemented simulator three versions polebalancing task using equations motion parameters given appendix gomez’s thesis. integration used order runge-kutta method time step seconds actions chosen every time steps. considered version task angle pole vertical version problem. values correspond hypercube centered origin covering state-space axes dimension triple pole-balancing task performed simulations using parameters usually adopted pole version problem added third pole length mass longer pole case decision policies evaluated test containing states equally distributed region drug schedule drug schedule task implemented using system ordinary diﬀerential equations given adams integration carried euler method using step size actions selected steps suggested ernst problem modeled discounted task parameters task well protocol used numerical simulations also followed suggestions authors. particular assumed existence patients monitored days. monitoring period content drug cocktail administered patient could changed ﬁxed intervals days. thus sample transition results reported section correspond performance greedy policy induced value function computed algorithms using available sample transitions. decision policies evaluated days starting unhealthy state corresponding basin attraction odes describing problem’s dynamics epilepsy suppression used generative model developed bush perform experiments epilepsy suppression task. model generated based labeled ﬁeld potential recordings brain slices electrically stimulated frequencies data used construct manifold embedding turn gave rise problem’s state space. objective minimize occurrence seizures using little stimulation possible therefore negative reward associated events bush al.’s generative model public available environment rl-glue package experiments problem modeled discounted task decision policies evaluated episodes transitions starting ﬁxed test states drawn uniformly random problem’s state space. helicopter hovering experiments helicopter hovering task used simulator developed abbeel available environment rl-glue package simulator built based data collected separate ﬂights xcell tempest helicopter. data used adjust parameters acceleration prediction model accurate linear model normally adopted industry. objective problem keep helicopter hovering close possible speciﬁc location. therefore time step agent gets negative reward proportional distance target largest values largest values ¯k¯τ order implement feature used kd-tree nearest neighbors computed states value ¯k¯τ outside neighborhood truncated zero iterations number trees composing ensemble number candidate cut-points evaluated generation trees minimum number elements required split node denoted ηmin. general increasing ﬁrst three improves performance ηmin inverse relation quality ﬁnal value function approximation. experiments indicate following conﬁguration fqit usually results good performance tasks considered paper iterations ensemble trees candidate points. parameter ηmin particularly strong eﬀect fqit’s performance computational cost correct value seems problem-dependent. therefore experiments ﬁxed parameters fqit described varied ηmin. work described technical report done andr´e barreto postdoctoral fellow school computer science mcgill university. authors would like thank yuri grinberg amir-massoud farahmand valid discussions regarding kbsf related subjects. also thank keith bush making epilepsy simulator available alicia bendz ryan primeau helping computational experiments. funding research provided national institutes health nserc discovery grant program.", "year": 2014}