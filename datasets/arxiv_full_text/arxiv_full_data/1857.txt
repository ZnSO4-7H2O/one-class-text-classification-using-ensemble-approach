{"title": "On Extending Neural Networks with Loss Ensembles for Text Classification", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Ensemble techniques are powerful approaches that combine several weak learners to build a stronger one. As a meta learning framework, ensemble techniques can easily be applied to many machine learning techniques. In this paper we propose a neural network extended with an ensemble loss function for text classification. The weight of each weak loss function is tuned within the training phase through the gradient propagation optimization method of the neural network. The approach is evaluated on several text classification datasets. We also evaluate its performance in various environments with several degrees of label noise. Experimental results indicate an improvement of the results and strong resilience against label noise in comparison with other methods.", "text": "ensemble techniques powerful approaches combine several weak learners build stronger one. meta learning framework ensemble techniques easily applied many machine learning techniques. paper propose neural network extended ensemble loss function text classiﬁcation. weight weak loss function tuned within training phase gradient propagation optimization method neural network. approach evaluated several text classiﬁcation datasets. also evaluate performance various environments several degrees label noise. experimental results indicate improvement results strong resilience label noise comparison methods. statistics machine learning ensemble methods multiple learning algorithms obtain better predictive performance proved ensemble methods boost weak learners whose accuracies slightly better random guessing arbitrarily accurate strong learners could possible directly design strong complicated learning system ensemble methods would possible solution. paper inspired ensemble techniques combine several weak loss functions order design stronger ensemble loss function text classiﬁcation. vector one-hot encoding target label output classiﬁer vector probability estimates label given input sample training parameters then loss function positive function measures error estimation different loss functions different properties well-known loss functions shown table different loss functions lead different optimum bayes estimators unique characteristics. environment picking speciﬁc loss function affect performance signiﬁcantly paper propose approach combining loss functions performs substantially better especially facing annotation noise. framework designed extension regular neural networks loss function replaced ensemble loss functions ensemble weights learned part gradient propagation process. implement evaluate proposed algorithm several text classiﬁcation datasets. paper structured follows. overview several loss functions classiﬁcation brieﬂy introduced section proposed framework proposed algorithm explained section section contains experimental results classifying several text datasets. paper concluded section typical machine learning problem reduced expected loss function minimization problem rosasco studied impact choosing different loss functions viewpoint statistical learning theory. section loss functions divided margin-based distance-based categories. margin-based loss functions often used classiﬁcation purposes since evaluate work classiﬁcation text datasets paper focus marginbased loss functions. margin-based loss function deﬁned penalty function based margin given application marginbased loss functions might several disadvantages advantages could certainly tell loss function preferable general. example consider zero-one loss function penalizes misclassiﬁed samples constant value correctly classiﬁed samples loss. loss function would result robust classiﬁer facing outliers would terrible performance application margin focus loss function margin enforcing minimization expected loss function leads classiﬁer enhancing margin learning classiﬁer acceptable margin would increase generalization. enhancing margin would possible loss function returns small amount loss correct samples close classiﬁcation hyperplane. example zero-one penalize correct samples therefore enhance general idea ensemble techniques combine different expert ideas aiming boosting accuracy based enhanced decision making. predominantly underlying idea decision made committee experts reliable decision expert alone ensemble techniques framework applied variety real problems better results achieved comparison using single expert. considered importance loss function learning algorithms order reach better learning system inspired ensemble techniques design ensemble loss function. weight applied weak loss function tuned gradient propagation optimization neural network working text classiﬁcation dataset. works combined loss functions weights speciﬁed hyperparameter prior learning process paper combine functions hyperparameter a-priory learned training process. applied proposed ensemble loss function several text datasets. table provides brief description datasets. reach better ensemble loss function choose three loss functions different approaches facing outliers weak loss functions correntropy loss assign high weight samples errors hinge loss penalizes linearly cross-entropy loss function highly penalizes samples whose predictions targets. compared results loss functions widely used neural networks cross-entropy square loss hinge loss. input features word frequencies input text. thus notation composed python tensorflow package implementing proposed approach. results shown table table compares results using individual loss functions ensemble loss. make optimization algorithm simpler instead second constraint omitted. incorporate constraint regularization term based concept augmented lagrangian. modiﬁed objective function using augmented lagrangian presented follows. figure illustrates framework proposed approach dashed representing contribution paper. training phase weight weak loss function trained gradient propagation optimization method. accuracy model calculated test phase shown ﬁgure. parameters neural network classiﬁer softmax layer probability estimates tmax). {li}m denote weak loss functions. addition ﬁnding optimal goal best weights combine weak loss functions order generate better application-tailored loss function. need constraint avoid yielding near zero values weights. proposed ensemble loss function deﬁned below. description coldata lection messagescollected different net-news newsgroups. nltk corpus moviereviews data reviews labeled already positive negative. collection sample emails corpus email already labeled spam ham. data originally collected labeled carnegie group inc. reuters ltd. course developing construe text categorization system also compared robustness proposed loss function individual loss functions. particular label noise randomly modifying target label training samples keep evaluation intact. conducted experiments noise e.g. noise means randomly changing labels training data. tables show results best results shown boldface. observe that virtually experiments ensemble loss least good individual losses cases loss worse. general ensemble loss performed comparatively better increased label noise. paper proposed loss function based ensemble methods. work focused text classiﬁcation tasks considered initial attempt explore ensemble loss functions. proposed loss function shows improvement compared wellknown individual loss functions. furthermore approach robust presence label noise. moreover according experiments gradient descent method quickly converged. used simple neural architecture work principle method could used systems neural networks. future work explore integration complex neural networks using convolutions recurrent networks. also plan study application method tasks sequence labeling another possible extension could focus handling sparseness adding regularization term. qinfeng mark reid tiberio caetano anzhenhua wang. hengel hybrid loss multiclass strucieee transactions pattern tured prediction. analysis machine intelligence https//doi.org/./tpami... peng zhang zhuo yanning zhang hanqiao huang kangli chen. bayesian tracking fusion framework online classiﬁer ensemble immersive visual applications. multimedia tools applications zhao musa mammadov john yearwood. convex nonconvex loss function analysis binary classiﬁcation. data mining workshops ieee international conference ieee pages references qinxun henry stan sclaroff. bayesian framework online classiﬁer ensemble. proceedings international conference machine learning pages badong chen xing haiquan zhao nanning zheng jose principe. kernel risksensitive loss deﬁnition properties application ieee transactions robust adaptive ﬁltering. signal processing inayatullah khan peter roth abdul bais horst bischof. semi-supervised image classiﬁcation huberized laplacian support vector maemerging technologies chines. ieee international conference ieee pages pokharel principe. correntropy properties applications nonieee transacgaussian signal processing. tions signal processing https//doi.org/./tsp... weifeng pokharel principe. correntropy localized similarity measure. ieee international joint conference neural network proceedings. pages https//doi.org/./ijcnn... hamed masnadi-shirazi vijay mahadevan nuno vasconcelos. design robust clascomputer vision siﬁers computer vision. pattern recognition ieee conference ieee pages hamed masnadi-shirazi nuno vasconcelos. design loss functions classiﬁcation theory robustness outliers savageboost. advances neural information processing systems. pages", "year": 2017}