{"title": "An Experimental Comparison of Hybrid Algorithms for Bayesian Network  Structure Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We present a novel hybrid algorithm for Bayesian network structure learning, called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. It is based on a subroutine called HPC, that combines ideas from incremental and divide-and-conquer constraint-based methods to learn the parents and children of a target variable. We conduct an experimental comparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning, on several benchmarks with various data sizes. Our extensive experiments show that H2PC outperforms MMHC both in terms of goodness of fit to new data and in terms of the quality of the network structure itself, which is closer to the true dependence structure of the data. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available.", "text": "abstract. present novel hybrid algorithm bayesian network structure learning called hybrid ﬁrst reconstructs skeleton bayesian network performs bayesian-scoring greedy hill-climbing search orient edges. based subroutine called combines ideas incremental divide-andconquer constraint-based methods learn parents children target variable. conduct experimental comparison max-min hill-climbing currently powerful state-of-the-art algorithm bayesian network structure learning several benchmarks various data sizes. extensive experiments show outperforms mmhc terms goodness data terms quality network structure itself closer true dependence structure data. source code well data sets used empirical tests publicly available. bayesian network probabilistic model formed structure parameters. structure directed acyclic graph whilst parameters conditional probability distributions associated variables model. graph independence useful many applications including feature selection inferring causal relationships observational data problem ﬁnding encodes conditional independencies present data attracted great deal interest last years ideally coincide dependence structure global distribution least identify distribution close possible correct probability space. step called structure learning similar approaches terminology model selection procedures classical statistical models. basically constraint-based learning methods systematically check data conditional independence relationships constraints construct partially oriented graph representative equivalence class whilst search-and-score methods make goodness-of-ﬁt score function evaluating graphical structures regard data set. hybrid methods attempt best worlds learn skeleton approach constrain dags considered phase. many excellent treatments survey learning methods instance). approaches advantages disadvantages. approaches relatively quick deterministic well deﬁned stopping criterion; however rely arbitrary signiﬁcance level test independence unstable sense error early search cascading eﬀect causes many errors present ﬁnal graph. approaches advantage able ﬂexibly incorporate users’ background knowledge form prior probabilities structures also capable dealing incomplete records database although methods favored practice dealing small dimensional data sets slow converge computational complexity often prevents ﬁnding optimal structures currently available exact algorithms decomposable score like bdeu computational complexity remains exponential therefore algorithms intractable around vertices current workstations larger sets variables computational burden becomes prohibitive. mind ability restrict search locally around target variable advantage methods methods. able construct local graph around target node without construct whole ﬁrst hence scalability view balancing computation cost desired accuracy estimates several hybrid methods proposed recently. tsamardinos proposed min-max hill climbing algorithm conducted extensive empirical comparison performed recent years showing mmhc fastest accurate method terms structural error based structural hamming distance. speciﬁcally mmhc outperformed terms time eﬃciency quality reconstruction sparse candidate three phase dependency analysis optimal reinsertion greedy equivalence search greedy hill-climbing search variety networks sample sizes parameter values. although mmhc rather heuristic nature mmhc currently considered powerful state-of-the-art algorithm structure learning capable dealing thousands nodes reasonable time. view enhance performance small dimensional data sets perrier proposed recently hybrid algorithm learn optimal undirected graph given structural constraint. deﬁned undirected graph super-structure algorithm learn optimal containing vertices average degree super-structure around sparse structural constraint assumed. extend feasibility hundred vertices average degree four proposed divide super-structure several clusters perform optimal search order scale larger networks. despite interesting improvements terms score structural hamming distance several benchmark report running times times longer mmhc average still prohibitive view. therefore great deal interest hybrid methods capable improving structural accuracy methods graphs containing thousands vertices. however make strong assumption skeleton contains least edges true network small possible extra edges. controlling false discovery rate learning attracted attention recently knowledge work controlling actively rate false-negative errors study compare mmhc another hybrid algorithm structure learning called hybrid mmhc share exactly procedure allow fair comparisons; diﬀerence lies procedure learning skeleton mmhc based max-min parents children learn parents children variable based subroutine called hybrid parents children combines ideas incremental divide-and-conquer methods. ability mmpc infer parents children candidate nodes assessed several empirical experiments. work conduct experimental comparison max-min hill-climbing several benchmarks various data sizes. formally tuple directed acyclic graph whose nodes represent variables domain whose edges represent direct probabilistic dependencies them. denotes joint probability distribution structure encodes conditional independence assumptions node conditionally independent descendants given parents independence assumptions turn imply many conditional independence statements extracted network using simple graphical criterion called d-separation denote conditional independence given variables underlying probability distribution. note exhaustive search combinatorial problem intractable high dimension data sets. denote assertion d-separated given denote dsep d-separates converse necessarily hold. satisﬁes faithfulness condition d-separations identify conditional independencies i.e. denote spouses i.e. variables common children sets unique satisﬁes faithfulness condition drop superscript induction local global structures handled methods identiﬁcation local neighborhoods hence scalability high dimensional data sets. methods systematically check data conditional independence relationships order infer target’s neighborhood. typically algorithms either independence test data discrete fisher’s test continuous order decide dependence independence upon rejection acceptance null hypothesis conditional independence. since limiting discrete data global local distributions assumed multinomial latter represented conditional probability tables. conditional independence tests network scores discrete data functions conditional probability tables observed frequencies {nijk; random variables conﬁgurations levels conditioning variables ni+k shorthand marginal nijk similarly ni+k n++k n+++ classic conditional independence test based mutual information. mutual information information-theoretic distance measure deﬁned proportional log-likelihood ratio test asymptotic null distribution degrees freedom. detailed analysis properties refer reader main limitation test rate convergence limiting distribution particularly problematic dealing small samples sparse contingency tables. decision accepting rejecting null hypothesis depends implicitly upon degree freedom increases exponentially number variables conditional set. several heuristic solutions emerged literature overcome shortcomings asymptotic tests. study following heuristics used mmhc. first perform assume independence enough samples achieve large enough power. require average sample count user deﬁned parameter equal heuristic called power rule. second consider structural zero either case n+jk ni+k example n+jk consider structurally forbidden value reduce known degrees freedom adjustment heuristic. section present brief overview hpc. details reader directed well references therein. viewed ensemble method combining many weak learners attempt produce stronger learner. based three subroutines data-eﬃcient parents children superset data-eﬃcient spouses superset interleaved incremental association parents children weak learner based inter-iamb requires little computation. thought compensate large number false negatives output weak learner performing extra computations. receives target node data variables input returns estimation hybrid combines beneﬁts incremental divide-and-conquer methods. procedure starts extracting superset pcst superset spst severe restriction maximum conditioning size order signiﬁcantly increase reliability tests. ﬁrst candidate obtained running weak learner pcst spst idea decentralized search lines includes candidate variables superset pcst vicinity. note that theory output inter-iapc output inter-iapc. however practice always true particularly working high-dimensional domains. loosening criteria nodes said adjacent eﬀective restrictions size neighborhood less severe. decentralized search signiﬁcant impact accuracy hpc. enables algorithm handle large neighborhoods still correct faithfulness condition. inter-iapc fast incremental method receives data target node input promptly returns rough estimation hence term weak learner. subroutines de-pcs de-sps search superset respectively severe restriction maximum conditioning size order signiﬁcantly increase reliability tests. variable ﬁltering advantages allows scale hundreds thousands variables restricting search subset relevant variables eliminates many deterministic relationships produce many false negative errors output algorithm. again reader encouraged consult papers gaining insight procedures. section discuss phase. following discussion draws strongly phase hybrid mmhc exactly same. idea constraining search improve time-eﬃciency ﬁrst appeared sparse candidate algorithm results eﬃciency improvements greedy search. recent hybrid algorithms build idea employ sound algorithm identifying candidate parent sets. hybrid ﬁrst identiﬁes parents children variable performs greedy hill-climbing search space search begins empty graph. edge addition deletion direction reversal leads largest increase score taken search continues similar fashion recursively. important diﬀerence standard greedy search search constrained consider adding edge discovered ﬁrst phase. extend greedy search tabu list list keeps last structures explored. instead applying best local change best local change results structure list performed attempt escape local maxima. changes occur without increase maximum score ever encountered search algorithm terminates. overall best scoring structure returned. clearly false positives heuristic allows enter candidate computational burden imposed phase. section conduct experimental comparison mmhc several benchmarks various data sizes. data sets used empirical experiments sampled eight well-known previously used benchmarks learning algorithms claim data sets resemble real-world problems however make possible compare outputs algorithms known structure. benchmarks downloaded bnlearn repository sample sizes considered experiments repeated times sample size investigate behavior algorithms using parametric tests reference. implemented integrated bnlearn package developed source code well data sets used empirical tests publicly available threshold considered type error test experiments carried intel core running windows bits. ﬁrst investigate quality skeleton returned phase. measure false positive edge ratio precision recall combination precision recall deﬁned measure euclidean distance perfect precision recall proposed second assess quality ﬁnal output phase report performance indicators described below posterior density network data learned from measure goodness known bayesian dirichlet equivalent score single parameter equivalent sample size thought size imaginary sample supporting prior distribution. equivalent sample size suggested data sampled true probability distribution benchmark ﬁrst learn network structure mmhc compute relevant performance indicators pair network structures. data used assess well network generalizes data generated true probability structure benchmark networks contains observations. notice using bdeu score metric reconstruction quality following problems. first score corresponds posteriori probability network certain conditions unknown degree assumptions hold distributions encountered practice. second score highly sensitive equivalent sample size depends network priors used. since typically arbitrary value parameter used learning scoring learned network metric favors algorithms bdeu score learning. fact bdeu score rely structure original gold standard network all; instead employs several assumptions score networks. reasons addition score also report score metric. figure report quality skeleton obtained obtained mmpc function sample size. results benchmark shown detail space restrictions. sake conciseness performance values averaged benchmarks depicted table increase factor given performance indicator expressed ratio performance value obtained obtained mmpc note indicators increase actually improvement worse clarity mention explicitly subplots whether increase factor interpreted improvement not. regarding quality superstructure advantages mmpc noticeable. observed consistently increases recall reduces rate false negative edges. expected beneﬁt comes little expense terms false positive edges. also improves euclidean distance perfect precision recall benchmarks increasing number independence tests thus running time phase worth noting capable maintaining mean false positive edge increase reducing euclidean distance range samples. results much line experiments presented figure report quality ﬁnal obtained obtained mmhc function sample size. regarding bdeu training test data improvements noteworthy. results terms goodness training data data using clearly dominate obtained using mmhc whatever sample size considered hence ability generalize better. regarding quality network structure pretty much dead heat algorithms small sample sizes however found perform signiﬁcantly better larger sample sizes. increase factor decays rapidly sample size increases. overall running time performance concerned table methods tendency work comparatively well small sample sizes total running time samples times slower average mmhc. overall appears running time increase factor grows somewhat linearly sample size. nonetheless worth mentioning implementation mmhc bnlearn package employs several heuristics speed learning implemented hpc. leads loss eﬃciency compared mmhc redundant calculations. notice optimization code currently undertaken allow fair comparisons mmhc. overall compares favorably mmhc. consistently lower generalization error data sets. large values recall cause much rise precision maintaining total running time control. experiment indicates mmpc best suited terms performance coupled optimal learning method discussed prime conclusion promising approach constructing structures. performances raises interesting possibilities context hybrid methods. emphasizes concentrating higher recall values keeping false positive rate possible pays terms goodness structure accuracy. focus study eﬃciency heuristics learning algorithms based i.e. maximization algorithms used score-based algorithms combined techniques learning dependence structure associated node algorithms. inﬂuence components overall learning strategy conditional independence tests network scores investigated. conclusions studies applicable shrinkage test robust small sample sizes permutation mutual information test large samples. tsamardinos recently showed exact tests based permutation procedures lead robust structural learning times slower asymptotic tests small sample sizes. similarly scutari investigated behavior permutation conditional independence tests tests based permutation pearson’s test permutation mutual information test shrinkage test based estimator mutual information. based single benchmark showed permutation tests result better network structures corresponding parametric tests terms goodness however output graphs often close true network structure ones learned corresponding parametric tests. shrinkage tests hand outperform parametric permutation tests quality network structure itself closer true dependence structure data well networks learned corresponding maximum likelihood tests. clear picture test employed given data set. still open question. noted possible reduce complexity optimal search exponential factor using structural constraint super-structure condition super-structure sound assumption accuracy resulting graph greatly improve according. consequently attention paid learning sound superstructures rather true skeleton data speed accuracy expected sophisticated search strategies greedy used study. although sound super-structures easier learn high values type error values produce denser structures many extra edges thereby resulting high computational overheads. therefore relaxing type error tests solution. show small change type error mmpc yields dramatical increase computational burden involved hybrid procedure almost gain accuracy. keep false positive rate small controlling false missing rate. finally worth mentioning neither mmpc optimized work learn global superstructures mmpc independently node without keeping track dependencies found previously. leads loss eﬃciency redundant calculations. reason initially designed infer local network around target node. optimized version super-structure discovery developed optimizations done order global method lower computational cost maintaining performance. optimizations include cache store dependencies global structure. optimizations reduce computational cost average according authors. discussed hybrid algorithm structure learning called hybrid extensive experiments show outperforms mmhc terms goodness training data data well hence ability generalize better little overhead terms running time mmhc. optimization code currently undertaken. regarding quality network structure found outperform mmhc signiﬁcant margin. importantly experimental results show clear beneﬁt terms edge recall without sacriﬁcing number extra edges crucial soundness super-structure used second stage hybrid methods like ones proposed though discussed here topic considerable interest would ascertain independence test suited data hand. needs substantiation experiments analysis.", "year": 2015}