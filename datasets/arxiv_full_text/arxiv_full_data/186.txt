{"title": "Option Discovery in Hierarchical Reinforcement Learning using  Spatio-Temporal Clustering", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.", "text": "every primitive action every granular state often lose sight bigger picture series actions abstracted single macro action agent remember series actions useful getting temporally distant useful state initial state. typically referred skill speciﬁcally option good analogy human planning movement current location destination identify intermediate destinations lead planning rather worrying exact mechanisms immediate movement abstracted over. options convenient formalise abstraction. keeping general philosophy reinforcement learning want build agents automatically discover options without prior knowledge purely exploring environment. thus approach falls broad category automated discovery skills. skills learnt task could easily reused different task necessary. focus paper present framework automated discovery skills. automated discovery skills options active area research several approaches proposed same. current methods could broadly classiﬁed sample trajectory based partition based methods. identifying bottlenecks state space state space partitioned sets. transitions sets states rare introduce bottleneck states respective points transitions. policies reach bottleneck states cached options example using structure present factored state representation identify sequences actions cause otherwise infrequent changes state variables sequences cached away options paper introduces automated skill acquisition framework reinforcement learning involves identifying hierarchical description given task terms abstract states extended actions abstract states. identifying structures present task provides ways simplify speed reinforcement learning algorithms. structures also help generalize algorithms multiple tasks without relearning policies scratch. ideas dynamical systems metastable regions state space associate abstract states. spectral clustering algorithm pcca+ used identify suitable abstractions aligned underlying structure. skills deﬁned terms sequence actions lead transitions abstract states. connectivity information pcca+ used generate skills options. skills independent learning task efﬁciently reused across variety tasks deﬁned model. approach works well even without exact model environment using sample trajectories construct approximate estimate. also present approach scaling skill acquisition framework complex tasks large state spaces perform state aggregation using representation learned action conditional video prediction network skill acquisition framework aggregated state space. core idea hierarchical reinforcement learning break reinforcement learning problem subtasks hierarchy abstractions. terms markov decision processes well studied framework reinforcement learning literature looking full reinforcement learning problem assume agent state time step. agent performs several possible primitive actions along current state decides next state. however large problems lead much granularity agent decide certain deﬁciencies methods even though varying degrees success. bottleneck based approaches natural identifying part state space options applicable without external knowledge problem domain. spectral methods need form regularization order prevent unequal splits might lead arbitrary splitting state space. also attempts learn skill acquisition robotics. attempt discover skills robot given abstractions surroundings operate with. inverse reinforcement learning skill discovery reward segmentation. however works assume knowledge abstractions world demonstrations expert trajectories respectively goal acquire skills discover abstractions without prior knowledge. present framework detects well-connected meta-stable regions state space model estimated trajectories. pcca+ spectral clustering algorithm conformal dynamics partitions different regions also returns connectivity information regions unlike clustering approaches used earlier option discovery. helps build abstraction call regions identiﬁed pcca+ abstract states. deﬁne options take agent abstract state another connected abstract state. since pcca+ also returns membership function states abstract states propose efﬁcient constructing option policies directly membership function perform hill climbing granular states membership function destination abstract state yield option policy without learning. since abstract states aligned underlying structure option policies could efﬁciently reused across multiple tasks mdp. options could standard reinforcement learning algorithms learn policy subtasks solve given task. speciﬁcally smdp q-learning intra option q-learning experiments. note approach works well even without exact model full able work approximate estimate model using sample trajectories demonstrated experiments. finally present attempt extend pipeline large state spaces spectral methods original state space infeasible. therefore propose operate pcca+ pipeline aggregated state space. original state space aggregated clustering representation state space learned using deep neural networks. speciﬁcally learn representation captures spatio-temporal structure state-space borrow framework uses convolutional-lstm action conditional video prediction network predict next frames game conditioned trajectory far. arcade learning environment platform provides simulator atari games. present results atari game seaquest relatively complex game requires abstract moves like ﬁlling oxygen evading bullets shooting enemies. summarize contributions paper below novel automated skill acquisition pipeline operate without prior knowledge abstractions world. pipeline uses spectral clustering algorithm conformal dynamics pcca+ builds abstraction segmenting different regions called abstract states also provides membership function states abstract states. elegant composing options membership function obtained hill climbing membership function target abstract state granular states belonging current abstract state. extension pipeline complex tasks large state spaces propose pipeline aggregated state space infeasibility operate original state space mdp. aggregate state space learn representation capturing spatio-temporal aspects state space using deep action-conditional video prediction network. approach considered attempt address automated option discovery using spatio-temporal clustering spatial abstract states segmenting state space abstractions temporal since transition structure used discover abstractions. markov decision process markov decision process widely used framework reinforcement learning algorithms used learn control policies. ﬁnite discrete formalised ﬁnite discrete state space ﬁnite action space distribution next states action performed state corresponding reward specifying scalar cost reward associated transition. policy sequence actions involved performing task value functions measure expected long term return following policy mdp. termination function agent enters state following option option could terminated probability agent state start option choose options primitive actions taken choice dictated policy guiding agent. transformed basis identiﬁed vertices simplex subspace ﬁrst order perturbation simplex linear transformation around origin simplex vertices needs points form convex hull deviation points hull minimized. achieved ﬁnding data point located farthest origin iteratively identify data points located farthest hyperplane current vertices. refer algorithm details. smdp value learning appended options semi-markov decision process smdp time step agent selects option initiated state follows corresponding option policy termination. smdp theory effects option modelled represent total return using probability terminating state option initiated state option viewed indivisible unit structure utilized. update rule smdp qlearning given action/option value function. intra-option value learning major drawback smdp learning methods option needs completely executed upto termination before learning outcome. point time would potentially learning value function option primitive action. slows learning signiﬁcantly large problems scale atari games. propose options consistent observed trajectory values updated efﬁciently. help learn values certain options without even executing options. option chosen state time terminates time t+τ. instead using single training example update smdp methods markov nature option exploited transitions time steps valid training examples. q-value options consistent actions taken time steps also updated allowing efﬁcient off-policy learning. perron cluster analysis given algebraic representation graph representing want suitable abstractions aligned underlying structure. spectral clustering algorithm this. central idea spectral clustering graph laplacian obtained similarity graph. approach spectra laplacian constructed best transformation spectra found transformed basis aligns clusters data points eigenspace. projection method described used membership states special points lying figure simplex first order higher order perturbation shows visualization ﬁrst order perturbation assumption identify simplex vertices data points higher order perturbations. ﬁrst case shows data points satisfy ﬁrst order assumption hence simplex perfectly without noise. second case simplex vertices obtained linear transformation able capture structure noise deviations could understood effect higher order perturbations present. compute eigenvectors corresponding eigenvalues stack column vectors eigenvector matrix let’s denote rows yy··· deﬁne index maximal. deﬁne span{y deﬁne index distance hyperplane i.e. γi−|| maximal. deﬁne span{y··· compute γi−|| γi−|| place clustering algorithms. inpired pcca+ detect conformal states dynamical system operating transition structure. providing graph pcca+ derive abstraction simplex vertices identiﬁed pcca+ abstract states. also membership function pcca+ deﬁnes degree membership state abstract state describe information used compose options next section. given membership functions abstractions discovered pcca+ provide elegant compose option policies move abstract state another. transition another abstract state done simply following positive gradient membership value destination abstract state. navigates agent states whose membership functions target abstract state progressively increases. states s··· abstract states s··· typically denote membership state abstract state said belong abstract state argmaxjχij. thus option abstract states start state state connectivity information abstract states given entry laplacian corresponding transition matrix. diagonal entries provide relative connectivity information within cluster. generate option every pair connected abstract states. option initiation represents states belong option policy takes agent abstract state stochastic gradient function given finally termination condition function assigns probability termination current option state could also viewed probability state decision epoch given current option being executed. option taking agent abstract state deﬁne follows comparison clustering algorithms particular concerned dynamical systems priori assumptions size relative placements metastable states. becomes essential clustering algorithm work generalized setting possible. tested different spectral clustering algorithms different classes problems. pcca+ found give better results capturing structure normalized algorithms pcca+ deep similarities main difference behind usage pcca+ change point view identiﬁcation metastable states crisp clustering relaxed almost invariant sets. methods similar till identiﬁcation eigenvector matrix points dimensional subspace this standard clustering algorithms like k-means pcca+ uses mapping simplex structure. since bottlenecks occur rather rarely shown general cases soft methods outperform crisp clustering. although spectral algorithms result good clusters simple room-in-aroom domain give poor results topologically complex spaces need form regularization work well settings pcca+ intrinsic regularization mechanism able cluster complex spaces neatly. spatial abstraction using pcca+ transition structure could arbitrarily complex hence identify abstractions well aligned state space transition structure pcca+ abstract offer intuitive explanation choice option policy option termination condition expected increase membership function abstract state taking action state membership function current state probability action involved taking agent state abstract state must proportional expected increase membership function abstract state positive expected increase negative. choice termination condition also simple heuristic transitioning agent would encounter bottleneck state would approximately satisfy hence suitable termination option. till then membership value would higher hence probability terminate would proportional much bottleneck current state even though look bottleneck states directly approach unlike termination condition propose naturally captures transitioning bottleneck states. previous section discussed discover options given abstractions obtained transition structure mdp. however online agent prior knowledge model learn abstractions scratch. present algorithms respectively online agent perform option discovery using spatiotemporal clustering cases small state space task large state space task sample trajectories using current behavioral policy estimate model sample trajectories operate pcca+ estimated model derive abstract states memberships discover options abstract states memberships augment agent options update value functions behavioral policy using smdp learning might expensive perform pcca+ large scale problems every episode hence could update skills options pcca+ ﬁxed number episodes depending problem complexity. though small scale problems ﬁrst initial exploration could come random policies wouldn’t work large problems like seaquest informed exploration partially trained deep network necessary ensure sample trajectories using current behavioral policy trajectory data points action conditional video pred network learned representation aggregate states microstates k-means estimate model microstate space sample trajectories operate pcca+ estimated model derive abstract macrostates memberships discover options abstract macrostates memberships augment agent options update value functions behavioral policy using intra-option learning model estimation incorporation reward structure every sampled trajectory maintain transition counts number times transition startφa transition counts used populate local adjacency matrix transition uposterior) uprior) every sampled trajectory. pcca+ used suitable spatial abstractions subtask options transition probabilities hill-climbing step calculated smdp value learning algorithm optimal policy constructed options. encodes topological properties state space. kinds structures environments would like autonomous agent discover. example monkey climbing tree pick fruit abstracts task reach branch tree nearest fruit. structures functional nature depending functional properties task. reward counts transition encode functional properties transition structure. idea spatio-temporal abstractions degenerate places spike reward distribution agent interprets state high reward different abstract state naturally composes options would lead state. reward performing transition modify local adjacency matrix posterior update regularization constant balances relative weight underlying reward transition distribution. exponential weighting rewards ensure adjacency function value spike points want abstraction degenerate. also returns room domain consider simple room domain shown. world composed rooms separated walls shown current task assume goal agent start tile marked reach goal tile marked agent either move step direction towards north east west south. pcca+ discovers three abstract states without incorporating reward structure corresponding room -room world. incorporation reward structure would abstract states additional corresponding goal state alone. doorway could seen bottleneck states membership functions lead termination conditions options exiting abstract state entering another abstract navigation task could simply abstracted exiting current room moving doorway entering correct room reach goal state visualize membership function identiﬁed pcca+ -room world without reward structure incorporation. clear abstract state identiﬁed three rooms world transition structure neatly segmented pcca+. shows segmentation option policies discovered without reward structure incorporated. ﬁrst case without reward structure abstract states corresponding room hence option policies discovered would navigation room another. ﬁgure shows stochastic option policies navigate room rooms. second case reward considered transition structure abstract states identiﬁed lonely goal state bottom right corner indicated. ﬁgure shows option policies getting room rooms getting group granular states goal state room goal state. hence clearly advantage discovering structural functional abstractions since solving task simple planning move abstract state another instead trying learn optimal policy every granular state mdp. seaquest atari games widely used benchmarks reinforcement learning algorithms large state spaces etc. infeasibility applying pcca+ framework directly exponential state space perform state space aggregation using k-means clustering. however aggregating directly input pixel space seaquest infeasible latent concepts captured space. therefore adopt action conditional video prediction network learning latent space captures spatio-temporal concepts. overall pipeline described algorithm helps appreciate semantics example option generated seaquest. plot agent’s membership abstract state corresponding particular option randomly chosen episode’s trajectory. skill learnt corresponds resurfacing replenish oxygen seaquest. paper presented framework automated skill acquisition reinforcement learning builds abstraction well aligned transition structure discovers skills options enable movement across abstract states. framework works without exact model option policies obtained elegant performing hill climbing membership function abstract states. options also efﬁciently reused across multiple tasks sharing common transition structure. also discuss approach scaling pipeline larger state spaces performing state aggregation clustering representation discovered unsupervised model prediction network.", "year": 2016}