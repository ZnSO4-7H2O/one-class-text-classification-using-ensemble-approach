{"title": "State Space Decomposition and Subgoal Creation for Transfer in Deep  Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Typical reinforcement learning (RL) agents learn to complete tasks specified by reward functions tailored to their domain. As such, the policies they learn do not generalize even to similar domains. To address this issue, we develop a framework through which a deep RL agent learns to generalize policies from smaller, simpler domains to more complex ones using a recurrent attention mechanism. The task is presented to the agent as an image and an instruction specifying the goal. This meta-controller guides the agent towards its goal by designing a sequence of smaller subtasks on the part of the state space within the attention, effectively decomposing it. As a baseline, we consider a setup without attention as well. Our experiments show that the meta-controller learns to create subgoals within the attention.", "text": "typical reinforcement learning agents learn complete tasks speciﬁed reward functions tailored domain. such policies learn generalize even similar domains. address issue develop framework deep agent learns generalize policies smaller simpler domains complex ones using recurrent attention mechanism. task presented agent image instruction specifying goal. meta-controller guides agent towards goal designing sequence smaller subtasks part state space within attention effectively decomposing baseline consider setup without attention well. experiments show meta-controller learns create subgoals within attention. introduction usually reinforcement learning agents cannot generalize policies learned small domains larger complicated ones. ability allow immediately apply skills learned simple settings without explore large state space. deep learning setting reduction size state also means smaller networks required easier train. work present approach decompose complicated environments simpler ones provide subgoals within ultimately solve larger task. agent pre-trained smaller environment solve subgoal independently conjunction subgoal creation algorithm. describe meta-controller learns decompose state space provide subgoals solvable within smaller space. meta-controller solving delayed reward problem gets positive reinforcement underlying agent solves original task. come sequence subgoals maximizes expectation reinforcement. addition creating subgoals meta-controller also fragments state space underlying agent presented smaller state easily learn optimal policy subgoal. using attention mechanism similar recurrent attention model meta-controller learns control attention passes part state within agent. meta-controller’s formulation states summaries past current attentions. actions locations attention lattn distribution subgoals rewards positive underlying agent solves task small negative step cost otherwise transitions underlying agent executes policy according state subgoal provided since meta-controller selects value lattn distribution state space location subgoal passed underlying agent. agent chooses atomic action moves towards achieving agent location lagent changes meta-controller’s environment picks attention subgoal. work make simplifying assumptions. first assume underlying agent access optimal policy subgoal. goal-dependent policy learned technique universal value function approximators uvfas learn approximate value function respect goal using function approximator deep neural nets. learned value function used construct policy achieves goal value function trained independent conjunction metacontroller providing intrinsic rewards achieving subgoals secondly assume agent remains still unless location subgoal present within state provided meta-controller. general meta-controller automatically incentivized focus attention provide subgoals underlying agent able solve given task reward structure. case means keeping agent location subgoal within attention. example game pacman subgoal closest pill underlying agent pacman least pill within state provided otherwise agent move randomly unable achieve overall goal getting high score. assumptions simplify training meta-controller methodology provide applicable general setting policy underlying agent learned well. related work work closely matches kulkarni present hierarchical framework agent learns intrinsic rewards provided higher level agent setting subgoals operating longer time frame. rewards higher level agent provided environment completing tasks. subgoals turn provided functions entities relations object oriented framework. sense approach takes step decomposing state space base agent small portion time. allows better computational efﬁciency base agent smaller networks allow transfer learnt policies different parts state space similar without explicitly explore them. achieve this higher level agent meta-controller must learn integrate information states observed far. therefore recurrent model represent meta-controller long short term memory network kulkarni pair dqns agent meta-controller. order train attention mechanism meta-controller employ technique similar mnih policy gradients train attention mechanism classiﬁcation simple control tasks. approach employ complex glimpse sensor instead simply crop input image. incorporated setup easily. further instead specifying lattn directly using continuous output discrete actions down noop move attention. finally schaul describe goal speciﬁc function approximators constructed deep agents. function constructed base learner independently learning subgoals image. provide results setting paper integrated future work. reinforcement learning addresses problem choosing behavior maximizes notion long term cumulative reward. typically formulated markov decision process characterized tuple states agent actions agent available state. typically agent chooses action execute lead state according transition function scalar value received upon executing action state. finally discount factor. policy informs agent action execute state. goal reinforcement learning agent optimal policy maximizes long term expected reward utility state. policy gradient methods look directly directly maximize expected reward adjusting policy parameters. expected reward trajectory sampled policy parametrized given expectation approximated sampling batch trajectories current policy averaging gradients them. reinforce algorithm vanilla policy gradient suffer high variance relies monte carlo samples. common modiﬁcation reduce variance subtract baseline returns. baseline computed taking average observed returns past time steps n=t−n rtn. work chose experiments environment consisting grid. grid consists four rooms room horizontal strip exceeding rooms stacked different color {red green blue yellow}. environment also generates instruction one-hot vector length specifying target room. episode terminates either agent reaches target room receiving positive reward times without reaching step cost construct three frameworks meta-controller tasked providing subgoals underlying agent navigates successfully target room. experiments meta-controller uses adam optimizer learning rate agent attention always start left corner grid. first simplify problem providing entire state space input meta-controller time step therefore employing attention mechanism. speciﬁcally meta-controller receives input image grid contains rooms well location agent lagent. output meta-controller distribution rooms. room sampled provided base agent instruction. subgoal must achieve. assumed underlying agent move optimally entire grid given instruction. setup optimal policy meta-controller always output target room directly. figure network architecture partial state decomposition constrained attention mechanism experiments. attention window target instruction inputted time step meta-controller outputs probability distributions attention actions subgoal instructions. setup meta-controller attention mechanism consists window grid. addition subgoal instruction must also output action control attention. here attention mechanism partially decomposes state space meaning agent move optimally provided subgoal even agent within current attention. subgoal however must located inside attention. goal meta-controller location target room using attention mechanism instruct agent room color every time step. architecture meta-controller consists state processor network takes attention window target instruction provided environment input time step. processes inputs using feedforward convolutional network uses lstm unit output probability distribution attention actions. convolutional layers network rectiﬁed linear unit activation functions. attention action affects next attention location lattn subgoal instruction affects next agent location lagent. thus lstm’s hidden state contains knowledge gained taking sequence instruction attention actions episode. order train network effectively policy gradients assume attention instruction actions probabilities independent other. setup agent move unless within attention. means meta-controller must instruct agent move room within view attention moving downwards repeating process agent reached target room. thus agent subgoal appear within decomposed state space target task achieved. partial state decomposition constrained attention mechanism experiments lstm unit allows meta-controller memory locations agent target room guide action selection either agent target room present within attention window particular time step. constrained attention mechnism framework adds additional step constructing optimal sequence subgoals agent reach target room overall goal paper. experiments using meta-controller attention mechanism. ﬁrst experiment environment ﬁxed i.e. room arrangement ﬁxed episodes target room always bottom. optimal policy meta-controller simply output instruction corresponding target room since underlying agent optimal agent entire grid. second experiment environment dynamic means room arrangement randomly generated episodes target room always located bottom. here must learn mapping color bottom-most room optimal instruction. experiments serve baseline experiments using meta-controller attention mechanism. cases meta-controller converges optimal policy guiding agent correct room. figure episode lengths training episodes. dots represent means variable number episodes dependent total number episodes displayed whiskers represent variance. case meta-controller attention converges quickly ﬁxed environment supplying agent target room hence completing episodes minimum time. takes longer converge dynamic case. meta-controller attention plots show results effect using attention guide subgoal creation. experiments environment kept ﬁxed. meta-controller attention mechanism here show results meta-controller partial state decomposition constrained attention mechanism. experiments keep environment ﬁxed episodes. compared figure cases takes longer train meta-controller output subgoals leading optimal policy. reason meta-controller control attention addition creating subgoals. also operating partially observed setting integrate information gleaned past attentions hidden state. note state space meta-controller combinations attention window target instruction hidden state lstm unit. since possible learn underlying agent trained entire image attention size. approach scale even larger sized domains directly learning policy original input image infeasible. conclusion overall contribution framework allows agent complete task large environment given knowledge smaller environment. attention mechanism smaller networks required easier train. three frameworks developed meta-controller learns representation room colors representation transfers sub-instructions lead agent desired goal. results show possible scale policy learned smaller environment decomposing large state space using attention mechanism. eventual goal train underlying agent conjunction meta-controller apply framework dynamic complex environments. schaul horgan karol gregor david silver. universal value function approximators. proceedings international conference international conference machine learning volume icml’ pages jmlr.org", "year": 2017}