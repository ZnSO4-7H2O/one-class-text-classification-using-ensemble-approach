{"title": "Sequential Feature Explanations for Anomaly Detection", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g. a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation's quality is related to the number of features that must be revealed to attain confidence. One of our main contributions is to present a novel framework for large scale quantitative evaluations of SFEs, where the quality measure is based on analyst effort. To do this we construct anomaly detection benchmarks from real data sets along with artificial experts that can be simulated for evaluation. Our second contribution is to evaluate several novel explanation approaches within the framework and on traditional anomaly detection benchmarks, offering several insights into the approaches.", "text": "given outlier point analyst faces problem analyzing data associated point order make judgement whether anomaly not. even points described tens features challenging especially feature interactions critical judgement. practice situation often much worse points described thousands features. cases signiﬁcant risk even anomaly detector passes true anomaly analyst analyst recognize properties make point anomalous information overload. means that eﬀect missed anomaly rate overall system combination miss rates anomaly detector analyst. thus avenue improving detection rates reduce eﬀort required analyst correctly identify anomalies intended side-eﬀect reducing analyst miss rate. paper consider reducing analyst’s detection eﬀort providing explanations points judged anomalous detector. given explanation analyst minimize eﬀort focusing investigation information related explanation. ﬁrst contribution introduce intuitive simple form explanation refer sequential feature explanations given point judged outlier detector point ordered sequence features order indicates importance respect causing high outlier score. presented analyst incrementally revealing features time order analyst acquired enough information make decision whether point anomaly investigative work analyst roughly related number features must revealed. hence goal computing sfes minimize number features must revealed order analyst conﬁdently identify true anomalies. second contribution formulate quantitative evaluation methodology evaluating sfes allowing comparison diﬀerent algorithms. idea approach construct simulated analyst anomaly detection benchmark using supervised learning ground truth points anomalies. simulated analyst used evaluate quality sfes respect number features must revealed reach speciﬁed conﬁdence level. best knowledge ﬁrst methodology quantitamany applications anomaly detection system presents anomalous data instance human analyst must determine whether instance truly interest unfortunately anomaly detectors provide explanation instance considered anomalous leaving analyst guidance begin investigation. address issue study problems computing evaluating sequential feature explanations anomaly detectors. anomaly sequence features presented analyst time information contained highlighted features enough analyst make conﬁdent judgement anomaly. since analyst eﬀort related amount information consider investigation explanation’s quality related number features must revealed attain conﬁdence. main contributions present novel framework large scale quantitative evaluations sfes quality measure based analyst eﬀort. construct anomaly detection benchmarks real data sets along artiﬁcial experts simulated evaluation. second contribution evaluate several novel explanation approaches within framework traditional anomaly detection benchmarks oﬀering several insights approaches. anomaly detection problem identifying anomalies data anomalies points generated process distinct process generating normal points. statistical anomaly detectors address problem seeking statistical outliers data. application however statistically outliers always correspond semantically-meaningful anomalies. example computer security application user considered statistically anomalous unusually high amount copying printing activity reality benign explanation hence true anomaly. statistics semantics analyst typically investigates statistical outliers order decide ones likely true anomalies tively evaluating type anomaly explanation method. third contribution deﬁne several algorithms computing sfes applied density-based anomaly detector. main requirement algorithms possible compute joint marginals detector’s density function operation supported commonly-used densities. finally fourth contribution provide empirical investigation several methods computing sfes. primary evaluations recently constructed anomaly detection benchmarks derived real-world supervised learning data. addition provide evaluation standard kdd-cup benchmark. investigation leads recommended method additional insights methods. remainder paper organized follows. section reviews related work explanations supervised learning anomaly detection. next section presents anomaly-concepts formulation used paper. section formally presents concept sfes possible quality metrics. section describes contrast several methods computing sfes. section introduces quantitative evaluation framework sfes ﬁnally section presents experiments evaluating introduced methods within framework. problem computing explanations supervised learning unsupervised settings anomaly detection received relatively little attention. related work area supervised classiﬁcation aims provide explanations classiﬁer predicted particular label particular instance. example number methods proposed produce explanations form relevance scores feature indicate relative importance feature classiﬁcation decision. scores computed comparing diﬀerence classiﬁer’s prediction score score feature assumed unobserved considering local gradient classiﬁer’s prediction score respect features particular instance work considered score features takes account joint inﬂuence feature subsets classiﬁcation score usually requires approximations exponential number subsets since methods typically based availability class-conditional probability function directly generalizable computing explanations anomaly detectors. experiments however evaluate method called dropout inspired approach form feature-relevance explanations similar nature sfes provide ordering features. however prior work explicitly considered concept sequentially revealing features analyst part proposal reducing analyst eﬀort. prior work feature-based explanations anomaly detection focused primarily computing explanations form feature subsets. explanations intended specify subset features jointly responsible object receiving high anomaly score. example micenkova computed subset features projection anomalous object onto features shows greatest deviation normal instances. issue approach computation explanation independent anomaly detector employed. contrary goal trying explain particular anomaly detector judged particular object anomalous. contrast explanation approaches consider paper sensitive particular anomaly detector. work computing feature-subset explanations developed anomaly detection system called lodi includes specialized explanation mechanism particular anomaly detector. similar approach considered dang anomaly detection mechanism directly searches discriminative subspaces used purpose explanation. contrast explanation approaches consider work instantiated anomaly detection scheme based density estimation includes large fraction existing detectors. existing approaches evaluating explanations methods supervised unsupervised settings typically quite limited scope. often evaluations limited visualizations illustrations several example explanations testing whether computed explanation collectively conforms known concept data often synthetically generated data. prior work proposed larger scale quantitative evaluation methodology explanations main contributions work. dimensional real-valued vector. contains mixture normal points anomaly points generally normal points account overwhelming fraction data. applications anomaly detection anomaly points generated distinct process normal points particular process important detect particular application. example data points describe usage behavior users corporate computer network anomalies correspond insider threats. since typically large manual search anomalies points generally practical. statistical anomaly detectors address issue seeking identify anomalies ﬁnding statistical outliers. problem however outliers correspond anomalies practice analyst must examine outliers decide ones likely anomalies. analyst detects anomaly presented anomaly point able determine enough evidence point indeed anomaly. success approach depends anomaly detector’s precision identifying anomalies outliers also analysts’ ability correctly detect anomalies. without assistance analyst need consider information related features anomaly point analysis. many cases considering information thoroughly impossible increasing chance detecting anomalies costly many domains. order reduce analyst’s eﬀort toward detecting anomalies propose provide analyst sequential feature explanations attempt eﬃciently explain point considered outlier. length point ordered list feature indices features appear earlier order considered important high outlier score point notation denote ﬁrst feature indices also feature indices data point denote projection onto subspace speciﬁed given point point incrementally presented analyst ﬁrst presenting feature analyst able make judgement based information ﬁnished point. otherwise next feature added information given analyst analyst sees process incrementally adding features presented information continues analyst able make decision. process also terminate early time constraints; however don’t study case paper. help analyst eﬃciently exonerate points. contrast anomalies reasonable expect analyst would able detect anomalies considering much smaller amount information without reduce chance missed detections. assume amount analyst eﬀort monotonically increasing function number features considered. motivates measuring quality target number features must revealed analyst correct detection. formally given anomaly point analyst minimum feature preﬁx denoted minimum number features must revealed order speciﬁed detect anomaly. provides quantitative measure quality deﬁnition requires access analyst. complicates comparison computation methods terms mfp. section addresses issue describes approach conducting wide evaluations terms mfp. consider methods computing sfes anomaly detectors. prior work computing explanations anomaly detectors either computed explanations depend particular anomaly detector used used methods speciﬁc particular anomaly detector wish avoid former approach since intuitively explanation attempt indicate particular detector employed found point outlier. considering latter approach seek general methods applied widely across different detectors. thus consider explanation methods widely-studied class density-based detectors. density-based detectors operate estimating probability density function methods actually employed general class score-based detectors provided scores computed given subset features. simplicity focus density-based detectors paper density function used compute scores. entire points treating density normal points. reasonable usual assumption anomalies rare compared normals. points ranked according ascending values least normal objects according highest order. methods assume knowledge form require interface allows joint marginal values computed. subset feature indices point require compute many choices mixtures gaussians joint marginals simple closed forms. closed form available exact approximate inference techniques employed. worth noting considering methods depend anomaly detector used performance terms depend quality anomaly detector well method. example consider situation anomaly detector judges anomaly point outlier reasons semantically relevant anomaly. likely help analyst eﬃciently determine anomaly since semantically critical features appear late ordering. possibility control method. thus designing methods assume outlier judgements made semantically meaningful respect application. present main classes methods refer marginal methods dropout methods. consider modeling analyst bayesian classiﬁer assumes normal points generated according anomalies uniform distribution support feature space reasonable assumption absence prior knowledge anomaly distribution. given point number revealed features analyst would make decision whether anomaly comparing likelihood ratio threshold. since assumed uniform threshold. intuitively means goal cause analyst quickly decide anomaly chose yields small values particularly small seqmarg method adds feature time step adding feature minimizes joint marginal density previously-selected features. formally seqm computes following explanation complement seqm requires joint marginal computations order compute explanation length note inherent greediness seqm necessarily optimal features minimizing rather goal optimize particular value would need consider feature subsets size however problem formulation provide target value thus seqm oﬀers tractable approach focuses addition seqmarg also consider computationally cheaper alternative called independent marginal requires computation individual marginals approach simply selects explanation sorting features increasing order requires marginal computations computing explanation length. indmarg offers computationally cheaper alternative seqmarg fails capture joint feature interactions. example seqmarg select optimizes joint value combined previous features ei−. instead indmarg ignores interactions previously-selected features. thus indmarg serves baseline understanding importance accounting joint feature interactions computing explanations. next methods inspired work robniksikonja kononenko computing feature-relevance explanations supervised classiﬁers. work relevance score feature diﬀerence classiﬁcation score feature provided classiﬁer classiﬁcation feature omitted analogous approach anomaly detection score features according change density value feature included feature included marginalized out. yields ﬁrst dropout method referred independent dropout given approach requires number marginal computations seqmarg. algorithm viewed dual seqmarg measures contribution feature sets according much normal point looks removal whereas seqmarg measures abnormal point looks features included. least challenges involved evaluating anomaly-explanation methods. first compared supervised learning area anomaly detection many fewer established benchmark data sets particularly benchmarks based real-world data. second given benchmark data immediately clear quantitatively evaluate explanations since benchmarks come either ground truth explanations analysts. benchmarks based real-world data. address second issue using supervised learning construct simulated analyst applied quantitatively evaluate explanations terms mfp. expand points. recent work described methodology systematically creating anomaly detection benchmarks supervised learning benchmarks given huge number real-world supervised learning benchmarks allows corresponding huge number diverse anomaly detection benchmarks. further benchmarks created controllable measurable properties anomaly frequency clusteredness normal anomalous points. brieﬂy sketch main idea. given supervised classiﬁcation data called mother approach selects classes represent anomaly class diﬀerent choices giving rise diﬀerent properties anomaly class. union classes represents normal class. individual anomaly detection benchmarks created sampling normal anomaly points speciﬁed proportions. table gives summary benchmarks emmott used experiments. example data shuttle used mother create distinct anomaly detection benchmarks. number points shuttle benchmarks range number anomalies ranges ability point normal considering features speciﬁed describe obtain function experiments below. given function point generate analyst certainty curve plots analyst’s certainty revealing features versus figure shows example three analyst curves experiments using simulated analysts benchmark computed abalone dataset. curves correspond diﬀerent anomaly data using explanations computed using seqmarg. diﬀerent anomalies lead diﬀerent rates analyst becomes certain anomaly certain point normal. recall proposed quality metric measures number features must revealed analyst according order detect anomaly evaluating metric requires deﬁne conditions analyst detects model probability normality becomes small enough. denote analyst given compute anomaly point recording number features required analyst certainty curve ﬁrst drop figure analyst certainity curves. example curves generated using simulated analyst anomalies abalone benchmark using sfes produced seqmarg. x-axis shows index feature revealed step y-axis shows analyst certainty anomalies normal. leftmost curve shows example analyst gradually becomes certain point anomalous middle curve shows rapidly growing certainty. middle curves example analyst certain anomaly ﬁrst feature revealed remains certain. tribution values models range reasonable thresholds. given distribution report expected mfp—the expected value e)—as quantitative measure anomaly experiments deﬁne uniform values noting results consistent across variety reasonable choices distribution. remains specify obtain analyst function since anomaly detection benchmarks derived mother classiﬁcation data construct training points anomaly normal classes. given training approach obtaining analyst would learn generative model joint distribution could used compute marginalizing features included however generative models tend much less accurate practice compared discriminative moddirectly support computing probability arbitrary subsets require. heuristics proposed purpose found unreliable applied widely. thus work follow brute force approach. simply pre-learn individual discriminative model possible subset features maximum size evaluating simply requires evaluating model associated subset number features number data points large possible pre-learn possible subsets. cases option learn cache models needed evaluation used approach kdd-cup results reported experiments. shown competitive density-based approach across wide range benchmarks. egmm based learning density function represented ensemble gaussian mixture models approach independently learns models training using expectation-maximization procedure bootstrap replicates data set. discards low-likelihood gmms retains others based pre-speciﬁed threshold. number components gmms varied across ensemble. experiments ensembles included gmms using components. ﬁnal egmm density simply uniform mixture densities retained gmms. egmm approach addresses least pitfalls using single models. first training sometimes produce poor models local optima. second diﬃcult select best number components single model. egmm gains robustness performing model averaging variations. advantage using egmm model straightforward derive closed forms marginal density computations required explanation methods. particular overall egmm density viewed single large model containing mixture components across ensemble. since individual gaussians simple closed forms marginal densities easily obtain closed forms mixture. worth noting closed forms also derived egmm marginals data points transformed linear projections reduce dimensionality recall evaluation framework based using supervised learning order obtain simulated analyst. experiments based using regularized random forests analyst model. model selected primary reasons. first rrfs wellknown provide high accuracies competitive state-of-the-art across wide range classiﬁcation problems. second rrfs relatively eﬃcient train important study since must train possible subset features trained rrfs composed trees using -fold worth noting evaluation framework potentially sensitive choice analyst model since diﬀerent models diﬀerent biases. beyond scope ﬁrst study replicate experiments using qualitatively diﬀerent model. point future work. evaluation anomaly detection benchmarks emmott derived seven mother sets. summary benchmarks given table benchmarks total contain number points ranging number anomalies ranging egmm model benchmarks serve anomaly detector models trained mother possible feature subsets. ﬁrst study chosen focus benchmarks relatively small dimensionality order allow large scale study requires training large numbers egmm models analyst models. data experiments including analysts models made publicly available. evaluated methods computing sfes. included four methods section seqmarg indmarg seqdo inddo. addition evaluated random explanation method. case random report average performance across randomly generated sfes. finally order provide lower-bound attainable performance consider optimal oracle method optoracle. method allowed access simulated analyst number features computes optimal feature subset size formally value optoracle ﬁnds feature subset minimizes analyst’s conditional probability minimum value constrained produce sequential explanations—rather optoracle produce necessarily contain si−. gives optoracle additional advantage compared methods constrainted produce sfes. clearly optoracle represents upper bound performance method evaluated respect simulated analyst. benchmarks used corresponding egmm model rank points. anomaly points ranked computed sfes using methods. choice attempt model fact that actual operation highly ranked anomalies presented expert. expected computed using distribution analyst thresholds uniform values mother report average across anomalies derived mother set. average mfps shown figure along conﬁdence intervals. ﬁrst note observations sensitive choice focusing anomalies indeed also compiled results percentage points including using anomalies. main observations qualitatively similar across choices. comparison random optoracle. observe figure methods outperform random explanations often large margin. comparing optoracle that three benchmarks— concrete yeast wine—the lower bound provided optoracle signiﬁcantly better best method. could either suboptimal computations poor match anomaly detector’s notion outlier versus analyst’s notion anomaly fact optoracle constrained output sequential explanations. investigate below. methods quite close lower bound optoracle though still room improvement. finally worth noting optoracle able achieve mfps close mother sets. thus average data sets single feature suﬃcient allow correct analyst detections. independent versus sequential. reasonable expect sequential version marginal dropout methods outperform independent versions. sequential versions attempt account aggressively feature interaction computing sfes requires additional computation time. however overall little diﬀerence performance independent sequential methods. seqmarg indmarg achieve nearly identical performance. exception magic.gamma small statistically significant advantage seqmarg indmarg. possible explanation results feature interactions critical domains detecting anomalies. explanation supported fact optoracle able achieve average mfps close one. marginal versus dropout. recall marginal dropout methods dual approaches. marginal evaluates features terms abnormal features alone make point appear dropout evaluates increase normality score features removed. overall marginal methods never signiﬁcantly worse dropout signiﬁcantly better abalone magic.gamma shuttle skin. diﬀerence particularly large shuttle marginal methods close optoracle dropout methods closer random. possible explanation observed often dropout produce weaker signal compared marginal making early decisions. example considering single features diﬀerences scores produced dropout features often much smaller diﬀerences produced marginal. make dropout less robust early decisions important ones achieving small scores. recall dropout method inspired prior work explanations supervised learning. results suggest worth investigating adaptations marginal supervised setting. figure performance explanation methods benchmarks. group bars shows performance methods benchmarks derived single mother set. bars show expected averaged across anomalies benchmarks corresponding mother set. conﬁdence intervals also shown. ﬂect methods quality detector. attempt factor performance methods supplying methods oracle anomaly detector. simply replace simulated analyst’s conditional probabilalso ﬁrst feature would selected optoracle. unlike optoracle however seqmarg sequentially constrained select second feature works best combined ﬁrst selected feature. comparison optoracle. primary observation seqmarg* performs nearly identically optoracle domain. diﬀerence seqmarg* optoracle would reﬂect loss performance requiring sequential explanations. data sets little loss. good news since motivation considering sequential explanations reduce analyst’s eﬀort. particular sequential constraint means analyst shown incrementally growing information. rather without constraint optoracle could independent versus sequential. here seqmarg* often outperforming indmarg* sometimes signiﬁcant amounts. contrast results obtained using egmm anomaly detector. observation indicates reasoning feature interactions done seqmarg* important higher quality anomaly detection models. leaves open question whether able observe advantage using non-oracle anomaly detection models realistic benchmarks. dropout versus marginal. marginal methods show consistently better performance using oracle detectors. performance quite large several benchmarks. provides evidence marginal approach generally better computing sfes. hypothesize weak signal early decisions observed dropout method. show results kddcup intrusion detection bechmark points data features consider subset data containing instances involving http service. resulting benchmark contains approximately points approximately figure performance explanation methods benchmarks using oracle anomaly detector. group bars shows performance methods benchmarks derived single mother set. bars show expected averaged across anomalies benchmarks corresponding mother set. conﬁdence intervals also shown. paper introduced concept sequential feature explanations anomaly detection. main motivation reduce amount eﬀort analyst required correctly detect anomalies. described several methods computing sfes introduced framework allows large-scale quantitative evaluation explanation methods. experiments indicated that overall sequential marginal method computing sfes preferred method among introduced paper. anomaly points representing network intrusions. employed egmm anomaly detector. infeasible train simulated analyst feature subsets thus followed adaptive approach described section subset models required evaluation process learned cached. overall resulted approximately models trained. domain egmm model quite eﬀective ranked anomalies close ranked list. thus evaluate anomalies domain. figure shows average achieved methods. clear marginal methods signiﬁcantly better dropout methods here. particular seqmarg indmarg achieve average close smallest possible. indicates combination egmm marginal explanations effective domain. particular simulated analyst needed shown single feature average order correctly detect anomalies. hypothesize much weaker performance dropout methods weak signal provide early decisions. problem ampliﬁed context larger numbers features case kddcup data. emmott dietterich fern w.-k. wong systematic construction anomaly detection benchmarks real data proceedings sigkdd workshop outlier detection description", "year": 2015}