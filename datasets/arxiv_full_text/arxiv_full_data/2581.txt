{"title": "Unsupervised Risk Estimation Using Only Conditional Independence  Structure", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We show how to estimate a model's test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently differentiate the error estimate to perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as hidden Markov models.", "text": "show estimate model’s test error unlabeled data distributions different training distribution assuming certain conditional independencies preserved train test. need assume optimal predictor train test true distribution lies parametric family. also efﬁciently differentiate error estimate perform unsupervised discriminative learning. technical tool method moments allows exploit conditional independencies absence fully-speciﬁed model. framework encompasses large family losses including exponential loss extends structured output settings hidden markov models. assess accuracy model test distribution different training distribution? address question study problem unsupervised risk estimation —that given loss function ﬁxed model estimate risk def= exy∼p∗ respect test distribution given access unlabeled examples unsupervised risk estimation lets estimate model accuracy novel input distribution thus important building reliable machine learning systems. beyond evaluating single model also provides harnessing unlabeled data learning minimizing estimated risk perform unsupervised learning domain adaptation. unsupervised risk estimation impossible without assumptions otherwise observable information—could arbitrary. satisﬁed estimator depends strong underlying assumptions are. paper present approach rests surprisingly weak assumptions—that satisﬁes certain conditional independencies lies parametric family close training distribution. give ﬂavor results suppose loss decomposes independent conditioned case estimate risk error poly/\u0001 samples independently dimension dependence roughly cubic practice. sections generalize result capture exponential losses extend beyond multiclass case allow hidden state hidden markov model. intuition provided figure ﬁxed value think predicting high since provide independent signals rate agreement gives information model accuracy. predict likely true equals loss small. conversely predict different values loss likely larger. figure possible loss proﬁles given value left minimized value likely correct value total loss likely small. right conversely small differing values loss likely large. intuition formalized dawid skene take values discrete model mixture independent categorical variables impute label mixture component. several others extended idea continue focus loss continuous losses loss ignored given utility gradient-based learning? issue /-loss involves discrete prediction loss involves predictions former fully modeled k-dimensional family latter requires inﬁnitely many parameters. could assume predictions distributed according parametric family assumption wrong risk estimates wrong well. sidestep issue make method moments; method moments seen recent machine learning ﬁtting non-convex latent variable models much older history econometrics literature used tool making causal identiﬁcations structural assumptions even explicit form likelihood known upon older literature draw conceptual inspiration though technical tools closely based newer machine learning approaches. insight certain moment equations–e.g. derived assumed independencies; show estimate risk relying moment conditions parametric assumptions moreover moment equations also hold gradient enables efﬁcient unsupervised learning. paper structured follows. section present basic framework state prove main result estimating risk given section extend framework several directions including hidden markov models. section present gradient-based learning algorithm show sample complexity needed learning poly/\u0001 dimension section investigate method performs empirically. related work. formal problem unsupervised risk estimation posed recently donmez several older ideas domain adaptation semi-supervised learning also relevant. covariate shift assumption assumes access labeled samples base distribution close together approximate sample re-weighting close another approach assume well-speciﬁed discriminative model family need heed ﬁnite-sample error estimation assumptions somewhat stringent re-weighting allows small perturbations mis-speciﬁed models common practice. indeed many authors report mis-speciﬁcation lead severe issues semi-supervised settings mentioned above approach closer spirit dawid skene extensions. similarly zhang jaffe method moments estimating latent-variable models however papers tool parameter estimation face non-convexity rather sidestep model mis-speciﬁcation. insight moments robust model mis-speciﬁcation lets extend beyond simple discrete settings consider order handle complex continuous structured losses. another approach handling continuous losses given intriguing work balasubramanian show distribution losses often close gaussian practice estimate figure left basic -view setup center extension hidden markov models; embedding views indicated blue. right extension include mediating variable focus multiclass classiﬁcation; assume unknown true distribution given unlabeled samples drawn i.i.d. given parameters loss function goal estimate risk def= exy∼p∗ throughout make -view assumption assumption split conditionally independent given moreover loss decomposes additively across views note views always partition views blocks addition sufﬁces independent rather give examples assumption holds state prove main result. start logistic regression primary focus later example suppose log-linear model conditioned loss function log-loss assumption holds equal partition function nothing special hinge loss; instance could instead take /-losses. ﬁnal example shows linearity necessary assumption hold; model arbitrarily non-linear view long predictions combined additively example suppose view neural network whose output suppose together predictions prediction vector log-normalization constant softmax hence satisﬁes assumption estimated unlabeled data alone. caveat class permutation. suppose training time learn predict whether image contains digit test time nothing changes except deﬁnitions reversed. clearly impossible detect unlabeled data; mathematically manifests recoverable column permutation. computing minimum risk permutations call optimistic risk denote def= minσ∈sym exy∼p∗ equals true risk long least aligned correct labels sense optimal computed main result theorem says recover permutation number samples polynomial practice dependence seems roughly cubic. theorem suppose assumption holds. then estimate column permutation error algorithm requires note estimates imply estimate importantly sample complexity theorem depends number classes dimension moreover theorem holds even lies outside model family even train test distributions different requirement -view assumption holds interpret term first tracks variance loss expect difﬁculty estimating risk increase variance. term typical shows even estimating parameter bernoulli variable accuracy samples. term appears because classes rare need wait long time observe even single sample class even longer estimate risk class accurately. perhaps least intuitive term large e.g. classes similar conditional matters consider extreme risk vectors independent also completely unconstrained impossible estimate all. contradict theorem answer case rows equal hence need inﬁnitely many samples theorem hold; thus measures close degenerate case. proof theorem outline proof theorem recall goal estimate conditional risk matrices deﬁned recover risk using insight certain moments expressed polynomial functions matrices therefore solve even without explicitly estimating approach follows technical machinery behind spectral method moments explain completeness. deﬁne loss vector conditional independence means similarly higher-order conditional moments. thus low-rank structure moments exploit. precisely algorithm algorithm estimating unlabeled data. input unlabeled samples estimate left-hand-side term using compute approximations using tensor decomposition. left-hand-side equation estimated unlabeled data; solve using tensor decomposition particular recover permutation recover ˆmiσ permutation sym. yields theorem section full proof. assumption therefore yields moment equations that solved estimate risk without labels summarize procedure approximate left-hand-side term sample averages; tensor decomposition solve maximum matching compute permutation obtain theorem provides basic building block admits several extensions complex model structures. several cases below omitting proofs avoid tedium. extension importantly latent variable need belong small discrete set; handle structured output spaces hidden markov model long matches structure. substantial generalization previous work unsupervised risk estimation restricted multiclass classiﬁcation. assuming markovian respect losses satisﬁes assumption views xt−t xt+t xt+t theorem estimate individually thus also full risk general idea applies structured output problem local -view structures. would interesting extend results structures general graphical models parse trees extension also relax additivity condition exponential loss. theorem estimate matrices corresponding expφv). extension assuming independent conditioned realistic; might multiple subclasses label would induce systematic correlations across views. address this show independence need hold conditioned mediating variable rather label itself. reﬁnement takes values suppose views independent conditioned figure estimate risk long extend loss vector full rank. reason recover matrices then letting express estimation learning turn attention unsupervised learning i.e. minimizing unsupervised learning impossible without additional information since even could learn classes wouldn’t know class label. thus assume small amount information break symmetry form seed model assumption access seed model assumption merely asks aligned true labels average. obtain small amount labeled data training nearby domain deﬁne difference next smallest permutation classes affect difﬁculty learning. simplicity focus case logistic regression show learn given assumptions however algorithm extends general losses show section learning moments. note logistic regression unobserved components linear sense therefore practice would need approximate samples ignore simplicity -constraint imparts robustness errors particular lemma suppose output satisﬁes minθ≤ρ r+\u0001ρ. assuming optimal parameter -norm lemma guarantees computing estimating done manner similar estimating itself. addition conditional risk matrix rk×k compute conditional moment matrix rdk×k deﬁned i+krj v=j+krj. solve using tensor algorithm theorem care needed avoid explicitly forming tensor would result third figure results modiﬁed mnist data set. risk estimation varying degrees distortion domain adaptation training test examples. domain adaptation training test examples. section anandkumar refer reader aforementioned references details cite ﬁnal sample complexity runtime theorem suppose assumption holds min). samples deﬁned probability recover error error measures -norm features. algorithm runs time errors frobenius norm ∞-norm samples times large theorem quantity typically grows sample complexity needed estimate typically times larger sample complexity needed estimate matches behavior supervised case need times many samples learning compared risk estimation ﬁxed model. summary. shown perform unsupervised logistic regression given seed model enables unsupervised learning surprisingly weak assumptions even mis-speciﬁed models zero train-test overlap without assuming covariate shift. section learning general losses. better understand behavior algorithms perform experiments version mnist data modiﬁed ensure -view assumption holds. create image sample class sample images random class letting every third pixel come respective image. guarantees conditionally independent views. explore train-test variation pixel image image center distance normalized maximum value show example images figure risk estimation. unsupervised risk estimation estimate risk model trained tested various values trained model adagrad training examples used test examples estimate risk. solve ﬁrst tensor power method implemented chaganty liang initialize locally minimize weighted -norm moment errors l-bfgs. comparison compute validation error test results shown figure tensor method isolation tensor l-bfgs estimate risk accurately latter performing slightly better. domain adaptation. next evaluate learning algorithm. used trained model constrained results shown figure small values algorithm performs worse baseline directly using however algorithm robust increases tracks performance oracle trained distribution test examples. semi-supervised learning. need provide algorithm seed model perform semi-supervised domain adaptation train model small amount labeled data unlabeled data learn better model distribution. concretely obtained labeled examples. tensor decomposition sometimes initializations regime case obtained different training smaller step size. results shown figure algorithm generally performs well higher variability before seemingly higher condition number matrices summary. experiments show given views estimate risk perform domain adaptation even small amount labeled data. presented method estimating risk unlabeled data relies conditional independence structure hence makes parametric assumptions true distribution. approach applies large family losses extends beyond classiﬁcation tasks hidden markov models. also perform unsupervised learning given seed model distinguish classes expectation; seed model trained related domain small amount labeled data combination thus provides pleasingly general formulation highlighting similarities domain adaptation semi-supervised learning. previous approaches domain adaptation semi-supervised learning also exploited multiview structure. given views blitzer perform domain adaptation zero source/target overlap two-view approaches also used semi-supervised learning methods assume form noise regret e.g. transductive svms focusing central problem risk estimation work connects multi-view learning approaches domain adaptation semi-supervised learning removes covariate shift low-noise/low-regret assumptions addition reliability unsupervised learning work motivated desire build machine learning system contracts challenge recently posed bottou goal machine learning systems satisfy well-deﬁned input-output contract analogy software systems theorem provides contract -view assumption test error close estimate test error; contrasts typical weak contract train test similar test error close training error. interesting contract given shafer vovk provide prediction regions contain true prediction probability online setting even presence model mis-speciﬁcation. restrictive part framework three-view assumption inappropriate views completely independent data structure captured terms multiple views. since balasubramanian obtain results gaussianity optimistic unsupervised risk estimation possible wider family structures. along lines following questions open question. -view setting suppose views completely independent. still possible estimate risk? degree dependence affect number views needed? open question. given independent views obtain upper bound risk results paper caused adopt following perspective handle unlabeled data make generative structural assumptions still optimize discriminative model performance. hybrid approach allows satisfy traditional machine learning goal predictive accuracy handling lack supervision under-speciﬁcation principled way. perhaps then needed learning understand structure domain. donmez lebanon balasubramanian. unsupervised supervised learning estimating classiﬁcation regression errors without labels. journal machine learning research sculley holt golovin davydov phillips ebner chaudhary young crespo dennison. hidden technical debt machine learning systems. advances neural information processing systems pages preliminary reductions. goal estimate error times many samples estimating estimating mostly exercise interpreting theorem anandkumar recall below modifying statement slightly language. denotes condition number since matrices question columns). def= theorem def= also ˆpvv sample estimates estimated independent samples size denote -norm unrolling vector. suppose that symmetry theorem recover matrices permutation columns. furthermore anandkumar show appendix paper match columns different single unknown permutation applied simultaneously. yields overall probability success part proof. analyze rate convergence implied theorem note take then since care polynomial factors enough note estimate error given samples polynomial following quantities note theorem anandkumar require columns thus applies matrix speciﬁc case take ﬁrst coordinates remaining coordinates equal deﬁnition difference scaled equal denote values section formed conditional moment matrix stored conditional expectation however nothing special computing general losses form conditional gradient matrix deﬁned clarity also denote conditional risk matrix value compute gradient jointly estimate since seed model assumption allows recover correct column permutation estimating jointly ensures recover correct column permutation well. ﬁnal ingredient gradient descent procedure robust errors gradient fortunately case many gradient descent algorithms including algorithm expressed mirror descent proof case exponentiated gradent). general learning algorithm given algorithm", "year": 2016}