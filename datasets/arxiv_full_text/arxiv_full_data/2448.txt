{"title": "Robust Subspace Outlier Detection in High Dimensional Space", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Rare data in a large-scale database are called outliers that reveal significant information in the real world. The subspace-based outlier detection is regarded as a feasible approach in very high dimensional space. However, the outliers found in subspaces are only part of the true outliers in high dimensional space, indeed. The outliers hidden in normal-clustered points are sometimes neglected in the projected dimensional subspace. In this paper, we propose a robust subspace method for detecting such inner outliers in a given dataset, which uses two dimensional-projections: detecting outliers in subspaces with local density ratio in the first projected dimensions; finding outliers by comparing neighbor's positions in the second projected dimensions. Each point's weight is calculated by summing up all related values got in the two steps projected dimensions, and then the points scoring the largest weight values are taken as outliers. By taking a series of experiments with the number of dimensions from 10 to 10000, the results show that our proposed method achieves high precision in the case of extremely high dimensional space, and works well in low dimensional space.", "text": "igure sample data plotted three-dimensional space twodimensional spaces. four outliers separated observed. outliers observed. outliers hidden normal clusters. exact description outliers normal data. case detecting outliers falls categories distancebased subspace-based methods. first uses robust distance density high dimensional space i.e. hilout loci gridlof abod etc. methods suitable outlier detection high dimensional space. however high dimensional space perform poor curse dimensions. subspace based detection optimum method find outliers high dimensional space. based assumption outliers projected dimensional subspaces taken real outliers high dimensional space. solution includes aggarwal’s fraction gls-sod curio spot gridclustering etc. since outliers easily found projected dimensions using search algorithms find suitable cell-grids divisions subspace widely used outlier detection high dimensional space. recent advance geo-spatial bioinformatics genetics particle physics also require robust subspace detection methods growing high dimensional data. however issue still uncertain truth outliers detected subspaces abstract—rare data large-scale database called outliers reveal significant information real world. subspace-based outlier detection regarded feasible approach high dimensional space. however outliers found subspaces part true outliers high dimensional space indeed. outliers hidden normalclustered points sometimes neglected projected dimensional subspace. paper propose robust subspace method detecting inner outliers given dataset uses dimensional-projections detecting outliers subspaces local density ratio first projected dimensions; comparing neighbor’s positions second projected dimensions. point’s weight calculated summing related values steps projected dimensions points scoring largest weight values taken outliers. taking series experiments number dimensions results show proposed method achieves high precision case extremely high dimensional space works well dimensional space. finding rare valuable data always significant issue data mining field. worthy data called anomaly data different rest normal data based measures. also called outliers located distance others. outlier detection many practical applications different domains medicine development fraud detection sports statistics analysis public health management according different perspectives many definitions outliers proposed. widely accepted definition hawkins’ outlier observation deviates much observations arouse suspicion generated different mechanism. definition describes difference data observation also points essential difference data mechanism; even though synthetic data generated according concept order verify outliers’ detection methods. although outlier detection special requirement high dimensional space large-scale data practicable real world. issues outlier detection high dimensional space first overcome complexity high dimensional space meet requirement real applications tremendous growth high dimensional data. dimensional space outliers considered points normal points based distance. however high dimensional space distance longer meets fact subspace-based detection methods find outliers different projected dimensional space ignore outliers hidden inside region normal data. inner outliers still different normal data high dimensional space. show simple example prove difference types outliers separately three-dimensional space projected two-dimensional subspaces shown fig. total points distributed three-dimensional space including normal points clusters outliers color. four outliers found differently belong normal clusters. outliers detected different projected dimensional spaces inner outliers hidden inside clusters projected dimensional space. therefore detecting fails. subspacebased methods fail detect inner outliers shown above find outliers subspace-based method still issue considered. paper solve issue utilizing dimensional-projections propose robust subspace detection method called k-ns. calculates first projected dimensional subspace nearest neighbors’ second projected dimensional subspace. then point’s weight summed statistically. outliers scoring largest weights. summarized follows apply dimensional-projections calculate weight values projected dimensions. point weight values order supply compare others extensively. proposed method employs k-ns based k-nn concept local density calculation second projected inner outliers detected dimensional space. successfully evaluating projecting dimensions. execute series experiments range dimensions evaluate proposed algorithm. experiment results show proposed algorithm advantages algorithms stability precision high dimensional dataset. also consider difference outliers noisy data. outliers obviously different high dimensional space noisy data mixed together dimensional space. paper organized follows. section give brief overview related works high dimensional outlier detection. section introduce concept approach describe algorithm. section evaluate proposed method experiments different dimensional datasets artificially generated real datasets. last conclude findings section important part data mining outlier detection developed years many study results achieved large scale database. categorize following five groups. distance density based outlier detection distance based outlier detection conventional method comes original outlier definition i.e. outliers points points based distance measures e.g. hilout. algorithm detects point k-nearest neighbors distance uses space-filling curve high dimensional space. well known uses k-nn density based algorithm detects outliers locally k-nearest distance neighbor points measures lof. algorithm runs smoothly dimensional space still effective relative high dimensional space. loci improved algorithm based sensitive local distance lof. however loci perform well high dimensional space. subspace clustering based outlier detection since difficult find outliers high dimensional space find points behaving abnormally dimensional space. subspace clustering feasible method outlier detection high dimensional space. approach assumes outliers always deviated others dimensional space different high dimensional space. aggarwal uses equi-depth ranges dimension expected fraction deviation points k-dimensional cube given method detects outliers calculating sparse coefficient cube outlier detection dimension deduction another method dimension deduction high dimensional space dimensional space mapping several dimensions dimensions detecting outliers dimensional space. findout detects outliers removing clusters deducts multidimensional data. however method cause information loss dimension reduced. result robust expected seldom applied outlier detection. subspace distribution points dimension coded data compression. hence high dimensional issue changed information statistic issue dimension. christian bohm proposed coco method outlier detection also applies method clustering issue e.g. robust information-theory clustering. outlier detection methods besides four groups detection measurements also distinctive useful. notable approach called abod based concept angle vector product scalar product. outliers usually smaller angles normal points. dimension. here call small region section. based section division construct data structure called section space. second calculate sparsity point section dimension computing average value dimension. third calculate scattering section points projecting original dimension dimensions. last results weight point compare points score. outliers points scoring largest values weight. section data structure proposed method based section data structure. mechanism compose section structure transform euclidean data space proposed section space introduced below. divide space number equi-width sections dimension space looks like cell-grid. conventional data space composed points dimensions proposed data structure represents data distribution point dimension section. structure advantages. first point section easily found dimensions. therefore related section calculated result denote point’s weight value. second easy calculating distribution change checking points’ section position projecting different dimensions. pointinfo records point’s section position different dimensions. sectioninfo records number points section different dimensions. main calculation sparsity points dimension projections processed based data structures. transforming process original data space proposed section-based space explained using example two-dimensional dataset shown fig. dataset includes points two-dimensional space shown fig. original data distribute data space based euclidean distance shown fig. proposed section-based structure construct pointinfo structure fig. sectioninfo structure fig. range dimension divided five sections example. section division shown fig. blue lines. data range dimension different. data range every dimension covering maximum certain dimension would produce many empty sections dimensions. empty sections producing meaningless values would affect result markedly following calculations. therefore minimum data range dimension covering area points exist. order avoid end-sections methods reduced high dimensional cures extent correct results special cases. however problem still exists affects point’s detection accuracy. christian bohm’s information-theory based method similar subspace clustering methods suffers subspace-based outlier detection methods. known last section outliers found projected dimensional subspace. outliers failing detected subspace called inner outliers. inner outliers mixed normal clusters projected dimensional subspaces detected anomaly high dimensional space. another point view inner outliers belong several normal clusters different subspaces belong cluster whole. paper mission find inner outliers high dimensional space. general idea learning subspace detection methods know high dimensional issue transformed statistical issue loop detection projected dimensional subspaces. moreover points’ distribution independent different dimensions. observing points learning existing outlier definitions found outliers placed cluster normal points certain dimension deviated dimensions. otherwise outliers clustered different normal points different dimensions normal points always clustered together. therefore proposed method needs solve sub-issues find outliers effectively projected-dimensional subspaces; detect deviation points region dimension points projected dimensions. larger density sections extend border enlarging original range taking data fig. example explain generate data range dimension original data range dimension length extendedrange enlarging length therefore length original data range dimension length data range length length section dimension dimension. definitions proposal definitions notations given table information point. refers point points. refers point dimension. range data dimension divided number equi-width parts called sections. number sections dimension. decided number total points average section density. defined equally dimension. number points section called section density short. section distance used evaluating section difference among points projected dimensions defined local density ratio introducing section replaced section density ratio calculation defined statistic information point composed weights defined presents specific meanings following cases. case section points section dimension section density means section density value points section dimension. case section density used compare average density dimension. sectiondensity means ratio average section means average section density dimension. density dimension. subspace first projection. points checked projected dimensional subspaces. that points projected dimensions still need checked different subspaces inner outliers. therefore points projected first projected dimension dimensions compare distribution changes other. called second dimension projection. whole procedure projects points twice high dimension dimension dimension dimensions. k-nearest sections section describe detection methods steps. first step employed evaluate sparsity points first projection dimensions. second step part proposal scattering points second projections projected dimensions calculated based k-ns. last summarize results steps statistically. dists definition used measure points’ scatter second projections. dimension second projection assume points section. applying second projection dimension assume points located different sections different section ids. compare distance points subtraction dists defined absolute difference value points’ sections plus order avoid computational complexity k-ns algorithm dists supplies effective factor evaluate scatter points second projected dimensions. dimension section refers section point count section number points section proof. first outlier normal’s cluster projected dimensions otherwise definition applied. count) count) dimension outliers appear clearly dimensions cannot detected first step since hidden among normal points similar distance density others. nevertheless points still detected second projected dimensions. step aims find outliers normal points projecting points different dimensions. section distance measurement describes sparsity points check second projected dimensions. basing section distance concept referring k-nearest neighbor concept nearest sections point projected dimensions. definition second dimension projection dimension k-ns definition outliers satisfying either following conditions detected first outliers detected first projection; second outliers still detected dists-based k-ns second projection even point appear abnormal first projection. although reflect outlier result difficult calculated point. therefore general statistic information point defined definition calculated value first calculated projected dimension value second projection dimension used calculate density ratio point dimensional-projections. detail calculation method introduced point’s final score points evaluated. outlier’s value obviously different normal point’s. different dataset adjust weight values bring better result. section density ratio calculated first projected dimension outliers always appear sparsely normal points detected projected dimensions. therefore section density outliers lower average section density dimension. proposal cited calculation. point reflect sparsity compared others dimension also keep value independent different dimensions. definition point points close outlier’s value obviously larger however true high dimensional space. normal points’ getting close outlier’s nevertheless outlier’s still obviously higher normal points’. therefore outliers detected finding points largest values. algorithm focus implement k-ns method language. pointinfo sectioninfo effectively different sections dimensions issue needs considered detail. proposed algorithm shown table pseudo-r code. here dataset points dimensional space. range data divided sections dimension. value definition point output outliers point score) three points need clarified algorithm. first point decide average sectionid value obtained dimension. however consider special case points several sections point sections. case becomes even close outlier’s section density. therefore count sections points. subsequently varied different dimensions. hence ratio section density definition measure sparsity points different sections dimension. points ratio value average value points’ section. point projected value calculated another dimension single time projection. totally -times projected dimensions point. lemma given dataset point dimension normal points’ cluster normal points second projection points projected dimension nearest neighbor nearest neighbor. outlier dists dists proof. normal points belong cluster dimensions. therefore dimension outlier neighbors side dists dist suitable weights considered order give sharp boundary compare points. evaluating different weighting values performance choose simple clear values. here reciprocal value average case points section. case point distribution difficult judged several points. addition section density ratio step must low. therefore points already detected previous step. here pass section too. three-step procedure considered separately state complexity k-ns algorithm. first step calculates section density projected dimension. time complexity second step knearest sections density calculated projected dimensions. time complexity o×m). noticed points section used time complexity expression changed o×m). last step summing weight values point time complexity hence total complexity time space complexity k-ns data retrieved dataset introduced section contains points dimensions. outliers placed middle region found differently normal points. noisy points labeled cloud symbol different projected twodimensional space. another example shown fig. outliers always obvious projected dimensional space noisy points distributed marginal area dimensions likely abnormal points. implemented algorithm applied several high dimensional datasets made comparison k-ns loci. order compare algorithms fair conditions performed language book .ghz intel core memory. synthetic datasets critical issue evaluating outlier detection algorithms benchmark datasets available real world satisfy explicit division outliers normal points. points found outliers first conduct two-dimensional experiment using dataset fig. result show three algorithms perform well. proposed algorithm dimensional dataset. next proposed algorithm evaluated thoroughly series experiments compared lof. loci excluded comparison performs poor every dataset. order measure performance algorithms precision recall outliers reprieved one. evaluation eight dataset experiments obtained precisions recalls respectively every dataset obtain f-measures. pick highest f-measure dataset demonstrating experiment performance kns. beginning need appropriate parameters eight experimental datasets. parameters best ones prepared datasets changed according data size number dimensions. parameter around experiments since dataset size points. reasonable ratio neighbor points whole dataset size. algorithm -dimensional experiment result shown fig. performs best -dimensional experiment. especially detect outliers high precision. nevertheless precision falls sharply increasing recall last result precision worse k-ns detecting outliers correctly. whole performance k-ns lof. reason performance poor algorithms outliers placed center normal data datasets prevents outliers found dimensional space. therefore difficult find exact outliers -dimensional space. provide reasonable real dataset explanation points picked outliers. hand learned statistical knowledge helpful generate artificial dataset points distributions apparently different normal points points regarded outliers. hence generate synthetic data based assumption. generate eight synthetic datasets points dimensions normal points conform normal distributions outliers conform random distributions fixed region. normal points experiment datasets generated rules outliers’ range within range normal points dimensions. therefore outliers cannot found dimensional space. data distribution example shown fig. dataset projected two-dimensional space outliers labeled color. clearly shown outliers within range normal points appear difference normal points two-dimensional space. noisy points placed margin distributed area likely regarded abnormal points. hence outliers normal data cannot separated straight observation different distributions. labeled positive negative except test dataset. instances test dataset know instances positive instances negative. best_svm_result available instances labeled positive instances labeled negative. result evaluating proposal create dataset adding randomly selected negative instances retrieved positive instances svm. first evaluation uses dataset total instances. second evaluation uses retrieved negative instances apply algorithms detect outlier them. result first experiment shown fig. points chosen algorithms. score point point points larger true outliers. three outliers outliers detected k-ns points. totally five outliers detected mixed result combined algorithms. twenty points seven outliers three outliers detected k-ns. totally nine outliers detected mixed result. results better k-ns. however k-ns help increase detection accuracy points points. another word supply reasonable alternative solution increase precision results. contrast also give loci result output point recall k-ns. however outliers detected loci also detected lof. effectiveness algorithms. different first dataset k-ns achieves precision recall time. obviously reduces precision increasing recall shown fig. fact k-ns keeps perfect result dimensions performs much poorer terms precision recall. experiments datasets shown fig. needs pick largest f-measure dataset k-ns needs pick largest f-measure first dataset. addition f-measures k-ns always datasets experiments show k-ns performs perfectly find inner outliers high dimensional space. suffered curse high dimension greatly. find precision become better dataset size increased; lof. efficiencies shown fig. faster experiments. algorithms take time number dimensions data size increase. reason dimension-loop calculation processes distance point neighbors. however proposed algorithm calculates values first projected dimensions second projecteddimensions. performance real world data task group distinguish cancer versus normal patterns mass-spectrometric data. two-class classification problem continuous input variables. dataset five datasets nips feature selection challenge. second experiment positive points miss-clarified svm. therefore finding points task experiment. seen table showing results point probably outliers intersection k-ns results. noted points appear three detected points results. consider loci result intersections point entirely different k-ns. nevertheless contrast former results first conclusion seems reasonable. paper introduce definition inner outlier present novel method called k-ns designed detect inner outliers largest score high dimensional dataset. algorithm based statistical method three steps. calculate section density ratio point dimension first projection. compute nearest sections density ratio point projected dimensions second projection. summarize values point denoted weight value compare difference outliers noisy data also discussed paper. issue difficult dimensional space. experiments noisy data outlier found differently comparing distribution projected dimension whole dimensions noisy data seem abnormal outliers projected dimensional spaces cases. ongoing future work continue improve algorithm finding best relationship two-step sdr. besides performing high dimensions dataset large-scale data size increment updates instead computing entire dataset outlier detection need conducted. another issue expensive cost processing time high dimensional space. solution reduce processing time needs investigated. approaches parallel processing. markus m.breunig hans-peter kriegel raymond t.ng jorg sander. indetify density-based local outliers. proceedings sigmod international conference management data. spiros papadimitriou hiroyuki kitagawa phillip b.gibbons. loci fast outlier detection using local correlation integral. ieee international conference data engineering hans-peter kriegel matthias schubert arthur zimek. angle-based outlier detection high dimensional data. sigkdd international conference conference knowledge discovery data mining. christian bohm katrin haegler. coco coding cost parameterfree outlier detection. proceedings sigkdd international conference conference knowledge discovery data mining. christian bohm christos faloutsos etc. outlier-robust clustering using independent components. proceedings sigmod international conference management data. anny lai-mei chiu wai-chee enhancements local outlier detection. proceedings seventh international database engineering applications symposium aaron ceglar john f.roddick david m.w.powers. curio fast outlier outlier cluster detection algorithm larger datasets. aidm proceedings international workshop integrating artificial intelligence data mining. australia feng chen chang-tien arnold boedihardjo. gls-sod generalized local statistical approach spatial outlier detection. proceedings sigkdd international conference knowledge discovery data mining. michal valko branislav kveton etc. conditional anomaly detection soft harmonic functions. proceedings ieee international conference data mining zhang etc. detecting projected outliers high dimensional data streams. proceedings international conference database expert systems applications", "year": 2014}