{"title": "Jointly Learning Multiple Measures of Similarities from Triplet  Comparisons", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Similarity between objects is multi-faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. We consider the problem of mapping objects into view-specific embeddings where the distance between them is consistent with the similarity comparisons of the form \"from the t-th view, object A is more similar to B than to C\". Our framework jointly learns view-specific embeddings exploiting correlations between views. Experiments on a number of datasets, including one of multi-view crowdsourced comparison on bird images, show the proposed method achieves lower triplet generalization error when compared to both learning embeddings independently for each view and all views pooled into one view. Our method can also be used to learn multiple measures of similarity over input features taking class labels into account and compares favorably to existing approaches for multi-task metric learning on the ISOLET dataset.", "text": "comparing three birds seen fig. annotators head bird similar head back similar ambiguity leads noise annotation results poor embeddings. better approach would tell annotator desired view perspective object measuring similarity. view-speciﬁc comparisons easier annotators also enable precise feedback human loop tasks interactive ﬁne-grained recognition thereby reducing human effort. main drawback learning view speciﬁc embeddings independently number similarity comparisons scales linearly number views. undesirable even learning single embedding objects require triplet comparisons worst case. propose method learning embeddings jointly addresses drawback. method exploits underlying correlations exist views allowing better training data. method models correlation views assuming view low-rank projection common embedding. model seen matrix factorization model local metric deﬁned matrix parametrizes similarity objects multi-faceted easier human annotators measure focus speciﬁc aspect. consider problem mapping objects view-speciﬁc embeddings distance consistent similarity comparisons form from t-th view object similar framework jointly learns view-speciﬁc embeddings exploiting correlations views. experiments number datasets including multi-view crowdsourced comparison bird images show proposed method achieves lower triplet generalization error compared learning embeddings independently view views pooled view. method also used learn multiple measures similarity input features taking class labels account compares favorably existing approaches multi-task metric learning isolet dataset. measure similarity plays important role applications content-based recommendation image search speech recognition. therefore number techniques learn measure similarity data proposed measure distance induced inner product low-dimensional space done many studies learning distance metric equivalent learning embedding objects low-dimensional space. useful visualization well using learned representation variety down-stream tasks require ﬁxed length representations objects demonstrated applications word embeddings language. among various forms supervision learning distance metric similarity comparison form ‘object similar call triplet comparison extremely useful obtaining embedding reﬂects perceptual similarity triplet comparisons obtained crowdsourcing also derived class labels available. jointly learning multiple metrics let’s assume sets triplets available. obtained asking annotators focus speciﬁc aspect making pair-wise comparisons human loop tasks alternatively different measures similarity come considering multiple related metric learning problems simple approach handle multiple similarities would parametrize aspect view positive semideﬁnite matrix would induce shared structure among views. goal learn global transformation maps objects common dimensional space well local view-speciﬁc metrics hinge loss experiments paper proposed framework readily generalizes loss functions proposed literature note input feature provided global transformation matrix becomes matrix consists dimensional embedding objects. intuitively global transformation plays role bottleneck forces local metrics share common dimensional subspace restricted form proposed model includes various simpler models special cases. first identity matrix sharing across different views indeed objective function decompose view-wise objectives; call independent learning. hand constrain equal metric apply views learned metric essentially learning single shared metric employ regularization terms local metric global transformation matrix trace penalties employed obtain low-rank matrices above. regularization term norm necessary resolve scale ambiguity. although formulation hyperparameters show below proposition product hyperparameter needs tuned. minimize objective update alternately. updates gradient descent. update followed projection onto positive semideﬁnite cone. note choose convex loss common embedding positive semideﬁnite matrix parametrizing individual view. model efﬁciently trained alternately updating view speciﬁc metric common embedding. experiment synthetic dataset realistic datasets namely poses airplanes crowd-sourced similarities collected different body parts birds datasets joint learning approach obtains lower triplet generalization error compared independent learning approach naively pooling views single especially number training triplets limited. furthermore apply joint metric learning approach multi-task metric learning setting studied demonstrate method also take input features class labels account. method compares favorably previous method isolet dataset. metric learning triplet comparisons given object similar object object features possibly input positive semideﬁnite matrix rh×h distances induced inner product parametrized agrees i.e. input feature given take coordinate vector learning would become would correspond ﬁnding embeddings objects euclidean space dimension equal rank loss function example logistic hinge choices loss functions lead crowd kernel learning t-distributed stochastic triplet embedding penalizing trace matrix seen convex surrogate penalizing rank regularization parameter. optimal obtained low-rank factorization rh×d. particularly useful input feature provided because case corresponds dimensional embedding object. algorithm multiple-metric learning input number objects dimension embedding triplet constraints regularization parameters number inner gradient updates mmax output global transformation matrices initialize randomly; initialize identity matrices; converged figure view-speciﬁc similarities poses planes obtained considering subsets landmarks shown different colored rectangles measuring similarity conﬁguration scaling translation. perceptual similarities bird species collected showing users either full image crops around various parts average image view also shown. compared method following baselines. independent conducted triplet embedding view treating independently. parametrized rn×d minimized using software provided maaten weinberger pooled learned single embedding training triplets views combined. synthetic data description synthetic datasets generated. consisted points uniformly sampled dimensional unit hypercube dataset objects mixture four gaussian variance whose centers randomly chosen hypercube side length views generated dataset. function e.g. hinge-loss becomes convex problem respect optimized independently since appear disjoint terms. algorithm summarized algorithm effective regularization term regularization terms employed reduced single effective regularization term hyperparameter show following proposition proposition number parameters simple parameter counting argument tells independently learning views requires parameters number input dimension large embedding dimension number views. hand joint learning model parameters. thus model much fewer parameters enables better generalization especially number triplets limited. efﬁciency reducing dimension common transformation also favorable terms computational efﬁciency. projection cone matrices much efﬁcient compared independently learning views. experimental setup dataset divided triplets training test measured quality embeddings triplet generalization error i.e. fraction test triplets whose relations incorrectly modelled learned embedding. error measured view averaged across views. numbers training triplets views. regularization parameter tuned using -fold cross-validation training candidate results embeddings learned embedding dimensions triplet generalization errors plotted fig. clustered uniform data respectively. algorithm achieved lower triplet generalization error independent pooled methods datasets. improvement particularly large number triplets limited simple pooled method worst datasets. note contrast pooled method proposed joint method choose different embedding dimension automatically view maintaining shared subspace. poses airplanes description dataset constructed images airplanes pascal dataset annotated landmarks nose wing tips used landmarks construct pose-based similarity. given planes positions landmarks images pose similarity deﬁned residual error alignment sets landmarks scaling translation. generated views associated subset landmarks; supplementary material details. three annotated images shown left panel fig. planes highly diverse ranging passenger planes ﬁghter jets varying size form results slightly different similarity instances view. however strong correlation views underlying landmarks shared. results used embedding dimensions. figure shows triplet generalization errors three methods. proposed joint model performed clearly better independent. average also uniformly view pooled method slightly larger error proposed joint learning approach better independent approach. collected using setup brieﬂy similarity triplets among images species collected crowd-sourced manner every time user asked judge similarity image bird target specie nine images birds different species {zk}k∈k using interface wilber species. display user partitioned nine images sets ksim kdissim ksim containing birds considered similar target kdissim ones considered dissimilar. partition broadcast equivalent triplet constraints associated species ksim kdissim}. therefore user response |ksim||kdissim| triplet constraints obtained. collect view-speciﬁc triplets presented different cropped versions bird images shown right panel fig. used procedure collect triplet comparisons. obtained triplets uncropped original images triplets cropped views. dataset reﬂects realistic situation triplet relations available noisy nature crowd-sourcing. addition triplet generalization error evaluated embeddings classiﬁcation task using biological taxonomy bird species. note embeddings used interactively categorize images; simplify process enable detailed comparisons. manually grouped classes super classes number objects classes balanced. class labels used training allowed evaluate quality embeddings using leave-one-out classiﬁcation error. precisely test stage predict class label embedded point according labels -nearest-neighbours learned metric. finally since triplets available ﬁrst view compared views ﬁrst sampled equal numbers triplets view total triplets. afterwards added triplets ﬁrst view. results used embedding dimensions; note joint learning dimensions roughly number parameters independent learning dimensions. figures show triplet generalization errors classiﬁcation errors respectively. solid vertical line shows point start training triplets ﬁrst view. comparing joint learning dimensions dimensions higher dimension gives lower error. error joint learning better independent learning small number triplets. interestingly error joint learning dimensions coincides independent learning dimensions seeing triplets. explained fact views models comparable complexity thus asymptotic variance. method obtains lower leave-one-out claslearning view cub- birds dataset simulated situation learning view drew training contains triplets second view triplets views. investigated joint learning helps estimating good embedding view extremely small number triplets. triplet generalization errors approaches shown fig. triplet generalization error proposed joint learning lower independent learning around triplets. embedding second view learned jointly views clearly better learned independently consistent quantitative evaluation; supplementary material. performance gain triplet consistency fig. relate performance gain obtained joint/pooled learning approaches compared independent learning approach underlying between-task similarity. performance gain measured difference area triplet generalization errors normalized independent learning. between-task similarity measured triplet consistency views averaged pairs views. cub- dataset subset valid triplet constraints available take independently learned embeddings largest number triplets compute triplet consistency. triplet consistency high pooled learning approach good enough. however triplet consistency high harm pool triplets together. proposed joint learning approach advantage intermediate regime. hand consistency close random cub- dataset possibly explaining performance gain signiﬁcant datasets. example employ idea multi-task large margin nearest neighbor algorithm adapt model handle classiﬁcation problem. loss function mtlmnn consists terms. ﬁrst term hinge loss triplet constraints triplets derived class labels. second term squared distances object target neighbors also deﬁned based class labels; weinberger details. major difference mtlmnn model mt-lmnn parametrizes local metric global average viewspeciﬁc metric thus learned metric generally full rank. hand method parametrizes product global transform local metric allows local embedding dimension controlled trace regularization. conduct experiments isolet spoken alphabet recognition dataset consists examples english alphabets spoken subjects example described dimension feature vector. task recognize letter spoken example english alphabets. subjects grouped sets similar speakers leading tasks. adapt experimental setting work mtlmnn. data ﬁrst projected onto ﬁrst leading components capture variance. train model dimensional space compare mt-lmnn trained code provided authors. experiment task rantest error rates -nearest-neighbor classiﬁers reported table left panel shows errors using view-speciﬁc training data classiﬁcation. right panel shows using training data view-speciﬁc distance. results averaged runs. simpler baseline methods euclidean metric pooled learning included mt-lmnn already performed better them. proposed method performed better mt-lmnn learning dimensional space reducing dimensional space comparable error rates. possible explanation mild dependence choice embedding dimension could given fact regularized effective embedding dimension determined regularization choice prop. averaged error rates reported original paper using dimensions view-speciﬁc case training data used; numbers still better theirs. embedding objects triplet paired distance comparisons goes back work shepard kruskal studied extensively recently. recently triplet embedding metric learning problems involve multiple measures similarity considered. parameswaran weinberger aimed jointly solving multiple related metric learning problems exploiting possible similarities. speciﬁcally modeled inner product view shared global matrix view-speciﬁc local matrix. moreover lian carin proposed bayesian approach multi-task metric learning. unfortunately structure work typically produce low-rank metric makes unsuitable learning view-speciﬁc embeddings. contrast method models product allowing trace norm regularizer determine rank local metric. xing wang studied metric learning problems multiple input views. different setting notion similarity varies view view. amid ukkonen considered task multi-view triplet embedding view latent variable; proposed greedy algorithm ﬁnding view membership object well embedding. could useful combine approach enough resource collect triplets possible views. proposed model jointly learning multiple measures similarities multi-view triplet observations. proposed model consists global transformation represents object ﬁxed dimensional vector local view-speciﬁc metrics. experiments synthetic real datasets demonstrate proposed joint model outperforms independent pooled learning approaches cases. additionally shown advantage joint learning approach becomes prominent views similar similar morevoer extended model incorporate class labels feature vectors. proposed model performed favorably compared mtlmnn isolet dataset. since many real applications similarity triplets expensive obtain jointly learning similarity metrics preferable recover underlying structure using relatively small number triplets. look proposed model view shared global transformation controlling complexity. however experiments shown generally higher dimension better performance thus alternative explanation could regularization global transformation local metrics implicitly controlling embedding dimension.", "year": 2015}