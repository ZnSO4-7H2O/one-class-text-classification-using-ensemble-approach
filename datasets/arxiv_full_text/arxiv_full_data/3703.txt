{"title": "Resilience: A Criterion for Learning in the Presence of Arbitrary  Outliers", "tag": ["cs.LG", "cs.AI", "cs.CC", "cs.CR", "stat.ML"], "abstract": "We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings. We provide new information-theoretic results on robust distribution learning, robust estimation of stochastic block models, and robust mean estimation under bounded $k$th moments. We also provide new algorithmic results on robust distribution learning, as well as robust mean estimation in $\\ell_p$-norms. Among our proof techniques is a method for pruning a high-dimensional distribution with bounded $1$st moments to a stable \"core\" with bounded $2$nd moments, which may be of independent interest.", "text": "introduce criterion resilience allows properties dataset robustly computed even presence large fraction arbitrary additional data. resilience weaker condition properties considered literature enables robust estimation broader variety settings. provide information-theoretic results robust distribution learning robust estimation stochastic block models robust mean estimation bounded moments. also provide algorithmic results robust distribution learning well robust mean estimation p-norms. among proof techniques method pruning highdimensional distribution bounded moments stable core bounded moments independent interest. keywords robust learning outliers stochastic block models p-norm estimation rank approximation fundamental properties allow robustly learn dataset even fraction dataset consists arbitrarily corrupted data? much work done setting noisy data restricted families outliers recently provable algorithms learning presence large fraction arbitrary data formulated high-dimensional settings work formulate conceptually simple criterion dataset satisfy–resilience–which guarantees properties mean dataset robustly estimated even large fraction additional arbitrary data inserted. illustrate setting consider following game alice bob. first points given alice. alice adds additional points create passes bob. wishes output parameter close possible mean points original error measured according norm question well assuming alice adversary knowledge bob’s algorithm? asumptions clearly incur arbitrarily large error worst case—alice points arbitrarily away true mean telling whether points actually belong added alice. ﬁrst pass assumption suppose diameter discarding points away points obtain error however high-dimensional settings diameter grows polynomially dimension supported fannie john hertz foundation fellowship graduate research fellowship future figure illustration robust mean estimation setting. first points given alice adds fraction adversarially chosen points bob’s goal output mean original moments. since then considerable amount additional work studied high-dimensional estimation presence adversaries discuss detail below. however general bob’s strategy analysis tend quite complex specialized particular distributional assumptions. raises question—is possible formulate general simple-to-understand criterion strategy incurring small error? paper provide criterion; identify assumption–resilience–on straightforward exponential-time algorithm estimating accurately. yields information-theoretic bounds number robust learning problems including robust learning stochastic block models discrete distributions distributions bounded moments. also identify additional assumptions efﬁcient strategy estimating yields efﬁcient algorithm robust learning discrete distributions well robust mean estimation p-norms. formally norm criterion follows deﬁnition points {xi}i∈s lying -resilient norm around point subsets size least deﬁnition above need equal mean distinction useful statistical settings sample mean ﬁnite points differs slightly true mean. however implies differs mean importantly deﬁnition satisﬁed high probability ﬁnite sample many settings. instance samples distribution moments bounded \u0001)-resilient -norm high probability. resilience also holds high probability many natural distributional assumptions discussed detail sections assuming original -resilient bob’s strategy actually quite simple—ﬁnd large -resilient subset corrupted output mean pigeonhole large intersection hence must similar means. establish formally section pleasingly resilience reduces question whether game purely algorithmic question—that ﬁnding large resilient set. rather wondering whether even informationtheoretically possible estimate instead focus efﬁciently ﬁnding resilient subsets provide algorithm section assuming norm strongly convex approximately solve certain generalized eigenvalue problem dual norm. specialized -norm general algorithm yields efﬁcient procedure robust learning discrete distributions. remainder section outline main results starting information-theoretic results moving algorithmic results. section show resilience indeed informationtheoretically sufﬁcient robust mean estimation. section provide ﬁnite-sample bounds showing resilience holds high probability i.i.d. samples distribution. section turn attention algorithmic bounds. identity property–bounded variance dual norm–under efﬁcient algorithms exist. show that long norm strongly convex every resilient large subset bounded variance thus enabling efﬁcient algorithms. connection resilience bounded variance technically non-trivial component results independent interest. information-theoretic algorithmic bounds yield results concrete settings discuss corresponding subsections. section also discuss extension resilience lowrank matrix approximation enables derive bounds setting well. section outline rest paper point technical highlights section discuss related work. first show resilience indeed information-theoretically sufﬁcient robust recovery mean follows denote smallest -resilient. proposition suppose contains size resilient around possible recover ˆµ−µ possible output generally alice make disjoint union identical copies determining copies true generally nevertheless situation still identify probability second part proposition says identify probability least estimation. apart interesting unexpectedness estimation regime immediate implications robust estimation mixtures distributions planted substructures random graphs. refer reader charikar full elaboration point. case simply search large resilient output mean; must large overlap resilience means must close mean intersection hence other. similar pigeonhole argument applies need consider covering show true must overlap least sets decent amount outputting mean sets random gives good approximation mean probability proposition provides deterministic condition robust mean estimation possible would also like checking resilience holds high probability given samples distribution first provide alternate characterization resilience says distribution resilient thin tails every direction lemma given norm deﬁne dual norm supx≤v ﬁxed vector denote \u0001-quantile px∼px− then -resilient around mean next provide meta-result establishing resilience population distribution generically transfers ﬁnite samples distribution. number samples necessary depends quantities deﬁned detail later; note ways measuring effective dimension space. note proposition guarantees resilience n-element subset rather perspective robust estimation sufﬁcient simply regard remaining points part points controlled alice. weaker requirement seems actually necessary achieve proposition also exploited charikar yield improved bounds graph partitioning problem. great deal recent interest showing prune samples achieve faster rates random matrix settings think general investigation pruning results likely fruitful. remark sample complexity proposition suboptimal many cases requiring roughly samples samples would sufﬁce. next subsection discuss tighter specialized bound based spectral graph sparsiﬁcation. applications. propositions together give powerful tool deriving information-theoretic robust recovery results needs simply establish resilience population distribution proposition obtain ﬁnite sample bounds proposition obtain robust recovery guarantees. three illustrative settings mean estimation learning discrete distributions stochastic block models. outline results below; formal statements proofs deferred section mean estimation -norm. suppose distribution bounded moments ex∼px− v|k]/k \u0001)-resilient -norm. propositions imply that given samples \u0001-fraction corruptions possible recover mean -error moreover α-fraction points good mean recovered error probability d./\u0001 term sample complexity likely loose believe true dependence log. looseness comes proposition uses na¨ıve covering argument could potentially improved sophisticated tools. nevertheless interesting resilience holds long empirical moments concentrate would require samples. distribution learning. suppose given k-tuples independent samples discrete distribution distribution taking empirical average samples treat sample element m-dimensional simplex stochastic block models. finally consider semi-random stochastic block model studied charikar graph vertices model posits subset good vertices connected probability vertices probability connections among vertices arbitrary. goal recover think adjacency matrix vector show good vertices vectors resilient truncated -norm deﬁned largest coordinates particular non-trivial guarantees long charikar derive weaker bound remark similarity famous kesten-stigum threshold conjectured threshold computationally efﬁcient recovery classical stochastic block model conjecture mossel massouli´e proof two-block case). information-theoretic upper bound matches kesten-stigum threshold factor. conjecture upper bound tight; evidence given steinhardt provides nearly matching information. theoretic lower bound existing algorithmic results robust mean estimation rely analyzing empirical covariance data diakonikolas balakrishnan section establish connections bounded covariance resilience show general sense bounded covariance indeed sufﬁcient enable robust mean estimation. important result converse also true provided norm strongly convex. norm γ-strongly convex example p-norm -strongly convex strongly convex norms show resilient highly resilient core bounded variance )-resilience equivalent bounded moments every using lemma show proof theorem involves minimax duality khintchine’s inequality. also strengthen theorem yield size |s|. proofs results given section independent interest. algorithmic results. given points bounded variance establish algorithmic results assuming solve generalized eigenvalue problem maxv∗≤ multiplicative accuracy speciﬁcally make following assumption assumption convex matrices every matrix moreover possible optimize linear functions polynomial time. result nesterov implies true quadratically convex norm includes q-norms also paper sometimes exploit weaker versions assumption require supm∈pa small certain matrices instance obtains algorithm robust sparse mean estimation even though assumption fails hold setting. essentially restrictive computationally efﬁcient version proposition note -norm algorithm implemented combined ﬁltering step; general norms replaced semideﬁnite program. speciﬁc norms several papers achieve stronger rates stronger results rely crucially speciﬁc distributional assumptions gaussianity. time writing paper results obtained rates better general class distributions initial publication paper kothari obtained rates distributions satisfying steinhardt surpassed poincar´e isoperimetric inequality. norms beyond -norm. paper achieves better rate think likely possible achieve applications. assumption holds p-norms perform robust estimation p-norms long data bounded variance dual q-norm corollary formal statement. ﬁrst efﬁcient algorithm performing robust mean estimation p-norm -norm particular often meaningful metric -norm discrete settings allowing improve existing results. indeed previous section suppose given k-tuples samples discrete distribution applying theorem -norm yields algorithm recovering substantially weaker maximum probability πmax large. result similar ﬂavor diakonikolas robustly estimating binary product distributions directly applying mean estimation also insufﬁcient. discuss bounds detail section finite-sample bound. best sample complexity applications above provide additional ﬁnite-sample bound focused showing points bounded variance. simple useful generalization proposition charikar shows generic sense given samples distribution bounded population variance subset samples bounded variance high probability. involves pruning samples non-trivial based ideas graph sparsiﬁcation formal statement given section finally illustrate idea behind resilience quite general restricted mean estimation also provide results recovering rank-k approximation data presence arbitrary outliers. given points matrix whose columns goal obtain low-rank matrix operator norm much larger denotes singular value; wish even corrupted adding arbitrary outliers. rank-resilience says variation sufﬁciently spread direction variation concentrated δ-fraction points. rank-resilience perform efﬁcient rank-k recovery even presence δ-fraction arbitrary data theorem points contains size δ-rank-resilient possible efﬁciently recover matrix rank power theorem comes fact error depends rather e.g. previous results yielded. distinction crucial practice since data large singular values followed many small singular values. note contrast theorem theorem holds relatively large least summary provided deterministic condition points enables robust mean estimation provided ﬁnite-sample bounds showing condition holds high probability many concrete settings. yields results distribution learning stochastic block models mean estimation bounded moments mean estimation norms. also provided extension condition yields results robust low-rank recovery. beyond results themselves following technical aspects work particularly interesting proof proposition simple nice pigeonhole argument found conceptually illuminating. addition proof theorem pruning resilient sets obtain sets bounded variance exploits strong convexity non-trivial conjunction minimax duality; think reveals fairly non-obvious geometric structure resilient sets also shows ability prune points yield sets meaningfully stronger properties. norm. given lemma roadmap. rest paper organized follows. section prove information-theoretic recovery result resilient sets section prove theorem establishing resilient sets strongly convex norms contain large subsets bounded variance; also prove precise version theorem section section prove algorithmic results warming -norm moving general norms section prove results rank-k recovery. section present prove ﬁnite-sample bounds discussed section sections provide applications information-theoretic algorithmic results respectively. number authors recently studied robust estimation learning high-dimensional settings study mean covariance estimation diakonikolas focus estimating gaussian binary product distributions well mixtures thereof; note implies mean/covariance estimation corresponding distributions. charikar recently showed robust estimation possible even fraction good data less refer papers overview broader robust estimation literature; since papers number additional results also published diakonikolas provide case study various robust estimation methods genomic setting balakrishnan study sparse mean estimation others studied problems including regression bayes nets planted clique several settings special cases resilience criterion implicit earlier works; instance resilience appears equation diakonikolas resilience sparsity-inducing norm appears theorem however conditions typically appear concurrently stronger conditions general sufﬁciency resilience information-theoretic recovery appears unappreciated despite already implicitly established resilience proves information-theoretic results reduction tournament lemma diakonikolas rank estimation studied bounds depend maximum eigenvalue covariance matrix bound provides robust recovery guarantees terms lower singular values shows estimate e.g. frobenius norm appears require samples drawn gaussian.) resilience robustness information-theoretic sufﬁciency recall deﬁnition resilience -resilient whenever |s|. establish proposition showing that ignore computational efﬁciency resilience leads directly algorithm robust mean estimation. proof prove proposition constructive algorithm. )-resilient around prove ﬁrst part suppose true -resilient claim sufﬁciently close given lemma second part proposition similar ﬁrst part requires consider )-resilient around µ–and thus also )-resilient lemma –and maximal collection subsets that clearly claim least close maximality collection {sj}m must cannot added collection. first suppose contradicts maximality {sj}m letting before γ-strongly convex |xi−µ exists core size least assumptions seem necessary i.e. core exist p-norm bounded moments proof proposition uses minimax duality khintchine’s inequality note lemma proposition together imply theorem proof without loss generality take suppose pose problem ﬁnding resilient core integer program khintchine’s inequality assumed ﬁrst moment bound. remains bound following inequality asserting dual norm strongly smooth whenever strongly convex lemma γ-strongly convex -strongly smooth applying lemma inductively finding resilient cores lemma together proposition show -resilience particular yields core size might hope much larger core size small tighten proposition make ﬁner-grained resilience information. recall denote resilience sets size |s|. given goal construct core size small second moments. following quantity tell small second moments warm-up recovery -norm ﬁrst prove warm-up theorem focuses -norm. warm-up result proposition subset size bounded variance i∈s) mean efﬁcient norm λmax probability outputs parameter idea behind re-construct average note assumption always re-construct element using mean small error. intuitively element cannot re-constructed well must safely removed. soft form removal maintaining weights points downweighting points high reconstruction error. also maintain active points form algorithm proof need show things outlier removal step removes many outliers good points many columns close outlier removal. analyze outlier removal step make following general lemma lemma says downweight points within least times slower overall particular never remove many points type lemma completeness prove section αnσ. note ﬁxed optimized independently. desired; covariance around mean smaller around point analyzing lemma eventually exit statement obtain xaw. therefore remains analyze show particular za∩s−µf small subscript indicates restricting columns high level sufﬁces show rank close spectral norm close assumption). probability least |s∩a| completes ﬁrst part proposition second part recall rank verify therefore letting particular resilient thus proof proposition fact ready prove general algorithmic result theorem section convenience recall theorem here theorem suppose contains subset size whose variance around mean bounded norm also suppose assumption holds dual norm then i∈sxi recall bounded variance means equivalent conditions bounded variance useful. ﬁrst supv∗≤ i∈s; useful assumption allows κapproximate supremum given second equivalent condition re-interprets terms matrix norm. denote norm above matrix deﬁne induced ψ-norm supu≤ norms satisfy helpful compositional properties ab→ψ a→ψb. algorithm establishing theorem almost identical algorithm changes. ﬁrst change line quantity replaced approximation factor assumption second change optimization constraint replaced feasible assumption words difference rather ﬁnding maximum eigenvalue κ-approximate norm using assumption therefore solving saddle point problem standard optimization algorithms frank-wolfe allow solve given precision polynomial number calls linear optimization oracle guaranteed assumption implies resilient uniformly random sign vector. compare esas directions. projection onto span hand similarly esas bining yields desired result. section present results rank-k recovery. ﬁrst justify deﬁnition rank-resilience showing information-theoretically sufﬁcient recovering best rank-k subspace. then provide algorithm showing subspace recovered efﬁciently. information-theoretic sufﬁciency i∈s. recall δ-rank-resilience asks |s|. justiﬁed following proposition points size output rank-k projection matrix σk+. proof find smallest projection onto singular vectors σk+) σk+. moreover well. pigeonhole |s|. therefore section provide general ﬁnite-sample concentration results establish resilience high probability. ﬁrst holds arbitrary resilient distributions suboptimal sample complexity latter specialized distributions bounded variance near-optimal sample complexity. ﬁrst result stated proposition section applies -resilient distribution recall -resilient event probability deﬁne covering number unit ball norm minimum vectors maxm supv∗≤x vectors note measure effective dimension unit ball i.e. norm norm. proof distribution samples conditioned note resilient since every event probability event probability moreover probability least samples come therefore focus establishing resilience samples idea analyze ﬁxed union bound possibilities. ﬁxed split components small magnitude large magnitude bound former hoeffding’s inequality using resilience able upper-bound second moment latter bernstein’s inequality. section state stronger restrictive ﬁnite-sample bound giving conditions samples bounded variance. straightforward extension proposition charikar defer proof section proposition suppose distribution bounded variance norm ex∼px then given samples probability subset points first consider case given k-tuples independent samples distribution taking empirical distribution samples element simplex instance given triple samples produce vector general denote resulting distribution elements ﬁrst establish ﬁnite-sample concentration omit proof corollary follows straightforwardly proposition proposition proof consider sample interpreted indicator vector first certainly therefore average independent samples invoking proposition yields ﬁrst part proposition provided moment generating function bounded next give results distributions bounded moments. proposition suppose distribution bounded moments v|k]/k then given samples probability exp) \u0001)-resilient -norm. proof invoke proposition uniform distribution unit sphere. first note therefore chebyshev’s inequality −/kσk therefore take proposition similarly take proposition ball elements together imply probability size \u0001)-resilient shown. before obtain following immediate corollary corollary distribution moments bounded suppose samples drawn sufﬁciently small. then given possible recover within -distance mean ﬁnal information-theoretic results semi-random stochastic block model charikar model consider graph vertices unknown good vertices. simplicity assume graph directed graph. connected probability edges allowed arbitrary. }n×n adjacency matrix rows jointly independent samples distribution mean distribution coordinates outside apply robust mean estimation coordinates lying results distribution robustly recover itself. denote samples corresponding elements lemma suppose drawn stochastic block model parameters take norm max|j|=αn maximum -norm coordinates then probability exp) deﬁne resilient around parameter presenting applications algorithmic result theorem first show theorem implies deterministic recovery result sets resilient p-norm corollary suppose points contains size -resilient around mean p-norm efﬁcient algorithm whose output satisﬁes proof first observe p-norm -strongly convex therefore assumed level resilience core proposition moreover size core size proposition latter case core second moments bounded tion holds. following result follows theorem nesterov theorem denote q-norm matrix rd×d maxvq≤ maxy diagq/≤a therefore particular p-norm dual norm q-norm theorem asserts assumption holds diagq/ probability yields recovery guarantee denote dual norm i.e. proposition implies particular large analog proposition hold implies implies implies know whether analog proposition holds e.g. proof ﬁrst part note mean subset coordinates lying -resilience. proof note distribution resilience around computed max{e probability mean write section proving lemma proof note part bound already established lemma sufﬁces show part. mean note must deﬁnition. thus replace statement lemma cost changing left-hand-side problem unitarily invariant solved efﬁciently svd. particular optimal lies span appropriate change variables sufﬁces minimize also check optimal diagonal basis need solve vector minimization corollary proposition charikar states given distribution probability possible covariance relation points above. applying proposition directly give result instead apply transformed version particular covariance given sample σ−/. identity covariance mean proposition charikar implies terms multiplying", "year": 2017}