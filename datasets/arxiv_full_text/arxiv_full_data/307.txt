{"title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "cs.SE"], "abstract": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.", "text": "typical approach table querying convert natural language sentence executable logic form known semantic parsing. traditionally building semantic parser requires extensive human engineering explicit features fast development deep learning increasingly number studies neural networks semantic parsing. dong lapata xiao apply sequence-to-sequence neural models generate logic form conditioned input sentence training requires groundtruth logic forms costly obtain speciﬁc certain dataset. realistic settings assume groundtruth denotations available know execution sequences intermediate execution results. liang train seqseq network reinforce policy gradient. known reinforce algorithm sensitive initial policy; also could difﬁcult started early stages. propose fully distributed neural enquirer comprising several neuralized execution layers ﬁeld attention annotation etc. model trained end-to-end fashion components differentiable. however lacks explicit interpretation efﬁcient execution intensive matrix/vector operation neural processing. building neural networks query knowledge base natural language emerging research topic deep learning. executor table querying typically requires multiple steps execution queries complicated structures. previous studies researchers developed either fully distributed executors symbolic executors table querying. distributed executor trained end-to-end fashion weak terms execution efﬁciency explicit interpretability. symbolic executor efﬁcient execution difﬁcult train especially initial stages. paper propose couple distributed symbolic execution natural language queries symbolic executor pretrained distributed executor’s intermediate execution results step-by-step fashion. experiments show approach signiﬁcantly outperforms distributed symbolic executors exhibiting high accuracy high learning efﬁciency high execution efﬁciency high interpretability. using natural language query knowledge base important task wide applications question answering human-computer conversation etc. table illustrates example knowledge base query long game largest host country size? answer question ﬁrst laboratory high conﬁdence software technologies moe; software institute peking university china deeplycurious.ai noah’s huawei technologies. work done ﬁrst author intern huawei. l.m. <double power.mougmail.com> z.l. <luzdeeplycurious.ai> h.l. <hangli.hlhuawei.com> z.j. <zhijinsei.pku.edu.cn>. deﬁning symbolic operators step possible execution results fused softmax layer predicts probability operator current step. step-by-step fusion accomplished weighted average model trained mean square error. hence approaches work numeric tables suited operations like string matching. also suffers problem exponential numbers combinatorial states model explores entire space time step-bystep weighted average. paper propose couple distributed symbolic execution natural language queries. symbolic execution mean deﬁne symbolic operators keep discrete intermediate execution results. intuition rises observation fully distributed/neuralized executor also exhibits symbolic interpretation. example ﬁeld attention gadget generally aligns column selection. therefore distributed model’s intermediate execution results supervision signals pretrain symbolic executor. guided imperfect stepby-step supervision symbolic executor learns fairly meaningful initial policy largely alleviates cold start problem reinforce. moreover improved policy back distributed executor improve neural network’s performance. evaluated proposed approach dataset experimental results show reinforce algorithm alone takes long started. even does stuck poor local optima. pretrained imperfect supervision signals symbolic executor recover execution sequences also achieves highest denotation accuracy. emphasized that experiment neither distributed executor symbolic executor aware groundtruth execution sequences entire model trained weak supervision denotations only. subsection provides uniﬁed view distributed symbolic execution explain symbolic executor pretrained distributed one’s intermediate execution results trained reinforce algorithm. distributed executor makes full neural networks table querying. distributed mean semantic units represented distributed real-valued vectors processed neural networks. notable studies distributed semantics word embeddings discrete words vectors meaning representations query encoder. words mapped word embeddings bi-directional recurrent neural network aggregates information sentence. rnns’ last states directions concatenated query representation table encoder. table cells also represented embeddings. cell column/ﬁeld name cell vector concatenation embeddings processed feed-forward neural network denote representation cell executor. shown figure neural network comprises several steps execution. execution step neural network selects column softcurrent annotation computed another based query previous execution results previous global information well selected current step select i.e. said last execution layer applies softmax classiﬁer cells select answer. similar equation weights softmax associated positions cell embeddings. words probability choosing i-th j-th column neural executor invariant respect order rows columns. order-sensitive architectures might also model table implicitly ignoring order information current treatment reasonable also better aligns symbolic interpretation. symbolic executor different neural programmer keep discrete/symbolic operators well execution results whereas neelakantan fuse execution results weighted average. design operators symbolic execution complete cover types queries scenario. similar distributed executor result one-step symbolic execution information row; here boolean scalar indicating whether selected particular step execution. symbolic execution step takes previous results input column/ﬁeld argument. green boxes figure illustrate process table summarizes primitive operator set. attention annotates vector i.e. embedding vector intuitively thought selection query execution represented distributed semantics here. last step execution softmax classiﬁer applied entire table select cell answer. details explained below. current execution step ﬁrst compute distribution ﬁelds soft ﬁeld selction. computation based query previous global information ﬁeld name embeddings i.e. here weights softmax ﬁeld embeddings difference shufﬂes table columns. besides ﬁeld name embedding shared among training samples containing ﬁeld pilot experiment tried gating mechanism indicate results selection hopes aligning symbolic table execution. however preliminary experiments show gates exhibit much interpretation results performance degradation. distributed semantics provide information -bit gate row. explanation choose whose value particular column mentioned query choose previously selected candidate rows minimum value particular column choose previously selected candidate rows maximum value particular column operator select argmin argmax greater choose rows whose value particular column greater previously selected less select value choose value particular column previously selected stacked multiple steps primitive operators executor answer fairly complicated questions like long last game smaller country size game whose host country example execution sequence training symbolic executor without step-by-step supervision signals non-trivial. typical training method reinforcement learning trial-and-error fashion. however random initial policy probability recovering accurate execution sequence extremely low. given table example probability probability obtaining accurate denotation also low. therefore symbolic executors efﬁcient learning. distributed executor end-to-end learnable execution efﬁciency intensive matrix/vector multiplication neural information processing. fully neuralized execution also lacks explicit interpretation. symbolic executor high execution efﬁciency explicit interpretation. however cannot trained end-to-end manner suffers cold start problem reinforcement learning. propose combine worlds using distributed executor’s intermediate execution results pretrain symbolic executor initial policy; reinforce algorithm improve policy. well-trained symbolic executor’s intermediate results also back distributed executor improve performance. distributed symbolic observe ﬁeld attention equation generally aligns column selection equation therefore pretrain column selector symbolic executor labels predicted fully neuralized/distributed executor. formally operator predictor argument predictor execution step actions reinforcement learning terwould like pretrain actions minologies. a··· cost function particular data sample label number labels j-th action. label predicted probability operator/argument predictors figure label induced action fully distributed model figure scenario pretrain column predictors. deﬁne binary reward indicating whether ﬁnal result symbolic execution matches groundtruth denotation. loss function policy negative expected reward actions sampled current predicted probabilities help training reinforce tricks balance exploration exploitation small probability words sample action predicted action distribution probability uniform distribution possible actions probability small fraction uniform sampling helps model escape poor local optima continues explore entire action space training. adjust reward subtracting mean reward averaged sampled actions certain data point. common practice reinforce also truncate negative rewards notice tricks applied coupled training baselines fairness. distributed symbolic distributed policy improvement reinforce could further feed back symbolic executor’s intermediate results distributed akin step-by-step supervision setting loss combination denotation cross entropy loss jdenotation ﬁeld attention cross entropy loss jﬁelds. overall training objective jdenotation λjﬁelds hyperparameter balancing factors. seen section feeding back intermediate results improves distributed executor’s performance. shows distributed symbolic worlds indeed coupled well. evaluated approach dataset dataset comprises different tables queries training; validation test sets contain samples respectively overlap training data. table size different samples different tables; queries divided four types selectwhere superlative wheresuperlative nestquery requiring execution steps groundtruth denotation execution actions dataset synthesized complicated rules templates research purposes. however denotations used labels training realistic setting; execution sequences used testing. sake simplicity presume number execution steps known priori training although knowledge execution limitation approach current focus. easily design dummy operator unnecessary step also train discriminative sentence model predict number execution steps small number labels available. process data synthesizing also provides intermediate execution results in-depth analysis. like babi machine comprehension dataset setting prerequisite general semantic parsing. data available project website; code data generation also downloaded facilitate development dataset. pretraining symbolic executor applied maximum likelihood estimation epochs column selection labels predicted distributed executor. used reinforce algorithm improve policy generated action samples data point exploration probability besides neural networks also included sempre system baseline comparison. results reported sempre version specially optimized table query adopted thus suited scenario. exception query rnn’s hidden states. used birnn found likely overﬁt symbolic setting hence used results slower training rugged error surfaces higher peak performance. distributed symbolic executors outperform sempre system showing neural networks capture query information effectively human-engineered features. further coupled approach also signiﬁcantly outperforms them. trained solely reinforce symbolic executor recover execution sequences simple questions however complicated queries learns last steps execution trouble recovering early steps even tricks section results execution accuracy near denotation accuracy because scenario still half chance obtain accurate denotation even nested execution wrong—the ultimate result either candidate list given wrong where-clause execution. accuracy execution crucial interpretability model. execution accurate actions correct. shown above accurate denotation necessarily imply accurate execution. coupled training recovers correct execution sequences symbolic executor alone cannot recover complicated cases fully distributed enquirer explicit interpretations execution. figure validation learning curves. symbolic executor trained reinforce only. symbolic executor -epoch pretraining using distributed executor supervised learning fashion. settings three trajectories different random initializations. dotted lines denotation accuracy. solid lines execution accuracy. figure shows symbolic executor hard train reinforce alone trajectory obtains near-zero execution accuracy epochs; take epochs started escape initial plateaus. even achieve execution accuracy denotation accuracy stuck poor local optima. figure presents learning curves symbolic executor pretrained intermediate ﬁeld attention distributed executor. since operation predictors still hanging pretraining denotation accuracy near reinforcement learning. however epochs reinforce training performance increases sharply achieves high accuracy gradually. notice epochs supervised pretraining. however time negligible compared reinforcement learning experiments reinforce generates samples hence theoretically times slower. results show coupled approach much higher learning efﬁciency pure symbolic executor. table compares execution efﬁciency distributed executor coupled approach. neural networks implemented theano titan black xeon cpu; symbolic execution assessed implementation. comparison makes sense theano platform specialized symbolic execution fortunately execution results affect actions experiment. hence easily disentangled. shown table execution efﬁciency approach times higher distributed executor depending implementation. distributed executor predicting maps every token table execution efﬁciency. present running time test containing samples. †the symbolic execution assessed implementation. others implemented theano including fully distributed model well operator argument predictors. table accuracy fully distributed model trained different methods. last ﬁrst train distributed executor feed intermediate execution results symbolic one; symbolic executor’s intermediate results back distributed one. †reported distributed real-valued vector resulting intensive matrixvector operations. symbolic executor needs neural network predict actions thus lightweight. further observe execution blazingly fast implying that compared distributed models approach could achieve even efﬁciency boost larger table complicated operation. feed back well-trained symbolic executor’s intermediate results fully distributed one. welltrained symbolic executor achieved high execution accuracy setting analogous strong supervision step-by-step groundtruth signals thus also achieves similar performance shown table showcase distributed executor’s ﬁeld attention figure trained end-to-end fashion neural network exhibits interpretation last three steps example early step ﬁeld attention incorrect after feeding back symbolic executor’s intermediate results step-by-step supervision distributed executor exhibits near-perfect ﬁeld attention. figure distributed executor’s intermediate results ﬁeld attention. trained end-to-end fashion bottom one-round co-training distributed symbolic executors plot indicates incorrect ﬁeld attention. symbolic worlds indeed coupled well. complicated applications could also possibilities iteratively training model leveraging co-training fashion. neural execution recently aroused much interest deep learning community. besides sql-like execution extensively discussed previous sections neural turing machines neural programmer-interpreters aimed general programs. former distributed analog turing machines soft operators semantics however cannot grounded actual operations. latter learns generate execution trace fully supervised step-by-step manner. another related topic incorporating neural networks external mechanisms. propose better train neural network leveraging classiﬁcation distribution induced rule-based system. propose induce sparse code reinforce make neural networks focus relevant information. machine translation alignment heuristics train attention signal neural networks supervised manner. studies researchers typically leverage external hard mechanisms improve neural networks’ performance. uniqueness work train fully neuralized/distributed model ﬁrst takes advantage differentiability guide symbolic model achieve meaningful initial policy. trained reinforcement learning symbolic model’s knowledge improve neural networks’ performance feeding back step-by-step supervision. work sheds light neural sequence prediction general example exploring word alignment chunking information machine translation coupling neural external mechanisms. paper proposed coupled view distributed symbolic execution natural language queries. pretraining intermediate execution results distributed executor manage accelerate symbolic model’s reinforce training large extent. well-trained symbolic executor could also guide distributed executor achieve better performance. proposed approach takes advantage distributed symbolic worlds achieving high interpretability high execution efﬁciency high learning efﬁciency well high accuracy. pilot study paper raises several open questions neural networks exhibit symbolic interpretations? better transfer knowledge distributed symbolic worlds? future work would like design interpretable operators distributed model better couple worlds ease training reinforce. would also like explore different ways transferring knowledge e.g. distilling knowledge action distributions sampling actions following distributed model’s predicted distribution symbolic one’s monte carlo policy gradient training tsung-hsien vandyke david mrkˇsi´c nikola gasic milica rojas barahona lina pei-hao ultes stefan young steve. network-based endto-end trainable task-oriented dialogue system. eacl thank pengcheng jiatao helpful discussions; also thank reviewers insightful comments. research partially supported national basic research program china grant nos. national natural science foundation china grant nos.", "year": 2016}