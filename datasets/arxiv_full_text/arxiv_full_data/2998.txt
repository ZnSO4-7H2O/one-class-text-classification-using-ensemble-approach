{"title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based  Localization", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a \"stronger\" deep network from a \"weaker\" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.", "text": "propose technique producing ‘visual explanations’ decisions large class convolutional neural network -based models making transparent. approach gradient-weighted class activation mapping uses gradients target concept ﬂowing ﬁnal convolutional layer produce coarse localization highlighting important regions image predicting concept. unlike previous approaches grad-cam applicable wide variety model-families cnns fully-connected layers cnns used structured outputs cnns used tasks multi-modal inputs reinforcement learning without architectural changes re-training. combine grad-cam existing ﬁne-grained visualizations create high-resolution class-discriminative visualization apply image classiﬁcation image captioning visual question answering models including resnet-based architectures. context image classiﬁcation models visualizations lend insights failure modes models robust adversarial images outperform previous methods ilsvrc- weakly-supervised localization task faithful underlying model help achieve model generalization identifying dataset bias. image captioning visualizations show even non-attention based models localize inputs. finally design conduct human studies measure grad-cam explanations help users establish appropriate trust predictions deep networks show gradcam helps untrained users successfully discern ‘stronger’ deep network ‘weaker’ one. code available https//github.com/ramprs/grad-cam/ demo available cloudcv video demo found youtu.be/cojubizke. convolutional neural networks deep networks enabled unprecedented breakthroughs variety computer vision tasks image classiﬁcation object detection semantic segmentation image captioning recently visual question answering deep neural networks enable superior performance lack decomposability intuitive understandable components makes hard interpret consequently today’s intelligent systems fail fail spectacularly disgracefully without warning explanation leaving user staring incoherent output wondering why. interpretability matters. order build trust intellegent systems move towards meaningful integration everyday lives clear must build ‘transparent’ models explain predict predict. broadly speaking transparency useful three different stages artiﬁcial intelligence evolution. first signiﬁcantly weaker humans reliably ‘deployable’ goal transparency explanations identify failure modes thereby helping researchers focus efforts fruitful research directions. second humans reliably ‘deployable’ categories trained sufﬁcient data) goal establish appropriate trust conﬁdence users. third signiﬁcantly stronger humans goal explanations machine teaching i.e. machine teaching human make better decisions. typically exists trade-off accuracy simplicity interpretability. classical rule-based expert systems highly interpretable accurate decomposable pipelines stage hand-designed thought interpretable individual component assumes natural intuitive explanation. using deep models sacriﬁce interpretable figure original image dog. support category according various visualizations resnet. guided backpropagation highlights contributing features. grad-cam localizes class-discriminative regions combining gives guided grad-cam gives highresolution class-discriminative visualizations.interestingly localizations achieved grad-cam technique similar results occlusion sensitivity orders magnitude cheaper compute. grad-cam visualizations resnet- layer. note regions corresponds high score class blue corresponds evidence class. figure best viewed color. modules uninterpretable ones achieve greater performance greater abstraction tighter integration recently introduced deep residual networks -layers deep shown state-of-the-art performance several challenging tasks. complexity makes models hard interpret. such deep models beginning explore spectrum interpretability accuracy. zhou recently proposed technique called class activation mapping identifying discriminative regions used restricted class image classiﬁcation cnns contain fully-connected layers. essence work trades model complexity performance transparency working model. contrast make existing state-of-the-art deep models interpretable without altering architecture thus avoiding interpretability accuracy tradeoff. approach generalization applicable signiﬁcantly broader range model families cnns fully-connected layers cnns used structured outputs cnns used tasks multi-modal inputs reinforcement learning. makes good visual explanation? consider image classiﬁcation ‘good’ visual explanation model justifying predicted class classdiscriminative high-resolution fig. shows outputs number visualizations ‘tiger cat’ class ‘boxer’ class pixel-space gradient visualizations guided backpropagation deconvolution high-resolution highlight ﬁne-grained details image class-discriminative order combine best worlds show possible fuse existing pixel-space gradient visualizations grad-cam create guided grad-cam visualizations high-resolution class-discriminative. result important regions image correspond decision interest visualized high-resolution detail even image contains evidence multiple possible concepts shown figures visualized ‘tiger cat’ guided grad-cam highlights regions also highlights stripes important predicting particular variety cat. summarize contributions follows propose grad-cam class-discriminative localization technique generate visual explanations cnn-based network without requiring architectural changes re-training. evaluate grad-cam localization pointing faithfulness model outperforms baselines. apply grad-cam existing top-performing classiﬁcation captioning models. image classiﬁcation visualizations help identify dataset bias lend insight failures current cnns showing seemingly unreasonable predictions reasonable explanations. captioning visualizations expose somesurprising insight common lstm models often good localizing discriminative image regions despite trained grounded image-text pairs. visualize resnets applied image classiﬁcation going deep shallow layers discriminative ability grad-cam signiﬁcantly reduces encounter layers different output dimensionality. cnn-based architecture including image captioning visual question answering. fully-convolutional architecture grad-cam reduces cam.thus grad-cam generalization cam. methods approach localization classifying perturbations input image. zeiler fergus perturb inputs occluding patches classifying occluded image typically resulting lower classiﬁcation scores relevant objects objects occluded. principle applied localization oquab classify many patches containing pixel average patch class-wise scores provide pixel’s class-wise score. unlike these approach achieves localization shot; requires single forward partial backward pass image thus typically order magnitude efﬁcient. recent work zhang introduce contrastive marginal winning probability probabilistic winner-take-all formulation modelling topattention neural classiﬁcation models highlight discriminative regions. slower gradcam like works image classiﬁcation cnns. moreover quantitative qualitative results worse grad-cam worse grad-cam. approach number previous works asserted deeper representations capture higher-level visual constructs furthermore convolutional features naturally retain spatial information lost fully-connected layers expect last convolutional layers best compromise high-level semantics detailed spatial information. neurons layers look semantic class-speciﬁc information image grad-cam uses gradient information ﬂowing last convolutional layer understand importance neuron decision interest. although technique generic used visualize activation deep network work focus explaining decisions network possibly make. order obtain classgrad-cam discriminative localization grad-cam ru×v width height class ﬁrst compute gradient score class respect feature maps convolutional layer i.e. gradients ﬂowing back globalk average-pooled obtain neuron importance weights conduct human studies show guided grad-cam explanations class-discriminative help humans establish trust also help untrained users successfully discern ‘stronger’ network ‘weaker’ even make identical predictions. related work work draws recent work visualizations model trust assessment weakly-supervised localization. visualizing cnns. number previous works visualized predictions highlighting ‘important’ pixels speciﬁcally simonyan visualize partial derivatives predicted class scores w.r.t. pixel intensities guided backpropagation deconvolution make modiﬁcations ‘raw’ gradients result qualitative improvements. approaches compared despite producing ﬁne-grained visualizations methods class-discriminative. visualizations respect different classes nearly identical visualization methods synthesize images maximally activate network unit invert latent representation although high-resolution class-discriminative visualize model overall predictions speciﬁc input images. assessing model trust. motivated notions interpretability assessing trust models evaluate grad-cam visualizations manner similar human studies show important tools users evaluate place trust automated systems. weakly supervised localization. another relevant line work weakly supervised localization context cnns task localize objects images using whole image class labels relevant approach class activation mapping approach localization approach modiﬁes image classiﬁcation architectures replacing fully-connected layers convolutional layers global average pooling thus achieving class-speciﬁc feature maps. others investigated similar methods using global pooling log-sum-exp pooling drawback requires feature maps directly precede softmax layers applicable particular kind architectures performing global average pooling convolutional maps immediately prior prediction architectures achieve inferior accuracies compared general networks tasks simply inapplicable tasks introduce combining feature maps using gradient signal require modiﬁcation network architecture. allows approach applied figure grad-cam overview given image class interest input forward propagate image part model task-speciﬁc computations obtain score category. gradients zero classes except desired class signal backpropagated rectiﬁed convolutional feature maps interest combine compute coarse grad-cam localization represents model look make particular decision. finally pointwise multiply heatmap guided backpropagation guided grad-cam visualizations high-resolution concept-speciﬁc. network downstream captures ‘importance’ feature target class note modiﬁcation architecture necessitates retraining architectures weights connecting features maps outputs. grad-cam applied architectures k—making grad-cam strict generalization generalization also allows generate visual explanations cnn-based models cascade convolutional layers much complex interactions. indeed apply grad-cam beyond classiﬁcation tasks models utilize cnns image captioning visual question answering guided grad-cam. grad-cam visualizations class-discriminative localize relevant image regions well lack ability show ﬁne-grained importance like pixel-space gradient visualization methods example figure grad-cam easily localize region; however unclear low-resolutions heat-map network predicts particular instance ‘tiger cat’. order combine best aspects both fuse guided backpropagation grad-cam visualizations pointwise multiplication fig. notice results coarse heat-map size convolutional feature maps alexnet networks). apply relu linear combination maps interested features positive inﬂuence class interest i.e. pixels whose intensity increased order increase negative pixels likely belong categories image. expected without relu localization maps sometimes highlight desired class achieve lower localization performance. figures show grad-cam visualizations ‘tiger cat’ ‘boxer respectively. ablation studies gradcam visualizations found supplementary. general need class score produced image classiﬁcation cnn. could differentiable activation including words caption answer question. grad-cam generalization cam. recall produces localization image classiﬁcation speciﬁc kind architecture global average pooled convolutional feature maps directly softmax. speciﬁcally penultimate layer produce feature maps ru×v. feature maps spatially pooled using global average pooling linearly transformed produce score class bottom-left illustrates fusion. visualization high-resolution class-discriminative replacing guided backpropagation deconvolution gives similar results found deconvolution artifacts chose guided backpropagation deconvolution. evaluating localization weakly-supervised localization section evaluate localization capability grad-cam context image classiﬁcation. imagenet localization challenge requires competing approaches provide bounding boxes addition classiﬁcation labels. similar classiﬁcation evaluation performed top- top- predicted categories. given image ﬁrst obtain class predictions network generate grad-cam maps predicted classes binarize threshold intensity. results connected segments pixels draw bounding around single largest segment. evaluate pretrained off-the-shelf vgg- model caffe model zoo. following ilsvrc evaluation report top- top- localization error table. grad-cam localization errors signiﬁcantly lower achieved c-mwp simonyan vgg- model uses grabcut post-process image space gradients heat maps. grad-cam also achieves better top- localization error requires change model architecture necessitates re-training thereby achieves worse classiﬁcation errors whereas gradcam makes compromise classiﬁcation performance. table classiﬁcation localization results ilsvrc- weakly-supervised segmentation. grad-cam localization weak-supervision train segmentation architecture provide details along qualitative results supplementary section pointing game zhang introduced pointing game experiment evaluate discriminativeness different attention maps localizing target objects scenes. evaluation protocol cues competing visualization technique ground-truth object label extracts maximum point generated heatmap evaluates lies annotated instances cued object category thereby miss counted. localization accuracy calculated hits+m isses. however evaluation figure interfaces evaluating different visualizations class discrimination trust worthiness guided grad-cam outperforms baseline approaches showing visualizations class-discriminative help humans place trust accurate classiﬁer. measures precision aspect visualization technique. hence modify protocol also measure recall follows. compute visualization class predictions classiﬁers evaluate using pointing game setup additional option visualization reject top- predictions model value visualization threshold i.e. visualization correctly rejects predictions absent ground-truth categories gets hit. approach grad-cam outperforms c-mwp signiﬁcant margin qualitative examples comparing c-mwp grad-cam coco imagenet pascal categories found supplementary section evaluating visualizations ﬁrst human study evaluates main premise approach grad-cam visualizations classdiscriminative previous techniques? established that turn understanding whether lead user trust visualized models appropriately. experiments compare vgg- alexnet cnns ﬁnetuned pascal train generate visualizations. evaluating class discrimination order measure whether grad-cam helps distinguish classes select images contain exactly annotated categories create visualizations them. vgg- alexnet cnns obtain category-speciﬁc visualizations using four techniques deconvolution guided backpropagation grad-cam versions methods show visualizations workers amazon mechanical turk which object categories depicted image? shown fig. image-category pairs ratings collected image evaluated ground truth averaged obtain accuracy. viewing guided grad-cam human subjects correctly identify category visualized cases similarly also grad-cam helps make deconvolution class-discriminative guided grad-cam performs best among methods. interestingly results seem indicate deconvolution class discriminative guided backpropagation although guided backpropagation aesthetically pleasing deconvolution. best knowledge evaluations ﬁrst quantify subtle difference. given prediction explanations want evaluate seems trustworthy. alexnet compare guided backpropagation guided gradcam visualizations noting vgg- known reliable alexnet accuracy pascal classiﬁcation. order tease apart efﬁcacy visualization accuracy model visualized consider instances models made prediction ground truth. given visualization alexnet vgg- predicted object category workers instructed rate reliability models relative scale clearly more/less reliable slightly more/less reliable equally reliable interface shown fig. eliminate biases alexnet assigned model approximately equal probability. remarkably human subjects able identify accurate classiﬁer despite viewing identical predictions simply different explanations generated two. guided backpropagation humans assign average score means slightly reliable alexnet guided grad-cam achieves higher score closer option saying clearly reliable. thus visualization help users place trust model generalize better based individual prediction explanations. faithfulness interpretability faithfulness visualization model ability accurately explain function learned model. naturally exists tradeoff interpretability faithfulness visualization faithful visualization typically less interpretable vice versa. fact could argue fully faithful explanation entire description model case deep models interpretable/easy visualize. veriﬁed previous sections visualizations reasonably interpretable. evaluate faithful underlying model. expectation explanations locally accurate i.e. vicinity input data point explanation faithful model comparison need reference explanation high local-faithfulness. obvious choice visualization image occlusion measure difference scores patches input image masked. interestingly patches change score also patches grad-cam guided gradcam assign high intensity achieving rank correlation averaged images pascal set. shows grad-cam visualizations faithful original model compared existing methods. localization pointing segmentation human studies grad-cam visualizations interpretable correlation occlusion maps gradcam faithful model important characteristics visualization technique. diagnosing image classiﬁcation cnns analyzing failure modes vgg- figure cases model failed predict correct class predictions. humans would hard explain predictions without looking visualization predicted class. grad-cam mistakes seem justiﬁable. guided grad-cam analyze failure modes vgg- imagenet classiﬁcation order mistakes network making ﬁrst list examples network fails classify correctly. misclassiﬁed examples guided grad-cam visualize correct predicted class. major advantage guided grad-cam visualization methods allows analysis high-resolution ability highly classdiscriminative. seen fig. failures re-trained model generalizes better balanced test additional analysis along grad-cam visualizations models found supplementary. experiment demonstrates grad-cam help detect remove biases datasets important generalization also fair ethical outcomes algorithmic decisions made society. counterfactual explanations propose explanation modality counterfactual explanations. using slight modiﬁcation grad-cam obtain counterfactual explanations highlight support regions would make network change decision. removing concepts occurring regions would make model conﬁdent given target decision. finally apply grad-cam technique image captioning visual question answering tasks. grad-cam leads interpretable visual explanations tasks compared baseline visualizations change noticeably across different predictions. note existing visualization techniques either class-discriminative simply cannot used tasks architectures image captioning section visualize spatial support image captioning model using grad-cam. build publicly available ‘neuraltalk’ implementation uses ﬁnetuned vgg- images lstmbased language model. note model ambiguities inherent imagenet classiﬁcation. also seemingly unreasonable predictions reasonable explanations observation also made hoggles effect adversarial noise vgg- goodfellow demonstrated vulnerability current deep networks adversarial examples slight imperceptible perturbations input images fool network misclassifying high conﬁdence. generate adversarial images imagenet trained vgg- model assigns high probability category absent image probability categories present. compute grad-cam visualizations categories present. fig. inspite network completely certain absence categories grad-cam visualizations correctly localize categories. shows robustness grad-cam adversarial noise. figure original image generated adversarial image category grad-cam visualizations original categories tiger liner. boxer along conﬁdence. inspite network completely fooled thinking image belongs airliner category high conﬁdence grad-cam localize original categories accurately. section demonstrate another grad-cam identifying thus reducing bias training datasets. models trained biased datasets generalize realworld scenarios worse perpetuate biases stereotypes ﬁnetune imagenet trained vgg- model task classifying doctor nurse. built training dataset using relevant images popular image search engine. trained model achieves good accuracy validation images search engine. test time model generalize well grad-cam visualizations model predictions revealed model learned look person’s face hairstyle distinguish nurses doctors thus learning gender stereotype. indeed model misclassifying several female doctors nurse male nurses doctor. clearly problematic. turns image search results gender-biased intuition gained visualization reduced bias training adding male nurses female doctors training maintaining number images class before. figure interpreting image captioning models class-discriminative localization technique grad-cam spatial support regions captions images. fig. visual explanations image captioning model highlighting image regions considered important producing captions. fig. grad-cam localizations global holistic captioning model captions generated dense captioning model three bounding proposals marked left. back grad-cam localizations agree bounding boxes even though captioning model grad-cam techniques bounding annotations. explicit attention mechanism. given caption compute gradient probability w.r.t. units last convolutional layer generate grad-cam visualizations described section fig. ﬁrst example grad-cam maps generated caption localize every occurrence kites people spite relatively small size. next example notice grad-cam correctly highlights pizza ignores woman nearby since ‘woman’ mentioned caption. qualitative examples found supplementary section comparison dense captioning. johnson recently introduced dense captioning task requires system jointly localize caption salient regions given image. model consists fully convolutional localization network lstmbased language model produces bounding boxes regions interest associated captions single forward pass. using densecap model generate region-speciﬁc captions. next visualize grad-cam localizations region-speciﬁc captions using holistic captioning model described earlier interestingly observe grad-cam localizations correspond regions image densecap model described even though holistic captioning model trained region bounding-box level annotations visual question answering typical pipelines consist model images language model questions. image question representations fused predict answer typically -way classiﬁcation. since classiﬁcation problem pick answer score compute grad-cam show image evidence supports answer. despite complexity task involving visual language components explanations described fig. suprisingly intuitive informative. comparison human attention. collected human attention maps subset dataset maps high intensity humans looked image order answer visual question. human attention maps compared grad-cam visualizations model question-image figure qualitative results experiments given image left question what color ﬁrehydrant? visualize grad-cams guided grad-cams answers red\" yellow\" yellow red\". gradcam visualizations highly interpretable help explain target prediction model focuses bottom part ﬁrehydrant; forced answer yellow model concentrates it‘s yellow forced answer yellow red\" looks whole ﬁrehydrant approach capable providing interpretable explanations even complex model architectures. pairs using rank correlation evaluation protocol developed grad-cam human attention maps correlation statistically higher chance random attention maps shows despite trained grounded image-text pairs even non-attention based lstm based models surprisingly good localizing discriminative regions required output particular answer. visualizing resnet-based model attention. layer resnet encode image jointly learn hierarchical attention mechanism question image. fig. shows grad-cam visualization network. visualize deeper layers resnet small changes grad-cam adjacent layers larger changes layers involve dimensionality reduction. visualizations various layers resnet found supplementary section best knowledge ﬁrst visualize decisions made resnet-based architectures. conclusion work proposed novel class-discriminative localization technique—gradient-weighted class activation mapping —for making cnn-based models transparent producing visual explanations. further combined grad-cam localizations existing high-resolution visualizations obtain high-resolution class-discriminative guided grad-cam visualizations. visualizations outperform existing approaches weaklysupervised localization pointing faithfulness original model. extensive human studies reveal visualizations discriminate classes accurately better reveal trustworthiness classiﬁer help identify biases datasets. finally showed broad applicability grad-cam various off-the-shelf available architectures tasks including image classiﬁcation image captioning providing faithful visual explanations possible model decisions. believe true system intelligent also able reason beliefs actions humans trust future work includes explaining decisions made deep networks domains reinforcement learning natural language processing video applications. agrawal mathialagan goyal chavali banik mohapatra osman batra. cloudcv large scale distributed computer vision cloud service. mobile cloud visual media computing pages springer cinbis verbeek schmid. weakly supervised object localization multi-fold multiple instance learning. ieee transactions pattern analysis machine intelligence ribeiro singh guestrin. \"why trust you?\" explaining predictions classiﬁer. sigkdd silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search. nature simonyan zisserman. deep convolutional networks large-scale image recognition. iclr springenberg dosovitskiy brox riedmiller. striving simplicity convolutional net. corr abs/. section qualitative results showing grad-cam guided grad-cam visualizations image classiﬁcation image captioning visual question answering image captioning visualizations expose somewhat surprising insight even non-attention based lstm models often good localizing discriminative input image regions despite trained grounded image-text pairs. section provide grad-cam explanations models described section section ablation studies explore validate design choices computing grad-cam visualizations. section weakly-supervised segmentation results pascal using weak-localization cues section details pointing game setup. section comparison existing visualization techniques c-mwp pascal coco visualizations superior faster compute time possible visualize wide variety cnn-based models including limited cnns fully-connected layers cnns stacked recurrent neural networks resnets etc.. section formally prove grad-cam generalization mentioned section main paper. recall architecture consists fully-covolutional cnns followed global average pooling linear guided grad-cam even localize tiny objects. example approach correctly localizes predicted class torch inspite size location image. method also class-discriminative places attention toilet seat even popular imagenet category exists image also visualized grad-cam guided backpropagation deconvolution grad-cam grad-cam images ilsvrc detection least unique object categories each. visualizations mentioned class found following links. publicly available neuraltalk code model image captioning experiments. model uses vgg- encode image. image representation passed input ﬁrst time step lstm generates caption image. model trained end-to-end along ﬁnetuning using coco captioning dataset. feedforward image image captioning model obtain caption. grad-cam coarse localization combine guided backpropagation high-resolution visualization highlights regions image provide support generated caption. grad-cam guided grad-cam explain publicly available model answered answered. model uses standard followed fully connected layer transform image -dim match lstm embeddings question. transformed image lstm embeddings pointwise multiplied combined representation image question multi-layer perceptron trained predict among answers. show visualizations model trained different cnns alexnet vgg- vgg- even though cnns ﬁnetuned task interesting approach serve tool understand networks better providing localized high-resolution visualization regions model looking note networks trained explicit attention mechanism enforced. notice ﬁrst fig. question person riding waves? model alexnet vgg- answered concentrated person mainly waves. hand vgg- correctly answered looked regions around order answer question. second question what person hitting? model trained alexnet answered tennis ball based context without looking ball. model might risky employed real-life scenarios. difﬁcult determine trustworthiness model based predicted answer. visualizations provide accurate explain model’s predictions help determining model trust without making architectural changes sacriﬁcing accuracy. notice last fig. question whole orange? model looks regions around orange answer figure guided backpropagation grad-cam guided grad-cam visualizations answers model. image-question pair show visualizations alexnet vgg- vgg-. notice attention changes change answer yellow green. section provide qualitative examples showing explanations models trained distinguishing doctors nursesmodel trained images popular search engine model trained balanced images search engine. figure grad-cam explanations model model. even though models made right decision biased model looking face person decide person nurse whereas unbiased model looking short sleeves make decision example image example biased model made wrong prediction looking face hairstyle unbiased model made right prediction looking white coat stethoscope fig. main paper show results occlusion sensitivity class. compute occlusion repeatedly masking regions image forward propagate masked image. location occlusion store difference original score particular class score obtained forward propagating masked image. choices mask sizes include zero-pad images resultant occlusion size original image. resultant occlusion maps found fig. note blue regions correspond decrease score particular class region around pixel occluded. hence serves evidence class. whereas regions correspond increase score region around pixel occluded. hence regions might indicate existence confusing classes. observe good trade-off sharp results smooth appearance. show results applying grad-cam tiger-cat category different convolutional layers alexnet vgg- cnn. expected results fig. show localization becomes progressively worse move shallower convolutional layers. later convolutional layers capture high-level semantic information time retain spatial information shallower layers smaller receptive ﬁelds concentrate local features important next layers. removing relu increases error table. negative values grad-cam indicate confusion multiple occurring classes. thus localization improves suppress figure grad-cam different convolutional layers ‘tiger cat’ class. ﬁgure analyzes localizations change qualitatively perform grad-cam respect different feature maps best looking visualizations often obtained deepest convolutional layer network localizations progressively worse shallower layers. consistent intuition described section main paper. instead global average pooling incoming gradients convolutional layer tried global pooling them. observe using lowers localization ability grad-cam technique. example found fig. below. observation also summarized table. fact statistically less robust noise compared averaged gradient. effect guided-relu springenberg introduced guided backprop modiﬁed backward pass relu pass positive gradients regions positive activations. applying change computation grad-cam maps introduces drop class-discriminative ability grad-cam seen fig. gives slight improvement localization ability ilsvrc’ localization challenge effect deconv-relu zeiler fergus deconvolution work introduced slight modiﬁcation backward pass relu pass positive gradients higher layers. applying modiﬁcation computation grad-cam gives worse results shown fig. recent work kolesnikov introduced loss function training weakly-supervised image segmentation models. loss function based three principles seed weak localization cues expand object seeds regions reasonable size constrain segmentations object boundaries. showed proposed loss function leads better segmentation. showed algorithm sensitive seed loss without segmentation network fails localize objects correctly work used weakly localizing foreground classes. replaced grad-cam show results fig. last shows failure cases. bottom left image clothes person weren’t highlighted correctly. could discriminative parts faces hence grad-cam maps highlights those. results segmentation highlights faces people. bottom right image bicycles extremely thin aren’t highlighed. could resolution grad-cam maps makes difﬁcult capture thin areas. figure grad-cam visualizations tiger category different modiﬁcations relu backward pass. best results obtained actual gradients computation grad-cam. pointing game setup evaluate discriminativeness different attention maps localizing ground-truth categories. sense evaluates precision visualization i.e. often attention intersect segmentation ground-truth category. evaluate often visualization technique produces maps correspond category interest. example evaluation penalize visualization fig. top-left highlighting zebra visualizing bird category. hence propose modiﬁcation pointing game evaluate visualizations top- predicted category. case visualizations given additional option reject top- predictions classiﬁers. visualizations grad-cam c-mwp choose threshold value visualization used determine category visualized exists image. compute maps top- categories based maximum value classify label category absent image. mentioned section main paper approach grad-cam outperforms c-mwp signiﬁcant margin fig. shows maps computed top- categories using c-mwp grad-cam. compare grad-cam c-mwp visualizations imagenet trained vgg- models ﬁnetuned pascal dataset. grad-cam c-mwp visualizations directly obtained existing models requires architectural change requires re-training leads loss accuracy. also unlike grad-cam c-mwp applied image classiﬁcation networks. visualizations ground-truth categories found fig. compare grad-cam c-mwp visualizations imagenet trained vgg- models ﬁnetuned coco dataset. visualizations top- predicted categories found fig. seen c-mwp highlights arbitrary regions predicted non-existent categories unlike grad-cam seem much reasonable. quantitatively evaluate pointing experiment. current resnets typically consist residual blocks. blocks identity skip connections sets residual blocks interspersed downsampling modules alter dimensions propagating signal. seen fig. visualizations applied last convolutional layer correctly localize dog. grad-cam also visualize correctly residual blocks last set. however towards earlier sets residual blocks different spatial resolution grad-cam fails localize category interest obseve similar trends resnet architectures", "year": 2016}