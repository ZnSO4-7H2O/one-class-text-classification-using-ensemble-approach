{"title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge  Potentials", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.", "text": "state-of-the-art techniques multi-class image segmentation labeling conditional random ﬁelds deﬁned pixels image regions. regionlevel models often feature dense pairwise connectivity pixel-level models considerably larger permitted sparse graph structures. paper consider fully connected models deﬁned complete pixels image. resulting graphs billions edges making traditional inference algorithms impractical. main contribution highly efﬁcient approximate inference algorithm fully connected models pairwise edge potentials deﬁned linear combination gaussian kernels. experiments demonstrate dense connectivity pixel level substantially improves segmentation labeling accuracy. multi-class image segmentation labeling challenging actively studied problems computer vision. goal label every pixel image several predetermined object categories thus concurrently performing recognition segmentation multiple object classes. common approach pose problem maximum posteriori inference conditional random ﬁeld deﬁned pixels image patches potentials incorporate smoothness terms maximize label agreement similar pixels integrate elaborate terms model contextual relationships object classes. basic models composed unary potentials individual pixels image patches pairwise potentials neighboring pixels patches resulting adjacency structure limited ability model long-range connections within image generally results excessive smoothing object boundaries. order improve segmentation labeling accuracy researchers expanded basic framework incorporate hierarchical connectivity higher-order potentials deﬁned image regions however accuracy approaches necessarily restricted accuracy unsupervised image segmentation used compute regions model operates. limits ability region-based approaches produce accurate label assignments around complex object boundaries although signiﬁcant progress made paper explore different model structure accurate semantic segmentation labeling. fully connected establishes pairwise potentials pairs pixels image. fully connected crfs used semantic image labeling past complexity inference fully connected models restricted application sets hundreds image regions fewer. segmentation accuracy achieved approaches limited unsupervised segmentation produces regions. contrast model connects figure pixel-level classiﬁcation fully connected crf. input image msrc- dataset. response unary classiﬁers used models. classiﬁcation produced robust classiﬁcation produced mcmc inference fully connected pixel-level model; algorithm hours partially converged bottom image. classiﬁcation produced inference algorithm fully connected model seconds. pairs individual pixels image enabling greatly reﬁned segmentation labeling. main challenge size model tens thousands nodes billions edges even low-resolution images. main contribution highly efﬁcient inference algorithm fully connected models pairwise edge potentials deﬁned linear combination gaussian kernels arbitrary feature space. algorithm based mean ﬁeld approximation distribution. approximation iteratively optimized series message passing steps updates single variable aggregating information variables. show mean ﬁeld update variables fully connected performed using gaussian ﬁltering feature space. allows reduce computational complexity message passing quadratic linear number variables employing efﬁcient approximate high-dimensional ﬁltering resulting approximate inference algorithm sublinear number edges model. figure demonstrates beneﬁts presented algorithm images msrc- dataset multi-class image segmentation labeling. figure shows results approximate mcmc inference fully connected crfs images mcmc procedure hours partially converged bottom image. also experimented graph inference fully connected models converge within hours. contrast single-threaded implementation algorithm produces detailed pixel-level labeling seconds shown figure quantitative evaluation msrc- pascal datasets provided section best knowledge ﬁrst demonstrate efﬁcient inference fully connected models pixel level. consider random ﬁeld deﬁned variables xn}. domain variable labels lk}. consider also random ﬁeld deﬁned variables in}. setting ranges possible input images size ranges possible pixel-level image labelings. color vector pixel label assigned pixel cliques induces potential gibbs energy labeling c∈cg maximum posteriori labeling random ﬁeld maxx∈ln notational convenience omit conditioning rest paper denote fully connected pairwise model complete graph unary pairwise cliques. corresponding gibbs energy range unary potential computed independently pixel classiﬁer produces distribution label assignment given image features. unary potential used implementation incorporates shape texture location color descriptors described section since output unary classiﬁer pixel produced independently outputs classiﬁers pixels labeling produced unary classiﬁers alone generally noisy inconsistent shown figure vectors gaussian kernel linear combination weights label compatibility function. kernel characterized symmetric positive-deﬁnite precision matrix deﬁnes shape. appearance kernel inspired observation nearby pixels similar color likely class. degrees nearness similarity controlled parameters smoothness kernel removes small isolated regions parameters learned data described section simple label compatibility function given potts model xj]. introduces penalty nearby similar pixels assigned different labels. simple model works well practice insensitive compatibility labels. example penalizes pair nearby pixels labeled bird extent pixels labeled cat. instead learn general symmetric compatibility function takes interactions labels account described section algorithm based mean ﬁeld approximation distribution. approximation yields iterative message passing algorithm approximate inference. observation message passing presented model performed using gaussian ﬁltering feature space. enables utilize highly efﬁcient approximations high-dimensional ﬁltering reduce complexity message passing quadratic linear resulting approximate inference algorithm fully connected crfs linear number variables sublinear number edges model. detailed derivation equation given supplementary material. update equation leads following inference algorithm algorithm mean ﬁeld fully connected crfs iteration algorithm performs message passing step compatibility transform local update. compatibility transform local update linear time highly efﬁcient. computational bottleneck message passing. variable step requires evaluating variables. naive implementation thus quadratic complexity number variables next show approximate high-dimensional ﬁltering used reduce computational cost message passing linear. convolution performs low-pass ﬁlter essentially band-limiting sampling theorem function reconstructed samples whose spacing proportional standard deviation ﬁlter thus perform convolution downsampling convolving samples upsampling result feature points common approximation gaussian kernel truncated gaussian values beyond standard deviations zero. since spacing samples proportional standard deviation support truncated kernel contains constant number sample points. thus convolution approximately computed sample aggregating values constant number neighboring samples. implies approximate message passing performed time high-dimensional ﬁltering algorithms follow approach still computational complexity exponential however clever ﬁltering scheme reduce complexity convolution operation permutohedral lattice highly efﬁcient convolution data structure tiles feature space simplices arranged along axes permutohedral lattice exploits separability unit variance gaussian kernels. thus need apply whitening transform feature space order whitening transformation found using cholesky decomposition transformed space high-dimensional convolution separated sequence one-dimensional convolutions along axes lattice. resulting approximate message passing procedure highly efﬁcient even fully sequential implementation make parallelism streaming capabilities graphics hardware provide acceleration desired. learn parameters model piecewise training. first boosted unary classiﬁers trained using jointboost algorithm using features described section next learn appearance kernel parameters potts model. found efﬁciently combination expectation maximization high-dimensional ﬁltering. unfortunately kernel widths cannot computed effectively approach since gradient involves non-gaussian kernels amenable acceleration techniques. found efﬁcient grid search holdout validation three kernel parameters smoothness kernel parameters signiﬁcantly affect classiﬁcation accuracy yield small visual improvement. found work well practice. compatibility parameters learned using l-bfgs maximize loglikelihood model validation images corresponding ground truth labelings l-bfgs requires computation gradient intractable estimate exactly since requires computing gradient partition function instead mean ﬁeld approximation described section estimate gradient leads simple approximation gradient training image single training image ground truth labeling binary image pixel value ground truth label pixel otherwise. detailed derivation equation given supplementary material. computationally expensive evaluate directly. section high-dimensional ﬁltering compute sums efﬁciently. runtime ﬁnal learning algorithm linear number variables unary potentials used implementation derived textonboost -dimensional ﬁlter bank suggested shotton follow ladick´y adding color histogram oriented gradients pixel location features. evaluation msrc- dataset uses extended version textonboost unary potentials. dataset include response bounding object detectors object class additional features. increases performance unary classiﬁers gain additional training logistic regression classiﬁer responses boosted classiﬁer. efﬁcient high-dimensional ﬁltering publicly available implementation permutohedral lattice found downsampling rate standard deviation work best experiments. sampling-based ﬁltering algorithms underestimate edge strength similar feature points. proper normalization cancel error. permutohedral lattice allows types normalizations. global normalization average kernel strength evaluate presented algorithm standard benchmarks multi-class image segmentation labeling. ﬁrst msrc- dataset consists color images size corresponding ground truth labelings object classes second pascal dataset contains color images size approximately total object classes background class presented approach evaluated alongside adjacency shotton robust kohli using publicly available reference implementations. ensure fair comparison models used unary potentials described section experiments conducted intel processor clocked .ghz. eight cores used training; experiments performed single core. inference algorithm implemented single thread. convergence. ﬁrst evaluate convergence mean ﬁeld approximation analyzing kl-divergence figure shows kl-divergence successive iterations inference algorithm. kl-divergence estimated constant described supplementary material. results shown different standard deviations kernels. graphs aligned iterations visual comparison. number iterations subsequent experiments. msrc- dataset. standard split dataset training validation test images unary potentials learned training parameters models learned using holdout validation. total training time minutes. learned label compatibility function performed potts model dataset. figure provides qualitative quantitative results dataset. report standard measures multi-class segmentation accuracy global denotes overall percentage correctly classiﬁed image pixels average unweighted average per-category classiﬁcation accuracy presented inference algorithm fully connected signiﬁcantly outperforms models evaluated standard ground truth data provided dataset. ground truth labelings provided msrc- dataset quite imprecise. particular regions around object boundaries often left unlabeled. makes difﬁcult quantitatively evaluate performance algorithms strive pixel-level accuracy. following kohli manually produced accurate segmentations labelings images msrc- dataset. image fully annotated pixel level careful labeling around complex boundaries. labeling performed hand representative images msrc dataset. labeling single image took minutes average. number images accurate ground truth shown figure figure reports segmentation accuracy ground truth data alongside evaluation standard ground truth. results obtained using -fold cross validation images used train pafigure convergence analysis. kl-divergence mean ﬁeld approximation successive iterations inference algorithm averaged across images msrc- dataset. visualization convergence distributions class labels image dataset. also adopt methodology proposed kohli evaluating segmentation accuracy around boundaries. speciﬁcally count relative number misclassiﬁed pixels within narrow band surrounding actual object boundaries obtained accurate ground truth images. shown figure algorithm outperforms previous work across trimap widths. pascal lack publicly available ground truth labeling test pascal training validation data experiments. randomly partitioned images groups training validation test set. segmentation accuracy measured using standard measure unary potentials learned training yielded average classiﬁcation accuracy parameters potts potentials fully connected model learned validation set. figure segmentation accuracy around object boundaries. visualization trimap measure. percent misclassiﬁed pixels within trimaps different widths. fully connected model potts potentials yielded average classiﬁcation accuracy label compatibility function learned validation increased classiﬁcation accuracy comparison grid achieves training time hours inference time seconds. qualitative results provided figure long-range connections. examined value long-range connections model varying spatial color ranges appearance kernel analyzing resulting classiﬁcation accuracy. experiment held constant results shown figure accuracy steadily increases longer-range connections added peaking spatial standard deviation pixels color standard deviation setting pairwise potential energy model assigned edges length pixels higher. however long-range connections also propagate misleading information shown figure figure inﬂuence long-range connections classiﬁcation accuracy. global classiﬁcation accuracy msrc images accurate ground truth function kernel parameters results image across slices parameter space shown black lines discussion. presented highly efﬁcient approximate inference algorithm fully connected models. results demonstrate dense pixel-level connectivity leads significantly accurate pixel-level classiﬁcation performance. single-threaded implementation processes benchmark images fraction second algorithm parallelized performance gains. acknowledgements. philipp kr¨ahenb¨uhl supported part stanford graduate fellowship. grateful daphne koller andrew adams jongmin baek helpful discussions. sergey levine vangelis kalogerakis provided comments draft paper. figure failure cases images pascal msrc- long-range connections propagated misleading information eroding bird wing left image corrupting legs right.", "year": 2012}