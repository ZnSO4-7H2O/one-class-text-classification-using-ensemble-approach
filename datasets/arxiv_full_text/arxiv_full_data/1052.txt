{"title": "Demystifying ResNet", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "The Residual Network (ResNet), proposed in He et al. (2015), utilized shortcut connections to significantly reduce the difficulty of training, which resulted in great performance boosts in terms of both training and generalization error.  It was empirically observed in He et al. (2015) that stacking more layers of residual blocks with shortcut 2 results in smaller training error, while it is not true for shortcut of length 1 or 3. We provide a theoretical explanation for the uniqueness of shortcut 2.  We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The shortcut 1, however, is essentially equivalent to no shortcuts, which has a condition number exploding to infinity as the number of layers grows. We further argue that as the number of layers tends to infinity, it suffices to only look at the loss function at the zero initial point.  Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with shortcut 2 achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process.", "text": "residual network proposed utilized shortcut connections signiﬁcantly reduce diﬃculty training resulted great performance boosts terms training generalization error. empirically observed stacking layers residual blocks shortcut results smaller training error true shortcut length provide theoretical explanation uniqueness shortcut show without nonlinearities adding shortcuts depth condition number hessian loss function zero initial point depth-invariant makes training deep models diﬃcult shallow ones. shortcuts higher depth result extremely stationary point initially optimization algorithm hard escape. shortcut however essentially equivalent shortcuts condition number exploding inﬁnity number layers grows. argue number layers tends inﬁnity suﬃces look loss function zero initial point. extensive experiments provided accompanying theoretical results. show initializing network small weights shortcut achieves signiﬁcantly better results random gaussian initialization orthogonal initialization shortcuts deeper depth various perspectives ranging ﬁnal loss learning dynamics stability behavior hessian along learning process. residual network ﬁrst proposed extended followed principled approach shortcut connections every layers vgg-style network network becomes easier train achieves lower training test errors. using structure managed train network layers virtually impossible before. unlike highway network shortcut paths also borrows idea gates lstm resnet many attempts made improve resnet extent. resnet resnet adds convolution layers data paths layer making capable representing several types residual units. resnets resnets construct multi-level shortcut connections means exist shortcuts skip multiple residual units. wide residual networks makes residual network shorter wider achieves state results several datasets using shallower network. moreover existing models also reported improved shortcut connections including inceptionv shortcut connections make deep network easier train. understanding shortcut connections resnet could help reduce training diﬃculty important question. indeed suggests layers residual networks learning residual mappings making easier represent identity mappings prevents networks degradation depths networks increase. however veit claims resnets actually ensembles shallow networks means solve problem training deep networks completely. hardt showed deep linear residual networks shortcut spurious local minimum analyzed experimented resnet architecture shortcut deeper non-bottleneck resnets also gain accuracy increased depth economical bottleneck resnets. usage bottleneck designs mainly practical considerations. note degradation problem plain nets also witnessed bottleneck designs. empirical observations inspiring. first shortcut mentioned ﬁrst paragraph work. clearly contradicts theory hardt forces conclude nonlinear network behaves essentially diﬀerent manner linear network. second noting non-bottleneck resnets shortcut bottleneck resnets shortcut sees shortcuts depth three also ease optimization diﬃculties. light empirical observations sensible reasonable theoretical explanation must able distinguish shortcut shortcuts depths clearly demonstrate shortcut special able ease optimization process signiﬁcantly deep models shortcuts depths job. moreover analyzing deep linear models able provide right intuitions. deep resnet suﬃces initialize weights zero search locally words exist global minimum whose weight functions layer vanishing norm number layers tends inﬁnity. deep resnet loss function zero initial point exhibits radically diﬀerent behavior shortcuts diﬀerent lengths. particular hessian zero initial point -shortcut network condition number growing unboundedly number layers grows -shortcut network enjoys depth-invariant condition number. resnet shortcut length larger zero initial point high order saddle point diﬃcult escape general. provide extensive experiments validating theoretical arguments. mathematically surprising although deep linear residual networks shortcut spurious local minimum result generalize nonlinear case training diﬃculty reduced. deep residual network shortcut length admits spurious local minimum general proves work practice. side product experiments reveal orthogonal initialization suboptimal. although better xavier initialization initial condition numbers networks still explode networks become deeper means networks still initialized submanifolds hard optimize using gradient descent. r-th residual unit consists layers whose weights rlr− denoted transformation path well shortcut connecting ﬁrst layer last denoted shortcut path. input-output mapping written deﬁnition n-shortcut linear network becomes n-shortcut network element-wise activation functions σpre σmid σpost added transformation paths transformation path σpre added before ﬁrst weight matrix σmid added weight matrices σpost added last weight matrix. resnet uses msra initialization kind scaled gaussian initialization tries keep variances signals along transformation path also idea behind xavier initialization however shortcut paths output variance entire network actually explode network becomes deeper. batch normalization units partly solved problem resnet still cannot prevent large output variance deep network. simple idea zero initialize weights output variances residual units stay along network. worth noting found deeper resnet smaller magnitudes layer responses. phenomenon conﬁrmed experiments. illustrated figure figure deeper residual network small average frobenius norm weight matrices training process training ends. also hardt proves weight matrices small norms linear residual network shortcut length critical points global optimum. figure average frobenius norms resnets diﬀerent depths training process. pre-resnet implementation https//github.com/ facebook/fb.resnet.torch used. learning rate initialized decreased epoch decreased epoch model trained epochs. figure average frobenius norms -shortcut networks diﬀerent depths training process zero initialized. left without nonlinearities. right relus positions. evidences indicate zero special residual network network becomes deeper training tends around thus looking hessian zero. zero saddle point experiments zero initialization small random perturbations escape ﬁrst xavier initialize weight matrices multiply small constant them. present simpliﬁed resnet structure shortcut length prove residual network becomes deeper exists solution whose weight functions vanishing norm observed resnet mentioned. argument motivated hardt seen simpliﬁed version resnet note although network -shortcut network hessian still follow form theorem thus condition number still depth-invariant. formats training samples describe common practice input data whitened labels one-hot encoded. furthermore borrow mild assumption hardt exists minimum distance every data points. indicates network become deeper exists solution closer zero. result possible zero initialized deep residual network weights initial point throughout training process condition number small making easy gradient decent optimize network. deﬁnition suppose function admits k-th order taylor expansion point point k-th order stationary point corresponding k-th order taylor expansion constant assumptions hold activation functions including tanh symmetric sigmoid relu note although relu derivatives zero local polynomial approximation hessian zero initial point -shortcut network follows block toeplitz structure well studied literature. particular condition number tends explode number layers increase compare networks xavier initialization networks orthogonal initialization -shortcut networks zero initialization. training dynamics -shortcut networks similar linear networks orthogonal initialization experiments. setup details found appendix seen figure -shortcut linear networks constant condition numbers expected. hand using xavier orthogonal initialization linear networks initial condition numbers inﬁnity depths become inﬁnity making networks hard train. also explains orthogonal initialization helpful linear network initial condition number grows slower xavier initialization. good beginning guarantee easy trip loss surface. order depict loss surfaces encountered diﬀerent initial points plot maxima percentiles absolute values hessians eigenvalues diﬀerent losses. figure maxima percentiles absolute values eigenvalues diﬀerent losses depth eigenvalues diﬀerent losses calculated using linear interpolation. shown figure condition numbers -shortcut networks different losses always smaller especially loss large. also notice condition numbers roughly evolved value orthogonal -shortcut linear networks. explained fact minimizers well point near them similar condition numbers. index important characteristic critical point. usually critical points neural network larger loss larger index experiments index -shortcut network always smaller drops dramatically beginning shown figure left. might make networks tend stop critical points. initial point near saddle point thus tends towards negative curvature directions eliminating negative eigenvalues beginning. phenomenon matches observation gradient reaches maximum index drops dramatically shown figure right. figure left ratio negative eigenvalues diﬀerent losses depth indexes diﬀerent losses calculated using linear interpolation. right dynamics gradient index -shortcut linear network single run. gradient reaches maximum index drops dramatically indicating moving toward negative curvature directions. figure left optimal ﬁnal losses diﬀerent linear networks. right corresponding optimal learning rates. depth ﬁnal losses xavier diﬀerent learning rates basically same optimal learning rate omitted unstable. figure shows results linear networks. like depth-invariant initial condition numbers ﬁnal losses -shortcut linear networks stay close optimal networks become deeper. higher learning rates also applied resulting fast learning deep networks. relus positions networks. make fair comparison numbers relu units diﬀerent networks depths same -shortcut -shortcut networks omitted. result shown figure figure left optimal ﬁnal losses diﬀerent networks relus positions. right corresponding optimal learning rates. note hard compute minimum losses relus plot instead log. depth ﬁnal losses note nonlinearities optimal losses vary diﬀerent networks diﬀerent depths. usually thought deeper networks represent complex models leading smaller optimal losses. however experiments show linear networks xavier orthogonal initialization diﬃculties ﬁnding optimal points -shortcut networks optimal points easily without nonlinear units. show eﬀect shortcut depth larger dataset modify preresnet implementation https//github.com/facebook/fb.resnet.torch make possible change shortcut depth keeping total number parameters ﬁxed. default stopping criteria used. result shown figure shown ﬁgure network becomes extremely deep resnets shortcut gain advantages growth depth networks suﬀer degradation network becomes deeper.", "year": 2016}