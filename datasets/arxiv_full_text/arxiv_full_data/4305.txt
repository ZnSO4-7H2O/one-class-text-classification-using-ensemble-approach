{"title": "Learning Stable Multilevel Dictionaries for Sparse Representations", "tag": ["cs.CV", "stat.ML"], "abstract": "Sparse representations using learned dictionaries are being increasingly used with success in several data processing and machine learning applications. The availability of abundant training data necessitates the development of efficient, robust and provably good dictionary learning algorithms. Algorithmic stability and generalization are desirable characteristics for dictionary learning algorithms that aim to build global dictionaries which can efficiently model any test data similar to the training samples. In this paper, we propose an algorithm to learn dictionaries for sparse representations from large scale data, and prove that the proposed learning algorithm is stable and generalizable asymptotically. The algorithm employs a 1-D subspace clustering procedure, the K-hyperline clustering, in order to learn a hierarchical dictionary with multiple levels. We also propose an information-theoretic scheme to estimate the number of atoms needed in each level of learning and develop an ensemble approach to learn robust dictionaries. Using the proposed dictionaries, the sparse code for novel test data can be computed using a low-complexity pursuit procedure. We demonstrate the stability and generalization characteristics of the proposed algorithm using simulations. We also evaluate the utility of the multilevel dictionaries in compressed recovery and subspace learning applications.", "text": "error goal representation. norm denoted used instead measure convexify variety methods found literature obtain sparse representations efﬁciently sparse coding model successfully used inverse problems images also machine learning applications classiﬁcation clustering subspace learning name dictionary used obtained predeﬁned bases designed union orthonormal bases structured overcomplete individual vectors optimized data wide range batch online dictionary learning algorithms proposed literature tailored speciﬁc tasks. conditions dictionary identiﬁed training data using minimization approach derived joint optimization problem dictionary learning sparse coding expressed paper propose hierarchical multilevel dictionary learning algorithm implicitly regularized sparse approximation data. proposed multilevel dictionary learning algorithm geared towards obtaining global dictionaries entire probability space data provably stable generalizable novel test data. addition algorithm involves simple schemes learning representation subspace clustering algorithm used infer atoms level −sparse representations obtained level using pursuit scheme employs correlate-and-max operations. summary algorithm creates sub-dictionary level obtains residual used training data next level process continued pre-deﬁned stopping criterion reached. primary utility sparse models learned dictionaries data processing machine learning applications stems fact dictionary atoms serve predictive features capable providing good representation aspect test data. viewpoint statistical abstract—sparse representations using learned dictionaries increasingly used success several data processing machine learning applications. availability abundant training data necessitates development efﬁcient robust provably good dictionary learning algorithms. algorithmic stability generalization desirable characteristics dictionary learning algorithms build global dictionaries efﬁciently model test data similar training samples. paper propose algorithm learn dictionaries sparse representations large scale data prove proposed learning algorithm stable generalizable asymptotically. algorithm employs subspace clustering procedure k-hyperline clustering order learn hierarchical dictionary multiple levels. also propose information-theoretic scheme estimate number atoms needed level learning develop ensemble approach learn robust dictionaries. using proposed dictionaries sparse code novel test data computed using low-complexity pursuit procedure. demonstrate stability generalization characteristics proposed algorithm using simulations. also evaluate utility multilevel dictionaries compressed recovery subspace learning applications. energy concentrated small number features represented using linear model. particular shown statistical structure naturally occurring signals images allows efﬁcient representation sparse linear combination elementary features ﬁnite collection normalized features referred dictionary. linear model used general sparse coding given data vector rm×k dictionary. column dictionary referred atom representative pattern normalized unit norm. sparse coefﬁcient vector noise vector whose elements independent realizations gaussian distribution learning theory good predictive model stable generalizable learning satisﬁes properties. best knowledge dictionary learning method proven satisfy properties. generalization ensures learned dictionary successfully represent test data drawn probability space training data stability guarantees possible reliably learn dictionary arbitrary training set. words asymptotic stability generalization provides theoretical justiﬁcation uniformly good performance global multilevel dictionaries. minimize risk overﬁtting choosing proper model order. propose method based minimum description length principle choose optimal model order case corresponds number dictionary elements level. recently approaches proposed choose best order given sparse model using generalization error minimized. however difference case that addition optimizing model order given training using prove dictionary learned using generalizable stable. since generalization stability asymptotic properties also propose robust variant algorithm using randomized ensemble methods obtain improved performance test data. note goal obtain dictionaries optimized speciﬁc task propose general predictive sparse modeling framework suitably adapted task. dictionary atoms structurally regularized therefore hierarchy representation imposed implicitly novel test data leading improved recovery ill-posed noise-corrupted problems. considering dictionary learning image patches example predominant atoms ﬁrst levels always contribute highest energy representation. natural image data known patches comprised geometric patterns stochastic textures combination since geometric patterns usually higher energy compared stochastic textures images learns geometric patterns ﬁrst levels stochastic textures last levels thereby adhering natural hierarchy image data. hierarchical multistage vector quantization related learning. important difference however dictionaries obtained sparse representations must assume data lies union-of-subspaces incorporate assumption. note multilevel learning also different work multiple sub-dictionaries designed chosen representing group patches. learning algorithm space training examples hypothesis space functional solutions. clustering learned function completely characterized cluster centers. stability clustering algorithm implies cluster centroids learned algorithm signiﬁcantly different different sets i.i.d. samples probability space used training unique minimizer clustering objective respect underlying data distribution stability clustering algorithm guaranteed analysis extended characterize stability k-means clustering terms number minimizers stability properties k-hyperline clustering algorithm analyzed shown similar k-means clustering. note stability characterizations depend underlying data distribution number clusters actual training data itself. generalization implies average empirical training error becomes asymptotically close expected error respect probability space data. generalization bound sparse coding terms number samples also referred sample complexity derived bound improved assuming class dictionaries nearly orthogonal. clustering algorithms k-means k-hyperline obtained constraining desired sparsity since stability characteristics clustering algorithms well understood employing similar tools analyze general dictionary learning framework beneﬁcial. paper propose learning algorithm design global representative dictionaries image patches. show that sufﬁcient number levels proposed algorithm converges also demonstrate multilevel dictionary sufﬁcient number atoms level exhibits energy hierarchy furthermore order estimate number atoms level provide information-theoretic approach based principle order compute sparse codes test data using proposed dictionary develop simple multilevel pursuit procedure quantify computational complexity also propose method obtain robust dictionaries limited training data using ensemble methods preliminary algorithmic details results obtained using reported using fact k-hyperline clustering algorithm stable perform stability analysis algorithm. sets i.i.d. training samples probability space number training samples show dictionaries learned become close asymptotically. unique minimizer objective level learning holds true even training sets completely disjoint. however multiple minimizers objective least level prove learned dictionaries asymptotically close difference corresponding training instability algorithm difference sets also shown case multiple minimizers furthermore prove asymptotic generalization learning algorithm analyzing stability unsupervised clustering algorithms valuable terms understanding behavior respect perturbations training set. algorithms extract underlying structure training data quality clustering determined accompanying cost function. result clustering algorithm posed empirical risk minimization procedure deﬁning hypothesis class loss functions evaluate possible cluster conﬁgurations measure quality example k-hyperline clustering posed problem distortion function class class constructed functions corresponding possible combinations unit length vectors space deﬁne probability space data sample space sigma-algebra i.e. collection subsets probability measure deﬁned. training samples {yi}t ideally interested computing cluster centroids minimize expected distortion respect probability measure however underlying distribution data samples known hence resort minimizing average empirical distortion respect training samples {yi}t refer class uniform glivenkocantelli addition class also satisﬁes version central limit theorem deﬁned uniform donsker order determine uniform donsker verify covering number respect supremum norm grows polynomially dimensions here denotes maximum distance arbitrary distortion function function covers k-hyperline clustering covering number upper bounded addition demonstrating stability generalization behavior learning image data evaluate performance compressed recovery images theoretical guarantees proposed effectively recovers novel test images severe degradation interestingly proposed greedy pursuit robust multilevel dictionaries results improved recovery performance compared minimization online dictionaries particularly reduced number measurements presence noise. furthermore perform subspace learning graphs constructed using sparse codes evaluate performance classiﬁcation show proposed approach outperforms subspace learning neighborhood graphs well graphs based sparse codes conventional dictionaries. section describe k-hyperline clustering subspace clustering procedure proposed forms building block proposed dictionary learning algorithm. furthermore brieﬂy discuss results stability analysis k-means k-hyperline algorithms reported respectively. ideas described section used section study stability characteristics proposed dictionary learning procedure. k-hyperline clustering algorithm iterative procedure performs least squares linear subspaces training data note k-hyperline clustering special case general subspace clustering methods proposed subspaces −dimensional constrained pass origin. contrast kmeans k-hyperline clustering allows data sample arbitrary coefﬁcient value corresponding centroid cluster belongs furthermore cluster centroids normalized unit norm. given data samples {yi}t number clusters khyperline clustering proceeds stages initialization cluster assignment cluster centroid update. cluster assignment stage training vector assigned cluster based minimum distortion criteria argminj distortion measure cluster centroid update stage perform singular value decomposition i∈cj {i|h contains indices training vectors assigned cluster cluster centroid updated left singular vector corresponding largest singular value decomposition. also computed using linear iterative procedure. iteration cluster centroid given residuals levels respectively matrix training image patches. implies residual matrix level serves training data level note sparsity representation level ﬁxed hence overall approximation levels learning interpreted block-based dictionary learning problem unit sparsity block sub-dictionary block allow -sparse representation block corresponds level. sub-dictionary level cluster centroids learned training matrix level using k-hyperline clustering. learning formally stated optimization problem proceeds ﬁrst level stopping criteria reached. level solve stability characterized based number minimizers clustering objective respect underlying data distribution. minimizer corresponds function minimum expectation stability analysis k-means clustering reported though geometry k-hyperline clustering different k-means stability characteristics algorithms found similar given sets cluster centroids learned training sets i.i.d. samples realized probability space deﬁne distance clusterings denotes convergence probability. holds true even learned completely disjoint training sets unique minimizer clustering objective. multiple minimizers holds samples true respect change samples distance cluster centroids deﬁned training data assumed outside mdimensional ball radius centered origin constant ˆccm depends clustering algorithm stable according admissible values lemma shows cluster centroids become arbitrarily close other. section develop multilevel dictionary learning algorithm whose algorithmic stability generalizability proved section furthermore propose strategies estimate number atoms level make learning process robust improved generalization. also present simple pursuit scheme compute representations novel test data using mld. number atoms level optimally estimated using information theoretic criteria minimum description length broad idea model order number dictionary atoms here chosen minimize total description length needed representing model data given model. codelength encoding data given model given negative likelihood description length model number bits needed code model parameters. order estimate number atoms level using principle need make assumptions residual obtained level. ﬁrst assumption fraction total energy level represented level remaining energy residual energy. residual representation energy total energy level because residual level orthogonal representation level. therefore level represented energy αl−e residual energy total energy training data ﬁrst level. simplicity also assume residual level follows zero-mean multinormal distribution combining assumptions variance estimated information-theoretic complexity negative likelihood number bits needed encode model. encoding model includes encoding nonzero coefﬁcients location dictionary elements themselves. score level data dictionary rm×kl coefﬁcient matrix here ﬁrst term represents data description length also negative log-likelihood data ignoring constant term. second term number bits needed code non-zero coefﬁcients reals coefﬁcient coded using bits third term denotes bits needed code locations integers fourth term represents total bits needed code dictionary elements reals. optimal model order number dictionary atoms results least number columns adopt notation denote problem number atoms stopping criteria provided either imposing limit residual representation error maximum number levels note total number levels maximum number non-zero coefﬁcients representation. error constraint stated column error goal. table lists learning algorithm ﬁxed notation denote element contains indices residual vectors level whose norm greater error goal. residual vectors indexed stacked matrix turn serves training matrix next level learning given level residual orthogonal representation ψlali. implies training vectors ﬁrst level algorithm ambient space residuals ﬁnite union subspaces. because dictionary atom ﬁrst level residual lies dimensional space orthogonal second level dictionary atoms possibly anywhere hence residuals ﬁnite union dimensional subspaces. hence generalize dictionary atoms levels whereas training vectors level ﬁnite unions rm−l+ dimensional subspaces space. convergence learning energy hierarchy representation obtained using shown providing guarantees. ﬁrst guarantee ﬁxed number atoms level algorithm converge required error within sufﬁcient number levels. k-hyperline clustering makes residual energy representation smaller energy training matrix level follows fact ψlal second guarantee sufﬁcient number atoms level representation energy level less representation energy level show this ﬁrst state sufﬁcient number dictionary fig. levels last level dictionary number atoms estimated using procedure. comprises geometric patterns ﬁrst levels stochastic textures last level. since level different number atoms sub-dictionary padded zero vectors appear black patches. score. practice test ﬁnite number model orders pick results least score. example train dictionary using grayscale patches size bsds dataset preprocess patches vectorizing subtracting mean vectorized patch elements. perform learning estimate estimate optimal number dictionary atoms level using maximum levels. sub-dictionary level number atoms varied provided least score chosen optimal. ﬁrst levels last level obtained using procedure shown figure minimum score obtained level shown ﬁgures clearly informationtheoretic complexity sub-dictionaries increase number levels atoms progress simple geometric structures stochastic textures. order compute sparse codes novel test data using multilevel dictionary propose perform reconstruction using multilevel pursuit procedure evaluates sparse representation level using dictionary atoms level. therefore coefﬁcient vector data sample level obtained using simple correlate-andmax operation whereby compute correlations pick coefﬁcient value index corresponding maximum absolute correlation. computational complexity correlate-and-max operation order hence complexity obtaining full representation using total number atoms dictionary. whereas complexity obtaining sparse representation full dictionary using orthogonal matching pursuit order although learning simple procedure capable handling large scale data useful asymptotic generalization properties described section procedure made robust generalization performance improved using randomization schemes. robust learning scheme closely related rvotes supervised ensemble learning method improves generalization performance evidenced figure rvotes scheme randomly samples training create sets samples each ﬁnal prediction obtained averaging predictions multiple hypotheses learned training sets. learning level rmld draw subsets randomly chosen training samples original training size allowing overlap across subsets. note here rl−. superscript denotes index size subset. subset learn sub-dictionary atoms using k-hyperline clustering. training sample compute −sparse representations using sub-dictionaries denote coefﬁcient matrices approximation training sample level computed average approximations using subdictionaries ensemble approximations training samples level used compute residuals process repeated desired number levels obtain rmld. reconstruction test data rmld performed extending multilevel pursuit. obtain approximations data sample given level average approximations compute residual repeat subsequent levels. note also implemented multiple correlate-and-max operations data sample level. clearly computational complexity obtaining sparse representation using rmld order section behavior proposed dictionary learning algorithm considered viewpoint algorithmic stability behavior algorithm respect perturbations training set. shown dictionary atoms learned algorithm different training sets whose samples realized probability space become arbitrarily close other number training samples since proposed learning equivalent learning k-hyperline cluster centroids multiple levels stability analysis distance cluster centers stable clustering cluster center clustering pick closest cluster center terms distortion measure form pairs. indicate pair cluster centers λlj. deﬁne disjoint sets {ai}τ training data clusterings exist deﬁning disjoint sets formalize notion training data lying union subspaces intuitive fact cluster centers clusterings close other given distortion functions close proved lemma below. lemma consider sub-dictionaries atoms obtained using training samples exist disjoint sets {ai}τ space dpl/dy sets. distortion functions become arbitrarily close other gλll smallest angle subspaces spanned cluster centers becomes arbitrarily close zero i.e. proof denote smallest angle subspaces represented deﬁne region {y|∠ ρlj/ illustration setup case given figure ﬁgure radius represents minimum value deﬁnition distance distortion functions clusterings data exists disjoint sets {ai}τ k-hyperline clustering brieﬂy discussed section ii-b utilized order prove stability. level learning cases single multiple minimizers clustering objective considered. proving learning algorithm stable show global dictionaries learned data depend probability space training samples belong actual samples themselves also show learning generalizes asymptotically i.e. difference expected error average empirical error training approaches zero therefore expected error novel test data drawn distribution training data approach average empirical training error. stability analysis algorithm performed considering different dictionaries levels each. level consists dictionary atoms sub-dictionaries level indicated respectively. sub-dictionaries cluster centers learned using k-hyperline clustering training data level steps involved proving overall stability algorithm showing level algorithm stable terms distance distortion functions deﬁned number training samples proving stability terms distances indicates closeness centers clusterings terms metric deﬁned showing level-wise stability leads overall stability dictionary learning algorithm deﬁne probability space data lies probability measure. training samples sub-dictionaries different sets i.i.d. realizations probability space. also assume norm training samples bounded note that general case data ﬁrst level dictionary learning ﬁnite union lower-dimensional subspaces subsequent levels. cases following argument stability hold. training data lies union lower dimensional subspaces assume still lying assign probabilities outside union subspaces zero. distortion function class clusterings deﬁned similar uniform donsker covering number respect supremum norm grows polynomially according unique minimizer exists clustering objective distortion functions corresponding different clusterings become arbitrarily close gλll even completely disjoint training sets however case multiple minimizers gλll holds respect change training samples clusterings obtained linear transformation dψlj probability measures deﬁned training data levels respectively. similarly pl+}). cluster center pairs become arbitrarily close other i.e. assumption. therefore symmetric difference sets dψlj dλlj approaches null implies implies arbitrary means residuals belong unique identical probability space. since proved arbitrary residuals clusterings belong identical probability space given proof level-wise stability shown section iv-a cases unique minimizer exists distortion function unique minimizer exist. lemma proved stability terms closeness distortion functions implied stability terms learned cluster centers. showing level-wise stability assumed training vectors level clusterings belonged probability space. however learning dictionary true ﬁrst level supply algorithm training vectors probability space note training vectors level residuals clusterings lemma showed residuals level clusterings belong identical probability space given training vectors level realizations probability denoting smallest angles subspaces spanned θψlj θλlj respectively deﬁnition region blij θψlj θλlj since bounded away zero inﬁnity holds blij true cluster center pairs shown arbitrary stability algorithm whole proved theorem level-wise stability using induction argument. proof depend following lemma shows residuals stable clusterings belong probability space. subdictionaries obtained probability space cluster center pairs become arbitrarily close residual vectors clusterings belong identical probability space proof cluster center pair deﬁne ¯λlj projection matrices respective orthogonal complement subspaces deﬁne sets dψlj ¯ψlj ψljα} dλlj ¯λlj λljα} arbitrary ﬁxed vector orthogonal differential element. residual vector cluster dψlj given rψlj ¯ψljy|y dψlj} equivalently rψlj ¯ψlj}. similarly cluster rλlj ¯λlj}. case figure shows subspace orthogonal complement dψlj residual ¯ψlj}. terms probabilities also pl+}) residual mulp coding scheme used test data training test data level obtained probability space probability space training test data level because mulp coding scheme learning associate data dictionary atom using maximum absolute correlation measure create residual orthogonal atom chosen level. hence assumption training test data drawn probability space levels hold expected test error similar average empirical training error. section present experiments demonstrate stability generalization characteristics multilevel dictionary evaluate compressed recovery images subspace learning. stability generalization crucial building effective global dictionaries model patterns novel test image. although possible demonstrate asymptotic behavior experimentally study changes behavior learning algorithm increase number samples used training. compressed recovery highly relevant application global dictionaries since possible infer dictionaries good reconstructive power directly low-dimensional random measurements image patches. typical employ minimization greedy pursuit methods recovering images compressed measurements. though minimization incurs higher computational complexity often provides improved recovery performance compared greedy approaches. hence compare recovery performance uses simple fig. demonstration stability behavior proposed learning algorithm. minimum frobenius norm difference dictionaries respect permutation columns signs shown. second dictionary obtained replacing different number samples training used training original dictionary data samples. space induction along fact training vectors level belong probability space shows training vectors dictionaries level indeed belong probability space corresponding level. hence levels dictionary learning stable learning stable whole. similar k-hyperline clustering multiple minimizers least level algorithm stable respect change training samples clusterings failts hold change holds learning algorithm generalizes seen expected error test data drawn probability space training data close average empirical error. therefore cluster centers level obtained minimizing empirical error expected test error also small. order show holds fact level learning obtained using k-hyperline clustering. hence average empirical distortion level converges expected distortion greedy pursuit. subspace learning another application beneﬁt multilevel dictionaries. subspace learning common obtain linear embedding training data apply novel test data dimensionality reduction classiﬁcation visualization. approaches unsupervised class label information learning embedding several subspace learning algorithms uniﬁed framework graph embedding wherein undirected graph describing relation data samples provided input. propose graphs constructed based sparse codes multilevel dictionary subspace learning supervised unsupervised settings. simulations stability/generalization compressed recovery dictionaries trained image patches berkeley segmentation dataset bsds dataset contains total images number patches used experiments vary images converted grayscale preprocessing performed images. used patches size noise added patches. evaluating performance dictionaries considered standard images subspace learning simulations used forest covertype dataset consists samples belonging different classes. standard procedure used ﬁrst samples training rest testing. order illustrate stability characteristics learning setup experiment consider multilevel dictionary levels atoms level. trained multilevel dictionaries using different number training patches showed section asymptotic stability guaranteed training changed samples. inferred dictionary atoms vary signiﬁcantly condition satisﬁed. ﬁxed size training different values fig. demonstration generalization characteristics proposed rmld algorithms. plot obtained representing patches test dataset using dictionaries learned different number training patches. learned initial dictionaries using proposed algorithm. second dictionaries obtained replacing different number samples original training set. case number replaced samples varied example number replaced training samples amount change initial second dictionaries quantiﬁed using minimum frobenius norm difference respect permutations columns sign changes. figure plot quantity different values function number samples replaced training set. case difference dictionaries increases increase replaced number training samples. furthermore ﬁxed number replaced samples difference reduces increase number training samples since becomes closer asymptotic behavior. generalization dictionary learning algorithm guarantees small approximation error test data sample training samples well approximated dictionary. order demonstrate generalization characteristics learning designed dictionaries using different number training image patches size evaluated sparse approximation error patches test dataset. test dataset consisted patches chosen randomly standard images. multilevel learning ﬁxed number levels used approach proposed section iii-c estimate number atoms needed level similarly ﬁxed number levels rmld learning. since rmld learning require careful choice number atoms level ﬁxed though learning multiple sets atoms level lead improved generalization beneﬁt seems level certain number rounds. example consider case vary number rounds rmld described section iii-e increasing number rounds results higher computational complexity evaluating sparse codes. figure illustrates training data test data obtained using rmld different number rounds level. since vary signiﬁcantly beyond rounds ﬁxed reconstruction experiments. figure compares approximation error obtained test dataset rmld respectively case ﬁgure plots number levels used reconstruction algorithm. figure shows approximation error test image patches obtained rmld dictionaries learned using different number training samples since proved section iv-d learning generalizes asymptotically expect approximation error test data reduce increase size training set. ﬁgures clear rmld scheme results improved approximation novel test patches compared mld. compressed recovery image recovered using low-dimensional random projections obtained patches. performance compressed recovery based random measurement systems compared rmld online dictionaries. case online dictionaries learned using algorithm described report results obtained using minimization algorithm. sensing recovery performed patch-by-patch basis non-overlapping patches size multilevel dictionaries obtained parameters described previous section using training samples. online dictionary trained using training number atoms ﬁxed measurement process described dictionary measurement projection matrix awgn vector added measurement process output measurement process sparse coefﬁcient vector size data vector measurement matrix measured vector entries random measurement matrix independent realizations standard normal distribution. recover underlying image compressed measurements using online rmld dictionaries. case present average results trial runs time different measurement matrix. recovery performance evaluated standard images reported table figure illustrates recovered images obtained using different dictionaries random measurements noise observed mulp reconstruction using proposed dictionary resulted improved recovery performance different measurement conditions compared using greedy pursuit online dictionary. however mulp reconstruction rmld -based reconstruction online dictionary perform signiﬁcantly section evaluate sparse codes obtained multilevel dictionaries unsupervised supervised subspace learning. particular locality preserving projections local discriminant embedding approaches perform classiﬁcation forest covertype dataset. unsupervised embedding approach computes projection directions pairwise distances projected training samples neighborhood preserved deﬁne training data {yi|yi rm}t undirected graph deﬁned training samples vertices similarity neighboring training samples coded afﬁnity matrix rt×t proposed setup learn dictionary using training samples compute afﬁnity matrix matrix sparse coefﬁcients. following this sparsify retaining largest similarities sample. note construction different graph construction computationally efﬁcient. denote graph laplacian degree matrix diagonal element containing corresponding column projection directions rm×d computed optimizing matrix obtained stacking data samples columns. embedding data sample obtained supervised setting deﬁne intra-class inter-class afﬁnity matrices respectively label training sample sets contain indices intraclass inter-class neighbors training sample. neighbors sample sorted based order decreasing absolute correlations sparse code using afﬁnity matrices local discriminant embedding performed solving subspace learning approaches varied number training samples class ﬁxed embedding dimension rmld learning ﬁxed number levels number rounds rmld ﬁxed comparison learned iterative dictionaries size using minimization spams toolbox lagrangian dual method finally classiﬁcation performed using simple −nearest neighbor classiﬁer. table table show classiﬁcation accuracies obtained using different dictionaries subspace learning approaches. observed graphs constructed proposed multilevel dictionaries provide discriminative embeddings compared approaches. learning algorithm design generalizable stable global dictionaries sparse representations. proposed algorithm uses multiple levels subspace clustering learn dictionaries. also proposed method infer number atoms level provided ensemble learning approach create robust dictionaries. proved learning algorithm converges exhibits energy hierarchy also generalizable stable. finally demonstrated superior performance applications compressive sensing subspace learning. future research could include providing online framework work streaming data also developing hierarchical dictionaries optimized robust penalties reconstruction error. efron hastie johnstone tibshirani least angle regression annals statistics vol. aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation ieee vol. november mairal bach ponce sapiro online learning matrix factorization sparse coding jmlr vol. jenatton mairal obozinski bach proximal methods sparse hierarchical dictionary learning icml frankranz joachims eds. omnipress caponnetto rakhlin stability properties empirical risk minimization donsker classes jmlr vol. saito simultaneous noise suppression signal compression using library orthonormal bases minimum description length criterion wavelets geophysics vol. available", "year": 2013}