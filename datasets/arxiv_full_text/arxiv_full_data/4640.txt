{"title": "Mean-Variance Optimization in Markov Decision Processes", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.", "text": "consider ﬁnite horizon markov decision processes performance measures involve mean variance cumulative reward. show either randomized history-based policies improve performance. prove complexity computing policy maximizes mean reward variance constraint np-hard cases strongly np-hard others. ﬁnally offer pseudopolynomial exact approximation algorithms. papadimitriou yannakakis approximability trade-offs optimal access sources. proceedings symposium foundations computer science washington usa. riedel dynamic coherent risk measures. stoch. proc. appl. shapley stochastic games. proc. national academy science math. sobel variance discounted markov decision processes. journal applied probability", "year": 2011}