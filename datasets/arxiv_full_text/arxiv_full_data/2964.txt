{"title": "Self-critical Sequence Training for Image Captioning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a \"baseline\" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.", "text": "recently shown policy-gradient methods reinforcement learning utilized train deep endto-end systems directly non-differentiable metrics task hand. paper consider problem optimizing image captioning systems using reinforcement learning show carefully optimizing systems using test metrics mscoco task signiﬁcant gains performance realized. systems built using optimization approach call self-critical sequence training scst form popular reinforce algorithm that rather estimating baseline normalize rewards reduce variance utilizes output test-time inference algorithm normalize rewards experiences. using approach estimating reward signal estimating normalization avoided time harmonizing model respect test-time inference procedure. empirically directly optimizing cider metric scst greedy decoding test-time highly effective. results mscoco evaluation sever establish state-of-the-art task improving best result terms cider image captioning aims generating natural language description image. open domain captioning challenging task requires ﬁne-grained understanding global local entities image well attributes relationships. recently released mscoco challenge provides larger scale platform evaluating image captioning systems complete evaluation server benchmarking competing methods. deep learning approaches sequence modeling yielded impressive results task dominating task leaderboard. inspired recently introduced encoder/decoder paradigm machine translation using recurrent neural networks deep convolutional neural network encode input image long short term memory decoder generate output caption. systems trained end-to-end using back-propagation achieved state-of-the-art results mscoco. recently spatial attention mechanisms layers incorporate visual context—which implicitly conditions text generated far—was incorporated generation process. shown qualitatively observed captioning systems utilize attention mechanisms lead better generalization models compose novel text descriptions based recognition global local entities comprise images. discussed deep generative models text typically trained maximize likelihood next ground-truth word given previous ground-truth word using back-propagation. approach called teacher-forcing however approach creates mismatch training testing since test-time model uses previously generated words model distribution predict next word. exposure bias results error accumulation generation test time since model never exposed predictions. several approaches overcoming exposure bias problem described recently proposed. show feeding back model’s predictions slowly increasing feedback probability training leads signiﬁcantly better test-time performance. another line work proposes professor-forcing technique uses adversarial training encourage dynamics recurrent network training conditioned ground truth previous words sampling freely network. models. similarly ﬁrst encode input image using deep embed linear projection words represented vectors embedded linear embedding output dimension beginning sentence marked special token token. model words generated back lstm image treated ﬁrst word following updates hidden units cells lstm deﬁne model architecture hidden states word image embeddings dimension denote parameters model. traditionally parameters learned maximizing likelihood observed sequence. speciﬁcally given target ground truth sequence objective minimize cross entropy loss attention model rather utilizing static spatially pooled representation image attention models dynamically re-weight input spatial features focus speciﬁc regions image time step. paper consider modiﬁcation architecture attention model captioning given input attention-derived image feature cell node lstm. cross entropy loss typically evaluated test time using discrete non-differentiable metrics bleu rouge meteor cider ideally sequence models image captioning trained avoid exposure bias directly optimize metrics task hand. recently shown exposure bias non-differentiable task metric issues addressed incorporating techniques reinforcement learning speciﬁcally ranzato reinforce algorithm directly optimize nondifferentiable sequence-based test metrics overcome issues. reinforce describe allows optimize gradient expected reward sampling model training treating samples ground-truth labels major limitation approach expected gradient computed using mini-batches reinforce typically exhibit high variance without proper context-dependent normalization typically unstable. recent discovery reinforce proper bias correction using learned baselines effective ﬂurry work applying reinforce problems supervised learning variational inference actor-critic methods instead train second critic network provide estimate value generated word given policy actor network also investigated sequence problems recently techniques overcome need sample policy’s action space enormous expense estimating future rewards training multiple networks based another’s outputs explore also unstable. paper present approach sequence training call self-critical sequence training demonstrate scst improve performance image captioning systems dramatically. scst reinforce algorithm that rather estimating reward signal reward signal normalized utilizes output test-time inference algorithm normalize rewards experiences. result samples model outperform current test-time system given positive weight inferior samples suppressed. using scst attempting estimate reward signal actor-critic methods must estimating normalization reinforce algorithms must avoided time harmonizing model respect test-time inference procedure. empirically directly optimizing cider metric scst greedy decoding test-time highly effective. results mscoco evaluation sever establish state-of-the-art task improving best result terms cider policy gradient reinforce. order compute gradient reinforce algorithm reinforce based observation expected gradient nondifferentiable reward function computed follows shows baseline change expected gradient importantly reduce variance gradient estimate. training case approximate expected gradient single sample attention-derived image feature. feature derived follows given features softmax tanh. work dimension zero. denote parameters model. deﬁned parameters attention models also traditionally learned optimizing loss attention model standard attention model presented also feeds attention signal input gates lstm output posterior. experiments feeding gates addition input boost performance feeding gates outputs resulted signiﬁcant gains adam used. reinforcement learning sequence generation problem. described previous section captioning systems traditionally trained using cross entropy loss. directly optimize metrics address exposure bias issue cast generative models reinforcement learning terminology recurrent models introduced viewed agent interacts external environment parameters network deﬁne policy results action prediction next word. action agent updates internal state upon generating end-of-sequence token agent observes reward instance cider score generated sentence—we denote reward reward computed evaluation metric comparing generated sequence corresponding ground-truth sequences. goal training minimize negative expected reward central idea self-critical sequence training approach baseline reinforce algorithm reward obtained current model inference algorithm used test time. gradient negative reward sample model w.r.t. softmax activations time-step becomes reward obtained current model inference algorithm used test time. accordingly samples model return higher reward pushed increased probability samples result lower reward suppressed. like mixer scst advantages reinforce algorithms directly optimizes true sequence-level evaluation metric avoids usual scenario learn estimate expected future rewards baseline. practice found scst much lower variance effectively trained mini-batches samples using sgd. since scst baseline based test-time estimate current model scst forced improve performance model inference algorithm used test time. encourages training/test time consistency like maximum likelihood-based approaches data demonstrator professor forcing importantly directly optimize sequence metrics. finally scst self-critical avoids inherent training difﬁculties associated actor-critic methods second critic network must trained estimate value functions actor must trained estimated value functions rather actual rewards. choice depicted figure minimizes impact baselining test-time inference algorithm training time since requires additional forward pass trains system optimized fast greedy decoding test-time. generalizations. basic scst approach described generalized several ways. generalization condition baseline generated makes baseline word-dependent reduces variance reward signal making dependent future rewards. achieved baselining reward word timestep reward obtained word sequence ˆwtt} generated sampling tokens timesteps executing inference algorithm complete sequence. resulting reward signal baselined future reward signal conditions input image sequence remains unbiased. call variant time-dependent scst another important generalization utilize inference algorithm critic replace learned critic traditional actor-critic approaches. like traditional actorcritic methods biases learning procedure used trade variance bias. speciﬁcally primary reward signal time based sequence samples future tokens executes inference algorithm complete sequence. primary reward based ˆwt+n+t} baselined time-dependent manner using td-scst. resulting reward signal case call variant true scst. experimented td-scst true scst described mscoco task found lead signiﬁcant additional gain. also experimented learning control-variate scst baseline mscoco avail. nevertheless anticipate generalizations important sequence modeling tasks policy-gradient-based generally. experiments dataset. evaluate proposed method mscoco dataset ofﬂine evaluation purposes used data splits training contains images along captions each. image validation report results test images well given report four widely used automatic evaluation metrics bleu- rougel meteor cider. prune vocabulary drop word count less vocabulary size words. image features models. type features features encode image resnet- note rescale crop image. instead encode full image ﬁnal convolutional layer resnet apply average pooling results vector dimension features stack average pooled layers resnet- layers layers conv conv exception layer conv omitted. results feature vector dimension spatial features attention models encode image using residual convolutional neural network resnet- note rescale crop image. instead encode full figure self-critical sequence training weight words sampled sentence model determined difference reward sampled sentence reward obtained estimated sentence test-time inference procedure harmonizes learning inference procedure lowers variance gradients improving training procedure. image ﬁnal convolutional layer resnet- apply spatially adaptive max-pooling output ﬁxed size time step attention model produces attention mask spatial locations. mask applied result spatially averaged produce dimension representation attended portion image. criterion eventually subsumed). since realized mscoco task required provides little boost performance. results reported fc-k fc-k models trained attention models trained directly entire sentence epochs initialized seed models. implementation details. lstm hidden image word attention embeddings dimension ﬁxed models discussed herein. models trained according following recipe except otherwise noted. initialize models training model objective using adam optimizer initial learning rate anneal learning rate factor every three epochs increase probability feeding back sample word posterior every epochs reach feedback probability evaluate epoch model development select model best cider score initialization scst training. scst training initialized model optimize cider metric using adam fc-k fc-k models utilized curriculum learning training proposed increasing number words sampled trained cider epoch versus mixer mixer without baseline test portion karpathy splits trained optimize cider metric improve seed cross-entropy trained model scst outperforms mixer. ofﬂine evaluation evaluating different training strategies. table compares performance scst mixer experiment utilize curriculum learning optimizing expected reward metric last words training sentence optimizing remaining sentence preﬁx table mean/std. performance scst versus reinforce reinforce learned baseline attall models karpathy test one-sample t-test gain scst mixer less rejects null hypothesis metrics except rouge slowly increasing results reported generated optimized schedule reported found necessary train scst reinforce learned baseline mscoco—turning sped training yielded equal better results. gain scst learned baselines consistent regardless schedule initial seed. figures table compare performance scst mixer less attall models karpathy validation test splits respectively. figure compare gradient variance word posterior entropy karpathy training set. techniques unbiased scst general lower gradient variance translates improved training performance. interestingly scst much higher gradient variance mixer less ﬁrst epoch training sampled sentences initially score signiﬁcantly lower sentence produced test-time inference algorithm. experimented training directly evaluation metrics mscoco challenge. results fc-k models depicted table general optimizing given metric training leads best performance metric test time expected result. experimented training multiple test metrics found unable outperform overall performance model trained cider metric lifts performance metrics considerably. reason experimentation since focused optimizing cider. single fc-models versus attention models. trained models well attention models using scst cider metric. trained different models attention type starting optimization four different random seeds report table system best performance family models test portion karpathy splits fc-k models outperform fc-k models. models outperformed attention models establish state single model performance karpathy splits. note quantitative evaluation favors attention models inline observation attention models tend generalize better compose outside context training mscoco section model ensembling. section investigate performance ensembling random seeds scst-trained models attention models. table ensembling improves performance conﬁrms supremacy attention modeling establishes another state result karpathy splits note case ensemble models table performance test portion karpathy splits function training metric optimizing cider metric increases overall performance evaluation metrics signiﬁcantly. performance seed cross-entropy model also depicted. models decoded greedily exception beam search result optimized beam validation set. table reports performance variants ensembled attention models trained self-critical sequence training ofﬁcial mscoco evaluation server. previous best result leaderboard also depicted. outperform previous best system evaluation metrics. example generated captions provide qualitative example captions generated systems image ﬁgure picture taken objects out-of-context dataset images depicts boat situated unusual context tests ability models compose descriptions images differ seen training. captions returned scsttrained fc-k fc-k attention model ensembles deployed decoding beam depicted ﬁgure image models fail completely scst-trained ensemble attention models system able correctly describe image. general found performance captioning systems mscoco data qualitatively similar images containing objects situated uncommon context attention models perform much better scst-trained attention models output accurate descriptive captions. general qualitatively found scst-trained attention models describe images accurately higher table performance best corr. scst-trained single models karpathy test split results obtained greedy decoding optimized beam search depicted. models learned using scst trained directly optimize cider metric. mixer less results also included. table performance ensembled scst-trained models karpathy test split models learned using self-critical sequence training trained optimize cider metric. mixer less results also included. conﬁdence reﬂected figure average log-likelihoods words generated caption also depicted. additional examples found supplementary material. note found attin attention models actually performed better attall models applied images from wild focus demonstrating them. paper presented simple efﬁcient approach effectively baselining reinforce algorithm policy-gradient based allows effectively train non-differentiable metrics leads signiﬁcant improvements captioning performance mscoco—our results mscoco evaluation sever establish state-of-the-art task. self-critical approach normalizes reward obtable performance ensembled attention models trained self-critical sequence training ofﬁcial mscoco evaluation server leaderboard also depicted tained sampled sentences reward obtained model test-time inference algorithm intuitive avoids estimate action-dependent action-independent reward functions. figure captions generated image depicted figure various models discussed paper. beside caption report average probability words caption. image presents object situated atypical context models fail give accurate description attention models handle previously unseen image composition well. models trained scst return accurate detailed summary image. john schulman philipp moritz sergey levine michael jordan pieter abbeel. high-dimensional continuous control using generalized advantage estimation. arxiv preprint arxiv. dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio. actor-critic algorithm sequence prediction. arxiv", "year": 2016}