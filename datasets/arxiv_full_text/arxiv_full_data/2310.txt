{"title": "On the Power of Over-parametrization in Neural Networks with Quadratic  Activation", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "We provide new theoretical insights on why over-parametrization is effective in learning neural networks. For a $k$ hidden node shallow network with quadratic activation and $n$ training data points, we show as long as $ k \\ge \\sqrt{2n}$, over-parametrization enables local search algorithms to find a \\emph{globally} optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when $k\\ge \\sqrt{2n}$, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.", "text": "provide theoretical insights over-parametrization eﬀective learning neural networks. hidden node shallow network quadratic activation training globally optimal solution general smooth convex loss functions. further despite number parameters exceed sample size using theory rademacher complexity show weight decay solution also generalizes well data sampled neural networks achieved remarkable impact many applications computer vision reinforcement learning natural language processing. though neural networks successful practice theoretical properties well understood. speciﬁcally intriguing empirical observations existing theories cannot explain. order algorithms like stochastic gradient descent able minimize training loss neural networks. researchers conjectured over-parametrization primary reason local search algorithms achieve training error. intuition over-parametrization alters loss function large manifold globally optimal solutions turn allows local search algorithms easily global optimal solution. tive generalization since greatly increases number parameters point number parameters exceed sample size. address this practitioners often explicit forms regularization weight decay dropout early stopping improve generalization. however non-convex setting theoretically good quantitative understanding regularizations help generalization neural network models. paper provide theoretical insights optimization landscape generalization ability over-parametrized neural networks. speciﬁcally consider neural network following form setting second layer although simpler case second layer ﬁxed eﬀect over-parameterization studied setting well restriction number hidden nodes. focus quadratic activation function though quadratic activations rarely used practice stacking multiple two-layer blocks used simulate higher-order polynomial neural networks sigmodial activated neural networks improve generalization ability often explicit regularization. paper focus particular regularization technique weight decay slightly change gradient descent algorithm local minima problem global saddle points strict. properties together recent algorithmic advances non-convex optimization imply gradient descent globally optimal solution random initialization. minor generalization results soltanolkotabi includes loss haeﬀele vidal haeﬀele include random positive semideﬁnite matrix arbitrarily small frobenius norm. show problem also desired properties local minima global saddle points strict probability since small frobenius norm optimal value problem close problem section precise statement. prove surprising fact bring forward ideas smoothed analysis constructing perturbed loss function believe useful analyzing landscape non-convex losses. weight-decay helps generalization. show weight-decay optimal solution problem also generalizes well. major observation weight-decay ensures solution problem frobenius norm equivalent matrix nuclear norm observation allows theory rademacher complexity directly obtain quantitative generalization bounds. theory applies wide range data distribution particular need assume model realizable. further generalization bound depend number epochs runs number hidden nodes. paper organized follows. section introduce necessary background deﬁnitions. section present main theorems over-parametrization helps optimization section give quantitative generalization bounds explain weight decay helps generalization presence over-parametrization. section prove main theorems. conclude list future works section neural networks enjoyed great success many practical applications explain success many works studied expressiveness neural networks. expressive ability shallow neural network dates back recent results give reﬁned analysis deeper models however point view learning theory well known training neural network hard worst case despite worst-case pessimism local search algorithms gradient descent successful practice. additional assumptions many works tried design algorithms provably learn neural network however algorithms gradient-based provide insight local search algorithm works well. focusing gradient-based algorithms line research analyzed behavior gradient descent structural assumption input distribution. major drawback papers focus regression setting least-squares loss assume model realizable meaning label output neural network plus zero mean noise unrealistic. case hidden unit papers yuan zhong require stringent initialization condition recover true parameters. finding optimal weights neural network non-convex problem. recently researchers found objective functions satisfy following properties local minima global saddle points local maxima strict ﬁrst order method like gradient descent global minimum. choromanska freeman bruna zhou feng nguyen hein safran shamir soltanolkotabi poston haeﬀele vidal haeﬀele soudry hoﬀer particular haeﬀele vidal poston nguyen hein studied eﬀect over-parameterization training neural networks. results require large amount over-parameterization width hidden layers greater number training examples unrealistic commonly used neural networks. recently soltanolkotabi showed shallow neural networks number hidden nodes required larger equal input dimension ℓ-loss. comparison theorems work general loss functions regularization assumption. also propose form over-parameterization namely well known classical learning theory cannot explain generalization ability vcdimension neural networks large line research tries explain phenomenon studying implicit regularization stochastic gradient descent algorithm however generalization bounds papers often depend number epochs runs large practice. another direction study generalization ability based norms weight matrices neural networks theorem generalization ability also uses idea specialized network architecture also considered smoothed analysis technique solve semi-deﬁnite programs penalty form. mathematical techniques work bhojanapalli similar focus distinct problems solving semi-deﬁnite programs quadratic activation neural networks. bold-faced letters vectors matrices. vector denote euclidean norm. matrix denote spectral norm kmkf frobenius norm. denote left null-space i.e. section present main results explaining over-parametrization helps local search algorithms global optimal solution. consider kinds over-parameterization result states given arbitrary data optimization landscape benign properties facilitate ﬁnding globally optimal neural networks. particular setting last layer average pooling layer local minima global minima saddles direction negative curvature. turn implies gradient descent ﬁrst layer weights initialized random converges global optimum. desired properties hold long arbitrary data perturbed objective function desired properties enable local search heuristics globally optimal solution general class loss functions. further choose perturbation arbitrarily small minimum close boumal summary boumal showed almost semidefinite programs every local minima rank non-convex formulation global minimum original sdp. however theorem applies important caveat applying semideﬁnite programs fall measure zero set. primary contribution develop procedure exploits constructing perturbed objective avoid measure zero proving perturbed objective property showing optimal value perturbed objective close original objective. note analysis boumal apply since loss functions logistic loss semi-deﬁnite representable. refer readers section technical insights. section switch focus generalization ability learned neural network. since weight-decay equivalently regularization frobenius norm learned weight small. therefore section focus weight matrix bounded frobenius norm space theorem pd/n dependency usual parametric rate. practice general theorems suggest frobenius norm fourth momentpn rows maxj∈e⊤ theorem reach generalization proofs over-parametrization helps optimization build upon existing geometric characterization matrix factorization. ﬁrst cite useful theorem haeﬀele problem convex optimization problem global minimum original problem. denote mxi. since convex function ﬁrst-order optimality condition global optimality ﬁrst term dimension matrices second term dimension null space last term dimension range) upper bounded next note therefore compute dimension union proof inspired exploits structure nuclear norm bounded space. ﬁrst prove general theorem depends property fourthmoment input random variables. paper provided theoretical results over-parameterized neural networks. using smoothed analysis showed long number hidden nodes bigger input dimension square root number training data loss surface benign properties enable local search algorithms global minima. theory rademacher complexity show learned neural generalize well.", "year": 2018}