{"title": "Efficient Probabilistic Performance Bounds for Inverse Reinforcement  Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In the field of reinforcement learning there has been recent progress towards safety and high-confidence bounds on policy performance. However, to our knowledge, no practical methods exist for determining high-confidence policy performance bounds in the inverse reinforcement learning setting---where the true reward function is unknown and only samples of expert behavior are given. We propose a sampling method based on Bayesian inverse reinforcement learning that uses demonstrations to determine practical high-confidence upper bounds on the $\\alpha$-worst-case difference in expected return between any evaluation policy and the optimal policy under the expert's unknown reward function. We evaluate our proposed bound on both a standard grid navigation task and a simulated driving task and achieve tighter and more accurate bounds than a feature count-based baseline. We also give examples of how our proposed bound can be utilized to perform risk-aware policy selection and risk-aware policy improvement. Because our proposed bound requires several orders of magnitude fewer demonstrations than existing high-confidence bounds, it is the first practical method that allows agents that learn from demonstration to express confidence in the quality of their learned policy.", "text": "explains demonstrated behavior. techniques based potential applications many settings manufacturing home hospital care autonomous driving. types real-world settings important perhaps critical provide performance bounds agent’s learned policy. example consider hospital assistant robot learned demonstrations lift patient bed. deploying learned policy would want provide high-conﬁdence bound difference performance robot’s learned policy optimal policy expert’s reward. bound policy loss high robot could request additional demonstrations until high conﬁdence policy loss respect optimal policy within allowable error margin. propose general method obtaining highconﬁdence performance bounds inverse reinforcement learning setting—where true reward function unknown samples expert behavior given. given demonstrated trajectories task goal allow agent bound difference expected return agent’s policy optimal policy task expert’s unknown reward function. problem inverse reinforcement learning ill-posed seek risk-sensitive bound performance difference takes account uncertainty posterior distribution reward functions conditioned demonstrations. perform markov chain monte carlo sampling using bayesian inverse reinforcement learning sample likely reward functions given demonstrations. using sampled reward functions compute samples expected return difference optimal policy expert’s reward function agent’s policy. samples used calculate probabilistic upper bound α-worst-case policy loss. obtain bound without knowing expert’s policy true reward function. main contributions formalize problem high-conﬁdence policy evaluation inverse reinforcement learning domain; present ﬁrst practical method obtaining high-conﬁdence bounds αworst-case difference expected return evalﬁeld reinforcement learning recent progress towards safety high-conﬁdence bounds policy performance. however knowledge practical methods exist determining high-conﬁdence policy performance bounds inverse reinforcement learning setting— true reward function unknown samples expert behavior given. propose sampling method based bayesian inverse reinforcement learning uses demonstrations determine practical high-conﬁdence upper bounds α-worst-case difference expected return between evaluation policy optimal policy expert’s unknown reward function. evaluate proposed bound standard grid navigation task simulated driving task achieve tighter accurate bounds feature count-based baseline. also give examples proposed bound utilized perform riskaware policy selection risk-aware policy improvement. proposed bound requires several orders magnitude fewer demonstrations existing high-conﬁdence bounds ﬁrst practical method allows agents learn demonstration express conﬁdence quality learned policy. growing interest safety risk-sensitive metrics machine learning artiﬁcial intelligence systems especially systems interact environment risk-aware approaches recently proposed applied many different problems including planning markov decision processes physical search problems reinforcement learning imitation learning however best knowledge investigated obtain sample-efﬁcient risk-aware conﬁdence bounds performance policy unknown reward function case learning demonstrations. learning demonstration popular method learn skill policy simply observing demonstrations expert popular variant inverse reinforcement learning goal infer reward function uation policy optimal policy expert’s unknown reward function; evaluate proposed bound standard grid navigation simulated driving tasks demonstrate signiﬁcant improvement existing statistical bounds empirical baseline based feature counts; give examples proposed bound enables risk-aware policy ranking risk-aware policy improvement given demonstrations task. markov decision processes markov decision process deﬁned tuple states actions transition function reward function discount factor initial state distribution. policy mapping states probability distribution actions. value policy reward function expected return policy γtr|π]. value denoted executing policy starting state deﬁned γtr|π given reward funcv tion q-value state-action pair deﬁned denote maxπ common literature assume reward function expressed linear combination features k-dimensional vector feature weights. thus write value policy γtwt φ|π] γtφ|π] expected feature counts. note affect expressiveness reward function since non-linear function. given reward function fully speciﬁed feature weights thus refer feature weights reward function interchangeably. bayesian inverse reinforcement learning given without reward function denoted mdp\\r. given demonstrations consisting state-action pairs problem recover reward function demonstrator. problem ill-posed algorithms variety heuristics simplifying assumptions estimate bayesian seeks estimate posterior reward functions given demonstrations birl makes assumption demonstrator following softmax policy resulting likelihood function softmax distribution actions commonly used likelihood function empirically shown effective model human behavior enabling accurate learning human demonstrations prediction human actions birl algorithm uses markov chain monte carlo sampling sample posterior feature weights sampled according proposal distribution sample solved obtain sample’s likelihood determine transition probabilities within markov chain. sample resulting typically quickly solved starting policy previous using steps policy iteration estimate expert’s reward function found averaging feature weights chain obtain mean reward function using maximum posteriori estimate advantages birl compared many algorithms ﬁnds distribution likely reward functions contain partial demonstrations even non-contiguous state action pairs works sub-optimal demonstrations. choice prior allows domain knowledge inserted algorithm. ramachandran give several possibilities uniform gaussian beta prior. remainder paper assume prior uniform. evaluating effects alternative priors left future work. assume given mdp\\r samples πdemo} state-action pairs demonstrator’s policy πdemo. make common assumption demonstrator attempts maximize total return reward executing possibly suboptimal stationary policy πdemo. given evaluation policy πeval interested following general problem high-conﬁdence policy evaluation given mdp\\r evaluation policy πeval demonstrations high-conﬁdence upper bound policy loss incurred using πeval place optimal policy demonstrator’s reward function solution budget feature maximal feature count difference giving solution two-norm always lower bounded inﬁnity-norm bound tighter bound proposed abbeel note practice know demonstrated trajectories estimate demonstrator’s expected feature counts note bound guaranteed upper bound πdemo must optimal empirical estimate expert’s feature counts require large number demonstrations converge limitations bound work partial demonstrations based adversarial reward function extremely unlikely given demonstrations. worst-case feature count bound described previous section requires sampled trajectories expert completely ignores structure problem actions taken demonstrator—giving worst-case bound likely overly pessimistic. goal common metric evaluating algorithms note evaluation policy policy including hand-tuned policy policy learned reinforcement learning different task known reward function; however natural form evaluation policy policy learned demonstrations seek bound difference expected return between evaluation policy πeval policy optimal respect demonstrator’s reward however optimal policy invariant nonnegative scaling reward function bounding ill-posed multiply feature weights scale anywhere range avoid scaling issue make common assumption note assumption eliminates trivial all-zero reward function potential solution—all reward functions appropriately normalized. setting eliminates invariance scaling factors bounds magnitude still inﬁnitely many rewards induce optimal policy resulting inﬁnitely many possible values evd. thus obtain upper bound need address uncertainty. show following section address uncertainty demonstrator’s true reward compute absolute worst-case policy loss bound using feature counts. however show evaluation section type worst-case bound sensitive adversarial reward functions highly unlikely given demonstrations often resulting loose bounds. thus rather focusing absolute worst-case focus computing probabilistic upper bound α-worst-case value note deﬁnes sensitivity risk represents conﬁdence estimate α-var. thus typically always high take range values depending possibility catastrophic failure domain risk-aversion end-user. practice commonly used applications figure example random grid world navigation task colors representing random features initial states denoted stars. snapshot driving simulation. agent must learn safely drive blue trafﬁc. advantages approach follows proposed bound takes full advantage information contained transition dynamics demonstrations focus reward functions likely given demonstrations require optimal demonstrations inherits birl ability work partial demonstrations even disjoint state-action pairs allows domain knowledge form prior. proposed conﬁdence bound useful needs meet several criteria upper bound accurate high-conﬁdence bound tighter worst-case bound derived above previous criteria true even given small number demonstrations. standard grid world navigation task simulated driving task validate proposed bound satisﬁes criteria. examples tasks shown figure compare high-conﬁdence α-var bound worst-case feature count bound deﬁned equation results α-var bounds reported conﬁdence bounds grid world navigation task ﬁrst empirically evaluate approach suite grid world navigation tasks cost traveling different terrains unknown must inferred obtain high-conﬁdence probabilistic worst-case bound focuses likely reward functions given demonstrations. seek probabilistic conﬁdence bound α-value risk given evaluation policy πeval. note using rather standard feature count bound discussed previous section desirable main reasons. ﬁrst reason works well partial noisy demonstrations. because compares evaluation policy optimal policy reward actual states visited potentially sub-optimal demonstrator. second explicitly takes account initial state distribution. thus measures generalizability error evaluation policy evaluating expected return states support even demonstrations sampled small number possible initial states. bound α-quantile worst-case samples posterior thus seek calculate motivated problem deﬁnition assume thus modiﬁed version birl policy walk algorithm ensures proposal samples mcmc stay l-norm unit ball. details given appendix. using mcmc generate sequence sampled rewards posterior distribution reward functions given demonstrations. sample calculate obtain point estimate α-var sort resulting samples ascending order obtain order statistics take α-quantile. however take account number samples conﬁdence point estimate. instead using point estimate compute single-sided conﬁdence bound α-var. given sample thus order statistic normal approximation binomial distribution obtain normal distribution added index continuity correction obtain index order statistic invert equation using inverse standard normal full approach summarized algorithm algorithm three hyperparameters deﬁnes conﬁdence optimality demonstrations deﬁnes risk-sensitivity represents desired conﬁdence level estimate α-var. demonstrations. available actions down left right. transitions noisy chance moving desired direction chance going directions perpendicular chosen direction. binary features feature active grid cell. show results artifact speciﬁc reward function evaluate method many random grid worlds randomly chosen ground truth reward. initial state distribution uniform different states spread across grid shown figure generating demonstrations select initial states round-robin fashion support however measuring accuracy bound errors compare true expected value difference full initial state distribution. inﬁnite horizon grid navigation ﬁrst task inﬁnite horizon grid world navigation task terminal states. evaluate different bounding methods generated random worlds random features grid cell. world generated random feature weight vector l-unit norm ball. generate demonstrations solve using random ground truth reward optimal policy policy generate trajectories length evaluation policy optimal policy reward function found using birl. demonstrations experiment perfect birl conﬁdence parameter large value figure shows accuracy bound wfcb worst-case feature count bound quantile value risk bound. accuracy proportion trials upper bound greater ground truth expected value difference random grid worlds. expected wfcb always gives upper bound true performance difference optimal policy evaluation policy. bounds α-var also highly accurate. always predicting high upper bound result high accuracy also measured tightness upper bounds. figure shows average bound error random navigation tasks. deﬁne bound error upper bound generated ground truth reward. bounds α-var much tighter worstcase feature count bound converging small number demonstrations. noisy demonstrations mentioned previously birl uses conﬁdence parameter represents optimality demonstrations. demonstrations assumed come completely random policy means demonstrations come perfectly optimal policy. prior work used values demonstrations generated expert policy investigate effect bound generated noisy figure results inﬁnite horizon grid navigation task. accuracy average error bounds based feature counts compared percentiles bound. accuracy averages computed replicates adjusting noisy demonstrations clear effect accuracy bound error. bound error decreases increases meaning bounds become tighter; however bounds often underestimate true expected value difference expert’s policy evaluation policy resulting error lower accuracy. values range result highly accuracy bounds tighter worst-case feature count bound. however birl overﬁts noise demonstrations assuming demonstrations optimal. tuning conﬁdence parameter particular demonstrator task left future work. evaluation policy previous examples used reward obtained birl create evaluation policy; however unlike previous theoretical conﬁdence bounds method applicable evaluation policy. investigated sensitivity bound driving task provide example closely matches real-world learning demonstration task. rather evaluate method true reward function examine bound used rank select appropriate policy existing policies. task designed driving simulator based previous benchmarks figure shows snapshot simulator. agent charge driving safely highway three actions switch lanes left switch lanes right stay current lane. agent traveling faster trafﬁc must change lanes avoid cars randomly appear screen. three highway lanes supposed drive also drive off-road right left highway. state space made binary features features possible lanes including off-road lanes features telling agent whether currently collision tailgating trailing another features adjacent lane indicating whether collision tailgating changes lanes. reward assumed linear combination features -dimensional binary feature vector indicates agent’s current lane whether collision another car. discount factor goal experiment evaluate ability probabilistic performance bound correctly rank different policies given single demonstration safe driving. constructed three different evaluation policies rightsafe policy avoids hitting cars driving off-road prefers driving right lane highway onroad policy avoids driving off-road pays attention cars changes lanes randomly nasty policy avoids going off-road actively tries cars. generated single demonstration collisionfree driving consisting consecutive state-action pairs. demonstration changed lanes randomly avoiding collisions avoiding driving off-road. evaluation policies demonstration created using q-learning hand-crafted reward functions resulted desired behaviors. driving task model-free used q-learning calculate q-values used likelihood calculations birl. calculated conﬁdence bound .-var evaluation policy. also computed worst-case feature count bounds comparison. results shown table bound uses demonstration focus reward functions likely given demonstrated stateaction pairs. results correctly ranking evaluation policies. worst-case feature count bound ignores likelihood assumes worst-case reward function penalizes largest discrepancy empirical feafigure sensitivity conﬁdence noisy demonstrations grid navigation task. demonstrator chance taking random action state. accuracy average error bounds based feature counts compared .-var bound. accuracy averages computed replicates. range different evaluation policies found bounds consistently outperforms baseline wfcb providing bounds often four times tighter maintaining high accuracy demonstrate ability method work evaluation policies derived algorithms compare existing high-conﬁdence bounds used projection algorithm proposed abbeel evaluation policy. abbeel provide high-conﬁdence bounds number demonstrations needed algorithm guarantee performance within demonstrator. tighter sample bound feature count-based methods later derived syed schapire also holds projection algorithm. inverted bound syed schapire obtain conﬁdence bound expected value difference given ﬁxed number demonstrations repeated inﬁnite horizon grid navigation experiment described above using policy found projection algorithm evaluation policy. compare average bound error proposed bounds syed schapire error bound projection algorithm table empirical bounds three orders magnitude tighter hoeffding style bound table comparison conﬁdence α-var bounds conﬁdence hoeffding-style bound bounds projection algorithm obtain evaluation policy. results averaged random navigation tasks. table policy rankings based upper bounds policy loss three different evaluation policies driving domain given single demonstration safe driving. results averaged replicates. figure given demonstration optimizing bound results risk-aware policy hedges cells much worse white. maximum likelihood reward assumes marginally worse white. ture counts demonstration expected feature counts evaluation policies. collision feature less frequently active lane features onroad nasty appear safer right-safe average state-occupancies closely align stateoccupancies demonstration. high-conﬁdence policy improvement previous section showed risk-sensitive policy evaluation choose multiple evaluation policies. take step give example uses risk-sensitive policy evaluation iteratively reduce policy learned demonstrations. highlight potential safe policy improvement consider simple navigation task shown figure task single terminal center reward features agent given single demonstration starting state must generalize demonstration second starting state note demonstration shows feature less desirable white feature true magnitudes feature weights left uncertain. implemented simple risk-sensitive policy improvement hill climbing algorithm. initialized hill climbing algorithm maximum likelihood policy found using birl uniform prior. step hillclimbing algorithm examined impact .var changing action taken policy single state chose change resulted largest decrease .-var single state changes. continued process reductions .-var could found. resulting risk-aware policy seeks minimize .-var avoiding feature whereas maximum likelihood reward leads less conservative policy resulting higher potential risk. learned policies shown figure future complex policy adaptation schemes ﬁnite difference methods black-box optimization techniques could also used approximate gradient α-var respect parameterized policy many different methods exist learning demonstration inverse reinforcement learning however give guarantees performance. abbeel syed schapire give probabilistic hoeffding-style bounds many demonstrations algorithms require guarantee policy expected return within epsilon expected return demonstrator’s policy. however shown table theoretical bounds loose useful practice customized speciﬁc algorithms. knowledge provide ﬁrst sample-efﬁcient high-conﬁdence bound policy loss evaluation policy respect optimal policy demonstrator’s true reward function. safety extensively studied within reinforcement learning community survey). approaches typically either focus safe exploration optimizing objective expected return. recently alternative objectives based ﬁnancial measures risk conditional shown provide tractable useful risk-sensitive measures performance mdps santara propose algorithm minimize conditional generative adversarial imitation learning provide bounds safety learned policy. work complements prior research safety reinforcement learning imitation learning showing risk-sensitive metrics applied obtain high-conﬁdence performance bounds. additional work safety mdps focused obtaining high-conﬁdence bounds performance policy policy deployed well methods high-conﬁdence policy improvement work draws inspiration previous approaches; however provide bounds policy performance applicable learning demonstrations i.e. rewards observed. space time constraints explore full range possible instantiations risk-sensitive performance bound learning demonstration irl. section discuss design choices limitations approach avenues future research. decided measure policy loss using commonly used metric; however measure performance used approach. method estimates posterior distribution reward functions risk measure loss function reward function policy inserted framework place evd. used well known widely used easy implement using monte-carlo samples probabilistic analogue wfcb. however proposed methodology extended risk measure computed samples distribution. alternative risk measures conditional value-atrisk entropic risk measure semideviations could replace framework. recently methods proposed explicitly optimize conditional policy future work examine whether approaches combined work risk-aware policy improvement irl. bound based bayesian method designed work partial demonstrations allows insertion domain knowledge prior reward functions. choi shown many standard algorithms transformed equivalent bayesian algorithm selecting appropriate likelihood prior. thus proposed performance bound easily extended alternative likelihoods priors match different assumptions preferences found literature. main drawbacks proposed framework requires running mcmc repeatedly samples rewards solves order calculate birl likelihood compute samples evd. future work investigate whether methods based policy gradients algorithms method also relies appropriate range conﬁdence parameter birl algorithm determines much trust demonstrations. recently expectation maximization approach used learn parameter large number demonstrations differing quality future work investigate whether similar approach used learn appropriate value possibly small number demonstrations similar quality. work formalized addressed problem risk-aware high-conﬁdence policy evaluation unknown reward function. knowledge present ﬁrst general framework obtaining practical highconﬁdence bounds performance difference evaluation policy optimal policy demonstrator’s true unknown reward. also give examples high-conﬁdence performance bound used perform risk-aware policy selection risk-aware policy improvement. proposed algorithms evaluated standard grid navigation task driving simulation. results demonstrate proposed bound signiﬁcant improvement baseline based feature counts—providing accurate tight bounds even small numbers demonstrations. additionally empirical results show orders magnitude improvement sample efﬁciency competing conﬁdence bounds result ﬁrst approach allows agents learn demonstrations express conﬁdence performance learned policy based limited demonstration data. believe techniques proposed paper provide starting point developing autonomous agents safely efﬁciently learn human demonstrations risk-sensitive real-world environments.", "year": 2017}