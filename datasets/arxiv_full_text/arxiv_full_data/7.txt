{"title": "A Deep Reinforcement Learning Chatbot (Short Version)", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML", "I.5.1; I.2.7"], "abstract": "We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than other systems. The results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real-world, open-domain conversational agents.", "text": "present milabot deep reinforcement learning chatbot developed montreal institute learning algorithms amazon alexa prize competition. milabot capable conversing humans popular small talk topics speech text. system consists ensemble natural language generation retrieval models including neural network template-based models. applying reinforcement learning crowdsourced data real-world user interactions system trained select appropriate response models ensemble. system evaluated testing realworld users performed signiﬁcantly better systems. results highlight potential coupling ensemble systems deep reinforcement learning fruitful path developing real-world open-domain conversational agents. conversational agents including chatbots personal assistants becoming increasingly ubiquitous. amazon proposed international university competition goal building socialbot spoken conversational agent capable conversing humans popular topics entertainment fashion politics sports technology. article describes experiments mila team university montreal emphasis reinforcement learning. socialbot based large-scale ensemble system leveraging deep learning reinforcement learning. ensemble consists deep learning models template-based models external webservices natural language retrieval generation. apply reinforcement learning including value function policy gradient methods intelligently combine ensemble retrieval generation models. particular propose novel off-policy model-based reinforcement learning procedure yields substantial improvements testing experiments real-world users. rating scale best performing system reached average user score average user score teams competition furthermore best performing system averaged turns conversation signiﬁcantly higher systems. school computer science mcgill university. cifar fellow. https//developer.amazon.com/alexaprize. throughout semi-ﬁnals carried several testing experiments evaluate different variants shown testing experiments ingredient achieving performance application off-policy deep reinforcement learning coupled inductive biases designed improve system’s generalization ability making efﬁcient bias-variance tradeoff. early work dialogue systems based mainly states rules hand-crafted human experts. modern dialogue systems typically follow hybrid architecture combines hand-crafted states rules statistical machine learning algorithms complexity human language however impossible enumerate states rules required building socialbot capable conversing humans open-domain popular topics. contrast rule-based systems core approach built entirely statistical machine learning. believe plausible path artiﬁcially intelligent conversational agents. system architecture propose aims make assumptions possible process understanding generating natural language. such system utilizes small number hand-crafted states rules. meanwhile every system component designed optimized using machine learning algorithms. optimizing system components ﬁrst independently massive datasets jointly real-world user interactions system learn implicitly relevant states rules conducting open-domain conversations. given adequate amount examples system outperform system based states rules hand-crafted human experts. further system continue improve perpetuity additional data. system architecture inspired success ensemble-based machine learning systems. systems consist many independent sub-models combined intelligently together. examples ensemble systems include winner netﬂix prize watson questionanswering system google’s machine translation system system consists ensemble response models response model takes input dialogue history outputs response natural language text. explained later response models engineered generate responses diverse topics using variety strategies. dialogue manager responsible combining response models together. input dialogue manager expects given dialogue history conﬁdence values automatic speech recognition system generate response dialogue manager follows three-step procedure. first uses response models generate candidate responses. second exists priority response candidate responses response returned system. example question \"what name?\" response alexa prize socialbot\" priority response. third priority responses response selected model selection policy. example model selection policy select response scoring candidate responses picking highest-scored response. candidate responses bowfactgenerator know male rabbits called bucks females does? gruquestiongenerator american family initiatorbot believe love ﬁrst sight? lstmclassiﬁermsmarco parts learning feed rabbit learning dualencoderredditnews doggie alicebot hurrah good number rabbits. elizabot elaborate bit? vhredredditmovies hmm. nope never heard vhredsubtitles would like them? bowwashingtonpost call people dogs crazy also. candidate responses bowfactgenerator here’s interesting fact. international telephone dialing code antarctica gruquestiongenerator york city? initiatorbot day? lstmclassiﬁermsmarco third anthology released dualencoderredditnews whisper time bby. alicebot like robots movie? elizabot make feel? star wars movie movie star wars series. vhredredditmovies please. please. pleeeease. happen. vhredsubtitles what? bowwashingtonpost much enjoyable feature last year’s old-timer’s convention masquerading star wars movie. response models system including neural network based retrieval models neural network based generative models knowledge base question answering systems template-based systems. examples candidate model responses shown table along model names. description models reader referred technical report serban generating candidate response dialogue manager uses model selection policy select response returns user. dialogue manager must select response increases satisfaction user entire dialogue. order this must make trade-off immediate long-term user satisfaction. example suppose user asks talk politics. dialogue manager chooses respond political joke user pleased turn. afterwards however user disappointed system’s inability debate political topics. instead dialogue manager chooses respond short news statement user less pleased turn. however inﬂuence user follow factual questions system better adept handling. make trade-off immediate long-term user satisfaction consider selecting appropriate response sequential decision making problem. section describes approaches investigated learn model selection policy. approaches evaluated real-world users next section. reinforcement learning framework dialogue manager agent takes actions environment order maximize rewards. time step ...t agent observes dialogue history must choose actions taking action receives reward transferred next state provided actions γtrt t+...ak discount factor. estimates expected discounted rewards referred expected return taking action given dialogue history given agent continue policy afterwards. given agent chooses action highest expected return parametrize scoring function action-value function neural networks layers. ﬁrst layer input consisting features representing dialogue history candidate response features based combination word embeddings dialogue acts part-of-speech tags unigram word overlap bigram word overlap model-speciﬁc features. second layer contains hidden units computed applying linear transformation followed rectiﬁed linear activation function input layer features. third layer contains hidden units computed applying linear transformation preceding layer units. fourth layer contains outputs units probabilities output units computed applying linear transformation preceding layer units followed softmax transformation. layer corresponds amazon mechanical turk labels described later. ﬁfth layer ﬁnal output layer single scalar value computed applying linear transformation units third fourth layers. order learn parameters different machine learning approaches described next. supervised learning crowdsourced labels ﬁrst approach learning policy parameters called supervised learning amt. approach estimates action-value function using supervised learning crowdsourced labels. also serves initialization approaches. amazon mechanical turk collect data training policy. follow setup similar show human evaluators dialogue along candidate responses score appropriate candidate response likert-type scale. score indicates response inappropriate make sense indicates response acceptable indicates response excellent highly appropriate. examples thousand dialogues recorded alexa users preliminary version systems. corresponding candidate responses generated response models. total collected labels split training development testing datasets consisting respectively labels each. optimize model parameters w.r.t. log-likelihood using mini-batch stochastic gradient descent predict layer represents labels. since labels last layer model corresponding linear transformation parameters case assign score inappropriate response acceptable response excellent response. off-policy reinforce next approach learns stochastic policy directly examples rd}dt examples dialogues recorded system real-world users. indicates dialogue indicates time step dialogue history agent’s action observed return. further parameters stochastic policy used dialogue re-weighted variant reinforce algorithm learning rate updates policy parameters example vice versa examples user scores decrease action probabilities. ratio left-hand-side corrects discrepancy learned policy policy data collected πθd. evaluate policy using estimate expected return training thousand dialogues scores collected interactions real users preliminary version system june july optimize policy parameters training based select hyper-parameters early-stop development based learned reward function next approaches trains linear regression model predict user score given dialogue. given dialogue history candidate response model parameters predicts corresponding user score. training data scarce higher-level features input. model trained dataset off-policy reinforce. regression model used ways. first used ﬁne-tune action-value function learned supervised learning accurately predict user score. speciﬁcally output layer ﬁne-tuned w.r.t. squared-error prediction policy called supervised learned reward. second regression model combined off-policy reinforce policy called off-policy reinforce learned reward. policy trained off-policy reinforce replaced predicted user score q-learning abstract discourse markov decision process ﬁnal approach based learning policy simpliﬁed markov decision process called abstract discourse mdp. approach somewhat similar training user simulator. ﬁtted dataset dialogues user scores before. particular time-step reward function score predicted supervised amt. description reader referred technical report serban given abstract discourse q-learning experience replay learn policy action-value parametrization experience replay memory buffer size \u0001-greedy exploration scheme experiment discount factors ∈{...}. training based carried alternating phases. every episodes training evaluate policy episodes w.r.t. average return. evaluation dialogue histories sampled separate dialogue histories. ensures policy overﬁtting ﬁnite dialogue histories. select policy performs best w.r.t. average return. policy called q-learning amt. quantitative analysis shows learned policy likely select risky responses perhaps learned effective remediation fall-back strategies carry testing experiments evaluate dialogue manager policies selecting response model. alexa user starts conversation system assigned random policy afterwards dialogue score recorded. major issue testing experiments distribution alexa users changes time. different types users using system depending time weekday holiday season. addition user expectations towards system change users interact socialbots competition. therefore must take steps reduce confounding factors correlations users. first testing experiment simultaneously evaluate policies interest. ensures approximately number users interacting policy w.r.t. time weekday. minimizes effect changing user distribution within testing period. second discard scores returning users users returning system likely inﬂuenced previous interactions system. example users positive previous experience biased towards giving higher scores next interaction. ﬁrst testing experiment conducted july august tested dialogue manager policies supervised supervised learned reward off-policy reinforce off-policy reinforce learned reward q-learning amt. used greedy variants off-policy reinforce policies. also tested heuristic baseline policy evibot alicebot selects evibot model response available otherwise selects alicebot model response. thousand user scores collected hundred user scores policy. experiment occurred middle competition semi-ﬁnals. period users likely relatively expectations towards systems competition further period july august overlaps summer holidays united states. such might expect children interact system seasons. second testing experiment conducted august august tested policies off-policy reinforce q-learning amt. prior beginning experiment minor system improvements carried w.r.t. initiatorbot ﬁltering profanities. total hundred user scores collected policy. experiment occurred competition semi-ﬁnals. point many users already interacted socialbots competition therefore likely developed expectations towards systems further period august august overlaps summer holidays beginning school year united states. means expect less children interacting previous testing experiment. third testing experiment carried august august surprising results previous testing experiment decided continue testing policies off-policy reinforce q-learning amt. total three hundred user ratings collected discarding returning users. experiment occurred competition semi-ﬁnals. means likely many alexa users already developed expectations towards systems. further period august august lies entirely within beginning school year united states. might expect less children interact system previous testing experiment. table shows average alexa user scores average dialogue length well average percentage positive negative user utterances according sentiment classiﬁer. observe q-learning performed best among policies w.r.t. alexa user scores ﬁrst third experiments. ﬁrst experiment q-learning obtained average user score signiﬁcantly better policies signiﬁcance level two-sample t-test. supported percentage user utterances positive negative sentiment qlearning consistently obtained lowest percentage negative sentiment user utterances maintaining high percentage positive sentiment user utterances. comparison average user score teams competition semi-ﬁnals next comes off-policy reinforce performed best second experiment. second third experiments offpolicy reinforce also performed substantially better policies ﬁrst experiment. further ﬁrst experiment off-policy reinforce also achieved longest dialogues average turns dialogue. comparison average number turns dialogue teams competition semi-ﬁnals means off-policy reinforce turns average teams competition semi-ﬁnals. remarkable since utilize non-conversational activities negative user utterances. remaining policies achieved average user scores suggesting learned select responses appropriately heuristic policy evibot alicebot. addition computed several linguistic statistics policies ﬁrst experiment. average q-learning responses contained noun phrases off-policy reinforce evibot alicebot responses contained noun phrases respectively. further average q-learning responses word overlap immediate preceding user utterances off-policy reinforce evibot alicebot responses word overlap respectively. suggests q-learning substantially topical speciﬁcity topical coherence compared policies. such seems likely returning users would prefer policy others. ﬁnding consistent analysis showing q-learning risk tolerant. conclusion policies q-learning off-policy reinforce demonstrated substantial improvements policies. further q-learning policy achieved average alexa user score substantially average teams amazon alexa competition semi-ﬁnals. strongly suggests learning policy simulations abstract discourse serve fruitful path towards developing open-domain socialbots. performance off-policy reinforce suggests optimizing policy directly towards user scores also serve fruitful path. particular off-policy reinforce obtained substantial increase average number turns dialogue compared average teams semi-ﬁnals suggesting resulting conversations signiﬁcantly interactive engaging. overall experiments demonstrate advantages ensemble approach many different models output natural language responses system policy selects response among them. interactions data learned policies bound continue improving. proposed evaluated large-scale ensemble-based dialogue system framework amazon alexa prize competition. system leverages variety machine learning methods including deep learning reinforcement learning. developed deep learning models natural language retrieval generation including deep learning models. further developed novel reinforcement learning procedure evaluated existing reinforcement learning methods testing experiments real-world amazon alexa users. innovations enabled make substantial improvements upon baseline system. best performing system reached average user score scale minimal amount hand-crafted states rules without engaging non-conversational activities comparison average user score teams competition semi-ﬁnals furthermore system averaged turns conversation substantially higher average number turns conversation teams semi-ﬁnals. improvement back-and-forth exchanges user system suggests system interactive engaging systems competition. since nearly system components trainable machine learning models system likely improve greatly interactions additional data. thank aaron courville michael noseworthy nicolas angelard-gontier ryan lowe prasanna parthasarathi peter henderson helpful feedback. thank christian droulers building graphical user interface text-based chat. thank amazon providing tesla gpus amazon services platform. titan gpus used research donated nvidia corporation. authors acknowledge nserc canada research chairs cifar research nuance foundation microsoft maluuba druide informatique inc. funding. c.-w. lowe serban noseworthy charlin pineau. evaluate dialogue system empirical study unsupervised evaluation metrics dialogue response generation. emnlp suendermann-oeft ramanarayanan teckenbrock neutatz schmidt. halef open-source standard-compliant telephony-based modular spoken dialog system review outlook. natural language dialog systems intelligent assistants. springer", "year": 2018}