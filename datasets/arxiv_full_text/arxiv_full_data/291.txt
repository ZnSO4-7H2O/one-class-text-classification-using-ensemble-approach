{"title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement  Learning", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.  We demonstrate two experimental results.  First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.  Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.", "text": "figure propose cooperative image guessing game agents q-bot a-bot– communicate natural language dialog q-bot select particular unseen image lineup. model agents deep neural networks train end-to-end reinforcement learning. sift large quantities surveillance data enabling users interact naturally intelligent assistants despite rapid progress intersection vision language particular image/video captioning question answering clear grand goal visual dialog agent. recent works proposed studying task visually-grounded dialog. perhaps somewhat counterintuitively works treat dialog static supervised learning problem rather interactive agent learning problem naturally speciﬁcally introduce ﬁrst goal-driven training visual question answering dialog agents. speciﬁcally pose cooperative ‘image guessing’ game agents q-bot a-bot– communicate natural language dialog q-bot select unseen image lineup images. deep reinforcement learning learn policies agents end-to-end pixels multi-agent multi-round dialog game reward. demonstrate experimental results. first ‘sanity check’ demonstration pure show results synthetic world agents communicate ungrounded vocabulary i.e. symbols pre-speciﬁed meanings bots invent communication protocol start using certain symbols ask/answer certain visual attributes thus demonstrate emergence grounded language communication among ‘visual’ dialog agents human supervision. second conduct large-scale real-image experiments visdial dataset pretrain supervised dialog data show ‘ﬁne-tuned’ agents significantly outperform agents. interestingly q-bot learns questions a-bot good ultimately resulting informative dialog better team. focus paper visually-grounded conversational artiﬁcial intelligence speciﬁcally would like develop agents ‘see’ ‘communicate’ understanding natural language believe next generation intelligent systems need posses ability hold dialog visual content variety applications e.g. helping visually impaired users understand surroundings social media content enabling analysts works ﬁrst collect dataset human-human dialog i.e. sequence question-answer pairs image next machine provided image human dialog recorded till round follow-up question supervised generate human response essentially round machine artiﬁcially ‘injected’ conversation humans asked answer question machine’s answer thrown away next round machine provided ‘ground-truth’ human-human dialog includes human response machine response ˆat. thus machine never allowed steer conversation would take dialog dataset making non-evaluable. paper generalize task visual dialog beyond necessary ﬁrst stage supervised learning posing cooperative ‘image guessing’ game dialog agents. deep reinforcement learning learn policies agents end-to-end pixels multi-agent multi-round dialog game reward. setup illustrated fig. formulate game between questioner answerer q-bot shown -sentence description unseen image allowed communicate natural language answering shown image. objective fullycooperative game q-bot build mental model unseen image purely natural language dialog retrieve image lineup images. notice challenging game. q-bot must ground words mentioned provided caption estimate images provided pool contain content follow-up questions help identify correct image. analogously a-bot must build mental model qbot understands answer questions precise enough allow discrimination similar images pool concise enough confuse imperfect q-bot. every round dialog q-bot listens answer provided a-bot updates beliefs makes prediction visual representation unseen image receives reward environment based close q-bot’s prediction true representation goal q-bot a-bot communicate maximize reward. critical issue agents imperfect noisy ‘forget’ things past sometimes repeat themselves stay consistent responses a-bot access external knowledge-base cannot answer questions etc. thus succeed task must learn play other’s strengths. important question force agents communicate discrete symbols opposed continuous vectors? reason twofold. first discrete symbols natural language interpretable. forcing agents communicate understand natural language ensure humans inspect conversation logs agents importantly communicate them. bots trained pair human questioner a-bot accomplish goals visual dialog pair human answerer q-bot play visual -questions game. second reason communicate discrete symbols prevent cheating q-bot a-bot allowed exchange continuous vectors trivial solution a-bot ignore q-bot’s question directly convey vector allowing q-bot make perfect prediction. essence discrete natural language interpretable lowdimensional bottleneck layer agents. contributions. introduce novel goal-driven training visual question answering dialog agents. despite signiﬁcant popular interest since previous approaches based supervised learning making ﬁrst instance goaldriven training visual question answering dialog. demonstrate experimental results. first ‘sanity check’ demonstration pure show results diagnostic task perception perfect synthetic world ‘images’ containing single object deﬁned three attributes synthetic world q-bot identify image must learn attributes. bots communicate ungrounded vocabulary i.e. symbols pre-speciﬁed human-interpretable meanings trained end-to-end task bots invent communication protocol q-bot starts using certain symbols query speciﬁc attributes a-bot starts responding speciﬁc symbols indicating value attribute essentially demonstrate automatic emergence grounded language communication among ‘visual’ dialog agents human supervision second conduct large-scale real-image experiments visdial dataset imperfect perception real images discovering human-interpretable language communication strategy scratch tremendously difﬁcult unnecessary re-invention english. thus pretrain supervised dialog data visdial ‘ﬁne tuning’ alleviates number challenges making deep converge something meaningful. show ﬁne-tuned bots signiﬁcantly outperform supervised bots. interestingly supervised q-bot attempts mimic humans questions trained q-bot shifts strategies asks questions a-bot better answering ultimately resulting informative dialog better team. vision language. number problems intersection vision language recently gained prominence e.g. image captioning visual question answering related paper recent works visually-grounded dialog proposed task visual dialog collected visdial dataset pairing subjects amazon mechanical turk chat image trained neural visual dialog answering models. vries extended referit game ‘guesswhat’ game person asks questions image guess object ‘selected’ second person answers questions ‘yes’/‘no’/na disadvantage guesswhat requires bounding annotations objects; image guessing game need annotations thus unlimited number game plays simulated. moreover described sec. works unnaturally treat dialog static supervised learning problem. although datasets contain thousands human dialogs still represent incredibly sparse sample vast space visually-grounded questions answers. training robust visually-grounded dialog agents supervised techniques still challenging task. work take inspiration alphago approach supervision human-expert games reinforcement learning self-play. similarly perform supervised pretraining human dialog data ﬁne-tune end-to-end goal-driven manner deep questions lewis signaling game. proposed image-guessing game naturally visual analog popular -questions game. formally generalization lewis signaling game widely studied economics game theory. cooperative game players sender receiver. classical setting world number ﬁnite discrete states known sender receiver. sender send discrete symbols/signals receiver upon receiving signal must take discrete actions. game perfectly cooperative simple nash equilibrium ‘identity mapping’ sender encodes world state bijective signal similarly receiver bijective mapping signal action. proposed ‘image guessing’ game generalization q-bot receiver a-bot sender. however proposed game receiver passive. actively solicits information asking questions. moreover signaling process ‘single shot’ proceeds multiple rounds conversation. text-only classical dialog. proposed using training dialog systems. however hand-deﬁne ‘good’ utterance/dialog looks like contrast taking adversarial learning cooperative game agents need hand-deﬁne ‘good’ dialog looks like ‘good’ dialog leads successful image-guessing play. emergence language. long history work language emergence multi-agent systems recent resurgence focused deep high-level ideas concurrent works similar synthetic experiments. large-scale realimage results want bots invent uninterpretable language pretraining visdial achieve ‘alignment’ english. players roles. game involves collaborative agents questioner answerer information asymmetry. a-bot sees image q-bot not. q-bot primed -sentence description unseen image asks ‘questions’ abot answers another sequence symbols. communication occurs ﬁxed number rounds. game objective general. round addition communicating q-bot must provide ‘description’ unknown image based dialog history players receive reward environment inversely proportional error description metric note general setting ‘description’ take varying levels speciﬁcity image embeddings textual descriptions pixel-level image generations. speciﬁc instantiation. experiments focus setting q-bot tasked estimating vector embedding image given feature extractor human annotation required produce target ‘description’ ˆygt reward/error measured simple euclidean distance image used visual grounding dialog. thus unlimited number ‘game plays’ simulated. target image representation dialog ended. note total reward summed time steps dialog function initial ﬁnal states cancellation intermediate terms i.e. fig. shows overview policy networks q-bot a-bot interaction within single round dialog. agent policies modeled hierarchical recurrent encoder-decoder neural networks recently proposed dialog modeling q-bot consists following four components fact encoder q-bot asks question ‘are animals?’ receives answer ‘yes elephants.’. q-bot treats concatenated -pair ‘fact’ knows unseen image. fact encoder lstm whose ﬁnal hidden state used embedding state/history encoder lstm takes encoded fact time step produce encoding notice prior dialog including time results two-level hierarchical encoding dialog feature regression network single fullyconnected layer produces image representation prediction current encoded state components relation shown left side fig. collectively refer parameters three lstm models feature regression network a-bot similar structure q-bot slight differences since also models image question encoder a-bot receives question fact encoder similar q-bot a-bot also encodes -pairs lstm purpose encoder a-bot remember already told q-bot able understand references entities already mentioned. section formalize training visual dialog agents reinforcement learning describing formally action state environment reward policy training procedure. begin noting although agents since game perfectly cooperative without loss generality view single-agent setup single meta-agent comprises constituent agents communicating natural language bottleneck layer. action. agents share common action space consisting possible output sequences token vocabulary action space discrete principle inﬁnitely-large since arbitrary length sequences produced dialog forever. synthetic experiment agents given different vocabularies coax certain behavior emerge visdial experiments agents share common vocabulary english tokens. addition round dialog q-bot also predicts current guess visual representation unseen image. component q-bot’s action space continuous. state. since information asymmetry agent observed state. dialog grounded image caption state q-bot round caption dialog history state a-bot also includes image policy. model q-bot a-bot operating stochastic policies questions answers sampled policies conditioned dialog/state history. policies learned separate deep neural networks parameterized addition q-bot includes feature regression network produces image representation prediction listening answer round i.e. thus goal policy learning estimate parameters environment reward. environment image upon dialog grounded. since purely cooperative setting agents receive reward. distance metric image representations round deﬁne reward state-action pair figure policy networks q-bot a-bot. round dialog q-bot generates question question decoder conditioned state encoding generates answer encode completed exchange predicts image representation receives reward. natural objective considering entire dialog single episode differentiate individual good exchanges within thus update model based per-round rewards standard practice estimate expectations sample averages. speciﬁcally sample question q-bot sample answer a-bot compute scalar reward round multiply scalar reward gradient log-probability exchange propagate backward compute gradients w.r.t. parameters update intuitive interpretation particular informative probabilities pushed conversely poor exchange leading negative reward pushed state/history encoder lstm takes round encoded question image features previous produce state encoding i.e. fact encoding allows model contextualize current question w.r.t. history looking image seek answer. code publicly available. recap dialog round time consists q-bot generating question conditioned state encoding a-bot encoding updating state encoding generating answer q-bot a-bot encoding completed exchange q-bot updating state based making image representation prediction unseen image. order train agents reinforce algorithm updates policy parameters response experienced rewards. section derive expressions parameter gradients setup. recall agents take actions communication feature prediction objective maximize expected reward agents’ policies summed entire dialog figure emergence grounded dialog ‘image’ three attributes tasks q-bot agents interact rounds followed attribute pair prediction q-bot. example -round dialog grounding emerges color shape style encoded respectively. improvement reward policy learning. succeed image guessing game q-bot a-bot need accomplish number challenging sub-tasks must learn common language develop mappings symbols image representations i.e. a-bot must learn ground language visual perception answer questions qbot must learn predict plausible image representations end-to-end manner distant reward function. diving full task real images conduct ‘sanity check’ synthetic dataset perfect perception even possible? setup. shown fig. consider synthetic world ‘images’ represented triplet attributes shapes colors styles total unique images. a-bot perfect perception given direct access representation image. q-bot tasked deducing attributes image particular order e.g. task q-bot would need output image seen a-bot form tasks image. vocabulary. conducted series pilot experiments found choice vocabulary size crucial coaxing non-trivial ‘non-cheating’ behavior emerge. instance found a-bot vocabulary large enough |va| optimal policy learnt simply ignores q-bot asks a-bot conveys entire image single token human communication impoverished vocabulary cannot possibly encode richness visual sensor necessary non-trivial dialog emerge. ensure least rounds dialog restrict agent produce single symbol utterance round ‘minimal’ vocabularies a-bot q-bot. since |va|rounds images non-trivial dialog necessary succeed task. policy learning. since action space discrete small instantiate q-bot a-bot fully speciﬁed tables q-values apply tabular q-learning monte carlo estimation episodes learn policies. updates done alternately frozen updated. training \u0001-greedy policies ensuring action probability greedy action split remaining probability uniformly across actions. test time default greedy deterministic policy obtained \u0001-greedy policies. task requires outputting correct attribute value pair based task image. since total unique values across attributes q-bot’s ﬁnal action selects attribute-pairs. rewards right wrong predictions. results. fig. shows reward achieved agents’ policies number iterations quickly learn optimal policy. fig. show example exchanges between trained bots. invent communication protocol q-bot consistently uses speciﬁc symbols query speciﬁc attributes color shape style. a-bot consistently responds speciﬁc symbols indicate inquired attribute e.g. qbot emits a-bot responds with purple green blue red. similar mappings exist responses attributes. essentially automatic emergence grounded language communication protocol among ‘visual’ dialog agents without human supervision synthetic experiments previous section establish faced cooperative task information must exchanged agents perfect perception capable developing complex communication protocol. general imperfect perception real images discovering human-interpretable language communication woman genders? adults? looks like sunglasses wear goggles? hats people? black color man’s hat? snowing now? woman wearing? blue jacket black pants smiling? trees? many people? male female? wearing? color skis? wearing goggles? wearing goggles? wearing goggles? wearing goggles? wearing hats? wearing goggles? people male female both? wearing? snowing? trees? buildings background? trees? buildings background? trees? buildings background? look like park? think male can’t wearing snow pants jackets appear snowing moment trees background trees background buildings trees background buildings appear look men? can’t tell close restaurant? maybe look might related? holding slice pizza hand giving thumbs other? holding slice holding box... cheese kind pizza have? pizza company name pictures graphics box? slender slender heavy set? either wear glasses? both? man? wearing helmet? wearing protective gear? color hair? color shirt? wearing shorts? color shorts? people visible? color shoes? skate park? people photo man? tennis player photo? can’t tell it’s close outdoor court? white shirt shorts wearing? net? bottom court? ball photo? close man’s racquet? take swing? left handed right handed? right giraffe adult? grass higher altitude feet? rock big? weather? color rock? grass green dry? giraffe look healthy? giraffe look happy? giraffe laying down? giraffe’s mouth closed? people picture? people cheese white? crackers white? lots greens? picture taken kitchen? zoomed many crackers there? design plate? good lighting? plate table? background cracker look delicious? people? color plate? plate table? color plate? plate table? anything else table? anything else table? anything else table? anything else table? anything else table? looks like late teens man? medium short hair long hair? brown color hair? wearing glasses? facial hair? dressed? he’s white dress shirt matching vest looks like school uniform larger skinny? picture taken outside? tell it’s daytime? tell room standing hallway table selected examples q-bot-a-bot interactions sl-pretrained rl-full-qaf. rl-full-qaf interactions diverse less prone repetitive safe exchanges image-discriminative. strategy scratch tremendously difﬁcult unnecessary re-invention english. leverage recently introduced visdial dataset contains human dialogs images coco dataset total qa-pairs. example dialogs visdial dataset shown tab. image feature regression. consider speciﬁc instantiation visual guessing game described sec. speciﬁcally round q-bot needs regress vector embedding image corresponding output distance metric used reward computation i.e. ||ygt ˆyt−|| training strategies. found training strategies crucial ensure/improve convergence framework described sec. produce meaningful dialog exchanges ground agents natural language. supervised pretraining. ﬁrst train agents supervised manner train split visdial objective. thus conditioned human dialog history q-bot trained generate follow-up question human a-bot trained generate response human feature network optimized regress a-bot pretrained imagenet. pretraining ensures agents generally recognize objects/scenes emit english questions/answers. space possible tremendously large without pretraining exchanges result information gain image. curriculum learning. supervised pretraining ‘smoothly’ transition agents training according curriculum. speciﬁcally continue supervised training ﬁrst rounds dialog transition policy-gradient updates remaining rounds. start gradually anneal curriculum ensures agent team suddenly diverge policy incorrect generated. models pretrained epochs visdial transition policy-gradient training annealing every epoch. lstms -layered hidden states. adam learning rate clamp gradients avoid explosion. code made publicly available. explicit state-dependent baseline training initialize supervised pretraining zero-centered reward ensures good proportion random samples positively negatively reinforced. model ablations. compare natural ablations full model denoted rl-full-qaf. first evaluate purely supervised agents i.e. trained visdial data comparison agents establishes much helps supervised learning. second q-bot a-bot supervised pretrained initialization train agent label frozen-q frozen-a respectively. comparing partially frozen agents tell importance coordinated communication. finally freeze regression network supervised pretrained initialization training q-bot a-bot measures improvements language adaptation alone. quantify performance agents along dimensions well perform image guessing task closely emulate human dialogs evaluation guessing game. assess well agents learned cooperate image guessing task setup image retrieval experiment based test split visdial never seen agents training. present image automatically generated caption agents allow communicate rounds dialog. round q-bot predicts feature representation sort entire test ascending distance prediction compute rank source image. fig. shows mean percentile rank source image method baselines across rounds percentile rank means source image closer prediction images set. tab. shows example exchanges humans sl-pretrained rl-full-qaf agents. make observations agents ‘forget’; agents forget less. interesting trend note fig. methods signiﬁcantly improve round rounds beyond methods exception rl-full-qaf worse even though strictly information. shown tab. agents often stuck inﬁnite repeating loops much rarer agents. moreover even agents repeat themselves after longer gaps conjecture goal helping partner multiple rounds encourages longer term memory retention. figure guessing game evaluation. plot shows rank percentile ‘ground truth’ image retrieved using predictions q-bot rounds dialog. round corresponds image guessing based caption alone. rl-full-qaf bots signiﬁcantly outperforms sl-pretrained bots error bars show standard error means. shows qualitative results predicted fc-based image retrieval. left column shows true image caption right column shows dialog exchange list images sorted distance ground-truth image. image predicted q-bot highlighted red. predicted image often semantically quite similar. visdial evaluation. performance a-bot visdial test mean reciprocal rank recallk mean rank metrics. higher better recallk lower better mean rank. proposed frozen-q-multi outperforms models visdial metrics relative gain. improvement entirely ‘for free’ since additional annotations required much detailed observations consistent recent literature text-only dialog hypothesis improvement human responses diverse trained agents tend ‘hedge bets’ achieve reasonable log-likelihood noncally every question visdial accompanied candidate responses. log-likehood assigned a-bot answer decoder sort candidates report results tab. despite a-bot’s answer informative improvements visdial metrics minor. believe answers correct necessarily mimic human responses order deeper train variant frozen-q multi-task objective simultaneous ground truth answer supervision image guessing reward keep a-bot close human-like responses. weight loss model denoted frozen-q-multi performs better approaches visdial answering metrics improving best reported result visdial mean rank note gain entirely ‘free’ since additional annotations required human study. conducted human interpretability study measure whether humans easily understand q-bot-a-bot dialog imagediscriminative interactions are. show human subjects pool images agent dialog humans pick top- guesses image agents talking about. mean rank ground-truth image sl-pretrained agent dialog rl-full-qaf dialog. terms comparison respectively. thus under metrics humans easier guess unseen image based rl-full-qaf dialog exchanges shows agents trained within framework successfully develop image-discriminative language language interpretable; deviate english. summarize introduce novel training framework visually-grounded dialog agents posing cooperative ‘image guessing’ game agents. deep reinforcement learning learn policies agents end-to-end pixels multi-agent multi-round dialog game reward. demonstrate power framework completely ungrounded synthetic world agents communicate symbols pre-speciﬁed meanings bots invent communication protocol without human supervision. instantiate game visdial dataset pretrain supervised dialog data. ‘ﬁne-tuned’ agents signiﬁcantly outperform agents learn play other’s strengths remaining interpretable outside humans observers. awards career award award grant n--- award ictas junior faculty award google faculty research award amazon academic research award cloud credits research nvidia donations. supported grant n--- partially supported bradley postdoctoral fellowship. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied u.s. government sponsor.", "year": 2017}