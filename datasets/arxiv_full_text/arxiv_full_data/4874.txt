{"title": "Exploration Potential", "tag": ["cs.LG", "cs.AI"], "abstract": "We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.", "text": "introduce exploration potential quantity measures much reinforcement learning agent explored environment class. contrast information gain exploration potential takes problem’s reward structure account. leads exploration criterion necessary sufﬁcient asymptotic optimality experiments multi-armed bandits exploration potential illustrate different algorithms make tradeoff exploration exploitation. good exploration strategies currently major obstacle reinforcement learning state deep relies ε-greedy policies every time step agent takes random action probability. ε-greedy poor exploration strategy environments sparse rewards quite ineffective takes long agent randomwalks ﬁrst reward. sophisticated exploration strategies proposed using information gain environment pseudo-count practice exploration strategies employed adding exploration bonus reward signal methods require agent model environment formalize strategy ‘explore going model high uncertainty’ also model-free strategies like automatic discovery options proposed machado bowling however none explicit exploration strategies take problem’s reward structure account. intuitively want explore parts environment reward high less low. readily exposed optimistic policies like ucrl stochastic policies like psrl make exploration/exploitation tradeoff explicitly. paper propose exploration potential quantity measures rewarddirected exploration. consider model-based reinforcement learning partially fully observable domains. informally exploration potential bayes-expected absolute deviation value optimal policies. exploration potential similar information gain environment explicitly takes problem’s reward structure account. show leads exploration criterion necessary sufﬁcient asymptotic optimality reinforcement learning agent learns optimal limit exploration potential converges such exploration potential captures essence means ‘explore right amount’. another exploration quantity necessary sufﬁcient asymptotic optimality information gain optimal policy contrast exploration potential measured scale rewards making explicit value-of-information tradeoff difﬁcult. example consider -armed gaussian bandit problem means information content identical every arm. hence exploration strategy based maximizing information gain environment would query third easily identiﬁable suboptimal frequently provides information information useful solving reinforcement learning task. contrast exploration potential based exploration strategy concentrates exploration ﬁrst arms. reinforcement learning agent interacts environment cycles time step agent chooses action receives percept consisting observation reward cycle repeats denote history length abuse notation treat histories outcomes random variables. policy function mapping history action probability taking action seeing history environment function mapping history probability generating percept history <tat. policy environment generate probability measure inﬁnite histories expectation measure denoted assume nonparametric setting denote countable class environments containing true environment prior probability distribution observing history prior updated posterior wν/ρ∈m wρ). policy asymptotically optimal mean every exploration potential consider model-based reinforcement learning agent learns model environment. model estimate value candidate policy. concretely denote estimate value policy time step assume agent’s learning algorithm satisﬁes on-policy value convergence imply model environment converges truth learn predict value policy following. on-policy value convergence require learn predict off-policy i.e. value policies. particular might learn predict value µ-optimal policy intuitively captures amount exploration still required learned entire environment class. asymptotically posterior concentrates around environments compatible current environment. quantiﬁes well model understands value compatible environments’ optimal policies. switch greedy policy on-policy value convergence reﬂects well agent learned environment’s response bayes-optimal policy. generally following greedy policy yield enough exploration converge order policy asymptotically optimal combine exploration policy ensures gradually phase exploration switching -greedy policy. property agent compute current value thus check close higher prior belief true environment smaller value assume continuous policy argument. converges total variation sense actions converges deﬁnition environment admits strongly unique optimal policy µ-optimal policy policies continuous environment unique optimal policy ties maxa admitting strongly unique optimal policy even stronger requirement requires exist policies approach optimal value asymptotically take different actions ﬁnite-state unique optimal policy policy also strongly unique. proposition policy asymptotically optimal mean environment class environment admits strongly unique optimal policy µπ-probability proof. since asymptotically optimal mean since admits strongly unique optimal policy converges probability thus on-policy value convergence therefore don’t require condition strongly unique optimal policies policy could asymptotically optimal might another policy different optimal policy whose µ-value approaches optimal value policy could converge without converging section experiments multi-armed bernoulli bandits illustrate properties exploration potential. class bernoulli bandits time step agent chooses action true environment. since uncountable exploration potential deﬁned integral instead figure exploration potential time different bandit algorithms bernoulli bandit arms shaded regions correspond standard deviation. lower exploration potential means exploration. notable change slope around time steps stems fact takes long reliably distinguish ﬁrst arms. dashed line corresponds optimal asymptotic rate t−/. stops exploring around time step focuses exploitation thompson sampling round robin ε-greedy explore continuously optimal strategy never explores hence exploration potential decreases slightly. exploration potential naturally gives rise exploration strategy greedily minimize bayes-expected exploration potential algorithm strategy unsurprisingly explores algorithms measured exploration potential bandits also turns decent exploitation strategy focuses attention promising arms. empirical performance figure however minep generally good exploitation strategy complex environments like mdps. several variants deﬁnition exploration potential given deﬁnition conceivable. however often satisfy least properties make deﬁnition appealing. either break necessity sufﬁciency proofs thereof make hard compute. agent’s future policy. preserves necessesity sufﬁciency relies computing agent’s future policy. agent uses exploration potential taking actions deﬁnition becomes self-referential equation might hard solve. following dearden could consider convenient side-effect model-free therefore applies reinforcement learning algorithms. however case necessity guarantee requires additional condition agent’s policy converges greedy policy moreover remove dependence model since still need model class posterior. based recent successes approximating information gain hopeful exploration potential also approximated practice. since computing posterior costly complex reinforcement learning problems could generate environments estimate deﬁnition them. paper scratch surface exploration potential leave many open questions. correct deﬁnition? good rates converge minimizing efﬁcient exploration strategy? compute efﬁciently information gain?", "year": 2016}