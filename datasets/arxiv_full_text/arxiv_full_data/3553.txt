{"title": "OLÉ: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for  Deep Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are ubiquitous tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, such solutions carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this paper, we propose a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple and elegant geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss, being the first reported deep metric learning framework of its kind. Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative, and more robust. We demonstrate improved classification performance in general object recognition, plugging the proposed loss term into existing off-the-shelf architectures. In particular, we show the advantage of the proposed loss in the small data/model scenario, and we significantly advance the state-of-the-art on the Stanford STL-10 benchmark.", "text": "deep neural networks trained using softmax layer cross-entropy loss ubiquitous tools image classiﬁcation. naturally enforce intra-class similarity inter-class margin learned deep representations. simultaneously achieve goals different solutions proposed literature pairwise triplet losses. however solutions carry extra task selecting pairs triplets extra computational burden computing learning many combinations them. paper propose plug-and-play loss term deep networks explicitly reduces intra-class variance enforces inter-class margin simultaneously simple elegant geometric manner. class deep features collapsed learned linear subspace union them inter-class subspaces pushed orthogonal possible. proposed orthogonal low-rank embedding require carefully crafting pairs triplets samples training works standalone classiﬁcation loss ﬁrst reported deep metric learning framework kind. improved margin features different classes resulting deep networks generalize better discriminative robust. demonstrate improved classiﬁcation performance general object recognition plugging proposed loss term existing off-the-shelf architectures. particular show advantage proposed loss small data/model scenario signiﬁcantly advance state-of-the-art stanford stl- benchmark. strated impressive results learning useful representations difﬁcult tasks object classiﬁcation detection face identiﬁcation veriﬁcation name examples. dnns typically consist sequence convolutional and/or fully-connected layers non-linear activation functions produce deep feature vector classiﬁed last layer linear classiﬁer linear classiﬁer typically uses softmax function cross-entropy loss. combination referred softmax loss rest article. layer previous last linear classiﬁer referred deep feature layer. training standard softmax loss explicitly enforce embedding learned deep features samples class closer together away classes. improve discrimination power deep neural networks previous approaches tried enforce embedding auxiliary supervisory loss functions acting euclidean distances deep features metric learning techniques particularly popular face identiﬁcation domain representative examples pairwise loss triplet loss drawback approaches require careful selection pairs triplets samples well extra data processing. recently methods proposed overcome limitation enforcing intra-class compactness representations inside random training minibatch work propose improve discriminability neural network simple elegant plug-and-play loss term that acting deep feature layer encourages learned deep features class linear subspace time inter-class subspaces orthogonal figs. best knowledge ﬁrst time deep learning framework proposed simultaneously reduces intraclass variance increases inter-class margin without refigure barnes-hut-sne visualization deep feature embedding learned validation cifar using vgg-. softmax loss softmax loss only. separation classes increased low-rank structure recovered class. angle features validation samples ordered class without angle features collapsed inside class inter-class features orthogonal. best viewed electronic format. figure illustrative comparison loss softmax loss. show actual deep feature vectors validation images -class classiﬁcation problems scarce training data. produces intra-class compactness inter-class orthogonality able achieve better classiﬁcation performance softmax loss. classes cifar trained images class. layer hidden units used. subjects facescrub dataset trained images class average. layer hidden units used. text details. best viewed electronic format. intuition based following observations. first decision boundary softmax loss determined angle feature vector vectors corresponding class last linear classiﬁer since weights initialized randomly class vectors high probability orthogonal initialization typically remain training. moreover rectiﬁed linear unit last activation function deep features live positive orthant. therefore improve margin deep features embed orthogonal low-dimensional linear subspaces aligned classiﬁer vector class. adapt shallow feature orthogonalization technique deep networks. novel theoretical insight improve objective formulation optimization. outcome loss function plugged existing deep architecture deep feature layer. demonstrate thorough experimentation approach produces orthogonal deep representations lead better generalization face identiﬁcation also general object recognition. illustrate different datasets using four popular architectures resnets preresnets densenets demonstrate proposed technique particularly successful small data scenario. signiﬁcantly advance state-of-the-art stl- standard benchmark training images class show advantage standard softmax loss increases fewer training samples used. also show face recognition application improved discriminability network better detecting novel classes source code publicly available. paper organized follows. section discuss related work similarity preserving large margin deep networks. section motivate describe orthogonalization loss optimization well warming-up example. experimental results presented section conclude paper section ﬁrst attempts reduce intra-class similarity deep features increase inter-class separation metric learning based approaches goal minimize euclidean distance deep features class keeping ||·||∗ denotes matrix nuclear norm i.e. singular values matrix. nuclear norm convex envelope rank function unit ball matrices additional condition ||t|| originally adopted prevent trivial solution consider neural network whose last fully connected layer rc×d dimension deep features number classes. represents linear classiﬁer class initialized randomly rows orthogonal. consider deep representation image activation function deep feature layer element-wise maximum always lives positive orthant. observations deduced successful training network classiﬁer vectors remain orthogonal separation classes. therefore strategy learn large-margin deep features make intra-class features fall linear subspace aligned corresponding classiﬁcation vector features different classes orthogonal other. natural geometry learned features imposed standard last-layer classiﬁers today’s leading architectures. deﬁnition propose enforce aforementioned orthogonalization adapting deep learning setting. namely suppose given training minibatch samples deep embedding data parameterized classes apart. pioneering contrastive loss imposes constraint using siamese network architecture pairwise strategy particularly popular face identiﬁcation community later extended triplet loss triplet loss image representation simultaneously enforced close positive example class away negative example different class. main drawback approaches require carefully mining pairs triplets effectively enforce constraints. different strategy encourage intra-class compactness proposed centroid deep representation class updated training iteration euclidean distances centroids penalized. simple strategy produces compact clusters class although large margin clusters explicitly enforced. contrary method center loss cannot used standalone classiﬁcation loss since centroids tend collapse zero related approach estimates distribution representation class penalizes class distribution overlap. based observation softmax loss function angles deep features classiﬁer vectors novel family methods proposed operate angles euclidean distances. works propose custom versions softmax loss encourage features class smaller angle corresponding classiﬁcation vector standard softmax loss. improved margin produces notorious performance boosting respect standard network unsupervised learning domain recent family methods proposes enforce locally linear structure deep representations subspace clustering later applied deep representations properties arise naturally deep representations learned although imposed supervised manner. work related enforce intra-class compactness inside minibatch. however ﬁrst time objective function also simultaneously encourages inter-class orthogonality without need carefully craft pairs triplets. work stems orthogonalization technique used shallow learning proposed orthogonalization achieved linear transformation enforcing low-rank constraint features class high-rank constraint matrix features classes. precisely consider matrix column data point classes denotes horizontal concatenation. denote submatrix formed columns c-th class. linear transform proof. give proof classes extension multiple classes straightforward. corresponding feature matrices classes. uaσava uaσava ubσbvb ubσbvb decomposition subscript corresponds singular values larger threshold subscript remaining singular values. generic matrix zeroes whose size determined context simplicity. then ||a||∗ ||b||∗ ||||∗ since orthogonal matrices rightmatrix also orthogonal must orthogonal. since orthogonal submatrices implies columns must orthogonal other. then orthogonal thus fig. show simple illustrative examples result applying loss objective function neural network. compare result applying traditional softmax loss. ﬁrst experiment used classes cifar trained multilayer perceptron hidden layers neurons each ﬁnal layer dimension network trained epochs images class evaluated images class. respect drop linear transformation normalization restriction bound intra-class nuclear loss certain point intraclass norm reduction longer enforced thus avoiding collapse features zero always experiments paper. global minimum reached matrices orthogonal next describe simple descent direction optimizing backpropagation show direction vanishes orthogonalization achieved. optimization order optimize backpropagation need compute subgradient nuclear norm matrix. uσvt decomposition matrix small threshold value number singular values larger ﬁrst columns ﬁrst columns correspondingly remaining columns remaining columns then subdifferential nuclear norm intuitively avoid numerical issues dropping directions subgradient onto data matrix energy already improves upon formulation directions used. suppose deep feature matrix minibatch. feature submatrix class principal left right singular vectors. principal left right singular vectors deep feature matrix classes combined. then propose following descent direction second experiment used randomly chosen subject identities facescrub dataset pacino helen hunt sean bean. identity contains average images training validation. used layer neurons hidden layer trained epochs. mlps relu activation functions batch normalization weight decay trained adam learning rate comparison architecture hyperparameters shared objective function changed. evaluation -nearest-neighbor cosine distance training times architecture dataset kept model giving best classiﬁcation result validation set. fig. plot actual deep feature vectors obtained networks validation set. observe successful orthogonalization learned features using better classiﬁcation performance particular facescrub experiments number samples class limited. require carefully crafting pairs triplets samples works simply plug-and-play loss appended existing network architecture. compared large-margin softmax loss a-softmax loss loss restricted used softmax classiﬁer used standalone complement loss impose orthogonality layer network. compared center loss deep objective function encourages intra-class compactness inter-class separation simultaneously whereas former. also center loss cannot used standalone. collapses deep features linear subspaces. used conjunction softmax loss linear classiﬁers last layer natural form vector aligned linear subspace. combination standard softmax loss several popular deep network architectures different standard visual classiﬁcation datasets. also analyze effect proposed embedding. standard softmax loss second term proposed loss parameter controls weight loss; corresponds standard network training. means every weight network except weights last fully-connected layer linear classiﬁer. loss applied deep features penultimate layer. third term represents standard weight decay. values used parameters detailed below. datasets svhn. street view house numbers dataset contains colored images digits images training testing. additional unlabeled training images performed data augmentation. mnist. mnist database contains grayscale images digits training testing contain examples respectively. data augmentation used. cifar cifar. cifar datasets contain colored images object classes respectively. datasets contain images training testing. using data augmentation append sufﬁx dataset name. used standard data augmentation cifar pixel padding random cropping horizontal ﬂipping. stl-. self-taught learning dataset contains colored images object categories. designed semi-supervised unsupervised learning training images test images labels class. data augmentation consisted pixel padding random cropping horizontal ﬂipping. sufﬁx reporting results using data augmentation. facescrub-. facescrub- dataset obtained selecting identities facescrub dataset remaining classes used evaluating sample performance. split images ﬁrst subjects training testing datasets average images training images testing class preprocess images aligning facial landmarks using crop resulting aligned face images color. table summary deep network architectures used experiments. last fully-connected layer whose size depends number classes used shown. convolutional block. kernel size always pooling kernel size stride fully-connected layer. rx/y prx/y resnet preresnet blocks respectively. global average pooling layer. plain convolutional layer. dx/g densenet block. pre-bn convolutional block kernel size modules number output channels. number inner channels blocks growth rate blocks. text detailed block deﬁnitions. loss always applied output last layer shown table. evaluated architectures summarized table vgg. architecture consists blocks convolutional layers relu activation functions batch normalization linked max-pooling layers fully-connected layers end. vgg- vgg- publicly available implementation. vgg- used implementation allow direct comparison. vgg-face. vgg-face variant optimized face identiﬁcation vgg-face convolutional blocks ﬁrst layers dropout rate added layer size present layer improves performance tested models facescrub-. used caffe implementation authors ﬁne-tune weights provided them. novel layer initialized using xavier initialization resnet preresnet. resnets composed residual blocks. concatenation layers inside resnet block conv.-bn-conv.-bn-conv.-bn-relu. intermediate convolution layers typically fourth number channels input output convolution layers block table output block added input. preresnet architecture similar resnet except inside residual blocks order layers inverted bn-conv.-bn-conv.-bnconv.-relu. dropout used. variants used publicly available implementation. densenet. densenets composed three densenet blocks. blocks composed multiple pre-bn convolutional blocks small number output channels. inside densenet block input pre-bn convolutional block concatenation output previous pre-bn convolutional blocks. transition pre-bn convolutional block https//github.com/bearpaw/pytorch-classification https//github.com/wyiu/largemargin_softmax_loss http//www.robots.ox.ac.uk/˜vgg/software/vgg_face/ experiments except stl- facescrub used nesterov momentum optimization batch size started learning rate decreased ten-fold total training epochs. stl-/facescrub experiments used adam starting learning rate batch size used epochs architectures except densenets used epochs facescrub ﬁnetuning done epochs. weight decay parameter always except stl-+ facescrub fig. shows typical convergence curves. implemented loss custom layer caffe pytorch. additional computation time between training depending implementation hardware runs current implementation. adjusted parameter held-out validation training set. note magnitude loss depends size norm features matrices. selected value produced best result validation averaging runs example. retrained network entire training computed accuracy test training. account randomness training process repeated training full training times. figure learning curves. loss used standalone. data model loss accuracy used combination softmax loss resnet- cifar+. learning rate drops epochs. table shows resulting classiﬁcation performance without experiments found value validation generalization network improved. reference include last column performance published articles presenting corresponding architecture datasets. note could implementation differences. compared state-of-the-art intra-class compactness method using vgg- cifar lowest classiﬁcation error obtained compared reported compared network standard softmax loss relative reduction error obtained adding loss. improvement generalization performance important scarce training data available. facescrub- experiment less samples available class average error dropped fig. illustrates advantage using signiﬁcant fewer training data available. ﬁxed trained cnn- stl- without data augmentation. varied number samples training samples class repeating experiment times. stl-+ experiment lowest classiﬁcation error rate test obtain signiﬁcantly lower reported state-of-the-art error rate note uses training data data augmentation procedure. subsection analyze facescrub- experiment show loss improves novelty detection capability network. goal novelty detection identify images test belong categories training set. identities facescrub dataset took identities form facescrub- dataset left remaining identities novel classes used assess novelty detection performance. images novel classes testing ideally since novel identities none known subjects class scores low. observe case using whereas using softmax loss typically class known conﬁdent softmax score fig. show this varied threshold softmax scores deﬁned false positive ratio number images novel classes whose softmax score higher fig. plot model accuracy known subjects fpr. using loss model able reject unknown classes without signiﬁcant loss accuracy known fig. shows histogram softmax scores images novel classes using scores concentrated around reﬂecting conﬁdence network gives novel classes. hand network trained softmax loss gives high conﬁdence scores images novel classes fig. veriﬁed deep network lose face representation power novel classes running standard veriﬁcation benchmark labeled faces wild observed similar veriﬁcation performance models without loss respectively. figure accuracy versus number samples. improved generalization using signiﬁcant fewer training samples available. experiment used stl- without data augmentation average ofer runs. figure application novelty detection. accuracy known identities versus ratio images novel classes wrongly classiﬁed known varying threshold class scores. using loss false positives avoided without losing classiﬁcation performance known classes. histogram maximum class scores samples novel classes without respectively. scores concentrated towards whereas false high conﬁdence scores generally obtained. visualization obtained embedding validation cifar. intra-class low-rank minimization reduces intra-class variance dimension. overall rank maximization produces margin classes. fig. show angle deep features validation images ordered class. relative angle mostly images class images different class. hand standard softmax loss learned deep features larger intra-class spread inter-class angles always orthogonal. finally show spectral decomposition deep feature matrices cifar validation fig. deep features concentrated along principal dimensions corresponding learned orthogonal linear subspaces. softmax loss deep feature matrix energy distributed along many directions reﬂecting spreading deep features vectors. figure spectral analysis deep feature matrix obtained cifar validation data using vgg-. plot normalized singular values feature matrix without using deep features concentrated along strong dimensions embedding space corresponding linear subspaces features compacted. standard softmax energy distributed evenly. proposed novel objective function deep networks simultaneously encourages intra-class compactness inter-class separation deep features. former imposed low-rank constraint latter orthogonalization constraint. proposed loss used standalone classiﬁcation loss combination standard softmax loss improved performance. showed produces discriminative deep networks deep representations whose energy embedding space concentrated dimensions. classiﬁcation particularly effective training data scarce. using signiﬁcantly advance state-of-the-art classiﬁcation performance standard stl- benchmark. proposed loss introduces paradigm deep metric learning believe valuable tool applications orthogonality deep representations required.", "year": 2017}