{"title": "Recurrent Neural Network Encoder with Attention for Community Question  Answering", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method.", "text": "apply general recurrent neural network encoder framework community question answering tasks. approach rely linguistic processing applied different languages domains. improvements observed extend encoders neural attention mechanism encourages reasoning entire sequences. deal practical issues data sparsity imbalanced labels apply various techniques transfer learning multitask learning. experiments semeval- task show improvement score compared information retrieval-based approach achieve comparable performance strong handcrafted feature-based method. community question answering paradigm provides forums users answer questions topic barely restrictions. websites attracted great number users accumulated large collection question-comment threads generated users. however restriction results high variation answer quality makes time-consuming search useful information existing content. would therefore valuable automate procedure ranking related questions comments users question looking solutions comments existing question. automation forums divided three tasks question-comment relevance question-question relevance questionexternal comment relevance might think classic retrieval models like language models information retrieval could solve tasks. however challenge tasks users used expressing similar meanings different words creates gaps matching questions based common words. challenges include informal usage language highly diverse content comments variation length questions comments. overcome issues previous work relied heavily additional features reasoning capa neural bilities. attention-based model proposed automatically recognizing entailment relations pairs natural language sentences. study ﬁrst modify model three tasks. also extend framework jointly trained model external resources available i.e. selecting external comment know question external comment answers ultimate objective classify relevant questions comments without complicated handcrafted features. applying rnn-based encoders avoid heavily engineered features learn representation automatically. addition attention mechanism augments encoders ability attend past outputs directly. becomes helpful encoding longer sequences since view existing annotated corpora generally small properly train end-to-end neural network. address this investigate transfer learning pretraining recurrent systems corpora also generating additional instances existing corpus. earlier work community question answering relied heavily feature engineering linguistic tools external resource. utilized rich non-textual syntactically analyzed question extracted name entity features. demonstrated textual entailment system enhance task casting question answering logical entailment. recent work incorporated word vector feature extraction system based designed different distance metric question answer approaches showed effectiveness difﬁcult generalize common tasks since linguistic tools external resource restrictive languages features highly customized task. recent work answer selection also involved neural networks. used lstm construct joint vector based question answer converted learning rank problem. proposed several convolutional neural network architectures cqa. method differs encoder applied adding attention mechanism jointly learn words question focus hence available conduct qualitative analysis. classiﬁcation feed extracted vector feed-forward neural network directly instead using mean/max pooling time steps. anism. next explain encode pair sentences dense vector predicting relationships using lstm attention mechanism. finally apply models predict question-question similarity question-comment similarity question-external comment similarity. lstms shown great success many different ﬁelds. lstm unit contains memory cell self-connections well three multiplicative gates control information ﬂow. given input vector previous hidden outputs previous cell state lstm units operate follows approach ﬁrst encodes arbitrary length input sequence ﬁxed-length dense vector used input subsequent classiﬁcation models initialize hidden state secondary decoder. however requirement compress necessary information single ﬁxed length vector problematic. neural attention model recently proposed alleviate issue enabling network attend past outputs decoding. thus encoder longer needs represent entire sequence vector; encodes information sequence vectors adaptively chooses subset vectors decoding. tasks pair objects relationship relevant/irrelevant. left side figure shows intuitive predict relationships using rnns. parallel lstms encode objects independently concatenate outputs input feed-forward neural network softmax output layer classiﬁcation. representations objects generated independently manner. however interested relationship instead object representations themselves. therefore consider serialized lstm-encoder model right side figure similar also allows augmented feature input classiﬁer. adding attention mechanism encoder allow second lstm attend sequence output vectors ﬁrst lstm hence generate weighted representation ﬁrst object according objects. last output second lstm sequence output vectors ﬁrst object. weighted representation ﬁrst object object pair’s relationship. parametrize model using another fnn. note framework also allow augmented features enhance classiﬁer. ﬁnal input classiﬁer well augmented features. modeling question-external comments task addition original question external comment question relc commented also given incorporate extra information consider multitask learning framework jointly learns predict relationships three pairs figure shows framework three lower models separate serialized lstm-encoders three respective object pairs whereas upper model takes input concatenation outputs three encoders predicts relationships three pairs. speciﬁcally output layer consists three softmax layers intended predict relationship particular pair. evaluate approach three tasks. datasets provided semeval task data organized follows original questions question related question related question comments. therefore task total number question-comment pairs. task question-question pairs. task question-comment pairs. test dataset includes questions related questions comments overlap baseline system figure illustrates baseline systems. ir-based system scored google search engine. question-comment pair question-question pair google’s rank calculate map. training target data expect google used many external resources produce ranks. featurerich system proposed semeval-. approach compute text-based vector-based metadata-based rank-based features pre-processed data. features used linear comment selection. system includes traditional handcrafted features rnn-based features also includes information system believe strong baseline compare model. encoder theano table gives list hyper-parameters considered. suggested hyper-parameters lstms tuned independently. tuned parameter separately development simply picked best setting. experiments show using word embeddings google-news provides modest improvements ﬁxing embedding degrades performance lot. also using separate parameters lstms better sharing. optimization method adadelta converged faster adagrad gives better performance. note parameters tuned task simply applied task saving computation also task well-deﬁned compared terms dataset size label balance. preliminary results table shows initial results using encoder different tasks. observe attention model always gets better results without attention especially task however model achieves score. task even worse random baseline. believe reason task pairs training limited training reasonable neural network. task believe problem highly imbalanced data. since related comments directly comment original question comments labeled irrelevant original robust parameter initialization improve models trained limited data external data pretrain neural network. therefore considered different datasets task. cross-domain stanford natural language inference corpus huge amount cleaned premise hypothesis pairs. unfortunately pairs different task. relationship premise hypothesis similar relation questions comments also different. utilize data ﬁrst trained model auxiliary data removed softmax layer. that retrain network using target data softmax layer randomly initialized. task snli cannot improve scores. actually slightly hurts performance. surmise probably domain different. investigation needed example could parameter embedding layers etc. task snli yields slight improvement task could give that. improvement observed task pretraining task also better using snli summary in-domain pretraining seems better overall improvement less expected especially task limited target data. make conclusion since investigation needed. mentioned section also explored multitask learning framework jointly learns predict relationships three tasks. main task auxiliary tasks. score improve increases believe because tasks balanced labels improves shared parameters task many sources external question-answer pairs could used tasks. example webquestion simplequestions dataset positive examples task easily create negative examples initial experiments indicate easy overﬁt obvious negative examples. believe negative since external data seems hurt performance in-domain pairs enhance task task task relative question relative question relevant original question positive sample either irrelevant relevant negative sample this samples task increase applying method score increased slightly score improved enhance system incorporate vector original ranking additional feature classiﬁer. table shows results. comparing models without augmented features large improvement task score task degrades slightly improves. might task already substantial amount training data. table gives ﬁnal comparison different models since baseline models additional data table system also restricted provided training data. task enough training data single system already performs better strong feature-rich based system. task since limited training data given feature-rich based system system worse system. task system also comparable results feature-rich based system. simple system combination system system combined system addition quantitative analysis natural qualitatively evaluate performance attention mechanism visualizing weight distribution instance. randomly picked several instances test task sentence lengths moderate demonstration. examples shown figure categorized short long noisy sentences discussion. darker blue patch refers larger weight relative words sentence. figure illustrates examples whose questions relatively short. comments corresponding questions ...snorkeling days coast dukhan... doha international airport.... observe model successfully learns focus representative part question pertaining classifying relationship place snorkeling ﬁrst example place visited qatar second example. long sentences figure investigate examples longer questions contain words. interestingly distribution weights become uniform; model still focuses attention small number words example puppy mall hectic driving doha insurance quite costly. additionally words appear frequently carry little information classiﬁcation assigned small weights i/we/my is/am like noisy sentence open nature forums content noisy. figure example excessive usage question marks. again model exhibits robustness allocating weights noise symbols therefore excludes noninformative content. paper demonstrate general encoder framework applied community question answering tasks. adding neural attention mechanism showed quantitatively qualitatively attention improve encoder framework. deal realistic scenario expanded framework incorporate metadata augmented inputs classiﬁer pretrained models larger datasets increasing stability performance. model consistently better comparable strong feature-rich baseline system superior ir-based system reasonable amount training data. model complimentary ir-based system uses vast amounts external resources trained general purposes. combining systems exceeds feature-rich irbased system three tasks. moreover approach also language independent. also performed preliminary experiments arabic portion semeval- task. results competitive handtuned strong baseline semeval-.", "year": 2016}