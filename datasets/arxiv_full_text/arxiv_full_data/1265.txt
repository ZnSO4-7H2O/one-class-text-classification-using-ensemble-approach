{"title": "Dynamic Weight Alignment for Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper, we propose a method of improving Convolutional Neural Networks (CNN) by determining the optimal alignment of weights and inputs using dynamic programming. Conventional CNNs convolve learnable shared weights, or filters, across the input data. The filters use a linear matching of weights to inputs using an inner product between the filter and a window of the input. However, it is possible that there exists a more optimal alignment of weights. Thus, we propose the use of Dynamic Time Warping (DTW) to dynamically align the weights to optimized input elements. This dynamic alignment is useful for time series recognition due to the complexities of temporal relations and temporal distortions. We demonstrate the effectiveness of the proposed architecture on the Unipen online handwritten digit and character datasets, the UCI Spoken Arabic Digit dataset, and the UCI Activities of Daily Life dataset.", "text": "feature using sparsely connected shared shared weights convolution. weights ﬁlter feature extractor maintains structural aspects input. furthermore shared weights linearly aligned corresponding window value input. interaction much like linear inner product linear kernel result large ﬁlter similar receptive ﬁeld small different. difﬁculties arise time series recognition issues temporal distortions varying rates. linear alignment assumes element receptive ﬁeld correspond directly weight ﬁlter one-to-one fashion. conversely time series patterns linear relationships elements. rnns address issues recurrent connections store persistent information across time without strict time step alignment. addition lstms capable overcoming local rate inconsistencies gates control learning develop long-term dependencies. however feedforward network models including cnns paper propose method improving convolutional neural networks determining optimal alignment weights inputs using dynamic programming. conventional cnns convolve learnable shared weights ﬁlters across input data. ﬁlters linear matching weights inputs using inner product ﬁlter window input. however possible exists optimal alignment weights compared linear matching. thus propose dynamic time warping dynamically align weights optimized input elements. dynamic alignment useful time series recognition complexities temporal relations temporal distortions. demonstrate effectiveness proposed architecture unipen online handwritten digit character datasets spoken arabic digit dataset activities daily life dataset. neural networks perceptron learning models become powerful machine learning pattern recognition. early models introduced recently achieved state-of-the-art results improvements data availability computational power convolutional neural networks particular achieved state-of-the-art results many areas image recognition ofﬂine handwritten digit recognition text digit recognition object recognition addition image domain models used time series patterns. predecessor cnns time delay neural networks used time-delay windows similar ﬁlters cnns. cnns also used classify time series embedding sequences vectors matrices however recent successes recurrent neural networks particular long short-term overcome this propose method ﬁnding optimal alignment weights inputs using dynamic programming namely dynamic time warping determines optimal distance between time series patterns elastically matching elements using dynamic programming along constrained path cost matrix. combines local distances between matched elements determine optimized global distance. exploit elastic matching utility align weights ﬁlter elements corresponding receptive ﬁeld create efﬁcient feature extractors cnns. contribution paper twofold. first propose novel method aligning weights within convolutional ﬁlters cnns. alignment weights allows cnns effectively tackle time series patterns used time series recognition. second demonstrate effectiveness proposed method multiple time series datasets including unipen online handwritten digit character datasets spoken arabic digit dataset activities daily life dataset. perform comparative study using traditional reveal beneﬁts proposed weight alignment. addition compare proposed method state-ofthe-art methods time series classiﬁcation. rest paper organized follows. section review neural methods time series recognition. section details proposed dynamic weight alignment method cnns. section reports experiments results section discusses advantages dynamic weight alignment. finally section provide conclusion future work. neural networks perceptron learning methods time series recognition extensively explored literature. many classic models used time series forecasting artiﬁcial neural networks provided sliding window discrete signal tasked predicting future time steps. however methods learn rigid relationships temporal features susceptible temporal distortions shift variance. tdnns classical feed-forward neural networks designed address shift variance introducing wider sliding window receptive ﬁelds represent multiple points time. windows time delays passed learned ﬁlters create subsequent feature time series. tdnns successful speech phoneme recognition cnns continue idea learned ﬁlters permeated across structural input domain images. however cnns used time series recognition considering time steps dimension convolutions. zheng cnns subsequences created multivariate time series. also attempts classify time series patterns embedding matrices classiﬁcation cnn. wavenet employs dilated causal convolutional layers generate audio. causal convolutions relatively small receptive ﬁelds however convolutions dilated meaning higher layers skip input values increase effective range receptive ﬁeld. wavenet architecture stacks multiple dilated causal convolutions large receptive ﬁeld considerate order. notable feedforward network architectures proposed address characteristics features time series patterns. instance mlps applied wavelet transforms arma time series data forecasting classiﬁcation. spiking neurons create sparse representations using brief spikes time series data. feedforward networks recurrent models like rnns successful time series recognition. similar models rnns given sliding window input time series. however unlike feedforward networks rnns recurrent connections stored hidden states maintain dependence time. lstms improvement rnns gates control amount learned information solve vanishing gradient problem lstms successful many time series sequence domains handwriting speech natural language processing dynamic neural networks emerging ﬁeld neural model learning generally approaches dynamically embedding trained parameters network using dynamic connections. dynamic filter networks ﬁlter-generating networks produce ﬁlters dynamically used depending input. klein similarly uses ﬁlters vary depending input. dynamic convolutional neural networks dynamic k-max pooling simplify cnns sentence modeling. distinction models proposed method dynamic programming optimize weight alignment inside nonlinear convolutions. dtw-nns similarly nonlinear inner product neural networks. difference dtw-nn proposed model dtw-nn uses euclidean distances learned prototype sequence input. means weights pseudo weights whereas described paper used true weights. addition shared weight dynamic time warping asymmetric positive semi-deﬁnite similarity function traditionally used distance measure sequences. calculated using dynamic programming determine optimal match elements sequences. matching elements sequences warped time dimension align similar features time series. distance shown successful distance measure time series recognition ﬁnds total cost optimal warping path local cost matrix using dynamic programming. given discrete time series sequence length sequence index time step elements time step dtw-distance global summation local distances pairwise element matches. namely dtw-distance denoted pair matched indices corresponding original indices respectively. contains matched pairs additionally matched pairs contain repeated skipped indices original sequences therefore nonlinear correspondence local distance function elements. distance measure similarity function unlike kernel function integrated kernel functions support vector machines replacement inner product fully-connected neural network layers dynamic time warping neural networks dynamic weight alignment shared weights. deﬁne distance function product consider nonlinear inner product. unlike conventional convolution deﬁned result would become nonlinear product weights window previous layer. namely propose using determine x+f}) model instead sparsely aligned network. advantage convolution output node retains temporal qualities instead scalar output allows ability stack multiple nonlinear inner product convolutional layers. goal proposed method exploit dynamic programming determine optimal alignment weights convolutional layers cnns. using adapt conventional linear inner product convolution optimize connections convolutional ﬁlter weights inputs. figure demonstrates difference conventional convolutional layer linear weight alignment proposed dynamic weight alignment. convolutional neural networks feature cnns maintain sparse connections promote local connectivity. element convolutional node receptive ﬁeld small number locally neighboring inputs. besides enforcing local relations sparsity reduces number parameters needed calculated allows larger input sizes. parameter sharing reduces number parameters required training cnn. idea weights convolutional layer shared corresponding output element’s local receptive ﬁeld. forward calculation convolutional layer identical convolution operation shared weights ﬁlter output feature map. element convolutional layer previous layer index ﬁlter window size. denote shared weights previous layer activations bias respectively. words inner product shared weights window previous layer inner product linearly matches weights inputs within window. however plausible exist instances particular weights matched optimal inputs example noisy elements feature translation scale variance within ﬁlter. dynamic weight alignment figure comparison conventional linear convolution proposed convolution dynamic weight alignment illustrate convolutions four weights stride layer previous layer elements layer resulting feature convolution elements black product corresponding weight input blue circle products. order train network stochastic gradient decent determine gradients weights respect error order update weights minimize loss. gradient shared weights respect error partial derivative loss function. conventional linear relationship calculated simply. however given nonlinearity weight alignment calculation gradient reliant matched elements determined forward pass create nonlinearity weight alignment ﬁlters allows relaxed matching determined warping path within constraints. constraint slope constraint applied warping path prevent overﬁtting. propose using asymmetric slope constraint modiﬁcation slope constraint proposed itakura traditional implementation local distance metric dissimilarity function goal minimize global distance. however using inner product space cost matrix local distance metric represents similarity therefore maximal product. thus slope constraint deﬁned recurrent function addition many possible slope local constraints researched however proposed method uses slope constraint ensures warping path monotonic number matches always equal number elements ﬁlter. experiment implement four layer dynamically aligned cnn. ﬁrst hidden layer convolutional layer nodes proposed dynamically aligned ﬁlters. addition batch normalization results convolutional layer. second third layers fully-connected layers hyperbolic tangent tanh activation nodes respectively. ﬁnal output layer uses softmax number outputs corresponding number classes. batch normalization. batch normalization helps generalization ability neural network aims reduce effects internal covariate shift forces higher layers network drift causes whitening inputs. solve problem batch normalization shifts values mean zero variance one. proposed method uses batch normalization described convolutional layer. learning rate. using learning rate progressive decay gradient decent network quickly learn weight parameters beginning slowly reﬁne later training. learning rate iteration deﬁned evaluate proposed method compare accuracy comparison methods. report classiﬁcation results literature well evaluate datasets established state-of-the-art neural network methods. online handwritten digits character evaluation compare results neural network models regional-fuzzy representation dtw-nn methods using gaussian kernel hidden markov model also compare results fuzzy adaptive system artbased model kohonen-perceptron reported neural network solution using wavelet neural network well models using tree distribution model continuous hidden markov model second-order derivative mfcc activities daily life dataset compare results original dataset proposal bruno modeled accelerometer data using gaussian mixture modeling gaussian mixture regression also report best results kanna used decision tree. evaluated baselines. evaluated baselines designed direct comparisons proposed method. lstm used established state-of-the-art neural network method sequence time series recognition traditional used direct comparison using standard convolutional layers. comparative models provided exact training test validation sets proposed method. furthermore evaluated methods batch size number iterations proposed method respective trials. second comparative evaluation using exact hyperparameters proposed method standard convolutional nodes. constructed -node convolutional layer window size stride equal proposed method evaluation batch normalization fully-connected tanh layers size respectively softmax output layer. convolutional layer weights progressive decay learning rate evaluation proposed method static learning rate fully-connected layers. used randomly selected digits classes experiment randomly selected characters classes experiments. experiments preprocessed patterns re-sampling sequence elements scaled square deﬁned corner coordinates datasets divided three sets training test data training data patterns side training validation set. given experimental dataset made sequences dimensional coordinates ﬁlters correspond accordingly. thus convolutional ﬁlters size applied input sequences stride stride used reduce redundant information decrease computation time. ﬁlters initialized using gaussian distribution minimum value maximum within similar range dataset. experiment uses batch gradient decent batch size iterations. results experiments unipen datasets shown table proposed method achieved test accuracy digits uppercase characters lowercase characters iterations training surpassing conventional lstm. furthermore proposed method surpassed comparative methods except dtw-nn unipen trial. audio prime example time series speech phoneme classiﬁcation essential speech recognition. popular method audio recognition mel-frequency cepstrum coefﬁcients mfccs transforms audio signals discrete time series using mel-frequency cepstrum representations number dimensions equaling number coefﬁcients. used spoken arabic digit data experiments. dataset consists -frequency mfccs spoken arabic numerals classes. samples taken sampling rate hamming window ﬁlters pre-emphasized .z−. spoken arabic digit data pre-deﬁned division data speaker-independent training test patterns patterns respectively. patterns training separated validation training. experiments preprocessed patterns re-sampling sequence ele. ments scaled spoken arabic digit data -frequency mfccs therefore data time steps dimensional data. convolutional ﬁlters stride minimum value maximum data. trained proposed method batch size iterations. proposed method accuracy predeﬁned test speaker independent mfcc spoken arabic digits. comparing results table noted proposed method outperformed chmm nonetheless proposed dynamically aligned weights performed better conventional lstm reported results. addition chmm method tailored mfcc speech recognition whereas proposed method works well domains. common method studying wearable sensors. example recognition wrist-worn accelerometer data set. dataset contains recorded accelerometer data split classes patterns each. actions include climbing stairs drinking glass getting pouring water glass sitting chair standing chair walking. recorded three spacial dimensions. acceleration gravity. time series also sampled frames. original experiment uses median ﬁlter however found unnecessary used data experiment. training dataset divided random training patterns validation patterns test patterns. dataset made tri-axial accelerometer data. length samples frames used larger convolutional ﬁlter stride initiated similarly gaussian distribution minimum value maximum three dimensions. finally batch size used small dataset. similar datasets results table show proposed method improved accuracy conventional linearly aligned weights respectively. result proposed dynamically aligned weights exceeds accuracy original dataset proposal using gmr. figure comparison test accuracies unipen online handwritten datasets conventional proposed dynamic weight alignment number training iterations. section discuss comparisons proposed method evaluated comparisons. also offer explanations improved accuracy. additional results confusion matrices accuracy comparisons provided supplementary materials. dynamically aligned weights versus lstm. lstms state-of-the-art sequence time series neural network model. rnns designed maintain long-term dependencies gating. online handwriting experiments lstm performed poorly compared cnns. reason limited performance lstm individual element datasets contain significant amount information model needs know elements work together form spacial structures. example large tri-axial accelerometer data individual long-term dependencies important local global structures whereas cnns excels. another reason poor performance dataset could amount training data high amounts noise high variation patterns within class. however lstm comparatively well spoken arabic digits. differences conventional linearly aligned weights proposed method dynamically aligned weights. table reveals reduction error unipen reduction reduction addition increased accuracy observe fig. compared conventional proposed method achieves higher accuracy parts training especially early stages. indicates nonlinear alignment optimizes weights generalization converges. efﬁcient feature extraction. explanation improved accuracy aligning weights similar corresponding inputs efﬁcient conventional linear matching. weights convolutional layer learned learned like ﬁlter feature extraction purpose using dynamically aligned weights warp assignment weights similar corresponding inputs. algorithm skips redundant noisy inputs ensuring target features detected. features extracted ﬁlter enhanced. improved discriminability layer level. figure visualization outputs convolutional layers test using t-distributed stochastic neighbor embedding t-sne algorithm visualize high-dimensional data allows visualization response convolutional node sample low-dimensional space. ﬁgure shows classes represented different colors discriminable dynamic weight alignment. figure test samples using conventional shows many classes made multiple small clusters whereas fig. generally contains larger uniﬁed clusters. example classes made multiple clusters uniﬁed t-sne ﬁgures evaluations found supplementary materials. higher discriminability means would less confusion higher layers. paper proposed novel method optimizing weights within convolutional ﬁlter dynamic programming. implemented method sequence element alignment weights ﬁlter inputs corresponding receptive ﬁeld. deﬁning local distance function product show possible less rigid nonlinear inner product. weights convolutional layer aligned maximize relationship data previous layer. figure t-sne visualization output convolutional node unipen experiment. point test sample colored indicate class membership. conventional proposed weight alignment. show proposed model able tackle time series pattern recognition. evaluated proposed model unipen online handwritten digit character datasets. results show n-class classiﬁcation test accuracies online handwritten digits uppercase characters lowercase characters. addition handwriting demonstrate effectiveness proposed model types data including -coefﬁcient mfcc speech arabic digits tri-axial accelerometer data. spoken arabic digit evaluate achieved accuracy accelerometer-based evaluation achieved accuracy. shows proposed method viable feedforward neural network model time series recognition effective method optimizing convolutional ﬁlter cnns. dynamic weight alignment convolutions cnns promising area research. proposed method applied based models deep cnns fully convolutional networks. future plan extending method time series datasets well domains images.", "year": 2017}