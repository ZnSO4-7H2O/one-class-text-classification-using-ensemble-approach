{"title": "Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Missing data and noisy observations pose significant challenges for reliably predicting events from irregularly sampled multivariate time series (longitudinal) data. Imputation methods, which are typically used for completing the data prior to event prediction, lack a principled mechanism to account for the uncertainty due to missingness. Alternatively, state-of-the-art joint modeling techniques can be used for jointly modeling the longitudinal and event data and compute event probabilities conditioned on the longitudinal observations. These approaches, however, make strong parametric assumptions and do not easily scale to multivariate signals with many observations. Our proposed approach consists of several key innovations. First, we develop a flexible and scalable joint model based upon sparse multiple-output Gaussian processes. Unlike state-of-the-art joint models, the proposed model can explain highly challenging structure including non-Gaussian noise while scaling to large data. Second, we derive an optimal policy for predicting events using the distribution of the event occurrence estimated by the joint model. The derived policy trades-off the cost of a delayed detection versus incorrect assessments and abstains from making decisions when the estimated event probability does not satisfy the derived confidence criteria. Experiments on a large dataset show that the proposed framework significantly outperforms state-of-the-art techniques in event prediction.", "text": "abstract—missing data noisy observations pose signiﬁcant challenges reliably predicting events irregularly sampled multivariate time series data. imputation methods typically used completing data prior event prediction lack principled mechanism account uncertainty missingness. alternatively state-of-the-art joint modeling techniques used jointly modeling longitudinal event data compute event probabilities conditioned longitudinal observations. approaches however make strong parametric assumptions easily scale multivariate signals many observations. proposed approach consists several innovations. first develop ﬂexible scalable joint model based upon sparse multiple-output gaussian processes. unlike state-of-the-art joint models proposed model explain highly challenging structure including non-gaussian noise scaling large data. second derive optimal policy predicting events using distribution event occurrence estimated joint model. derived policy trades-off cost delayed detection versus incorrect assessments abstains making decisions estimated event probability satisfy derived conﬁdence criteria. experiments large dataset show proposed framework signiﬁcantly outperforms state-of-the-art techniques event prediction. noisy multivariate longitudinal data—repeated observations irregularly-sampled example application consider challenge reliably predicting impending adverse events hospital. many lifethreatening adverse events sepsis cardiac arrest treatable detected early towards this leverage vast number signals—e.g. heart rate respiratory rate blood cell counts creatinine—that already recorded clinicians time track individual’s health status. however repeated observations signal recorded regular intervals. instead choice record driven clinician’s index suspicion. example past observation blood cell count suggests individual’s health deteriorating likely order test frequently leading frequent observations. further different tests ordered different times leading different patterns missingness across different signals problems similar nature arise monitoring health data centers predicting failures based longitudinal data product system usage statistics statistics task event prediction cast framework time-to-event survival analysis here main classes approaches. ﬁrst longitudinal event data modeled jointly conditional distribution event probability obtained given longitudinal data observed given time; e.g. rizopoulos example posits linear mixed-effects model longitudinal data. time-to-event data linked longitudinal data parameters. thus given past longitudinal data time compute conditional distribution probability occurrence event within future interval futoma allow ﬂexible model makes fewer parametric assumptions speciﬁcally mixture gaussian processes focus single time series. general state-of-the-art techniques jointmodeling longitudinal event data require making strong parametric assumptions form longitudinal data order scale multiple signals many observations. need making strong parametric assumptions limits applicability challenging time series alternative class approaches uses two-stage modeling features computed longitudinal data separate timeto-event predictor learned given features signals irregularly sampled missing values completed using imputation point estimates features extracted completed data timeto-event model issue latter class approaches principled means accounting uncertainty missingness. example features estimated reliably regions dense observations compared regions measurements. ignoring uncertainty missingness resulting event predictor likely trigger false missed detections regions unreliable feature estimates. series classiﬁcation task. requires transforming event data sequence binary labels event likely occur within given horizon otherwise. however binarize event data must assume ﬁxed horizon further lose valuable information precise timing event prediction sliding window used computing point estimates features using imputation techniques complete data using model parameters ﬁtting sophisticated probabilistic model time-series data methods suffer similar shortcomings two-stage time-to-event analysis approaches described above fully leverage uncertainty missingness longitudinal data. discuss works detail section paper explore following question exploit uncertainty missingness longitudinal data improve reliability predicting future events? propose reliable event prediction framework comprising innovations. propose ﬂexible bayesian nonparametric model jointly modeling high-dimensional multivariate longitudinal time-to-event data. speciﬁcally model used computing probability occurrence event within given horizon conditioned longitudinal data observed compared existing state-of-the-art joint modeling proposed approach scales large data without making strong parametric assumptions form longitudinal data. speciﬁcally relax need assume simple parametric models time series data. multiple-output gaussian processes model multivariate longitudinal data. accounts nontrivial correlations across time series ﬂexibly capturing structure within series. further order facilitate scalable learning inference propose stochastic variational inference algorithm leverages sparse-gp techniques. reduces complexity inference cubic number observations signal number signals linear decision-theoretic approach derive optimal detector uses predicted event probability associated uncertainty trade-off cost delayed detection versus cost making incorrect assessments. shown example detector output fig. detector choose wait order avoid cost raising false alarm. others explored notions reliable prediction. instance classiﬁcation abstention studied decision making methods based point-estimates features event probabilities. others considered reliable prediction classiﬁcation segmented video frames containing single class. approaches goal determine class label early possible rest paper organized follows. section reviews survival analysis joint models. section present joint modeling framework. then section develop robust prediction policy. review related fig. shows estimates joint-model longitudinal time-to-event data. data shaded region used estimate probability occurrence event. further within given distribution event probability shown right. describes observed event data latent deterioration state shows example pattern lead observed events. here patient gradually transitions healthy becoming sick worse enough symptoms associated event—in case septic shock—become visible. desired output ideally system identify patient deteriorating soon starts occur. detector output positive prediction shown axis. color indicates whether prediction correct wrong given time detector choose predict. shown intervals neither positive negative prediction made. here detection much prior event time considered false detection. work section section show results challenging dataset patients admitted hospital task predicting deadly adverse event called septic shock. finally concluding remarks section background survival analysis section review survival analysis joint models. survival analysis class statistical models developed predicting analyzing survival time remaining time event interest happens. includes instance predicting time mechanical system fails patient experiences septic shock. main focus survival analysis computing survival probability; i.e. probability individual survives certain period time given information observed far. formally individual non-negative continuous random variable representing longitudinal data time individual longitudinal component models time estimates distribution features series conditioned given distribution time-to-event component models survival data estimates event probability. note features random variables distribution random quantity; i.e. every realization features drawn computes different estimate event probability. result random induces distribution distribution obtained distribution using change-of-variable techniques situations exact event time individuals observed censoring. consider types censoring right censoring interval censoring. right censoring know event happen time exact time event unknown. similarly interval censoring know event happened within time window given partial information write likelihood time-toevent component tli} dropped explicit conditioning brevity. value hazard function time depends history features alternatively hazard rate deﬁned function instantaneous features; i.e. exp)∀s latter requires accurate model extrapolating features duration hours compute survival probabilchallenging therefore include dependence instantaneous features problem domain. training evaluate likelihood individual series grid points grid point likelihood evaluated based longitudinal data observed time timeto-event component survival time hazard occurrence time impending event. survival analysis random variable usually characterized using survival function i.e. probability individual survives time given survival function compute probability density function survival analysis distribution usually speciﬁed terms hazard function deﬁned instantaneous probability event happens conditioned information individual survived time i.e. special case constant distribution reduces exponential distribution exp. general hazard function depend time-varying factors individualspeciﬁc features. standard parametric choice hazard function individual survived time vector features estimated based longitudinal observations time vector observed time-invariant time-varying covariates vectors free parameters learned also baseline hazard function speciﬁes natural evolution risk individuals independently individual-speciﬁc features. typical parametric forms piece-wise constant functions exp)∀s free parameters paper choose latter form. condition difference instead because application priori time prediction bearing risk event.. s/s. event important quantity many applications. instance used risk score prioritize patients intensive care unit allocate resources greater risk experiencing adverse health event next hours. applications require dynamically updating failure probability observations become available time. joint modeling hazard function event probability assume features deterministically computed longitudinal data time however computing features challenging setting longitudinal data missingness. mean covariance i.e. parameters kernel shared across different signals. signal-speciﬁc function generated whose kernel parameters signal-speciﬁc assume generated non-standardized student’s t-distribution scale degrees freedom choose student’s t-distribution heavier tail gaussian distribution robust outliers; e.g. jyl¨anki intuitively particular structure model posits patterns exhibited multivariate time-series individual described components low-dimensional function space shared among signals signal-speciﬁc latent function. shared component primary mechanism learning correlations among signals; signals highly correlated give high weights latent functions similar). modeling correlations natural domains like health deterioration single organ system likely affect multiple signals. further modeling correlations model improve estimation data missing sparsely sampled signal based correlations frequently sampled signals. experiments initialize length-scales kernel captures short-term changes learns long-term trends shared latent functions. general kernel length-scales individual-speciﬁc free parameters. however capture common dynamic patterns share statistical strength across individuals found helpful share lengthscale latent function across individuals. challenge individuals different length observations length-scale all. experimentally found length-scale long-range kernel linear relation logarithm length observation individual. capture relation deﬁne maxd maxn tidn maximum observed time individual population-level parameters estimate along model parameters. thus instead sharing length-scale individuals different length observations share also appropriate mapping obtain positive length-scale. obtain prevents small large lengthfunction ﬁnal training objective logarithm likelihoods grid points evaluating objective multiple grid points leading event facilitates learning weights hazard function prioritize features estimated partial traces highly associated occurrence adverse event downstream. contrast classical approach evaluating likelihood based complete longitudinal event data useful retrospective analyses poorly suited setting early warning. joint longitudinal time-to-event model section describe framework jointly model longitudinal time-to-event data. probabilistic joint model consists sub-models longitudinal submodel time-to-event sub-model. intuitively timeto-event model computes event probabilities conditioned features estimated longitudinal model. sub-models learned together maximizing joint likelihood longitudinal time-to-event data. observed longitudinal data individual time develop probabilistic joint modeling time-to-event information deﬁned section unless ambiguity suppress superscripting hereon. longitudinal sub-model multiple-output gaussian processes model multivariate longitudinal data individual. provide ﬂexible priors functions capture complicated patterns exhibited clinical data. develop longitudinal sub-model based linear models coregionalization framework naturally capture correlations different signals individual. provides mechanism estimate sparse signals based correlations densely sampled signals. {yid∀n nid} collection observations signal individual denote collection observations longitudinal signals individual yid}. assume data missing-at-random i.e. missingness mechanism depend unobserved factors. assumption ignore process caused missing data infer parameters model based observed data appendix schulam saria longer discussion). hazard function deﬁned based linear t)fi) dt)). linear features common survival analysis interpretable. application interest interpretable features preferred non-linear features challenging interpret. non-linear features incorporated within framework. recently number useful proposals instance joensuu saul fern´andez propose variants learn complex dependencies covariates time-to-event data. ranganath deep exponential families develop latent representation diverse multivariate data weibull link function predict time-to-event inferred latent representation. though papers focus cross-section setting approach learning representations nonlinear features incorporated needed within proposed framework. learning inference section describe learning inference proposed joint model. model global local parameters. global parameters denoted parameters time-to-event model parameters deﬁning kernel length-scales i.e. βd}. procedure update local parameters minibatch individuals independently resulting distributions update global parameter. unlike classical stochastic variational inference procedures local updates highly nonlinear make gradient-based optimization inside loop. local parameters bottleneck inference robust sparse longitudinal sub-model. speciﬁcally matrix inversion even univariate longitudinal setting inference scales cubically number observations. reduce computational complexity develop learning algorithm based sparse variational approach also assumption heavy-tailed noise makes model robust outliers means usual conjugate relationship lost variational approach also allows approximation non-gaussian posterior latent functions. speciﬁcally integrate gaussian process latent function posit variational distribution approximate posterior. local parameters model denoted comprise variational parameters controlling approximations noise-scale inter-process weights make point-estimates parameters. model involves multiple individual latent functions signal-speciﬁc scales. initialize longrange kernel yielding length-scales range minutes longitudinal data duration also initialize short-term kernel obtain initial length-scales minutes minor dependence duration longitudinal data. initialization learn parameters along parameters model. similarly deﬁne kernels length-scales signalspeciﬁc latent functions kid) βd)∀d ¯tid maxn tidn free parameters. initialize capture short-term signalsspeciﬁc trends. unless ambiguity hereon drop index individual also simplify notation assume ti∀d write emphasize observations different signals need aligned learning algorithm. time-to-event sub-model time-to-event sub-model computes event probabilities conditioned features estimated longitudinal sub-model. speciﬁcally given predictions individual survived time deﬁne dynamic hazard function time ¯fi) fid]t here weighting factor integral free parameter. time gives exponentially larger weight recent history feature trajectories; parameter controls rate exponential weight. relative weight given recent history increases increasing ﬂexible formulation possible adding individualspeciﬁc term noise level experiments observe signiﬁcant performance improvement using alternative formulation. functions variational approximation functions assumed independent controlled inducing input-response pairs pseudo-inputs values process points distribution give variables variational distribution gives rise gp∀r individual given longitudinal data time-to-event data censoring data collecting likelihood function individual hereon unless ambiguity drop individual subscript explicit conditioning given approximations using jensen’s inequality obtain eqp. computing used fact time-to-event longitudinal data independent conditioned first consider computation since conditioned distribution factorizes computed given choice noise distribution cannot compute expectation analytically. however conditioned also factorizes individual observations. thus expectation reduces several one-dimensional integrals observation easily approximate using gausshermite quadrature. next consider computation unlike likelihood time-to-event sub-model factorize also need take expectations terms involving hazard function requires computing integral latent functions time. make following property using property easily show t)fi) gaussian random variable mean compute analytically closed form. compute replacing likelihood function deﬁned following dynamic approach deﬁning hazard function described section expectation term related interval censoring likelihood function available closed form. instead compute monte carlo estimate term reparameterization tricks computing gradients term respect model parameters. detailed derivations given appendix compute elboi term global parameters here describe estimation global parameters βd}. overall objective elboi total number individuals. since elbo additive terms stochastic gradient techniques. iteration algorithm randomly choose minibatch individuals optimize elbo respect local parameters keeping ﬁxed. perform step stochastic gradient ascent based gradients computed mini-batch update global parameters. repeat process either relative change global parameters less threshold maximum number iterations reached. adagrad stochastic gradient optimization. computational complexity computing variational approximation latent function requires inverting matrix scales cubically multiplying matrices complexity therefore overall complexity inference uncertainty-aware event prediction joint model developed section computes probability occurrence event within given horizon here derive optimal policy uses event probability associated uncertainty detect occurrence event. desired behavior detector wait data abstain classifying estimated event probability unreliable risk incorrect classiﬁcation high. obtain policy take decision theoretic approach given time detector takes three possible actions makes positive prediction negative prediction abstains detector decides actions trading cost incorrect classiﬁcation penalty abstention. deﬁne risk function specifying relative cost term associated type possible error abstention. derive optimal decision function minimizing speciﬁed risk function. speciﬁcally every individual given observations time goal determine whether event occur within next hours hereon drop subscripts brevity. treat unobserved bernoulli random variable probability joint model estimates probability computing distribution detailed derivations distribution provided appendix distribution provides valuable information uncertainty around estimate robust policy derive next uses information improve reliability event predictions. denote decision made detector optimal policy chooses action indicates abstention respectively denote negative positive prediction. specify risk function deﬁning respectively cost terms associated false positive false negative errors deﬁning cost abstention conditioned overall risk function since unobserved random variable instead minimizing minimize expected value respect distribution i.e. random variable expected risk function also random variable every possible choice distribution easily computed based distribution obtain robust policy minimizing quantiles risk distribution. intuitively this minimize maximum cost could occur certain probability. example probability cost choice less quantile risk distribution q-quantile risk function similarly case q-quantile risk function here property q-quantile random variable -quantile finally q-quantile risk function minimize compute optimal policy. optimal policy determines choose function cost terms particular choose optimal policy depends relative cost terms simplify notation deﬁne further assume deﬁne here conﬁdence interval therefore substituting condition choosing simpliﬁes thresholds take possible values depending compared prediction special case made comparing conﬁdence interval thresholds particular entire conﬁdence interval shown fig. predict entire conﬁdence interval shown fig. declare none conditions classiﬁer abstains making decision case respectively less greater summarize policy fig. principle cost terms provided ﬁeld experts based preferences penalizing different types error desired conﬁdence level. alternatively could perform grid search choose combination achieves desired performance regard speciﬁcity sensitivity false alarm rates. experiments take latter approach. special case policy without uncertainty information imputation-based methods approaches account uncertainty missingness compute point-estimates failure probability case think distribution degenerate distribution mass point estimate i.e. point estimate here degenerate distribution fig. three example decisions made using policy described fig. shaded area conﬁdence interval choice three distributions arrows respectively. cases satisfy optimal decisions min{l policy similar classiﬁcation abstention framework introduced chow example consider case here relative cost abstention policy binary classiﬁcation abstention threshold equal alternatively abstention interval case classiﬁer chooses abstain event probability individuals robust policy length abstention region max{ abstention region adapts individual based length conﬁdence interval estimate abstention interval larger cases classiﬁer uncertain estimate helps prevent incorrect predictions. instance consider example fig. expected value greater conﬁdence interval relatively large. suppose negative sample making decision based result false positive error. order abstain individual policy abstention interval large. abstention interval individuals making interval large leads abstaining many individuals classiﬁer correct. robust policy however abstention interval adjusted individual based conﬁdence interval particular case instance resulting abstention interval large therefore false positive prediction avoided. related work joint models longitudinal event data proposed model builds upon extensive prior literature joint models longitudinal time-to-event data. here joint probability distribution posited longitudinal time-to-event data. example rizopoulos uses generalized mixed-effects models modeling longitudinal data computes time-to-event distribution conditioned mean predictions longitudinal model. proust-lima propose ﬂexible joint model individual’s data assumed generated ﬁxed number classes longitudinal data individual class modeled using polynomial function. coefﬁcients longitudinal model predictors time-to-event distribution. models—by jointly modeling longitudinal event data—provide principled propagating uncertainty missingness estimating event probabilities applicability challenging domains clinical data limited need make strong parametric assumptions form longitudinal data. recently others introduced ﬂexible ways represent longitudinal data. example proust-lima extends work discussed latent class modeling include ﬂexible forms longitudinal data speciﬁcally given class multiple longitudinal signals correlated shared latent process modeled gaussian process mean represented linear mixed-effects model. inference model scales cubically number unique time-points observations obtained i.e. futoma leverage ﬂexible semi-parametric models introduced schulam saria modeling canonical progression patterns longitudinal data. approach also scales cubically number observations further work focuses setting single longitudinal marker assumes alignment across time series multiple individuals. two-stage approaches instead jointly modeling longitudinal time-to-event data take twostage approach. here common approach imputation missing data apply time-to-event techniques completed data; e.g. commonly used imputation techniques longitudinal data mean substitution last-observationcarried-forward regression imputation latter approach instance regression model used impute values missing feature given observed covariates. sophisticated methods imputation also used. major drawback imputation methods missing data single substituted value cannot propagate error imputing missing values towards estimating event probabilities. multipleimputation techniques circumvent shortcoming impute multiple values missing data point sampling posterior distribution missing point given observed data. creates multiple completed datasets posterior uncertainty quantiﬁed averaging across datasets. methods however suffer curse dimensionality applied high-dimensional multivariate longitudinal signals many irregularly sampled observations classiﬁcation irregular time series alternatively treat event forecasting time-series classiﬁcation task. since literature vast brieﬂy review relevant irregular time-series classiﬁcation approaches focusing clinical data. typically imputation methods probabilistic models used extracting point estimates features example ghassemi multi-task gaussian processes model multiple longitudinal series features estimated resulting ﬁtted data predict occurrence event. multi-task model used ghassemi also known intrinsic correlation model assumes within-signal correlation structure signals. similarly alaa multi-task computing risk scores patients intensive care units. approaches however principled mechanism incorporate uncertainty missing longitudinal data event prediction. further method scale well multivariate signals many observations. speciﬁcally computational cost ﬁtting model grows cubically number signals number observations signal i.e. cost prohibitive either large. lasko model univariate longitudinal data train autoencoders predictions extract expressive nonlinear features classiﬁcation. parametric approaches hidden markov models linear dynamical systems also used feature computation clinical time-series downstream timeseries classiﬁcation tasks non-probabilistic methods based recurrent neural networks also used modeling irregularly sampled time series again method generally lack proper mechanism incorporating uncertainty associated missing data. approaches exist modeling event streams—e.g. piecewise-constant conditional intensity models model dependency timing events across multiple discrete event types model continuousvalued time series. reliable prediction classiﬁcation accounting uncertainty training classiﬁer investigated before. instance marlin proposed framework classiﬁcation irregularly sampled time series using gps. estimates evaluated grid points features classiﬁer. account uncertainty missingness training optimize expected loss. however prediction incorporate individual classiﬁcation decisions. contrast rather optimizing expected loss taking account quantiles distribution policy leverages shape event occurrence distribution test time. speciﬁcally using uncertainty associated event probability proposed policy chooses wait collect samples making decision. classiﬁcation abstention also investigated deciding abstention classiﬁcation methods based point-estimates event probabilities unlike methods approach incorporates uncertainty event probabilities form conﬁdence intervals. parrish proposed framework reliable classiﬁcation incomplete data. notion reliability different ours focus setting sample belongs single class; reliable classiﬁcation entails predicting class sample partial sequence frames decision remains stable observing complete sample. sangnier hoai similarly exploit monotonicity property training classiﬁers video classiﬁcation. works different ways. first deﬁnition reliability holds time series segmented episodes containing single event. second consider settings missing data. experimental results evaluate proposed framework task predicting patients hospital high risk septic shock— life-threatening adverse event. currently clinicians rudimentary tools real-time automated prediction risk shock tools suffer high false alert rates. early identiﬁcation gives clinicians opportunity investigate provide timely remedial treatments data mimic-ii clinical database publicly available database consisting clinical data collected patients admitted hospital annotate data used deﬁnitions described henry septic shock. censoring common issue dataset patients high-risk septic shock receive treatments delay prevent septic shock. cases true event time censored unobserved. following approach treat patients received treatment developed septic shock interval-censored exact time shock onset could time time treatment observed shock onset time. patients never developed septic shock receiving treatment treated rightcensored. patients exact shock onset time could point treatment. model following longitudinal streams clinical signals found highly predictive septic shock henry heart rate systolic blood pressure urine output respiratory rate blood urea nitrogen creatinine glasgow coma score blood measured arterial line partial pressure arterial oxygen white blood cell count addition based also include following time-varying timeinvariant observed features found signiﬁcant identifying septic shock time since ﬁrst antibiotics time since organ failure status chronic liver disease chronic heart failure diabetes. sub-sampled original mimic-ii database include patients least measurements signal. technical requirement proposed model. many baseline methods described next cannot naturally handle signals measurements result perform poorly. inclusion criterion chosen allow comparing baselines reasonable operating point. sub-sampled patients septic shock maintain ratio septic shock original cohort yields dataset patients. randomly divided patients train test sets ensuring ratio septic shock both. training consists patients including patients observed septic shock event-free patients. further patients training received treatment sepsis later developed septic shock remaining patients right censored. test consists patients observed shock event-free patients. test patient make predictions evaluation points. spaced equally two-day interval ending minutes prior time shock onset censoring hospital stay. choose setting monitoring early warning applications require frequent evaluations patient risk multiple time points leading event. different standard time series classiﬁcation tasks prediction made given entire time series data. however purpose evaluation paper avoid reporting bias patients long hospital stays choose make predictions points every patient. size data existing state-of-the-art joint models handle second shown fig. signals challenging properties non-gaussian noise sampled frequently others sampling rate varies widely even within given signal individual signals contain structure multiple scales. mogp ﬁrst baseline implement twostage joint modeling approach modeling longitudinal time-to-event data. speciﬁcally multi-output provides highly ﬂexible imputing missing data. ghassemi shown stateof-the-art performance modeling physiologic data using multivariate gp-based models. previously discussed inference scales cubically number recordings; thus making impossible dataset size. here approximations described section learning inference. mean predictions ﬁtted mogp compute features hazard function time-to-event model used baseline similar model used proposed approach. difference two-stage training approach mogp cannot propagate uncertainty latent functions time-to-event component. using baseline assess extent robust policy—that accounts uncertainty missing longitudinal data estimating event probabilities— contributes improving prediction performance. second baseline two-stage joint model random-effects regression model longitudinal data. b-spline regression model knots independently signal every patient complete missing data used imputed values compute features hazard function. also placed population level gaussian prior diagonal covariance regression coefﬁcients. time-to-event component similar used proposed approach. logistic regression baseline timeseries classiﬁcation approach. recordings time series signal binned -hour windows; bins multiple measurements average value. bins missing values covariate-dependent regression imputation. binned values consecutive windows signals used features logistic regression classiﬁer event prediction. regularization used learning model; regularization weight selected using -fold cross-validation training data. fig. data signals longitudinal along conﬁdence intervals patients patient septic shock patient observed shock. right show estimated event probability following hour period conditioned longitudinal data patient shown left. septic shock patient occurs stay. j-ltm observes ﬁrst days longitudinal data patient predicts shock hours onset. evaluation patients test make predictions given evaluation points. evaluation treat prediction independently aggregate predictions across evaluation points patients. these compute true positive rate false positive rate positive predictive value follows abstain different prediction points. make fair comparison different methods shown compute rates respect prediction points patients rather subset points classiﬁers chooses make predictions; speciﬁcally compute respect reported experiments prediction horizon hours compute alerting policy. however note different choices seen change scale event probabilities; affect ordering patients result choice affect computation performance metrics reported paper. also sweep cost terms plot curves. determine statistical signiﬁcance results perform non-parametric bootstrap test bootstrap sample size report average standard error performance criteria. setup learning inference algorithm learning rate maximum number iterations global optimization respectively mini-batch size number monte carlo samples reparameterization trick l-bfgs-b local optimization maximum number iterations number inducing points number shared latent functions based based visual analysis convergence results global parameters training data. implementation details implemented proposed model using tensorflow gpﬂow automatically compute gradients elbo respect variables. experiments reported section obtained using tensorflow implementation running single machine -core ram. local optimization parameters individual within iteration learning algorithm takes average seconds. step main computational bottleneck; however embarrassingly parallelizable distributed version algorithm enables scaling larger datasets patients longitudinal signals. results qualitative analysis example patients first qualitatively investigate ability proposed model—from hereon referred j-ltm—to model longitudinal data estimate event probability. fig. show achieved j-ltm longitudinal signals patients patient septic shock patient experience shock despite complexity physiologic data j-ltm data well. also j-ltm robust outliers; e.g. respiratory rate patient fig. also shows event probability computed following hours conditioned data observed patient. j-ltm detects patient high risk stay. septic shock patient occurs hours later. shown fig. j-ltm computes high event probability high conﬁdence patient prediction horizon contrast event probability predicted patient septic shock relatively low. also gain insight main contributing factors jltm’s predictions comparing different components weighted sums ¯fit hazard function. instance three factors patient high heart rate clinically relevant factors could contribute organ failure septic shock. interpreting model parameters shared signal-speciﬁc kernels fig. urine output respiratory rate densely sampled compared signals. sparsely sampled signals expect shared latent components contribute signal-speciﬁc kernels. test hypothesis compare ratio weights shared signal-speciﬁc kernels across different signals. median ratio across patients wbc. coefﬁcients shared functions much greater weight signal-speciﬁc kernels sparse signals wbc. capturing correlations across signals shared latent functions also help j-ltm capture correlations across signals. evaluate correlation patterns discovered j-ltm compute correlation coefﬁcient across different signals patients signals highest cross-correlations correlation coefﬁcient urine output creatinine signals fact known related other. example creatinine measures kidney function typically correlated. quantitative evaluation next quantitatively evaluate performance j-ltm. report curves jltm baseline methods fig. plot curve method performed grid search relative cost terms recorded obtained pairs. j-ltm achieves outperforms mogp aucs respectively. shown fig. increased j-ltm compared baseline methods primarily occurs fprs ranging range relevant practical use. particular true positive rate j-ltm mogp respectively fig. compares performance using make explicit number true alerts. important performance criterion alerting systems positive predictive probability ratio true positives total number alarms. every positive prediction classiﬁer requires attendance investigation clinicians. therefore rate increases workload clinicians causes alarm fatigue. ideal classiﬁer detects patients septic shock false alarms fig. plot maximum obtained level j-ltm baselines. sweep recorded best achieved level. j-ltm greater baselines. particular range j-ltm shows improvement mogp next best baseline improvement methods typically implemented standard-of-care tools. practical standpoint evaluation leads context switch cost caregiver minutes; improvement amount many hours saved daily. elaborate comparison further report method function number decisions made given decision rate model abstain different subset patients. fig. show best achieved given decision rate different settings minimum ppv. fig. example every abstention rate plot best achieved every model greater j-ltm achieves signiﬁcantly higher baseline methods decision rates. words given decision rate j-ltm able correctly identify subset instances make predictions. similar plots shown fig. maximum ppv>. j-ltm decision rates signiﬁcantly greater best level mogp natural question whether reported tprs good enough practical use. best standard-of-care tools implement baselines without abstention. corresponds performance methods figs. decision rate shown gain achieved j-ltm large settings. conclusion propose probabilistic framework improving reliability event prediction incorporating uncertainty missingness longitudinal data. proposed approach comprised several innovations. first developed ﬂexible bayesian nonparametric model jointly modeling high-dimensional continuous-valued longitudinal event time data. order facilitate scaling large datasets proposed stochastic variational inference algorithm leveraged sparse-gp techniques; significantly reduced complexity inference joint-modeling cubic number signals number measurements signal linear compared state-of-the-art joint modeling approach scales datasets several order magnitude larger without compromising model expressiveness. joint-model enabled computation event probabilities conditioned irregularly sampled longitudinal data. second derived policy event prediction incorporates uncertainty associated event probability abstain making decisions alert likely incorrect. important challenging task predicting impending in-hospital adverse events demonstrated proposed model scale timeseries many measurements patient estimate good signiﬁcantly improve event prediction performance state-of-the-art alternatives. smith wood in-hospital cardio-respiratory arrests prevented? prospective survey resuscitation vol. kumar duration hypotension initiation effective antimicrobial therapy critical determinant survival human septic shock critical care medicine vol. pelkonen gorilla fast scalable in-memory time series database proceedings vldb endowment vol. kalbﬂeisch prentice statistical analysis failure time data. van. houwelingen putter dynamic prediction clinical survival analysis. press rizopoulos joint models longitudinal timeto-event data applications press dynamic predictions prospective accuracy joint models longitudinal time-to-event data biometrics vol. package joint modelling longitudinal time-to-event data journal statistical software vol. proust-lima joint latent class models longitudinal time-to-event data review. statistical methods medical research vol. proust-lima dartigues jacqmin-gadda joint modeling repeated multivariate cognitive measures competing risks dementia death latent process latent class approach statistics medicine vol. rizopoulos combining dynamic predictions joint models longitudinal time-to-event data using bayesian model averaging journal american statistical association vol. sweeting thompson joint modelling longitudinal timetoevent data application predicting abdominal aortic aneurysm growth rupture biometrical journal vol. henry targeted real-time early warning score septic shock. science translational medicine vol. ghassemi multivariate timeseries modeling approach severity illness assessment forecasting sparse heterogeneous clinical data aaai understanding vasopressor intervention weaning risk prediction public heterogeneous clinical time series database journal american medical informatics association allison missing data. sage publications young johnson handling missing values longitudinal panel data multiple imputation marriage family vol. stanculescu williams freer autoregressive hidden markov models early detection neonatal sepsis ieee journal biomedical health informatics vol. alaa personalized risk scoring critical care patients using mixtures gaussian process experts icml workshop computational frameworks personalization quinn williams mcintosh factorial switching linear dynamical systems applied physiological condition monitoring ieee trans. pattern anal. mach. intell vol. james hensman received meng degrees mechanical engineering university shefﬁeld respectively. following doctoral-prize fellowship joined professors rattray lawrence postdoc machine learning computational biology. research interests include approximate bayesian inference large complex systems biology biomedicine. awarded career development fellowship hossein soleimani received degree pennsylvania state university degree university tehran tehran iran degree ferdowsi university mashhad mashhad iran electrical engineering. currently postdoctoral fellow johns hopkins university baltimore research interests include machine learning healthcare probabilistic graphical models approximate posterior inference statistical modeling. suchi saria professor computer science joint appointments applied mathematics statistics health policy johns hopkins university. also technical director malone center engineering healthcare johns hopkins. research interests include statistical machine learning decision-making uncertainty. prior this received computer science stanford university. term related interval censoring cannot computed analytically. also need take derivative term respect parameters variational distribution time-to-event parameters. this reparameterization tricks compute monte carlo estimate expectation gradients ρk)ρ) finally continuity property characteristic functions conclude indeed characteristic function random variable proves claim. computing ﬁrst compute integral latent functions using lemma proved above. ieee transactions pattern analysis machine intelligence appendix distribution recall depends which described section gaussian random variable; thus also random variable whose distribution computed based distribution", "year": 2017}