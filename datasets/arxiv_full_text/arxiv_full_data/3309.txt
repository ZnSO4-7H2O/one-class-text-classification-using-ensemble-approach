{"title": "Clustering by Hierarchical Nearest Neighbor Descent (H-NND)", "tag": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "abstract": "Previously in 2014, we proposed the Nearest Descent (ND) method, capable of generating an efficient Graph, called the in-tree (IT). Due to some beautiful and effective features, this IT structure proves well suited for data clustering. Although there exist some redundant edges in IT, they usually have salient features and thus it is not hard to remove them.  Subsequently, in order to prevent the seemingly redundant edges from occurring, we proposed the Nearest Neighbor Descent (NND) by adding the \"Neighborhood\" constraint on ND. Consequently, clusters automatically emerged, without the additional requirement of removing the redundant edges. However, NND proved still not perfect, since it brought in a new yet worse problem, the \"over-partitioning\" problem.  Now, in this paper, we propose a method, called the Hierarchical Nearest Neighbor Descent (H-NND), which overcomes the over-partitioning problem of NND via using the hierarchical strategy. Specifically, H-NND uses ND to effectively merge the over-segmented sub-graphs or clusters that NND produces. Like ND, H-NND also generates the IT structure, in which the redundant edges once again appear. This seemingly comes back to the situation that ND faces. However, compared with ND, the redundant edges in the IT structure generated by H-NND generally become more salient, thus being much easier and more reliable to be identified even by the simplest edge-removing method which takes the edge length as the only measure. In other words, the IT structure constructed by H-NND becomes more fitted for data clustering. We prove this on several clustering datasets of varying shapes, dimensions and attributes. Besides, compared with ND, H-NND generally takes less computation time to construct the IT data structure for the input data.", "text": "abstract previously proposed nearest descent method capable generating efficient graph called in-tree beautiful effective features structure proves well suited data clustering. although exist redundant edges usually salient features thus hard remove them. subsequently order prevent seemingly redundant edges occurring proposed nearest neighbor descent adding neighborhood constraint consequently clusters automatically emerged without additional requirement removing redundant edges. however proved still perfect since brought worse problem over-partitioning problem. paper propose method called hierarchical nearest neighbor descent overcomes over-partitioning problem using hierarchical strategy. specifically h-nnd uses effectively merge over-segmented sub-graphs clusters produces. like h-nnd also generates structure redundant edges appear. seemingly comes back situation faces. however compared redundant edges structure generated h-nnd generally become salient thus much easier reliable identified even simplest edge-removing method takes edge length measure. words structure constructed h-nnd becomes fitted data clustering. prove several clustering datasets varying shapes dimensions attributes. besides compared h-nnd generally takes less computation time construct data structure input data. previously proposed physically-inspired clustering method mimicking moving behavior particle swarm. imagine data point behaves like particle generate field whose magnitude declines distance fields different data points added places. result point system would generate uneven space turn uneven space could enforce point system evolve. intuitively points system tend move along descending direction potential eventually clustered places locally lowest potentials. accordingly devised method follows. first simplified descending tendency points general rule uneven means potentials varies places. usually points dense areas would lower potentials. descend nearest point called nearest descent namely among nodes decreasing direction potential point designed choose nearest descend view point node link node node descends directed edge rule result graph structure. graph theory graph composed nodes edges fact graph constructed special fully connected graph call in-tree graph theory proven several salient features sparsely connected graph number directed edges less data points dataset; wonderful order inside every node directed path reach root node. besides that structure constructed method fig. shows additional meaningful features underlying cluster structure datasets well captured except redundant edges clusters requiring removed; redundant edges usually salient features thus hard identify them. features make structure constructed well suited task data clustering basic classic still challenging task statistics data mining machine learning aiming discovering underlying cluster structure datasets assigning data points belonging clusters groups. fig. structure data set. elongated clusters dataset well captured structure sparse effective form exists redundant salient edge bridging clusters. colors points denote estimated potential values. purpose remove redundant edges search root nodes assign nodes belonging root nodes groups. step since foremost feature redundant edges usually relatively longer edges simplest method determine rank edge lengths decreasing order take ones relative large edge lengths redundant edges. works well least dataset fig. beside simple method fact devised bunch reliable methods determine redundant edges either interactively automatically it-map it-dendrogram g-ap methods removing redundant edges immediately obtain several unconnected sub-graphs sub-graph represents cluster. step root searching process simply performed searching along directions edges. result node find root node cluster. nodes reaching root nodes thus assigned groups. unlike iterative methods k-means mean-shift searching process graph almost negligible computation time since searching paths already specified graph searching process operated parallel points. motivation idea work instead proposing method determine redundant edges motivation modify rule redundant edges become salient thus easily reliably determined even simplest edge-removing method fact according start node redundant edge extreme point cluster generally true node. node lower potential start node whereas necessary extreme node another cluster. fig. illustrate clearly. fig. shows take -dimensional dataset main clusters example. structure constructed shown fig. note difference fig. values estimated potentials points denoted additional axis fig. denoted different colors. fig. data points clusters respectively organized bowl-like shapes. redundant edge starts bottom node left bowl ends right bowl obviously node bottom. specifically motivation interpreted make node edge also bottom node corresponding bowl fig. shows. obviously least good thing change that compared fig. redundant edge fig. becomes longer larger difference rest edges. fig. illustration motivation. input -dimensional data points. graphs generated h-nnd shown respectively. horizontal axis denotes coordinates input data points. vertical axis added denotes potential estimated data point. edges denote redundant edges need removed. solution paper fact based another method proposed already started modify rule different purpose. method there denoted nearest neighbor descent could prevent redundant edges occurring generated graph constraining rule neighbor graph namely compared additional letter middle refers neighborhood constraint added intended make rule consistent real physical circumstance thus took account physical space particle could travel space reach another place. practice fig. shows approximated physical space neighbor graph severs neighborhood constraint performing rule. leads result shown fig. admittedly thing fascinating that neighborhood constraint redundant edges emerge fig. fig. shows thus effort removing redundant edges needed. another less obvious benefit also brought neighborhood constraint computation complexity searching parent nodes narrowed local scopes rather whole dataset means huge reduction computation time processing datasets large numbers data points. nevertheless neighborhood constraint could bring nontrivial problem over-partitioning problem. revealed also fig. certain cluster partitioned part. surprising since neighborhood constraint would make locally extreme nodes directly become roots generated sub-graphs therefore estimated potentials nodes poor representation underlying density distribution number obtained extreme nodes main clusters inside dataset case over-partitioning problem occur. however sheds lights goal paper. hand extreme nodes identified bring over-partitioning problem. hand could also treasure make effectively. instance rerun rule extreme nodes obtained nnd? words extreme node continue descend nearest point among extreme nodes. wholly linked graph constructed. obviously newly generated graph still structure start node undesired edges sure extreme nodes. namely initial goal mentioned section achieved. besides also worth noting over-partitioned clusters fig. effectively linked together fig. means over-partitioning problem faces also largely solved proposed method. proposed method h-nnd therefore feature proposed method hierarchical strategy i.e. performed second stage extreme nodes obtained first stage. denote proposed method hierarchical nearest neighbor descent containing following four steps graph constructed input graph-based datasets first performed obtain result first layer hierarchy. result sparse fully connected graph usually unconnected sub-graphs. then second stage performed merge sub-graphs fully connected graph note needed link root nodes sub-graphs therefore whole process viewed process graph transformation resulting sparse effective last. following steps clustering purpose steps nd-based clustering process mentioned section i.e. edge removing root searching. plotting lengths edges denoted edge plot immediately find redundant edge underlying constructed structure. note that edge plot edge length represent distance data parent node original space. example fig. edge lengths edge plot refer lengths edges plotted fig. edge lengths edge plot refers lengths mappings edges horizontal axis rather edges plotted. removing redundant edges sub-graph actually represents cluster. parameters h-nnd parameter involved step input data graph dataset data points chosen constructed k-nn graph; another parameter involved step kernel-based method estimate potentials nodes however nonparametric graph step non-kernel-based estimate potential step number parameters become zero. show section experiments tested performance h-nnd clustering wide range data sets varying shapes dimensions attributes collected real life; others synthetic datasets. three high-dimensional datasets; others -dimensional challenging clustering method deal different kinds datasets. instance classical method like k-means modern method like able detect spherical clusters thus hard cluster datasets table besides iris mushroom datasets widely used check performance clustering methods experiments mentioned achieved excellent results datasets experiments used simplest edge removing method results testing synthetic datasets shown fig. left column shows edge plots structures generated proposed h-nnd method right column shows corresponding clustering results edge removing root searching. except last dataset edge plots datasets salient enough undesired edges datasets easy determined. unlike k-means cluster number needs specified advance cluster number determined interactively. words cake made advance cake users also well indicated instruction note used k-nn first step h-nnd euclidean distance measure distance pair data points. k-nn graphs intermediate results supplementary material attached paper. fig. results synthetic datasets h-nnd. left column edge plots. horizontal axis denotes edge length. right column corresponding clustering results. tests iris mushroom datasets kernel-based method used compute potentials nodes respectively. k-nn chosen step construct initial neighbor graph. besides iris cosine euclidean distance used measure pair-wise distance respectively. mushroom dataset since attributes data instances characters rather real values took number different characters pair data instances distance. test results follows. iris dataset data instances falsely assigned clustered integer except error number slightly increased fig. shows typical edge plot dataset. dataset almost achieve ideal result data points correctly assigned clusters. fig. edge plot mushroom dataset performance also comfortable. cluster error rate clusters. however performance became better clusters zero error rate. therefore although h-nnd designed make undesired edges salient cannot h-nnd always superior reason behind needs consideration. fig. made comparisons minimal spanning tree algorithm algorithms h-nnd since clustering purpose graphs generated involve redundant edges requires removed. compared structure constructed redundant edges structure constructed h-nnd fig. overall salient much longer least. also demonstrated corresponding edge plots fig. average points largest edge lengths others fig. obviously larger fig. however overall performances k-nnd much better compared mst. edge plot also shows smaller average points largest edge lengths others. worth noting points popping line fact fig. undesired edges short lengths even shorter non-redundant edges. even chose longest edges still leads poor clustering result shown fig. fake clusters clusters segmented well. contrast h-nnd methods number undesired edges threshold edges easy determined fig. show edge-removing process proves accurate sound clustering results fig. present. fig. used first step h-nnd. fact comparable results still obtained k-nn used fig. comparison h-nnd methods. columns left right correspond results h-nnd methods respectively. comparison taken three aspects generated graphs edge plots clustering results removing undesired edges potentials h-nnd computed kernel-based method parameter fig. made experiments demonstrate fact h-nnd effectively solve over-partitioning problem nnd. experiments mainly focused dataset table although could achieve performance h-nnd reduced decreased over-partitioning problem emerged respectively clusters true cluster number comparison almost invariably ideal results achieved h-nnd furthermore tried reduce number parameters eliminating parameter using local information neighborhood graph introduced estimate potentials nodes eliminating parameter using rather k-nn initial neighborhood graph over-partitioning problem became worse shown fig. many respectively. surprising since non-parametric approaches make estimated distribution potentials less smooth thus locally extreme nodes generated taken cluster centers. however still made almost impact results h-nnd. moreover edge plots h-nnd quite salient thus undesired edges numbers easily reliably determined. fig. h-nnd. number parameters involved methods gradually decreased zero over-partitioning problem gets worse reaching large number clusters performance h-nnd remains constant clusters. h-nnd advantages terms computation time saliency redundant edges compared h-nnd significantly reduces computation time determining parent nodes. explained aspects. non-extreme nodes neighborhood constraint local computations involved determining parent nodes stark contrast needs search whole dataset determine parent nodes nodes. extreme nodes although process searching parent nodes follows process without neighborhood constraint search scopes among extreme nodes rather whole dataset again. instance h-nnd took step step testing mushroom dataset personal computer namely given neighborhood graph took h-nnd construct data structure dataset much faster besides note removing redundant edges computation time remaining process searching root nodes generally negligible thus given neighborhood graph costs around h-nnd output clustering results compared h-nnd significantly increases saliency redundant edges making insignificant change non-redundant edges reasons analyzed way. first datasets divided three classes non-extreme nodes non-valid extreme nodes valid extreme-nodes. accordingly three classes edges i.e. edges started non-extreme nodes non-valid extreme nodes valid extreme-nodes respectively. first classes edges usually non-redundant edges third class edges redundant edges. compared generated directly first class edges almost change since parent nodes non-extreme nodes hardly affected neighborhood constraint. second class edges exist small changes usually insignificant density roughly captured third class edges i.e. redundant edges start edge nodes extreme nodes contract nodes generated redundant edges usually extreme nodes. since extreme nodes redundant edges usually center clusters make redundant edges distinguishable conclusion compared h-nnd makes insignificant changes non-redundant edges making significant changes redundant edges thus redundant edges become distinguishable. risk behind h-nnd second class edges could comparable third class edges density badly estimated. compared clustering h-nnd largely resolves over-partitioning problem provides simple effective able fast simultaneously find local extreme points estimated potential function. faced over-partitioning problem h-nnd able distinguish extreme points detected valid ones ultimate cluster centers. fact common advantage shared note that like h-nnd refers process constructing also short term clustering h-nnd. interestingly development h-nnd quite consistent negation negation formulated philosopher hegel. realized mistake made i.e. taking constraint physical space account thus added neighbor graph constraint consequently redundancy prevented. could viewed negation. refers negation choosing constraint negation choosing redundancy however didn’t make things better. therefore paper abandon constraint come back again time performed subset original dataset i.e. root nodes resulted nnd. like h-nnd also generates structure redundant edges appear. could viewed negation. refers negation choosing constraint negation abandoning redundancy nnd. negation things become better. compared redundant edges structure generated h-nnd generally become salient thus much easier reliable identified. words structure constructed h-nnd becomes fitted data clustering. also interestingly look development inversely namely h-nnd that constraints require broken reach simple powerful rule. words h-nnd indirectly demonstrates simplicity effectiveness rodriguez laio’s decision graph perspective comparing adding constraints could challenging meaningful find constraint fixed thinking modes mind break instance although already done several works theme surprisingly find till made mistake unconsciously latest paper redundantly added term neighbor introducing consequently became there. another example gradient. together associated methods gradient ascent gradient descent gradient often used gradient-based clustering methods. nevertheless gradient describes local feature thus like neighborhood constraint gradient-based methods also face over-partitioning problem does. discuss next paper. generally speaking elements view mythology quite contributing h-nnd hierarchical strategy neighborhood constraint. guidance hierarchical strategy h-nnd attempt construct structure stage makes possible role fully played. effectiveness hierarchical strategy guaranteed neighborhood constraint since neighborhood constraint affects extreme nodes narrowing choices parent nodes local scopes consequently become root nodes generated sub-graphs postponed processed second stage. another perspective that since neighborhood constraint tell extreme nodes whole dataset beforehand thus deal extreme non-extreme nodes separately different stages. fact know neighborhood constraint useful strategy also benefits lots non-linear dimensionality reduction methods like isomap etc. theoretically promote performance gradient-based methods similarly h-nnd here. performance well showed here since gradient-based methods sensitive parameters thus cannot provide reliable basis layer. show next paper. macqueen methods classification analysis multivariate observations. proceedings fifth berkeley symposium mathematical statistics probability neyman fig. first column k-nn graphs. second column results nnd. third column structures made h-nnd. kernel-based method used compute potential parameter fig. results k-nn chosen neighborhood graph h-nnd. rows bottom correspond results respectively. fact even increased h-nnd could still perform well since case h-nnd reduced thus results column fig. shows. note used kernel-based compute potential. ……the negation negation first formulated hegel particular features previously established philosophy hegel’s dialectical system development emergence logical contradiction subsequent sublation. sense development birth internal negation previous stage followed negation negation extent negation previous negation proceeds sublation always certain sense restoration negated return past stage development. however simple return starting point concept higher richer concept previous enriched negation opposite; contains concept contains concept alone unity opposite thus negation negation universal form splitting single whole transition opposites other— universal manifestation unity struggle opposites…… from http//encyclopedia.thefreedictionary.com/negation+of+the+negation+law+of+the", "year": 2015}