{"title": "Prediction with a Short Memory", "tag": ["cs.LG", "cs.AI", "cs.CC", "stat.ML"], "abstract": "We consider the problem of predicting the next observation given a sequence of past observations, and consider the extent to which accurate prediction requires complex algorithms that explicitly leverage long-range dependencies. Perhaps surprisingly, our positive results show that for a broad class of sequences, there is an algorithm that predicts well on average, and bases its predictions only on the most recent few observation together with a set of simple summary statistics of the past observations. Specifically, we show that for any distribution over observations, if the mutual information between past observations and future observations is upper bounded by $I$, then a simple Markov model over the most recent $I/\\epsilon$ observations obtains expected KL error $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to the optimal predictor that has access to the entire past and knows the data generating distribution. For a Hidden Markov Model with $n$ hidden states, $I$ is bounded by $\\log n$, a quantity that does not depend on the mixing time, and we show that the trivial prediction algorithm based on the empirical frequencies of length $O(\\log n/\\epsilon)$ windows of observations achieves this error, provided the length of the sequence is $d^{\\Omega(\\log n/\\epsilon)}$, where $d$ is the size of the observation alphabet.  We also establish that this result cannot be improved upon, even for the class of HMMs, in the following two senses: First, for HMMs with $n$ hidden states, a window length of $\\log n/\\epsilon$ is information-theoretically necessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the $d^{\\Theta(\\log n/\\epsilon)}$ samples required to estimate the Markov model for an observation alphabet of size $d$ is necessary for any computationally tractable learning algorithm, assuming the hardness of strongly refuting a certain class of CSPs.", "text": "consider problem predicting next observation given sequence past observations consider extent accurate prediction requires complex algorithms explicitly leverage long-range dependencies. perhaps surprisingly positive results show broad class sequences algorithm predicts well average bases predictions recent observation together simple summary statistics past observations. speciﬁcally show distribution observations mutual information past observations future observations upper bounded simple markov model recent observations obtains expected error \u0001—and hence error \u0001—with respect optimal predictor access entire past knows data generating distribution. hidden markov model hidden states bounded quantity depend mixing time show trivial prediction algorithm based empirical frequencies length windows observations achieves error provided length sequence size observation alphabet. also establish result cannot improved upon even class hmms following senses first hmms hidden states window length information-theoretically necessary achieve expected error error second samples required accurately estimate markov model observations drawn alphabet size necessary computationally tractable learning/prediction algorithm assuming hardness strongly refuting certain class csps. consider problem predicting next observation given sequence past observations could complex long-range dependencies. sequential prediction problem basic learning tasks encountered throughout natural language modeling speech synthesis ﬁnancial forecasting number domains sequential chronological element. abstract problem received much attention last half century multiple communities including machine learning coding theory. fundamental question consolidate reference memories past order eﬀectively predict future? given immense practical importance prediction problem enormous eﬀort explore diﬀerent algorithms storing referencing information sequence. eﬀorts recurrent neural networks —which encode past real vector ﬁxed length updated every observation—and speciﬁc classes networks long short-term memory networks recently popular models explicit notions memory include neural turing machines memory networks diﬀerentiable neural computer etc. models quite successful nevertheless seem largely unable consistently learn long-range dependencies crucial many settings including language. parallel eﬀorts design systems explicitly memory much eﬀort neuroscience community understand humans animals able make accurate predictions environment. many eﬀorts also attempt understand computational mechanisms behind formation memories retrieval aside intrinsic theoretical value questions answers could serve guide construction eﬀective practical prediction systems well informing discussion computational machinery cognition prediction/learning nature. work provide insights ﬁrst three questions. begin establishing following proposition addresses ﬁrst questions respect pervasively used metric average prediction error proposition distribution sequences mutual information between past observations future observations best order markov model makes predictions based recent observations predicts intuition behind statement proof general proposition following time either predict accurately unsurprised revealed predict poorly surprised value must contain signiﬁcant amount information history sequence leveraged subsequent predictions etc. sense every timestep prediction ‘bad’ learn information past. mutual information history sequence future general proposition framed terms mutual information past future immediate implications number well-studied models sequential data hidden markov models hidden states mutual information generated sequence trivially bounded yields following corollary proposition. state proposition provides helpful reference point discussion general proposition. setting observations generated according hidden states best order markov model easy learn given suﬃcient data corresponds naive empirical -gram model based previous observations. speciﬁcally model that given xt−+ outputs observed distribution observation followed length sequence. following theorem makes claim precise. theorem suppose observations generated hidden markov model hidden states output alphabet size exists window length absolute constant chosen uniformly random expected distance true distribution given entire history distribution predicted naive empirical order markov model based bounded theorem states window length necessary predict well independent mixing time question holds even model mix. amount data required make accurate predictions using length windows scales exponentially —corresponding condition theorem chosen uniformly do)—our lower bounds discussed section argue exponential dependency unavoidable. mutual information past observations future observations intuitive parameterization complexity distribution sequences fact right quantity subtle. tempting hope mutual information bound amount memory would required store information past observations relevant distribution future observations. consider following setting given joint distribution random variables suppose wish deﬁne function maps binary advice/memory string possibly variable length independent given shown harsha joint distributions even average minimum length advice/memory string necessary task exponential mutual information setting also interpreted two-player communication game player generates generates given limited communication results show markov model—a model cannot capture long-range dependencies structure data—can predict accurately data-generating distribution provided order markov model scales complexity distribution parameterized mutual information past future. strikingly parameterization indiﬀerent whether dependencies sequence relatively short-range mixes quickly long-range mixes slowly all. independent nature dependencies provided mutual information small accurate prediction possible based recent observation. figure depiction states repeats given length binary sequence outputs hence mix. corollary theorem imply accurate prediction possible based short sequences observations. time increasingly complex models recurrent neural networks neural turing machines vogue results serve baseline theoretical result. also help explain practical success simple markov models kneser-ney smoothing crucial components state-of-the-art machine translation speech recognition systems. although recent recurrent neural networks yielded empirical gains current models still seem largely incapable successfully capturing long-range dependencies. worth noting advice/memory string sampled ﬁrst deﬁned random functions length related latter setting generated ﬁrst corresponds allowing shared randomness two-player communication game; however relevant sequential prediction problem. amusing example recent sci-ﬁ short sunspring whose script automatically generated lstm. locally sentence dialogue makes sense though cohesion longer time frames overarching plot trajectory settings natural language capturing long-range dependencies seems crucial achieving human-level results. indeed main message narrative conveyed single short segment. generally higher-level intelligence seems ability judiciously decide aspects observation sequence worth remembering updating model world based aspects. thus settings proposition interpreted negative result—that average error good metric training evaluating models. important note average prediction error metric ubiquitously used practice natural language processing domain elsewhere. results suggest diﬀerent metric might essential driving progress towards systems attempt capture long-range dependencies leverage memory meaningful ways. discuss possibility alternate prediction metrics section many settings ﬁnancial prediction lower level language prediction tasks used speech recognition average prediction error meaningful metric. settings result proposition extremely positive matter nature dependencies ﬁnancial markets suﬃcient learn markov model. obtains data learn higher higher order markov model average prediction accuracy continue improve. applications question becomes computational question naive approach learning th-order markov model domain alphabet size might require space store data learn. computational standpoint better algorithm? properties underlying sequence imply models learned approximated eﬃciently less data? positive results show accurate prediction possible algorithmically simple model— markov model depends recent observations—which learned algorithmically straightforward fashion simply using empirical statistics short sequences examples compiled suﬃcient amount data. nevertheless markov model parameters hence requires amount data scales learn bound size observation alphabet. prompts question whether possible learn successful predictor based signiﬁcantly less data. show that even special case data sequence generated hidden states possible general assuming natural complexity-theoretic assumption. hmms hidden states output alphabet size deﬁned parameters samples suﬃcient information theoretic standpoint learn model predict accurately. learning computationally hard begs question whether accurate prediction achieved computationally eﬃcient algorithm amount data signiﬁcantly less naive markov model would require. main lower bound shows exists family hmms sample complexity requirement necessary computationally eﬃcient algorithm predicts accurately average assuming natural complexity-theoretic assumption. speciﬁcally show hardness holds provided problem strongly refuting certain class csps mutual information observations drawn alphabet size computationally eﬃcient algorithm requires dω/\u0001) samples achieve average error bound holds large compared diﬀerent equally relevant regime alphabet size small compared scale dependencies sequence show lower bounds regime ﬂavor theorem except based problem learning noisy parity function; subexponential algorithm blum task means lose least superconstant factor exponent comparison positive results proposition proposition absolute constant suﬃciently large exists hidden states information-theoretically possible obtain average prediction error less error less using recent observations make prediction. mentioned above settings capturing long-range dependencies seems essential worth re-examining choice average prediction error metric used train evaluate models. possibility worst-case ﬂavor evaluate algorithm chosen time steps instead time steps. hence naive markov model longer well predicting well time steps prediction easy. context natural language processing learning respect metric intuitively corresponds training model well respect question answering task instead language modeling task. fertile middle ground average error worst-case error might re-weighted prediction error provides reward correctly guessing less common observations. seems possible however techniques used prove proposition extended yield analogous statements error metrics. given many settings average error natural metric prediction accuracy upper bounds proposition natural consider additional structure might present avoids computational lower bounds theorem possibility robustness property—for example property markov model would continue predict well even observation obscured corrupted small probability. lower bound instances theorem proposition rely parity based constructions hence sensitive noise corruptions. learning product distributions well known connections noise stability approximation low-degree polynomials additionally low-degree polynomials learned agnostically arbitrary distributions polynomial regression tempting hope thread could made rigorous establishing connection natural notions noise stability arbitrary distributions accurate low-degree polynomial approximations. connection could lead signiﬁcantly better sample complexity requirements prediction robust distributions parameter estimation. interesting compare using markov model prediction methods attempt properly learn underlying model. example method moments algorithms allow estimate certain class hidden markov model polynomial sample computational complexity. ideas extended learning neural networks input-output rnns using diﬀerent methods arora showed learn certain random deep neural networks. learning model directly result better sample eﬃciency also provide insights structure data. major drawback approaches usually require true data-generating distribution model family learning. strong assumption often hold practice. universal prediction coding theory. spectrum class no-regret online learning methods assume data generating distribution even adversarial however nature results fundamentally diﬀerent ours whereas comparing perfect model look inﬁnite past online learning methods typically compare ﬁxed experts much weaker. much work sequential prediction based kl-error information theory statistics communities. philosophy approaches often adversarial perspectives ranging minimum description length individual sequence settings model data distribution process assumed. regards worst case guarantees regret notion optimality line work minimax rates performance bayesian algorithms latter favorable guarantees sequential setting. regards minimax rates provides exact characterization minimax strategy though applicability approach often limited settings strategies available learner relatively small must exist). generally considerable work regret information-theoretic statistical settings works regards log-loss broadly considerable work information consistency minimax rates regards statistical estimation parametric non-parametric families settings e.g. minimax risk also work universal lossless data compression algorithm celebrated lempel-ziv algorithm here setting rather diﬀerent coding entire sequence rather prediction loss. sequential prediction practice. work initiated desire understand role memory sequential prediction belief modeling long-range dependencies important complex tasks understanding natural language. many proposed models explicit notions memory including recurrent neural networks long short-term memory networks attention-based models neural turing machines memory networks diﬀerentiable neural computers etc. models quite successful practice still largely fail capture many long-range dependencies–in case lstms example diﬃcult show forget past exponentially quickly stable gain insight problem began analyzing simplest markov predictor found surprise performed nearly well could hope. provide sketch proof theorem stronger proposition applies speciﬁcally sequences generated hidden markov model. core proof following lemma guarantees markov model knows true marginal probabilities short sequences predicting well. additionally good expected prediction hold respect randomness short window opposed randomness window begins settings ﬁnancial forecasting additional guarantee particularly pertinent; need worry possibility choosing unlucky time begin trading regime long plan trade duration spans entire short window. beyond extra strength result hmms proof approach intuitive pleasing comparison direct proof proposition ﬁrst state lemma sketch proof conclude section describing yields theorem lemma consider hidden states hidden state time chosen according arbitrary distribution denote observation time denote conditional distribution given observations knowledge hidden state time denote conditional distribution given corresponds naive order markov model knows joint probabilities sequences ﬁrst observations. probability least /nc− choice initial state proof lemma hinge establishing connection ts—the bayes optimal model knows initial hidden state time predicts true distribution given xs−—and naive order markov model knows joint probabilities sequences observations predicts accordingly. latter model precisely model knows observation contain signiﬁcant amount information hidden state time zero improve ms+. submartingale precisely capture sense signiﬁcant deviation expect probability initial state conditioned signiﬁcantly probability conditioned xs−. given observations x··· initial state drawn according hidden state time note convex combination hence verify submartingale property note bayes rule change time step ratio probability observing output according distribution probability according distribution expectation kl-divergence related error using pinsker’s inequality. several slight complications approach including fact submartingale construct necessarily nicely concentrated bounded diﬀerences ﬁrst term submartingale could change arbitrarily. address noting ﬁrst term decrease much except tiny probability corresponds posterior probability true hidden state sharply dropping. direction simply clip deviations prevent exceeding timestep show submartingale property continues hold despite clipping proving following modiﬁed version pinsker’s inequality corresponding empirical distribution length windows occurred connection lemma theorem established showing that high probability memp close denotes empirical distribution hidden states distribution corresponding drawing hidden state generating provide full proof appendix given distribution inﬁnite sequences {xt} generated model random variable denoting output time shorthand denote collection random variables subsequence outputs {xi··· xj}. distribution {xt} stationary joint distribution subset sequence random variables {xt} invariant respect shifts time index. hence looks past outputs. predictor maps sequence observations predicted distribution next observation. denote predictive distribution time refer bayes optimal predictor using windows length hence prediction time naive order markov predictor entire history model prediction time evaluate predictions respect long time window past future observations. stochastic process {xt} generated model deﬁne mutual information model mutual information past future averaged window measure distance predictive distributions. work consider kl-divergence distance relative zero-one loss distributions. kldivergence distance distributions deﬁned standard way. deﬁne proposition data-generating distribution mutual information past future observations best order markov model obtains average kl-error δkl) respect optimal predictor access inﬁnite history. also predictor ˆδkl) average kl-error estimating joint probabilities windows length gets average error δkl) ˆδkl). proof. bound expected error splitting time interval blocks length begin note decompose error error knowing past history beyond recent observations error estimating true joint distribution data length block. consider time recall deﬁnition i−−∞ xt|xt− therefore δkl) δkl) ˆδkl). it’s easy verify relation expresses intuition current output extra information past cannot predict well using recent observations done using entire past upper bound total error window expand error yields following statement also trivially applies zero/one loss respect optimal predictor expected relative zero/one loss time step loss time step. corollary data-generating distribution mutual information past future observations best order markov model obtains average -error predictor average -error estimating joint probabilities gets average error collection sets size whose elements consist variables negations. instance satisﬁable exists assignment variables predicate evaluates every clause. generally value instance maximum assignments ratio number satisﬁed clauses total number clauses. lower bounds based presumed hardness distinguishing random instances certain class versus instances high value. much work attempting characterize diﬃculty csps—one notion leverage complexity class csps ﬁrst deﬁned studied example k-xor k-sat well-studied classes k-csps corresponding respectively predicates pxor boolean inputs psat inputs. predicates support -wise uniform distributions k-wise uniform random instance predicate instance clauses chosen uniformly random random instance value close expectation uniform distribution. contrast planted instance generated ﬁrst ﬁxing satisfying assignment sampling clauses satisﬁed uniformly associated predicate. hence planted instance always value noisy planted instance planted assignment noise level generated sampling consistent clauses would expect diﬃculty distinguishing random instances noisy planted instances decreases number sampled clauses grows. following conjecture feldman asserts sharp boundary number clauses problem becomes computationally intractable remaining information theoretically easy. notation made explicit appendix conjectured hardness distribution k-clauses variables complexity polynomial-time algorithm that given access distribution equals either uniform distribution k-clauses planted distribution planted distribution decides correctly whether probability least needs clauses. feldman proved conjecture class statistical algorithms recently kothari showed polynomial time sum-of-squares algorithm requires clauses refute random instances complexity hence proving conjecture polynomial-size semideﬁnite programming relaxation refutation. note tight allen give algorithm refuting random csps beyond regime. recent papers daniely shalev-shwartz daniely also used presumed hardness strongly refuting random k-sat random k-xor instances small number clauses derive conditional hardness learning results. ﬁnal time step clauses correspond samples model algorithm would need solve predict ﬁnal time step however outputs ﬁnal time step random trivial prediction algorithm guesses randomly predict output time would near optimal. strong lower bounds statistical algorithms extension statistical query model algorithms access samples distribution instead access estimates expectation bounded function sample oracle. feldman point almost algorithms work random data also work limited access samples refer feldman details examples. elementary results theory error correcting codes achieve this prove hardness reduction speciﬁc family csps conjecture applies. choosing carefully obtain near-optimal dependence mutual information error \u0001—matching upper bounds implied proposition provide short outline argument followed detailed proof appendix. sketch construction proof construct sequential model making good predictions model requires distinguishing random instances k-csp variables instances high value. output alphabet {ai} size choose mapping characters {ai} variables {xi} negations {¯xi}. clause planted assignment k-bit string values assigned literals model randomly uniformly outputs characters time correspond literals hence outputs correspond clause csp. speciﬁed later construct binary matrix }m×k correspond good error-correcting code. time steps probability model outputs bits. note mutual information outputs time predicted. claim simulated hidden states. done follows. every time step maintain hidden states corresponding hidden states corresponding states stores current value bits takes total hidden states. hidden states time step output bits. deﬁne terms collection predicates conjecture directly apply deﬁned collection predicates instead single later show reduction related deﬁned single predicate conjecture holds. predicate satisfy label clause deﬁne planted distribution clauses ﬁrst uniformly randomly sampling label sampling consistent clause probability otherwise probability sample uniformly random clause. uniform distribution k-clauses uniformly chosen labels show conjecture implies distinguishing distributions hard without suﬃciently many clauses. gives hardness results desire sequential model algorithm obtains prediction error outputs time deﬁne another show reduces predicate hence planted assignment satisfying clauses clauses nullspace probability uniformly random k-clause. connext sketch reduction idea csps deﬁned linear equations. clause satisﬁed assignment variables clause therefore satisﬁes clause assignment variables obtained clause switching literal hence label eﬃciently convert clause clause desired label satisﬁed particular assignment variables satisﬁed assignment variables. also hard ensure uniformly sample consistent clause original clause uniformly sampled consistent clause output alphabet model letter maps variable maps similarly planted assignment deﬁnes particular model output model ﬁrst three time steps corresponds clause literals ﬁnal time step probability model outputs clause planted assignment probability outputs uniform random bit. algorithm make good prediction ﬁnal time step needs able distinguish output ﬁnal time step always random dependent clause hence needs distinguish random instances planted instances. ﬁxed constant exists family hmms hidden states output alphabet size that polynomial time prediction algorithm achieves average kl-error error relative zero-one error less probability greater randomly chosen family needs requires samples window length algorithm uses prediction. lower bounds sample complexity binary alphabet case based average case hardness decision version parity noise problem reduction straightforward. distribution examples parity noise instance support parity function noise level. distribution examples labels label chosen uniformly independent example. strength deﬁnition deﬁne function uniformly random support probability least choice algorithm distinguish success probability greater randomness examples algorithm requires time samples. example coupled several parity bits. denote model }m×n time outputs model i.i.d. uniform vector outputs time outputs next time steps given random noise entry i.i.d random variable noise level. note full row-rank chosen uniformly random distribution uniform also binary bits time predicted using past inputs. higher alphabet case simulated hidden states deﬁne matrices speciﬁes family sequential models. matrices sub-matrix corresponding rows ﬁrst columns full rank. need restriction lower bound otherwise could small dependence parity bits inputs time denote family models lemma shows high probability choice distinguishing outputs model random examples requires lemma chosen uniformly random then probability least choice algorithm distinguish outputs model distribution random examples success probability greater proposition deﬁned deﬁnition suﬃciently large ﬁxed constant exists family hmms hidden states algorithm achieves average relative zero-one loss average loss average loss less probability greater randomly chosen family needs requires time samples samples window length algorithm uses prediction. show information theoretically windows length ci/\u0001 necessary expected relative zero-one loss less expected relative zero-one loss loss bounded square kl-divergence automatically implies window length requirement also tight loss loss. fact it’s easy show tightness loss choose simple model emits uniform random bits time repeats bits time time choose desired error mutual information lower bound zero-one loss probabilistic method argue exists long windows required perform optimally respect zero-one loss hmm. state lower bound rough proof idea deferring details appendix proposition absolute constant suﬃciently large exits states information theoretically possible average relative zero-one loss loss less using windows length smaller loss less using windows length smaller n/\u0001. prediction. transition matrix permutation output alphabet binary. state assigned label determines output distribution. states labeled emit probability states labeled emit probability randomly uniformly choose labels hidden states. randomness choosing labels looks outputs time making prediction time show high probability choice labels hidden states outputs output hidden states close hamming distance label segment hidden states hence predictor using past outputs cannot distinguish whether string emitted hence cannot make good prediction time whose label close proof proceeds simple concentration bounds. theorem suppose observations generated hidden markov model hidden states output alphabet size exists window length absolute constant chosen uniformly random expected distance true distribution given entire history distribution predicted naive empirical order markov model based bounded empirical frequency hidden state time normalized consider predictor makes prediction distribution observation xt+s given observations xt+s− based true distribution conditioned observations xt+s− distribution hidden state time show expectation gets small error averaged across time steps respect optimal prediction distribution xt+s lemma probability choice hidden state time probability least proof. consider ordered time indices hidden state sets corresponding hidden states probability less cardinality |sj| cardinality small sets hence probability uniformly random lies sets consider time indices corresponding hidden state probability least among ﬁrst time indices hidden state probability least fraction time steps corresponding hidden state probability least total fraction time steps therefore using union bound failure probability hidden state time probability least consider time index simplicity assume denote conditional distribution given observations knowledge hidden state time denote conditional distribution given given hidden state time distribution lemma randomly chosen probability hidden state time probability less prior distribution hence using lemma expected average error predictor across n/\u0001. consider predictor predicts xt+s given xt+s− according empirical distribution xt+s given xt+s− based observations time argue predictions close expectation predictions recall prediction time true distribution conditioned observations xt+s− distribution hidden state time drawn refer prediction time refer prediction time show small expectation. using martingale concentration argument. consider string length empirical probability string time true probability string given hidden state time distributed show note zt−s/ azuma’s inequality union bound strings length dc/n /dc/ failure probability de−√ similarly strings length estimated probability string error /dc/ failure probability conditional distribution xt+s given observations xt+s− ratio joint distributions xt+s− xt+s} xt+s−} therefore long empirical distributions length length strings estimated error /dc/ string xt+s−} probability least /dc/ conditional distributions satisfy union bound strings total probability mass strings occur probability less /dc/ /dc/ therefore overall failure probability using triangle inequality fact expected average error follows expected average error note expected average error average expected errors empirical s-gram markov models hence must exist least s-gram markov model gets expected error prior distribution hidden states time true hidden state time without loss generality. refer output time posterior probability hidden state time seeing observations time prior distribution hidden states time convenience denote distribution output time conditioned hidden state time observations before deﬁne conditional distribution given observations x··· initial distribution hidden state note convex combination i.e. usop vsrs. hence deﬁne proof relies martingale concentration argument order ensure martingale bounded diﬀerences ignore outputs cause signiﬁcant drop posterior true hidden state time outputs time clog hence union bound failure probability output clog emitted window length clog n/\u0001. hence concern sequences outputs clog outputs output emitted step satisﬁes expectation random variable conditioned note deﬁning log− log. change seeing output bound average error window failure probability randomness outputs −log sequences satisfy −log note log. consider last point decreases remains every subsequent step window. point point deﬁne total contribution error every step step average error term error step note hence sequences variables satisfying assignment k-csp deﬁned predicate represent k-clause ordered k-tuple literals {x··· ¯x··· ¯xn} repetition variables k-clauses. k-clause k-bit string values assigned literals {σ··· value literal assignment model draw clauses probabilities depend value choose ensure high complexity. predicate solutions system deﬁne uniform distribution consistent assignments i.e. satisfying planted distribution deﬁned based according clause chosen ﬁrst picking uniformly random clause distribution qσy. planted deﬁne distribution consistent clauses along labels uniform distribution k-clauses clause assigned uniformly chosen ﬁxed noise level consider label deﬁne small constant less corresponds adding noise problem mixing planted uniform clauses. problem gets harder becomes larger eﬃciently solved using gaussian elimination. deﬁne another show reduces obtain hardness using conjecture label ﬁxed zero vector hence distribution satisfying assignments uniform distribution vectors null space binary ﬁeld. refer planted distribution case uniform distribution k-clauses clause label planted assignment denote distribution consistent clauses deﬁne chosen success probability least similarly problem randomly uniformly chosen distinguishing success probability least thought problem distinguishing random instances csps instances high value. note least hard problem refuting random instances corresponds case claim algorithm implies algorithm }m}. parity check matrix code matrix distance code weight minimum weight codeword relative distance deﬁned d/k. codeword deﬁne dual codeword codeword generator matrix parity check matrix note rank dual codeword code rank following standard taking hence exists code whenever setting we’re interested choose generator matrix hence null space -wise uniform hence complexity hence }m×k ensure complexity construct sequential model derives hardness hardness slightly diﬀer outline presented beginning section cannot base sequential model directly generating random k-tuples without repetition increases deﬁne instance allowing repetition diﬀerent setting examined feldman hardness setting repetition follow hardness setting allowing repetition though converse true. consider following family sequential models }m×k chosen deﬁned previously. output alphabet models family size even. choose subset size choice corresponds model family. letter output alphabet encoded represents whether letter included vector stores encoding whenever letter determine subset entry choose uniformly random choice represents subset hence model partition output alphabet subsets size time chooses uniformly random time {··· model chooses letter uniformly random otherwise chooses letter uniformly random probability outputs next time steps probability uniform random bits. model resets time repeats process. given samples naturally seen csp. sample clause literal corresponding output letter whenever ¯xi/ even. refer reader outline beginning section example. denote modiﬁcation literal clause literal corresponding letter deﬁne distribution consistent clauses deﬁne uniform distribution k-clauses additional constraint literal clause literal corresponding letter deﬁne note samples model equivalent clauses show hardness follows hardness lemma solved time clauses solved time clauses. hence conjecture true cannot solved polynomial time less clauses. ﬁxed constant exists family hmms hidden states output alphabet size that polynomial time prediction algorithm achieves average kl-error error relative zero-one error less probability greater randomly chosen family needs requires samples window length algorithm uses prediction. average zero-one loss polynomial time algorithm output time steps average relative zero-one loss output time steps respect optimal predictions. distribution possible clauses label independent chosen uniformly random hence algorithm gets error used distinguish therefore lemma polynomial time algorithm gets tween probability greater choice needs least samples. note optimal predictor gets therefore note average error time steps contribution error time steps non-negative. also hence polynomial time algorithm gets average relative zero-one loss less probability greater needs least samples. result loss follows directly result relative zero-one loss next consider loss. average error algorithm time steps application jensen’s inequality pinsker’s inequality needs samtherefore previous argument algorithm gets ples. before hence polynomial time algorithm succeeds probability greater gets average loss less needs least samples. next claim randomly generated clause distribution drawn randomly generated clause distribution drawn construction label clause chosen uniformly random. note choosing clause uniformly random equivalent ﬁrst uniformly choosing k-tuple unnegated literals choosing negation pattern literals uniformly random. clear clause still uniformly random adding another negation pattern uniformly random before. hence original clause drawn uniform distribution distributed according similarly choosing clause uniformly random equivalent ﬁrst uniformly choosing k-tuple unnegated literals choosing negation pattern uniformly random makes clause consistent. original negation pattern corresponds randomly chosen null space ﬁnal negation pattern adding corresponds negation pattern uniformly random chosen solution proof lemma lemma solved time clauses solved time clauses. hence conjecture true cannot solved polynomial time less clauses. proof. deﬁne event clause generated distribution property literal belongs also refer property clause notational ease. it’s easy verify probability event /kk. claim conditioned event equivalent. uniform consistent clauses. clauses non-zero probability clauses non-zero probability furthermore satisﬁes constraint clauses similarly clauses note subset clauses satisfy holds every consistent distributions uniform consistent clauses distribution clauses identical distribution clauses conditioned event equivalence note k-tuples chosen uniformly random satisfying k-tuples high probability tuples property clauses problems equivalent conditioned event solved time clauses solved time clauses. lemma conjecture cannot solved polynomial time less clauses. hence cannot solved polynomial time less clauses. constant respect cannot solved polynomial time less clauses. proof lemma lemma chosen uniformly random then probability least choice algorithm distinguish outputs model distribution random examples success probability greater proof. suppose }m×n chosen random entry i.i.d. distribution uniform sub-matrix corresponding ﬁrst columns rows. recall matrices sub-matrix full row-rank. claim m−n/. verify consider addition probability linearly dependent previous rows i−−n/. hence union bound full row-rank failure probability mm−n/ m−n/. deﬁnition union bound parities algorithm distinguish outputs model uniformly chosen distribution random examples probability least choice needs time examples. m−n/ uniformly randomly chosen probability least m−n/) choice algorithm distinguish outputs model distribution random examples proposition deﬁned deﬁnition suﬃciently large ﬁxed constant exists family hmms hidden states algorithm achieves average relative zero-one loss average loss average loss less probability greater randomly chosen family needs requires time samples samples window length algorithm uses prediction. suﬃciently large ﬁxed constant hence proving hardness obtaining error implies hardness obtaining error choose matrix am×n outlined earlier. family deﬁned model deﬁned previously matrix am×n chosen uniformly random average zero-one loss algorithm output time steps average relative zero-one loss output time steps respect optimal predictions. distribution possible clauses label independent chosen uniformly random information theoretically possible hence algorithm gets error used distinguish lemma algorithm gets probability greater choice needs least time samples. note optimal predictor gets therefore note average error time steps contribution error time steps non-negative. also therefore hence algorithm gets average relative zero-one loss less probability greater choice needs time samples. result loss follows directly result relative zero-one loss next consider loss. proposition absolute constant suﬃciently large exits states information theoretically possible average relative zero-one loss loss less using windows length smaller loss less using windows length smaller n/\u0001. proof. consider hidden markov model markov chain permutation states. output alphabet hidden state binary. state marked label mapping hidden state label states labeled emit probability probability similarly states labeled emit probability probability fig. illustrates construction figure lower bound construction note notation used rest proof respect example corresponds label case. similarly case. segments shaded nodes comprise assume multiple constant regard constant respect refer hidden states refers sequence hidden states show model looking past outputs cannot average zero-one loss less optimal prediction looking past outputs gets average zero-one loss minimize expected zero-one loss given outputs time predict mode distribution r+|x sequence r+|hi=i) outputs time also note r+|x hidden state time hence predictor weighted average prediction hidden state weight probability hidden state. make prediction time providing index true hidden state time hence narrows possible hidden states time bayes optimal prediction time given outputs time note deﬁnition hence index predict mode r+|x bayes optimality average zero-one loss prediction using r+|x cannot worse average zero-one loss prediction using r+|x permutations. show expected average zero-one loss predictor randomness choosing permutation means must exist permutation average zero-one loss predictor permutation .−o. expected average zero-one loss predictor randomness choosing permutation expected average zero-one loss predictor given state time without loss generality hence hidden state time sequence labels hidden states time expected average zero-one error predictor randomness rest outputs. argue e−)] hidden states deﬁnes segment permutation label g+)− segment excluding last corresponds predictions. labels excluding ﬁrst label {g+)+)∀ predicted bits high probability output hidden states inequality outputs independent conditioned hidden state– argue high probability hamming distance hence segments assigned much weight predicting next output means output cannot predicted high accuracy output bits corresponding diﬀerent segments independent. high probability randomly choosing segments subset segments segments hamming distance less consider segments subset respect string permutations predictor places least much weight hidden states true hidden state prediction hidden state corresponding notice bits predictor randomness choosing permutation means must exist permutation average zero-one loss predictor permutation hence exists states information using windows length smaller result relative zero-one loss follows replacing setting result follows immediately expected relative zero-one loss less expected loss. kl-loss pinsker’s inequality jensen’s inequality.", "year": 2016}