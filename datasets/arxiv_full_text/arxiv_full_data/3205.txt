{"title": "Beyond L2-Loss Functions for Learning Sparse Models", "tag": ["stat.ML", "cs.CV", "cs.LG", "math.OC", "I.2.6; G.1.6"], "abstract": "Incorporating sparsity priors in learning tasks can give rise to simple, and interpretable models for complex high dimensional data. Sparse models have found widespread use in structure discovery, recovering data from corruptions, and a variety of large scale unsupervised and supervised learning problems. Assuming the availability of sufficient data, these methods infer dictionaries for sparse representations by optimizing for high-fidelity reconstruction. In most scenarios, the reconstruction quality is measured using the squared Euclidean distance, and efficient algorithms have been developed for both batch and online learning cases. However, new application domains motivate looking beyond conventional loss functions. For example, robust loss functions such as $\\ell_1$ and Huber are useful in learning outlier-resilient models, and the quantile loss is beneficial in discovering structures that are the representative of a particular quantile. These new applications motivate our work in generalizing sparse learning to a broad class of convex loss functions. In particular, we consider the class of piecewise linear quadratic (PLQ) cost functions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge loss, and smoothed variants of these penalties. We propose an algorithm to learn dictionaries and obtain sparse codes when the data reconstruction fidelity is measured using any smooth PLQ cost function. We provide convergence guarantees for the proposed algorithm, and demonstrate the convergence behavior using empirical experiments. Furthermore, we present three case studies that require the use of PLQ cost functions: (i) robust image modeling, (ii) tag refinement for image annotation and retrieval and (iii) computing empirical confidence limits for subspace clustering.", "text": "deriving predictive inference data requires approximating generating process using model estimating model parameters input data observed responses. generating process approximated input data sample parameters. linear representation reduces classical linear model given vector observed responses input data matrix rk×t parameters estimated using linear regression loss residual minimized. complexity linear model reduced shrinking small entries zero approach gives sparse linear model small fraction parameters non-zero sparse parameters allow improved model interpretability parsimony viewpoint statistical learning theory sparsity also improves generalizability hence usefulness model. loss function measures distance between sparsity regularizer regularization penalty controls trade-oﬀ between loss regularization. choice loss function corresponds noise deviation model discrepancy observed predicted data. sparse models widespread applications speech audio processing image analysis recovery compressive sampling blind source separation unsupervised supervised semi-supervised transfer learning assumed pre-deﬁned dictionary available sparse coding. however given obseri= suﬃciently large dictionary adapted data itself jointly minimizing additional constraints also placed dictionary sparse codes. existing dictionary learning incorporating sparsity priors learning tasks give rise simple interpretable models complex high dimensional data. sparse models found widespread structure discovery recovering data corruptions variety large scale unsupervised supervised learning problems. assuming availability suﬃcient data methods infer dictionaries sparse representations optimizing high-ﬁdelity reconstruction. scenarios reconstruction quality measured using squared euclidean distance eﬃcient algorithms developed batch online learning cases. however application domains motivate looking beyond conventional loss functions. example robust loss functions huber useful learning outlier-resilient models quantile loss beneﬁcial discovering structures representative particular quantile. applications motivate work generalizing sparse learning broad class convex loss functions. particular consider class piecewise linear quadratic cost functions includes huber well quantile vapnik hinge loss smoothed variants penalties. propose algorithm learn dictionaries obtain sparse codes data reconstruction ﬁdelity measured using smooth cost function. provide convergence guarantees proposed algorithm demonstrate convergence behavior using empirical experiments. furthermore present three case studies require cost functions robust image modeling reﬁnement image annotation retrieval computing empirical conﬁdence limits subspace clustering. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. york city copyright x-xxxxx-xx-x/xx/xx ..... squared error penalty arguably widely used loss function. however expect outliers data robust loss function imposed; general proper choice loss function necessary estimate parameters noise-robust manner turn lead improved predictive power future data. straightforward examples econometrics model market value company linear combination various accounting numbers could years company events like economic depression image processing pixels corrupted saturation noise sensors. convex asymmetric function used extensively regression. used understand predict response process various quantiles. example time-varying attrition workforce company posed regression problem incentive variables quantile regression allows predict future attrition various quantiles planning purposes management best-case worst case attrition high quantiles respectively. actual attrition along median high quantile estimates period time particular company provided figure addition predictions quantiles used obtain interquartile range robust measure statistical dispersion. provides non-parametric distribution free conﬁdence limit estimates. using proposed approach dictionaries obtained various quantiles used obtain estimates predictions. case using block-assignable penalty arise observation heterogeneous. needs constructed penalize elements residual diﬀerently others. example image analysis applications overall feature obtained image combination multiple features require diﬀerent loss. could also situations diﬀerent noise model applies component hence diﬀerent loss functions needed. case predicting tags images combination image level features loss used) user tags loss required possibility sparse errors). paper propose dictionary learning framework general class piecewise linear quadratic penalties show sparse code update problem formulation measurement regularization graph structure penalties solved recently developed solver generalized approach full dictionary learning problem implement block-coordinate scheme prove convergence assumption measurement penalty diﬀerentiable. classic dictionary learning framework requires alternating minimization sparse codes dictionary latter problem block-coordinate descent update columns using eﬃcient l-bfgs method barzilai-borwein step-length selection. important note method frameworks literature customized case loss function. applications dictionaries inferred misﬁt loss eﬀective include robust background modeling emerging topic detection novel document identiﬁcation paper explore develop ﬂexible dictionary learning sparse coding framework allowing member class functions rich enough address realworld challenges. class include goals achieved considering general class piecewise linear quadratic penalties comprise convex penalties whose domain represented union ﬁnitely many polyhedral sets relative penalty expressed general quadratic. wide class contains robust penalties huber vapnik asymmetric penalties quantile quantile important penalties shown figure details proposed framework available section note learning dictionary also referred learning sparse model paper without loss generality assume regularization measure. going beyond loss functions allows much better noise rejection compared recovery obtained penalty. second case consider problem reﬁning human annotated tags image data set. since tags similar images similar perform joint sparse coding features tags using mixed penalty penalty features huber penalty tags. mixed penalty provides robust estimates compared using features tags varying levels impulse noise. third application evaluate performance subspace clustering using graphs various data sets multiple quantiles. performance diﬀerent quantiles around median used evaluate empirical conﬁdence bounds median accuracy. problem nonconvex typically solved block-coordinate descent fashion dictionary codes updated turn held ﬁxed. note ﬁxed problem updating fully decouples frobenius norm every column updated parallel. generalizes perfectly loss function written penalties across columns; assume loss functions property. refer problem code update problem. ﬁxed problem updating requires paper propose modeling framework optimization scheme general enough handle requirements well simple constraints speciﬁcally allow come class piecewise linear quadratic penalties mixture several penalties. recently showed broad subclass penalties given natural statistical interpretation used conjugate representation devise generic interior point method solution. method also eﬃciently incorporates simple constraints next section review general class penalties characterize properties class make particularly useful modeling speciﬁc applications. specify representations penalties present experimental section. discuss method solving dictionary update problem consider convergence entire scheme. enable practitioners develop test kinds penalties extended interface allow diﬀerent penalties diﬀerent blocks residual vector automatic moreau-yosida smoothing arbitrary penalties. latter feature ensures convergence block coordinate descent applied formulation. extensions communicated theoretical lemmas related conjugate representation calculus. illustrate utility proposed approach apply algorithm three diﬀerent real-world scenarios provide experimental evaluations. ﬁrst scenario concerns robust modeling images corrupted sparse noise. case train dictionary sparse coding patches taking robust huber penalty. dictionary used reconstruct patches brieﬂy review class quadratic support functions referring reader full exposition. every penalty class written convex conjugate quadratic function arbitrary ability represent penalties structures gives rise representation calculus addition aﬃne composition manipulations done using underlying structures. highlight three results particularly useful encoding variants code update problem. lemmas show inference problems involving sums aﬃne compositions coordinate-wise different penalties written minimization problem primal variable note simple evaluation function candidate point requires partial minimization respect conjugate variable therefore seem made problem complicated; however keep mind choose component penalties wide common candidates alternative representations disposal conjugate representation introduced purpose obtaining minimum conjugate representation able write karush-kuhn-tucker system optimality conditions entire class interest. systems often used characterize optimality optimization programs design algorithms; advantage conjugate representation uniform approach characterizing wide variety nonsmooth optimization programs. details encoded within representation formed automatically individual components using calculus described previous section. system dual variables corresponding resulting equality constraints. hand problem solved relaxing complementarity slackness conditions using damped newton’s method directly optimize relaxed system. full convergence theory problem without inequality constraints presented shows constraints included. context dictionary learning penalty used sparse code update problem. since directly solve system using method always direct access optimality certiﬁcate; namely system itself. however guarantees hold code update problem next section discuss overall approach dictionary learning focus dictionary update problem. consider full nonconvex problem natural approach alternate updating spare codes dictionary instance block coordinate descent. penalties smooth standard convergence results block coordinate descent obtained e.g. however sparse dictionary learning taken nonsmooth usually norm. addition interested theorem allows smooth member class particular amount obtain representation resulting function closed form. power idea shown following corollary ﬁxed sparse codes explain solve problem prove convergence scheme. least squares case straightforward implement block-coordinate optimization scheme columns obtaining closed-form updates loop columns. general case pose wish update j-th column letting denote denote column denote dictionary column deleted easy since -dimensional optimization problem barzilaiborwein line search method equivalent newton’s method quadratic case motivated this l-bfgs barzilai-borwein line search solve quadratic method converges iterations column expected general smooth huber also rapidly convergent. since theorem requires smooth block-column coordinate descent converges empirical convergence overall block-coordinate descent scheme proposed batch dictionary learning problem shown figure real-world data set. clearly loss fastest convergence compared huber quantile huber since well-behaved among three. block-coordinate descent class problems general enough accommodate framework studied main theorem still depends smoothness condition unfortunately points condition sense sharp block coordinate descent fail converge convexity block required. proof. assumption diﬀerentiable eﬀective domain; furthermore entire objective convex every cluster point sequence generated block-coordinate descent stationary point application perspective claim requirement smooth particularly limiting. understand recall behavior penalty origin strongest inﬂuence sparsity properties; particular penalty choice contrast acts residuals; implication choosing smooth means data points exactly. potentially useful applications others; recently demonstrated smoothed version quantile penalty called quantile huber outperform standard quantile penalty sparse regression setting. next natural question suppose given penalty candidate smooth; disciplined procedure smooth remain class? amazingly turns penalty easily smoothed using moreau envelope moreover application technique represented using calculus relied lemmas unique minimizer smoothing parameter. moreau-yosida envelope function also know prox operator plays major role optimization formulations many signal processing applicasalient feature penalties closed moreau-yosida smoothing envelope function precisely captured representation shown reproduced convenience notation section present three diﬀerent case studies demonstrate importance adopting general framework.though sophisticated applications sparse models considered emphasis illustrating ﬂexibility robustness proposed framework comparison conventional sparse modeling approaches. conclude section brieﬂy discussing possible extensions work relevant applications beneﬁt general framework developed paper. statistics natural images motivates sparse models describe makes possible recover diﬀerent forms corruption. simplicity consider corruption additive noise happen sensing transmission. scenarios generalizable model ignore underlying noise describe relevant patterns image. robust model used denoise image improve quality. noise gaussian traditional sparse modeling framework uses loss function eﬀective discovering patterns masked noise. however noise model nongaussian sparse model learned using procedure longer robust. consider case image corrupted saltand-pepper noise manifests randomly occurring white black pixels image. typical noise reduction strategy kind noise apply median ﬁltering. therefore propose huber penalty loss function since learn median patterns dictionary thereby resulting robust model. however using loss function infer noisy patterns since tends spread noise pattern hence denoising performance using dictionary also poor. matrix denoted adding salt-and-pepper noise speciﬁc percentage equivalent randomly replacing percentage pixels black white pixel. experiment vary noise level learn dictionaries using diﬀerent penalties compare reconstruction obtained using learned sparse model original clean image. model robust expect impulse noise part dictionary elements hence reconstruction high quality. note perform explicit denoising evaluate quality reconstruction model. measure peak-signal-to-noise ratio noisy image images recovered using sparse models learned huber penalties. figure shows results obtained increasing levels impulse noise robustness huber penalty clearly evident higher psnr values well improved visual quality. textual descriptors tags useful meta-data images retrieval applications. large scale retrieval systems typical present textual query retrieve semantically relevant images. since single semantic concept manifest wide range visual representations often diﬃcult mine database using visual features tags. furthermore human annotation subjective error-prone. goal automatic image annotation predict tags possibly reﬁne existing noisy tags based information visually similar images. experiment consider problem reﬁning noisy tags novel image using training images. image vector typically binary vector indicates relevance semantic topic pre-deﬁned vocabulary. human errors limitation prediction systems unrelated concepts could included image description important topics could left out. sparse low-rank models learned using visual features noisy tags eﬀective reﬁning semantic descriptors given training images gist features describe visual content. visual features stored matrix corresponding textual descriptors stored matrix given novel image feature noisy vector goal obtain reﬁned estimate propose exploit correlations features tags using sparse coding perform reﬁnement. using training examples construct dictionary scaling factor used balance total energy features tags. similarly test sample described assuming features tags clustered along subspaces structure discovered figure robust image modeling shows images corrupted increasing levels salt pepper noise. rows show images recovered using sparse models learned huber penalties respectively. case corresponding psnr value also reported. robustness achieved considering appropriate loss function corruption non-gaussian clearly evident. reﬁned vector estimated formulation assumes features semantic descriptors recovered using sparse coeﬃcients. however penalty robust thus unsuitable measuring misﬁt reconstruction vectors. improve recovery diﬀerent penalties modeling visual features vectors experiment used corel-k data commonly used comparative data image annotation. images total image annotated keywords. used images training data evaluated performance using rest. total number keywords vocabulary varied level noise test tags entries binary vector. estimated reﬁned tags using schemes described earlier computed average noise reﬁned vectors. figure plots performance obtained using penalty seen robust variant using mixed penalty provides improved recovery noise levels. furthermore corrupted vectors training data also diﬀerent levels noise studied performance deterioration found using mixed penalties provided superior performance cases. assuming data samples union subspaces allows perform unsupervised clustering using sparse coeﬃcients constructing suitable graph describe relationship data samples analyze eigen spectrum graph laplacian determine underlying clusters. particular build graph unlabeled data solving sparse codes using data samples dictionary constraint figure evaluating performance subspace clustering using -graphs various quantiles diﬀerent datasets. quantile-speciﬁc performances used obtain empirical conﬁdence limits median performance. clustering performance loss also provided comparison quantile huber results nearest neighbors. coeﬃcient matrix rt×t identity matrix. model eﬀective several scenarios using loss function obtain sparse code matrix makes highly non-resilient outliers. furthermore possible estimate conﬁdence measures clustering performance using loss function. propose employ quantile huber penalty overcome shortcomings generate reliable clusterings. using quantile measure misﬁt equivalent allowing fraction entries residuals positive. unless complete model misﬁt penalty deteriorate gradually consider quantiles away median. observed previous case studies using huber penalty makes sparse models robust outliers. order derive empirical conﬁdence intervals understand reliability clustering generate multiple graph based clusterings diﬀerent quantiles study corresponding clustering performances. experiment consider three datasets repository ecoli wine breast cancer datasets. case build graphs multiple quantiles evaluate clustering performance accuracy. figure illustrates clustering performances three datasets. case report performance obtained using penalty comparison. addition improving clustering accuracy using ﬂexible loss function enables understand reliability clustering results. example case wine dataset though median performance high performance drops signiﬁcantly move away median quantile. shows clustering sensitive outliers small perturbations dataset might result sub-optimal performance. behavior attributed limited availability samples non-suitability chosen generative model. though union subspace assumption seems valid data lack high conﬁdence suggests choose diﬀerent model assumptions clustering. analyzing complex high-dimensional data requires design interpretable robust scalable models. proposed general framework inherent advantage allowing compute sparse codes optimize dictionaries using broad class misﬁt losses. however challenges scaling framework large scale settings inability perform warm-starts interior point methods make design online learning algorithms difﬁcult. using proximal methods lieu interior point solvers possible approach overcome shortcomings. addition enabling fast sparse code computation using warm-starts online dictionary inference allow beyond sparsity regularization incorporate penalties nuclear norm regularization. another important extension evaluate proposed framework using graph penalties sparse codes batch learning. useful incorporate similar penalties online learning well. believe general scalable framework expand applicability sparse models data analytics. important application areas beneﬁt development include matrix completion recommender systems topic modeling text analytics analysis interactions large networks semantic content analysis images/videos data visualization. component analysis blind source separation principles perspectives challenges. esann’ proceedings-th european symposium artiﬁcial neural networks pages hastie tibshirani friedman hastie friedman tibshirani. elements statistical learning. springer wang zhang h.-j. zhang. multi-label sparse coding automatic image annotation. computer vision pattern recognition cvpr ieee conference pages ieee", "year": 2014}