{"title": "Learning a bidirectional mapping between human whole-body motion and  natural language using deep recurrent neural networks", "tag": ["cs.LG", "cs.CL", "cs.RO", "stat.ML"], "abstract": "Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions (e.g. in the form of motion primitives), which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2,846 human whole-body motions and 6,187 natural language descriptions thereof from the KIT Motion-Language Dataset. Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed natural language descriptions from human motions.", "text": "abstract linking human whole-body motion natural language great interest generation semantic representations observed human behaviors well generation robot behaviors based natural language input. large body research area approaches exist today require symbolic representation motions deﬁned a-priori require complex segmentation algorithms. contrast recent advances ﬁeld neural networks especially deep learning demonstrated sub-symbolic representations learned end-to-end usually outperform traditional approaches applications machine translation. paper propose generative model learns bidirectional mapping human whole-body motion natural language using deep recurrent neural networks sequence-to-sequence learning. approach require segmentation manual feature engineering learns distributed representation shared motions descriptions. evaluate approach human whole-body motions natural language descriptions thereof motion-language dataset. results clearly demonstrate eﬀectiveness proposed model show model generates wide variety realistic motions descriptions thereof form single sentence. conversely model also capable generating correct detailed natural language descriptions human motions. intriguing instruct robot ﬁrst demonstrate task hand. setup human teacher performs necessary steps robot observes human’s motion. robot programming commonly referred programming demonstration extensively studied. however observing motion human teacher often suﬃcient demonstrator often include additional corrective instructions student using natural language. words teacher-student interaction inherently multi-modal. natural language presents intuitive communicating robot since used describe even rather complex motions parameterizations. example description person waves left hand times. encodes motion body part perform number repetitions enabling robot combine rich descriptions natural language human wholebody motion therefore facilitate much richer humanrobot communication. corresponding author matthias plappert high performance humanoid technologies institute anthropomatics robotics karlsruhe institute technology adenauerring karlsruhe germany paper deep learning techniques link human whole-body motion natural language. speciﬁcally make sequenceto-sequence learning learn bidirectional mapping human whole-body motion natural language. human whole-body motion represented joint space mater motor framework descriptions thereof form complete english sentences. figure illustrates desired mapping. hand mapping allows generate rich descriptions observed human motion example used motion database. hand model capable generating wide range diﬀerent motions description thereof natural language. even proposed system capable successfully synthesizing certain variations motion e.g. waving left right hand well walking quickly slowly simply specifying parametrization natural language description. approach combining human motion natural language. section describes detail represent modalities human motion natural language proposed bidirectional mapping. model used learn mapping presented section section show proposed approach capable learning desired bidirectional mapping. also analyze model learned representations depth. finally section summaries discusses results points promising areas future work. diﬀerent models encode human motion proposed literature. takano kulic herzog hidden markov models learn observation. model also used generate motion sequences sampling taylor taylor hinton propose conditional restricted boltzmann machines learn generate human whole-body motion. calinon gaussian mixture model encode motion data. diﬀerent approach proposed schaal uses dynamic movement primitives model motion using diﬀerential equations. recently fragkiadaki jain used recurrent neural networks learn generate human motion observation. mordatch combined trajectory optimization deep neural networks generate complex goal-directed movement diverse characters. b¨utepage deep feed-forward neural networks encoder-decoder architecture learn lower-dimensional latent representation used classiﬁcation motion generation. many models proposed encode human motion less research conducted question combine human motion natural language. takano takano nakamura describe system allows learn mapping human motion word labels. authors segment human motion encode resulting motion primitives hidden markov models form motion symbol space proto symbol space. similarly word space constructed associated word labels. finally authors describe projection motion symbol space word space allows obtain sequence motion symbols sequence words vice versa. takano nakamura learn bidirectional mapping human motion natural language form complete sentence. authors propose components that combined realize desired mapping. motion language model motion primitives encoded hmms probabilistically related words using latent variables. latent variables represent non-observable properties like semantics. conditional probabilities govern association motion language obtained using algorithm. second part natural language model captures syntactical structure natural language. diﬀerent approaches realize model described authors e.g. hmms bigram models models ﬁnally combined generate natural language descriptions motion ﬁrst recognizing motion obtaining unordered likely words associated motion ﬁnally ﬁnding likely sequence words approach commonly referred bag-of-words. similarly likely motion obtained description thereof natural language searching motion symbol maximum likelihood given word sequence. learned using parametric hidden markov models used adverbs parametrize phmms natural language modeled similar aforementioned works using bigram language model. although approach enable generation textual representations motions presented evaluation covers direction motion generation textual descriptions. evaluation considers rather limited diﬀerent motion primitives words based virtual scenario humanrobot cooperation task degrees freedom haptic interface. diﬀerent approach described sugita tani ogata authors recurrent neural network parametric bias model combine movement simple robots simple commands form words authors demonstrate approach generate appropriate trajectories corresponding given command vice versa. attempts made gradually increase complexity supported variety commands recent years extensive work ﬁeld deep learning combine natural language modalities like images videos recent survey proposed solutions problem visualizing natural language descriptions given hassani work consider human whole-body motion recorded optical markerbased motion capture system. brieﬂy speaking system observes reﬂective markers placed speciﬁc anatomical landmarks human subject using multiple cameras positioned around subject. cartesian coordinates marker reconstructed using triangulation. indepth discussion motion capture techniques refer reader field approach allows highly accurate motion acquisition resulting representation motion form marker trajectories several drawbacks. first positions markers depend reference coordinate system varies across recordings thus would require sort normalization obtain invariant representation. second diﬀerent marker sets varying number also introduce additional binary feature enabled long motion ongoing. feature necessary reasons first sequences equal length implementation details binary indicates active part motion. second importantly generative part proposed model also predict binary thus used indicate generation still ongoing ﬁnished. joints zero mean variance one. also downsample motion results shorter sequences less resourceexpensive training evaluation model. however discard remaining motion data since split original motion sequence down-sampled sequences applying variable figure representation exemplary human motion subject walks forward. ﬁrst dimensions joint values model dimension binary active feature plotted time. motion padded explains constant values ﬁrst timesteps. notice motion active part indicated binary feature. markers diﬀerent marker locations human body used. third data high-dimensional since marker position requires three dimensions usually markers used. therefore master motor framework represent human whole-body motion joint space. realized reference model speciﬁes kinematics human body conversion cartesian joint space achieved minimizing squared distance physical markers human subject virtual markers reference model w.r.t. joint angles kinematic model. resulting joint representation longer dependent reference coordinate system abstracts away concrete marker used recording signiﬁcantly lower dimensionality cartesian representation. represent natural language annotations word-level. concretely ﬁrst normalize transforming sentence lower case letters remove punctuation apply minor spelling corrections commonly misspelled words. also sequences equal length introducing special word similarly special words mark start sentence respectively. next tokenize sentence individual words assign word unique integer index. however representation problem dimensionality grows linearly word embeddings resolve problem projecting one-hot encoded word continuous much lower-dimensional embedding space. depending training procedure embeddings also shown group semantically similar words closer together. work learn projection end-to-end training entire model. however future work might consider weights embedding layer pre-trained large text corpus e.g. using open source wordvec implementation.∗ sequence-to-sequence learning used great success e.g. large-scale machine translation name suggests goal models generate target sequence input sequence sequences diﬀer length modality. property makes sequenceto-sequence learning excellent purpose learning mapping human motion natural language. model direction human motion natural language well natural language human motion individually. figure depicts details models. cases input sequence ﬁrst transformed latent context vector recurrent neural network stack thereof output recurrent encoder network last timestep input sequence processed used context vector. context vector decoded decoder network therefore acts coupling mechanism encoder decoder network. diﬀerent approaches proposed provide context vector input decoder network timestep. decoder produces desired target sequence step step. vanilla like long short-term memory gated recurrent units used. encoder often uses bidirectional rnns process input sequence directions combine computed latent representations e.g. concatenation. grus bidirectional encoder work proposed model used recurrent network architecture. model decoder output probabilistically means network predicts parameters probability distribution instead predicting output value directly. intermediate output denoted form depends figure overview proposed models directions. figure depicts model learns mapping motion language ﬁrst encoding motion sequence context vector using stack bidirectional rnns context vector decoded another stack unidirectional rnns also takes embedded word generated previous timestep input. fully-connected layer produces parameters output probability distribution denoted decoder ﬁnally takes transforms concrete word combined model thus generates corresponding description word word special token emitted description obtained. direction language motion depicted figure model works similar fashion uses description natural language generate corresponding whole-body motion models directions trained individually share weights. probabilistic representation concrete deterministic instance. would greedily select instance highest probability distribution. however strategy necessarily yield sequence highest probability. hand expanding possible node computational expensive discrete case intractable continuous case. therefore common middle-ground extremes beam search beam search modiﬁcation best-ﬁrst search step limited stored nodes considered expansion. since output network depend decision decoder feed back decision timestep. input timestep thus concatenation decoded output previous timestep context vector constant timesteps. described general sequence-to-sequence framework used throughout work explain concrete case mapping human motion natural language. encoder network takes motion sequence input encodes context vector decoder network then step step produces desired description natural language context vector described previous section. model architecture architecture encoder network straightforward. stacked bidirectional rnns compute context vector given motion sequence. concretely context vector output last layer processed timesteps input sequence. layers outputs forward backward processing concatenated passed next layer input. keep around likely sequences model. process repeated iteratively candidate completed indicated token resulting diﬀerent descriptions. importantly also obtain probability sequence accumulating product corresponding step-wise probabilities. turn allows rank candidates according probability model. although approach language-to-motion mapping similar previously describe motionto-language mapping architecture decoder network necessarily diﬀerent since output modality multi-dimensional continuous. model architecture architecture encoder network similar already described section network stacked bidirectional rnns encode input description context vector. decoder uses stacked rnns layer access context vector computed encoder network previously generated motion timestep output preceding layers. complete model depicted figure however signiﬁcant diﬀerence ﬁnal fully-connected layer since problem requires output multi-dimensional continuous frame motion timestep section furthermore approach uses probabilistic decoder network allows generate non-deterministic outputs also provides likelihood scores model generated sequence. joint values last dimension binary indicates active parts motion. approach similar graves based mixture gaussians continuous part bernoulli distribution discrete part. denotes unnormalized activation i-th output neuron corresponds i-th item vocabulary. interpreted probability i-th item vocabulary conditioned input motion encoded context vector previously emitted words encoded hidden state recurrent decoder network decoder network also uses stacked rnns connects access context vector embedding previously emitted word output preceding finally fully-connected softmax layer produces discrete probability distribution vocabulary described access output layer. details number layers units layer hyperparameters detailed section detailed schematic model architecture also found appendix denotes ground truth timestep form reference natural language description denotes corresponding prediction model. bptt mini-batches update network parameters. exact hyperparameters training procedure described section decoding prediction problem deciding concrete word given predicted probabilities mentioned before beam search middle ground greedily selecting word highest probability performing exhaustive search space possible word sequences. concretely expand candidate sequences predicting diﬀerent probability vectors timestep. given vocabulary size yields candidates maximize likelihood ground truth mixture gaussians minimize cross-entropy. however practice formulation equation numerical stability issues. computing likelihood multi-variate gaussian requires computing product individual likelihoods easily subject numerical underoverﬂow. decoding similar motion-to-language mapping face problem decoding concrete motion frame prediction vector )i=...k ˆp]. solved sampling joint values multivariate gaussian mixture distribution sampling selected multivariate gaussian) well sampling bernoulli distribution binary active indicator. like beam search obtain couple candidates decoding process previously described section diﬀerence cannot compute likelihood discrete possibility anymore. resolve problem sampling couple candidates hypothesis respective distributions truncating keep around ﬁxed number hypotheses next timestep. given formulation deﬁne likelihood given motion frame conditioned input description encoded context vector well previously emitted frames encoded hidden state recurrent decoder network loss consists parts. ﬁrst part describes likelihood ground truth joint values predicted mixture distribution. second part binary crossentropy ground truth active encoder type encoder layers decoder type decoder layers embedding dimension dropout rate optimizer learning rate gradient clipping batch size training epochs vocabulary size motion joints mixture components descriptions motion natural language. recently proposed motion-language dataset uses human whole-body motions whole-body human motion database‡ graphics motion capture database§. motion descriptions form single english sentence collected using crowd-sourcing approach. motion-language dataset experiments. version dataset contains recordings human whole-body motion aforementioned representation annotations natural language. dataset publicly available¶ results paper reproduced. evaluation ﬁlter motions duration seconds order reduce computational overhead excessive padding. results usable motion samples total duration hours natural language annotations consist words total. randomly split remaining data training validation test sets ratio respectively. results reported using test otherwise indicated. processing steps described section performed obtain representations suitable training model. encoder decoder parts model gated recurrent units recurrent neural network architecture. regularize models dropout embedding recurrent fully-connected layers. cases train models using adam optimizer nesterov momentum training language-to-motion model proved diﬃcult presumably complex dynamics recurrent model made necessary gradient clipping well layer normalization also experimented batch normalization instead layer normalization unable good results table summaries aforementioned lists hyperparameters models language-tomotion motion-to-language. hyperparameters used experiments throughout work. code used obtain following results available online https//gitlab.com/ht/ deepmotionlanguagemapping/ training. optimizing model turned straightforward require special measures. figure depicts loss training training validation split respectively. seen training validation loss continuously decrease indicating model overﬁt training data. additionally validation loss seems converged training epochs indicating training time suﬃciently long. qualitative results provide insight style quality natural language descriptions generated model present examples figure depicted human whole-body motion compute natural language description sort descending order respective probability model seen examples descriptions complete english sentences valid grammatical structure. pattern observe throughout. interestingly model also produces semantically identical descriptions varying grammatical structure. example describing standing model creates description using present simple another using present continuous model also uses synonyms interchangeably. example model refers subject scene human person someone. especially apparent figure produced sentences identical except variation. generated natural language descriptions also rich detail. example model successfully diﬀerentiates wiping surface right left hand creates corresponding descriptions mention handedness motion. similar behavior observed waving motion stomping motion pushing motion descriptions correctly mention correct hand foot perturbation direction respectively. overall contents generated descriptions highly encouraging demonstrate capabilities model generate syntactically valid descriptions also semantically meaningful detailed ones. quantitative results previously presented results allow gather understanding generated descriptions necessarily represent overall behavior model. therefore provide following quantitative evaluation performance proposed model training test splits dataset. select bleu score measure performance proposed model. brieﬂy speaking bleu score metric initially proposed measure performance machine translation systems also used work targeting similar problem learning mapping human motion natural language bleu score obtained ﬁrst counting unigrams bigrams provided reference computing n-gram precision hypothesis. essentially modiﬁed weighted n-gram precision additional smoothing. bleu score deﬁned figure exemplary natural language descriptions generated proposed model nine diﬀerent human whole-body motions. description hypotheses sorted descending order probability model. problem. important distinction since sentence-level computation estimates n-gram model reference sentences leading estimates therefore unreliable scores. concretely generate descriptions qualitative results similar before provide insight type motion proposed model generates visualizing several examples figure language description compute human whole-body motion hypotheses space constraints depict motion highest loglikelihood model. interestingly results also demonstrate model capable generating correct motion primitive also adjust generated motion desired parametrization e.g. model generates motions waving right left hands indeed included training suggesting discover underlying periodic structure motion. however unable parametrize number repetitions using language. hypothesize training data contain enough training examples learn counting. hypotheses human motion sort descending order probability model. compute diﬀerent bleu scores correspond selecting best hypothesis. cases available annotations created human annotators ground truth. bleu scores computed separately training test split. table lists achieved bleu scores. couple observations noteworthy. first bleu scores lower would expect machine translation system example. much ambiguity generating descriptions human motion translating text diﬀerent language. former case diﬀerent levels details diﬀerent styles also semantically correct whereas latter case much constrained. second bleu scores clearly correlated order hypotheses even though loss used train model bleu score completely separate. means probability suitable measure quality turn model indeed captures understanding objectively high-quality description third model achieves slightly worse still comparable performance test split. this again demonstrates model overﬁt training data generalizes previously unseen motions. additionally second point bleu scores ranking deﬁned respective probabilities hypotheses correlated still holds; however pattern noisy. predict pose human subject space observed bowing squatting kicking motions root pose human model remains ﬁxed. furthermore since consider kinematic aspects motion generated motions necessarily dynamically stable. similarly since consider contact information objects point generated manipulation motions violating constraints would necessary achieve desired outcome. observed wiping motion wiping diverges table surface time. including dynamic properties contact information important area future work. quantitative quantitative results performance language-to-motion model complicated since deﬁning appropriate metric non-trivial. reason motion performed large variety diﬀerent styles semantically correct given description diﬀerent joint-level characteristics. computing metric like mean squared error reference hypotheses therefore ill-suited judge correct motion hypothesis semantically. resolve problem exploit fact already compute semantic description given motion using previously evaluated motionto-language model. since already evaluated concretely essentially chain models. first language-to-motion model want evaluate here compute motion given description natural language. next previously trained evaluated motion-to-language model transform generated motion back description natural language. finally compute bleu score described section quantitatively measure performance language-to-motion model. results approach given table make easier compare performance propose measure language-to-motion model relative performance motion-to-language model. formally relate bleu score language-to-motion model highest bleu score motionto-language model dividing two. thus measure percentage performance retained transforming natural language motion hypothesis transforming generated motion back description. table lists relative performance language-to-motion model. performance training test split. expected bleu scores training split slightly higher test split still comparable. suggests minimal overﬁt generalization capabilities previously unseen examples. second ranking hypotheses deﬁned loglikelihood model seems less correlated performance model proposed evaluation metric. currently diﬃcult identify prime origin cause since could also caused error motion-to-language model. ﬁnal experiment concerned providing insight proposed model uses analyze context vectors produced motion-to-language model languageto-motion model. since extremely highneighbor embedding dimensional representation. t-sne particular wellsuited task since known maintain structure original high-dimensional vector space low-dimensional projection. color low-dimensional projection context vector space according type motion performed. large variety diﬀerent motion select color motions type walking running waving wiping mixing squatting combined color motions. labels obtained whole-body human motion database provides multiple labels motion record. figure depicts visualization models motion-to-language language-tomotion. visualization exhibit clear structure type. contain clusters motions motion-tointerestingly language model denser clusters less variance cleaner separation clusters visualization language-tomotion model makes intuitive sense since describing motion natural language ambiguities compared observing motion directly. comparing running walking motions observation especially apparent motion-tolanguage case types motion nicely separated wheres language-to-motion case another interesting observation motions type wiping mixing used interchangeably directions. since include object information mixing wiping appear same. also investigate structure within given type motion case walking. purpose projection previous motionto-language visualization. however time color motions respective direction walking motions. resulting -dimensional t-sne projection depicted figure shows walking motions direction grouped cluster. however separation often less clear motions completely diﬀerent type. overall analysis context vector space clearly suggests proposed models extract semantic meaning given input encode context vector. decoder part models generate correct sequence using semantic representation. paper proposed deep recurrent neural networks sequence-to-sequence setting learn bidirectional mapping human whole-body motion descriptions natural language. presented models used model direction bidirectional mapping individually. important property proposed models probabilistic allowing produce diﬀerent candidate hypotheses ranking accordingly. additionally system makes minimal assumptions natural language descriptions human whole-body motions requiring minimal preprocessing explicit motion segmentation motion action primitives clusters thereof a-priori. furthermore model experiments clearly demonstrated capabilities proposed system generate rich detailed descriptions large variety diﬀerent human whole-body motions. conversely showed model capable generating similarly large variety realistic human whole-body motion given description thereof natural language. quantiﬁed reported performance system using bleu score well-known frequently used context machine translation. also presented results indicate model successfully learns distributed semantically meaningful latent representations given input produce desired output. limitation proposed system input sequence needs encoded single vector becomes especially problematic sequence length increases. overcome problem attention mechanisms proposed literature. integrating mechanisms future likely going improve performance system. similarly hierarchical rnns proposed used successfully model human motion. integrating ideas system would likewise interesting experiment. increasing size motion-language important dataset area future work well. data would allow complex models reduces risk overﬁtting. additionally including complex motions subject performs sequence distinct steps would allow interesting experiments test generalization capabilities proposed system permutating order steps. representing human whole-body motion using joint values kinematic model insuﬃcient. future work therefore intend incorporate dynamic properties motion well contact information environment objects involved execution motion. multi-contact motions recently studied mandery", "year": 2017}