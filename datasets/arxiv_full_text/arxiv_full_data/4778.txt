{"title": "Fast Online Clustering with Randomized Skeleton Sets", "tag": ["cs.AI", "cs.LG"], "abstract": "We present a new fast online clustering algorithm that reliably recovers arbitrary-shaped data clusters in high throughout data streams. Unlike the existing state-of-the-art online clustering methods based on k-means or k-medoid, it does not make any restrictive generative assumptions. In addition, in contrast to existing nonparametric clustering techniques such as DBScan or DenStream, it gives provable theoretical guarantees. To achieve fast clustering, we propose to represent each cluster by a skeleton set which is updated continuously as new data is seen. A skeleton set consists of weighted samples from the data where weights encode local densities. The size of each skeleton set is adapted according to the cluster geometry. The proposed technique automatically detects the number of clusters and is robust to outliers. The algorithm works for the infinite data stream where more than one pass over the data is not feasible. We provide theoretical guarantees on the quality of the clustering and also demonstrate its advantage over the existing state-of-the-art on several datasets.", "text": "present fast online clustering algorithm reliably recovers arbitrary-shaped data clusters high throughout data streams. unlike existing state-of-the-art online clustering methods based k-means k-medoid make restrictive generative assumptions. addition contrast existing nonparametric clustering techniques dbscan denstream gives provable theoretical guarantees. achieve fast clustering propose represent cluster skeleton updated continuously data seen. skeleton consists weighted samples data weights encode local densities. size skeleton adapted according cluster geometry. proposed technique automatically detects number clusters robust outliers. algorithm works inﬁnite data stream pass data feasible. provide theoretical guarantees quality clustering also demonstrate advantage existing state-of-the-art several datasets. online clustering massive data streams becoming important data variety ﬁelds including social media ﬁnance applications arrives high throughput stream. social networks detecting tracking clusters communities important analyze evolutionary patterns. similarly online clustering lead spam fraud detection applications detecting unusual mass activities email services online reviews. exist several challenges developing good clustering algorithm high throughput online scenario. realworld applications number shape clusters typically unknown. existing state-of-the-art online clustering methods provable theoretical guarantees primarily based k-means k-median/medoid assume apriori knowledge number clusters inherently make strong generative assumptions clusters. assumptions force retrieved clusters convex leading poor clustering many real-world tasks. exist several nonparametric techniques make simplistic generative assumptions mostly based heuristics lack theoretical guarantees. moreover true online scenario needs deal continuos streams precluding multiple passes data applicable ﬁnite-size streams commonly assumed many techniques. potential drift data distribution time another practical difﬁculty needs handled effectively. finally clustering procedure efﬁcient space time able handle massive data streams. paper propose novel skeleton-based online clustering algorithm address challenges. basic idea represent cluster compact skeleton faithfully captures geometry cluster. skeleton maintains small random sample corresponding cluster online fashion updated fast using data points. skeleton point weighted according local density around number skeleton points automatically adapted structure cluster complicated shapes approximated skeleton points. skeleton sets updated random procedure provides robustness presence outliers. proposed algorithm automatically recovers correct number clusters data high probability data seen. update strategy skeleton sets also allows clustering method automatically adapt drift data distribution. clusters merged well split time. also provide theoretical guarantees quality clusters obtained proposed method. comparison huge literature ofﬂine clustering work online clustering somewhat limited. existing online clustering algorithms theoretical guarantees fall model-based techniques k-mean k-median k-medoid assume speciﬁc shape clusters spheres trivially leads compact representation using parameters e.g. center radius number points. however discussed before model based algorithms fail capture arbitrary clusters data perform poorly. exist several nonparametric clustering methods assumption made cluster shapes. popular among dbscan clustream denstream recent surveys described several variants algorithms denstream clustream methods create microclusters based local densities combine form bigger clusters time. however methods need periodically perform ofﬂine partitioning microclusters form clusters suitable online clustering massive data streams. leaderfollower algorithm another popular method exist several variants techniques typically encode every cluster center updated continuously points belonging cluster detected. cluster representation rich enough encode complex clusters. overall main drawback nonparametric online clustering algorithms mostly based heuristics lack theoretical guarantees. also require extensive hand tuning parameters. authors assume cluster clique order provide theoretical guarantees restrictive real-world. another popular method used context incremental clustering doubling algorithm standard version encodes every cluster point. furthermore even though allows merging clusters permit split them. implement variant method instead center several centers kept cluster. show experimental section purely deterministic approach even though proposed algorithm shares similarities existing techniques cure algorithm core-set cure similar cluster represented random sample data instead center handle arbitrary cluster shapes. cure however ofﬂine hierarchical agglomerative clustering approach running time quadratic size data slow online applications. core-set based clustering encode complicated cluster shape compact sample points. existing state-of-the-art algorithms idea core-set computationally intensive useful online clustering practice. running time exponential number stored skeleton points. furthermore variants give provable theoretical guarantees inherently ofﬂine methods often require several passes data produce good-quality clustering. instance algorithm presented needs rerun times size core-set dataset size. nonparameteric graph-based techniques spectral clustering recover arbitrary shaped clusters appropriate mainly ofﬂine setting moreover also assume priori knowledge number clusters. several relaxations iterative biclustering proposed overcome need know number clusters apriori methods cannot extended online setting. recently work incremental spectral clustering essentially iteratively modiﬁes graph laplacian true online setting however even building good initial graph laplacian infeasible either lack enough data computational bottlenecks. work since interested retrieving arbitraryshaped clusters important ﬁrst deﬁne constitutes good cluster. traditional model-based techniques assume global distance measure restricts convex-shaped clusters. instead proposed algorithm works nonparametric setting clusters deﬁned intuitive notion paths ’short-edges’. points likely belong cluster exist many paths data neighborhood graph consecutive points path other. clearly overall distance points large still cluster. setting enables consider many complicated cluster shapes. emphasize idea neighborhood graph understand cluster deﬁnition implicitly utilize work. need explicitly construct graph approach. idea behind algorithm represent cluster pseudorandom samples called skeleton set. algorithm stores constantly updates skeleton sets st}. note size corresponds number clusters change time data seen. skeleton represents cluster consists sample points belonging cluster time together carefully chosen random numbers thus skeleton weights elements form weights points belonging skeleton updated given time skeleton points pseudouniformly distributed entire cluster. mentioned before number skeleton points cluster ﬁxed vary time. cluster arises initialized hinit skeleton points. algorithm take hinit theoretical section show lower bounds hinit translated strict provable guarantees regarding quality produced clustering. algorithm tries maintain skeleton skeleton points exists path relatively short edges consisting entirely skeleton points skeleton set. cluster grows average distance skeleton points large number skeleton points increased. number never exceeds given upper bound determining memory user inclined delegate encode cluster. overall algorithm works ﬁrst initializing number samples stored skeleton set. work without loss generality assume skeleton initially sample. propose variants algorithm lazy cluster-merging performed other merging aggressive since splitting clusters also allowed experimental section latter produces good approximation groundtruth clustering fewer steps cost extra time needed check perform splitting. mergesplitsoc version turned algorithm keeps undirected graphs element associated different skeleton encodes topology connections points set. denote element associated skeleton overview algorithm given algorithm splitting turned time step given existing skeleton cluster algorithm checks cluster split. then data point arrives stream ﬁrst ball radius centered i.e. created. then intersection ball existing skeleton computed. weight skeleton points intersection assumed belong cluster corresponding skeleton updated. note possible multiple clusters claim point case clusters considered merging. intersection ball skeleton sets empty singleton cluster created. goal merge subprocedure merge point clusters assigned multiple clusters basically acts linking point merge them resulting uniﬁed cluster. step crucial online clustering scenario points cluster assigned different clusters initially evidence builds data combine clusters recover true underlying cluster. merge subprocedure skeleton updated cluster constructed. skeleton sets representing clusters merged given input. describing merge procedure introduce important subroutine. skeleton size denote extension obtained adding exactly h−hs triples according following procedure. newly added triple weight newly added skeleton point chosen independently random skeleton point chosen prob. corresponding random number ability generated pseudorandom number generator seed seed initialized randomly alternatively chosen function skeleton point conceptually partitioning entire input space grid lengh using cell occupied grid. latter procedure useful inﬁnite streams avoid correlated random sequences away points space. subroutine also ﬁrst argument single point case output skeleton form sequence random numbers generated according gen. merge algorithm computes average weighted distance point skeleton points reside next cases considered. number skeleton points skeleton sets merged algorithm decides increase size merged skeleton denote minimum total number skeleton points skeleton sets merged. merged skeleton size hun. describe skeleton point newly formed cluster computed. first contributing skeleton extended size weighted random sampling also case take skeleton points clusters merged choose point corresponding value shown algorithm using random sequence generated newly added point contribute weight distribution cluster. point skeleton closest point vmin skeleton found weight increased skeleton replaces skeleton sets clusters merged. total number assume skeleton points skeleton sets merged smaller intuitivaly speaking means cluster grown much thus number skeleton points encoding cluster increase snew {snew}; {gsin}; used entirely). case procedure previous case conducted skeleton corresponding excluded. finally added last skeleton-singleton weight newly formed cluster. procedure addsingleton adds cluster consisting existing cluster found close enough based intersection skeleton sets next skeleton singleton cluster created. since skeleton aims cover uniformly entire mass cluster using hinit random samples point repeated hinit times form skeleton cluster-singleton. furthermore sequence hinit random values generated copy skeleton assigned values weight build hinit triples complete skeleton. proposed implementation initialize newly created clustersingleton skeleton point thus hinit newly created skeleton undirected graph-singleton gsin created. checksplit determines whether given skeleton split looking breaking point skeleton point whose weight half average weight points within skeleton set. point found algorithm determines whether cluster split follows. first points deleted corresponding graph gdel rem; algorithm gdel obtain ...ct}; input family graphs skeleton point radius subfamily gsk} skeleton snew; output updated version gsk; snew b∃i=jx gsj}; gmerged graph obtained adding edges update gsk}; updatedgraph procedure shown algorithm responsible constructing graph gunion newly formed cluster replacing graphs corresponding merged skeleton sets. graph gunion constructed combining elements corresponding skeleton sets need merged. graphs combined adding edges skeleton points newly constructed skeleton close neighborhood linking point description updatedgraph denote g... graph vertex edge section provide theoretical results regarding algorithm clustering model described sec. start introducing general mathematical model analyze. many variants planted partition models used construct data hard clustering outliers. notice algorithm require input produced according model. particular speciﬁc parameters model algorithm. disjoint compact sets called cores denoted ...rk cores called ∆-separable minimum distance cores greater i.e. ∀≤i<j≤kx∈riy∈rjx cores arbitrary shapes giving rise observed clusters points cluster come core high probability rest space probability. formally given probabilities ...pk} points cluster sampled core probability outside probability important note even though cores separable longer case clusters presence outliers. words short-edge paths points different clusters exist many expectation. call clustering model presented -model quite general model good-quality clustering cores however outliers task recovering clusters nontrivial even ofﬂine setting. simple heuristics connectedcomponents cannot used recover clusters ofﬂine mode. online setting brings additional algorithmic computational challenges. below give details proposed clustering mechanism. probability point data stream belongs cluster covering arbitrary ball covering furthermore lower bound probability point given belongs core expressed denote πipipb probability point came ﬁxed ball covering given belongs cluster similarly probabiltiy point outlier given belongs cluster since outliers expected lower points cluster cores denote mini maxi since keep samples skeleton cluster points set. error made algorithm point deﬁned follows suppose comes core gets assigned cluster contains points cores well exists another cluster also contains points note strict deﬁnition error online setting transient overclustering expected lack enough data early phase. algorithm reaches saturation phase skeleton reaches size ready state main results analysis regarding mergeonlysoc version algorithm. theorem assume given dataset constructed according -model cores outliers. cores ∆-separable. assume core cluster -coverable. number points seen algorithm saturation phase reached. probability least hinit points corresponding cluster log. formulate following lemma lemma assume time algorithm merges clusters contains point contains point either least skeleton points time outliers least skeleton points time outliers. proof. lemma follows deﬁnition according which points taken different cores least distance apart. clusters merged time exists data point ball contains least skeleton points least skeleton points cannot contain points different cores since ∆-separable. thus least clusters from least skeleton points outliers. evaluated performance proposed algorithm using four synthetic datasets shown fig. sets contain data points dimensions. ﬁrst dimensions randomly drawn predeﬁned clusters shown ﬁgure dimensions random noise. data sets data points randomly drawn banana shaped clusters ﬁrst dimensions outliers randomly generated vicinity shapes respectively. data sets data points randomly drawn four letter shaped clusters. outliers randomly generated vicinity shapes respectively. values dimensions data gaussian white noise standard deviation data points randomly permuted orders data stream would affect results. examples datasets plotted ﬁrst dimensions shown fig. used mergesplitsoc version algorithm since provided faster convergence number clusters under quality guarantees. compared method several state-of-theart nonparametric clustering methods i.e. dbscan leader-follower algorithm doubling algorithm denstream clustering quality quantitatively evaluated using average clustering purity deﬁned fig. shows comparative results method well several methods. fig. shows clustering purity methods. method requires parameters experiments selected results produced using best choice parameters method. note parameter tuning trivial methods require least parameters. results showed method able cluster data well even though slightly over-clustered datasets leader-follower algorithm well streaming dbscan simply handle type data. obtains similar results denstream algorithm dbscan worked well banana sets failed cluster outliers overnumbered true clusters. doubling failed work noisy data sets datasets clustering purity probably noise dimensions. leader-follower method worked poorly mostly nature method partly noise dimensions. standard variant leader-follower uses small number centers cluster clusters contained convex well-separable objects recognition poor. denstream worked well cases. method faster denstream reason denstream purely online approach performs ofﬂine clustering periodically. part expensive. denstream another serious drawback. cluster newly coming point assigned computed based recent snapshot ofﬂine clustering produced on-ﬂy. thus accuracy method depends heavily special parameter determining much data distribution evolves time. since parameter tuning always nontrivial figure clustering results different methods. left column shows four different synthetic datasets. resulting clusters method marked using different colors. note denstream hybrid online-ofﬂine technique purely online method. method slightly overclustered because online nature algorithm presence outliers. nontheless able correctly throw outliers produced results high purity. because even though outliers become part skeleton cluster typically replaced true cluster points eventually true cluster points higher density arrive higher rate outliers. fig. shows skeleton points generated data sets. maximum number skeleton points used hundred points. number skeleton points grows gradually clusters become bigger. sented efﬁciently skeleton continuously updated dynamically adapt changing data distribution. proposed technique theoretically sound well fast space-efﬁcient practice. produced good-quality clusters various experiments nonconvex clusters. outperforms several online approaches many datasets produces results similar effective hybrid ones combine online ofﬂine steps future would like investigate methods updating skeletons within given framework online fashion since mechanism crucial effectiveness presented approach. interesting area research maximal number clusters created execution algorithm. precise bound provide accurate theoretical estimate memory usage. alon noga seannie parnas michal dana. annual symposium testing clustering. foundations computer science focs november redondo beach california b¯adoiu mihai har-peled sariel indyk piotr. approceedings proximate clustering core-sets. thiry-fourth annual symposium theory computing stoc feng ester martin qian weining zhou aoying. density-based clustering evolving data proceedings sixth siam stream noise. international conference data mining april bethesda charikar moses chekuri chandra feder tom´as incremental clustering dynamic motwani rajeev. proceedings twentyinformation retrieval. ninth annual symposium theory computing stoc andrade silva jonathan faria elaine barros rodrigo hruschka eduardo carvalho andr´e carlos ponce leon ferreira gama jo˜ao. data stream clustering survey. comput. surv. shindler wong meyerson fast accurate k-means large datasets. advances neural information processing systems annual conference neural information processing systems conference proof. lemma conclude order upper bound event algorithm point merge wrong clusters sufﬁces upper bound probability algorithm point produce cluster least skeleton points outliers least nonoutlier denote latter event denote intersection event comes core note points point dominates following true least random numbers assigned algorithm smaller corresponding random numbers assigned data points d\\{v}. denote data points outliers core constant denote time stamps ﬁrst points collected. ﬁrst lower bound probability following event )-covering every ball points arrived time azuma’s inequality know number tightly concentrated around average i.e. every probability ﬁxed ﬁxed ball number less fact using general version azuma’s inequality ﬁxed upper bound simultanously. consider following event every ball time every least data points ball arrived. thus using union bound balls covering conclude happens probability least se−w analyze conditioned ﬁxed average number dominating points ﬁxed ball covering least besides previously easily note actual number tightly concentrated around average. taking using azuma’s inequality again derive upper bound form e−hδ probability ﬁxed ball covering ﬁxed time contains fewer dominating points assumed above. using taking union bound balls covering se−w )sne−hδ stands complement event denote event among ﬁrst points outliers where ﬁxed constant. chernoff’s inequality assume happens points already arrived. denote following event balls covering time skeleton points ball outliers. note holds points seen least points outliers must seen time ball covering take next points coming ﬁxed probability points outliers chernoff’s bound tγi. denote following event ...} outliers points. union bound have tγi. assume holds probability points seen outliers become skeleton point ﬁxed ball based ﬁxed random number assigned algorithm probability case least outliers γi)αh−w thus take union bound conclude nγi)αh−w notice thus obtain conclude se−w γi)αh−w thus taking union bound cores obtain kse−w knsγi)αh−w bounding term expression last inequality obtain lower bound statement theorem. since already noticed sufﬁces appropriate upper bound concludes proof. deﬁnition know holds number clusters computed algorithm containing least point increase time ﬁrst data points corresponding cluster seen. assume holds. notice time every ball covering gets least data point cluster computed algorithm points core case errors regarding points core made. azuma’s inequality union bound immediately number extra data points coming need seen populate every ball covering least probability se−t conclude probability se−w points already seen algorithm still make mistakes data points upper-bound every ingredient taking number points previous theorem concludes proof.", "year": 2015}