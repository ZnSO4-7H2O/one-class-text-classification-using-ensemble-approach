{"title": "Gated Graph Sequence Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.", "text": "graph-structured data appears frequently domains including chemistry natural language semantics social networks knowledge bases. work study feature learning techniques graph-structured inputs. starting point previous work graph neural networks modify gated recurrent units modern optimization techniques extend output sequences. result ﬂexible broadly useful class neural network models favorable inductive biases relative purely sequence-based models problem graph-structured. demonstrate capabilities simple graph algorithm learning tasks. show achieves state-of-the-art performance problem program veriﬁcation subgraphs need described abstract data structures. many practical applications build graph-structured data thus often want perform machine learning tasks take graphs inputs. standard approaches problem include engineering custom features input graph graph kernels methods deﬁne graph features terms random walks graphs closely related goal work methods learn features graphs including graph neural networks spectral networks recent work learning graph ﬁngerprints classiﬁcation tasks graph representations chemical molecules main contribution extension graph neural networks outputs sequences. previous work feature learning graph-structured inputs focused models produce single outputs graph-level classiﬁcations many problems graph inputs require outputting sequences. examples include paths graph enumerations graph nodes desirable properties sequences global classiﬁcations mixed with example start node. aware existing graph feature learning work suitable problem. motivating application comes program veriﬁcation requires outputting logical formulas formulate sequential output problem. secondary contribution highlighting graph neural networks broadly useful class neural network model applicable many problems currently facing ﬁeld. settings feature learning graphs learning representation input graph learning representations internal state process producing sequence outputs. here mostly achieved previous work graph neural networks make several minor adaptations framework including changing modern practices around recurrent neural networks. important desire outputs graphstructured problems solely individual classiﬁcations. cases challenge learn features graph encode partial output sequence already produced still needs produced show framework adapted settings leading novel graph-based neural network model call gated graph sequence neural networks illustrate aspects general model experiments babi tasks graph algorithm learning tasks illustrate capabilities model. present application veriﬁcation computer programs. attempting prove properties memory safety core problem mathematical descriptions data structures used program. following brockschmidt phrased machine learning problem learn input graphs representing state memory logical description data structures instantiated. whereas brockschmidt relied large amount hand-engineering features show system replaced ggs-nn cost accuracy. section review graph neural networks introduce notation concepts used throughout. gnns general neural network architecture deﬁned according graph structure nodes take unique values edges pairs focus work directed graphs represents directed edge note framework easily adapted undirected graphs; scarselli node vector node denoted graphs also contain node labels node edge labels edge types edge. overload notation nodes edges. function returns predecessor nodes analogously successor nodes edges nodes neighboring edges incoming outgoing gnns graphs outputs steps. first propagation step computes node representations node; second output model maps node representations corresponding labels output notation leave dependence parameters implicit continue throughout. system differentiable end-to-end parameters learned jointly using gradient-based optimization. here iterative procedure propagates node representations. initial node representations arbitrary values node representation updated following recurrence convergence denotes timestep several variants discussed scarselli including positional graph forms node-speciﬁc updates alternative representations neighborhoods. concretely scarselli suggest decomposing per-edge terms independent node implemented mapping ﬁnal node representations node handle graph-level classiﬁcations suggest output create dummy super node connected nodes special type edge. thus graph-level regression classiﬁcation handled manner node-level regression classiﬁcation. learning done almeida-pineda algorithm works running propagation convergence computing gradients based upon converged solution. advantage needing store intermediate states order compute gradients. disadvantage parameters must constrained propagation step contraction map. needed ensure convergence limit expressivity model. neural network encouraged using penalty term -norm network’s jacobian. appendix example gives intuition contraction maps trouble propagating information across long range graph. describe gated graph neural networks adaptation gnns suitable non-sequential outputs. describe sequential outputs next section. biggest modiﬁcation gnns gated recurrent units unroll recurrence ﬁxed number steps backpropagation time order compute gradients. requires memory almeida-pineda algorithm removes need constrain parameters ensure convergence. also extend underlying representations output model. gnns point initializing node representations contraction constraint ensures ﬁxed point independent initializations. longer case gg-nns lets incorporate node labels additional inputs. distinguish node labels used inputs ones introduced before call node annotations vector denote annotations. illustrate node annotations used consider example task training graph neural network predict whether node reached node given graph. task problem-related special nodes mark nodes special give initial annotation. ﬁrst node gets annotation second node gets annotation nodes initial annotation intuitively marks ﬁrst input argument second input argument. initialize node state vectors using label vectors copying ﬁrst dimensions padding extra allow hidden states larger annotation size. reachability example easy propagation model learn propagate node annotation nodes reachable example setting propagation matrix associated forward edges position cause ﬁrst dimension node representation copied along forward edges. setting parameters propagation step cause nodes reachable ﬁrst node representation output step classiﬁer easily tell whether node reachable looking whether node nonzero entries ﬁrst dimensions representation vector. figure example graph. color denotes edge types. unrolled timestep. parameter tying sparsity recurrent matrix. letters denote edge types corresponding reverse edge type denote distinct parameters. matrix rd|v|×d|v| determines nodes graph communicate other. sparsity structure parameter tying illustrated fig. sparsity structure corresponds edges graph parameters submatrix determined edge type direction. rd|v|×d columns blocks corresponding node initialization step copies node annotations ﬁrst components hidden state pads rest zeros. step passes information different nodes graph incoming outgoing edges parameters dependent edge contains activations edges directions. remaining type direction. gru-like updates incorporate information nodes previous timestep update node’s hidden state. update reset gates logistic sigmoid function element-wise multiplication. initially experimented vanilla recurrent neural network-style update preliminary experiments found gru-like propagation step effective. several types one-step outputs would like produce different situations. first node output node gg-nns support node selection tasks making scores applying softmax node scores. second graph-level outputs deﬁne graph level representation vector describe gated graph sequence neural networks several gg-nns operate sequence produce output sequence output step denote matrix node annotations predicting r|v|×lv gg-nns predicting seen states carried step contain propagation model output model. propagation models denote matrix node vectors propagation step output step r|v|×d. before step -extending node. overview model shown fig. alternatively share single propagation model separate output models. simpler variant faster train evaluate many cases achieve similar performance level full model. cases desired propagation behavior introduce node annotation output model predicting prediction done node independently using neural network takes concatenation settings training ggs-nns specifying intermediate annotations training full model end-to-end given graphs target sequences. former improve performance domain knowledge speciﬁc intermediate information represented internal state nodes latter general. describe both. sequence outputs observed annotations consider task making sequence predictions graph prediction part graph. order ensure predict output part graph exactly once sufﬁces node indicating whether node explained far. settings small number annotations sufﬁcient capture state output procedure. case want directly input information model labels indicating target intermediate annotations. cases annotations sufﬁcient deﬁne model gg-nns rendered conditionally independent given annotations. case training time given annotations sequence prediction task decomposes single step prediction tasks trained separate gg-nns. test time predicted annotations step used input next step. analogous training directed graphical models data fully observed. sequence outputs latent annotations generally intermediate node annotations available training treat hidden units network train whole model jointly backpropagating whole sequence. section present example applications concretely illustrate ggs-nns. focus selection babi artiﬁcial intelligence tasks graph algorithm learning tasks. babi tasks meant test reasoning capabilities systems capable babi suite tasks test basic forms reasoning like deduction induction counting path-ﬁnding. deﬁned basic transformation procedure maps babi tasks gg-nns ggs-nns. --symbolic option released babi code stories involve sequences relations entities converted graph. entity mapped node relation mapped edge edge label given relation. full story consumed mapped single graph. questions marked eval data comprised question type argument arguments converted initial node annotations i-th i-th argument node’s annotation vector example eval line eval true gets initial annotation question type output class tasks multiple question types example task question types tasks simply train separate ggnn task. strong supervision labels give ggs-nns intermediate annotations experiments. simple transformation preserve information story easily handle ternary higher order relations also emphasize non-trivial task general natural language symbolic form could directly apply approach arbitrary natural language. relaxing restrictions left future work. however even simple transformation variety babi tasks formulated including task arguably hardest task. provide baselines show symbolic representation help rnns lstms signiﬁcantly show ggs-nns solve problem small number training instances. also develop babi-like tasks involve outputting sequences graphs shortest paths simple form eulerian circuits point experiments illustrate capabilities ggs-nns across variety problems. ﬁrst lines describe facts gg-nn facts build graph. capital letters nodes fear interpreted edge labels edge types. last lines questions asked input data. fear lines interpreted question type. task question node special e.g. eval fear assign single value annotation vector special node nodes. n<id> nodes e<id> edges q<id> question types extra tokens added give lstm access complete information available dataset. ﬁnal number class label. ﬁrst lines describe edges different edge types. last line path question answer sequence directions path going ﬁrst west south question lines treated output classes. training details. tasks section generate training examples test examples training examples used validation. evaluating model performance babi tasks contain questions example predictions different questions evaluated independently. randomness dataset generation process generated datasets task report mean standard deviation evaluation performance across datasets. explanatory tasks start training different models training examples gradually increase number training examples model’s test accuracy reaches above success babi standard weston method report minimum number training examples needs reach accuracy along accuracy reaches amount training examples. cases unrolled propagation process steps. babi task used gg-nn size node vectors respectively. ggs-nns section used simpler variant share single propagation model. shortest path eulerian circuit tasks used models trained long enough adam validation used choose best model evaluate avoid models overﬁtting. choose four babi tasks suited restrictions described require single step outputs task node selection gg-nn used. task used graphlevel classiﬁcation version. ggnn networks contain less parameters. baselines train lstm models symbolic data sequence form. rnns lstms dimensional embeddings dimensional hidden layers; predict single output sequences output treated classiﬁcation problem loss cross entropy. rnns lstms contain around parameters respectively. test results appear table tasks gg-nn achieves perfect test accuracy using training examples rnn/lstm baselines either training examples fail solve tasks table break performance baselines task amount training data varies. lstm able solve task almost perfectly ggnn reaches accuracy much less data. babi task treated question types trained gg-nn question type strictly speaking babi task gg-nn model times number parameters single gg-nn model. experiments used gg-nn parameters question type means parameters total. babi task arguably hardest task among babi tasks reports accuracy less methods strong supervision). apply ggs-nn problem symbolic form data extra ‘end’ class added output sequence; test time network keep making predictions predicts ‘end’ class. developed babi-like tasks based algorithmic problems graphs shortest paths eulerian circuits. ﬁrst generate random graphs produce story lists edges graphs. questions come choosing random nodes asking shortest path connects chosen nodes. constrain data generation produce questions unique shortest path length least eulerian circuits generate random two-regular connected graph separate random distractor graph. question gives nodes start circuit question return eulerian circuit given subgraph starts going results shown table lstm fail tasks ggs-nns learns make perfect predictions using training examples. work ggs-nns motivated practical application program veriﬁcation. crucial step automatic program veriﬁcation inference program invariants approximate program states reachable execution. finding invariants data structures open problem. example consider simple function right. prove program indeed concatenates lists pointer dereferences valid need characterize program’s heap iteration loop. this separation logic uses inductive predicates describe abstract data structures. example list segment deﬁned n.ls {val next {val next means points memory region contains structure next ﬁelds whose values turn connective conjunction boolean logic additionally requires operators refer separate parts heap. thus implies either null points values heap described again. formula ∃t.ls∗ invariant loop using prove program fail dereferencing unallocated memory address function indeed concatenates lists using hoare-style veriﬁcation scheme hardest part process coming formulas describe data structures propose machine learning. given program times extract state memory relevant program locations predict separation logic formula. static program analysis tools check whether candidate formula sufﬁcient prove desired properties representing heap state graph inputs consider directed possibly cyclic graphs representing heap program. graphs automatically constructed program’s memory state. graph node corresponds address memory sequence pointers stored graph edges reﬂect pointer values i.e. edges labeled point nodes respectively. subset nodes labeled corresponding program variables. example input graph displayed input fig. node displayed node. edge labels correspond speciﬁc ﬁelds program e.g. example corresponds next pointer example function previous section. binary trees types pointers left right pointing left right children tree node. output representation mathematically describe shape heap. model restrict syntactically restricted version separation logic formulas form xn.a atomic formula either tree none existential quantiﬁers used give names heap nodes needed describe shape labeled program variable. example describe panhandle list ﬁrst list element cycle needs named. separation logic expressed ∃t.ls data generate synthetic datasets problem. this predicates tree together inductive deﬁnitions. enumerate separation logic formulas instantiating predicates using given program variables. finally formula enumerate heap graphs satisfying formula. result dataset consisting pairs heap graphs associated formulas used learning procedures. easy obtain node annotations intermediate prediction steps data generation process. train variant ggs-nn observed annotations infer formulas heap graphs. note also possible unobserved ggsfigure illustration ﬁrst steps predict separation logic formula memory state. label is-named signiﬁed variable near node active double border is-explained white ﬁll. variant end-to-end learning. procedure breaks production separation logic formula sequence steps. ﬁrst decide whether declare existential variables choose node corresponds variable. declared existentials iterate variable names produce separation logic formula describing data structure rooted node corresponding current variable. full algorithm predicting separation logic formula appears below alg. three explicit node annotations namely is-named active is-explained initial node labels directly computed input graph isnamed nodes labeled program variables active is-explained always commented lines algorithm implemented using gg-nn i.e. alg. instance ggs-nn model. illustration beginning algorithm shown fig. step related line algorithm. algorithm separation logic formula prediction procedure input heap graph named program variables compute initial labels initialize node vectors -extending quantiﬁer needed fresh variable name pick node turn is-named print node label is-named model setup details full ggs-nn model separate propagation models. gg-nn components ggs-nn pipeline unrolled propagation process time steps. ggs-nns associated step uses dimensional node representations. ggs-nn components used. adam used optimization models trained minibatches graphs optimized training error low. graph-level classiﬁcation tasks also artiﬁcially balanced classes even number examples class minibatch. ggs-nn components contain less parameters overﬁtting observed training. practice heap graphs given input single output formula expected describe consistent input graphs. different heap graphs snapshots heap state different points program execution process different runs program different inputs. call batch prediction setup contrasting single graph prediction described main paper. node selection outputs common named variables link nodes different graphs togeter aggregating predictions batch. compute score particular named output score named variable graph applying softmax names using graph independently different graphs completely different nodes. however algorithm tries update annotation named variable nodes associated variable graphs updated. training labels intermediate steps available data generation process training process decomposed single output single graph training. paper produced dataset formulas involves three program variables graphs formula yielding around formula/heap graph combinations. evaluate split data training validation test sets using split formulas measure correctness whether formula predicted test time logically equivalent ground truth; equivalence approximated canonicalizing names order formulas comparing exact equality. compared ggs-nn-based model method developed earlier earlier approach treats prediction step standard classiﬁcation requires complex manual problem-speciﬁc feature engineering achieve accuracy contrast model trained feature engineering little domain knowledge achieved accuracy example heap graph corresponding separation logic formula found ggs-nn model shown fig. example also involves nested data structures batching extension developed previous section. also successfully used model program veriﬁcation framework supplying needed program invariants theorem prover prove correctness collection list-manipulating algorithms insertion sort. following table lists benchmark list manipulation programs separation logic formula invariants found ggs-nn model successfully used veriﬁcation framework prove correctness corresponding programs. figure heap graph example contains named variables isolated null node edges null shown clarity. numbers edges indicate different edge types. ggs-nn model successfully ﬁnds right formula ls)) tree) ls)). table example list manipulation programs separation logic formula invariants ggsnn model founds input graphs. parts produced deterministic procedure goes named program variables graphs checks inequality. closely related work gnns discussed length above. micheli proposed another closely related model differs gnns mainly output model. gnns applied several domains appear widespread iclr community. part publicize gnns useful interesting neural network variant. must near convergence good gradients) replaced truncated belief propagation updates model trained truncated iteration produce good results ﬁxed number iterations. similarly recursive neural networks extended tree lstms analogous using updates gg-nns instead standard recurrence improving long-term propagation information across graph structure. general idea expressed paper assembling problem-speciﬁc neural networks composition learned components long history dating back least work hinton assembling neural networks according family tree structure order predict relations people. similar ideas appear hammer jain bottou graph kernels used variety kernelbased learning tasks graph-structured inputs aware work learns kernels outputs sequences. perozzi convert graphs sequences following random walks graph learns node embeddings using sequence-based methods. sperduti starita graphs graph vectors classify using output neural network. several models make similar propagation node representations graph structure. bruna generalize convolutions graph structures. difference work gnns analogous difference convolutional recurrent networks. duvenaud also consider convolutional like operations graphs building learnable differentiable variant successful graph feature. lusci converts arbitrary undirected graph number different dags different orientations propagates node representations inwards towards root training ensemble models. above focus one-step problems. gnns extensions many desirable properties pointer networks using node selection output layers nodes input chosen outputs. main differences ﬁrst gnns graph structure explicit makes models less general provide stronger generalization ability; second pointer networks require node properties gnns represent nodes deﬁned position graph makes general along different dimension. ggs-nns related soft alignment attentional models kumar sukhbaatar respects ﬁrst graph representation uses context focus attention nodes important current decision; second node annotations program veriﬁcation example keep track nodes explained gives explicit mechanism making sure node input used sequence producing output. learned? instructive consider learned gg-nns. draw analogy babi task would solved logical formulation. example consider subset lines needed answer example right. encoding tasks simpliﬁes parsing story graph form provide background knowledge. gg-nn model seen learning this results stored neural network weights. discussion results paper show ggs-nns desirable inductive biases across range problems intrinsic graph structure them believe many cases ggs-nns useful. however limitations need overcome make apply even broadly. limitations mentioned previously babi task translation incorporate temporal order inputs ternary higher order relations. imagine several possibilities lifting restrictions concatenating series gg-nns gg-nns edge representing higher order relations factor graphs. signiﬁcant challenge handle less structured input representations. example babi tasks would desirable symbolic form inputs. possible approach incorporate less structured inputs latent vectors ggs-nns. however experimentation needed best addressing issues. current ggs-nns formulation speciﬁes question facts consumed. implies network must derive consequences seen facts store pertinent information node within node representation. likely ideal; would preferable develop methods take question initial input dynamically derive facts needed answer question. optimistic applications ggs-nns. particularly interested continuing develop end-to-end learnable systems learn semantic properties programs learn complicated graph algorithms applying ideas problems require reasoning knowledge bases databases. generally consider graph neural networks representing step towards model combine structured representations powerful algorithms deep learning taking advantage known structure learning inferring reason extend representations. brockschmidt marc chen yuxin cook byron kohli pushmeet tarlow daniel. learning decipher heap program veriﬁcation. workshop constructive machine learning international conference machine learning kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. massa vincenzo monfardini gabriele sarti lorenzo scarselli franco maggini marco gori marco. comparison recursive neural networks graph neural networks. international joint conference neural networks ieee duvenaud david maclaurin dougal aguilera-iparraguirre jorge g´omez-bombarelli rafael hirzel timothy aspuru-guzik al´an adams ryan convolutional networks graphs learning molecular ﬁngerprints. arxiv preprint arxiv. goller christoph kuchler andreas. learning task-dependent distributed representations backpropagation structure. ieee international conference neural networks volume ieee kashima hisashi tsuda koji inokuchi akihiro. marginalized kernels labeled graphs. proceedings international conference machine learning volume kumar ankit irsoy ozan jonathan bradbury james english robert pierce brian ondruska peter gulrajani ishaan socher richard. anything dynamic memory networks natural language processing. arxiv preprint arxiv. lusci alessandro pollastri gianluca baldi pierre. deep architectures deep learning chemoinformatics prediction aqueous solubility drug-like molecules. chem model perozzi bryan al-rfou rami skiena steven. deepwalk online learning social representations. proceedings sigkdd international conference knowledge discovery data mining shervashidze nino schweitzer pascal leeuwen erik mehlhorn kurt borgwardt karsten weisfeiler-lehman graph kernels. journal machine learning research socher richard cliff manning chris andrew parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning stoyanov veselin ropson alexander eisner jason. empirical risk minimization graphical model parameters given approximate inference decoding model structure. international conference artiﬁcial intelligence statistics sheng socher richard manning christopher improved semantic representations tree-structured long short-term memory networks. arxiv preprint arxiv. uwents werner monfardini gabriele blockeel hendrik gori marco scarselli franco. neural networks relational learning experimental comparison. machine learning contraction example consider linear -hidden unit cycle-structured nodes simplicity ignored edge labels node labels equivalently simple example timestep update hidden states derivative approach exponentially fast grows. intuitively means impact node another node away decay exponetially therefore making difﬁcult model long range dependencies. lstm performance sequence prediction tasks i.e. babi task shortest path eulerian circuit poor compared single output tasks. eulerian circuit task lstm fail dramatically. typical training example task looks like following connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to connected-to eval eulerian-circuit describes graph cycles ---- ---- target cycle smaller distractor graph. edges presented twice directions symmetry. task cycle starts given nodes direction ﬁrst second. distractor graph added increase difﬁculty task also makes output cycle strictly eulerian. note node different ones original symbolic data. lstm read whole sequence start predict ﬁrst output reading token. prediction step token input target node expected output. current setup output prediction step input next. ggs-nn model uses setup output step used input next predicted node annotations carry step next comparison still fair lstm. changing method baselines make previous predictions left future work. example sequences lstm handle quite long close tokens predictions made. predictions really depend long range memory example ﬁrst edge ﬁrst tokens sequence needed make prediction third prediction step keeping long range memory rnns challenging lstms better rnns still can’t completely solve problem. another challenge task output sequence appear order input sequence. fact data sequential nature even edges randomly permutated target output sequence change. applies babi task shortest path task. ggs-nns good handling type static data lstm not. however future work needed determine best apply ggs-nns temporal sequential data lstm good limitation ggs-nns model discussed section data structures like list lists nested data structures pointer node data structure points another data structure. data structures represented separation logic allowing predicates nested. example list lists represented lambda expression says node list pointer satisﬁes list node list points another list. simple list without nested structures represented represents empty predicate. note unlike non-nested case pointer always points null consider pointers order describe handle nested data structures. make ggs-nns able predict nested formulas adapt alg. alg. outer loop goes named variable generate nested predicate node associated variable active node. nested prediction procedure handles prediction similarly alg. calling nested prediction procedure recursively node annotation update line annotates nodes current structure is-explained also annotates nodes linked pointer nodes current structure active. list lists example predicting procedure procedure predictnestedformula initialize node vectors -extending quantiﬁer needed update node annotations fresh lambda variable name predictnestedformula print procedure", "year": 2015}