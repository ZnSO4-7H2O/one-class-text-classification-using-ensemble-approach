{"title": "Symbolic Knowledge Extraction using Łukasiewicz Logics", "tag": ["cs.AI", "cs.LG", "03B52, 92B20, 68T05"], "abstract": "This work describes a methodology that combines logic-based systems and connectionist systems. Our approach uses finite truth-valued {\\L}ukasiewicz logic, wherein every connective can be defined by a neuron in an artificial network. This allowed the injection of first-order formulas into a network architecture, and also simplified symbolic rule extraction. For that we trained a neural networks using the Levenderg-Marquardt algorithm, where we restricted the knowledge dissemination in the network structure. This procedure reduces neural network plasticity without drastically damaging the learning performance, thus making the descriptive power of produced neural networks similar to the descriptive power of {\\L}ukasiewicz logic language and simplifying the translation between symbolic and connectionist structures. We used this method for reverse engineering truth table and in extraction of formulas from real data sets.", "text": "abstract. work describes methodology combines logic-based systems connectionist systems. approach uses ﬁnite truth-valued lukasiewicz logic wherein every connective deﬁned neuron artiﬁcial network allowed injection ﬁrst-order formulas network architecture also simpliﬁed symbolic rule extraction. trained neural networks using levenderg-marquardt algorithm restricted knowledge dissemination network structure. procedure reduces neural network plasticity without drastically damaging learning performance thus making descriptive power produced neural networks similar descriptive power lukasiewicz logic language simplifying translation symbolic connectionist structures. used method reverse engineering truth table extraction formulas real data sets. essentially representation paradigms usually taken diﬀerently. hand symbolic-based descriptions speciﬁed grammar fairly clear semantics. hand usual information presented using connectionist description codiﬁcation neural network artiﬁcial principle combine among things ability learn robustness insensitivity perturbations input data. usually taken black boxes thereby providing little insight information codiﬁed. natural seek synergy integrating white-box character symbolic base representation learning power artiﬁcial neuronal networks. neuro-symbolic models currently active area research extraction logic programs trained networks approach neuro-symbolic models knowledge extraction based comprehensive language humans representable directly topology able used. done knowledge-based networks generate initial network architecture crude symbolic domain knowledge. ther direction hardest problem neural language translated symbolic language. however processes used identiﬁng signiﬁcant determinants decision classiﬁcation. hence individual unit must associated single concept feature problem domain. work used ﬁrst-order language wherein formulas interpreted nns. framework formulas simple inject multilayer feedforward network system free need giving interpretation hidden units problem domain. approx generation neuro-symbolic models used lukasiewicz logic. type many-valued logic useful property motivated linearity logic connectives. every logic connective deﬁned neuron artiﬁcial network having activation function identity truncated zero allows direct codiﬁcation formulas network architecture simpliﬁes extraction rules. multilayer feedforward type activation function trained eﬃciently using levenderg-marquardt algorithm generated network simpliﬁed quickly using optimal brain surgeon algorithm proposed hassibi stork g.j. stork strategy good performance applied reconstruction formulas truth tables. type reverse engineering problem presuppose noise. however process stable introduction gaussian noise. motivates application extract comprehensible symbolic rules real data. classical propositional logic earliest formal systems logic. algebraic semantics logic given boolean algebra. both logic algebraic semantics generalized many directions. manyvalued logics generalizations conceived formal representation languages proven useful real world computer science applications. applications many-valued logic like fuzzy logic properties boolean conjunction rigid overtake extending binary connective usually called fusion. generalization boolean algebra based relationship conjunction implication given operators deﬁned partially ordered truth values thereby extending two-valued boolean algebra. values associated logics called many-valued logics. many-valued logic truth values called fuzzy logic. fuzzy logic deﬁned using lukasiewicz t-norm called lukasiewicz logic corresponding propositional calculus nice complete axiomatization type logic implication called residuum operator given min. like ﬁrst-order languages llogic sentences usually built propositional variables fusion operator implication truth constant connectives deﬁned follows however want apply learn lukasiewicz sentences seems promising non-recursive approach proposition evaluation. deﬁning ﬁrst-order language circuits generated plugging atomic components. this used library components presented table interpreted neural units linked together form output without loops. interpretation formulas structure neuron deﬁnes connective identiﬁed label. task construct complex structures based simplest ones formalized using generalized programming neurons types networks inputs output interpreted function generically denoted following represent bias weights input values. context network functional interpretation sentence string-based notation relation deﬁned network execution corresponds sentence truth table. interpretation formulas simpliﬁes transformation string-based representations network representation allowing write proposition every well-formed formula llogic language codiﬁed using network deﬁnes formula interpretation activation function identity truncated zero one. instance semantic sentence described using bellow network codiﬁed presented matrices. matrices must note partial interpretation unit seen simple exercise pattern checking must take reference relation formulas conﬁguration described table original formula. truth table formula fuzzy logic number propositional variables used integer deﬁnes sub-table deﬁned called -valued truth sub-table. call castro neural network type activation function min) weights bias integer. called lukasiewicz neural network codiﬁed binary i.e. neuron inputs. network called un-representable impossible codify using binary cnn. note that binary translated directly lukasiewicz ﬁrsorder language using correspondences described table present functional interpretation formulas deﬁned using neuron inputs. interpretation classiﬁed disjunctive interpretations conjunctive interpretations. sense every representable network codiﬁed neural units satisfy patterns. also examples representable conﬁgurations neuron three inputs. table presente codiﬁed using representable units inputs corresponding interpreting formula sting-based notation. since n-nary operator commutative variables could interchange position function without changing operator output. mean that string-based representation variable permutation generates equivalent formulas. concluded recall disjunctive formulas written using disjunctions negations conjunctive formulas written using conjunctions negations. leave task classifying neuron conﬁguration according representation. that established relationship using conﬁguration bias number negative positive weights. extraction knowledge trained translate neuron conﬁguration propositional connectives form formulas. however neuron conﬁgurations translated formulas approximate formulas. quantify approximation quality deﬁned notion interpretation λ-similar formula. observed similarity selected best approximation quality approximation improves increase logics number truth values. similarity increases increase number evaluations. un-representable conﬁguration generate ﬁnite representable networks similar using rule given -valued logic formulas select approximation formula interpretation similar identiﬁcation unrepresentable conﬁguration using representable approximations used transform networks un-representable neurons representable structures. stress associated transformation characterizes translation accuracy. weights cnns assume values naturally every weighs seen approximation cnns. process identifying weighs lnns called crystallization essentially consists rounding neural weight nearest integer less equal denoted ⌊wi⌋. sense crystallization process seen pruning network structure links neurons weights near removed weights near consolidated. however process crispy. need smooth procedure crystallize network learning iteration avoid drastic reduction learning performance. iteration restricted representation bias making network representation bias converge structure similar cnn. that deﬁned representation cnns smooth crystallization process results iterating function sign sign absolute value. denoting function input output weights output network results applying input network weights neurons biases. interactive application produce networks progressively similar cnns. since every network have applications selected based learning eﬃciency test formulas. greater values imposes stronger restrictions learning. procedure induces quicker convergence admissible conﬁguration cnns. trained feed-forward using truth table. methodology trains progressively complex networks crystallized network good performance found. methodology described algorithm used truth table reverse engineering task. algorithm reverse engineering given -valued truth sub-table llogic proposition define inicial network complexity generate inicial apply backpropagation algorithm using data generated network performance crystallization using crisp process. given part truth table codiﬁes data. generated ﬁxed number hidden layers process detects learning performances aborts training generating network random heights. ﬁxed number tries network topology changed. number tries topology depends number network inputs. trying conﬁgure networks given complexity learning performance system tries apply selected back-propagation algorithm complex networks. system ﬁnds network codifying data network crystallized. error associated process increase system returns learning phase tries conﬁgure network. process converges resulting network codiﬁed crisp system prunes network selected optimal brain surgeon algorithm proposed g.j. wolf hassibi d.g. stork performance function. algorithm signiﬁcant improvement research weak convergence rate. many eﬀorts made speed algorithm. levenberg-marquardt algorithm ensued development algorithm-dependent methods. gives good exchange speed newton algorithm stability steepest descent method basic algorithm adjusts weights steepest descent direction. training method iteration algorithm deﬁnes change weights form wk+=wk−αgk gradient performance index learning rate. since newton’s method implicitly uses quadratic assumptions hessian matrix need evaluated exactly. rather approximation used hk≈j jacobian matrix contains ﬁrst derivatives network errors respect weights simple gradient descent newtonian iteration complementary advantages provide. levenberg proposed algorithm based observation whose update rule blends aforementioned algorithms given jacobian matrix evaluated learning rate. update rule used follows. error goes following update implies quadratic assumption function working reduce reduce inﬂuence gradient descent. performance function always reduced iteration algorithm hand error goes would like follow gradient increased factor. obtain advantage second derivative scaling component gradient according curvature. result larger movements along direction gradient smaller classic error valley problem occur more. crucial insight provided marquardt. replaced identity matrix levenberg update rule diagonal hessian matrix approximation resulting update rule. changed algorithm applying soft crystallization step update rule last regularization technic avoids redundancies sense redundant information codiﬁed diﬀerent locations. minimized selecting weights eliminate. task used optimal brain surgeon method uses criterion minimal increase training error. uses information second-order derivatives error function perform network pruning. given translated form string base formula every neuron representable. proposition deﬁnes tool translate connectionist representation symbolic representation. remarkable that truth table sample used learning generated formula reverse engineering algorithm converges representable equivalent original formula evaluated cases used truth table sample. number layers used implementation imposes structural restrictions formula reconstruction. truth table generated requires least hidden layers reconstructed; number levels required associated parsing tree. table presents mean times need conﬁguration mean square error less mean time computed using trials -valued truth llogic formula. implemented algorithm using matlab neural network package executed athlon dual-core processor windows vista system memory. table last formula approximated since complexity exceeds structures modiﬁable three hidden layers. others formules extraction process made equivalent reconstructions. described extraction process applied real data expresses information using cnns. naturally means process searches simple understandable models data able codify directly approximated using llogic ﬁrst-order language. process gives preference simplest models subject strong pruning criteria. strategy avoid overfetting problems associated algorithm complexity. mushrooms mushroom data available machine learning repository. data includes descriptions hypothetical samples corresponding species gilled mushrooms agaricus lepiota family. species identiﬁed deﬁnitely edible deﬁnitely poisonous unknown edibility recommended. latter class combined poisonous one. guide clearly states simple rule determining edibility mushroom. however using data truth table. data instances deﬁned using nominally valued attributes presented table below. missing attribute values attribute instances classiﬁed edible classiﬁed poisonous. used unsupervised ﬁlter converted nominal attributes binary numeric attributes. attribute values transformed binary attributes. produced data containing binary attributes. binarization used described method select relevant attributes mushroom classiﬁcation ﬁxing weak stoping criterion. result method produced model accuracy depending binary attributes deﬁned values odorgill.sizestalk.surface.above.ring ring.type spore.print.color. ❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯ ❳❳❳❳❳❳❳❳❳❳❳❳❳❳❳❳❳❳❳ ❬❬❬❬❬❬❬❬❬❬❬❬❬❬❬❬❬❬❬❬ ❝❝❝❝❝❝❝❝❝❝❝❝❝❝❝❝❝❝❝ ❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢ ✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐✐ ♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠ model accuracy since attribute values well values auto-exclusive used propositions deﬁne data set. data enriched negative cases introducing original case truth value attribute multiplied instance eatable mushroom case values edible=e poisonous=p bell=bconical=cconvex=xflat=fknobbed=k sunken=s fibrous=fgrooves=gscaly=ysmooth=s brown=nbuff=bcinnamon=cgray=ggreen=r pink=ppurple=ured=ewhite=wyellow=y bruises=tno=f almond=aanise=lcreosote=cfishy=yfoul=fmusty=mnone=npungent=pspicy=s attached=adescending=dfree=fnotched=n close=ccrowded=wdistant=d broad=bnarrow=n black=kbrown=nbuff=bchocolate=hgray=ggreen=rorange=opink=ppurple=ured=e white=wyellow=y enlarging=etapering=t bulbous=bclub=ccup=uequal=erhizomorphs=zrooted=rmissing=? stalk.shape stalk.root stalk.surface.above.ring ibrous=fscaly=ysilky=ksmooth=s stalk.surface.below.ring ibrous=fscaly=ysilky=ksmooth=s stalk.color.above.ring stalk.color.below.ring veil.type veil.color ring.number ring.type spore.print.color population habitat brown=nbuff=bcinnamon=cgray=gorange=o pink=pred=ewhite=wyellow=y brown=nbuff=bcinnamon=cgray=gorange=o pink=pred=ewhite=wyellow=y partial=puniversal=u brown=norange=owhite=wyellow=y none=none=otwo=t cobwebby=cevanescent=eflaring=flarge=lnone=npendant=psheathing=szone=z black=kbrown=nbuff=bchocolate=hgreen=rorange=opurple=uwhite=wyellow=y abundant=aclustered=cnumerous=nscattered=s several=vsolitary=y grasses=gleaves=lmeadows=mpaths=purban=uwaste=wwoods=d precise model produced restricting stopping criteria. however general produces complex propositions diﬃcult understand. instance stopping criterion systems generated model. misses cases accuracy easy convert proposition. times algorithm converged un-representable conﬁgurations like presented below accuracy. frequency type conﬁgurations increases increase required accuracy. methodology codify extract symbolic knowledge simple eﬃcient extraction comprehensible rules medium-sized data sets. moreover sensible attribute relevance. theoretical point view particularly interesting restricting values assumed neurons weights restrict information propagation network thus allowing emergence patterns neuronal network structure. case linear neuronal networks activation function identity truncate structures characterized occurrence patterns neuron conﬁguration directly presentable formulas llogic.", "year": 2016}