{"title": "LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "The alignment of heterogeneous sequential data (video to text) is an important and challenging problem. Standard techniques for such alignment, including Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from inherent drawbacks. Mainly, the Markov assumption implies that, given the immediate past, future alignment decisions are independent of further history. The separation between similarity computation and alignment decision also prevents end-to-end training. In this paper, we propose an end-to-end neural architecture where alignment actions are implemented as moving data between stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture supports a large variety of alignment tasks, including one-to-one, one-to-many, skipping unmatched elements, and (with extensions) non-monotonic alignment. Extensive experiments on synthetic and real datasets show that our algorithm outperforms state-of-the-art baselines.", "text": "alignment heterogeneous sequential data important challenging problem. standard techniques alignment including dynamic time warping conditional random fields suffer inherent drawbacks. mainly markov assumption implies that given immediate past future alignment decisions independent history. separation similarity computation alignment decision also prevents end-to-end training. paper propose end-to-end neural architecture alignment actions implemented moving data stacks long short-term memory blocks. ﬂexible architecture supports large variety alignment tasks including one-to-one one-to-many skipping unmatched elements non-monotonic alignment. extensive experiments synthetic real datasets show algorithm outperforms state-of-the-art baselines. sequence alignment prevalent problem ﬁnds diverse applications molecular biology natural language processing historic linguistics computer vision paper focus aligning heterogeneous sequences complex correspondences. heterogeneity refers lack obvious surface matching prime example alignment visual textual content. alignment requires sophisticated extraction comparable feature representations modality often performed deep neural network. common solution alignment problem consists stages performed separately learning similarity metric elements sequences ﬁnding optimal alignment figure types sequence correspondence. matching blocks sequences identical colors numbers. one-to-one matching white blocks match anything. one-to-many matching block bottom sequence matches multiple blocks top. non-monotonic situation matching always proceed strictly left right red- block yellow- top. sequences. alignment techniques based dynamic programming dynamic time warping canonical time warping widely popular. simple form understood ﬁnding shortest path edge costs computed similarity metric decision markov. variations dynamic programming perform inference markov chains order allow amount non-monotonicity cases approaches disadvantaged separation stages. conceptually learning metric directly helps optimize alignment beneﬁcial. further methods ﬁrst-order markov assumptions able take limited local context account contextual information improve alignment decisions scattered entire sequence. example knowledge narrative structure movie help align shots sentence descriptions. differentiable neural architecture heterogeneous sequence alignment call neumatch. neumatch architecture represents state partially aligned sequences using long short-term memory chains well lstm chains matched content historical alignment decisions. four recurrent lstm networks collectively capture decision context classiﬁed available alignment actions. compared traditional two-stage solution network optimized end-to-end allowing similarity metric speciﬁcally optimized alignment task. addition network utilize previous matched content inform future alignment decisions non-markov manner. example match person’s face name frodo beginning movie able identify person later alternatively sequence sampled coarser rate decision context learns alignment frequency prove useful. although proposed framework easily applied different types sequential data paper focus alignment video textual sequences especially containing narrative content like movies. task important link joint understanding multi-momdal content closely related activity recognition dense caption generation multimedia content retrieval reason choosing narrative content among challenging computational understanding multitude causal temporal interactions events disambiguation difﬁcult needed contextual information positioned apart. thus narrative contents make ideal application testbed alignment algorithms. contributions. contributions paper twofold. first propose novel end-to-end neural framework heterogeneous multi-sequence alignment. unlike prior methods architecture able take account rich context making alignment decisions. extensive experiments illustrate framework signiﬁcantly outperforms traditional baselines accuracy. second annotate dataset containing movie summary videos share research community. topics. brieﬂy review relevant literature below. representations. learned cnn-based features emerged effective representations images. recent architectures include resnet googlenet proved effective wide variety computer vision problems. representations videos received less attention. predominant paradigms include various pooling methods across features computed frame including mean pooling rnns attention-based weighted averaging alternative convolutional encoders like also proposed. language side distributed word vector representations glove models developed words combined rnns represent sentences; variants longer text include paragraph skip-thought vector representations. image/video description search retrieval. emergence effective multimodal representations spawn wide variety applications. typical problem formulations include image captioning natural languagebased retrieval visual question answering approaches along lines classiﬁed belonging either joint language-visual embeddings encoder-decoder architectures. joint vision-language embeddings facilitate image/video caption/sentence retrieval learning embed images/videos sentences space example uses simple kernel images sentences mapped common semantic meaning space deﬁned <object action scene> triplets. recent methods directly minimize pairwise ranking function positive images-captions pairs contrastive negative pairs; various ranking objective functions proposed including max-margin order-preserving losses encoder pre-training neumatch take form joint vision-language embeddings. encoder-decoder architectures similar instead attempt encode images embedding space sentence decoded; decoding process typically takes form lstm consecutively generates word token time period reached. applications approaches video captioning dense video captioning explored respectively video retrieval video-text alignment. early works video/image-text alignment include dynamic time warping help dialogs visual audio features later works correspondences nouns pronouns between text objects scenes speciﬁcally aligns scripts videos/movies using features location face speech recognition dtw. tapaswi present approach align plot synopses corresponding shots guidance subtitles facial features characters. extend algorithm introducing constraint control one-to-many matching. tapaswi present another extension allow non-monotonic matching table comparison existing video-text alignment approaches. prior method based dtw/dynamic programming conditional random field convex quadratic programming *non-monotonicity requires extensions appendix alignment book chapters video scenes. formulations markov property efﬁcient solutions dynamic programming. time historic context considered limited. develops sophisticated neural network approach computation similarities videos book chapters using skip-thought vectors order capture historic context convolutional network similarity tensor. alignment formulated conditional random field similar although method considers historic context alignment similarity computed separately. bojanowski formulate alignment quadratic integer programming solve relaxed problem. weak supervision introduced optimization constraints. method considers global context relates video text features linear transformation consider non-monotonic alignment. table compares aspects methods. overall approaches perform alignment separate stages extracting features well deﬁned metric performing alignment using similarity propose exploit temporal structure sequences performing alignment endto-end differentiable neural architecture considers local similarities. inspired augment lstm stack operations integrate large amount data future past inform alignment. section present neural framework temporal alignment heterogeneous sequences. framework general remainder paper focus speciﬁcally video textual sequence alignment. video sequence consists number consecutive video clips {vi}i=...n textual sequence consists number consecutive sentences {si}i=...m task align sequences example ﬁnding function maps index video segment corresponding sentence example input algorithm movie segmented individual shots accompanying movie script describing scenes actions broken sentences video segmentation could achieved using shot boundary detection algorithm; neumatch handle one-to-many matching caused over-segmentation. outline neumatch framework given figure goal method predict sequence alignment actions process input sequences. alignment actions deﬁne alignment input sequences manipulating contents lstm networks encode input visual sequence textual sequence matched. manipulations seen stack operations either remove insert element ﬁrst position lstm network example elements ﬁrst position removed matched. elements matched stored separate matched stack. addition features unmatched sequences video clips sentences framework also takes account complete sequence previous alignment actions well alignments thereby selection alignment actions informed rich context comes previous alignment history. core property framework learn end-to-end. however complexity feature encoders need encode video sentence content pre-training procedure ﬁrst learns aligned representations modalities ﬁne-tune encoders full framework. ﬁrst create encoders video clip sentence. that perform optional pre-training step jointly embed encoded video clips sentences space. pre-training step produces good initialization entire framework trained endis vij. condition satisﬁed similarity decreases. relative spatial position deﬁnes entailment relation entails make similarity based intuition video typically contains information described text. adopt following ranking loss objective randomly sampling contrastive video clip contrastive sentence every ground truth pair. minimizing loss function maintains similarity contrastive pair true pair least margin max{ si)} es=si max{ note expectations approximated sampling. neumatch alignment network previous section pre-trained video language encoders embed vector space. naive approach alignment hence maximizing collective similarity matched video clips sentences. however ignores temporal structures sequences lead degraded performance. observe difﬁcult sequence alignment problems exhibit following characteristics. first heterogeneous surface forms video text conceal true similarity structure suggests satisfactory understanding entire content necessary alignment. second difﬁcult problems contain complex correspondence like many-to-one matching unmatched content framework accommodate. third contextual information needed learning similarity metric scattered entire sequence. thus important consider history future making alignment decision create end-to-end network gradient alignment decisions inform content understanding similarity metric learning. to-end allows similarity metric speciﬁcally optimized alignment task. video encoder. extract features using activation ﬁrst fully connected layer vgg- network produces -dim vector frame. clip relatively short homogeneous perform mean pooling frames video yielding feature vector entire clip. vector transformed three fully connected layers using relu activation function resulting encoded video vector clip. sentence encoder. input text parsed sentences contains sequence words. transform unique word embedding vector pretrained using glove entire sentence encoded using -layer lstm recurrent network hidden state ﬁrst layer second layer memory cells layers respectively; word embedding time step sentence represented vector obtained transformation last hidden state three fully connected layers using relu activation function. encoding alignment pre-training. feeding encoders alignment network perform pairwise pre-training step coordinate encoders achieve good initialization end-to-end training. ground-truth pair adopt asymmetric similarity proposed probability optimized greedily always choosing probable action using beam search. classiﬁcation trained supervised manner. ground truth alignment sequences easily derive correct sequence actions used training. correct action sequence randomly picked. training objective minimize cross-entropy loss every time step. alignment actions. propose basic alignment actions together handle alignment sequences unmatched elements one-to-many matching. actions include clip sentence match match-retain clip match-retain sentence table provides summary effects. clip action removes element video stack. desirable match element text stack. analogously sentence action removes element text stack match action removes matches them pushes matched stack. actions matchretain clip match-retain sentence used one-to-many correspondence. many sentences matched video clip match-retain clip action pops matches pushes pair matched stack stays video stack next possible sentence. clip action must used. match-retain sentence action similarly deﬁned. formulation matching always elements stacks. worth noting actions used together. subset picked based knowledge sequences matched. example oneto-one matching know clips match sentences every sentence least matching clip need clip match. alternatively consider many-to-one scenario sentence match multiple video clips clips unmatched every sentence least matching clip. need subset clip sentence matchretain sentence. desirable choose actions possible simpliﬁes training reduces branching factor inference. discussion. utility action stack becomes apparent one-to-many setting. discussed earlier encode element matched stack features different video clips mean-pooled. result algorithm needs learn constraint many clips merged together features matched stack effective features action stack would carry necessary information. alignment actions discussed section allow monotonic matching sequences focus paper experiments. discuss extensions allow multi-sequence matching present nuematch framework meets requirements. central idea store historic information future portion sequences matched lstm recurrent networks. ﬁnal hidden states considered encode information throughout sequences. concatenated hidden states classiﬁed available alignment actions subsequently modiﬁes content lstm networks. complete framework illustrated figure lstm stacks. ﬁrst stack contains sequence video clips processed direction lstm goes allows information future clips current clip. refer lstm network video stack denote hidden state similarly text stack contains sentence sequence processed hidden state third stack action stack stores alignment actions performed past. actions denoted encoded one-hot vectors reason including stack capture patterns historic actions. different ﬁrst stacks information ﬂows ﬁrst action immediate past last hidden state fourth stack matched stack contains texts clips matched previously places last matched content stack. denote sequence similar action stack information ﬂows past present. paper consider case sentence match multiple video clips since matched video clips probably similar content perj vj/k. input lstm unit hence concatenation modalities last hidden state matched stack alignment action prediction. every time step state four stacks shorthand sequence similarly xt−. approximately represented lstm hidden states. thus conditional probability alignment action time computation implemented softmax operation fully connected layers relu activation t−]. concatenated state dataset. table shows statistics three datasets experiments. datasets create based lsmdc dataset contains matched shot-sentence pairs. lsmdc dataset extracted movies accurate descriptions intended visually impaired. generate video textual sequences following first consecutive pairs video clips descriptions movie collected. that randomly insert video clips movies video sequence. order increase difﬁculty alignment make dataset realistic select confounding clips similar true neighboring clips. randomly choosing insertion position sample video clips select similar neighboring clips using pretrained similarity metric insertion position clips away last insertion. procedure creates dataset one-to-one matching. create dataset one-to-many matching randomly split every video clip sub-clips. create also lsmdc dataset. difference that instead inserting video clips randomly delete sentences. deletion position sentences last deletion. prepare version one-to-many matching split video clips hm-. require detection clips matching sentences refer null clips. version one-to-one matching version one-to-many matching. dataset. create dataset youtube channels movie spoiler alert movies minutes narrator orally summarizes movies alongside clips actual movie. annotators transcribed audio aligned narration text video clips. release ground truth annotations links video. dataset challenging among three several reasons sequences long. average video sequence contains clips textual sequence contains sentences. unlike lsmdc employs rich textual descriptions intended storytelling; meticulously faithful descriptions video. sentence match long sequence video clips discussed sec. customize action inventory using knowledge dataset. one-to-one matching null video clips actions clip match. one-to-many matching null video clips clip sentence match-retain sentence. joint pre-training dimensions lstm sentence encoder joint embeddings. dimensions word image embedding respectively margin ranking objective function regularization used prevent over-ﬁtting. batch size number contrastive samples every positive pair. model trained adam optimizer using learning rate gradient clipping early stopping validation used avoid over-ﬁtting. alignment network uses dimensions video text stacks dimensions matched stack history stack. optionally feed additional variables fully connected layer numbers elements left video text stacks improve performance long sequences dataset. alignment network ﬁrst trained encoding networks ﬁxed learning rate that entire model trained end-to-end learning rate original data split lsmdc. split. following performance measures. oneto-one matching measure matching accuracy percentage sentences video clips correctly matched correctly assigned null. one-to-many matching sentence match multiple clips cannot accuracy measure sentences. therefore turn jaccard index computes overlap ranges video clips using intersection union case compute overlap between predicted range ground truth. create three baselines minimum distance dynamic time warping canonical time warping baselines jointly trained language-visual neural network encoders carefully trained exhibit strong performance. method matches similar clip-sentence pairs smallest distance compared others. artiﬁcially boost baseline using speciﬁc optimization accuracy measures. evaluation video clips match every clip similar sentence distance greater threshold assign clip null status. sentence accuracy match every sentence similar clip without assigning null sentences. computes optimal path distance matrix. uses fact ﬁrst sentence always matched ﬁrst clip last sentence always matched last clip shortest path upper left corner lower right corner distance matrix. note constraint neumatch aware order handle null clips make threshold again. case sentence matched several clips clips whose distances sentence threshold assigned null. thresholding method improve performance tune margin maximize performance baselines. adopt source code provided assignment method dtw. tables show performance one-toone one-to-many scenarios respectively. oneto-one versions datasets neumatch demonstrates considerable improvements best baselines. improves clip accuracy percentage points improves sentence accuracy points. unlike neumatch major clip sentence recall performance since considerably better detecting null clips. one-to-many versions well dataset neumatch consistently shows superior performance baselines. advantage best baselines points clip accuracy sentence iou. expect dataset difﬁcult hm-. still relative improvement clip accuracy sentence closest baseline. across experimental conditions observe that unsurprisingly one-to-one matching easier one-tomany matching. interestingly neumatch performs better baselines largely indifferent datasets. attribute neumatch’s ability extract features matched stack. recall created inserting random clips video sequence. even though inserted clips similar neighboring clips extent still dissimilar aspects cinematography style color. clips original video sequence likely consistent aspects. thus order pick null clips neumatch needs recognize difference style original sequence inserted clips. however difference property speciﬁc video sequence jointly trained encoders video-text pairs cannot pick difference. contrast sentences video clips movie neumatch cannot rely video style pick null clips. result appeared difﬁcult hm-. qualitative example alignment result given figure distance matrix ground alignment goes left bottom right green yellow represent ground truth alignment predicted alignment intersection two. ground truth path columns mark clips matched sentence. shown distance matrix exhibit clear alignment path. baselines matchings failing detection null clips. hand neumatch superior detecting null clips seen table recall clips recall sentences -action model given percentage data. datasets require detection null clips one-to-many matchings sentences. figure example video-text sequence pair alignment results. vertical horizontal axes represent text sequence video sequence respectively. green yellow represents ground truth matching predicted results intersection two. paper propose end-to-end neural architecture heterogeneous sequence alignment focusing alignment video textural data. alignment actions implemented network data moving operations lstm stacks. show ﬂexible architecture supports variety alignment tasks encode rich context making alignment decisions. results number datasets settings illustrate superiority model traditional alignment approaches. basic action inventory tackles alignment sequences. alignment sequences simultaneously like video audio textual sequences requires extension action inventory. introduce parameterized match-retain action. three sequences parameters -bit binary vector indicate element sequence being matched otherwise. table shows example using parameterized match-retain. instance match elements sequence action parameterized actions enables nonmonotonic matching sequences. previous examples matching happens stack tops. non-monotonic matching equivalent allowing stack elements match element matched stack. propose parameterized action match-withhistory single parameter indicates position matched stack. deal fact matched stack variable length adopt indexing method pointer networks probability choosing matched element", "year": 2018}