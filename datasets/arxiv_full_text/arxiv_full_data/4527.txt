{"title": "Learning Class-Level Bayes Nets for Relational Data", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "Many databases store data in relational format, with different types of entities and information about links between the entities. The field of statistical-relational learning (SRL) has developed a number of new statistical models for such data. In this paper we focus on learning class-level or first-order dependencies, which model the general database statistics over attributes of linked objects and links (e.g., the percentage of A grades given in computer science classes). Class-level statistical relationships are important in themselves, and they support applications like policy making, strategic planning, and query optimization. Most current SRL methods find class-level dependencies, but their main task is to support instance-level predictions about the attributes or links of specific entities. We focus only on class-level prediction, and describe algorithms for learning class-level models that are orders of magnitude faster for this task. Our algorithms learn Bayes nets with relational structure, leveraging the efficiency of single-table nonrelational Bayes net learners. An evaluation of our methods on three data sets shows that they are computationally feasible for realistic table sizes, and that the learned structures represent the statistical information in the databases well. After learning compiles the database statistics into a Bayes net, querying these statistics via Bayes net inference is faster than with SQL queries, and does not depend on the size of the database.", "text": "abstract many databases store data relational format different types entities information links entities. ﬁeld statistical-relational learning developed number statistical models data. paper focus learning class-level ﬁrst-order dependencies model general database statistics attributes linked objects links classlevel statistical relationships important themselves support applications like policy making strategic planning query optimization. current methods class-level dependencies main task support instance-level predictions attributes links speciﬁc entities. focus class-level prediction describe algorithms learning class-level models orders magnitude faster task. algorithms learn bayes nets relational structure leveraging efﬁciency single-table nonrelational bayes learners. evaluation methods three data sets shows computationally feasible realistic table sizes learned structures represent statistical information databases well. learning compiles database statistics bayes querying statistics bayes inference faster queries depend size database. introduction many real-world applications store data relational format different tables entities links. standard machine learning techniques applied data stored single table nonrelational propositional format ﬁeld statistical-relational learning aims extend machine learning algorithms relational data major machine learning tasks data build generative statistical model variables application domain single-table learning setting goal often represent predictive dependencies attributes single individual setting task description modelling class-level dependencies many models distinguish different levels class type dependency model instance dependency model graphical model nodes instance dependency model represent attributes individuals relationships. nodes class dependency model correspond attributes tables. term class unrelated classiﬁcation; views analogous concept class object-oriented programming. example class-level probabilistic dependency among students high intelligence rate instance-level prediction would given jack highly intelligent probability thus class-level dependencies concerned rates events occur properties hold within class whereas instance-level dependencies concerned speciﬁc events properties speciﬁc entities terms predicate logic classlevel model features terms involve ﬁrst-order variables variable ranging domain like students) whereas instance-level graph features terms involve constants jack constant denotes particular student) typically systems instantiate class-level model speciﬁc entities attributes relationships given database obtain instance dependency graph important purpose instance graph support predictions attributes individual entities. issue making predictions combining problem combine information different related entities predict property target entity. current model construction algorithms learn class-level dependencies instance-level predictions time. approach focus classlevel variables rather making predictions individual entities. apply bayes technology design algorithms especially learning class-level dependencies. experiments algorithms orders magnitude faster benchmark method. models thus trade-off tractability learning ability answer queries individual entities. policy making strategic planning. university administrator wish know program characteristics attract high-ranking students general rather predict rank speciﬁc student speciﬁc program. query optimization applications statistical model predicts probability given table join conditions used infer size join result estimating join sizes problem database query optimization. queries involve several tables joined together ideal scenario smaller intermediate joins class-level statistical model used estimating frequency counts database select smaller joins optimize speed answering queries taking efﬁcient intermediate steps. example suppose wish predict size join student table registered table records students registered courses selection condition student high intelligence. logical query language like domain relational calculus join would expressed conjunctive formula registered intelligence high. query join bayes used estimate frequency conjunction true database immediately translates estimate size join corresponds conjunction. join conditions often involve speciﬁc individuals. private data. domains information individuals protected conﬁdentiality concerns. example analyzes database police crime records. database anonymized would unethical data mining researcher predict crimes committed individuals. however appropriate important look general risk factors associated crimes example spatial patterns heading privacy-preserving data mining researchers devoted much attention problem discovering class-level patterns without compromising sensitive information individuals approach. apply bayes nets model classlevel dependencies attributes appear separate tables. bayes nets widely studied applied generative model classes. directed acyclic graph whose nodes represent random variables whose edges represent direct statistical associations. class-level bayes nets contain nodes correspond descriptive attributes database tables plus boolean nodes indicate presence relationship; refer join bayes nets apply machine learning algorithms learn structure join bayes database need deﬁne empirical database distribution values class-level nodes based frequencies events database. logical setting question assign database frequencies conjunction atomic statements intelligence high registered diﬃculty high. follow deﬁnition established fundamental research concerning combination logic statistics especially classic work halpern bacchus research generalized concept single-table frequencies relational domain frequency ﬁrst-order formula relational database number instantiations variables formula satisfy formula database divided number possible instantiations. example above instantiation frequency would number student-course pairs student highly intelligent course highly difﬁcult student registered course divided possible student-course pairs. terms table joins instantiation frequency number tuples join corresponds conjunction divided maximum size join. learn-and-join algorithm aims model database distribution. upgrades single table learner chosen user carry relational learning decomposing learning problem entire database learning problems smaller tables. basic idea apply learner repeatedly tables join tables database merge resulting graphs single graphical model entire database. algorithm treats single-table learner module within relational structure learning system. means minimal work required build statisticalrelational learner single-table learner. main algorithmic problem parameter estimation join bayes nets counting number satisfying instantiations ﬁrstorder formula database. show recent virtual join algorithm applied solve problem efﬁciently. virtual join algorithm efﬁcient algorithm designed compute database instantiation frequencies experiments provide evidence learn join algorithm leverages efﬁciency scalability reliability single-table learning efﬁciency scalability reliability statistical-relational learning. benchmark computational performance algorithm structure learning markov logic networks prominent statistical-relational formalisms experiments small datasets run-time learn-andjoin algorithm times faster state-of-the alchemy program learning structure. medium-size datasets financial database pkdd alchemy return result given system resources whereas learn-and-join algorithm produces join bayes model within less min. evaluate learned structures apply inference algorithms predict relational frequencies compare gold-standard frequencies computed queries. learned join bayes nets predict database frequencies well. experiments prediction take advantage fact jbns standard bayes format class-level queries answered standard inference algorithms used formalisms focus instance-level inference support class-level inference least current implementations. datasets code available download ftp//ftp.fas.sfu.ca/pub/cs/oschulte. paper organization. statistical-relational learning complex subject variety approaches proposed review related work detail. preliminary section introduces bayes nets predicate logic. formally deﬁne join bayes nets instantiation frequency distribution model. main part paper describes structure parameter learning algorithms join bayes nets. evaluate algorithms synthetic dataset public domain datasets examining learning runtimes predictive performance learned models. much research theoretical foundations database distribution representing reasoning instantiation frequencies axiomatic framework based theorem proving approach utilizes graphical models learning inference database distribution rather logical calculus. inference approach appears ﬁrst utilizes standard inference algorithm carry probabilistic reasoning database frequencies. learning work appears ﬁrst bayes nets learn statistical model database distribution. formalisms. various formalisms proposed combining logic graphical models bayes logic networks parametrized belief networks object-oriented bayes nets probabilistic relational model markov logic networks summarize main points comparison. general overviews provided main difference jbns formalisms semantics. semantics jbns speciﬁed class level without reference instance-level model; models database distribution deﬁned instantiation frequencies. syntactically join bayes nets similar parametrized belief networks blns. blns require speciﬁcation combining rules standard concept bayes design solve combining problem. instance class-level model specify probability student highly intelligent given obtained difﬁcult course. grades thus translates multiset probabilities combining rule translates single probability. essentially without combining rules. difference prms blns prms aggregation functions combining rules shows aggregate functions added blns; construction works adding jbns. object-oriented bayes nets aggregation like prms special constructs capturing class hierarchies; otherwise expressive power similar blns jbns. markov logic networks combine ideas undirected graphical models logical representation. formulas weight assigned each. mlns like jbns unlike blns prms complete without specifying combining rule aggregation function. like blns prms unlike jbns semantics speciﬁed terms instance-level network whose nodes contain ground formulas instance-level prediction deﬁned using log-linear form markov random ﬁelds bayes nets random variable pair rangle possible values called domain probability distribution values. simplicity assume paper random variables ﬁnite domains generic values domain denoted lowercase letters like atomic assignment assigns value random variable dom. joint distribution assigns probability conjunction atomic assignments; write sometimes abbreviated compactly refer variables like ..xn} assignment values boldface sets variables conditional probability deﬁned employ notation terminology bayesian network. bayes structure directed acyclic graph whose nodes comprise random variables denoted discussing bayes refer interchangeably nodes variables. parents node graph denoted assignment values parents denoted risk confusion often simply write pax. bayes pair θgrangle parameter values specify probability distributions children conditional instantiations parents i.e. conditional probabilities form conditional probabilities speciﬁed conditional probability table variable θgrangle deﬁnes joint probability distribution according formula value node speciﬁed assignment term apai denotes assignment value parent speciﬁed assignment corresponding cp-table entry. thus joint probability assignment obtained multiplying conditional probabilities node value assignment given parent value assignments. relational schemas first-order formulas begin standard relational schema containing tables ﬁelds descriptive attributes foreign pointers. database instance speciﬁes tuples contained tables given database schema. table reftableuniversity-schema shows relational schema inference learning jbns bayes format class-level probabilistic queries involve ﬁrstorder variables constants answered standard efﬁcient bayes nets inference algorithms used seen instance lifted ﬁrst-order probabilistic inference topic received good deal attention recently formalisms focus instance-level queries involve constants only support class-level inference least current implementations. common approach structure learning instance-level structure assign likelihood given database likelihood counterpart likelihood sample given statistical model single table learning. learning searches parametrized model whose instance-level graph assigns maximum likelihood given database typically likelihood balanced complexity penalty prevent overﬁtting approach conceptually elegant learn class-level dependencies instance-level predictions time. jbns separate task ﬁnding generic class-level dependencies task predicting attributes individuals. advantages learning directed graphical models bayes nets class-level include following. class-level learning avoids problem instantiated model contain cycles even classlevel model not. example suppose classlevel model indicates student friends another student smoking habits likely similar smokes predicts smokes. database situation friends instance level model would contain cycle smokes smokes smokes smokes learning deﬁned terms instance model cycles cause difﬁculties concepts algorithms directed acyclic models longer apply. particular likelihood database measures data longer deﬁned. difﬁculties researchers conclude acyclicity constraints directed models severely limit applicability relational data directed formalisms require extra structure solve combining problem instance-level predictions aggregation functions combining rules. extra structure increases representational power model also leads considerable complications learning. preliminaries combine concepts graphical models machine learning relational schemas database theory conjunctions literals ﬁrst-order logic. section indatabase related university figure refﬁguniversity-tables displays small database instance schema. table join tables contains rows cartesian products tables whose values match common ﬁelds. tion entity sets correspond types descriptive attributes functions relationship tables predicates foreign constraints type constraints arguments relationship predicates. formulas syntax constructed using three types symbols constants variables functions. sometimes refer logical variables ﬁrst-order variables distinguish random variables. type domain constants. case entity type constants correspond primary keys entity table. constant variable assigned type arguments values functions. predicate function whose values special truth values constants discusses appropriate ﬁrst-order logic srl. logical syntax standard give deﬁnition detail clarify space statistical patterns learning algorithms applied give rigorous deﬁnition database frequency learning method. table reftablevocab deﬁnes logical vocabulary. illustrates correspondence schema. term expression denotes single object; notation denotes vector list terms. predicate sometimes write terms recursively constructed follows. schema derived entity-relationship model tables relational schema divided entity tables relationship tables. algorithms generalize data model translated logical vocabulary based ﬁrst-order logic case model. entity types schema reftableuniversity-schema students courses professors. relationship tables registration records courses taken student grade satisfaction achieved records research assistantship contracts students professors. university example entity tables student table table. relationship table foreign pointers student tables whose tuples indicate students registered courses. intuitively entity table corresponds type entity relationship table represents relation entity types. well known relational schema translated follow logic-based approaches logic rigorous expressive formalism representing regularities found data. speciﬁcally ﬁrst-order logic typed variables function symbols formalism rich enough represent constraints schema following transla. function term argument type list terms matches type matches argument type expression function term whose type value type atom equation form types match. negative literal atom form atoms positive literals. formulas consider conjunctions literals short conjunctions. prolog-style notation vector notation conjunctions literals. term ground contains variables; otherwise term open. ﬁnite open function terms assignment conjunction form constant. relationship literal literal variables. database instance assigns denotation constant ground function term denote database instance figure refﬁguniversity-tables since registered false case assign descriptive attribute special value undeﬁned. general constraint descriptive attribute predicate ground literal write evaluates true otherwise. predicate sometimes write ground conjunction evaluates true case literals evaluate true. make unique names assumption distinct constants denote different objects entity type domain constants satisfying domain variable type domain type domd domd. grounding variables assigns constant right type variable domd). grounding variables occur conjunction write result replacing occurrence variable constant number groundings satisfy conjunction deﬁned discussion. function terms contain single variable equation reduces standard deﬁnition single-table frequency events. example ratio students intelligence level number students. halpern gave deﬁnition frequency ﬁrst-order formula holds given database assumes distribution domain type. instantiation frequency special case uniform distribution elements domain halpern explains intuitive interpretation deﬁnition corresponds generic regularities random individuals probability randomly chosen bird greater conditional instantiation frequency plays important role discriminative learning inductive logic programming instance classic foil system generalizes entropy-based decision tree learning ﬁrst-order rules goal predict class label foil uses conditional instantiation frequency deﬁne entropy empirical class distribution conditional body rule. introduce variable constants type student {jack paul}. true ground literals include intelligence ¬registered. table reftableliterals shows various open literals frequency database derived number true groundings. structure learning join bayes nets deﬁne class bayes models model database distribution present structure learning algorithm. definition database associated logical vocabulary vocab. join bayes structure directed acyclic graph whose nodes ﬁnite open function terms. domain node range figure refﬁguniversity-jbn shows example join bayes net. also refer relationship terms appear relationship indicator variables simply relationship variables. assigns probabilities conjunctions literals form term join conjunctions correspond table joins databases. step model learning deﬁne empirical distribution random variables model. since random variables function terms amounts associating probability conjunction literals given database. instantiation frequency empirical distribution nodes jbn. goal structure learning construct model given database input. upgrading propositional learner statistical relational learner. upgrading mean propositional learner used function call module body algorithm. require propositional learner takes input addition single table cases also edge constraints specify required forbidden directed edges. output algorithm database variables speciﬁed deﬁnition refdefjbn. approach learn join apply learner single tables combine results successively larger graphs corresponding larger table joins. principle contain open function terms depending attributes relationships interest. keep description structure learning algorithm simple assume contains default nodes follows node descriptive attribute entities relationships boolean indicator node relationship nodes contain constants. type entity introduce ﬁrstorder variable. algorithm four phases analyze single tables. learn structure descriptive attributes entity table database separately phase within-table dependencies among descriptive attributes ranking). analyze single join tables. relationship table considered. input table relationship join table entity tables linked foreign constraint edges attributes entity table constrained agree structure learned phase additional edges variables corresponding attributes different tables added. phase dependencies descriptive attributes conditional existence relationship. phase analyze double join tables. extended input relationship tables second phase joined pairs form input tables learner. edges variables considered phases constrained agree structures previously learned. graphs learned join pair merged produce phase dependencies descriptive attributes conditional existence relationship chain length satisfy slot chain constraints. link attributes different tables arrows boolean relationship variables added required satisfy following constraints share variable among arguments parents contain chain foreign links connecting figure refﬁgstructure-learn illustrates increasingly figure illustrate learn-and-join algorithm. given learner applied table join tables presence absence edges lower levels inherited higher levels. number variables entity type. data patterns require variable type express. simple example suppose social network friends represented friend relationship whose arguments type person. correlation smoking person friends place link nodes smokes smokes requires variables type person. principle case several variables given entity type translated case variable entity type follows variable given type make copy entity table. example would obtain person tables person person copy treated type associated variable. leave future work efﬁcient implementation learn-and-join algorithm several variables given entity type. input database ..ee entity tables ...rr relationship tables output graph calls propositional bayes learner accepts edge constraints single table cases input. notation denotes output pbn. getconstraints speciﬁes edge constraints namely edges required edges missing variables forbidden. correlation coverage. join-and-learn algorithm ﬁnds correlations descriptive attributes within single table attributes different linked tables. however dependencies between relationship variables predicts friend). search dependencies could local search methods described learn-and-join algorithm suitable goal correlations descriptive attributes conditional given link structure. parameter learning join bayes nets section treats problem computing conditional frequencies database distribution corresponds computing sample frequencies single table case. main problem computing probabilities conditional absence relationship. problem arises because includes relationship indicator variables building therefore requires modelling case relationship hold. apply recent virtual join algorithm address computational bottleneck. constraint algorithm seeks satisfy avoid enumerating number tuples satisfy negative relationship literal. numerical example illustrates necessary. consider university database students courses tas. student registered courses size registered table notation d)d) number complementary student-course pairs much larger number database systems. consider joins complemented tables even difﬁcult deal with suppose course tas. whereas d¬ta) order explaining basic idea behind algorithm give pseudocode utilizing estimate cp-table entries joint probabilities. conclude run-time analysis. virtual join algorithm cp-tables. instead computing conditional frequencies algorithm computes joint probabilities form correspond conjunctions literals. conditional probabilities easily obtained joint probabilities summation. generally easier conditional rather joint probabilities efﬁcient dynamic programming scheme relies simpler structure conjunctions. enumeration groundings negative literals avoided recursively applying probabilistic principle consider equation entails conjunction literals literal. equation shows computation probability involving negative relationship literal recursively reduced computations positive literal neither contains less negative relationship literal. base case literals positive problem database instance contains positive relationship literals only. done standard database table join. figure refﬁgexample illustrates recursion. algorithm computes database frequency single input conjunction literals. adapt cp-table estimation changes compute frequencies deﬁned join table join built cp-table data structure storing results intermediate computations database frequencies derived. algorithm visualized dynamic program successively ﬁlls rows joint probability table jp-table ﬁrst rows nonexistent relationships rows nonexistent relationship etc. jp-table like cptable whose rows correspond joint probabilities rather conditional probabilities. algorithm refalgadapted shows pseudocode algorithm. implementation complexity analysis. intermediate results computation stored extended jp-table structure features third value relationship predicates used represent frequencies relationship predicate attributes unspeciﬁed. algorithm satisﬁes constraint never enumerates groundings satisfy negative relationship literal. detailed complexity analysis algorithm given summarize main points relevant cp-table estimation. essentially computation step ﬁlls entry extended jp-table. compared cp-tables given join bayes structure algorithm adds extra auxilliary value domain relationship indicator variable. thus increase size data structure manageable compared cp-table original jbn. important point recursive update line reﬂineupdate require database access. data access occurs lines reflinestart-join– reflineend-join. therefore cost terms database accesses essentially cost counting frequencies join table comprising relationships occur child parent nodes computation necessary algorithm estimates cp-table entries. optimized using tuple propagation method crucial parameter complexity join number relationship predicates. learn-and-join algorithm parameter bounded provide analysis whose upshot typical applications parameter treated small constant. brief summary reasons follows. space models rules need searched becomes infeasible many relationships considered once. patterns involve many relationships instance relationship chains length more hard understand users. objects related long relationship chains tend carry less statistical information target object. next examine empirically performance structure parameter learning algorithms. figure frequency negated relationships computed frequencies involve positive relationships only. leaves computation tree involves existing database tables only. subtractions involve looking results previous computations. reduce clutter abbreviated predicates. evaluate sides trade-off three datasets run-time algorithm especially dataset grows larger performance predicting database probabilities class level main task motivates algorithm. important dependencies algorithm misses worse predictive performance evaluating predictions provides information quality structure learned. implementation datasets experiments done quad .ghz ram. implementation used many procedures version cmu’s tetrad package single table search used tetrad implementation search bdeu score edge constrains implemented using tetrad’s knowledge functionality. inference carried tetrad’s rowsum exact updater algorithm. datasets code available download ftp//ftp.fas.sfu.ca/pub/cs/oschulte. datasets university database. order check correctness algorithms directly manually created small dataset based schema given reftableuniversity-schema. entity tables contain students courses professors. table rows table rows. movielens database. second dataset movielens dataset irvine machine learning repository. contains entity tables user tuples item tuples relationship table rated ratings. user table descriptive attributes gender occupation. discretized attribute three bins equal frequency. table item represents information movies. boolean attributes indicate genres given movie; movie belong several genres time. example movie value action attributes. performed preliminary data analysis financial database. third dataset modiﬁed version ﬁnancial dataset pkdd cup. entity tables client tuples account tuples. relationship tables creditcard tuples disposition tuples relate client account. client table descriptive attributes client’s gender attributes demographic data client. account table descriptive attributes information loan amount associated account account opening date frequently account used. learning experimental design benchmark runtimes learning algorithm applied structure learning routine learnstruct alchemy package mlns chose mlns following reasons mlns currently active areas research part reason undirected graphical models avoid computational representational problems caused cycles instance-level directed models discriminative mlns viewed logic-based templates conditional markov random ﬁelds prominent formalism relational classiﬁcation alchemy provides open-source state-of-the-art learning software mlns. structure learning software alternative systems like blns prms easily available. blns prms require speciﬁcation additional structure like aggregation functions combining rules. confounds experiments parameters specify. also incorporating extra structure complicates learning formalisms arguably direct comparison learning fair. contrast formalism like rethe webpage lists different systems software available them. balios engine supports parameter learning structure learning; could obtain source code structure learning. notation corresponds partial complete assignment function terms. value assigned function term denoted probability stored jp-table denoted input database child variable parent variables divided relationship predicates function terms relationship predicates. calls function join-frequencies. computes join frequencies conditional relationships true. output joint probability table entry frequencies database distribution {ﬁll rows false relationships using table joins} learning results present results applying learning algorithms three relational datasets. resulting jbns shown figures refﬁguniversity-jbn refﬁgstructmovie refﬁgstructﬁnancial. movielens dataset algorithm ﬁnds number cross-entity table links involving user. genres high negative correlation algorithm produces dense graph among genre attributes. richer relational structure financial dataset reﬂected complex graph several cross-table links. birthday customer especially many links variables. cp-tables learned graph structures ﬁlled using dynamic programming algorithm refalgadapted. table reftableruntime presents summary times datasets. databases translate ground atoms alchemy input follows university movielens financial system alchemy able process university database sufﬁcient computational resources return result movielens financial data. therefore subsampled datasets obtain small databases compare alchemy’s runtime join-and-learn algorithm. alchemy returned result complete datasets formed three subdatabases randomly selecting entities dataset. restricted relationship tuples subdatabase involve selected entities. resulting subdatabase sizes follows. movielens users movies atoms users movies atoms users movies atoms. financial clients accounts atoms clients accounts atoms clients accounts atoms. financial dataset contains numerous descriptive attributes alchemy returned result smallest subdatabase table reftableruntime shows runtime learning algorithm applied entire dataset times faster alchemy’s learning time dataset half size. emphasize criticism alchemy structure learning aims structure optimal instance-level predictions rather illustrates task ﬁnding class-level dependencies much less computationally complex taken independent task taken conjunction optimizing instance-level predictions. next section compares probabilities estimated database frequencies computed directly queries. parameter estimation. appropriate goal evaluate whether statistical model adequately summarizes data distribution. data frequencies smoothed support generalizations beyond data ﬁrst step compute data frequencies experiments relevant case too. randomly generated queries data according following procedure. first choose randomly target node value probability predicted. choose randomly number conditioning variables ranging make random selection variables corresponding values query answered table reftableinference shows representative test queries. compares probabilities predicted frequencies database computed query well runtimes computing probability using sql. inference results averages reported taken random queries dataset. table reftableinference figure refﬁgresults predicted probabilities close data frequencies average difference less movielens less financial. measurements financial taken queries positive relationship literals only queries involve negated relationships terminate result financial dataset. graph shows nontermination corresponding high time-out number. observations processing speed hold datasets include following. queries involving negated relationships inference much faster movielens dataset. queries negated relationships infeasible financial dataset whereas returns answer around seconds. queries speed number conditioning variables increases. higher probability queries slower correspond larger joins whereas size probability affect query processing. observations depend difference datasets movielens relatively many tuples attributes whereas financial relatively tuples many attributes. factors differentially affected performance inference follows larger number attributes and/or larger number categories attributes schema decreases speed inference queries. slowdown greater queries required marginalization steps expensive number tuples database table signiﬁcant difﬁcult available implementations support instance-level queries. reﬂects fact current systems designed task predicting attributes individual entities rather class-level prediction. therefore evaluated class-level predictions system gold standard frequency counts obtained directly data using queries. follow approach experiments sample frequencies table table shows representative randomly generated queries. compare probability estimated learned model database frequency computed direct queries execution times inference method. query last line exceeded system resources. simplicity omitted ﬁrst-order variables. jects links. algorithms upgrade single-table bayes learner self-contained module perform relational learning. inference carried standard algorithms answer class-level probabilistic queries. evaluation methods three data sets shows computationally feasible realistic table sizes learned structures represented statistical information databases well. querying database statistics often faster directly queries depend size database. fundamental limitation approach join bayes nets directly support instance-level queries limitations addressed future research include restrictions types correlation structure learning algorithm discover dependencies relationships dependencies require ﬁrst-order variable entity type represent. acknowledgments research supported discovery grants ﬁrst ﬁfth author natural sciences engineering research council canada. preliminary version results presented struck workshops ijcai computational intelligence forum university british columbia. thank audiences wang helpful comments.", "year": 2008}