{"title": "A unified view of entropy-regularized Markov decision processes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a general framework for entropy-regularized average-reward reinforcement learning in Markov decision processes (MDPs). Our approach is based on extending the linear-programming formulation of policy optimization in MDPs to accommodate convex regularization functions. Our key result is showing that using the conditional entropy of the joint state-action distributions as regularization yields a dual optimization problem closely resembling the Bellman optimality equations. This result enables us to formalize a number of state-of-the-art entropy-regularized reinforcement learning algorithms as approximate variants of Mirror Descent or Dual Averaging, and thus to argue about the convergence properties of these methods. In particular, we show that the exact version of the TRPO algorithm of Schulman et al. (2015) actually converges to the optimal policy, while the entropy-regularized policy gradient methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally, we illustrate empirically the effects of using various regularization techniques on learning performance in a simple reinforcement learning setup.", "text": "gergely universitat pompeu fabra barcelona spain vicenç gómez universitat pompeu fabra barcelona spain anders jonsson universitat pompeu fabra barcelona spain propose general framework entropy-regularized average-reward reinforcement learning markov decision processes approach based extending linear-programming formulation policy optimization mdps accommodate convex regularization functions. result showing using conditional entropy joint state-action distributions regularization yields dual optimization problem closely resembling bellman optimality equations. result enables formalize number state-of-the-art entropy-regularized reinforcement learning algorithms approximate variants mirror descent dual averaging thus argue convergence properties methods. particular show exact version trpo algorithm schulman actually converges optimal policy entropy-regularized policy gradient methods mnih fail converge ﬁxed point. finally illustrate empirically eﬀects using various regularization techniques learning performance simple reinforcement learning setup. introduction reinforcement learning discipline model-based optimal sequential decisionmaking unknown stochastic environments. average-reward reinforcement learning goal behavior policy maximizes long-term average reward taking account eﬀect decision future evolution decisionmaking process. known environments optimization problem studied since inﬂuential work bellman howard optimal behavior policy formulated solution bellman optimality equations. unknown environments partially known misspeciﬁed models greedily solving equations often results policies optimal true environment. rooted statistical learning theory notion regularization oﬀers principled dealing issue among many others. particular entropy regularization proven successful tools machine learning related ﬁelds idea entropy regularization also used extensively reinforcement learning literature entropyregularized variants classic bellman equations entailing reinforcementlearning algorithms proposed induce safe exploration risk-sensitive policies model observed behavior imperfect decision-makers among others. complementary approaches rooted dynamic programming another line work proposes direct policy search methods attempting optimize various entropy-regularized objectives main goal driving safe online exploration procedure unknown markov decision process. notably state-of-the-art methods mnih schulman based entropy-regularized policy search. work connect seemingly disparate lines work showing strong lagrangian duality entropy-regularized bellman equations certain regularized average-reward objective. speciﬁcally extend linearprogramming formulation problem optimization mdps accommodate convex regularization functions resulting convex program. show using conditional entropy joint state-action distribution gives rise nonlinear equations resembling bellman optimality equations. observing duality enables establish connection regularized versions value policy iteration methods incremental convex optimization methods like mirror descent dual averaging instance convex-optimization view propose reveals trpo algorithm schulman regularized policy-gradient method mnih approximate versions mirror descent dual averaging respectively interpreted regularized policy iteration methods. work provides theoretical justiﬁcation various algorithms ﬁrst derived heuristically. particular framework reveals exact version trpo identical mdp-e algorithm even-dar establishes fact policy updates trpo converge optimal policy improving theoretical results claimed schulman also argue formulation useful pointing possible inconsistencies heuristic learning algorithms. particular show approximation steps employed mnih break convexity objective thus possibly leading convergence local optima even divergence. observation accordance recent ﬁndings asadi littman show value iteration poorly chosen approximate updates lead divergence. complement results suggest alternative objective optimized consistently avoiding possibility diverging. similar lagrangian duality bellman equations entropy maximization previously noted ziebart rawlik special class episodic markov decision processes time index within episode part state representation. particular setting convexity conditional entropy obvious. observations pointing convexity conditional entropy distribution functions deﬁned general state spaces enables develop much broader theory regularized markov decision processes. note theory also readily extends discounted mdps replacing stationary state-action distributions consider discounted state-action occupancy measures. consistency discuss particular algorithm natural average-reward version noting conclusions remain valid simpler discounted episodic settings. rest paper organized follows. section provide background average-reward markov decision processes brieﬂy discussing linear-programming dynamic-programming derivations optimal control. section provide convex-programming formulation regularized average-reward markov decision processes show connection regularized bellman equations. section provides brief summary complementary dynamic-programming formulation discusses regularized equivalents related concepts expressions regularized policy gradient. section describe several existing learning algorithms framework. provide empirical evaluation various regularization schemes simple reinforcement learning problem section preliminaries markov decision processes consider ﬁnite markov decision process ﬁnite state space ﬁnite action space ×a×x transition function denoting probability moving state state taking action reward function mapping state-action pairs rewards. round learner observes state selects action moves next state obtains reward goal select actions maximize notion cumulative reward. paper consider stationary state-feedback policy deﬁnes probability distribution learner’s actions state theory stipulates mild conditions average-reward criterion maximized stationary policies. throughout paper make following mild assumption particular assumption satisﬁed policies induce irreducible aperiodic markov chain ease exposition section also make following simplifying assumption general assumption restrictive allow policies cover diﬀerent parts state space. stress results later sections require assumption hold. assumptions mind deﬁne average reward policy round existence limit ensured assumption furthermore average reward policy simply νππr linear function stationary state-action distribution νππ. suggests ﬁnding optimal policy equivalently written linear program decision variable stationary state-action distribution. deﬁning feasible stationary distributions policy induces stationary distributions stationary distribution policy deﬁned µ/νµ denominator strictly positive recurrent states assumption since compact polytope here dual variables commonly referred value functions. strong duality assumption solution equals optimal average reward dual variables optimum solution average-reward bellman optimality equations regularized mdps convex-optimization view inspired formulation average-reward optimization problem deﬁne regularized optimization objective—a framework lead main results. results section require mild assumption regularized optimization problem takes form rx×a convex regularization function learning rate trades original objective regularization. note recovers unregularized objective. unlike previous work formulations mdps useful regard primal. form standard fact bregman divergence immediately obvious. following proposition asserts statement prove appendix work aware establishes comparable result recent paper gómez proceed derive dual functions optimal solutions choices regularization functions. without loss generality assume reference policy full support implies corresponding stationary distribution strictly positive recurrent provide derivations bregman divergences; calculations analogous solutions expressed help dual variables useful think value functions case formulation also deﬁne corresponding advantage functions analogy bellman optimality equations call regularized guaranteed ﬁnite solution optimization problem well-deﬁned bounded unique constant shift again make solution unique imposing constraint expected value equal dynamic programming regularized mdps present dynamic-programming view regularized optimization problem choice similar derivations done several times discounted episodic mdps aware work considers average-reward case. said generalization straightforward existence unicity assumption proposition bertsekas easy show system equations unique solution satisfying additional constraint correspond bellman equations corresponds equation include results useful deriving approximate dynamic programming algorithms. ﬁrst provide counterpart performance-diﬀerence lemma statement rely regularized advantage function deﬁned policy counterpart policy gradient theorem sutton formalize statement consider policy parametrized vector assume gradient exists form policy gradient given following lemma prove appendix lemma assume satisﬁes algorithms section derive several reinforcement learning algorithms based results. clarity presentation assume fully known assumption later relax experimental evaluation. study generic sequential optimization framework sequence policies computed iteratively. inspired online convex optimization literature convex-optimization formulation study families algorithms mirror descent dual averaging ﬁrst remark relative entropy policy search algorithm peters formulated instance mirror descent bregman divergence easily seen comparing form update rule problem formulation peters slight diﬀerence regularization additive enforced constraint. easy amounts change learning rate. connection ﬁrst shown zimin recently rediscovered montgomery levine independently other zimin dick show mirror descent achieves near-optimal regret guarantees online learning setup transition function known reward function allowed change arbitrarily decision rounds. implies reps duly converges optimal policy setup. next show dynamic policy programming algorithm azar trust-region policy optimization algorithm schulman approximate variants mirror descent bregman divergence this note full mirror descent update requires computing optimal value function baseline e.g. regularized value iteration regularized policy iteration since full update expensive trpo provide ways approximate remark algorithm rawlik also viewed instance mirror descent ﬁnite-horizon episodic setting exact update computed eﬃciently dynamic programming. dynamic policy programming. ﬁrst claim iteration single regularized value iteration step starting previous value function extracts greedy policy applies bellman optimality operator follows comparing form presented appendix azar update rules precisely match discounted analogue expressions appendix convergence guarantees proved azar demonstrate soundness approximate update. trust-region policy optimization. second claim iteration trpo single policy iteration step trpo ﬁrst fully evaluates policy compute unregularized value function extracts regularized greedy policy baseline. seen inspecting trpo update takes form objective approximates mirror descent ignoring eﬀect changing policy state distribution. surprisingly using formalism update expressed closed form present detailed derivations appendix particularly interesting consequence result trpo completely equivalent mdp-e algorithm even-dar known minimize regret online setting thus implying trpo also converges optimal policy stationary setting. guarantee much stronger ones provided schulman claim trpo produces monotonically improving sequence policies iterative policy optimization dual averaging next study algorithms arising dual averaging scheme commonly known follow-the-regularized-leader online learning algorithm deﬁned iteration usually increasing sequence ensure convergence limit. unaware pure instance dual averaging using relative entropy discuss conditional entropy below. mirror descent full update requires computing optimal value function various approximations update long studied literature—see e.g. section focus stateof-the-art algorithms mnih o’donoghue originally derived optimization formulation resembling equation main insight algorithm adjusted dynamic-programming interpretation convergence guarantee. natural parameters cause premature convergence local optimum. even serious concern objective function changes iterations gradient descent fail converge stationary point. problem avoided trpo since trpo objectives sensible optimization objective however clear interpretation objective o’donoghue study stationary points objective similarly show connection certain type value function policy achieving stationary point. however show stationary point unique gradient descent converges stationary point. argue above well case. observations consistent asadi littman show softmax policy updates lead inconsistent behavior used tandem unregularized advantage functions. overcome issues advocate directly optimizing objective instead gradient descent. fact convex standard results regarding dual averaging scheme guaranteed converge optimal policy. estimating gradients done analogously unregularized objective lemma experiments section analyze empirically several algorithms described previous section objective illustrating interplay regularization model-estimation error simple reinforcement learning setting. consider iterative setup episode execute policy observe sample transitions update estimated model maximum likelihood. focus regularization aspect approximation error introduced model estimation. important emphasize comparison extend variants algorithms presence sources approximation. consider simple deﬁned grid agent four possible actions succeed probability fail probability case failure agent move goes random adjacent location. negative rewards given hitting wall cases agent sent back starting locations reward diamonds proportional distance starting locations. therefore challenge experiment discover path towards top-right reward learning dynamics incrementally exploit figure left used evaluation. reward walls diamonds. optimal policy indicated arrows. cell colors correspond stationary state distribution open locations. middle average reward function learning rate algorithms number iterations samples iteration results taken random runs value right performance trpo version modiﬁed regularized policy iteration ﬁxed note optimal agent ignores intermediate reward center even prefers wall locations away largest reward number iterations samples iteration analyze average reward ﬁnal policy function compare following algorithms regularized value iteration ﬁxed reference uniform policy ﬁxed several variants approximate mirror descent including trpo dual averaging methods corresponds optimizing objective guaranteed lead optimal policy da-rv corresponds iteration convergence guarantees variants linear annealing schedule fig. shows results function maximum reward depicted blue top. small algorithms perform poorly even reach intermediate reward. contrast large converge prematurely greedy policy exploits intermediate reward. typically intermediate value algorithms occasionally discover optimal path exploit note case regvi never obtains optimal policy. shows using ﬁxed value ﬁxed reference policy choice. observe performance dual averaging methods similar general slightly better approximate mirror descent variants. also show interesting relationship mirror descent approximations. analysis section suggests entire array algorithms lying trpo modiﬁed policy iteration lies value iteration policy iteration fig. illustrates idea showing convergence trpo ﬁxed value trpo tends converge faster locally optimal policy since uses single value update iteration. using value updates leads modiﬁed regularized policy iteration algorithm interpolates trpo. conclusion presented unifying view entropy-regularized mdps convexoptimization perspective. believe unifying theories useful moving ﬁeld forward recall ﬁeld online learning theory convex-optimization view enabled uniﬁed treatment many existing algorithms acts today primary framework deriving algorithms shalev-shwartz hazan paper argued convex-optimization view also useful analyzing algorithms reinforcement learning particular demonstrated framework used provide theoretical justiﬁcation state-of-the-art reinforcement learning algorithms highlight potential problems them. expect newly-found connection also open door constructing advanced reinforcement learning algorithms borrowing ideas convex optimization literature composite objective mirror descent duchi regularized dual averaging finally point work provide statistical justiﬁcation using entropy regularization reinforcement learning. case online learning known markov decision processes changing reward functions entropyregularization known yield near-optimal learning algorithms remains seen technique also provably helps driving exploration process unknown markov decision processes. mohammad gheshlaghi azar vicenç gómez hilbert kappen. dynamic policy programming function approximation. international conference artiﬁcial intelligence statistics pages daniel braun pedro ortega evangelos theodorou stefan schaal. path integral control bounded rationality. adaptive dynamic programming reinforcement learning ieee symposium pages ieee pakman naftali tishby. taming noise reinforcement learning soft updates. proceedings thirty-second conference uncertainty artiﬁcial intelligence sham kakade john langford. approximately optimal approximate reinforcement learning. proceedings nineteenth international conference machine learning pages john laﬀerty andrew mccallum fernando pereira. conditional random ﬁelds probabilistic models segmenting labeling sequence data. icml pages citeseer.ist.psu.edu/laffertyconditional.html. steven marcus emmanual fernández-gaucherand daniel hernández-hernandez stefano coraluppi pedram fard. risk sensitive markov decision processes. systems control twenty-ﬁrst century pages springer martinet. perturbation méthodes d’optimisation. applications. esaim mathematical modelling numerical analysis modélisation mathématique analyse numérique http//eudml.org/doc/. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning pages brendan o’donoghue remi munos koray kavukcuoglu volodymyr mnih. combining policy gradient q-learning. international conference learning representations peters katharina mülling yasemin altun. relative entropy policy search. proceedings twenty-fourth aaai conference artiﬁcial intelligence pages marek petrik dharmashankar subramanian. approximate solution method large risk-averse markov decision processes. proceedings twenty-eighth conference uncertainty artiﬁcial intelligence pages konrad rawlik marc toussaint sethu vijayakumar. stochastic optimal control reinforcement learning approximate inference. proceedings robotics science systems viii john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning pages brian ziebart andrew bagnell anind dey. modeling interaction principle maximum causal entropy. international conference machine learning pages appendix complementary technical results convexity negative conditional entropy consider joint state-action distribution ﬁnite denote µ/νµ study negative conditional entropy function proves bregman divergence corresponding coincides claimed. conclude proof note average relative entropy distributions πµ—that positive terms. indeed shows nonnegative state-action distributions proving convex. derivation optimal policies prove results stated equations proposition give expressions dual optimization problems optimal solutions corresponding primal optimization problem choices regularization function start generic derivations helpful analyzing cases turn studying individual regularizers. begin noting state-action pairs optimal solution also zero thanks form regularized objective. thus without loss generality assume states recurrent holds state-action pairs. ﬁrst-order stationary condition unique optimal solution satisﬁes system equations. obtain ﬁnal solution compute optimal values lagrange multipliers optimizing dual optimization objective insert expression unconstrained dual satisﬁes give derivations regularizer below. precisely dual given equation note dual function associated constraints. expression optimal state-action distribution equation obtained inserting advantage function corresponding optimal value function together equations deﬁne dual optimization problem proposition expression optimal policy equation obtained inserting optimal advantage function norm. state following result claiming requirement indeed holds present simple proof below. note analogous results proven several times literature e.g. continuous result immediately together easily-seen fact implies ﬁxed point brouwer’s ﬁxed-point theorem. furthermore insight allows treat value iteration method instance generalized value iteration deﬁned littman szepesvári non-expansion. similar section assume without loss generality initial reference policy full support i.e. recurrent state action inspecting greedy policy operator easy show induction full support particular equals either equals exponent equals possible unbounded. since full support trajectory always small probability reaching given recurrent state. similar argument bertsekas show regularized value iteration converges ﬁxed point regularized policy gradient prove lemma gives gradient regularized average reward policy parameterized following sutton ﬁrst compute gradient ∂πθ/∂θi state-dependent constant multiplier ∂πθ/∂θi; adding term results given expression. summing sides stationary state distribution yields closed form trpo update derive closed-form solution trpo update. ﬁrst brieﬂy summarize mechanism algorithm. main idea schulman replacing surrogate diﬀerence dtrpo approximate version ignores impact changing policy stationary distribution. given surrogate objective trpo approximately computes distribution", "year": 2017}