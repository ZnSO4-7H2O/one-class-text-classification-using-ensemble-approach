{"title": "Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning  in Hierarchy", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "This work investigates how the traditional image classification pipelines can be extended into a deep architecture, inspired by recent successes of deep neural networks. We propose a deep boosting framework based on layer-by-layer joint feature boosting and dictionary learning. In each layer, we construct a dictionary of filters by combining the filters from the lower layer, and iteratively optimize the image representation with a joint discriminative-generative formulation, i.e. minimization of empirical classification error plus regularization of analysis image generation over training images. For optimization, we perform two iterating steps: i) to minimize the classification error, select the most discriminative features using the gentle adaboost algorithm; ii) according to the feature selection, update the filters to minimize the regularization on analysis image representation using the gradient descent method. Once the optimization is converged, we learn the higher layer representation in the same way. Our model delivers several distinct advantages. First, our layer-wise optimization provides the potential to build very deep architectures. Second, the generated image representation is compact and meaningful. In several visual recognition tasks, our framework outperforms existing state-of-the-art approaches.", "text": "work investigates traditional image classiﬁcation pipelines extended deep architecture inspired recent successes deep neural networks. propose deep boosting framework based layer-by-layer joint feature boosting dictionary learning. layer construct dictionary ﬁlters combining ﬁlters lower layer iteratively optimize image representation joint discriminative-generative formulation i.e. minimization empirical classiﬁcation error plus regularization analysis image generation training images. optimization perform iterating steps minimize classiﬁcation error select discriminative features using gentle adaboost algorithm; according feature selection update ﬁlters minimize regularization analysis image representation using gradient descent method. optimization converged learn higher layer representation way. model delivers several distinct advantages. first layer-wise optimization provides potential build deep architectures. second generated image representation compact meaningful. several visual recognition tasks framework outperforms existing state-of-the-art approaches. visual recognition challenging domains ﬁeld computer vision smart computing. many complex image video understanding systems employ visual recognition basic component analysis. thus design robust visual recognition algorithm becoming fundamental engineering computer vision literature attracting many related researchers. since inadequate visual representation greatly inﬂuence performance visual recognition system almost related methods concentrated developing eﬀective visual representation. traditional visual recognition systems always adopt shallow model construct image/video representation. among them bag-of-visual-words model successful visual content representation widely adopted many computer vision tasks object recognition image classiﬁcation basic pipeline model consists local feature extraction feature encoding pooling operation. order improve performance crucial schemes involved. first traditional model discards spatial information local descriptors seriously limited descriptive power feature representation. overcome problem spatial pyramid matching method proposed capture geometrical relationships among local features. second dictionaries adopted encode local feature traditional methods learned unsupervised manner hardly capture discriminative visual pattern category. issue inspired series works train discriminative dictionaries supervised learning implemented introducing discriminative term dictionary learning phase regularization according various criteria. research going deep models seen type hierarchical representation played signiﬁcant role computer vision machine learning literature recent years. generally hierarchical architecture represents diﬀerent layer vision primitives pixels edges object parts basic principles deep models concentrated folds layerwise learning philosophy whose goal learn single layer model individually stack form ﬁnal architecture; feature combination rules utilizing combination layer detected features construct high layer impressive features introducing activation function. paper related exciting researches inspire explore traditional image classiﬁcation pipelines include feature encoding spatial pyramid representation salient pattern extraction extended deep architecture. paper proposes novel deep boosting framework aims construct eﬀective discriminative features image classiﬁcation task jointly adopting feature boosting dictionary learning. layer followed famous boosting principle proposed method sequentially selects discriminative visual features learn strong classiﬁer minimizing empirical classiﬁcation error. hand analysis dictionary learning strategy involved make selected features suitable object category. twostep learning process investigated iteratively optimize objective function. order construct high-level discriminative representations composite learned ﬁlters corresponding selected features layer feed compositional results next layer build higher-layer analysis dictionary. another approach introducing model compression strategy constructing analysis dictionary reduces complexity feature space shortens model training time. experiment shows method achieves excellent performance general object recognition tasks. fig. illustrates pipeline deep boosting method compared traditional based method analysis operation model encoding process maps image feature space. pooling stage traditional method compute histogram representation adopting spatial pyramid matching. diﬀerent traditional models capturing salient properties visual patterns spatial pooling operation adopt feature boosting discriminative features mining image representation. figure two-layer illustration proposed deep boosting framework. horizontal pipelines show layer-wised image representation joint feature boosting analysis dictionary learning. optimization single layer done compositional ﬁlters higher-layer generate novel analysis dictionary processing. note feature higher-layer dependents training images combined ﬁlters relevant layer. novel deep boosting framework proposed leverages generative discriminative feature representation. presents novel formulation jointly adopting feature boosting analysis dictionary learning image representation. experiment several standard benchmarks shows learned image representation well discovers discriminative features achieves good performance various object recognition tasks. rest paper organized follows. sec. presents brief review related work followed overview background technique details sec. introduce deep boosting framework sec. sec. gives experimental results comparisons. sec. concludes paper. past decades many works done design diﬀerent kinds features express characteristics image visual tasks. hand-craft features vary global expressions local representation designed features roughly divided types geometric features texture features. geometric features explicitly record locations edges employed describe noticeable structures local areas. features include canny edge descriptor gabor-like primitives shape context descriptor contrast texture features express cluttered object appearance histogram statistics. sift gist delegates feature representation. beyond hand-craft feature descriptors bag-of-feature model seems classical image representation method computer vision area. illuminating studies published improve traditional approach diﬀerent aspects. among extensions class sparse coding based methods employ spatial pyramid matching kernel proposed lazebnik achieved great success image classiﬁcation problem. however despite developing eﬀective representation methods lack high-level image expression still plagues build ideal vision system. hand learning hierarchical models simultaneously construct multiple levels visual representation paid much attention recently. proposed hierarchical image representation partially motivated recent developed deep learning approaches diﬀerent previous hand-craft feature design method deep model learns feature representation data validly generates highlevel semantic representation. abstract semantic representations expected provide intra-class variability. recently many vision tasks achieve signiﬁcant improvement using convolutional architectures deep convolutional architecture consists multiple stacked individual layers followed empirical loss layer. among layers convolutional layer feature pooling layer full connection layer play major roles abstract feature representation. stochastic gradient descent algorithm always applied parameters training layers according back-propagation principle. however shown recent study network-based hierarchical models always contain thousands parameters. learning useful network usually depends expertise parameter tuning complex control real visual application. contrast build hierarchical image representation according simple eﬀective rules. method also achieve near optimal classiﬁcation rate layer. another related work paper learning dictionary analysis prior idea analysis-based model utilizing analysis operator deal latent clean signal leading sparse outcome. paper consider analysis-based prior regularization prior learn discriminative features certain category. please refer sec. details analysis dictionary learning. start brief review gentle adaboost algorithm without loss generality considering two-class classiﬁcation problem training samples feature representation sample sample weight related gentle adaboost provides simple additive model form called weak classiﬁer machine learning literature. often deﬁnes regression stump denotes indicator function returns d-th dimension feature vector threshold parameters contributing linear regression function. iteration algorithm learns parameter weighted leastsquares weight dimension feature space. order give much attention cases misclassiﬁed round gentle adaboost adjusts sample weight next iteration wie−yifm updates +fm. last algorithm outputs result strong classiﬁer form sign function sign]. paper adopt gentle adaboost basic component proposed model. please refer technique details. work also inspired recent developed analysis-based sparse representation prior learning represents input signal dual viewpoint commonly used synthesis model main idea analysis prior leaning learn analysis operators return special responses latent signal according given constraint. scalar constant symbol indicates analysis operation. ﬁrst term denotes reconstruction error second denotes sparsity constraint forward transform coeﬃcient. usually redundant dictionary employing analysis operator. diﬀerent context analysis prior frequently adopted enforce regularity signal. paper utilize philosophy analysis-based prior seek discriminative ﬁlters image feature representation. please refer technique details theoretical analysis. considering two-class classiﬁcation problem given training data corresponding label order construct rich discriminative image representation category propose deep boosting framework based compositional feature selection analysis dictionary learning. single layer ﬁrstly introduce term empirical error discriminative features mining. equal learn weak classiﬁer gentle adaboost algorithm. category suppose analysis dictionary denoted rp×m selected feature suitable category analysis transformation feature representation would eﬀective visual recognition. based idea fundamental single layer image representation expressed follows feature representation corresponding image denotes empirical error classiﬁer. indicates positive training means image belong positive samples. deﬁne analysis dictionary indicates linear ﬁlter. thus considered series convolutional operations output feature maps related special linear ﬁlter. properties proposed model folds. hand diﬀerent traditional analysis prior learning adopt empirical error suitable training classiﬁer replace reconstruction error eq.. hand analysis operator introduced regularized term learn discriminative features category. second term desire analysis dictionary large ﬁlter response positive training set. analysis dictionary learning process could discover category coherent features promote discriminative ability weak classiﬁers. equivalent make analysis dictionary small response negative samples thus extract negative training samples minimize objective function train analysis dictionary. note that learned ﬁlter small response positive negative samples related feature representation eliminated iteration feature selection process. discriminative image representation enhanced joint feature boosting analysis dictionary learning leading model feature vector i-th image associated analysis transformation order obtain feature representation employ pyramid-wise histograms quantize ﬁlter responses provide degree translation invariance extracted features handcrafted features learned features average maximum pooling process convolution neural network. suppose total number ﬁlters. construct pyramid-wise histograms special image ﬁrstly activate maximum ﬁlter responses pixel abandon others follows according previous operation obtain feature maps training image locations activated according shown apply three-level spatial pyramid representation resulting feature resulting individual spatial blocks. compute histogram ﬁlter responses block. finally long feature vector formed concatenating histograms blocks feature maps. dimension feature vector note constant scalar paper value could dynamically changed process analysis dictionary learning. please refer sec. details. order optimize objective function propose two-step optimizing strategy integrating feature boosting dictionary learning. subsection describe details feature boosting method setting relationship weak classiﬁer image feature representation. pyramid-wise histogram calculated select discriminative features obtain single layer classiﬁer given feature set. follow previous notation feature representation image dimension feature space described previous content. feature boosting phase gentle adaboost applied discriminative features mining separate positive negative samples nicely round. note rest paper apply round feature boosting procedure algorithm retrieves candidate regression functions formulated associated d-th element function parameter according discussion build bridge weak classiﬁer feature representation thus weak classiﬁers learning viewed feature boosting procedure model. feature boosting usually terminated training error converged. regularization perspective another advantage method introducing analysis dictionary learning conducted selected features feature boosting phase emphasize discriminative ability analysis operator target category. framework since rely discriminative ﬁlters generate higher-layer proper analysis dictionary consider update subset ﬁlters corresponding selected features. ﬁrst need construct relationship feature responses ﬁlters. feature response four-item index recorded isactivited indicates whether feature response selected feature boosting stage. horizontal vertical coordinate image lattice domain respectively. denotes relative ﬁlter deﬁned eq.. apply gradient descent algorithm optimize ﬁlters corresponding selected features. fig. illustrates combine optimized ﬁlters features generate ﬁlters next layer. ﬁlter’s optimization next layer independent previous features. note ﬁrst layers number ﬁlters limited thus almost every ﬁlter taken account optimization. however show sec.. collection compositional ﬁlters becomes large along architecture going deep thus screening mechanism introduced control complexity keep eﬀectiveness model. integrating stages described sec. sec. achieve feature boosting analysis dictionary learning single layer. algorithm summarized alg.. next subsection introduce ﬁlter combination rules construct hierarchical architecture model. context boosting method strong classiﬁer usually weighted linear combination weak classiﬁers hardly decease test error training error approaching zero. based fact interest learn high-level feature representations discriminative ability. order achieve goal propose ﬁlter combination rules output compositional ﬁlters layer treated whole generate analysis dictionary next layer. figure illustration compositional ﬁlters deep boosting. composite ﬁlters pairwise manners layer treat output compositional ﬁlters base ﬁlters next layer. combination similar matrix ﬁlters built drop redundancies sigmoid function. indicate i-th j-th ﬁlters optimized subset illustrated fig. number ﬁlters layer quite diﬀerent adopt optimized ones related selected features construct image ﬁlters next layer. although carefully select ﬁlters combination number compositional ﬁlters still control architecture going deep. assuming exists optimized ﬁlters layer thus obtain maximum number compositional ﬁlters. dimension image layer would make feature space complex training time becomes intolerable. introduce model compression training phase. couple ﬁlers distance calculated measure similarity them. distance smaller threshold maintain ﬁlters similar dropped randomly fig. fig. illustrate similarity matrix ﬁlters diﬀerent layer. intensity every square indicates similar degree ﬁlters. please refer fig. fig. details classiﬁcation accuracy training time comparison without model compression diﬀerent depth proposed framework. according sec. build hierarchical architecture deep boosting framework. testing phase employ weak classiﬁers learned every layer produce ﬁnal classiﬁer. overall proposed method summarized alg. beginning initialize ﬁlters size adopting gabor wavelets. gabor wavelet elements image deﬁned image lattice domain parameters central position belonging lattice domain denote orientation scale parameters. diﬀerent orientation scale parameters makes gabor wavelets variant. simplicity apply scale orientations implementation total ﬁlters ﬁrst layer. notably multi scales promote performance ﬁlter combination process becomes complicated combination allowed scale. followed total number pixels image number orientations. denotes convolution process. image normalize local energy hαsi|/δ deﬁne positive square root normalized result feature response. multiclass situation consider naive one-vs-all scheme train multiple binary classiﬁers learns distinguish samples single class samples remaining classes. given training data i=yi train strong classiﬁers returns classiﬁcation score special test image. testing phase predict label image referring classiﬁer maximum score. reason adopt one-vs-all scheme throughout paper concentrated folds. hand according desire learned analysis dictionary powerful capability distinguish images category. thus select negative samples categories optimize ﬁlters strategy naturally consistent scheme. hand shown many multiclass models oﬀer advantages simple scheme solution classiﬁcation problem. circumstances ﬁnally choose strategy followed intuitive concept. conduct several experiments investigate properties proposed deep boosting framework evaluate performance diﬀerent challenging visual recognition tasks experiments carried core memory. tasks demonstrate superior comparable performances framework state-of-the-art approaches. ﬁrst experiment focus whether algorithm learn select meaningful discriminative features diﬀerent image categories. take cifar- dataset example. cifar- dataset consists color images classes including airplane automobile bird deer frog horse ship truck. randomly select images class training samples learn hierarchical image representation. fig. shows learned templates diﬀerent layers image categories. according visualizations obviously higher layer goes informative features gain. cifar- stl- also ten-category image dataset image size images class. training images test images. training mapped predeﬁned folds. relatively large image size much prior research chose downsample images tab. shows comparison average test accuracies folds stl-. clear method achieve competitive results compared state-of-the-art methods. section interested performance method context analysis dictionary learning. mentioned above analysis operator introduced regularized term learn discriminative features positive samples. desire analysis dictionary able make margin positive negative training sets larger possible. analysis dictionary large response positive training vice versa. note that related feature representation eliminated iteration feature selection process learned ﬁlter responds small value negative positive set. gain discriminative features feature boosting procedure resulting robust compact image representation model. tab. shows classiﬁcation accuracy without regularized term. result using regularized term outperforms standard deviation among folds smaller illustrates feature discriminative model experiment perform classiﬁcation experiments stl- context diﬀerent number layers. learn deep boosting model construct multiple levels visual representation simultaneously. order construct high-level discriminative representations composite learned ﬁlters corresponding selected features layer feed compositional results next layer build higherlayer analysis dictionary. hopefully model goes higher features discriminative. fig. exhibits performance image classiﬁcation stl- different layers. results demonstrate features higher layer conduct better performance. order avoid sudden explosion ﬁlters drop similar ﬁlters randomly pairwise combination learned ﬁlters. although losses accuracy slightly control training time make limitless growth model possible illustrated fig. fig. datasets lhi-animal-faces contains animal human faces similar other. challenging discern evolutional relationship shared parts. besides interesting within-class variation shown face categories including rotation transforms posture variation sub-types. compare result reported obtained methods include feature trained partbased feature trained latent experiment splits dataset training test following method resize images uniform size pixels number layers tab. exhibits classiﬁcation accurracy lhi-animal-faces. shown method achieves increase compared second best competitor. human estimation based facial images plays important role many applications e.g. intelligent advertisement security surveillance monitoring automatic face simulation. best knowledge morph-ii largest publicly available dataset facial estimation. morph-ii dataset facial images individuals labeled images individual. ages vary wide range individuals come diﬀerent races among africans accounted europeans remaining includes hispanic asian races. sample images shown fig. input image locate face bounding detect facial points bounding box. facial points include centers nose mouth corners. align facial image based points. finally images resized size pixels. aligned images shown fig. compare results several existing algorithms designed estimation i.e. iis-lld ages moreover also conduct experiments using feature descriptors usually used face recognition including multi-level sift features estimation treated classiﬁcation problem using multi-class svms. method number layers six-folder cross validation performed. tab. summarizes results based measure. method achieves better results compared state-of-the-art methods estimation. also report results terms cumulative scores diﬀerent error levels fig. exhibiting method outperforms state-of-the-arts almost levels. paper propose novel deep boosting framework applied construct high-level discriminative features general image recognition task. layer feature boosting analysis dictionary learning integrated uniﬁed framework discriminative feature selection learning. order construct high-level image representation combined ﬁlters layer next layer generate novel analysis dictionary. experiments several benchmarks demonstrate eﬀectiveness proposed method achieve good performance various visual recognition tasks. guangdong science technology program guangdong natural science foundation special project integration industry education research guangdong province program guangzhou zhujiang star science technology corresponding authors work liang lin.", "year": 2015}