{"title": "Source-Target Inference Models for Spatial Instruction Understanding", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "abstract": "Models that can execute natural language instructions for situated robotic tasks such as assembly and navigation have several useful applications in homes, offices, and remote scenarios. We study the semantics of spatially-referred configuration and arrangement instructions, based on the challenging Bisk-2016 blank-labeled block dataset. This task involves finding a source block and moving it to the target position (mentioned via a reference block and offset), where the blocks have no names or colors and are just referred to via spatial location features. We present novel models for the subtasks of source block classification and target position regression, based on joint-loss language and spatial-world representation learning, as well as CNN-based and dual attention models to compute the alignment between the world blocks and the instruction phrases. For target position prediction, we compare two inference approaches: annealed sampling via policy gradient versus expectation inference via supervised regression. Our models achieve the new state-of-the-art on this task, with an improvement of 47% on source block accuracy and 22% on target position distance.", "text": "models understand semantics block selection moving instructions topic study since e.g. shrdlu system focus recent block-arrangement instructions dataset bisk yuret marcu important challenging because several reasons. first instructions freeform substantially diverse language vocabulary structure making hard formulaic pattern-based grammar model capture correct semantics. secondly reference source block target position solely based complex spatial-relative information because blocks identical except positions hence involve varying hops inference diverse blocks reference anchors. third supervision target task provided directly ﬁnal target position intermediate reference block offset value. lastly dataset size limited compared diversity complexity. propose novel models conﬁguration-based instruction understanding dataset task using joint-subtaskloss representation learning dual based attention expectation sampling based inference approaches. first source reference offset subtask models models execute natural language instructions situated robotic tasks assembly navigation several useful applications homes ofﬁces remote scenarios. study semantics spatially-referred conﬁguration arrangement instructions based challenging bisk- blank-labeled block dataset. task involves ﬁnding source block moving target position blocks names colors referred spatial location features. present novel models subtasks source block classiﬁcation target position regression based joint-loss language spatial-world representation learning well cnn-based dual attention models compute alignment world blocks instruction phrases. target position prediction compare inference approaches annealed sampling policy gradient versus expectation inference supervised regression. models achieve state-of-the-art task improvement source block accuracy target position distance. task robotic instruction execution involves developing models understand semantics freeform natural language instructions execute sequence actions. models several useful applications domain navigation manipulation assembly scenarios homes ofﬁces warehouses esp. remote settings. paper address task executing assembly-style conﬁguration instructions goal predict spatially-referred source block move target position turn referred terms reference block offset task idealization general assembly problem still involving similar challenges features well requiring solutions extended robotic instruction problems navigation manipulation copyright association advancement artiﬁcial intelligence rights reserved. share sentence block-location representation parameters learn shared spatial-relative semantics across subtasks given limited diverse data. next advanced tasksuited bilinear attention models based dual language-toblock relationships ﬁlters align different parts instruction appropriate spatial-relative features different blocks. finally present inference optimization methods combine reference offset values target prediction expectation positions optimized supervised regression sampling interpretable single reference block optimized policy gradient improvements previous work source block selection accuracy target position mean distance. starting shrdlu system several papers aimed building mapping natural language instructions manipulation assembly style actions objects. overcome constraint using ﬁxed-template instructions several papers presented mapping based induction semantic grammars allows instructions complex human-like also addressing spatial concepts cardinality ordinality referring expressions. addition assembly manipulation style instruction understanding tasks signiﬁcant amount work focused navigational instruction understanding task i.e. mapping translating sequence instructions navigation visual sequence travel-based actions focus end-to-end neural models address joint multi-step task source block prediction moving target position based reference offset explore connections freeform assembly-style conﬁguration instructions actions recent useful dataset created bisk yuret marcu created versions dataset labeled unlabeled. labeled dataset block assigned unique pattern number logo. unlabeled dataset blocks blank easy names numbers hence referred complex spatial-relative features. focus latter challenging spatial-semantics dataset task. source block selection model proposed based softmax classiﬁer built last hidden state instruction lstm-rnn. target position prediction trained using supervised regression. ﬁnal model rnn-based ‘end-to-end’ neural model works well pattern-labeled dataset well blank-labeled dataset propose joint-subtask location-aware alignment-learning models unlabeled task sampling expectation based inference combine reference offset values also learn shared world language attention parameters across source reference offset tasks. related sampling-based loss policy gradient optimization branavan adopt policy gradient based reinforcement learning executing instructions system troubleshooting game tutorials. also recent policy gradient approaches tasks machine translation image captioning using metric-based rewards since losses models non-differentiable policy gradient approach used optimization. recently misra langford artzi extended pattern-labeled version bisk yuret marcu dataset sequential motion planning task based visual simulation input reinforcement learning model. hand focus different setup i.e. original bisk yuret marcu source+target directprediction task dataset; address challenging blank-labeled-blocks version hence relying spatial location-based semantics. figure illustrates overall model. block-moving task involves subproblems source block selection target position prediction ﬁrst classiﬁcation problem latter regression problem source label target position inherently independent ﬁrst present separate models each. however instruction world representations beneﬁt shared knowledge common spatial terms used refer source reference offset hence also propose joint model uses combined loss function source target tasks compute gradients update shared language lstm-rnn world representation parameters. source block selection model source block selection subtask classiﬁcation problem ﬁnite blocks world. input block positions instruction words goal predict block source part instruction refer source block selection model consists phases encoding alignment. encoding phase instruction blocks encoded respective embedding representations. alignment phase attention module used measure matching instruction block emfigure overall model assembly instruction understanding task showing instruction world representation learning language-to-block alignment modules source target loss functions. input feature representation block consisting coordinates relative distance stack based features discussed below; block weights bias parameters. block features block represented absolute coordinate features alone aware surrounding blocks relative position board important information understanding spatial instructions esp. given blocks ‘blank’ given limited size dataset. hence addition original coordinates employ simple kinds relative-position features euclidean distance corner edge board single binary feature indicating whether block part stack. given encoded vectors instruction block next attention module predict probability block answer source block output attention module measures alignment matching block’s embedding instruction’s embedding thus probability block source block softmax attention value block instruction sentence. source loss function cross-entropy conditional distribution ground truth distribution filters instead using last lstmrnn hidden vector represent instruction embedding concatenation ﬁlters different kernel sizes following idea sentiment analysis different convolutions hidden vectors lstm-rnn compute outputs cnn. pooling layer followed reduce output sequence vectors single vector hcnn used bilinear attention form above. ﬁlters help capture local patterns hence allow lstm focus structure sentence. dual attention third approach develop novel two-step attention process word-to-block attention block-to-instruction attention. first block word-to-block attention part computes alignment score instruction word block inference sampling expectation given distribution above target position’s random variable strategies infer target position sampling expectation. inference sampling strategy allows choose speciﬁc single block reference reference block offset sampled following distributions summed sample compatible different inference strategies types losses sampling loss expectation loss. sampling loss expectation distance ground truth target random variable policy gradient minimize non-differentiable sampling loss following previous work reward-based reinforcement learning. expectation loss fully differentiable supervised regression method used. next describe optimization losses detail. finally second stage block-to-instruction attention part computes block’s alignment score full instruction alignment score overall context vector block embedding dual approach hence allows model learn part sentence refers block block-conditioned context vector instruction then second stage block compared context vector instead global target position prediction divide target position task task ﬁnding reference block offset e.g. move bottom block left rightmost block. here rightblock reference block left offset. next describe model reference offset random variables training methods ﬁnally combination predict target position reference offset distributions model random variable assign block probability answer reference block. similar source block selection probability block reference block softmax attention value block instruction sentence separate attention module independent context parameters shared block-instruction bilinear matrices reinforce algorithm also used previous image captioning classiﬁcation work negative distance ground truth target target prediction viewed reward. annealing method solve instability oneblock sampling method slowly anneal expectation loss lexp sampling loss lsmp sample-averaging intermediate loss build intermediate loss sample pairs following distribution random variables average pairs target prediction loss distance ground truth prediction. motivation expectation loss lexp sampling loss lsmp limits intermediate loss therefore starting expectation loss anneal sampling loss slowly decreasing sample size expectation loss loss used expectation loss lexp whole dataset. similar mean squared error commonly used regression. hence supervised regression problem end-to-end fully differentiable simply optimize variant stochastic gradient descent although source target subproblems represent independent tasks still share language instruction spatial features world blocks {bi}. instance source reference block referred terms ‘leftmost’ ‘top’ etc. offset also uses spatial-directional terms. hence also propose joint model learns shared embeddings words lstm-rnn blocks. further bilinear block-instruction attention matrices also shared across source reference tasks. optimize source loss target loss compute gradient joint loss learn shared parameters. dataset employ challenging blank-labeled dataset introduced bisk yuret marcu datum includes natural language instruction positions blocks world answer source block index plus coordinates ﬁnal target position better exponential-moving-average selfcritical baselines. supervision exists intermediate reference offset values). collect instructions show automaticallyrendered images source target choice mturkers give unconstrained free-form instructions describe given movement without allowed refer name pattern color blocks. dataset contains instructions image pairs standard training/dev/test splits bisk yuret marcu hyperparameter tuning. metrics source metrics primary metric source block classiﬁcation accuracy correct prediction full blocks intuitively selecting nearby wrong block causes full conﬁguration task fail matter close wrong block correct source block. following previous work also report mean median distance errors predicted block answer block coordinates across dataset distance errors computed terms block lengths. target metrics similar source case above report mean median distance errors predicted ground truth target position coordinates training details sentence encoder lstm-rnn dimensional hidden vectors word embeddings. block embedding layer -dimensional fully-connected layer. generalized xavier initialization keep variance feature constant across layers stabilizes training process. adam optimizer used update parameters learning rate ﬁxed gradient clipping applied lstm parameters avoid exploding gradients. annealmentbased sampling approach start expectation loss sample anneal speed training process initial annealing decay step reduced ﬁnally ﬁnal sequence block samples regularization regularize network weight decay trainable variables dropout layer probability added lstm layer. data noising stabilize training limited-size dataset types noise local noise global noise. local noise gaussian standard deviation added block independently. global noise shift board adds gaussian standard deviation coordinate system. global noise continuous target position variable. empirically found small amount added gaussian noise position features last hidden state attention dual attention annealed sampling loss non-joint training source non-joint training target full model expectation model ensemble sampling model ensemble table validation results show ablations model components. full expectation-based model uses features attention joint training. ablation shows results changing component time full model. finally last rows represent ﬁnal -ensemble versions full expectation model well sampling model. pretrained language embeddings also tried initializing instruction’s lstm-rnn pretrained glove word embeddings signiﬁcant improvements likely embeddings trained based unsupervised word context able differentiate identical-context spatial terms like left right task. hence allow embeddings trained scratch directly task supervision. ablation results ﬁrst discuss ablation results i.e. effect various model components based validation results table shows four major model component choices discussed section block-world spatial features three attention modules sampling expectation target loss joint non-joint training embedding representations. table full expectation-based model represents model uses features attention joint training. ablation third-last shows results changing component time full model. finally last rows table -sized ensemble full expectation model; setting used ﬁnal test results table feature selection show impact different representations world blocks compare results using coordinate values novel relative stack-based features shown table utilizing features gives decent improvements bilinear attention modules source block reference block selection model distributions three different bilinear attention modules bilinear matching last hidden state instruction block ﬁlters lstm-rnn vectors dual word-to-block block-toinstruction attention. comparison among three attention modules shown table models using ﬁlters dual attention outperform last hidden state source target tasks. ﬁlter attention model slightly better dual attention model hence ﬁnal full model. note dual attention model similar attention model performance. sec. also discuss complementary nature dual attention models report improved combination results. target training methods shown table model target prediction trained types inference methods optimization loss functions. using expectation loss gives slightly better performance using sampling loss likely losses quite different inference procedures. sampling inference explicitly chooses block reference block expectation inference calculates reference expected several blocks inference choices advantages disadvantages hence report results models. joint training fourth part table compares joint training non-joint training world-block language representations across source target tasks non-joint training results source target tasks worse joint training results showing advantage learning shared spatial world language representations across source target figure analysis positive negative output examples showing interesting instruction scenarios. ﬁrst second image pair depict ground truth movement source block target position. report predicted source accuracy target distance bottom-right second image. also cross represent predicted target position also cases model predicted incorrect source represent wrongly-predicted source block circle. final test results next table present test-set results inference approaches using ﬁnal model choices based ablation studies without ensemble. inference models achieve strong improvements previous best work dataset bisk yuret marcu employ three neural models task. compare ﬁnal best model rnnbased ‘end-to-end’ neural model model achieves improvement source task accuracy reduction target distance mean. moreover results quite stable inference models standard deviation based runs around source accuracy block length target mean. complementarity attention models found attention models complementary nature achieving source accuracy combining ensemble models dual attention i.e. improvement model’s table experiments figure shows several positive negative examples output full model. correctly understand semantics complex source target descriptions ‘bottom right slightly right center’ ‘place stack blocks furthest back’. negative examples show complex cases model cannot handle correctly mostly special scenarios phrases hasn’t seen diverse small dataset. examples include instructions mentioning shape-based block patterns ‘backwards ‘tetris structure’ complex count-based patterns ‘-piece-long line’. presented sampling expectation based models source target prediction conﬁgurational robotic instructions models also spatial-relative features dual attention models joint-subtask-loss training world language representations achieving substantial improvements previous work metrics. zettlemoyer collins learning sentences logical form structured classiﬁcation proceedings probabilistic categorial grammars. twenty-first conference uncertainty artiﬁcial intelligence auai press. thank anonymous reviewers helpful comments. work partially supported google faculty research award bloomberg data science research grant faculty award nvidia awards.", "year": 2017}