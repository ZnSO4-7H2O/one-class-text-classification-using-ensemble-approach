{"title": "Knowledge Matters: Importance of Prior Information for Optimization", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.", "text": "explore eﬀect introducing prior information intermediate level deep supervised neural networks learning task black-box state-of-the-art machine learning algorithms tested failed learn. motivate work hypothesis optimization obstacle involved nature tasks humans learn useful intermediate concepts individuals form supervision guidance using curriculum. experiments conducted provide positive evidence favor hypothesis. experiments two-tiered architecture trained dataset image input contains three sprites binary target class three shape. black-box machine learning algorithms chance task. standard deep supervised neural networks also failed. however using particular structure guiding learner providing intermediate targets form intermediate concepts allows nail task. much better chance imperfect results also obtained exploring architecture optimization variants pointing towards diﬃcult optimization task. hypothesize learning diﬃculty composition highly non-linear tasks. ﬁndings also consistent hypotheses cultural learning inspired observations eﬀective local minima recent emerging interest diﬀerent ﬁelds science cultural learning groups individuals exchanging information learn ways superior individual learning. also witnessed emergence research ﬁelds social neuroscience. learning agents environment means cultural transmission knowledge peer-to-peer communication eﬃcient natural acquiring propagating common knowledge. popular belief information transmitted individuals bits information transmitted small units called memes share characteristics genes self-replication mutation response selective pressures paper based hypothesis human culture evolution ideas crucial counter optimization issue diﬃculty would otherwise make diﬃcult human brains capture high level knowledge world without help educated humans. paper machine learning experiments used investigate elements hypothesis seeking answers following questions machine learning tasks intrinsically hard lone learning agent become easy intermediate concepts provided another agent additional intermediate learning cues spirit curriculum learning makes learning tasks diﬃcult? speciﬁc initial values neural network parameters yield success random initialization yield complete failure? possible verify problem faced optimization problem regularization problem? questions discussed here relate following broader question humans learn complex concepts? pentomino tetris sprites task ﬁgure sprites image diﬀerent sprite shapes image. several state-of-the-art machine learning algorithms tested none could perform better random predictor test set. nevertheless providing hints intermediate concepts problem easily solved same-architecture neural network without intermediate concepts guidance fails. surprisingly attempts solving problem unsupervised pre-training algorithms failed solve problem. however speciﬁc variations network architecture training procedure found make dent problem. showing impact intermediate level guidance experimented two-tiered neural network supervised pre-training ﬁrst part recognize category sprites independently orientation scale diﬀerent locations second part learns output ﬁrst part predicts binary task interest. objective paper propose novel learning algorithm architecture rather reﬁne understanding learning diﬃculties involved composed tasks particular training diﬃculties involved deep neural networks. results also bring empirical evidence favor hypotheses bengio discussed below well introducing particular form curriculum learning building diﬃcult problems long history computer science. speciﬁcally hard problems studied create captcha’s easy solve humans hard solve machines paper investigating diﬃcult problem oﬀ-the-shelf black-box machine learning algorithms. unable descent path paper hypothesized abstract learning tasks obtained composing simpler tasks likely yield eﬀective local minima neural networks generally hard general-purpose machine learning algorithms. idea learning enhanced guiding learner intermediate easier tasks starting animal training shaping bengio introduce computational hypothesis related presumed issue eﬀective local minima directly learning target task good solutions correspond hard-to-ﬁnd-by-chance eﬀective local minima intermediate tasks prepare learner’s internal conﬁguration similar continuation methods global optimization point much easier train neural network supervision expect unsupervised learning discover concept poor results obtained unsupervised pre-training reinforce hypothesis. point directly training layers deep network together makes diﬃcult exploit extra modeling power deeper architecture many cases actually yields worse results number required layers increased experiments performed also reinforce hypothesis. point erhan observed training trajectories ended eﬀective local minimum hundreds runs even comparing solutions functions input output rather parameter space suggests number diﬀerent eﬀective local minima must huge. point unsupervised pre-training changes initial conditions descent procedure sometimes allows reach substantially better eﬀective local minima better local minima appear reachable chance alone experiments performed provide another piece evidence favor hypothesis random initialization yield rather poor results speciﬁcally targeted initialization drastic impact i.e. recombination memes constitute eﬃcient evolutionary recombination operator meme-space. helps human learners collectively build better internal representations environment including fairly high-level abstractions. paper focused point testing guided learning hypothesis using machine learning algorithms provide experimental evidence. experiments performed also provide evidence favor deeper harder hypothesis associated abstractions harder hypothesis. machine learning still beyond current capabilities humans important tackle remaining obstacles approach purpose question answered tasks humans learn eﬀortlessly examples machine learning algorithms fail miserably? recent work showed rather deep feedforward networks successfully trained large quantities labeled data available nonetheless experiments reported suggest depends task considered since even large quantities labeled examples deep networks trained unsuccessful. hypothesized local descent hypothesis human brains would rely local approximate descent like multi-layer perceptron trained gradient-based iterative optimization. main argument favor hypothesis relies biologically-grounded assumption although ﬁring patterns brain change rapidly synaptic strengths underlying neural activities change gradually making sure behaviors generally consistent across time. learning algorithm based form local descent sensitive eﬀective local minima trains neural network point training phase evaluation error seems saturate even examples introduced. particular erhan early examples much larger weight ﬁnal solution. looks like learner stuck near local minimum. since diﬃcult verify near true local minimum simply eﬀect strong ill-conditioning call stuck conﬁguration eﬀective local minimum whose deﬁnition depends optimization objective also limitations optimization algorithm. erhan highlighted issue eﬀective local minima regularization eﬀect initializing deep network unsupervised pre-training. interestingly network gets deeper diﬃculty eﬀective local minima seems pronounced. might number eﬀective local minima increases maybe good ones harder reach work needed clarify question. result point hypothesize diﬃcult individual’s brain discover higher level abstractions chance only. mentioned guided learning hypothesis humans hints humans learn high-level concepts guidance humans. curriculum learning incremental learning examples this. done properly choosing sequence examples seen learner simpler examples introduced ﬁrst complex examples shown learner ready them. hypothesis curriculum works states curriculum learning acts continuation method allows discover good minimum ﬁrst ﬁnding good minimum smoother error function. recent experiments human subjects also indicates humans teach using curriculum strategy parts human brain known hierarchical organization consistent deep architecture studied machine learning papers. sensory level higher levels visual cortex higher level areas corresponding abstract concepts. consistent deep abstractions hypothesis. training neural networks machine learning algorithms decomposing learning task sub-tasks exploiting prior information task well-established fact constitutes main approach solving industrial problems machine learning. contribution paper rather rendering explicit eﬀective local minima issue providing evidence type problems diﬃculty arises. prior information hints given learner viewed inductive bias particular task important ingredient obtain good generalization error interesting earlier ﬁnding line research done explanation based neural networks neural network transfers knowledge across multiple learning tasks. ebnn uses previously learned domain knowledge initialization search bias another related work machine learning mainly focused reinforcement learning algorithms based incorporating prior knowledge terms logical rules learning algorithm prior knowledge speed bias learning discussed memes divide conquer hypothesis societies viewed distributed computational processing systems. civilized societies knowledge distributed across diﬀerent individuals yields space eﬃciency. moreover computation i.e. individual specialize particular task/topic also divided across individuals society hence yield computational eﬃciency. considering limitations human brain whole processing done single agent eﬃcient manner. recent study paleoantropology states substantial decline endocranial volume brain last years henneberg volume brain shrunk hypothesis reduction volume skull claims that decline volume brain might related functional changes brain arose result cultural development emergence societies given time period overlaps transition hunter-gatherer lifestyle agricultural societies. tasks seem reasonably easy humans learn nonetheless appearing almost impossible learn current generic state-of-art machine learning algorithms. study closely task becomes learnable provides hints learner appropriate intermediate concepts. interestingly task used experiments hard deep neural networks also non-parametric machine learning algorithms svm’s boosting decision trees. result experiments varying size dataset several oﬀ-the-shelf black machine learning algorithms popular deep learning algorithms provided table detailed explanations algorithms hyperparameters used algorithms given appendix section also provide explanations methodologies conducted experiments section order test hypothesis artiﬁcial dataset object recognition using binary images designed. task tiered task ﬁrst part recognize locate pentomino object class image. second keeping mind humans exploit prior knowledge either previous learning innate knowledge. source code script generates artiﬁcial pentomino datasets available https//github.com/caglar/arcade-universe. implementation based olivier breuleux’s bugland dataset generator. human learner seem need taught shape categories pentomino sprite order solve task. hand humans lots previously learned knowledge notion shape central deﬁning categories. part/ﬁnal binary classiﬁcation task ﬁgure pentominos image shape class not. neural network learned detect categories object location image remaining task becomes xor-like operation detected object categories. types pentomino objects used generating dataset follows shown figures synthesized images fairly simple texture. foreground pixels background pixels images training test sets generated iid. notational convenience assume domain input images sprites intermediate object categories selected without replacement. using uniform probability distribution possible scales scale chosen accordingly sprite image scaled. rotate sprite randomly rotated multiple degrees. sprite placement upon completion sprite transformations uniform grid generated divided blocks block size pixels randomly select three diﬀerent blocks grid place transformed initially models cross-validated using -fold cross-validation. examples gives examples training examples testing. neural network algorithms stochastic gradient descent used training. following standard learning algorithms ﬁrst evaluated decision trees svms gaussian kernel ordinary fully-connected multi-layer perceptrons random forests k-nearest neighbors convolutional neural networks stacked denoising auto-encoders supervised ﬁne-tuning. details conﬁgurations hyper-parameters given appendix section better chance results obtained variations structured multi-layer perceptron described below. lower part shared weights local connectivity identical instance patch image typically -element output vector patch idea outputs patch could represent detection sprite shape category upper part fully connected hidden layer takes concatenation outputs patch-wise pnns input. note ﬁrst layer similar convolutional layer stride equals kernel size windows overlap i.e. decomposed separate networks sharing parameters applied diﬀerent patches input image network actually trained patch-wise case target provided outputs. output patch extracted image computed follows input patch/receptive ﬁeld extracted location single image. rdh×d weight matrix ﬁrst layer vector biases ﬁrst layer pnn. activation function ﬁrst layer rectifying non-linearity nair hinton glorot krizhevsky rdh×do second layer’s weights matrix biases second layer overcomplete representation input patch potentially represent possible pentomino shapes factors variations patch hand trained hints expected lower dimensional representation pentomino shape category invariant scaling rotation given patch. pentomino shape not. smlp without hints outputs local activations patch gradients backpropagated upper layers. cases produces input representation part neural thus input representation concatenated output across patch locations makes mean standard deviation computed hidden unit hidden unit activation unit standard deviation average minibatch. pentomino images minibatch rdin×n matrix images. vector activations i-th hidden unit hidden layer j-th example concatenated output input pnn. feedforward sigmoid output layer using single relu hidden layer. task perform nonlinear logical operation representation provided output pnn. smlp-hints architecture exploits hint presence category pentomino objects specifying semantics outputs. trained intermediate target specifying type pentomino sprite shape present patches image. possible answer given location none object types i.e. empty patch take possible values rejection rest pentomino shape classes illustrated figure smlp-hints architecture takes advantage dividing task subtasks training prior information intermediate-level relevant factors. training losses decomposes loss patch pre-trained patchstandardization crucial step training smlp pentomino dataset yields much sparser outputs seen figures standardization used even smlp-hints could solve pentomino task. general standardization step dampens small activations augments larger ones. centering activations feature detector neural network studied proposed transforming outputs hidden neuron multi-layer perceptron network zero output zero slope average makes ﬁrst order optimization methods closer second order techniques. default smlp uses rectiﬁer hidden units activation function found signiﬁcant boost using rectiﬁcation compared hyperbolic tangent sigmoid activation functions. highly overcomplete architecture hidden units patch weight decay regularization coeﬃcients weights respectively learning rate training epoch enough learn features pentomino shapes perfectly training examples. hidden units. penalty coeﬃcients learning rate selected trial error based validation error. fully-connected neural networks even though globally special kind convolutional neural network. filters ﬁrst layer smlp shown figure examples ﬁlters obtained slmp-hints trained examples whose results given table ﬁlters look noisy work perfectly pentomino task. smlp-nohints uses connectivity pattern also used smlp-hints architecture without using intermediate targets directly predicts ﬁnal outcome task using number hidden units figure softmax output activations smlp-hints standardization. positive spiked outputs locations pentomino shape. positive negative spikes arise outputs near average value. activations higher locations pentomino shape. connectivity activation function hidden units smlp-hints. hyperparameter values evaluated randomly selecting number hidden units randomly sampling learning rates uniformly log-domain within interval fully connected hidden layers hidden units patch used last hidden layer twenty training epochs. network best results obtained learning rate chose experiment various smlp-nohint architectures optimization procedures trying unsuccessfully achieve good results smlp-nohint smlp-hints. rectiﬁer non-linearity rectiﬁer nonlinearity used activations hidden layers. observed using piecewise linear nonlinearity activation function rectiﬁer make optimization tractable. intermediate layer output considered intermediate layer smlp. smlp-hints softmax output activations tried intermediate layer suﬃced learn task. since things work nearly well standardization layer normalization last layer convolutional neural networks used occasionaly encourage competition hidden units. used local contrast normalization layer architecture performs subtractive divisive normalization. local contrast normalization layer enforces local competition adjacent features feature features spatial location diﬀerent feature maps. similarly observed using local response layer enjoys beneﬁt using local normalization scheme aids generalization. standardization observed crucial smlp trained withhints. smlp-hints smlp-nohints experiments neural network able generalize even learn training without using standardization smlp intermediate layer chance performance. speciﬁcally smlp-nohints architecture standardization part computational graph hence gradients backpropagated mean standard deviation computed hidden unit separately intermediate layer equation order prevent numerical beneﬁt sparse activations speciﬁcally important ill-conditioned problems following reasons. hidden unit gradient usually close well seen here. means oﬀ-diagonal second derivatives involving hidden unit also near basically like removing columns rows hessian matrix associated particular example. observed condition number hessian matrix increases size network increases making training considerably slower ineﬃcient hence would expect sparsity gradients increases training would become eﬃcient training smaller sub-network example shared weights across examples dropouts figure activation histogram smlp-nohints intermediate layer showing distribution activation values standardization. sparsifying eﬀect standardization apparent. figures intermediate level activations smlp-nohints shown standardization. smlp-nohints architecture whose results presented table smlp adadelta adaptive learning rate scheme used hidden units hidden layer rectiﬁer activation function. output sigmoidal units used hidden units rectiﬁer activation function. output nonlinearity sigmoid training objective binary crossentropy. adaptive learning rates experimented several diﬀerent adaptive learning rate algorithms. tried rmsprop adadelta adagrad linearly decaying learning rate smlp-nohints sigmoid activation function found adadelta converging faster eﬀective local minima usually yielding better generalization error compared others. several experiments conducted using architecture similar smlp-nohints using unsupervised pre-training denoising auto-encoder and/or contractive auto-encoders supervised ﬁne-tuning proceeds deep structured without hints. unsupervised learner focus representation shapes larger number intermediate-level units output explored previous work unsupervised pre-training generally found larger hidden layers optimal using unsupervised pre-training unsupervised features relevant task hand. instead limiting units patch experimented networks hidden units patch second-layer patch-wise auto-encoder. sigmoid nonlinearity binary cross entropy cost function denoising auto-encoder used. second layer experiments performed rectiﬁer hidden units utilizing sparsity weight decay weights auto-encoder. greedy layerwise unsupervised training procedure used train deep auto-encoder architecture unsupervised pretraining experiments tied weights used. diﬀerent combinations unsupervised pretraining tested none conﬁgurations tested managed learn pentomino task shown table explore eﬀect changing complexity input representation diﬃculty task experiments designed symbolic representations information patch. cases empty patch represented vector. representation seen alternative input pnn-like network i.e. input another black-box classiﬁer. learning rate scaling method discussed hinton video lecture rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning nearest neighbors decision tree randomized trees convnet/lenet maxout convnet layer struct. supervised hints struct. mlp+cae supervised finetuning struct. mlp+cae+dae supervised finetuning struct. mlp+dae+dae supervised finetuning experiment -disentangled representations experiment trials binary inputs patch one-hot bits representing object category rotations scaling i.e. whole information input given perfectly disentangled. would ideal input unsupervised perfectly job. information input image patch spread kind non-parametric non-informative like perfect memory-based unsupervised learner could produce. nevertheless shape class would easier read representation image representation experiment -onehot representation choices representation one-hot representation patch target task deﬁned diﬀerently. objects diﬀerent patches considered exactly -bit onehot representation figure tanh training curves. left training test errors experiment training epochs training examples using tanh mlp. right training test errors experiment training epochs training examples using tanh mlp. results experiment given table improve results experimented maxout non-linearity feedforward hidden layers. unlike typical maxout network mentioned original paper regularizers deliberately avoided order focus optimization issue weight decay norm constraint weights dropout. although learning disentangled representation diﬃcult learning perfect object detectors feasible architectures maxout network. note representation kind representation could hope unsupervised learning algorithm could discover best argued bengio results obtained validation experiment experiment shown respectively table table experiments tanh hidden layers tested hyperparameters. experiment complexity experiment source complexity task comes number diﬀerent object types. results complete failure complete success observed experiments suggesting task could become solvable better training training examples. figure illustrates progress training tanh training test error experiments clearly something learned task nailed yet. experiment maxout tanh maxout long plateau training error objective stays almost same. maxout chance experiment iterations training test set. iteration training test error started decline eventually able solve task. moreover seen curves figure training test error curves almost tasks. implies onehot inputs whether increase number possible transformations object number results shown section indicate problem pentomino task clearly regularization problem rather basically hinges optimization problem. otherwise would expect test error decrease number training examples increases. shown ﬁrst studying online case studying ordinary training case ﬁxed size training considering increasing training sizes. online minibatch setting parameter updates performed follows estimates online cause parameter ﬂuctuate near local optima. however online directly optimizes expected risk examples drawn ground-truth distribution thus would like know problem pentomino dataset regularization optimization problem. smlp-nohints model trained online randomly generated online pentomino stream. learning rate adaptive adadelta procedure minibatches examples. online experiments smlp-nohints trained without standardization intermediate layer exactly hyperparameters tested. smlp-nohints patch-wise submodel smlp-nohints trained either without standardization output units pnn. experiments illustrated figures smlp without hints architecture results given table graphs results training randomly generated pentomino samples presented. shown plots smlp-nohints able generalize without standardization. although without standardization training loss seems decrease initially eventually gets stuck plateau training loss doesn’t change much. training smlp-nohints online minibatch performed using standardization intermediate layer adadelta learning rate adaptation training examples randomly generated pentomino stream. training test error much better chance score obtained smlp-hints near error. another smlp-nohints experiment without standardization model trained pentomino examples using online minibatch sgd. hidden units sigmoidal outputs patch. hidden layer. hidden units hidden layer. adadelta used adapt learning rate. training smlp test error remained stuck figure test errors smlp-nohints without standardization intermediate layer. sigmoid intermediate layer activation used. tick x-axis represents examples. figure training errors smlp-nohints without standardization intermediate layer. sigmoid nonlinearity used intermediate layer activation function. x-axis units blocks examples training set. consider eﬀect training diﬀerent learners diﬀerent numbers training examples. experimental results shown table training sizes used. dataset generated diﬀerent random seeds figure also shows error bars ordinary three hidden layers larger range training sizes examples. number training epochs three hidden layers feature detectors. learning rate used experiments activation function tanh nonlinearity penalty coeﬃcients table shows that without guiding hints none state-of-art learning algorithms could perform noticeably better random predictor test set. shows importance intermediate hints introduced smlp. decision trees svms overﬁt training could generalize test set. note numbers reported table hyper-parameters selected based validation error hence lower training errors possible avoiding regularization taking large enough models. training large hidden layers could reach nearly training error still manage achieve good test error. initialization parameters neural network impact learning generalization previously erhan showed initializing parameters neural network unsupervised pretraining guides learning towards basins attraction local minima provides better generalization training dataset. section analyze eﬀect initializing smlp hints continuing without hints rest training. experimental analysis hints based initialization smlp trained training epoch using hints epochs trained without hints examples training set. also compared architecture hyperparameters smlp-nohints trained iterations dataset. iteration hint-based training smlp obtained training error test error. following hint based training smlp trained without hints epochs epoch already training test error. hyperparameters experiment experiment results shown smlp-hints table same. test results initialization without hints shown figure ﬁgure suggests initializing hints give generalization performance training takes longer. test error pentomino training dataset. used hidden units hidden layer softmax output patch. used hidden units sigmoid learning rate without using adaptive learning rate method. smlp uses rectiﬁer nonlinearity hidden layers pnn. considering architecture uses softmax intermediate activation function smlp-nohints. likely trying learn presence speciﬁc pentomino shape given patch. architecture large capacity probably provides enough capacity learn presence pentomino shapes patch eﬀortlessly. hidden layers rectiﬁer units trained using lbfgs training examples gradients computed batches examples iteration. however convergence training still chance test dataset. also observed using linear units intermediate layer yields better generalization error without standardization compared using activation functions sigmoid tanh relu intermediate layer. smlp-nohints able generalization error linear units without standardization whereas activation functions tested failed generalize number training iterations without standardization hints. suggests using non-linear intermediate-level activation functions without standardization introduces optimization diﬃculty smlp-nohints maybe intermediate level acts like bottleneck architecture. paper shown example task seems almost impossible solve standard black-box machine learning algorithms almost perfectly solved encourages semantics intermediate-level representation guided prior knowledge. task particularity deﬁned composition nonlinear sub-tasks interesting case neural network compare networks exactly architecture diﬀerent pre-training uses known intermediate concepts teach intermediate representation network. enough capacity training time overﬁt capture essence task seen test performance. know structured deep network learn task initialized right place training examples. furthermore shown pre-trains smlp hints epoch nail task. exactly architecture started training random initialization failed generalize. consider fact even smlp-nohints standardization trained using online generated examples still gets test error. indication problem regularization problem possibly inability good eﬀective local minima generalization error. hypothesize initializations architectures although possible good eﬀective local minimum training error enough capacity provided diﬃcult good local minimum generalization error. hand network architecture constrained enough still allows represent good solution seems optimization problem still diﬃcult even training error remains stuck high standardization isn’t used. standardization obviously makes training objective smlp easier optimize helps least better eﬀective local minimum training error. ﬁnding suggests using speciﬁc architectural constraints sometimes domain speciﬁc knowledge problem alleviate optimization diﬃculty generic neural network architectures face. could combination network architecture training procedure produces training dynamics tends yield minima poor point view generalization error even manage nail training error providing enough capacity. course number examples increases would expect discrepancy decrease optimization problem could still make task unfeasible practice. note however preliminary experiments increasing training size mlps reveal signs potential improvements test error shown figure even using online training pentomino examples smlp-nohints architecture still perfect terms generalization error ﬁndings bring supporting evidence guided learning hypothesis deeper harder hypothesis bengio higher level abstractions expressed composing simpler concepts diﬃcult learn diﬃculty overcome another agent provides hints importance learning other intermediate-level abstractions relevant task. many interesting questions remain open. would network without guiding hint eventually solution enough training time and/or alternate parametrizations? extent ill-conditioning core issue? results lbfgs disappointing changes architectures seem make training much easier. clearly reach good solutions appropriate initialization pointing direction issue local minima good solutions also reachable initializations albeit going tortuous ill-conditioned path parameter space. attempts learning intermediate concepts unsupervised fail? results speciﬁc task testing limitation unsupervised feature learning algorithm tested? trying many unsupervised variants exploring explanatory hypotheses observed failures could help answer that. finally ambitious solve kinds problems allow community learners collaborate collectively discover combine partial solutions order obtain solutions abstract tasks like presented here? indeed would like discover learning algorithms solve tasks without prior knowledge speciﬁc strong used smlp here. experiments could inspired inform potential mechanisms collective learning cultural evolutions human societies.", "year": 2013}