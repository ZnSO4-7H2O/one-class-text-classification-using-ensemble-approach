{"title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.", "text": "determining trust individual predictions important problem model used decision making. using machine learning medical diagnosis terrorism detection example predictions cannot acted upon blind faith consequences catastrophic. apart trusting individual predictions also need evaluate model whole deploying wild. make decision users need conﬁdent model perform well real-world data according metrics interest. currently models evaluated using accuracy metrics available validation dataset. however real-world data often signiﬁcantly diﬀerent further evaluation metric indicative product’s goal. inspecting individual predictions explanations worthwhile solution addition metrics. case important users suggesting instances inspect especially large datasets. paper propose providing explanations individual predictions solution trusting prediction problem selecting multiple predictions solution trusting model problem. main contributions summarized follows. jects measure impact explanations trust associated tasks. experiments non-experts using lime able pick classiﬁer pair generalizes better real world. further able greatly improve untrustworthy classiﬁer trained newsgroups feature engineering using lime. also show understanding predictions neural network images helps practitioners know trust model. explaining prediction mean presenting textual visual artifacts provide qualitative understanding relationship instance’s components model’s prediction. argue explaining predictions important aspect despite widespread adoption machine learning models remain mostly black boxes. understanding reasons behind predictions however quite important assessing trust fundamental plans take action based prediction choosing whether deploy model. understanding also provides insights model used transform untrustworthy model prediction trustworthy one. work propose lime novel explanation technique explains predictions classiﬁer interpretable faithful manner learning interpretable model locally around prediction. also propose method explain models presenting representative individual predictions explanations non-redundant framing task submodular optimization problem. demonstrate ﬂexibility methods explaining diﬀerent models text image classiﬁcation show utility explanations novel experiments simulated human subjects various scenarios require trust deciding trust prediction choosing models improving untrustworthy classiﬁer identifying classiﬁer trusted. machine learning core many recent advances science technology. unfortunately important role humans oft-overlooked aspect ﬁeld. whether humans directly using machine learning classiﬁers tools deploying models within products vital concern remains users trust model prediction important diﬀerentiate diﬀerent deﬁnitions trust trusting prediction i.e. whether user trusts individual prediction suﬃciently take action based trusting model i.e. whether user trusts model behave reasonable ways deployed. directly impacted permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others author must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. francisco copyright held owner/author. publication rights licensed acm. isbn ----//. http//dx.doi.org/./. figure explaining individual predictions. model predicts patient lime highlights symptoms patient’s history prediction. sneeze headache portrayed contributing prediction fatigue evidence these doctor make informed decision whether trust model’s prediction. process explaining individual predictions illustrated figure clear doctor much better positioned make decision help model intelligible explanations provided. case explanation small list symptoms relative weights symptoms either contribute prediction evidence humans usually prior knowledge application domain accept reject prediction understand reasoning behind observed example providing explanations increase acceptance movie recommendations automated systems every machine learning application also requires certain measure overall trust model. development evaluation classiﬁcation model often consists collecting annotated data held-out subset used automated evaluation. although useful pipeline many applications evaluation validation data correspond performance wild practitioners often overestimate accuracy models thus trust cannot rely solely looking examples oﬀers alternative method assess truth model especially examples explained. thus propose explaining several representative individual predictions model provide global understanding. several ways model evaluation wrong. data leakage example deﬁned unintentional leakage signal training data would appear deployed potentially increases accuracy. challenging example cited kaufman patient found heavily correlated target class training validation data. issue would incredibly challenging identify observing predictions data much easier explanations figure provided patient would listed explanation predictions. another particularly hard detect problem dataset shift training data diﬀerent test data insights given explanations particularly helpful identifying must done convert untrustworthy model trustworthy example removing leaked data changing training data avoid dataset shift. figure explaining individual predictions competing classiﬁers trying determine document christianity atheism. chart represents importance given relevant words also highlighted text. color indicates class word contributes show individual prediction explanations used select models conjunction accuracy. case algorithm higher accuracy validation actually much worse fact easy explanations provided hard otherwise. further frequently mismatch metrics compute optimize actual metrics interest user engagement retention. able measure metrics knowledge certain model behaviors inﬂuence them. therefore practitioner wish choose less accurate model content recommendation place high importance features related clickbait articles even exploiting features increases accuracy model cross validation. note explanations particularly useful scenarios method produce model variety models compared. essential criterion explanations must interpretable i.e. provide qualitative understanding input variables response. note interpretability must take account user’s limitations. thus linear model gradient vector additive model interpretable. example hundreds thousands features signiﬁcantly contribute prediction reasonable expect user comprehend prediction made even individual weights inspected. requirement implies explanations easy understand necessarily true features used model thus input variables explanations need diﬀerent features. finally note notion interpretability also depends target audience. machine learning practitioners able interpret small bayesian networks laymen comfortable small number weighted features explanation. another essential criterion local ﬁdelity. although often impossible explanation completely faithful unless complete description model itself explanation meaningful must least locally faithful i.e. must correspond model behaves vicinity instance predicted. note local ﬁdelity imply global ﬁdelity features globally important important local context vice versa. global ﬁdelity would imply local ﬁdelity identifying globally faithful explanations interpretable remains challenge complex models. models inherently interpretable explainer able explain model thus model-agnostic apart fact many state-ofthe-art classiﬁers currently interpretable also provides ﬂexibility explain future classiﬁers. addition explaining predictions providing global perspective important ascertain trust model. mentioned before accuracy often suitable metric evaluate model thus want explain model. building upon explanations individual predictions select explanations present user representative model. present local interpretable model-agnostic explanations overall goal lime identify interpretable model interpretable representation locally faithful classiﬁer. present explanation system important distinguish features interpretable data representations. mentioned before interpretable explanations need representation understandable humans regardless actual features used model. example possible interpretable representation text classiﬁcation binary vector indicating presence absence word even though classiﬁer complex features word embeddings. likewise image classiﬁcation interpretable representation binary vector indicating presence absence contiguous patch similar pixels classiﬁer represent image tensor three color channels pixel. approximate drawing samples weighted sample instances around drawing nonzero elements uniformly random given perturbed sample recover sample original representation obtain used label explanation model. given dataset perturbed samples associated labels optimize explanation primary intuition behind lime presented figure sample instances vicinity away even though original model complex explain globally lime presents explanation locally faithful locality captured worth noting method fairly robust sampling noise since samples weighted present concrete instance general framework. algorithm sparse linear explanations using lime require classiﬁer number samples require instance interpretable version require similarity kernel length explanation user. estimate faithfulness also used selecting appropriate family explanations multiple interpretable model classes thus adapting given dataset classiﬁer. leave exploration future work linear explanations work quite well multiple black-box models experiments. figure explain predictions support vector machine kernel trained unigrams diﬀerentiate christianity atheism although classiﬁer achieves held-out accuracy would tempted trust based this explanation instance shows predictions made quite arbitrary reasons word posting appears examples training class atheism. even headers removed proper names proliﬁc posters original newsgroups selected classiﬁer would also generalize. getting insights explanations clear dataset serious issues classiﬁer held-out evaluation cannot trusted. also clear problems steps taken issues train trustworthy classiﬁer. using sparse linear explanations image classiﬁers wish highlight super-pixels positive weight towards speciﬁc class give intuition model would think class present. explain prediction google’s pre-trained inception neural network fashion arbitrary image figures show superpixels explanations predicted classes neural network picks classes quite natural humans figure particular provides insight acoustic guitar predicted electric fretboard. kind explanation enhances trust classiﬁer shows acting unreasonable manner. figure example present intuition lime. black-box model’s complex decision function represented blue/pink background cannot approximated well linear model. bold cross instance explained. lime samples instances gets predictions using weighs proximity instance explained dashed line learned explanation locally faithful. adapted user handle could diﬀerent values diﬀerent instances. paper constant value leaving exploration diﬀerent values future work. image classiﬁcation using super-pixels instead words interpretable representation image binary vector indicates original super-pixel indicates grayed super-pixel. particular choice makes directly solving intractable approximate ﬁrst selecting features lasso learning weights least squares since algorithm produces explanation individual prediction complexity depend size dataset instead time compute number samples practice explaining random forests trees using scikit-learn laptop takes seconds without optimizations using gpus parallelization. explaining prediction inception network image classiﬁcation takes around minutes. choice interpretable representations inherent drawbacks. first underlying model treated black-box certain interpretable representations powerful enough explain certain behaviors. example model predicts sepia-toned images retro cannot explained presence absence super pixels. second choice means underlying model highly non-linear even locality prediction faithful explanation. however estimate faithfulness figure explaining image classiﬁcation prediction made google’s inception neural network. classes predicted electric guitar acoustic guitar labrador although explanation single prediction provides understanding reliability classiﬁer user suﬃcient evaluate assess trust model whole. propose give global understanding model explaining individual instances. approach still model agnostic complementary computing summary statistics held-out accuracy. even though explanations multiple instances insightful instances need selected judiciously since users time examine large number explanations. represent time/patience humans budget denotes number explanations willing look order understand model. given instances deﬁne pick step task selecting instances user inspect. pick step dependent existence explanations main purpose tools like modeltracker others assist users selecting instances themselves examining data predictions. however since looking data enough understand predictions insights pick step take account explanations accompany prediction. moreover method pick diverse representative explanations show user i.e. non-redundant explanations represent model behaves globally. want pick instances cover important components explanations must redundant components show users i.e. avoid selecting instances similar explanations. figure second picked third adds value user already seen features last exposes user completely features. selecting second last results coverage almost features. formalize non-redundant coverage intuition deﬁne coverage function marginal coverage gain adding instance submodularity greedy algorithm iteratively adds instance highest marginal coverage gain solution oﬀers constant-factor approximation guarantee section present simulated user experiments evaluate utility explanations trust-related tasks. particular address following questions explanations faithful model explanations users ascertaining trust predictions explanations useful evaluating model whole. code data replicating experiments available https//github.com/marcotcr/lime-experiments. sentiment analysis datasets task classify product reviews positive negative train decision trees logistic regression regularization nearest neighbors support vector machines kernel using words features. also include random forests trained average wordvec embedding model impossible interpret without technique like lime. implementations default parameters scikitlearn unless noted otherwise. divide dataset train test explain individual predictions compare proposed approach parzen method approximates black classiﬁer globally parzen windows explains individual predictions taking gradient prediction probability function. parzen take features highest absolute gradients explanations. hyper-parameters parzen lime using cross validation also compare greedy procedure greedily remove features contribute predicted class prediction changes random procedure randomly picks features explanation. experiments. decision trees). particular train classiﬁers maximum number features instance thus know gold features considered important models. prediction test generate explanations compute fraction gold features recovered explanations. report recall averaged test instances figures observe greedy approach comparable parzen logistic regression substantially worse decision trees since changing single feature time often eﬀect prediction. overall recall parzen likely diﬃculty approximating original highdimensional classiﬁer. lime consistently provides recall classiﬁers datasets demonstrating lime explanations faithful models. order simulate trust individual predictions ﬁrst randomly select features untrustworthy assume users identify would want trust features thus develop oracle trustworthiness labeling test predictions black classiﬁer untrustworthy prediction changes untrustworthy features removed instance trustworthy otherwise. order simulate users assume users deem predictions untrustworthy lime parzen explanations prediction linear approximation changes untrustworthy features appear explanations removed greedy random prediction mistrusted untrustworthy features present explanation since methods provide notion contribution feature prediction. thus test prediction evaluate whether simulated user trusts using explanation method compare trustworthiness oracle. present accuracy picking correct classiﬁer varies averaged runs figure omit sp-parzen rp-parzen ﬁgure since produce useful explanations performing slightly better random. lime consistently better greedy irrespective pick method. further combining submodular pick lime outperforms methods particular much better rp-lime examples shown users. results demonstrate trust assessments provided sp-selected lime explanations good indicators generalization validate human experiments next section. predictions explanation method averaged runs table results indicate lime dominates others datasets black models. methods either achieve lower recall lower precision lime maintains high precision high recall. even though artiﬁcially select features untrustworthy results indicate lime helpful assessing trust individual predictions. ﬁnal simulated user experiment evaluate whether explanations used model selection simulating case human decide competing models similar accuracy validation data. purpose artiﬁcially noisy features. speciﬁcally training validation sets artiﬁcial feature appears examples class other test instances artiﬁcial feature appears examples class. recreates situation models features informative real world also ones introduce spurious correlations. create pairs competing classiﬁers repeatedly training pairs random forests trees validation accuracy within other test accuracy diﬀers least thus possible identify better classiﬁer accuracy validation data. goal experiment evaluate whether user identify better classiﬁer based explanations instances validation set. simulated human marks artiﬁcial features appear explanations untrustworthy following evaluate many total predictions validation trusted then select classiﬁer section recreate three scenarios machine learning require trust understanding predictions models. particular evaluate lime sp-lime following settings users choose atheism documents newsgroups dataset mentioned beforehand. dataset problematic since contains features generalize thus validation accuracy considerably overestimates real-world performance. order estimate real world performance create religion dataset evaluation. download atheism christianity websites dmoz directory human curated lists yielding webpages class. high accuracy dataset classiﬁer trained newsgroups indicates classiﬁer generalizing using semantic content instead placing importance data speciﬁc issues outlined above. unless noted otherwise kernel trained newsgroups data hyper-parameters tuned cross-validation. section want evaluate whether explanations help users decide classiﬁer generalizes better i.e. classiﬁer would user deploy wild. specifically users decide classiﬁers trained original newsgroups dataset version classiﬁer trained cleaned dataset many features generalize manually removed. original classiﬁer achieves accuracy score religion dataset cleaned classiﬁer achieves score contrast test accuracy original newsgroups split respectively suggesting worse classiﬁer would selected accuracy alone used measure trust. recruit human subjects amazon mechanical turk means machine learning experts instead people basic knowledge religion. measure ability choose better algorithm seeing side-byside explanations associated data restrict number words explanation number documents instances words explanation reminder users experts machine learning unfamiliar feature engineering thus identifying words based semantic content. further users access religion dataset even know existence. start experiment subjects. mark words deletion train diﬀerent classiﬁers subject explanations classiﬁer presented users round interaction results classiﬁers. ﬁnal round classiﬁers path interaction tracing back ﬁrst subjects. explanations instances shown user produced sp-lime rp-lime. show average accuracy religion dataset interaction round paths originating original subjects average across paths figure clear ﬁgure crowd workers able improve model removing features deem unimportant task. further sp-lime outperforms rp-lime indicating selection instances show users crucial eﬃcient feature engineering. subject took average minutes round cleaning resulting minutes produce classiﬁer generalizes much better real world data. path average words removed indicating incorporating coverage important features useful feature engineering. further average words selected selected least half users users. along fact variance accuracy decreases across rounds high agreement demonstrates users converging similar correct models. evaluation example explanations make easy improve untrustworthy classiﬁer case easy enough machine learning knowledge required. often artifacts data collection induce undesirable correlations classiﬁers pick training. issues diﬃcult identify looking data predictions. eﬀort reproduce setting take task distinguishing photos wolves eskimo dogs train logistic regression classiﬁer training images hand selected pictures wolves snow background pictures huskies not. features images ﬁrst max-pooling layer google’s pre-trained inception neural network collection additional images classiﬁer predicts wolf snow husky otherwise regardless animal color position pose etc. trained classiﬁer intentionally evaluate whether subjects able detect experiment proceeds follows ﬁrst present balanced test predictions wolf snowy background husky show husky mistake figure examples classiﬁed correctly. subject three questions trust algorithm figure feature engineering experiment. shaded line represents average accuracy subjects path starting initial subjects. solid line represents average across paths round interaction. person inspects position algorithm order instances seen randomized subjects. examining explanations users asked select algorithm perform best real world. explanations produced either greedy lime instances selected either random submodular pick modify greedy step algorithm slightly alternates explanations classiﬁers. setting repeat experiment users. results presented figure note methods good identifying better classiﬁer demonstrating explanations useful determining classiﬁer trust using test accuracy would result selection wrong classiﬁer. further submodular pick greatly improves user’s ability select best classiﬁer compared random pick lime outperforming greedy cases. notes classiﬁer untrustworthy common task machine learning feature engineering i.e. modifying features retraining order improve generalization. explanations process presenting important features particularly removing features users feel generalize. learning speciﬁcally vision tasks letting users know systems likely fail lead increase trust avoiding silly mistakes solutions either require additional annotations feature engineering speciﬁc vision tasks provide insight decision trusted. furthermore assume current evaluation metrics reliable case problems data leakage present. recent work focuses exposing users diﬀerent kinds mistakes interestingly subjects study notice serious problems newsgroups data even looking many mistakes suggesting examining data suﬃcient. note groce alone regard many researchers ﬁeld unwittingly published classiﬁers would generalize task. using lime show even non-experts able identify irregularities explanations present. further lime complement existing systems allow users assess trust even prediction seems correct made wrong reasons. recognizing utility explanations assessing trust many proposed using interpretable models especially medical domain models appropriate domains apply equally well others terpretability cases comes cost ﬂexibility accuracy eﬃciency. text elucidebug full human-in-the-loop system shares many goals however focus already interpretable model computer vision systems rely object detection produce candidate alignments attention able produce explanations predictions. however constrained speciﬁc neural network architectures incapable detecting object parts images. focus general model-agnostic explanations applied classiﬁer regressor appropriate domain even ones proposed. common approach model-agnostic explanation learning potentially interpretable model predictions original model explanation gradient vector captures similar locality intuition lime. however interpreting coeﬃcients gradient diﬃcult particularly conﬁdent predictions further explanations approximate original model globally thus maintaining local ﬁdelity becomes signiﬁcant challenge experiments demonstrate. contrast lime solves much feasible task ﬁnding model approximates original model locally. idea perturbing inputs explanations explored authors focus learning speciﬁc contribution model opposed general framework. none approaches explicitly take cognitive limitations account thus produce non-interpretable explanations gradients linear models thousands non-zero weights. problem becomes worse original features nonsensical humans contrast lime incorporates interpretability optimization notion interpretable representation domain task speciﬁc interpretability criteria accommodated. work well real world think algorithm able distinguish photos wolves huskies. getting responses show images associated explanations figure questions. since task requires familiarity notion spurious correlations generalization subjects experiment graduate students taken least graduate machine learning course. gathering responses independent evaluators read reasoning determine subject mentioned snow background equivalent feature model using. pick majority decide whether subject correct insight report numbers showing explanations table observing explanations third trusted classiﬁer little less half mentioned snow pattern something neural network using although speculated patterns. examining explanations however almost subjects identiﬁed correct insight much certainty determining factor. further trust classiﬁer also dropped substantially. although sample size small experiment demonstrates utility explaining individual predictions getting insights classiﬁers knowing trust why. problems relying validation accuracy primary measure trust well studied. practitioners consistently overestimate model’s accuracy propagate feedback loops fail notice data leaks order address issues researchers proposed tools like gestalt modeltracker help users navigate individual instances. tools complementary lime terms explaining models since address problem explaining individual predictions. further submodular pick procedure incorporated tools users navigating larger datasets. paper argued trust crucial eﬀective human interaction machine learning systems explaining individual predictions important assessing trust. proposed lime modular extensible approach faithfully explain predictions model interpretable manner. also introduced sp-lime method select representative non-redundant predictions providing global view model users. experiments demonstrated explanations useful variety models trust-related tasks text image domains expert non-expert users deciding models assessing trust improving untrustworthy models getting insights predictions. number avenues future work would like explore. although describe sparse linear models explanations framework supports exploration variety explanation families decision trees; would interesting comparative study real users. issue mention work perform pick step images would like address limitation future. domain model agnosticism enables explore variety applications would like investigate potential uses speech video medical domains well recommendation systems. finally would like explore theoretical properties computational optimizations order provide accurate real-time explanations critical human-in-the-loop machine learning system. would like thank scott lundberg tianqi chen tyler johnson helpful discussions feedback. work supported part awards wnf-- n--- part terraswarm centers starnet semiconductor research corporation program sponsored marco darpa. burnett w.-k. wong stumpf shinsel bice mcintosh. possible oracle eﬀective test selection users interactive machine learning systems. ieee trans. softw. eng. herlocker konstan riedl. explaining sanchez rocktaschel riedel singh. towards extracting faithful descriptive representations latent variable models. aaai spring syposium knowledge representation reasoning integrating symbolic neural approaches", "year": 2016}