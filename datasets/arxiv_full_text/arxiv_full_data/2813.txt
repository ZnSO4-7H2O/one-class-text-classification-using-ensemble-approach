{"title": "Scalable Lévy Process Priors for Spectral Kernel Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a L\\'evy process. The resulting distribution has support for all stationary covariances--including the popular RBF, periodic, and Mat\\'ern kernels--combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the L\\'evy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for $\\mathcal{O}(n)$ training and $\\mathcal{O}(1)$ predictions. We perform extrapolations having reasonable uncertainty estimates on several benchmarks, show that the proposed model can recover flexible ground truth covariances and that it is robust to errors in initialization.", "text": "gaussian processes rich distributions functions generalization properties determined kernel function. used long-range extrapolation predictions particularly sensitive choice kernel parameters. therefore critical account kernel uncertainty predictive distributions. propose distribution kernels formed modelling spectral mixture density l´evy process. resulting distribution support stationary covariances—including popular periodic mat´ern kernels— combined inductive biases enable automatic data efﬁcient learning long-range extrapolation state predictive performance. proposed model also presents approach spectral regularization l´evy process introduces sparsity-inducing prior mixture components allowing automatic selection model order pruning extraneous components. exploit algebraic structure proposed process training predictions. perform extrapolations reasonable uncertainty estimates several benchmarks show proposed model recover ﬂexible ground truth covariances robust errors initialization. gaussian processes naturally give rise function space view modelling whereby place prior distribution functions reason properties likely functions prior given data infer posterior distribution functions make predictions. generalisation behavior gaussian process determined prior support inductive biases turn encoded kernel function. however popular kernels even multiple kernel learning procedures typically cannot extract highly expressive hidden representations envisaged neural networks discover representations recent approaches advocated building expressive kernel functions. instance spectral mixture kernels introduced ﬂexible kernel learning extrapolation modelling spectral density scale-location mixture gaussians promising results. however wilson adams specify number mixture components hand characterize uncertainty mixture hyperparameters. kernel functions become increasingly expressive parametrized becomes natural also adopt function space view kernel learning—to represent uncertainty values kernel function reﬂect belief kernel simple form. gaussian processes functions model data apply function space view step hierarchical model—with prior distribution kernels. paper introduce scalable distribution kernels modelling spectral density fourier transform kernel l´evy process. consider scale-location mixtures gaussians laplacians basis functions l´evy process induce prior kernels gives rise sharply peaked spectral densities often occur practice—providing powerful inductive bias kernel learning. moreover choice basis functions allows kernel function conditioned l´evy process expressed closed form. prior distribution kernels also support stationary covariances—containing instance composition popular mat´ern rational quadratic gamma-exponential spectral mixture kernels. unlike spectral mixture representation wilson adams proposed process prior allows natural automatic inference number mixture components spectral density model. moreover priors implied popular l´evy processes gamma process symmetric α-stable process result even stronger complexity penalties regularization yielding sparse representations removing mixture components noise. conditioned distribution kernels model data gaussian process. form predictive distribution take bayesian model average predictive distributions large possible kernel functions represented support prior kernels weighted posterior probabilities kernels. procedure leads non-gaussian heavytailed predictive distribution modelling data. develop reversible jump mcmc scheme infer posterior distribution kernels including inference number components l´evy process expansion. scalability pursue structured kernel interpolation approach case exploiting algebraic structure l´evy process expansion inference predictions compared standard computations inference predictions gaussian processes. flexible distributions kernels especially valuable large datasets often contain additional structure learn rich statistical representations. contributions paper summarized follows ﬁrst fully probabilistic approach inference spectral mixture kernels incorporate kernel uncertainty predictive distributions realistic coverage extrapolations. feature demonstrated section spectral regularization spectral kernel learning. l´evy process prior acts sparsityinducing prior mixture components automatically pruning extraneous components. feature allows automatic inference model order hyperparameter must hand tuned original spectral mixture kernel paper. stochastic process gaussian process ﬁnite collection inputs {x··· vector function values jointly gaussian. distribution completely determined mean function covariance kernel used specify distribution functions denoted generalization properties encoded covariance kernel hyperparameters. exploiting properties joint gaussian variables obtain closed form expressions conditional mean covariance functions unobserved function values given observed function values. given observed training inputs values predictive distribution unobserved function values testing inputs given kernels limited expressiveness primarily smoothing interpolators covariance structure learn data length scale determines quickly covariance decays distance. wilson adams introduce expressive spectral mixture kernel capable extracting complex covariance structures kernel formed placing scale-location mixture gaussians spectrum covariance kernel. kernel comparison model single gaussian centered origin frequency space. l´evy processes stochastic process {l}ω∈r+ l´evy process stationary independent increments continuous probability. words must satisfy l´evy process viewed combination brownian motion drift superposition independent poisson processes differing jump sizes l´evy measure determines expected number poisson events unit time particular jump size brownian component l´evy process considered model. higher dimension input spaces deﬁnes general notion l´evy random measure also characterized l´evy measure show sample realizations l´evy processes used draw sample parameters adaptive basis expansions. l´evy process priors adaptive expansions family priors naturally suited purpose one-to-one correspondence jump behavior l´evy prior components expansion. illustrate point suppose basis function parameters one-dimensional consider integral resembles sample path compound poisson process number jumps jump sizes jump locations corresponding number basis functions basis function weights basis function parameters respectively. compound poisson process deﬁne prior piecewise constant paths. generally l´evy process deﬁne prior l´evy-khintchine representation jump behavior prior characterized l´evy measure controls mean number poisson events every region parameter space encoding inductive biases model. number parameters framework random form trans-dimensional reversible jump markov chain monte carlo sample parameter space popular l´evy processes gamma process symmetric gamma process symmetric α-stable process possess desirable properties different situations. gamma process able produce strictly positive gamma distributed without transforming output space. symmetric gamma process produce positive negative according wolpert achieve nearly commonly used isotropic geostatistical covariance functions. symmetric α-stable process produce heavy-tailed distributions appropriate might expect basis expansion dominated heavily weighted functions. could dispense l´evy processes place gaussian laplace priors obtain regularization expansions respectively beneﬁt particular l´evy process priors implied priors coefﬁcients yield even stronger complexity penalties regularization. property encourages sparsity expansions permits scalability mcmc algorithm. refer supplementary material illustration joint priors coefﬁcients exhibit concave contours contrast convex elliptical diamond contours regularization. furthermore posterior l´evy process complexity penalty term encourages sparsity expansions. refer clyde wolpert details. l´evy distributions kernels section motivate choice prior kernel functions describe generate samples prior distribution practice. l´evy kernel processes hence spectral density entirely characterizes stationary kernel. therefore desirable model spectrum rather kernel since view kernel estimation lens density estimation. order emulate sharp peaks characterize frequency spectra natural phenomena model spectral density location-scale mixture laplacian components parameters interpreted total number terms mixture scale frequency contribution central frequency governs rapidly term decays basis functions used place model spectrum well. example gaussian mixture chosen along maximum likelihood estimation learning procedure obtain spectral mixture kernel spectral density takes form adaptive expansion deﬁne l´evy prior densities hence corresponding kernels form. chosen basis function l´evy measure drawn l´evy kernel process denoted lkp. wolpert discuss necessary regularity conditions summary propose following hierarchical model functions figure shows three samples l´evy process speciﬁed corresponding covariance kernels. also show realization kernel functions. placing l´evy process prior spectral densities induce l´evy kernel process prior stationary covariance functions. sampling l´evy priors discuss generate samples l´evy kernel process practice. short kernel parameters drawn according {j{}j l´evy) used evaluate values recall section choice l´evy measure completely determined choice corresponding l´evy process vice versa. though processes mentioned produce sample paths inﬁnitely many jumps almost jumps inﬁnitesimally small therefore processes approximated compound poisson process jump size distribution truncated desired l´evy process chosen truncation bound basis expansion parameters generated drawing poisson i.i.d. samples ω··· refer supplementary gamma symmetric gamma material error bounds formulas symmetric α-stable processes. form also depends choice l´evy process found supplementary material details wolpert choose draw uninformed uniform prior reasonable range frequency domain gamma distribution gamma. choices frequency limits left hyperparameters hyperprior distributions. drawing values specify figure samples l´evy kernel mixture prior distribution. three spectra laplace components drawn l´evy process prior. corresponding stationary covariance kernel functions prior mean standard deviations model determined samples. samples respective covariance kernel functions. l´evy process realization corresponding covariance function evaluated analytical expression inverse fourier transform laplacian frequency mixture components). scalable inference given observed data yi}n interpolation extrapolation. model observations hierarchical model compute approximating using rj-mcmc samples sample draws kernel posterior distribution. sample enables draw sample posterior predictive distribution estimate predictive mean variance. although chosen gaussian observation model inference procedures introduced would also apply non-gaussian likelihoods poisson processes gaussian process intensity functions classiﬁcation. requires drawing kernels distribution difﬁcult distribution approximate particularly ﬁxed number parameters varies. employ rj-mcmc extends capability conventional mcmc allow sequential samples different dimensions drawn thus posterior distribution limited coefﬁcients parameters ﬁxed basis expansion represent changing number basis functions required description l´evy processes described previous section. indeed rj-mcmc used automatically learn appropriate number basis functions expansion. case spectral kernel learning inferring number basis functions corresponds automatically learning important frequency contributions kernel lead interpretable insights data. choice initialization procedure often important practical consideration machine learning tasks severe multimodality likelihood surface many cases however spectral kernel learning rj-mcmc automatically learn salient frequency contributions simple initialization uniform covering broad range frequencies many sharp peaks. frequencies important describing data quickly attenuated removed within rj-mcmc learning. typically hundred rj-mcmc iterations needed discover salient frequencies way. wilson proposes alternative structured approach initialization previous spectral kernel modelling work. first pass data fourier transform obtain empirical spectral density treated observed. next empirical spectral density using standard gaussian mixture density estimation procedure assuming ﬁxed number mixture components. then learned parameters gaussian mixture initialization spectral mixture kernel hyperparameters gaussian process marginal likelihood optimization. observe successful adaptation procedure l´evy process method replacing approximation laplacian mixture terms using result initialize rj-mcmc. scalability |kxx kernel matrix evaluated training points direct approach computing cholesky decomposition kernel matrix requires computations storage restricting size training sets furthermore computation must performed every iteration rj-mcmc compounding standard computational constraints. however bottleneck readily overcome structured kernel interpolation approach introduced wilson nickisch approximates kernel matrix kzzm exact kernel matrix evaluated much smaller inducing points sparse interpolation matrix facilitates fast computations. calculation reduces computations storage. described wilson nickisch impose toeplitz structure allowing rj-mcmc procedure train massive datasets. experiments conduct four experiments total. order motivate model kernel learning later experiments ﬁrst demonstrate ability l´evy process recover—through direct regression—an observed noise-contaminated spectrum characteristic sharply peaked naturally occurring spectra. second experiment demonstrate robustness rjmcmc sampler automatically recovering generative frequencies known kernel even presence signiﬁcant noise contamination poor initializations. third experiment demonstrate ability method infer spectrum airline passenger data perform long-range extrapolations real data demonstrate utility accounting uncertainty kernel. ﬁnal experiment demonstrate scalability method training model data point sound waveform. code available https //github.com/pjang/levy-spectral-kernel-learning. explicit spectrum modelling begin applying l´evy process directly function modelling inference described wolpert laplacian basis functions. choose class test function proposed donoho johnstone standard wavelet literature. spatially inhomogeneous function deﬁned represent spectral densities arise scientiﬁc engineering applications. gaussian i.i.d. noise added give signal-to-noise ratio consistent previous studies test function wolpert noisy test function lark regression shown figure synthetic spectrum well characterized l´evy process false positive basis function terms ﬁtting noise owing strong regularization properties l´evy prior. contrast regression kernel learns length scale maximum marginal likelihood training gaussian process posterior sharp peaks test function also overﬁts additive noise. point experiment show l´evy process laplacian basis functions forms natural prior spectral densities. words samples prior typically look like types spectra occur practice. thus process powerful inductive bias used kernel learning explore next experiments. figure l´evy process regression noisy test function captures locations scales spike ignoring noise falls slightly short modes since black spikes parameterized rather laplacian. figure ground truth recovery known frequency components. spectrum gaussian process used generate noisy training data shown black. noisy data erroneous spectral initialization shown dashed blue maximum posteriori estimate spectral density shown red. kernel also identiﬁes salient frequencies broader support shown magenta. noisy training data shown scatterplot withheld testing data shown green. learned posterior predictive distribution captures test data. ground truth recovery next demonstrate ability method recover generative frequencies known kernel robustness noise poor initializations. data generated kernel spectral laplacian peaks partitioned training testing sets containing points each. moreover training data contaminated i.i.d. gaussian noise based observed training data estimate kernel gaussian process inferring spectral density using rj-mcmc iterations. empirical spectrum initialization described section results discovery generative frequencies. critically also recover salient frequencies even poor initialization shown figure comparison also train gaussian kernel initializing based empirical spectrum. resulting kernel spectrum recover salient frequencies though less conﬁdence higher overhead even poor initialization spectral kernel learning rj-mcmc. spectral kernel learning long-range extrapolation next demonstrate ability method perform long-range extrapolation real data. figure shows time series monthly airline passenger data data show long-term rising trend well short term seasonal waveform absence white noise artifacts. wilson adams ﬁrst monthly data points used train model last months withheld testing data indicated green. initialization empirical spectrum rj-mcmc steps model able automatically learn necessary frequencies shape spectral density capture rising trend seasonal waveform allowing accurate long-range extrapolations withpre-specifying number model components advance. experiment also demonstrates impact accounting uncertainty kernel withheld data often appears near crosses upper bound predictive bands whereas model yields wider conservative predictive bands wholly capture test data. extrapolations highly sensitive choice parameter values ﬁxing parameters kernel yield overconﬁdent predictions. l´evy process prior allows account range possible kernel parameters achieve realistically broad coverage possible extrapolations. note l´evy process spectral densities induces prior kernel functions. figure shows side-by-side comparison covariance function draws prior posterior distributions kernels. sample covariance functions prior vary quite signiﬁcantly concentrated posterior movement towards empirical covariance function. figure learning airline passenger data. training data scatter plotted withheld testing data shown green. learned posterior distribution proposed approach captures periodicity rising trend test data. analogous interval using kernel illustrated magenta. figure covariance function draws kernel prior posterior distributions empirical covariance function shown black. rj-mcmc covariance distribution centers upon correct frequencies order magnitude. figure learning natural sound texture. close-up training interval displayed true waveform data scatter plotted. learned posterior distribution retains periodicity signal within corrupted interval. three samples drawn posterior distribution. scalability demonstration ﬂexible fully bayesian approach kernel learning come additional computational overhead. demonstrate scalability achieved integration l´evy process model. consider data point waveform taken ﬁeld natural sound modelling l´evy kernel process trained sound texture sample howling wind middle removed. training involved initialization signal empirical covariance rj-mcmc samples took less hour using intel memory. four distinct mixture components model automatically identiﬁed rj-mcmc procedure. learned kernel used inﬁlling training points taken down-sampling training data applied original natural sound inﬁlling. posterior distribution region interest shown figure along sample realizations appear capture qualitative behavior waveform. experiment demonstrates applicability proposed kernel learning method large datasets shows promise extensions higher dimensional data. discussion introduced distribution covariance kernel functions well suited modelling quasiperiodic data. shown place l´evy process prior spectral density stationary kernel. resulting hierarchical model allows incorporation kernel uncertainty predictive distribution. spectral regularization properties l´evy process priors found trans-dimensional sampling procedure suitable automatically performing inference model order robust initialization strategies. finally incorporated structured kernel interpolation training inference procedures linear time scalability enabling experiments large datasets. advances conventional spectral mixture kernels able interpretably automatically discover number mixture components representing uncertainty kernel. here considered dimensional inputs stationary processes clearly elucidate properties l´evy kernel processes. however could generalize process multidimensional non-stationary kernel learning jointly inferring properties transformations inputs alongside kernel hyperparameters. alternatively could consider neural networks basis functions l´evy process inferring distributions parameters network numbers basis functions step towards automating neural network architecture construction. acknowledgements. work supported part natural sciences engineering research council canada national science foundation iis- awards. references bochner lectures fourier integrals. volume princeton university press following formulas section taken wolpert reference. suppose hyperparameters prior distributions drawn hyperprior distribution order sample l´evy prior follow steps taken determined speciﬁc choice l´evy process given formulas below. computational purposes βj’s truncated |βjη| poisson approximation true l´evy process represents error approximation figure illustrates contours joint distribution independent draws different priors contours gamma process would taken upper-right quadrant symmetric gamma process. gaussian laplace priors result regularization respectively. l´evy processes contrast yield inward curving contours leading sparsity inducing effect similar regularization intuitively discourages simultaneous large values strongly regularization unless added basis functions signiﬁcantly improve needed de-mean training data subtracting deterministic mean function sample mean best line. eliminate large peaks origin dominate rest spectrum. de-meaned training data {yj}n input rj-mcmc. sample empirical spectral density gaussian mixture components sampled data. good initial guess done examining number peaks empirical spectrum. modelled prior gamma estimated maximum gamma controls expected value coefﬁcients basis functions integrate βj’s equal total area underneath spectrum parseval’s identity represents total variance data. hence sample variance training data used upper bound coefﬁcient values accordingly. gamma proportional expected number basis functions shown section controls sparsity expansions. cover range values encourage sparsity. symmetric α-stable process controls heaviness tails distribution smaller values yielding heavier tails. maximum likelihood initial βj’s.", "year": 2018}