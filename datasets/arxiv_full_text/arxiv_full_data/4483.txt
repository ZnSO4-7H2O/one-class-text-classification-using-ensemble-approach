{"title": "Unsupervised Learning in a Framework of Information Compression by  Multiple Alignment, Unification and Search", "tag": ["cs.AI", "cs.LG", "I.2.4; I.2.6; I.2.7"], "abstract": "This paper describes a novel approach to unsupervised learning that has been developed within a framework of \"information compression by multiple alignment, unification and search\" (ICMAUS), designed to integrate learning with other AI functions such as parsing and production of language, fuzzy pattern recognition, probabilistic and exact forms of reasoning, and others.", "text": "paper describes novel approach unsupervised learning developed within framework designed integrate learning things parsing production language fuzzy pattern recognition best-match information retrieval class hierarchies inheritance attributes probabilistic exact forms reasoning others. framework called information compression multiple alignment uniﬁcation search founded principles minimum length encoding pioneered solomonoﬀ others. work originated earlier programme research unsupervised learning natural languages many goals remain same framework knowledge representation powerful enough accommodate structure natural languages integrate learning segmental structure learning disjunctive structures distinguish ‘correct’ generalisations ‘over’ generalisations without negative samples external error correction grading samples simple complex learning ‘correct’ forms despite corrupted data others. main body paper describes computer model icmaus framework incorporates processes unsupervised learning. examples presented show model infer plausible grammars appropriate input. limitation current model overcome brieﬂy discussed. paper describes novel approach unsupervised learning developed within research programme whose overarching goal integration diverse functions—learning recognition reasoning others—within relatively simple framework. substantial impact learning processes organised. reasons explained conceptual framework developed within research programme called information compression multiple alignment uniﬁcation search next section describes origins motivation icmaus approach relates work machine learning. followed section describing icmaus theory outline. bulk article describes model—a ﬁrst implementation icmaus approach learning—and shows limitations current model—and overcome—are brieﬂy discussed. years developed computer model quite successful discovering without supervision words linguistic segments unsegmented natural language texts developed snpr model able without supervision discover plausible context-free phrase-structure grammars unsegmented samples artiﬁcial languages also langley stromsten models seen largely process information compression matching patterns merging ‘unifying’ patterns same. models incorporate process searching abstract space alternative matches yield relatively large amounts compression. early struck parallels seemed exist programs unsupervised learning prolog system designed originally theorem proving. prominent feature learning models pattern matching uniﬁcation search. although recognised feature prolog process searching patterns match fundamental system merging matching patterns important part ‘uniﬁcation’ term understood logic. seemed possible might fundamental role logic—and perhaps also ‘computing’ mathematics—as grammatical inference. observations thought might possible integrate unsupervised learning logical inference within single system dedicated pattern matching uniﬁcation search. thinking suggested scope integrated system might expanded include things best-match information retrieval fuzzy pattern recognition parsing production language probabilistic inference aspects given overall goal integrating diverse functions within single system evident quite early system would need organised rather diﬀerent organisation snpr models. notwithstanding development inductive logic programming seemed prolog itself suitable vehicle proposed developments—largely unwanted complexity system relative inﬂexibility search processes prolog. seemed necessary build proposed integrated system ‘deeper’ foundations. initial eﬀorts focussed development improved version ‘dynamic programming’ ﬁnding full matches good partial matches pairs patterns became apparent scope system could greatly enhanced replacing concept ‘pattern matching’ speciﬁc concept ‘multiple alignment’ similar concept bio-informatics important diﬀerences. notwithstanding origin ideas research inductive learning eﬀort date focussed things probabilistic reasoning natural language processing application ideas interpretation ‘computing’ concepts mathematics logic integration learning capabilities within framework circle largely complete. goals earlier research learning natural languages carried forward icmaus programme. main reason unsupervised learning natural language remains biggest challenges research machine learning seems likely insights gained area prove useful areas. system represent structures form readable comprehensible—which means explicit symbolic style associated linguistics rather implicit style artiﬁcial neural networks. ultimately system able learn things discontinuous dependencies syntax able learn semantic structures able learn structures integrate syntax semantics. system able create rules generalise structures found corpus data able correct ‘over’ generalisations formed. corresponds childrens ability generalise beyond language hear correct kinds overgeneralisations make young. system able learn ‘correct’ grammars despite corruption corpus data. corresponds learn distinguish sharply utterances belong native language despite fact language hear children often contains false starts incomplete sentences ungrammatical forms. apart learning discontinuous dependencies semantic structures goals already degree snpr model current work challenge achieve goals framework. weight available evidence child learn ﬁrst language languages without error correction ‘teacher’ without provision ‘negative’ samples without grading language samples simple complex. short children apparently learn without ‘supervision’ although take advantage error correction like available. unless subscribes chomskyan ‘nativist’ view language learning feature children’s learning appears conﬂict gold’s proof language learning conceived language identiﬁcation limit possible without least sources error correction mentioned above. apparent conﬂict resolved accepted gold’s concept grammatical inference good model child learns native language. rather suppose ‘correct’ grammar ‘target’ learned suppose learning process optimisation. given abstract ‘space’ potential grammars natural language astronomically large learning must rely heuristic methods hill climbing. means possible know whether found ‘best’ grammar terms one’s chosen criterion. however practical purposes matter. necessary grammar ‘good enough’. major goal research develop idea principles might serve unifying foundation kinds information processing. care taken ensure icmaus framework operate accordance principles. already noted major goal research integration learning functions pattern recognition reasoning planning problem solving others. requirement substantial reorganisation learning processes compared earlier models dedicated purely learning. relevant understanding brains nervous systems well computers model developed possible realisation terms neural mechanisms. recent work shown main elements model indeed instantiated terms research relates work machine learning best seen terms goals research described. example closely related research grammar induction using phrase-structure grammars research learning dfas n-grams markov models—which known less adequate representing structure natural languages research closer spirit research unsupervised learning research learning external error correction negative samples. closer relation work learning conceived process optimisation using principles research language identiﬁcation limit related ideas. stronger aﬃnity systems designed learn explicit symbolic structures readable comprehensible systems—such many artiﬁcial neural networks—that learn knowledge implicit form. studies perhaps closely related present research include adriaans allison clark denis henrichsen johnson riezler klein manning nevillmanning witten oliveira sangiovanni-vincentelli rapp s.watkinson manandhar system receives data world ‘senses’. data designated ‘new’. course learning information transferred repository stored information initially empty designated ‘old’. system tries compress section much possible searching matching patterns within given section section information already stored old. example pattern ‘information compression’ appears times assigned relatively short identiﬁer ‘code’ used abbreviation pattern wherever appears. system stores repeating patterns kind. also stores unmatched portions case prove useful compressing sections come later. shall system also creates stores patterns represent higher levels abstraction. patterns stored appropriate code symbols. broad terms incremental scheme similar well-known widely-used lempel-ziv algorithms diﬀerent icmaus scheme emphasis partial matching relatively thorough searching space alternative possible matches. also distinctive concept ‘multiple alignment’ developed research support encoding information terms information hierarchy ‘levels’ information compression interpreted process maximising simplicity information whilst retaining much possible non-redundant descriptive power. hence sobriquet ‘sp’ adopted icmaus proposals computer models framework realised. given intended wide scope framework goal research devise ‘universal’ scheme representation knowledge capable representing diverse kinds knowledge succinct manner capable integrating diverse kinds knowledge seamless manner. naturally design scheme would depend part ways stored knowledge going used. emerged almost simplest conceivable format knowledge arrays patterns atomic symbols dimensions. focus one-dimensional arrays—‘strings’ ‘sequences’ symbols. however since system intended eventually embrace arrays dimensions possibly more relatively general term ‘pattern’ normally used. apparent exception slogan hidden meanings that within pattern normally distinction symbols represent substance contents pattern symbols serve identify ‘encode’ pattern kind distinction justiﬁed grounds part mechanism system organises uses knowledge. slogan really applies meanings part knowledge itself. ‘granularity’ symbols icmaus framework undeﬁned. symbols used represent ﬁne-grained details body information used represent relatively large chunks information. despite extreme simplicity format representing knowledge processed within icmaus framework means model variety established representational schemes including context-free phrasestructure grammars context-sensitive grammars production rules networks trees schemes. examples seen many found wolﬀ respect representation structures icmaus system accommodate major features syntax including discontinuous dependencies represent various kinds non-syntactic ‘semantic’ structures preliminary work suggests syntax semantics indeed integrated. idea that grammatical inference related kinds learning seek minimise size grammar size sample encoded terms grammar. principle guards induction grammars trivially small also guards induction grammars small value achieved cost disproportionately large value human brain currently best learning system existence. human intuitions constitutes ‘good’ ‘natural’ ways structuring information seem seem conditioned large extent principles widely recognised principles translated bayesian concepts probability although views equivalent level analysis diﬀer terms heuristic value primitive concepts pattern matching uniﬁcation identical patterns much direct transparent relation probabilistic concepts. research focus matching uniﬁcation patterns proved fruitful source insights analyses terms probabilities. realises encoding information terms predeﬁned information without attempt modify adding patterns purging them. also contains procedures calculating probabilities inferences drawn system described section below. ﬁrst example described relatively fully serves illustrate main elements multiple alignment concept developed research. owing limitations space remaining examples icmaus applications necessarily brief. figure shows best alignment found sentence patterns representing grammatical rules old. context ‘best’ means alignment allows encoded economically terms patterns explained below. apart pattern patterns example like re-write rules context-free phrase-structure grammar re-write arrow omitted. ignore alignment shown figure much like conventional parsing marking main components sentence words phrases sentence pattern shows ‘discontinuous’ dependency exists singular noun subject sentence singular verb marked within alignment relatively direct manner. despite simplicity format representing knowledge formation multiple alignments enables system express ‘context sensitive’ aspects language kinds knowledge. within pattern symbols status identiﬁcation symbols others serve contents symbols. general symbols given pattern comprise bracket symbols deﬁne left right boundaries pattern together symbols usually near start pattern serve identify pattern uniquely within otherwise deﬁne relates patterns. symbols symbols. examples symbols figure include ‘num corresponding symbols encoding derived alignment looking columns alignment containing single symbol matched symbol copying symbols code pattern sequence appear alignment. example code pattern derived neat feature icmaus framework that without modiﬁcation used produce sentences well parse them. program sentence replaced code pattern best alignment found system apart shown figure words original sentence appear within alignment correct order. dynamic programming built models means home ‘fuzzy’ partial matches patterns exact matches. means lend well modelling human-like capabilities recognising patterns objects despite errors omission commission substitution including recognise something even partially obscured things. similar models able model best-match retrieval information old. icmaus framework also lends quite well modelling class hierarchies inheritance attributes recognition objects patterns multiple levels abstraction. class associated attributes represented pattern ‘isa’ links classes established much grammatical patterns linked means matching symbols parsing example shown figure column within alignment contain symbol regarded inference drawn alignment. example column figure contains symbol seen representing inference verb. chains reasoning abductive reasoning default reasoning nonmonotonic reasoning phenomenon explaining away solution geometric analogy problems. aspects icmaus framework explained quite fully examples wolﬀ wolﬀ argued icmaus framework provides interpretation turing model ‘computing’ equivalent models post canonical system. although icmaus model little complex earlier models appears illuminate range issues outside scope earlier models. wolﬀ argued icmaus framework provides interpretation range concepts mathematics logic including static structures found disciplines dynamics calculation inference. description model follows examples ‘linguistic’ ﬂavour emphasis simpler aspects english syntax. however term ‘grammar’ context construed broadly since noted section assumed similar principles applied many diﬀerent kinds knowledge. model gives results area unsupervised learning good enough show framework sound. shall model able abstract plausible grammars sets simple sentences without prior knowledge word segments classes belong computational complexity model appears acceptable however current form model least signiﬁcant shortcomings deﬁciencies. programme development experimentation reﬁnement still needed realise full potential model unsupervised learning. model governed mathematical principles—explained pertinent points below—but remarkably simple accordance established theory. main focus follows organisation model computational techniques employed within generalize grammatical rules beyond data correct over-generalizations without feedback ‘teacher’ provision ‘negative’ samples grading data ‘easy’ ‘hard’. solutions problems found snpr model noted earlier organisation model quite unsuited wider goals present research—integration diverse functions within framework. finding solutions problems within icmaus framework signiﬁcant challenge. model provides solutions ﬁrst three problems partial solutions fourth ﬁfth problems. development required achieve robust learning structures levels abstraction work required generalization grammatical rules correction overgeneralizations reading patterns system compiles ‘alphabet’ diﬀerent types symbols appearing patterns counts frequencies occurrence calculates encoding costs described below. values needed evaluation alignments. term symbol type connection means representative template example identical symbols. patterns selection best alignments found program ‘learns’ patterns explained below adds old. copy pattern also added marked symbols explained. function create multiple alignments referred figure creates zero multiple alignments comprising current pattern patterns old. alignment compression score measure amount compression cpfn achieved encoding terms patterns old. apart minor modiﬁcations improvements function essentially main component model described quite fully wolﬀ readers referred source detailed description multiple alignments formed icmaus framework. heart function process ﬁnding full alignments good partial alignments pairs patterns. process described quite fully wolﬀ version ‘dynamic programming’ advantages standard methods matching process applied iteratively build alignments containing rows pattern row. process stops alignments found compression scores reached peak fallen three cycles. ﬁrst cycle matching process applied alignments cpfn patterns stored old. completed sorted order compression scores several best selected processing. general alignments selected ones treated single sequence symbols described section below. second subsequent cycles building process three best alignments selected previous cycle chosen ‘driving’ patterns matched ‘target’ patterns comprising current patterns together alignments selected previous cycles resulting alignments driving pattern target pattern. ﬁrst cycle several best alignments formed selected processing provided treated single sequences symbols. qualiﬁcation described number bits required encode given symbol type appears calculated using shannon-fano-elias method method similar well-known huﬀman coding method gives similar results—but advantages huﬀman method codes used calculation probabilities noted above values frequencies symbol types required method computed patterns new. notice encoding cost symbol totally independent size symbol appears reader. general symbols represented character strings chosen reasons readability mnemonic value. strings quite independent number bits required discriminate symbol another eﬃcient manner. qualiﬁcation mentioned encoding costs symbol types appearing multiplied cost factor normally means symbols representing original ‘data’ program treated system relatively large chunks information—by contrast symbols used encode data given additional weighting. reason applying cost factor data symbols that without weighting best grammar found kind small example convenient experimentation demonstration often simply repetition original patterns without recognition structures within patterns. data symbols treated larger chunks information beneﬁts recognising substructures outweighs costs encoding structures. larger examples frequency values substructures normally higher problem disappear. copies them—are created system operation variety types symbols frequencies occurrence constantly changing. reason diﬃcult stage calculate encoding costs using method. accordingly encoding costs symbols created system initially ﬁxed arbitrary value. shall precise values calculated sifting sorting phase processing. approximation stage seem serious impediment learning perhaps selection alignments depends relative values compression scores absolute values. search alignments maximise conforms principles outlined section repository patterns taken current ‘grammar’ encoding cpfn given cpfn size grammar constant. since constant goal minimising equivalent goal minimising given cpfn thus since constant given cpfn seeking minimise equivalent attempt maximise important point notice multiple alignments icmaus framework pattern appear times alignment. example sentence like winds west strong instances simple form noun phrase that multiple alignment parsing sentence would appearances pattern representing structure form noun phrase. notice multiple appearances pattern within alignment multiple copies pattern within alignment. latter case distinct patterns quite acceptable fully aligned another. former case single pattern permissible symbol appearance matched corresponding symbol another appearance—because would mean matching given symbol itself. however permissible form match symbol within given pattern another symbol pattern. situation given pattern matched—directly indirectly— itself matching process create multiple alignments function constrained prevent symbol matched itself. indicated operation figure described section below copy cpfn added process building alignments. however purpose encoding cpfn entire copy counts part thus given cpfn indeed constant. mismatch alignment occurs unmatched symbols pattern appear opposite unmatched symbols another pattern within alignment. symbols ‘opposite’ columns matched symbols column matched symbols beginning alignment. icmaus framework mismatches illegal occur patterns old. alignment contains kind mismatch discarded. notice mismatch cpfn pattern legal shall kinds mismatches drive learning process. noted above process building multiple alignments requires alignment created intermediate stages treated single sequence symbols. critical issue whether given alignment contains illegal mismatches. contain illegal mismatches cannot treated single sequence. otherwise can. notice mismatches cpfn patterns consequence. example alignment figure treated sequence alignment ﬁgure treated unmatched symbols within cpfn simply ignored. bare essentials ‘learning’ achieved addition patterns old. occurs ways copying pattern deriving patterns alignments function derive patterns ﬁrst described second described next subsection. create multiple alignments function cpfn copied symbol time symbol cpfn matched earlier symbol copy cannot matched corresponding symbol copy subsequent symbol. transfer complete symbols added copy provide ‘code’ pattern described below. detect redundancy exist within pattern avoid detecting redundancy resulting fact cpfn copied old. constraint imposed much reason constraint prevents symbol within alignment matched itself. reason copying pattern rather simply moving pattern candidate inclusion best grammars selected sifting sorting function cannot evaluated properly unless copy corresponding pattern pattern symbols added copy cpfn comprise left right brackets pattern together symbols immediately left bracket serve identify pattern uniquely amongst patterns old. sake consistency derive patterns function symbols follow left bracket. thus example pattern like might become symbols added. operation figure derive patterns function applied selection best alignments formed case looks sequences unmatched symbols within alignment also sequences matched symbols. consider alignment shown figure alignment like that function ﬁnds unmatched sequences within also ﬁnds matched sequences respect focus interest matched unmatched sequences symbols—id symbols ignored. addition another ‘abstract’ pattern made records sequence matched unmatched patterns within alignment. result case patterns like shown figure clear patterns figure eﬀect simple grammar sentences figure patterns representing grammatical rules much style shown figure abstract pattern describes overall structure kind sentence slots receive individual words appropriate points pattern. system derives patterns much except unmatched sequence assigned class itself without alternative pattern appear context. arguably kind ‘null’ alternative cases like order capture idea apple green apple acceptable variants phrase. possible reﬁnement model future. main reason adopting style shown figure overall organisation model simpler newly-derived pattern automatically referenced contexts contexts appear. another reason anticipated that realistically large corpora patterns ultimately turn signiﬁcant terms principles appear contexts case principles likely dictate pattern referenced contexts rather written redundantly places appears. ‘class’ symbol normally starts character. class symbol eﬀect reference context contexts given pattern appear. thus example symbol ﬁrst pattern figure shows pattern appear matching symbol occurs pattern pattern belong class contain symbol classes belongs ‘discrimination’ symbol serves distinguish pattern others belong class. stage discrimination symbol simply unique identiﬁer given pattern amongst patterns alignments created program. alignments built coded patterns added class symbols discrimination symbols created quite liberally. however many symbols weeded sifting sorting phase processing remain renamed tidy manner. course deriving patterns alignments adding easily happen newly-derived pattern symbols already old. reason newly-derived pattern checked patterns already stored discarded existing pattern found symbols. although discarded pattern symbols pre-existing pattern comes diﬀerent context. symbol type created represent context copy symbol type added pre-existing pattern another copy symbol type added abstract pattern appropriate position. sequence symbols appear pattern containing several diﬀerent class symbols representing contexts symbols appear. program stands one-to-one relation contexts classes. easily happen patterns appear context patterns appear another. stage intended program augmented check kind redundancy merge classes turn equivalent. given alignment derive patterns function works looking unmatched sequences symbols cpfn sequences unmatched symbols pattern things. happens alignment contains patterns old? consider alignment shown figure case like this necessary identify patterns purpose deriving patterns alignment. pattern chosen deemed ‘abstract’ pattern amongst rows row. abstract alignment starts furthest left within alignment e.g. figure figure typically also ﬁnishes furthest right. general within alignment that directly indirectly encodes largest number symbols cpfn. course alignment contains rows abstract row. here unmatched sequence cpfn converted pattern system recognises lies opposite sequence within figure sequence reference class ‘%’. accordingly pattern assigned class. unmatched sequence opposite could recognised reference class unmatched sequence opposite system would create class patterns manner examples beginning section given system dedicated seem strange that stage processing considerable replication information amongst patterns old. patterns added stage nothing removed old. example considered pattern coexists pattern even though contain sequence example figures patterns coexist pattern despite obvious duplication information amongst patterns. reason designing system guarantee given pattern derived alignment ultimately turn ‘correct’ terms principles one’s intuitions correct grammar indeed many patterns abstracted system clearly ‘wrong’ terms. retention older patterns store alongside patterns derived leaves door open system create ‘correct’ patterns later stages regardless whether ‘wrong’ patterns identiﬁcation ‘wrong’ patterns occurs sifting sorting stage processing system develops alternative grammars patterns accordance principles. figure shows overall structure sifting sorting function. pattern associated frequency occurrence start function values zero. then patterns reprocessed create multiple alignments function building multiple alignments before pattern patterns old. diﬀerence occasion that cpfn best alignments ﬁltered remove contain unmatched symbols cpfn unmatched symbols pattern old. remaining ‘full’ alignments provide basis processing. maximum number times appears alignment subset using maximum value alignment given necessary alignments alternative analyses corresponding pfn. simply counted number times pattern appeared alignments given frequency values would high. values encoding cost symbol type computed using method encoding cost ‘data’ symbol types weighted data symbols behave relatively large chunks information. program stands alternative grammars simply presented user inspection. however intended patterns purged patterns except best grammar found. anticipated program developed patterns processed batches kind purging occur batch remove ‘rubbish’ retain patterns proved useful encoding cumulative patterns new. given grammar pattern derived full alignments simply listing patterns appear alignment counting multiple appearances pattern one. grammar augmented cover additional selecting full alignments second adding patterns appear within alignment already present grammar taking time grammar compiled patterns new. complication course often full alignments given pfn. means that given patterns generate tree alternative grammars branching occurring wherever alternative alignments given pfn. without constraints tree become unmanageably large. compile alternative grammars function tree alternative grammars pruned periodically keep within reasonable bounds. tree grown successive stages stage processing alignments patterns grammar processing alignment pfn. values calculated grammar stage grammars high values eliminated. present frequencies derived entire patterns would probably appropriate frequency values derived grammar individually stage process compiling grammars. results likely similar cases since encoding costs depend relative frequency values absolute values symbol types appearing grammar likely ranking frequency values derived grammar would similar ranking derived entire patterns old. produce knowledge structures look ‘natural’ ‘reasonable’ since humans ‘powerful’ learning systems planet justiﬁcation using human intuitions structuring information touchstone success failure artiﬁcial learning system. given evidence human cognition conditioned principles human intuitions provide indirect check whether principles successfully implemented model. ﬁrst criteria discussed next subsection. evidence bearing second criterion presented section below intuitive plausibility results obtained model considered various points sections attempt made evaluate terms fourth criterion. common programs unsupervised learning attempt theoretically ideal solutions. abstract space possible grammars normally large searched exhaustively. general heuristic techniques like hill climbing genetic algorithms simulated annealing must used. using techniques time complexity create multiple alignments function estimated approximately size pattern lengths patterns parallel processing environment time complexity approach depending well parallel processing applied. serial parallel environments space complexity initially empty grows learning proceeds. size approximately linear function size new. given growth size time required create alignments given pattern grow learning proceeds. again relationship approximately linear. ignore operations create multiple alignments function estimate time complexity program number patterns new. parallel processing environment time complexity approach depending well parallel processing applied. serial parallel environments space complexity time complexity program improved developed envisaged patterns processed batches purging batch remove patterns except best grammar. case size batch patterns approximately constant running time program roughly proportional number batches patterns. there requirement model patterns should example complete sentences. equally well arbitrary portions incoming data perhaps measured kind input buﬀer. best grammars found program sentences shown figure best grammar shown form ﬁrst compiled grammar shown cleaned figure shows second-best grammar cleaned cleaning grammars means removing class symbols referents renumbering class symbols discrimination symbols starting set. renumbering purely cosmetic matter makes diﬀerence encoding cost calculated symbol. grammars shown figure reasonably plausible grammars four original sentences. best grammar picked discrete words assigned grammatical class similar picked discrete entity—corresponding ‘stem’ verb—and assigned class ‘%’. suﬃx verb stems picked distinct entity overall sentence structure captured pattern second best grammar except suﬃx verb separated stem. ﬁrst pattern processed empty except copy ﬁrst pattern added symbol time pattern processed. alignment formed stage shown figure remember symbols must aligned corresponding symbols pattern because eﬀect would mean matching symbol evident that stage opportunities gain useful insights overall structure patterns quite limited. ‘bad’ alignment shown figure program abstracts ‘bad’ patterns applied alignments patterns words ‘bad’ ‘good’ shorthand bad/good terms principles perhaps also terms one’s intuitions appropriate grammar data. quote marks dropped remainder paper. second alignment system derives pattern would create pattern detects pattern symbols already exists however since occur context assigned context-deﬁned class. accordingly program adds class symbol pattern becomes creates abstract pattern tying whole structure together. value best grammar bits cleaning bits cleaning second-best grammar corresponding values contrast calculated bits ‘naive’ grammar comprises four patterns added symbols figure shows values change successive patterns processed alternative grammars compiled. point lower three graphs represents relevant value best grammar found full alignments given processed. graph shows successive values ‘naive’ grammar mentioned above. clearly recognises verb stems distinct entities. reasons still entirely clear program build entities plausible versions full sentence structure. program isolates pattern alignment shown reason fails internal structure within pattern—although recognises contexts. issue discussed section below. failure model internal structure within sentence example seems manifestation general shortcoming. although model current form isolate basic segments together overall abstract structure good ﬁnding intermediate levels abstraction. seems needed kind additional reprocessing patterns including abstract patterns added discover partial matches detected initial processing patterns new. allow system detect intermediate levels structure phrases clauses structures exist within smaller units words. development model date attempt made enable system detect discontinuous dependencies number dependency between subject sentence main verb gender dependencies languages like french. although kind capability seem like reﬁnement aﬀord without stage development deﬁciency area seems impact program’s performance elementary level. even quite simple structures dependencies exist bridge intervening structure current form program encode kind information satisfactory manner. well-documented phenomenon young children learn language things like ball hitted look gooses apparently applying general rules constructing words applying generally. children eventually learn avoid kinds overgeneralizations? tempting suppose children corrected parents adults weight empirical evidence that corrections helpful actually necessary language learning. principles provide elegant solution puzzle. without kind feedback supplementary information postulated gold possible search grammars good terms normally ones steer path generalizations intuitively ‘correct’ others appear ‘wrong’. kind eﬀect demonstrated snpr model ﬁrst three four patterns shown figure best grammar found exactly grammar generates missing sentence well three sentences generate anything else. example model generalizes seems intuitively correct avoids creating overgeneralizations glaringly wrong. however relatively little attention given aspect model work required. particular better understanding needed alternative ways grammatical rules generalized. present program applies create multiple alignments function twice part process generating patterns added sifting sorting phase. seems possible phases could integrated create multiple alignments function need applied pfn. although computational complexity model serial machine within acceptable limits improvements area higher absolute speeds obtained application parallel processing. residual problems model solved envisaged system developed software virtual machine existing high-parallel hardware perhaps forms hardware dedicated needs model. possible exploit optical techniques achieve high-parallel matching patterns core model. tentative suggestion motivations emotions impact patterns purged system. things equal suppose principles govern choice patterns retained discarded. pattern represents something special signiﬁcance retained system even score well terms measures. although still short ‘industrial strength’ system unsupervised learning results obtained good enough show general approach sound. problems identiﬁed appear soluble. particular attraction approach learning icmaus framework provides uniﬁed view variety issues thus facilitating integration learning aspects intelligence.", "year": 2003}