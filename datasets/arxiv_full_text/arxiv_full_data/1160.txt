{"title": "Joint Line Segmentation and Transcription for End-to-End Handwritten  Paragraph Recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition. In this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can recognize one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM database yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.", "text": "ofﬂine handwriting recognition systems require cropped text line images training recognition. hand annotation position transcript line level costly obtain. hand automatic line segmentation algorithms prone errors compromising subsequent recognition. paper propose modiﬁcation popular efﬁcient multidimensional long short-term memory recurrent neural networks enable end-to-end processing handwritten paragraphs. particularly replace collapse layer transforming two-dimensional representation sequence predictions recurrent version recognize line time. proposed model neural network performs kind implicit line segmentation computing attention weights image representation. experiments paragraphs rimes database yield results competitive networks trained line level constitute signiﬁcant step towards end-to-end transcription full documents. ofﬂine handwriting recognition consists recognizing sequence characters image handwritten text. traditional approaches contain ﬁrst segmentation step followed transcription step. unlike printed texts images handwriting diffucult segment characters. early methods tried compute segmentation hypotheses characters example performing heuristic over-segmentation followed scoring groups segments nineties kind approach progressively replaced segmentation-free methods whole word image system providing sequence scores. lexicon constrains decoding step allowing retrieve character sequence. examples sliding window approach features extracted vertical frames line image spacedisplacement neural networks last decade word segmentations abandoned favor complete text line recognition statistical language models nowadays standard handwriting recognition systems multi-dimensional long short-term memory recurrent neural networks consider whole image alternating mdlstm layers convolutional layers. transformation structure sequence computed simple collapse layer summing activations along vertical axis. conversion sequence predictions sequence characters achieved simple mapping involving non-character label allowing consider possible character segmentations training connectionist temporal classiﬁcation loss models become popular recent evaluations handwriting recognition however current models still need segmented text lines full document processing pipelines include automatic line segmentation algorithms. although segmentation documents lines assumed descriptions handwriting recognition systems several papers surveys state crucial step handwriting text recognition systems need line segmentation train recognition system also motivated several efforts paragraph-level page-level transcript line positions image paper pursue traditional tendency relax hard segmentation hypotheses handwriting recognition systems character word segmentation full text lines consistently improved performance. propose model multi-line recognition based popular mdlstm-rnns augmented attention mechanism inspired recent models machine translation image caption generation speech recognition proposed model collapse layer modiﬁed attention network providing weights modulate importance given different positions input. iteratively applying layer paragraph image network transcribe text line turn enabling purely segmentation-free recognition full paragraphs. carried experiments public datasets handwritten paragraphs rimes iam. report results competitive state-of-the-art systems ground-truth line segmentation. remaining paper organized follows. section presents methods related presented here terms tackled problem modeling choices. section introduce baseline model mdlstm-rnns. expose section proposed modiﬁcation give details system. experimental results reported section followed short discussion section explain system could improved present challenge generalizing complete documents. work clearly related mdlstm-rnns improve replacing simple collapse layer elaborated mechanism made mdlstm layers. model propose iteratively performs implicit line segmentation level intermediate representations. classical text line segmentation algorithms mostly based image processing techniques heuristics however methods devised using statistical models machine learing techniques hidden markov models conditional random ﬁelds neural networks model line segmentation performed implicitely integrated neural network. intermediate features shared transcription segmentation models jointly trained minimize transcription error. ﬁeld computer vision particularly object detection recognition many neural architectures proposed locate recognize objects overfeat spatial transformer networks although systems able detect multiple similar objects scene methods localize object several objects different. scene text recognition maybe topic computer vision closest problem systems still rely two-step process even though approaches jointly optimize character segmentation word recognition recently many attention-based models proposed iteratively select encoded signal relevant parts make next prediction. paradigm already suggested fukushima successfully applied various problems machine translation image caption generation speech recognition cropped words scene text works localization implicitely performed inside neural network. papers present similar methods read short sequence characters different implementations attention e.g. draw recurrent spatial transformer networks recently proposed attention-based model transcribe full paragraphs handwritten text predicts character turn encoded image summarized single vector timestep sequence vectors representing full text lines. represents huge speedup factor comeback original mdlstm-rnn architecture collapse layer augmented mdlstm attention network similar presented section brieﬂy present mdlstm-rnns mdlstm layers generalize lstms two-dimensional inputs. ﬁrst introduced context handwriting recognition. general architecture displayed figure figure mdlstm-rnn achitecture handwriting recognition. lstm layers four scanning directions followed convolutions. feature maps layer summed vertical dimension character predictions obtained softmax normalization mdlstm layers scan input four possible directions. lstm cell inner state output computed states outputs previous positions horizontal vertical directions. lstm layer followed convolutional layer. resolution learnt representations decreased setting step size convolutional ﬁlters greater one. size feature maps decreases number extracted features increases. network feature character. collapse layer sums features along vertical axis yielding sequence prediction vectors normalized softmax activation. order transform sequence predictions sequence labels additionnal non-character label introduced simple mapping deﬁned retrieve transcription. connectionist temporal classiﬁcation objective considers possible labellings sequence applied train network recognize text lines. conversion happens collapsing layer applies simple aggregation feature maps vector sequences i.e. maps height achieved simple across vertical dimension i-th output vector input feature vector coordinates information vertical dimension reduced single vector regardless position feature maps preventing recognition multiple lines within framwork. scalar weights computed every time position weights computed recurrent neural network illustrated figure enabling recognition text line timestep. figure proposed modiﬁcation collapse layer. standard collapse computes simple weighted collapse includes neural network predict weights weighted sum. collapse weighted neural network interpreted attention module attention-based neural network similar mechanism differentiable trained backpropagation. weighted collapse attention mechanism providing view encoded image timestep form weighted feature vector sequences. attention network computes score feature vectors every position module applied several times features encoder. output attention module iteration computed eqn. sequence feature vectors intended represent text line. therefore module soft line segmentation neural network. advantages neural networks trained line segmentation works features used transcription trained maximize transcription accuracy standard mdlstm architecture section decoder simple softmax. however bidirectional lstm decoder could applied collapsed representations. particularly interesting proposed model blstm would potentially process whole paragraph allowing modeling dependencies across text lines. figure training strategies. ground-truth available line level objective function applied line segments independently ground-truth available paragraph level objective applied concatenation line predictions model trained ctc. line breaks known transcript could applied segments corresponding line prediction line transcript. moreover enforce prediction timestep correspond complete text line. otherwise directly apply whole paragraph. different training strategies model illustrated figure work mainly investigated second strategy training paragraph level blstm decoder applied concatenation collapsing steps reasons developed next section. compared model presented iterative decoder requires step text line instead step character represents huge speedup factor however loose ability handle arbitrary reading orders. moreover version model predict stop token. thus network predicts arbitrary number sequence ﬁxed experimenter. experiments corresponds maximum number lines dataset. blstm decoder applied sequences efﬁcient ignore supplementary lines shorter paragraphs. numerous cases observed additional steps attention located interlines decoder easily predict non-characters. however missing ability determine automatically number required steps important limitation ﬁxed future work. finally collapsing paradigm forces model output sequences span whole width image. replace column-wise softmax eqn. sigmoid ignore parts input shorter lines example reﬁned mechanism selects portion image become crucial handle complete documents complex layouts. issue discussed details section carried experiments public databases. database made handwritten english texts copied corpus. documents training documents validation documents test set. rimes database contains handwritten letters french. data consists training paragraphs test paragraphs held last paragraphs training validation set. networks following architecture. encoder ﬁrst computes tiling input alternate mdlstm layers units convolutions ﬁlters overlap. last layer linear layer outputs rimes. attention network mdlstm network units direction followed linear layer output softmax columns decoder blstm network units. networks trained rmsprop base learning rate mini-batches examples minimize loss entire paragraphs. following study impact adding blstm decoder attention-based collapse compare method baseline results automatic ground-truth line segmentation present comparison system state explained section model weighted collapse method followed blstm decoder. experiment compare baseline system proposed model. order dissociate impact weighted collapse blstm decoder also trained intermediate architecure blstm layer standard collapse still limited text lines. database collapse decoder standard softmax standard blstm softmax attention blstm softmax softmax standard standard blstm softmax attention blstm softmax character error rates validation sets reported table images. observe proposed model outperforms baseline large margin gain attributed blstm decoder attention mechanism. model performs implicit line segmentation transcribe paragraphs. baseline considered last section somehow cheating evaluated ground-truth line segmentation. experiment comparison baseline models evaluated real scenario applied result automatic line segmentation algorithm. table character error rates ctc-trained rnns ground-truth lines automatic segmentation paragraphs different resolutions. last column contains error rate attention-based model presented work without explicit line segmentation. table report cers obtained ground-truth line positions three different segmentation algorithms end-to-end system validation sets databases different input resolutions. applying baseline networks automatic segmentations increases error rates absolute best case. also observe models better higher resolutions. models yield better performance methods based explicit automatic line segmentation comparable better results ground-truth segmentation even resolution divided two. figure display visualisation implicit line segmentation computed network. color corresponds step iterative weighted collapse. images color represents weights given attention network texts predicted transcriptions chunks colored according corresponding timestep attention mechanism. section also compute word error rates evaluate models test sets order compare proposed approach existing systems. applied gram language model lexicon words trained brown wellington corpora. language model perplexity rate validation results presented table rimes table different input resolutions. comparing error rates important note systems litterature used explicit line segmentation language model. used hybrid character/word language model tackle issue out-of-vocabulary words. moreover systems except carefully pre-processed line image whereas normalized pixel values zero mean unit variance. finally combination four systems. rimes system applied images already outperforms state cer% competitive terms wer%. system images comparable best single system wer% signiﬁcantly better cer%. language model turned quite important probably variability language. images results state results. wer% improve much images lower cer%. analysing errors noticed punctuation often missed attention mechanism. already discussed section proposed model transcribe complete paragraphs without segmentation orders magnitude faster model however mechanism cannot handle arbitrary reading orders. rather implements sort implicit line segmentation. current implementation iterative collapse runs ﬁxed number timesteps. model handle variable number text lines interestingly focus interlines additional steps. elegent solution include prediction binary variable indicating stop reading. method applied paragraph images document layout analysis applied detect paragraphs applying model. naturally next step transcription complex documents without explicit assumed paragraph extraction. limitation paragraphs inherent system. indeed weighted collapse always outputs sequences corresponding whole width encoded image which paragraphs correspond text lines. order switch full documents several issues arise. first size lines determined size text block. thus method devised select smaller part feature maps representing considered text line. possible presented framework. potential solution could come spatial transformer networks performing differentiable crop. however method based learning grid transformation ﬁxed grid size would like crop variable-sized parts. another solution would hierarchical comprise ﬁrst attention text block level second line level inside block. note would probably still need crop text block. different direction could also abandon differentiability requirement learn predict crops reinforcement learning techniques. hand training practice become difﬁcult complexity task also reading order complex documents cannot exactly inferred many cases. even deﬁning arbitrary rules tricky. therefore matching predictions ground-truth texts addressed. finally would like point important factors take account training presented model. training difﬁculties good alignments network predict actual characters non-character symbols convergence much faster pre-trained encoder. example ﬁrst train mdlstm-rnn standard collapse text lines ﬁnetune attention-based collapse paragraphs second step. however training attention model full paragraphs directly actually easy found curriculum methods useful. switching full paragraphs train epochs three lines initiate attention mechanism. also taken account complete documents. good curriculum harder design probably crucial. nonetheless amount data used experiments quite limited careful training might become less important data. presented model transcribe full paragraphs handwritten texts without explicit line segmentation. contrary classical methods relying two-step process system directly considers paragraph image without elaborated pre-processing outputs complete transcription. proposed simple modiﬁcation collapse layer standard mdlstm architecture iteratively focus single text lines. implicit line segmentation learnt backpropagation along rest network minimize error paragraph level. reported comparable error rates state public databases. switching explicit implicit character word segmentation handwriting recognition showed line segmentation also learnt inside transcription model. next step towards endto-end handwriting recognition full page level.", "year": 2016}