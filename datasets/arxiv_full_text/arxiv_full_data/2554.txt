{"title": "Debugging Machine Learning Tasks", "tag": ["cs.LG", "cs.AI", "cs.PL", "stat.ML", "D.2.5; I.2.3"], "abstract": "Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets.", "text": "unlike traditional programs large amounts code machine learning tasks programs relatively small amounts code voluminous amounts data. like developers traditional programs debug errors code developers machine learning tasks debug errors data. however algorithms tools debugging ﬁxing errors data less common compared counterparts detecting ﬁxing errors code. paper consider classiﬁcation tasks errors training data lead misclassiﬁcations test points propose automated method root causes misclassiﬁcations. root cause analysis based pearl’s theory causation uses pearl’s scoring metric. implementation encodes computation probabilistic program uses recent work probabilistic programs transformations probabilistic programs eﬃciently compute able identify root causes data errors interesting data sets. machine learning techniques used perform data-driven decision-making large number diverse areas including image processing medical diagnosis credit decisions insurance decisions email spam detection speech recognition natural language processing robotics information retrieval online advertising. time techniques honed tuned stage machine learning libraries used black-boxes programmers little expertise details machine learning algorithms themselves. black-box nature reuse however unfortunate downside. current implementations machine learning techniques provide little insight particular decision made. absence transparency debugging outputs machine learning algorithm become incredibly hard. programmers implement machine learning libraries build models voluminous training data models perform predictions. machine learning libraries often employ complex stochastic approximate search optimization algorithms search optimal model given training data set. model applied unseen test samples hope satisfactory generalization. generalization fails i.e. incorrect result produced test input often diﬃcult debug cause failure. failures arise several reasons. common causes failure include bugs implementation machine learning algorithm incorrect choice features incorrect setting parameters invoking machine learning library noise training set. time bugs implementation machine learning algorithms detected ﬁxed. work feature selection parameter choices made systematically building models various parameter values choosing model best validation score however since training data typically voluminous errors training data common notoriously diﬃcult debug. suggests class debugging problems programs learnt data bugs program result faults data. paper focus debugging machine learning tasks presence errors training data. speciﬁcally consider classiﬁcation tasks typically implemented using algorithms logistic regression boosted decision trees suppose train classiﬁer training data classiﬁer produces incorrect results test points. desire produce automated procedure identify root cause failure. would like identify subset training points inﬂuences classiﬁcation test points most. therefore correcting mistakes training points likely incorrect results. algorithm identifying root causes inspired structural equations framework causation formulated judea pearl think training data points possible causes misclassiﬁcation test data calculate training point score corresponding likely current label point cause misclassiﬁcation test data set. simple measure score training point obtained merely ﬂipping label training point observing improves results classiﬁer test points. however simple measure work errors exist several training points several training points together cause incorrect results test points. thus score calculate training point considers alternate counterfactual worlds training points labeled several possible values sums probability ﬂipping label causes misclassiﬁcation error test data among alternate worlds. pearl’s framework score called probability suﬃciency short. main diﬃculties calculating probability suﬃciency classiﬁer needs relearnt alternate worlds. model computing steps expensive. gray view machine learning library proﬁle intermediate values initial training phase. using values build gray-box abstraction training process model training obtained eﬃciently without need perform complete retraining. finally able amortize cost computing score sharing common work across computation diﬀerent training points. order carry optimizations model computation probabilistic program probabilistic programs allow represent optimizations using gray-box models using instrumented values actual training runs sharing work across multiple computations program transformations. also able leverage recent progress eﬃcient inference probabilistic programs scale computation scores large data sets. implemented root cause detection algorithm tool psi. currently works popular classiﬁers logistic regression boosted decision trees. classiﬁers runs production quality implementation techniques proﬁles speciﬁc values builds abstract gray-box model classiﬁer avoids expensive re-training. armed gray-box model performs scalable inference compute values points training set. able identify root causes misclassiﬁcations several interesting data sets. summary main contributions paper follows ables leverage eﬃcient techniques developed perform inference probabilistic programs calculate scores. build gray-box models machine learning techniques proﬁling actual training runs library using proﬁled values build abstract models training process. amortize work across computations diﬀerent training points. probabilistic programs allow carry optimizations reason program transformations. boosted decision trees. able identify root causes misclassiﬁcations several interesting data sets. hypothesize approach generalized machine learning tasks well. alice machine learning expert needs write classiﬁer images vehicles animals. mallory machine learning expert built classiﬁcation library using state machine learning techniques. alice decides mallory’s library since machine learning libraries driven stating whether image vehicle animal respectively. test picks favorite algorithm logistic regression learn binary classiﬁer separates vehicles animals. figure classiﬁcation example. cars incorrectly labeled animals training set. thus learned classiﬁer incorrectly classiﬁes cars test data animals. errors training ﬁxed correct classiﬁer learnt. goal identify errors training eﬃciently. cause misclassiﬁcation case cars training data classiﬁed animals error script collected training data shown figure since test instance classiﬁer incorrectly classiﬁes animal. alice’s tried-and-tested approach debugging start point error work backward portions code error logistic regression uses training data model n-dimensional vector number features training data. model calculated test input sigmoid function applied output given test point depends indices vector turns depends elements training set. thus unfortunately tracing backward program dependences enable alice narrow root cause failure. next alice turns experimental approaches answering classiﬁer picks training data points random changes labels looks outcome rerunning logistic regression. ﬁnds changing label single training point makes diﬀerence classiﬁer. hence chooses subsets training points changes labels simultaneously reruns training phase observes resulting classiﬁer correctly classiﬁes ﬁnds switching label sets training data points include several cars much greater inﬂuence classiﬁcation sets points. given combinations causes involved wonders systematic identify possible causes. black-box experiment alice runs time consuming since training expensive. thus alice wonders better perform experiments systematically eﬃciently. goal identify root causes systematically eﬃciently carrying experimentation approach alice attempts main diﬃculties alice faces experimentation changing single training instance error. hand trying identify subsets training points error infeasible explosively large number possible subsets points. experiment takes long time since reruns training algorithm scratch. consider diﬃculties detail. order counter ﬁrst diﬃculty instead considering sets training points measure inﬂuence individual points causing classiﬁcation. pearl’s theory probabilities causation formalizes inﬂuence training instance misclassiﬁed considering possible labellings training points possible worlds measuring number worlds changing label changes outcome classiﬁcation spirit perform experiments large number alternate worlds world evaluate world changing label inﬂuences classiﬁcation given world perform experiment simultaneously training points record training points inﬂuence classiﬁcation world repeat entire experiment several alternate worlds compute aggregate score training point alternate worlds. aggregate score called probability suﬃciency second diﬃculty cost building model experiment black-box experimentation scale huge number training points running entire implementation machine learning algorithm experimental trial takes time. potentially take white-box view since access source; however incorporating full details source poses scalability issue even state-of-art inference techniques. practical compromise take gray-box view machine learning implementation proﬁle intermediate values initial training phase. using proﬁling information able intermediate values runs training eﬃciently calculate value related training diﬀer small number labels. exact values proﬁled nature gray-box model depends speciﬁc machine learning algorithm. section show gray-box models logistic regression decision trees. probabilistic program computation. model computation score probabilistic program writing computation probabilistic program enables techniques developed probabilistic program inference calculate also enables concretely represent reason various optimizations calculate program transformations. probabilistic program write score models bernoulli distribution mean given value inference probabilistic program gives estimate value. empirical results. implemented approach tool psi. section show empirical evaluation using synthetic benchmarks well real-world data sets. introduce random errors training data synthetic benchmarks systematic errors training data real-world data sets. able identify signiﬁcant fraction systematically introduced errors using calculated scores. randomly introduced errors performance depends nature benchmark. instance benchmark already inherent noise adding extra noise really change classiﬁer hence enough information perform root causing. also evaluate eﬀect changing labels training points high scores validation score. validation score improves monotonically make changes points high scores starts degrade change labels scores conﬁrming higher scores likely candidates root causing errors. also using values multiple test points enables root causes well improve validation score. section details. section formal details machine learning tasks consider. introduce intuition formalization probabilistic causality framework propose. also introduce probabilistic programs speciﬁcation mechanism probabilistic causality. consider classiﬁcation tasks supervised machine learning formally machine learning task consists training test labeled samples called feature vectors called classiﬁcation labels example consider scenario people vote government; choices ﬁnal count votes people voted voted everyone voted contributed winning; however individual voter appears sole cause theory causality counterfactuals proposes solution apparent predicament considering alternate worlds individual vote indeed aﬀects outcome. informally vote cause exists alternate world alternate world outcome decided vote alone. alternate worlds called counterfactuals considering existence counterfactual worlds helps establish causality qualitative sense useful consider quantitative measure causation. example consider presidential election diﬀerent states diﬀerent numbers electoral votes. california electoral votes whereas wyoming electoral votes. intuitively california voting signiﬁcant cause winning wyoming voting notion captured considering number counterfactual worlds outcome aﬀected. number alternate worlds california aﬀects outcome election greater number alternate worlds wyoming aﬀects outcome election. next focus formally stating deﬁnition. consider counterfactual possibilities need express quantities following form value would obtained world tool expressing counterfactuals intervention. intervention presentation simpliﬁcation structural model semantics causality suitable application. refer reader comprehensive introduction counterfactual causality. conditional probability given conditional probability appear zero ﬁrst glance counterfactual interpretation actually evaluated diﬀerent assignment input variable fact conditional probability known probability suﬃciency quantitative measure measure causality setting. results changing voting example represents individual vote represents outcome election worlds ones individual vote aﬀects outcome election. therefore probability suﬃciency voter voted probability intuitively larger number electoral votes worlds satisfy condition greater thereby leading greater value california electoral votes wyoming electoral votes. probabilistic programs usual programs additional constructs sample statement provides ability draw values distributions observe statement provides ability condition values variables. purpose probabilistic program implicitly specify probability distribution. probabilistic inference problem computing explicit representation probability distribution implicitly speciﬁed probabilistic program. consider probabilistic program shown figure program tosses fair coins assigns outcomes coin tosses boolean variables observe statement line blocks executions ﬁrst deﬁne causal model training label associate input random variable model. thus lower-case variables denote instances upper case variables corresponding random variable. probability distribution represents prior beliefs training labels based training structural equation simply machine learning algorithm succinct representing given probabilistic program shown figure notation represents labels training line reassigns labels sample labels distribution assignment represents diﬀerent world observe statements lines reﬂect counterfactual essence probability suﬃciency. correspond conditions equation line models intervention line reruns learning algorithm changed training return value probabilistic program bernoulli random variable expresses precisely quantity deﬁned equation specifying probabilistic program allows leverage recent advances inference techniques probabilistic programs well apply various program transformations enable eﬃcient scalable inference. section showed computation encoded probabilistic program. thus implement computing using recent advances inference techniques probabilistic programs techniques estimate posterior probability distribution probabilistic programs using sampling. however directly performing inference probabilistic program figure intractable following reasons noise level assume small number training points mislabeled. encoded distribution decays fast number mislabeled points grows. thus restrict samples small portion space possible sample reuse address challenge observe samples drawn label reused labels. think following transformation probabilistic programs move observe statement line figure observe statement line rewrite observe statement conditional even though programs figure figure equivalent program figure performs important optimization. since lines figure work executing statements shared across then sample generated quick check condition line satisﬁed execute lines algorithms generally robust small subset input changes internal computations leading ﬁnal classiﬁer change signiﬁcantly. fact insensitive small changes inputs desirable machine learning algorithms reduce overﬁtting. thus design approximate causal models widely used classiﬁcation algorithms logistic regression decision trees enable eﬃcient computation below. classiﬁer learnt iterative gradient descent process iteratively ﬁnds better classiﬁer data. stochastic gradient descent classiﬁer improved iterating following update equation here known step size number training instances. vector equation represents update component penultimate iteration gradient descent i.e. mation ﬁnal classiﬁer diﬀerent labelling training labels decision trees tree-shaped statistical classiﬁcation models internal tree node represents decision split feature value associated leaf node score score particular point computed evaluating decision internal node taking corresponding branches leaf reached. leaf score returned score leaf viewed region feature space tree partitioning number trees called ensemble iteratively learnt gradient descent process. informally trees learnt tree learnt ﬁtting decision tree error residual training label error residual thought diﬀerence aggregate score decision trees learnt true label. iterative process stops ﬁxed number learning rounds. here number decision trees learnt ensemble number leaves tree number training instances. region represented leaf tree ¯yni error residual training label iterations. linear computation {yi}i coeﬃcients precomputed misclassiﬁed written linear constraint training labels nonzero corresponding training point test point belong leaf tree hence small number training labels actually aﬀect classiﬁcation training point logistic regression record iterates line search steps stochastic gradient descent variant search algorithm. additionally memoize classiﬁcation scores reuse across diﬀerent training labels training algorithm order compute scores training training instances sorted highest ranked score instances returned proposed causes. user examines instances closely ﬁxes labeling inconsistency detect re-runs modevaluate applicability respect diﬀerent debugging requirements identifying errors training data reducing errors unseen data. evaluate metrics follow workﬂow described figure ﬁrst noise training labels dataset. perturbation introduces misclassiﬁcations test set. misclassiﬁcations goal ﬁnding likely training instances cause misclassiﬁcations make following measurements reducing validation error. introduce separate validation instances independent training test sets. measure accuracy classiﬁer learnt noisy training versus classiﬁer learnt noisy training probable causes suggested ﬂipped. measure reduction errors second classiﬁer respect ﬁrst. table shows datasets study. consider kinds datasets real-world data synthetic data. ﬁrst rows table real-world datasets sentiment imdb movie review dataset used sentiment analysis income census income dataset used predicting income levels. third fourth rows synthetic datasets gauss concentric produced generating data spherical gaussian distributions. also consider kinds noise errors datasets systematic noise random noise. identify systematic noise signiﬁcant accuracy thereby leading reduction classiﬁcation errors unseen data. also observe adding random noise lead signiﬁcant increase errors therefore causal analysis room identify errors. results experiments summarized table explained detail sections additionally observe combining information multiple misclassiﬁcations leads better noise identiﬁcation. systematic noise sentiment income datasets simulating systematic errors data collection process. sentiment pick word appears percent instances mark negative reviews. income pick demographic covers population mark high income. table summary experimental results. dataset report accuracy change validation error logistic regression decision trees corresponding runtimes minutes. validation errors sequence classiﬁers learnt noise free dataset noisy dataset noisy dataset psi’s suggestions ﬂipped. noise added validation error increases. instance sentiment dataset logistic regression validation error increases able successfully identify systematic noise datasets. instance sentiment dataset logistic regression returns causes points incorrectly labeled dataset. call number accuracy. decision trees accuracy accuracy results income benchmark even better—% logistic regression decision trees. note datasets noise baseline randomly picking noise would yield accuracy discuss reasons behind diﬀerence accuracy algorithms section systematic errors real world dataset using psi’s output training labels leads reduction validation error even points suggested true errors. instance sentiment income benchmarks decision trees validation error reduces respectively synthetic datasets gauss concentric randomly labels points noise dataset. synthetic datasets evaluate random noise rule preexisting systematic noise. random noise cleanly separable dataset gauss however inherent noise concentric dataset added noise cannot distinguished existing noise using causal measures. observe validation error changes little random noise added dataset misclassiﬁcations introduced. explains causal analysis depends observing changes outcome inputs perturbed identify random noise correctly. however presence inherent noise random noise stands easy identify case gauss. individual misclassiﬁcation. sentiment precision improves adding test points. encouraging— evidence test errors able better root causing indicates robustness analysis. study sorting training points based score measuring validation error points starting point highest score iteratively ﬂipping points. figure shows interesting trend validation error reaches minimum threshold changing labels detrimental eﬀect. plotting variation validation error threshold part debugging process could provide empirical method choosing optimal thresholds. explain logistic regression decision trees behave diﬀerently causal analysis important understand training points inﬂuence score particular test point. logistic regression single hyperplane best separates training set. therefore changing label training point aﬀects hyperplane hence score small degree. decision trees hand divide feature space smaller regions training points region contribute score test point. eﬀect actual points aﬀect outcome test point much smaller decision trees logistic regression. program analysis. program slicing computes subset statements program inﬂuence variable program point. since slicing used eﬀectively reduce size code analysis many applications software debugging unfortunately described section slicing eﬀective debugging machine learning tasks data slice inﬂuences test error usually whole training set. contrast based formal notion causality introduced uses eﬀectively isolate training data slices responsible test errors. software model checker generate correct incorrect traces compute diﬀerences traces localize software defects. unlike work looks bugs code look bugs training data. however consider correct incorrect traces consider misclassiﬁed test data. also make correctly classiﬁed test data leave idea future work. delta-debugging. debugging technique analyzes diﬀerences failing passing runs program detect reasons failure setting cause deﬁned smallest part program state smallest part input changed converts passing failing run. discussed section main challenge setting usually several causes misclassiﬁed test point clear delta-debugging search possible subsets misclassiﬁed training labels. instead pearl’s score unique work. statistical debugging. project variants information collected program runs together statistical analysis order compute predicates highly correlated failures. technique also requires information large number passing failing runs program applicable bugs code large number users using code triggering bug. contrast assume machine learning code implemented correctly bugs training data. addition user training data could errors diﬀerent problem setting addressed statistical debugging. terfactual theories causation ﬁrst proposed lewis particular book pearl formulates structural equations mathematical framework reasoning causality. actual causation. application structural equations framework halpern pearl’s deﬁnition actual causation setting actual causation aims identify input variables actually caused outcome. hp-deﬁnition able explain large variety subtle issues around causality operationally intractable employ scenario since verify whether alternate world satisﬁes deﬁnition needs consider exponentially large number possibilities. additionally hp-deﬁnition qualitative suggest quantitative measure causality necessary rank likely causes. unlike debuggers coding errors ubiquitous debuggers data errors less common. consider machine learning tasks particular classiﬁcation tasks incorrect classiﬁers inferred errors training data. proposed approach based pearl’s theory causation speciﬁcally pearl’s score rank training points likely causes arrived incorrect classiﬁer. score easy deﬁne expensive compute. tool employs several optimizations scalably compute scores including modeling computation score probabilistic program exploiting program transformations eﬃcient inference techniques probabilistic programs building gray-box models machine learning algorithms optimizations able correctly root cause data errors interesting data sets. work opens several opportunities interesting future work. immediate next step consider tasks involving algorithms support vector machines neural networks order this need design develop scalable approximate models algorithms. another interesting direction support wider class causes. potential causes include identifying feature test causes misclassiﬁcation overﬁtting learning algorithm incorrect parameters learning algorithm others. practical dimension explore scale. able study datasets thousands points using psi. industrial big-data systems millions points work needs done scale root cause analysis techniques work scale.", "year": 2016}