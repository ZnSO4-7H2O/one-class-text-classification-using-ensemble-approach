{"title": "Exact and Consistent Interpretation for Piecewise Linear Neural  Networks: A Closed Form Solution", "tag": ["cs.CV", "cs.AI"], "abstract": "Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretation. In this paper, we propose an elegant closed form solution named $OpenBox$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation.", "text": "abstract strong intelligent machines powered deep neural networks increasingly deployed black boxes make decisions risksensitive domains finance medical. reduce potential risk build trust users critical interpret machines make decisions. existing works interpret pretrained neural network analyzing hidden neurons mimicking pre-trained models approximating local predictions. however methods provide guarantee exactness consistency interpretation. paper propose elegant closed form solution named openbox compute exact consistent interpretations family piecewise linear neural networks major idea first transform plnn mathematically equivalent linear classifiers interpret linear classifier features dominate prediction. apply openbox demonstrate effectiveness nonnegative sparse constraints improving interpretability plnns. extensive experiments synthetic real world data sets clearly demonstrate exactness consistency interpretation. keywords deep neural network exact consistent interpretation closed form. reference format lingyang juhua lanjun wang jian exact consistent interpretation piecewise linear neural networks closed form solution. proceedings york pages. https//doi.org/./nnnnnnn.nnnnnnn introduction machine learning systems making significant decisions routinely important domains medical practice autonomous driving criminal justice military decision making impact machine-made decisions increases demand clear interpretations machine learning systems growing ever stronger blind deployments decision machines accurately reliably interpreting machine learning model many significant tasks identifying permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights third-party components work must honored. uses contact owner/author. arxiv version canada copyright held owner/author. isbn -x-xxxx-xxxx-x/yy/mm. https//doi.org/./nnnnnnn.nnnnnnn interpretation problem machine learning models studied decades. conventional models logistic regression support vector machine well interpreted practical theoretical perspectives powerful nonnegative sparse constraints also developed enhance interpretability conventional models sparse feature selection however complex network structure deep neural network interpretation problem modern deep models challenging field awaits exploration. reviewed section existing studies interpret deep neural network three major ways. hidden neuron analysis methods analyze visualize features learned hidden neurons neural network; model mimicking methods build transparent model imitate classification function deep neural network; local explanation methods study predictions local perturbations input instance provide decision features interpretation. methods gain useful insights mechanism deep models. however guarantee compute interpretation truthfully exact behavior deep neural network. demonstrated ghorbani existing interpretation methods inconsistent fragile perceptively indistinguishable instances prediction result easily manipulated dramatically different interpretations. compute exact consistent interpretation prepaper provide affirtrained deep neural network? mative answer well elegant closed form solution family piecewise linear neural networks. here piecewise linear neural network neural network adopts piecewise linear activation function maxout family relu wide applications great practical successes plnns call exact consistent interpretations overall behaviour type neural networks. make following technical contributions. first prove plnn mathematically equivalent local linear classifiers linear classifier classifies group instances within convex polytope input space. second propose method named openbox provide exact interpretation plnn computing equivalent local linear classifiers closed form. third interpret classification result instance decision features local linear classifier. since instances convex polytope share local linear classifier interpretations consistent convex polytope. fourth also apply openbox study effect non-negative sparse constraints interpretability plnns. find plnn trained constraints selects meaningful features dramatically improve interpretability. last conduct extensive experiments synthetic real-world data sets verify effectiveness method. rest paper organized follows. review related works section formulate problem section present openbox section report experimental results section conclude paper section hidden neuron analysis methods hidden neuron analysis methods interpret pre-trained deep neural network visualizing revert-mapping labeling features learned hidden neurons. yosinski visualized live activations hidden neurons convnet proposed regularized optimization produce qualitatively better visualization. erhan proposed activation maximization method unit sampling method visualize features learned hidden neurons. visualized neural networkâ€™s attention target objects feedback loop infers activation status hidden neurons. visualized compositionality clauses analyzing outputs hidden neurons neural model natural language processing. understand features learned hidden neurons mahendran proposed general framework revert-maps features learned image reconstruct image. dosovitskiy performed task mahendran training up-convolutional neural network. zhou interpreted labeling hidden neuron best aligned human-understandable semantic concept. however hard golden dataset accurate complete labels human semantic concepts. hidden neuron analysis methods provide useful qualitative insights properties hidden neuron. however qualitatively analyzing every neuron provide much actionable quantitative interpretation overall mechanism entire neural network model mimicking methods imitating classification function neural network model mimicking methods build transparent model easy interpret achieves high classification accuracy. proposed model compression method train shallow mimic network using training instances labeled deep neural networks. hinton proposed distillation method distills knowledge large neural network training relatively smaller network mimic prediction probabilities original large network. improve interpretability distilled knowledge frosst hinton extended distillation method training soft decision tree mimic prediction probabilities deep neural network. proposed mimic learning method learn interpretable phenotype features. proposed tree regularization method uses binary decision tree mimic regularize classification function deep time-series model. mimic models built model mimicking methods much simpler interpret deep neural networks. however reduced model complexity mimic model guarantee deep neural network large vc-dimension successfully imitated simpler shallow model. thus always interpretation mimic model actual overall mechanism target deep neural network. local interpretation methods local interpretation methods compute visualize important features input instance analyzing predictions local perturbations. simonyan generated class-representative image class-saliency class images computing gradient class score respect input image. ribeiro proposed lime interpret predictions classifier learning interpretable model local region around input instance. zhou proposed identify discriminative image regions class images using global average pooling cnns. selvaraju generalized grad-cam identifies important regions image flowing classspecific gradients final convolutional layer cnn. local interpretation methods generate insightful individual interpretation input instance. however interpretations perspectively indistinguishable instances consistent purposefully manipulated simple transformation input instance without affecting prediction result problem definition plnn contains layers neurons write l-th layer hence input layer output layer layers hidden layers. neuron hidden layer called hidden neuron. represent number neurons total number hidden denote bias output total weighted inputs. neurons write biases vector outputs vector inputs vector weight edge i-th neuron denote j-th neuron +-by-nl matrix. compute i-th neuron layer number neurons layer total number hidden neurons input i-th neuron layer configuration i-th neuron layer h-th configuration plnn h-th convex polytope determined h-th linear classifier determined linear inequalities define input instance denoted d-dimensional input space. also called instance short. denote i-th dimension input layer contains neurons output nldimensional output space. output layer adopts softmax function compute output softmax). plnn works classification function maps input output widely known piecewise linear function however complex network plnn overall behaviour hard understand. thus plnn usually regarded black box. interpret overall behavior plnn humanunderstandable manner interesting problem attracted much attention recent years. following principled approach interpreting machine learning model regard interpretation plnn decision features define decision boundary call model interpretable explicitly provides interpretation closed form. definition given fixed plnn constant structure parameters task interpret overall behaviour computing interpretable model satisfies following requirements. exactness mathematically equivalent interpretations provided truthfully describe exact behaviour consistency provides similar interpretations classi openbox method section describe openbox method produces exact consistent interpretation plnn computing interpretation model piecewise linear closed form. first define configuration plnn specifies activation status hidden neuron then illustrate interpret classification result fixed instance. last illustrate interpret overall behavior computing interpretation model mathematically equivalent consists linear funck constant integer apply. according linear function applied encode activation status hidden neuron states uniquely corresponds linear functions denote since inputs different neuron neuron states different hidden neurons differ other. states hidden neurons configuration -dimensional vector denoted specifies states hidden neurons configuration fixed plnn uniquely determined instance write function maps instance configuration conf neuron slope intercept respectively linear function corresponds state hidden neurons write variables slopes intercepts respectively. then rewrite activation function neurons hidden layer follows equation linear function constant parameters fixed. summary given fixed linear function show convex polytope showing conf equivalent linear inequalities respect recall denote bijective function real interval maps configuration then conf equivalent constraints denoted linear function real interval constraint equivalent linear inequalities respect therefore conf equivalent linear inequalities means convex polytope. according theorem instances sharing configuration form unique convex polytope explicitly defined linear inequalities since also determines linear classifier fixed instance equation instances convex polytope share linear classifier determined denote linear classifier shared instances interpret local linear classifiers linear classifier applies instances convex polytope denote tuple h-th fixed plnn equivalent llcs denoted final interpretation model fixed plnn states hidden neurons independent plnn configurations means contains llcs. however hierarchical structure plnn states hidden neuron strongly correlate states neurons former layers therefore volume much less number local linear classifiers much less discuss phenomenon later table section practice need compute entire llcs once. instead first compute active subset llcs actually used classify available instances. then update whenever used classify newly coming instance. coefficient matrix remaining terms. superscript indicates equivalent plnnâ€™s forward propagation layer layer since output input softmax) closed form fixed plnn fixed instance constant parameters uniquely determined fixed configuration conf therefore fixed input instance linear classifier whose decision boundary explicitly defined inspired interpretation method widely used conventional linear classifiers logistic regression linear interpret prediction fixed instance decision features specifically entries i-th decision features i-th class instances. equation provides straightforward interpret classification result fixed instance. however individually interpreting classification result every single instance understanding overall behavior plnn next describe interpret overall behavior computing interpretation model mathematically equivalent exact interpretation plnn fixed plnn hidden neurons configurations. represent h-th configuration configurations recall instance uniquely determines configuration conf since volume denoted number instances arbitrarily large clear least configuration shared instances denote conf instances configuration prove theorem configuration convex polytope theorem given fixed plnn hidden neurons conf convex polytope proof. prove showing conf equivalent algorithm summarizes openbox method computes active llcs actually used classify training instances denoted dtrain. ready introduce interpret classification result instance |c|}. first interpret classification result using decision features second interpret contained using polytope boundary features decision features polytope boundaries. specifically polytope boundary defined linear inequality equation linear function respect pbfs coefficients also discover linear inequalities redundant whose hyperplanes intersect simplify interpretation polytope boundaries remove redundant inequalities caronâ€™s method focus studying pbfs non-redundant ones. advantages openbox three-fold follows. first interpretation exact llcs mathematically equivalent classification function second interpretation group-wise consistent. reason instances convex polytope classified exactly thus interpretations consistent respect given convex polytope. last interpretation easy compute since openbox computes one-time forward propagation instance dtrain. experiments section evaluate performance openbox compare state-of-the-art method lime particular address following questions llcs look like? interpretations produced lime openbox exact consistent? decision features llcs easy understand improve interpretability features non-negative sparse constraints? interpret pbfs llcs? effective interpretations openbox hacking debugging plnn model? table shows details models used. plnn plnn-ns network structure described table adopt widely used activation function relu apply non-negative sparse constraints proposed chorowski train plnn-ns. since goal comprehensively study interpretation effectiveness openbox rather achieving state-of-the-art classification performance relatively simple network structures plnn plnnns still powerful enough achieve significantly better classification performance logistic regression decision features lr-f lr-ns lr-nsf used baselines compare decision features llcs. python code lime published authors. methods models implemented matlab. plnn plnnns trained using deeplearntoolbox experiments conducted core-i- main memory hard drive running windows following data sets. detailed information data table models interpret. logistic regression. means non-negative sparse constraints. flip means model trained instances flipped labels. lr-nsf table network structures number configurations plnn plnn-ns. neurons successive layers initialized fully connected. number linear functions relu number hidden neurons. neurons fmnist- fmnist- data sets. data sets contains classes images fashion mnist data fmnist- consists images ankle boot bag. fmnist- consists images coat pullover. images fmnist fmnist- -by- grayscale images. represent image cascading pixel values -dimensional feature vector. fashion mnist data available online. https//github.com/zalandoresearch/fashion-mnist figure shows llcs whose convex polytopes cover decision boundary plnn contain positive negative instances. shown solid lines show decision boundaries llcs capture difference positive negative instances form overall decision boundary plnn. convex polytope cover boundary plnn contains single class instances. llcs convex polytopes capture common features corresponding class instances. analyzed following subsections llcs produce exactly prediction plnn also capture meaningful decision features easy understand. exact consistent interpretations naturally favored human minds. subsection systematically study exactness consistency interpretations lime openbox fmnist- fmnist-. since lime slow process instances hours fmnist- fmnist- uniformly sample instances testing conduct following experiments sampled instances. first analyze exactness interpretation comparing predictions computed local interpretable model lime llcs openbox plnn respectively. prediction instance probability classifying positive instance. figure since lime guarantee zero approximation error local predictions plnn predictions lime exactly plnn fmnist- dramatically different plnn fmnist-. difference predictions significant fmnist- images fmnist- difficult distinguish makes decision boundary plnn complicated harder approximate. also predictions lime exceed output interpretable model lime probability all. result arguable interpretations computed lime truthfully describe exact behavior plnn. contrast since llcs computed openbox mathematically equivalent plnn predictions openbox exactly plnn instances. therefore decision features llcs exactly describe overall behavior plnn. next study interpretation consistency lime openbox analyzing similarity interpretations similar instances. general consistent interpretation method provide similar interpretations similar instances. instance denote nearest neighbor euclidean distance decision features classification respectively. measure consistency interpretation cosine similarity larger cosine similarity indicates better interpretation consistency. figure cosine similarity decision features instance nearest neighbour. results lime openbox separately sorted cosine similarity descending order. shown figure cosine similarity openbox equal instances openbox consistently gives interpretation instances convex polytope. since nearest neighbours belong convex polytope cosine similarity openbox always equal instances. constrast since lime computes individual interpretation based unique local perturbations every single instance cosine similarity lime significantly lower openbox instances. demonstrates superior interpretation consistency openbox. summary interpretations openbox exact much consistent interpretations lime. besides exactness consistency good interpretation also strong semantical meaning thoughts intelligent machine easily understood human brain. subsection first show meaning decision features llcs study effect non-negative sparse constraints improving interpretability decision features. decision features plnn plnn-ns computed openbox. decision features lr-f lr-ns lr-nsf used baselines. table shows accuracy models. figure shows decision features models fmnist-. interestingly decision features plnn easy understand decision features lr-f. features clearly highlight meaningful image parts ankle heel ankle boot upper left corner bag. closer look average images suggests decision features describe difference ankle boot bag. decision features plnn capture detailed difference ankle boot decision features lr-f. llcs plnn capture difference subset instances within convex polytope however lr-f capture overall difference instances figure decision features models fmnist-. show average image decision features models ankle boot respectively. plnn plnn-ns show decision features whose convex polytope contains instances. figure decision features models fmnist-. show average image decision features models coat pullover respectively. plnn plnn-ns show decision features whose convex polytope contains instances. ankle boot bag. accuracies plnn lr-f comparable instances ankle boot easy distinguish. however shown figure instances hard distinguish plnn captures much detailed features lr-f achieves significantly better accuracy. figure shows decision features models fmnist-. shown lr-f capture decision features strong semantical meaning collar breast coat shoulder pullover. however features general accurately distinguish coat pullover. therefore lr-f achieve high accuracy. interestingly decision features plnn capture much details lr-f leads superior accuracy plnn. superior accuracy plnn comes cost cluttered decision features hard understand. fortunately applying non-negative sparse constraints plnn effectively improves interpretability decision features without affecting classification accuracy. easier understand decision features plnn. particular shown figure decision features plnn-ns clearly highlight collar breast coat shoulder pullover much easier understand cluttered features plnn. results demonstrate effectiveness non-negative sparse constraints selecting meaningful features. moreover decision features plnn-ns capture details lr-ns lr-nsf thus plnn-ns achieves comparable accuracy plnn significantly outperforms accuracy lr-ns lr-nsf fmnist-. summary decision features llcs easy understand non-negative sparse constraints highly effective improving interpretability decision features llcs. pbfs llcs easy understand? polytope boundary features polytope boundaries interpret instance contained convex polytope llc. subsection systematically study semantical meaning pbfs. limited space plnn-ns models trained fmnist- fminst- target model interpret. llcs plnn-ns computed openbox. since activation pbfs coefficients since either function relu values pbfs non-negative plnn-ns convex images strongly correlate polytope images pbfs strongly correlated pbfs analysis pbfs demonstrated results tables figure take first convex polytope table example whose pbfs figures show features ankle boot respectively. therefore convex polytope contains images ankle boot bag. careful study results suggests pbfs convex polytopes easy understand accurately describe images convex polytope. also pbfs figure look similar decision features plnn-ns figures shows strong correlation features learned different neurons plnn-ns probably caused hierarchy network structure. strong correlation neurons number configurations much less shown table surprisingly shown table top- convex polytope fmnist- contains training instances. instances training accuracy much higher training accuracies lr-ns lr-nsf. means training instances top- convex polytope much easier linearly separated training instances fmnist-. perspective behavior plnn-ns like divide conquer strategy aside small proportion instances hinder classification accuracy majority instances better separated llc. shown top- top- convex polytopes table aside instances grouped convex polytopes corresponding llcs also achieve high accuracy. table shows similar phenomenon fmnist-. however since instances fmnist- easy linearly separated training accuracy plnn-ns marginally outperforms lr-ns lr-nsf. hack model using openbox? knowing intelligent machine thinks provides privilege hack here hack target model significantly change prediction instance modifying features possible. general biggest change prediction achieved modifying important decision features. precise interpretation target model reveals important decision features accurately thus requires modify less features achieve bigger change prediction. following idea apply lime openbox hack plnn-ns compare quality interpretations comparing change plnn-nsâ€™s prediction modifying number decision features. instance denote decision features classification hack plnn-ns setting values top-weighted decision features zero prediction plnn-ns changes significantly. change prediction evaluated measures follows. first change prediction probability absolute change probability classifying positive instance. second number label-changed instance number instances whose predicted label changes hacked. again inefficiency lime sampled data sets section evaluation. figure average nlci openbox always higher lime data sets. demonstrates interpretations computed openbox effective lime applied hack target model. interestingly advantage openbox significant fmnist- fmnist-. because shown figure prediction probabilities instances fmnist- either provides little gradient information lime accurately approximate classification function plnn-ns. case decision features computed lime cannot describe exact behavior target model. debug model using openbox? intelligent machines perfect predictions fail occasionally. failure occurs apply openbox interpret instance mis-classified. figure shows images mis-classified plnn-ns high probability. figures original image coat however since scattered mosaic pattern cloth hits features pullover coat original image classified pullover high probability. figures original image pullover however mis-classified coat white collar breast typical features coat dark shoulder sleeves miss significant features pullover. similarly ankle boot figure highlights features upper left corner thus mis-classified bag. figure mis-classified images coat pullover ankle boot show original images. rest subfigures caption shows prediction probability corresponding class; image shows decision features supporting prediction corresponding class. figure mis-classified ankle boot hits features ankle heel ankle boot however misses typical features upper left corner. conclusion demonstrated mis-classified examples figure openbox accurately interprets mis-classifications potentially useful debugging abnormal behaviors interpreted model. conclusions future work paper tackle challenging problem interpreting plnns. studying states hidden neurons configuration plnn prove plnn mathematically equivalent llcs efficiently computed proposed openbox method. extensive experiments show decision features polytope boundary features llcs provide exact consistent interpretations overall behavior plnn. interpretations highly effective hacking debugging plnn models. future work extend work interpret general neural networks adopt smooth activation functions sigmoid tanh. models model extraction. arxiv. statistics). springer york yang wang wang huang wang huang look think twice capturing top-down visual attention feedback convolutional neural networks. iccv. caron mcdonald ponic. degenerate extreme point strategy classification linear constraints redundant necessary. jota purushotham khemani liu. distilling knowledge deep networks applications healthcare domain. arxiv. chorowski jacek zurada. learning understandable neural networks nonnegative weight constraints. tnnls convolutional networks. cvpr. layer features deep network. university montreal meaningful perturbation. arxiv. bengio. maxout networks. arxiv. decision-making right explanation\". arxiv. bounds piecewise linear neural networks. arxiv. human-level performance imagenet classification. iccv. neural network. arxiv. patrik hoyer. non-negative sparse coding. wnnsp. pieter-jan kindermans sara hooker julius adebayo maximilian alber kristof schÃ¼tt sven dÃ¤hne dumitru erhan kim. reliability saliency methods. arxiv. influence functions. arxiv. dimension. nips. cation deep convolutional neural networks. nips. coding algorithms. nips. understanding neural models nlp. arxiv. representations inverting them. cvpr. guido montufar razvan pascanu kyunghyun yoshua bengio. number linear regions deep neural networks. nips. vinod nair geoffrey hinton. rectified linear units improve restricted boltzmann machines. icml. drew multimodal deep learning. icml. data. razvan pascanu guido montufar yoshua bengio. number response regions deep feed forward networks piece-wise linear activations. arxiv. learning towards biomedical knowledge discovery. ijmsc marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classifier. kdd. selvaraju vedantam cogswell parikh batra. grad-cam that? visual explanations deep networks gradient-based localization. arxiv. propagating activation differences. arxiv. karen simonyan andrea vedaldi andrew zisserman. deep inside convolutional networks visualising image classification models saliency maps. arxiv. removing noise adding noise. arxiv. computer systems sciences deep networks. arxiv. hughes parbhoo zazzi roth doshi-velez. beyond sparsity tree regularization deep models interpretability. aaai xiao kashif rasul roland vollgraf. fashion-mnist novel neural networks deep visualization. arxiv. learning fair representations. icml. bolei zhou david aude oliva antonio torralba. interpreting deep visual representations network dissection. arxiv. bolei zhou aditya khosla agata lapedriza aude oliva antonio torralba. learning deep features discriminative localization. cvpr.", "year": 2018}