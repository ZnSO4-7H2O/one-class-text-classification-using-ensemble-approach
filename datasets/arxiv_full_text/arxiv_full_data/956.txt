{"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "tag": ["stat.ML", "cs.LG", "cs.NE", "q-bio.NC"], "abstract": "Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations.  We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks.", "text": "science example convolutional networks possibly capture aspects organization visual cortex indeed closely related biological models like hmax deep boltzmann machines applied models generative cortical processing impressive recent deep learning results achieved classiﬁcation tasks processing mode akin rapid feed-forward recognition humans required supervised training large amounts labeled data. perhaps less clear whether current deep networks truly support neuronal representations processes naturally allow ﬂexible rich reasoning e.g. objects relations visual scenes machinery necessary learn representations data mostly unsupervised way. implementational level host cortical computations captured simpliﬁed mechanisms utilized deep networks complex laminar organization cortex dendritic computations neuronal spikes timing. mechanisms might realizing richer representations challenge identify mechanisms functionally relevant discarded mere implementation details. candidate mechanism temporal coordination neuronal output particular synchronization neuronal ﬁring. various theories posit synchrony element cortex processes sensory information though theories also contested degree synchrony neuronal spikes affects output downstream neurons synchrony postulated allow gating information transmission neurons whole cortical areas moreover relative timing neuronal spikes carry information sensory input dynamic network deep learning recently great successes tasks image recognition however deep networks still outmatched power versatility brain perhaps part richer neuronal computations available cortical circuits. challenge identify neuronal mechanisms relevant suitable abstractions model them. here show aspects spike timing long hypothesized play crucial role cortical information processing could incorporated deep networks build richer versatile representations. introduce neural network formulation based complex-valued neuronal units biologically meaningful also amenable variety deep learning frameworks. here units attributed ﬁring rate phase latter indicating properties spike timing. show formulation qualitatively captures several aspects thought related neuronal synchrony including gating information processing dynamic binding distributed object representations. focusing latter demonstrate potential approach several simple experiments. thus neuronal synchrony could ﬂexible mechanism fulﬁlls multiple functional roles deep networks. deep learning approaches proven successful various applications machine vision language processing deep networks often taken inspired brain idealized neural networks learn representations several stages non-linear processing perhaps akin mammalian cortex adapts represent sensory world. approaches thus also relevant computational neurostate beyond addition conveyed ﬁring rates. particular neuronal subpopulations could dynamically form synchronous groups bind distributed representations signal perceptual content represented group forms coherent entity visual object scene. here demonstrate potential functional role neuronal synchrony framework amenable deep learning. rather dealing realistic elaborate spiking neuron models thus seek mathematical idealization naturally extends current deep networks still interpretable context biological models. complex-valued units neuron’s output described ﬁring rate phase variable. phase variables across neurons represent relative timing activity. section brieﬂy describe effect synchrony neuronal information processing. present framework based complex-valued networks show functional roles synchrony could play within framework. thanks speciﬁc formulation employed success converting deep nets trained without synchrony incorporate synchrony. using approach section underpin argument several simple experiments focusing binding synchrony. exploiting presented approach require learning synchrony. discuss principled ways challenges overcome section noted complex-valued neural networks however seem attracted much attention within deep learning community—perhaps beneﬁts still need explored further. cases networks employed interpretation neuronal synchrony including work cecchi similar ours. prior approaches discussed section cortical neurons communicate electric action potentials long-standing debate neuroscience whether various features spike timing matter neuronal information processing rather average ﬁring rates common deep neural networks output neuronal unit characterized single scalar; state network relates interpretation sensory input fully determined joint scalar outputs across units. suggests interpretation terms average static ﬁring rates lacking notion relative timing. here consider incorporate notions deep networks. consider figure example dynamic code could transmitted neurons example based hypothesis neuronal rhythms ubiquitous throughout brain play functional role information processing neuron receives spike train inputs modulated oscillatory ﬁring rates. results rhythmic output activity average ﬁring rate depends amplitudes relative phase inputs interactions difﬁcult represent static ﬁring rates. deep networks neuronal unit receives inputs neurons states vector synaptic weights vector denote total ‘postsynaptic’ input w·x. output computed activation function conditional probability chines output state sampled). model aspects spike timing replacing real-valued states complex states unit state rieφi magnitude |zi| interpreted average ﬁring rate analogously real-valued case. phase could correspond phase neuronal rhythm figure generally timing maximal activity temporal interval neuronal messages added complex plane neuron’s total input longer depends ﬁring rates input units strength synapses also relative timing. naturally accounts earlier spiking neuron example input states synchronous i.e. similar phases result stronger total input whereas less synchronous inputs result weaker total input straightforward deﬁne neuron’s output state rieiφi total input apply activation function input’s magnitude compute output magnitude output phase phase total input figure transmission rhythmical activity corresponding model using complex-valued units. hodgkin–huxley model neuron receives rhythmic spike trains input plus background activity. inputs modeled inhomogeneous poisson processes modulated sinusoidal rate functions identical frequencies differing phases. output neuron rhythmical neuron’s output rate modulated phase difference inputs represent timing maximal activity neuron phase complex number corresponding direction complex plane. ﬁring rate magnitude complex number. also shown color coding used indicate phase throughout paper outputs input neurons scaled synaptic weights added complex plane. phase resulting complex input determines phase output neuron. activation function applied magnitude input compute output magnitude. together models inﬂuence synchronous neuronal ﬁring postsynaptic neuron. output magnitude function phase difference inputs. second term added neuron’s input out-of-phase excitation never cancels completely compare however issues simple approach modeling neuronal synchrony problematic biological model also possibly functional capabilities network. analogy spiking neuron example consider inputs neuron excitatory furthermore equal magnitude |wz| |wz|. desirable total input decreased inputs phase input complex-valued formulation actually zero difference input phases matter strong individual inputs biologically seems unrealistic strong excitatory input even synchronized would excite neuron. moreover formulation role inhibition changed inputs negative weights equivalent excitatory inputs opposite phase eiπ. again desirable property leads desynchronization neuronal groups line biological models show below. however also means inputs connections negative weights strongly drive neuron; sense longer actual inhibition always suppressive effect neuronal outputs. additionally found phase shifting caused inhibition could result instability networks dominant negative weights leading fast switching phase variables. ﬁrst term refer synchrony term before. second classic term applies weights magnitudes input units thus depend phases; network using classic terms reduces real-valued counterpart together presence classic term implies excitatory input always excitatory component even input neurons phase synchrony term zero similarly input negative connections alone never greater zero. lastly formulation also makes possible give different weightings synchrony classic terms thus controlling much impact synchrony network; explore possibility here. advantage using complex-valued neuronal units rather than spiking neuron models natural consider apply deep learning techniques extend existing deep learning neural networks framework. indeed experiments presented later based pretraining standard real-valued nets converting complex-valued nets training. section brieﬂy describe framework lends realize functional roles synchrony postulated biological theories. activation real-valued unit artiﬁcial neural network often understood signaling presence feature combination features data. phase complex-valued unit could provide additional information feature. binding synchrony theories postulate neurons brain dynamically form synchronous assemblies signal distributed representations together correspond coherent sensory entities. example different objects visual scene would correspond different synchronous assemblies visual cortex. formulation phases analogously signal soft assignment different assemblies. real neuronal networks realistic simulations many degrees freedom hence make claim formulation general quantitative model neuronal interactions. figure gating interactions. out-of-phase input combined stronger input weakened. example long |wz| effective input neuron right zero input strength hence neuronal groups different phases decouple. sages also naturally leads different synchronous groups emerging excitatory connections messages ‘agree’ phases prevail not; inhibitory messages hand equate excitatory messages opposite phases thus encourage desynchronization neurons. comparison consider realistic spiking model visual cortex miconi vanrullen synchronous groups arise similar interaction excitation inhibition. interactions indeed lead meaningful groupings neuronal representations deep networks shown empirically section synchrony affects neuronal messages transmitted preferentially also postulated synchrony gate information dynamically depending sensory input current network state top-down control well modulate effective interactions cortical areas depending level coherence similar modulation interactions reproduced framework. consider example scenario neuron member synchronous assembly receiving excitatory inputs neurons similar phases. consider effect adding another neuron also provides excitatory input different phase assume |w·z| |wz| effect latter additional input depends phase difference. particular phase difference maximal contribution second neuron turns zero. output magnitude computed taking synchrony classic terms account. complex plane antiparallel thus synchrony term reduced |wz|. however reduction exactly canceled classic term contribution second input also effect output phase phase total input remains equal phase w·z. analogous reasoning applies inhibitory connections. thus effective connectivity neuronal units modulated units’ phases result network interactions. particular inference results neurons segregated different assemblies existent connections groups weakened. experiments based pretraining networks normal real-valued deep boltzmann machines dbms multi-layer networks framed probabilistic models. visible units make ﬁrst layer according data e.g. images. several hidden layers learn internal representations data generate latter sampling visible units. deﬁnition connections adjacent layers connections within layer. given inputs adjacent layers unit’s state updated stochastically probability given sigmoid activation function training carried layer-wise standard methods including contrastive divergence training experiments implemented within pylearn framework goodfellow emphasize however framework speciﬁc dbms principle adapted various deep learning approaches learning inference procedures derive deﬁnition probabilistic model purpose appropriate simply think multi-layer recurrent neural network logistic activation function; demonstrate framework works taking pretrained network introducing complex-valued unit states applying activation function magnitudes deconversion procedure applied real-valued networks offers simple method exploring aspects synchrony guarantee work show functional roles synchrony could principle; learning synchrony required move beyond simple experiments throughout experiments clamped magnitudes visible units according input images seen training network infer hidden states multiple iterations. phases visible layer initialized randomly determined input hidden layer above. hence synchronization observed spontaneous. ﬁrst experiment trained hidden layer version classic ‘bars problem’ binary images created randomly drawing horizontal vertical bars dataset classically used test whether unsupervised learning algorithms independent components constitute image learning represent individual bars chose dataset speciﬁcally elucidate role synchrony context distributed representations. hard-coded receptive ﬁeld sizes hidden units restricted regions smaller entire image necessity implies individual unit never fully represent full-length sense unit’s weights correspond read presence full unit’s state alone. however imply full network cannot learn images constituted bars example found sampling model resulting images contained full-length bars time similarly network would remainder visible units clamped part figure binding synchrony shallow distributed representations. image version bars problem contained vertical horizontal bars random positions. restricted boltzmann machine trained bars images converted complex-valued network. magnitudes visible units clamped according input image whereas hidden units phases visible units activated freely. iterations units representing various bars found synchronized neurons synchronized even though receptive ﬁelds hidden units constrained smaller bars. thus binding synchrony could make ‘independent components’ sensory data explicit distributed representation particular single neuron possibly represent component own. histogram unit phases visible layer example shown input images iterations each. results plotted figure depicting visible hidden states input image found visible neurons along would often synchronize phase whereas different bars tended different phase values. figure shows histogram phase values visible layer example image clear peaks corresponding phases bars. synchronization also found hidden layer units based results make three points. first results show formulation indeed allows neurons dynamically organize meaningful synchronous assemblies even without supervision towards neurons synchronize e.g. providing phase targets training—here synchrony used training all. conversion real-valued network work suggests unsupervised semi-supervised approach learning synchrony could successful long synchrony beneﬁts task hand. second synchronization visible hidden units together represent individual bars occur neurons several synapses apart. time bars synchronized different phases. number distinct stable phase groups formed likely limited. notably argued aspect synchrony coding explains certain capacity limits cognition third point relates nature distributed representations. bars problem whether neural discovers bars usually evaluated examining whether individual units correspond individual bars seen inspecting weights probing response properties individual neurons similar ‘localist’ approach taken recent attempts make sense somewhat opaque hidden representations learned deep networks experiment possible individual neurons image constituents construction; bars could represented distributed fashion. synchrony could make explicit neurons together represent sensory entity e.g. read well offer mechanism establishes grouping ﬁrst place. examine effects synchrony deeper networks trained three hidden layers another dataset consisting binary images contained four ‘corners’ arranged square shape four corners independently drawn receptive ﬁeld sizes ﬁrst hidden layer chosen ﬁelds would cover individual corners whole square arrangements making impossible ﬁrst hidden layer discover latter training. receptive ﬁeld sizes larger higher layers topmost hidden layer fully connected. figure binding synchrony deep network. image contained four corners arranged square shape four randomly positioned corners. four corners arranged square usually found synchronize. synchronization corresponding hidden units also clearly visible hidden layers. receptive ﬁeld sizes ﬁrst hidden layer small hidden unit ‘see’ individual corners. hence synchronization neurons representing square hidden visible layers feedback higher layers make several observations. first restricted receptive ﬁelds synchronization units representing parts square visible layer ﬁrst hidden layer necessarily top-down input higher layers. whether corner represented ﬁrst layer neuron part larger whole made explicit synchronous state. second example also demonstrates neurons need synchronize connected image regions case bars experiment. lastly note that without synchrony restricted receptive ﬁelds topographic arrangement hidden units intermediate hidden layers make possible roughly identify units participate representing image content virtue position layers. longer possible topmost globally connected layer. identifying hidden units topmost layer visible units similar phase however becomes possible establish connection hidden units activated image space. group time. trained additional datasets containing multiple simple objects images geometric shapes three randomly chosen instances image dataset combined handwritten digits commonly used mnist dataset geometric shapes before found tendency complex-valued network synchronize individual objects image distinct phases network ﬁxed number steps layer units clustered according activity vectors complex plane. clustering assumed simplicity number objects known advance used k-means number clusters number objects plus general background. fashion neuron assigned cluster assignments could used deﬁne masks read representational component time. visible layer thus obtained segmented images shown figures especially modiﬁed mnist images segmentations often noisy. however noteworthy segmentations obtained given network training involved notion segmentation. moreover binding synchrony general segmentation applies neurons network principle arbitrarily abstract non-visual forms representation. thus units also selected hidden layers according phase. phase-masked representations could instance used classiﬁcation object time. also decode representations corresponded image space. took masked states cluster used simple decoding procedure described reichert reichert performing single deterministic top-down pass network obtain reconstructed image. figure example. though images decoded fashion somewhat noisy apparent higher layer units indeed represent individual objects visible layer units assumed phase this works network units signal presence image content setting units zero effect removing image content. achieved inductive biases sparsity applied training discussion reichert figure simple segmentation phases. -shapes consisted binary images containing three simple geometric shapes visible states synchronization segmented images image mnist+shape dataset mnist digit shape drawn probability analogous figure using phase access decode internal object representations. selecting subsets neurons according phase representations object read hidden layers plotted images decoded synchronous sub-populations using simple decoding procedure tions. selecting explicitly individual synchrony assemblies processing done here another potential form gating synchrony. brain cortical regions prefrontal cortex highly interconnected rest cortex implement functions executive control working memory demand ﬂexible usage capacity-limited resources according context task-demands. coherence cortical activity synchrony suggested possibly play causal role establishing dynamic routing areas similarly attentional processing hypothesized emerge dynamically changing globally coherent state across cortex possible dedicated structures brain pulvinar thalamus coordinate cross-cortical processing cortical rhythms model could interpret selecting synchrony assemblies prefrontal areas reading subsets neuronal populations demanded task. binding argue extending neural networks beyond realvalued units could allow richer representations sensory input. extension could motivated fact brain supports richer coding least principle. speciﬁcally explored notion neuronal synchrony deep networks. motivated hypothetical functional roles synchrony biological theories introduced formulation based complex-valued units showed formulation related biological phenomenon examined potential simple experiments. neuronal synchrony could versatile mechanism supports various functions gating modulating neuronal interactions establishing signaling semantic grouping neuronal representations. latter case synchrony realizes form soft constraint neuronal representations imposing sensory world organized according distinct perceptual entities objects. unfortunately melding various functional roles might make difﬁcult treat synchrony mechanism principled theoretical terms. formulation introduced part motivated interpretable biological model. understood description neurons rhythmically and/or relation global network rhythm. aspects spike timing could functionally relevant complex-valued formulation seen step beyond current ‘static ﬁring rate’ neural networks. formulation also advantage making possible explore synchrony converted pretrained real-valued networks however machine learning application various alternative formulations would worthy exploration presented simulation results terms representative examples. provide quantitative analysis simply claim current simple approach would compete with example dedicated segmentation algorithm point. particular found conversion arbitrary real-valued nets consistently lead favorable results paper demonstrate synchrony concept could implemented functions could fulﬁll principle deep learning community. whether synchrony useful real applications necessary develop appropriate learning algorithms. address learning context related work following. aware examples prior work employing complex-valued neural networks interpretation neuronal synchrony. zemel introduced ‘directional unit boltzmann machine’ extension boltzmann machines complex weights states related approach used mozer model binding synchrony vision performing phase-based segmentation simple geometric contours behrmann model aspects object-based attention. dubm principled probabilistic framework within complexvalued extension hebbian learning derived interest also work cadieu olshausen complex-valued formulation separate motion form generative model natural movies. make connection neuronal synchrony however. potentially interesting functional implications biological interpretations terms spike timing dependant plasticity purposes dubm energy function could extended include synchrony classic terms desired allow units switch e.g. ‘spike slab’ formulation performing mean-ﬁeld inference resulting model qualitatively similar running networks used work here. original dubm applications limited simple data shallow architectures supervised training would worthwhile reexamine approach context recent deep learning developments; however training boltzmann machines successfully straightforward clear whether approximate training methods contrastive divergence translated complex-valued case success. weber wermter brieﬂy describe complexvalued neural network image segmentation modeling synchrony mediated lateral interactions primary visual cortex though results analysis presented perhaps limited conclude much approach. model binding synchrony multi-layer network proposed cecchi there neuronal dynamics weight learning derived optimizing objective function resulting formulation actually similar several respects underlying motivation analysis. became aware work developed approach. work complementary several regards goal provide broader perspective synchrony could used neural networks rather proposing particular model; performed different experiments conceptual analyses al.’s approach relied model seeing individual objects training showed unnecessary; lastly even though applied synchrony learning dataset used experiments arguably even simpler datasets. thus remains tested whether particular model formulation ideal. finally currently also exploring training synchrony networks backpropagation. even feed-forward network could potentially beneﬁt synchrony latter could carry information sensory input network state though complex-valued weights necessary detecting synchrony patterns. alternatively allow dynamic binding synchrony network could trained recurrent network backpropagation time given appropriate input data cost functions. experiments number iterations required order tens hundreds thus making training challenging. again complex-valued weights could beneﬁcial establishing synchrony assemblies rapidly. thank nicolas heess christopher k.i. williams michael frank david mély goodfellow helpful feedback earlier versions work. would also like thank elie bienenstock stuart geman insightful discussions motived project. work supported early career award supported fellowship within postdoc-programme german academic exchange service additional support provided robert nancy carney fund scientiﬁc innovation brown institute brain sciences center vision research center computation visualization additional outcome examples various experiments shown figure also provide following supplementary videos arxiv sample generated model trained bars problem example synchronization process visible hidden layers bars image several examples visible layer synchronization -shapes mnist+shape datasets training implemented within pylearn framework goodfellow networks trained real-valued deep boltzmann machines using layer-wise training. layers trained epochs -step contrastive divergence exception model trained mnist+shape -step persistent contrastive divergence used instead datasets training images divided mini-batches size biases initialized encourage sparse representations initial weights drawn randomly uniform distribution support number hidden layers number hidden units sizes receptive ﬁelds varied experiment experiment demonstrate various properties neuronal synchronization networks speciﬁc numbers chosen mostly line earlier work importance. detail model architectures follows bars problem input images restricted boltzmann machine hidden layer units receptive ﬁelds. corners dataset input images three hidden layers units respectively receptive ﬁelds -shapes dataset input images hidden layer dimensions receptive ﬁelds mnist+shape data input images hidden layer dimensions receptive ﬁelds synchronization ﬁgures number steps chosen synchronization fairly stable point lastly mentioned main text note conversion pretrained real-valued dbms always lead models exhibiting successful synchronization. here successful refers ability model separately synchronize different objects input images. unsuccessful setups resulted either visible units synchronizing single phase objects synchronizing fully across images dataset. found whether setup worked depended dataset training procedures used. presented results representative well performing networks. proper synchronization outcome right balance excitatory inhibitory connectivity patterns. analysis network parameters affect synchronization subject ongoing work incorporating synchronization learning achieve desired synchronization behavior. supplementary ﬁgure additional results. samples generated restricted boltzmann machine trained bars problem. generated images consist mostly full-length bars. individual receptive ﬁelds hidden layer constrained image regions smaller extent bars. thus bars necessarily represented distributed fashion. additional examples synchronized visible units various datasets. magnitudes visible units according binary input images phases determined input hidden units. also supplementary videos main text details. following summarize parts discussion iclr review period paraphrasing comments iclr reviewers. expand several points brieﬂy covered main text. important issue still considering. perhaps issue generally underlying biological theories rather speciﬁc approach. noted main text theories pose limit many discrete objects represented oscillation cycle without interference explains certain capacity limits cognition. references cited refer working memory example would posit that generally analysis visual scenes requiring concurrent separation multiple objects limited accordingly question then brain cope limitation? usual face perceptual capacity limits solution likely would involve attentional mechanisms. mechanisms might dynamically change grouping sensory inputs depending task context whether questions asked individual parts detail object groups larger patterns. bars example might perceive bars single group texture focus individual bars capacity allows perhaps relegating rest image general background. dynamically changing phase assignments according context principle possible within proposed framework similar grouping according parts wholes top-down input experiment section acknowledged work similar several points make additional contributions. first clarify issue training multiple objects al.’s work training data consisted small number ﬁxed pixel images containing simple patterns demonstrate binding synchrony patterns superimposed test time. believe going beyond extremely constrained task particular showing binding work trained tested multiple objects multiple datasets including mnist containing thousands images valid contribution side. results also provide insights nature representations trained multiple objects. similarly discuss gating aspect speciﬁc issues excitation inhibition pointed motivation using classic synchrony terms. lastly following issues addressed experiments only network behavior objects; synchronization objects contiguous input images well part whole effects decoding distributed hidden representations according phase particular seems case al.’s networks localist representation hidden layer majority cases. agree framing approach proper probabilistic model would helpful time value presenting heuristic based speciﬁc neuronal activation function emphasize idea could application neural networks generally probabilistic interpretation boltzmann machines particular performed exploratory experiments networks trained backpropagation including feed-forward neural networks autoencoders recurrent networks well biological essentially normal training form pretraining ﬁnal complex-valued architecture. resulting neural network likely exactly interpreted probabilistic model. however interpretation desired understanding running network could seen approximation inference suitably extended dubm experiments used procedures analogy inference either sampling binary output magnitude letting determine output magnitude deterministically; output phase always phase total postsynaptic input. ﬁrst procedure similar inference extended dubm rather sampling circular normal distribution unit circle unit simply take mode distribution. second procedure qualitatively correspond mean-ﬁeld inference extended dubm using slightly different output function. references aizenberg moraga multilayer feedforward neural network based multi-valued neurons backpropagation learning algorithm. soft computing behrmann zemel r.s. mozer m.c. objectbased attention occlusion evidence normal participants computational model. journal experimental psychology. human perception performance pmid gabernet jadhav s.p. feldman d.e. carandini scanziani somatosensory integration controlled dynamic thalamocortical feed-forward inhibition. neuron hinton g.e. practical guide training restricted boltzmann machines. technical report utml department computer science machine learning group university toronto. kivinen williams transformation equivariant boltzmann machines. honkela duch girolami kaski eds. artiﬁcial neural networks machine learning icann vol. lecture notes computer science springer berlin heidelberg. imagenet classiﬁcation deep convolutional neural networks. bartlett f.c.n. pereira c.j.c. burges bottou k.q. weinberger eds. advances neural information processing systems a.r. cecchi g.a. objective function utilizing complex sparsity efﬁcient segmentation multi-layer oscillatory networks. international journal intelligent computing cybernetics reichert seriès storkey hallucinations charles bonnet syndrome induced homeostasis deep boltzmann machine model. lafferty c.k.i. williams shawe-taylor r.s. zemel culotta eds. advances neural information processing systems reichert d.p. seriès storkey a.j. hierarchical generative model recurrent object-based attention visual cortex. honkela duch girolami kaski eds. artiﬁcial neural networks machine learning icann vol. springer berlin heidelberg berlin heidelberg. spratling m.w. unsupervised learning generative discriminative weights encoding elementary image components predictive coding model cortical function. neural computation tieleman training restricted boltzmann machines using approximations likelihood gradient. proceedings annual international conference machine learning helsinki finland. uhlhaas p.j. pipa lima melloni neuenschwander nikoli´c singer neural synchrony cortical networks history concept current status. frontiers integrative neuroscience image segmentation complex-valued units. duch kacprzyk zadro˙zny eds. artiﬁcial neural networks biological inspirations icann lecture notes computer science springer berlin heidelberg. smolensky information processing dynamical systems foundations harmony theory. parallel distributed processing explorations microstructure cognition. vol. foundations press cambridge", "year": 2013}