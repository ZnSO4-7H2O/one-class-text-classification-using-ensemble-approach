{"title": "Student-t Processes as Alternatives to Gaussian Processes", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "abstract": "We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.", "text": "investigate student-t process alternative gaussian process nonparametric prior functions. derive closed form expressions marginal likelihood predictive distribution student-t process integrating away inverse wishart process prior covariance kernel gaussian process model. show surprising equivalences different hierarchical gaussian process models leading student-t processes derive sampling scheme inverse wishart process helps elucidate equivalences. overall show studentprocess retain attractive properties gaussian process nonparametric representation analytic marginal predictive distributions easy model selection covariance kernels enhanced ﬂexibility predictive covariances that unlike gaussian process explicitly depend values training observations. verify empirically student-t process especially useful situations changes covariance structure applications like bayesian optimization accurate predictive covariances critical good performance. advantages come additional computational cost gaussian processes. gaussian processes rich distributions functions provide bayesian nonparametric approach regression. owing interpretability non-parametric ﬂexibility large support consistency simple exact learning inference procedures impressive empirical performances gaussian processes kernel machines steadily grown popularity last decade. heart every gaussian process parametrized covariance kernel determines properties likely functions typically simple parametric kernels gaussian kernel used parameters determined marginal likelihood maximization analytically integrated away gaussian process. however fully bayesian nonparametric treatment regression would place nonparametric prior gaussian process covariance kernel represent uncertainty kernel function reﬂect natural intuition kernel simple parametric form. likewise given success gaussian processes kernel machines also natural consider general families elliptical processes student-t processes collection function values desired elliptical distribution covariance matrix constructed using kernel. show student-t process derived placing inverse wishart process prior kernel gaussian process. given intuitive value surprising various forms student-t processes used diﬀerent applications however connections models theoretical properties models remain largely unknown. similarly practical utility models remains uncertain. example rasmussen williams wonder whether student-t process perhaps exciting might hoped. short paper answers detail many what why? questions might student-t processes inverse wishart processes elliptical processes general. speciﬁcally principal submatrix distributed. property makes wishart distribution appear attractive prior covariance matrices. unfortunately wishart distribution suﬀers makes impractical nonparametric bayesian modelling. suppose wish model covariance matrix using expected value since require must deﬁne process marginals arbitrary size. however requirement prohibits deﬁning useful process wishart marginals arbitrary size. nevertheless inverse wishart distribution suﬀer problem. dawid parametrized inverse wishart distribution follows hierarchical gaussian process models. derive analytic forms marginal predictive distributions process analytic derivatives marginal likelihood. verse wishart process intuitively resolves seemingly bizarre marginal equivalence between inverse wishart inverse gamma priors covariance kernels hierarchical models. haviour variety applications. speciﬁcally robust change-points model misspeciﬁcation notably improved predictive covariances useful tail-dependence distant function values particularly promising bayesian optimization predictive covariances especially important. begin introducing inverse wishart process section derive student-t process using inverse wishart process covariance kernels discuss properties section argue inverse wishart distribution attractive choice prior covariance matrices arbitrary size. wishart distribution probability distribution real tion. principal submatrix distributed. note difference parameterizations distributions parameter need depend size matrix inverse wishart distribution. properties desirable motivate deﬁning process inverse wishart marginals arbitrary section discuss conditional distribution relationship ancovariance prior leads elliptical processes sampling scheme gives insight equivalence. finally consider modelling noisy functions gaussian processes popular nonparametric bayesian distributions functions. thorough guide provided rasmussen williams characterized mean function kernel function. practitioners tend parametric kernel functions learn hyperparameters using maximum likelihood sampling based methods. propose placing inverse wishart process prior kernel function leading student-t process. since inverse wishart distribution conjugate prior covariance matrix gaussian likelihood analytically marginalize generative model collection data parameter controls heavy tailed process smaller values correspond heavier tails. gets larger tails converge gaussian tails. illustrated prior sample draws shown figure notice samples tend extreme behaviour figure marginal distributions. show plots samples gaussian marginals diﬀerent joint distributions. notice tail dependency distributions controlled example dependencies diﬀerent depending whether even kernel. tends inﬁnity predictive distribution tends gaussian process predictive distribution would expect given lemma perhaps less intuitively predictive distribution also tends gaussian process predictive tends inﬁnity. predictive mean form gaussian process conditioned kernel hyperparameters. diﬀerence predictive covariance explicitly depends training observations. indeed somedisappointing feature gaussian process given kernel predictive covariance samples depend training observations. importantly since marginal likelihood diﬀers marginal likelihood predictive mean predictive covariance diﬀer learning kernel hyperparameters. scaling constant multivariate student-t predictive covariance intuitive explanation. note distributed squares independent distributions hence observed value larger predictive covariance scaled vice versa. magnitude scaling controlled despite apparent ﬂexibility inverse wishart distribution illustrate lemma surprising result multivariate student-t distribution derived using much simpler covariance prior considered previously proof found supplementary material. result surprising previously integrated inﬁnite dimensional nonparametric object derive student-t process show integrate single scale parameter arrive marginal process. provide insight distinct priors lead marginal multivariate student-t distribution section deﬁnition. elliptically symmetric exists nonnegative random variable matrix maximal rank uniformly distributed unit sphere independent µ+rωu denotes equality distribution. tells uniformly distributed exchangeable i.e. permuting diag aﬀect probability. denote exchangeable distribution generate draw inverse wishart distriburesult provides geometric interpretation sample looks like. ﬁrst uniformly random pick orthogonal basis vectors stretch basis vectors using exchangeable scalar random variables. analogous interpretation holds wishart distribution. elliptically symmetric distributions characterize large class distributions unimodal likelihood point decreases distance mode. properties natural assumptions often want encode prior distribution making elliptical distributions ideal multivariate modelling tasks. idea naturally extends inﬁnite dimensional objects. since student-t process generalizes gaussian process general elliptical process analytically representable density. thus expressive tool nonparametric bayesian modelling. analytic expressions predictive distributions computational costs gaussian process increased ﬂexibility student-t process used drop-in replacement gaussian process many applications. show density inverse wishart distribution depends eigenvalues positive deﬁnite matrix. best knowledge change variables computed previously. decomposition oﬀers novel sampling inverse wishart distribution insight student-t process derived using inverse gamma inverse wishart process covariance prior. figure posterior distributions sample synthetic data prior prior solid line posterior mean shaded area represents predictive interval circles training points crosses test points. common practice assume outputs latent gaussian process independent gaussian noise. advantage model fact independent gaussian distributions gaussian distributed hence gaussian process model remains analytic presence noise. unfortunately independent mvts analytically intractable. problem encountered rasmussen williams went dismiss multivariate student-t process practical purposes. approach incorporate noise kernel function example letting parametrized kernel diagonal kernel function. model equivalent adding independent noise since scaling parameter eﬀect squared-exponential kernel well noise kernel. zhang yeung propose similar method handling noise; however incorrectly assume latent function noise independent model. noise uncorrelated latent function independent. gaussian noise. figure consider samples various dimensional processes small signal noise ratio small. noise incorporated kernel behaves similarly independent student-t noise. work consider parametric kernel functions. task using kernels learning parameters chosen kernel called hyperparameters model. include derivatives marginal likelihood respect hyperparameters supplementary material. test student-t process regression model number datasets. sample hyperparameters using hamiltonian monte carlo kernel function squared exponential delta kernel function results experiments summarized table synthetic data sample functions prior gaussian noise data goal predicting test points. function train data points test generalizes superior predictive uncertainty example. figure posterior distribution function maximize prior acquisition functions solid green line acquisition function dotted dashed black lines priors respectively. hyperparameters kept same. note expected improvement current best value choosing sample point given current observations hyperparameters note distribution form derived fbest−˜µ synthetic data construct data drawing functions squared exponential kernel adding student-t noise independently. posterior distribution sample shown figure predictive means also identical since posterior distributions hyperparameters differ superior predictive mean since hyperparameter training better able model student-t noise well better predictive uncertainty. whistler snowfall data. daily snowfall amounts whistler recorded years data exhibits clear changepoint type behaviour seasonality handles much better wine data. dataset cortez consists attributes various wines including acidity density alcohol level. wine given corresponding quality score choose random subset wines training testing. machine learning algorithms often require tuning parameters control learning rates abilities optimizing objective function. model objective function using gaussian process powerful iterative optimization procedure known gaussian process bayesian optimization pick query objective function next optimize expected improvement running optimum probability improving current best upper conﬁdence bound. figure function evaluations synthetic function branin-hoo function hartmann function evaluations student-t process prior gaussian process prior shown. error bars represent standard deviation runs. panel minimizing objective function. vertical axis represents running minimum function value. hartmann function function local minima runs initialised observations corners unit cube notice tends behave like step function whereas gaussian process’ rate improvement somewhat constant. reason behaviour tends thoroughly explore modes found before moving away modes. phenomenon seems prevalant higher dimensions. shown inverse wishart process appropriate prior covariance matrices arbitrary size. used prior kernel showed marginalizing results student-t process consistent marginals closed form conditionals contains gaussian process special case. also proved elliptical process analytically representable density function. prior applied regression bayesian optimization tasks showing improved performance additional computational costs. take home message practitioners many beneﬁts increased modelling ﬂexibility extra cost. work suggests could useful replace almost application. added ﬂexibility orthogonal choice kernel could complement recent expressive closed form kernels future work. intuition changes behaviour acquisition function study example figure hyperparameters plot acquisition functions varying example clear certain scenarios prior prior lead diﬀerent proposals given information. compare prior mat´ern plus delta function kernel prior kernel bayesian optimization. integrate away uncertainty slice sample hyperparameters consider functions -dim synthetic sinusoidal -dim branin-hoo function -dim hartmann function. results shown figure", "year": 2014}