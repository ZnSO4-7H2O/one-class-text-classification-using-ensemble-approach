{"title": "Why is Posterior Sampling Better than Optimism for Reinforcement  Learning?", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\\tilde{O}(H S \\sqrt{AT})$ for any reinforcement learning algorithm.", "text": "mate future value selects action greatest estimate. selected action near-optimal estimate must overly optimistic case agent learns experience. efﬁciency relative less sophisticated exploration arises agent avoids actions neither yield high value informative data. alternative approach based thompson sampling involves sampling statistically plausibly action values selecting maximizing action. values generated example sampling posterior distribution mdps computing state-action value function sampled mdp. approach originally proposed strens called posterior sampling reinforcement learning computational results osband demonstrate psrl dramatically outperforms existing algorithms based ofu. primary paper provide insight extent performance boost phenomenon drives show that bayesian expectation constant factors psrl matches statistical efﬁciency standard algorithm ofu-rl. highlight shortcomings existing state algorithms demonstrate psrl suffer inefﬁciencies. leverage insight produce bound bayesian regret psrl ﬁnite-horizon episodic markov decision processes horizon number states number actions time elapsed. improves upon best previous bound algorithm. discuss believe psrl satisﬁes tighter though proved that. complement theory computational experiments highlight issues raise; empirical results match theoretical predictions. importantly highlights tension between statistical efﬁciency computational tractability. argue algorithm matches psrl statistical efﬁciency would likely computationally intractable. provide proof claim restricted setting. insight potential beneﬁts exploration guided posterior sampling restricted simple tabular mdps analyze. computational results demonstrate posterior sampling learning dramatically outperforms existing algorithms driven optimism ucrl. provide insight extent performance boost phenomenon drives leverage insight establish bayesian regret bound psrl ﬁnite-horizon episodic markov decision processes. improves upon best previous bayesian regret bound reinforcement learning algorithm. theoretical results supported extensive empirical evaluation. consider reinforcement learning problem agent interacts markov decision process maximizing expected cumulative reward performance agent balances exploration acquire information long-term beneﬁt exploitation maximize expected near-term rewards. principle dynamic programming applied compute bayes-optimal solution problem however computationally intractable anything beyond simplest problems direct approximations fail spectacularly poorly such researchers proposed analyzed number heuristic reinforcement learning algorithms. literature efﬁcient reinforcement learning offers statistical efﬁciency guarantees computationally tractable algorithms. provably efﬁcient algorithms predominantly address exploration-exploitation trade-off optimism face uncertainty state agent assigns action optimistically biased estipart literature efﬁcient sharply divided frequentist bayesian perspective volume papers focus minimax regret bounds hold high probability m∗∈m class mdps bounds bayesregret generally weaker analytical statements minimax bounds regret. regret bound m∗∈m implies identical bound bayesreget support partial converse available drawn non-zero probability hold general another common notion performance guarantee given so-called sample-complexity analyses bound number \u0001-sub-optimal decisions taken algorithm general optimal bounds regret imply optimal bounds sample complexity whereas optimal bounds sample complexity give bound regret formulation focuses simple setting ﬁnite horizon mdps several problems interest literature. common formulations include discounted setting problems inﬁnite horizon under connectedness assumption paper contain insights carry settings leave analysis future work. analysis focuses upon bayesian expected regret ﬁnite horizon mdps. criterion amenable simple analysis obtain actionable insight design practical algorithms. absolutely close book exploration/exploitation problem remain many important open questions. nonetheless work help develop understanding within outstanding issues statistical computational efﬁciency particular shed light posterior sampling performs much better existing algorithms ofu-rl. crucially believe many insights extend beyond stylized problem ﬁnite tabular mdps help guide design practical algorithms generalization exploration randomized value functions well-known connection posterior sampling optimistic algorithms section highlight similarity approaches. argue posterior sampling thought stochastically optimistic algorithm. consider problem learning optimize random ﬁnite-horizon repeated episodes interaction state space action space horizon initial state distribution. time period within episode agent observes state selects action receives reward transitions state note formulation unknown treated random variable often called bayesian reinforcement learning. policy mapping state period action policy deﬁne state-action value function period subscript indicates actions periods ...h selected according policy µh). policy optimal argmaxµ denote history observations made prior time time evolution within episodes abuse notation state period episode deﬁne analogously. algorithm deterministic sequence {πk|k ...} functions mapping probability distribution policies agent samples policy episode. deﬁne regret incurred algorithm time episode typical algorithm constructs conﬁdence represent range mdps statistically plausible given prior knowledge observations. then policy selected maximizing value simultaneously policies mdps set. agent follows policy episode. interesting contrast approach psrl instead maximizing conﬁdence psrl samples single statistically plausible selects policy maximize value mdp. section highlight connection posterior sampling optimistic algorithm spirit section central analysis following notion stochastic optimism deﬁnition real-valued random variables ﬁnite expectation. stochastically optimistic convex increasing notion optimism dual second order stochastic dominance ssd−x. psrl stochastically optimistic algorithm since random imagined value function stochastically optimistic true optimal value conditioned upon possible history function observation leads general relationship psrl bayesregret optimistic algorithm. sketch proof. result established osband special case πopt πucrl. include small sketch refresher guide high level intuition. first note conditioned upon data true sampled identically distributed. means therefore establish bound upon bayesian regret theorem suggest that according bayesian expected regret psrl performs within factor optimistic algorithm whose analysis follows section includes algorithms ucrl ucfh mormax many more. importantly unlike existing approaches algorithm performance separated analysis conﬁdence sets means psrl even attains scaling as-yet-undiscovered approaches computational cost greater solving single known even matched algorithm πopt computationally intractable. section discuss existing algorithms forgo level statistical efﬁciency enjoyed psrl. high level lack statistical efﬁciency emerges sub-optimal construction conﬁdence sets present several insights prove crucial design improved algorithms ofu. worryingly raise question perhaps optimal statistical conﬁdence sets would likely computationally intractable. argue psrl offers computationally tractable approximation unknown ideal optimistic algorithm. launch mathematical argument useful take intuition simple estimation problem without decision making. consider described figure every episode agent transitions uniformly {..n} receives deterministic reward depending upon state. simplicity examples means even naive monte-carlo estimate value concentrate episodes interaction. nonetheless conﬁdence sets suggested state ofu-rl algorithm ucrl become incredibly mis-calibrated grows. problem occurs consider algorithm model-based ofu-rl builds conﬁdence sets state action independently ucrl. even estimates tight state action resulting optimistic simultaneously optimistic across state action optimistic. geometrically independent bounds form rectangular conﬁdence set. corners rectangle misspeciﬁed underlying distribution ellipse combined across independent estimates several algorithms ofu-rl exist address loose dependence upon however algorithms depend upon partitioning data future value leads poor dependence upon horizon equivalently effective horizon discounted problems. similar example figure understand combining independently optimistic estimates time contribute loose bound natural question don’t simply apply observations design optimistic algorithm simultaneously efﬁcient ﬁrst impediment designing algorithm requires intricate concentration inequalities analysis. doing rigorously challenging believe possible careful application existing tools insights raise above. bigger challenge that even able formally specify algorithm resulting algorithm general computationally tractable. similar observation problem optimistic optimization shown setting linear bandits works show problem efﬁcient optimization ellipsoidal conﬁdence sets np-hard. means computationally tractable implementations rely upon inefﬁcient rectangular conﬁdence sets give dimension underlyfactor problem. contrast thompson sampling approaches remain computationally tractable suffer loose conﬁdence construction. remains open question whether algorithm designed ﬁnite mdps. however previous results simpler bandit setting show problems ofu-rl cannot overcome general. computational illustration section present simple series computational results demonstrate looseness sample episodes data examine optimistic/sampled q-values ucrl psrl. implement version ucrl optimized ﬁnite horizon mdps implement psrl uniform dirichlet prior initial dynamics prior rewards updating rewards noise. algorithms known mean true inside ucrl psrl. experiment estimates guided become extremely mis-calibrated psrl remains stable. results figure particularly revealing. demonstrates potential pitfalls ofu-rl even underlying transition dynamics entirely known. several algorithms proposed remedy loose ucrl-style concentration transitions none address inefﬁciency hyper-rectangular conﬁdence sets. expected loose conﬁdence sets lead extremely poor performance terms regret. push full results appendix along comparison several approaches. analyses psrl come comparison existing algorithm ofu-rl. previous work spirit theorem leveraged existing analysis ucrl establish bound upon bayesian regret section present result bounds expected regret psrl also include conjecture improved analysis could result bayesian regret bound hsat psrl result would unimo. section present analysis improves bound bayesian regret proof result somewhat technical essential argument comes simple observation loose rectangular conﬁdence sets section analysis technical lemma gaussian-dirichlet concentration lemma ﬁxed /α/α) dirichlet lemma establish similar concentration bound error sampling lemma independent prior rewards additive sub-gaussian noise independent dirichlet prior transitions state-action pair sketch proof. proof relies heavily upon technical results note osband cannot apply lemma directly since future value random variable whose value depends sampled transition however although vary structure means resultant still optimistic optimistic possible ﬁxed begin proof simply family mdps call write ﬁrst component unknown transition similarly ˆpk. bound transition concentration conclude proof application lemma extend argument multiple states consider marginal distribution subset states beta distributed similar push details appendix proof theorem mirrors standard ofucondense norl analysis section tation write xkh= µkh. rewards ˆrk=e|hk] posterior mean transitions ˆpk=e|hk] respective deviations sampling noise wr=rk−ˆrk ˆpk)t note that conditional upon data true reward transitions independent rewards transitions sampled psrl e|hk] rke|hk] however e|hk] e|hk] generally non-zero since agent chooses policy optimize reward under rewrite regret concentration bellman operator piece analysis show h=|wp concentrates rate independent root argument notion stochastic optimism introduces partial ordering random variables. make particular lemma relates concentration dirichlet posterior matched gaussian distribution completes proof theorem prior work designed similar approaches improve learning scaling mormax delayed q-learning particular come sample complexity bounds linear match lower bounds. even terms sample complexity algorithms necessarily improvement ucrl variants clarity compare algorithms recent analyses suggest simultaneously reducing dependence possible. note local value variance satisﬁes bellman equation. intuitively captures transition state cannot transition anywhere much worse durh behave independent grow unlike analysis crudely upper bounds present sketch towards analysis conjecture appendix conjecture prior rewards additive sub-gaussian noise independent dirichlet prior transitions conjecture particularly interesting aspect conjecture construct another algorithm satisﬁes proof theorem would satisfy argument conjecture appendix call algorithm gaussian psrl since operates manner similar psrl actually uses gaussian sampling analysis psrl algorithm. algorithm gaussian psrl input posterior estimates visit counts output random qkhsoq∗ initialize qkh+← timestep h=hh−.. vkh+←maxαqkh+ sample wk∼n algorithm presents method sampling random qvalues according gaussian psrl algorithm follows samples greedily duration episode similar psrl. interestingly experimental evaluation consistent hsat ucrl gaussian episodes learn optimal policy evaluate several learning algorithms random seeds million episodes each. goal investigate empirical performance scaling. believe ﬁrst ever large scale empirical investigation scaling properties algorithms efﬁcient exploration. highlight results three algorithms bayesian regret bounds ucrl gaussian psrl psrl. implement ucrl conﬁdence sets optimized ﬁnite horizon mdps. bayesian algorithms uniform dirichlet prior transitions prior rewards. view priors simple ways encode little prior knowledge. full details link source code available appendix figure display regret curves algorithms ∈{}. suggested analysis psrl outperforms gaussian psrl outperforms ucrl. differences seems scale length chain even relatively small mdps psrl many orders magnitude efﬁcient ucrl. investigate empirical scaling algorithms respect results theorem conjecture bound bayesian regret according prior family environments consider example decidedly uniform distribution; fact chosen difﬁcult possible. nevertheless results theorem conjecture provide remarkably good description behavior observe. remarkably high level predictions match empirical results almost exactly show figure results provide support conjecture even since spirit environments similar example used existing proofs ongoing questions fundamental lower bounds further note every single seed psrl gaussian psrl learned optimal policy every single believe suggests possible extend bayesian analysis provide minimax regret bounds style ucrl suitable choice diffuse uninformative prior. psrl orders magnitude statistically efﬁcient ucrl computational cost solving known mdp. believe analysts able formally specify approach whose statistical efﬁciency matches psrl. however argue resulting conﬁdence sets address coupling result computationally intractable optimization problem. posterior sampling offers computationally tractable approach statistically efﬁcient exploration. stress ﬁnite tabular setting analyze reasonable model problems interest. curse dimensionality practical settings require generalization states actions. goal paper improve mathematical bound example instead hope simple setting highlight shortcomings existing approaches efﬁcient provide insight algorithms based sampling offer important advantages. believe insights prove valuable move towards algorithms solve problem really care about synthesizing efﬁcient exploration powerful generalization. work generously supported deepmind research grant boeing marketing research award adobe stanford graduate fellowship courtesy paccar. authors would like thank daniel russo many hours discussion insight leading research shipra agrawal lattimore pointing several ﬂaws early proof steps anonymous reviewers helpful comments many colleagues deepmind including remi munos mohammad azar inspirational conversations. asmuth john lihong littman michael nouri wingate david. bayesian sampling approach exploration reinforcement learning. proceedings twenty-fifth conference uncertainty artiﬁcial intelligence auai press bartlett peter tewari ambuj. regal regularization based algorithm reinforcement learning proceedings weakly communicating mdps. conference uncertainty artiﬁcial intelligence june brafman ronen tennenholtz moshe. r-max general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research filippi sarah cappé olivier garivier aurélien. optimism reinforcement learning kullback-leibler divergence. communication control computing annual allerton conference ieee fonteneau raphaël korda nathan munos rémi. optimistic posterior sampling strategy bayesian nips workshop reinforcement learning. bayesian optimization dann christoph brunskill emma. sample complexity episodic ﬁxed-horizon reinforcement learning. advances neural information processing systems szita istván szepesvári csaba. model-based reinforcement learning nearly tight exploration complexity bounds. proceedings international conference machine learning section centers around proof lemma reproduce completeness. main paper present simple sketch special case extend argument general mdps main strategy proof proceed inductive argument consider contribution component turn. that choice component resultant random variable dominated matched gaussian random variable lemma independent prior rewards additive sub-gaussian noise independent dirichlet prior transitions state-action pair analysis lemma rely heavily upon technical analysis osband ﬁrst reproduce lemma osband terms stochastic optimism rather second order stochastic dominance. lemma random variable dirichlet constants next consider ﬁxed xkh) vary arbitrary maximize variation kh+∈ effects future value transition upper bound deviation transitions deviation worst possible current proof theorem bounds however approach loose pre-supposes timestep could maximally single episode. repeat geometric intuition assumed worst-case hyper-rectangle timesteps actual geometry ellipse. therefore suffer additional term every timestep episode since sample gets transition future value deplete. rather independent timestep would enough saving actually kind anti-correlation property total variance. similar observation used recent analyses sample complexity setting also ﬁnite horizon mdps seems suggest possible combine insights lemma with example lemma remove note informal argument would apply gaussian psrl since generates gaussian posterior satisfy bellman operators. therefore able evidence conjecture domains ucrl gaussian psrl psrl demonstrate predicted scalings. present evidence effect section empirical results consistent conjecture. section expand upon simple examples given section full decision problem actions. deﬁne similar figures actions. ﬁrst action identical figure second action modiﬁes transition probabilities favor rewarding states probability assigning non-rewarding states. investigate regret several learning algorithms adapt setting. algorithms based upon bolt \u0001-greedy gaussian psrl optimistic psrl psrl ucfh ucrl link full code implementation appendix loose estimates algorithms figures lead performance decision problem. poor scaling number successor states occurs either rewards transition function unknown. note stochastic environments pac-bayes algorithm bolt relies upon optimistic fake prior data sometimes concentrate quickly incur maximum linear regret. general although bolt pac-bayes concentrates fast pac-mdp like figure similar effect increase episode length note second order ucfh modiﬁcation improves upon ucrl’s miscalibration reﬂected bounds note bolt scale poorly horizon code experiments used paper available full github. review request removed link code instead include anonymized excerpt code submission ﬁle. hope researchers simple codebase useful quickly prototyping experimenting tabular reinforcement learning simulations. addition results already presented also investigate scaling similar bayesian learning algorithms bolt neither algorithms scale gracefully psrl although bolt comes close. however observed appendix bolt perform poorly highly stochastic environments. bolt also requires s-times computational cost psrl beb. include algorithms figure well known provably-efﬁcient algorithms perform poorly practice. response observation many practitioners suggest rescaling conﬁdence sets obtain better empirical performance figure present performance several algorithms conﬁdence sets rescaled ∈{....}. rescaling tighter conﬁdence sets sometimes give better empirical performance. however change fundamental scaling algorithm. also aggressive scalings seeds converge all. bayesian algorithms uninformative independent priors rewards transitions. rewards updated observed noise gaussian precision transitions uniform dirichlet prior dirchlet. figures examine performance gaussian psrl psrl chain length vary algorithms extremely robust several orders magnitude. large values caused problems seeds environment. developing clear frequentist analysis bayesian algorithms direction important future research. optimistic posterior sampling compare implementation psrl similar optimistic variant samples samples posterior forms optimistic q-value envelope sampled q-values. algorithm sometimes called optimistic posterior sampling experiment algorithm several values resultant algorithm performs similarly psrl increased computational cost. display effect several magnitudes figures algorithm optimistic psrl spiritually similar boss previous work suggested could lead improved performance. believe important difference psrl unlike thompson sampling resample every timestep previous implementations compared faulty benchmark", "year": 2016}