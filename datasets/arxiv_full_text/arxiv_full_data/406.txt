{"title": "Visual Features for Context-Aware Speech Recognition", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "abstract": "Automatic transcriptions of consumer-generated multi-media content such as \"Youtube\" videos still exhibit high word error rates. Such data typically occupies a very broad domain, has been recorded in challenging conditions, with cheap hardware and a focus on the visual modality, and may have been post-processed or edited. In this paper, we extend our earlier work on adapting the acoustic model of a DNN-based speech recognition system to an RNN language model and show how both can be adapted to the objects and scenes that can be automatically detected in the video. We are working on a corpus of \"how-to\" videos from the web, and the idea is that an object that can be seen (\"car\"), or a scene that is being detected (\"kitchen\") can be used to condition both models on the \"context\" of the recording, thereby reducing perplexity and improving transcription. We achieve good improvements in both cases and compare and analyze the respective reductions in word error rate. We expect that our results can be used for any type of speech processing in which \"context\" information is available, for example in robotics, man-machine interaction, or when indexing large audio-visual archives, and should ultimately help to bring together the \"video-to-text\" and \"speech-to-text\" communities.", "text": "automatic transcriptions consumer generated multi-media content youtube videos still exhibit high word error rates. data typically occupies broad domain recorded challenging conditions cheap hardware focus visual modality postprocessed edited. paper extend earlier work adapting acoustic model dnn-based speech recognition system language model show adapted objects scenes automatically detected video. working corpus how-to videos idea object seen scene detected used condition models context recording thereby reducing perplexity improving transcription. achieve good improvements cases compare analyze respective reductions word error rate. expect results useful type speech processing context information available example robotics machine interaction indexing large audio-visual archives ultimately help bring together video-to-text speechto-text communities. robustness adaptation signal variability challenge automatic speech recognition systems become universally useful. could achieved adapt acoustic model language model broad context input. context mean essentially anything known input speech. work used extreme science engineering discovery environment supported national science foundation grant number oci-. submitted publication fond memory yajie miao state-of-the-art recognition accuracy wide range acoustic modeling tasks deﬁned dnns variants thereof. consumer-generated content however even models exhibit word error rates although standardized test exists. work reports signiﬁcantly lower higher wers showing wide variability exists data. recently word error rates recorded extremely high resource setup effective strategy deal variability incorporate additional longer-term knowledge explicitly models study incorporation speakerlevel i-vectors smooth effect speaker variability. time delay neural networks wide temporal input windows improve robustness dynamically. extracts long-term averages audio signal adapt acoustic model. similarly learn dnn-based extractor model speaker-microphone distance information dynamically frame level. distance-aware dnns built appending descriptors inputs. important distinction work require localization regions and/ extraction framesynchronous visual features case traditional audio-visual developed mostly focus noise robustness. majority data lip-related information available quality extremely poor. paper present extension comparison previously published work adapting acoustic model also language model system visual context present video stream open-domain internet video. approach based deep learning involves major steps first extract visual features using deep convolutional neural networks trained object recognition scene labeling tasks. extract information single random frame within utterance only rather level frame levels granularity smoothing approaches also possible. thus require perfect alignment audio video channels often almost impossible achieve data then adapt acoustic model recognizer using framework residual error feature inputs reduced adaptation network. network trained context vector predicts linear shift main dnn’s input features. central idea somewhat similar resnets originally developed ivector based adaptation works well adaptation knowledge sources well. ﬁrst-pass decoding in-domain -gram language model. adapt language model recognizer re-rank -best lists language model conditioned segment-level context vector acoustic model. show approach results signiﬁcant reductions perplexity also reduces word error rate. extraction visual features follows previous work adaptation dnns using speaker attributes visual features suppose dealing utterance acoustic features total number speech frames. video transcribing task always exists video segment corresponding video segment represented number video frames. video frames sampled normally lower sampling rate speech frames i.e. segment randomly select frame serves image representation utterance. types image features extracted ﬁrst type visual information derived object recognition task deep learning accomplished tremendous success intuition object features contain information regarding acoustic environment. example classifying image classes computer keyboard monitor indicates speech segment recorded ofﬁce. extract object information using deep model trained comprehensive object recognition dataset million image subset imagenet used ilsvr challenge resulting model referred object-cnn. then target task video frame model distribution object classes. probabilities encode object-related information ﬁnally incorporated acoustic models. object-cnn network follows standard alexnet architecture network contains convolution layers rectiﬁer non-linearity activation function. ﬁrst second convolution layers local response normalization layer added relu activation pooling layer follows layer. third fourth convolution layers apply pooling layers. ﬁfth convolution layer apply pooling layer without applied. fully-connected layers placed convolution layers. ﬁrst second layers neurons whereas number neurons last layer equal number classes case. model training optimizes standard cross-entropy objective. resulting object-cnn achieves top- error rate ilsvrc testing set. utility object features comes place information implicitly encoded object classiﬁcation results. natural utilize place features explicit way. achieve this train deep model meant scene labeling task. given video frame classiﬁcation outputs place-cnn encode place information incorporated acoustic models. convenience formulation resulting visual feature vector utterance represented order extract place information train placecnn network places dataset contains million images belonging scene categories. examples scenes include dining room coast conference center courtyard etc. complete million images training follow image pre-processing used imagenet architecture place-cnn almost object-cnn. difference ﬁnal layer place-cnn neurons corresponding scene classes whereas object-cnn contains neurons. chose investigate context-aware dataset real-world english instructional videos downloaded online video archives videos uploaded social media users share expertise speciﬁc tasks videos challenging recorded various environments giving variety contexts rich speech making suitable proposed work. main training comprises hours speech hours testing used kaldi pdnn experiments training -layer acoustic model using cross-entropy decoding trigram language model trained training transcripts. interpolated another trigram trained additional hours transcriptions instructional videos. complete hours also used training language model. previous work presented framework perform speaker adaptive training models. approach requires i-vector extracted speaker. based well-trained speaker-independent separate adaptation neural network learned convert i-vectors speaker-speciﬁc linear feature shifts. adding shifts original inputs produces speaker-normalized feature space. parameters si-dnn re-updated space generating sat-dnn model. framework also applied successfully descriptors speaker-microphone distance robust straightforward feature concatenation port idea visual input features enables conduct context adaptation dnns simply replacing i-vector representation visual features. adaptation network learned take visual features inputs generate adaptive feature space respect visual descriptors. note case linear feature shifts generated adaptation network utterance-speciﬁc rather speaker-speciﬁc. re-updating parameters normalized feature space gives adaptively trained video adaptive training vatdnn model vat-dnn model takes advantage visual features additional knowledge generalizes better unseen variability. setup generate -dimensional utterance-level visual context features projecting output vector placecnn object-cnn dimensions using principal component analysis outputs adaptation network -dimensional shifts lmel features. adapt language model used features also adapting acoustic model topic information context dependent recurrent neural network language model vocabulary contains words. two-layer bidirectional lstm embedding layer size cells layer gave lowest perpexities initial cross-validation experiments hours data. architecture thus adopted experiments below. table word error rates applying acoustic model adaptation using object features place features combination thereof i-vectors combination visual features i-vectors used tanh non-linearities dropout factor withadditional regularization except gradient clipping initial learning rate training used adagrad batch size network implemented lasagne reduce dimensionality adaptation feature facilitate comparison earlier work i-vector adaptation reduce dimensionality place object features using estimated training part audio-visual dataset. table shows result adapting acoustic model visual features i-vectors comparison well combination visual features i-vectors. gains consistent quite complementary using concatenation visual features i-vectors adaptation. also cases adaptation network method outperforms simply concatenating adaptation vector input features. next method adapt language model visual information. best meta parameters lstm language model performed -fold cross-validation entire hours training data averaged results. figure shows conditioning lstm video features reduces perplexity training data signiﬁcant reduction also carry word error rates unseen test data. generated -best lists using baseline acoustic models oracle re-ranked neural network language models averaging language model scores. using concatenation object place features inputs achieve word error rate close performance achieved adaptation acoustic model. training hours data only adaptation object-cnn features results perplexity adaptation place-cnn features gives perplexity seems intuitive objects would slightly salient topic how-to video scene. figure shows typical keyframes database typical pattern improvements acoustic model adaptation tends give signiﬁcant improvements outdoor videos language model adaptation tends give smaller improvements across board paper described system extracts context information relevant speech processing visual channel video. showed information incorporated acoustic language models approach leads systematic consistent improvements. observations line recent work multi-modal machine translation currently expanding acoustic model adaptation experiments larger version corpus expect performance improvements combining acoustic language model adaptation. also experimenting different better ways incorporating video features language model attempt insightful analysis results e.g. much different types features contribute different models types errors reduced long term work help improve fully end-to-end video-to-text approaches generate image video summaries based multi-modal embeddings reference captions rather speech recognition transcriptions. dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. fig. training validation perplexity trained without visual features averaged across folds hours data. parameters optimized adaptive hence baseline converges quickly. validation perplexity lower training perplexity initially measured training iteration training perplexity computed processing data updating model. fig. keyframes typical videos dataset. baseline home video left train video right. acoustic model adaptation improve home video reduces train video. language model adaptation improves videos slightly respectively. manually inspected videos observed relative reductions recorded either outdoor environments nontypical indoor conditions music/ noise interfere actual speech lot. adding scene descriptors helps model normalize acoustic characteristics rare conditions thus beneﬁts generalization unseen testing speech. labeled testing videos either typical indoors other analyzed relative improvements system adapted place-cnn features only quiet seide chen feature engineering context-dependent deep neural networks conversational speech transcription proc. asru. ieee liao mcdermott senior large scale deep neural network acoustic modeling semi-supervised training data youtube video transcription proc. asru. ieee gupta kenny ouellet stafylakis i-vectorbased speaker adaptation deep neural networks french broadcast audio transcription proc. icassp. ieee senior lopez-moreno improving speaker independence i-vector inputs proc. icassp. ieee miao zhang metze towards speaker adaptive training deep neural network acoustic models proc. interspeech singapore sept. isca. miao zhang metze speaker adaptive training deep neural network acoustic models using i-vectors ieee/acm transactions audio speech language processing vol. nov. waibel hanazawa hinton shikano lang phoneme recognition using time-delay neural networks ieee transactions acoustics speech signal processing vol. vesel watanabe karaﬁ honza sequence summarizing neural network speaker adaptation ieee international conference acoustics speech signal processing ieee yuhas goldstein sejnowski integration acoustic visual speech signals using neural networks ieee communications mag. vol. zhang deep residual learning miao jiang zhang metze improvements speaker adaptive training deep neural networks proc. ieee workshop spoken language technology south lake tahoe dec. ieee best poster presentation. krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems zhou lapedriza xiao torralba oliva learning deep features scene recognition using places database advances neural information processing systems duchi hazan singer adaptive subgradient methods online learning stochastic optimization journal machine learning res. vol. july dieleman schl¨uter raffel olson sønderby nouri maturana thoma battenberg kelly lasagne first release zenodo", "year": 2017}