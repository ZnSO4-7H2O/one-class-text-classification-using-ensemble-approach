{"title": "Memorize or generalize? Searching for a compositional RNN in a haystack", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Neural networks are very powerful learning systems, but they do not readily generalize from one task to the other. This is partly due to the fact that they do not learn in a compositional way, that is, by discovering skills that are shared by different tasks, and recombining them to solve new problems. In this paper, we explore the compositional generalization capabilities of recurrent neural networks (RNNs). We first propose the lookup table composition domain as a simple setup to test compositional behaviour and show that it is theoretically possible for a standard RNN to learn to behave compositionally in this domain when trained with standard gradient descent and provided with additional supervision. We then remove this additional supervision and perform a search over a large number of model initializations to investigate the proportion of RNNs that can still converge to a compositional solution. We discover that a small but non-negligible proportion of RNNs do reach partial compositional solutions even without special architectural constraints. This suggests that a combination of gradient descent and evolutionary strategies directly favouring the minority models that developed more compositional approaches might suffice to lead standard RNNs towards compositional solutions.", "text": "neural networks still need specialized specific tasks little cross-task transfer require huge amounts training data perform well reason limitations able perform compositional learning discover store skills common across problems re-combine hierarchical fashion solve challenges ability perform compositional learning would provide better generalization therefore result reduction sample complexity learning algorithms. stark contrast neural networks compositional abilities generally agreed core aspect human cognition direct evidence claim humans compositional learners provided explored human intuitions functions extrapolation completion experiments concluded intuitions best described compositional. strikingly shown year olds generalize function composition chance even trained composition process itself. view clear advantages compositional learning growing interest equipping neural networks compositional abilities opposed line research paper explore compositional generalization capabilities standard recurrent neural networks without special architectural constraints. first introduce lookup table composition domain simple highly flexible setup test compositional behaviour analytically sketch represent compose functions demonstate learn behaviour explicit supervision provided hidden layer finally attempt rnns discover compositional solution sets tasks standard example-driven gradient-descent-based training examine proportion trained models discover solution. experiments show average training converge behaves compositionally large random search initializations compositional solution discovered small non-negligible number cases. interestingly convergence compositional solution determined initialization model rather seemingly minor random factors order task presentations weight updates. results suggest combination gradient descent evolutionary strategies directly favouring minority models developed compositional approaches might specifically authors show participants prefer compositional noncompositional function extrapolations samples human prior functions best described compositional model people perceive compositional functions predictable non-compositional otherwise similar counterparts. abstract neural networks powerful learning systems readily generalize task other. partly fact learn compositional discovering skills shared different tasks recombining solve problems. paper explore compositional generalization capabilities recurrent neural networks first propose lookup table composition domain simple setup test compositional behaviour show theoretically possible standard learn becompositionally domain trained standard gradient descent provided additional supervision. remove additional supervision perform search large number model initializations investigate proportion rnns still converge compositional solution. discover small non-negligible proportion rnns reach partial compositional solutions even without special architectural constraints. suggests combination gradient descent evolutionary strategies directly favouring minority models developed compositional approaches might suffice lead standard rnns towards compositional solutions. keywords compositional learing lifelong learning finite-state automata reference format adam liška germán kruszewski marco baroni. memorize generalize? searching compositional haystack. proceedings york article pages. https//doi.org/./ nnnnnnn.nnnnnnn permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights third-party components work must honored. uses contact owner/author. copyright held owner/author. isbn -x-xxxx-xxxx-x/yy/mm.... https//doi.org/./nnnnnnn.nnnnnnn composing table lookup functions ability discover apply function composition natural starting point test compositional skills learning system. mastering function composition increases expressivity system function composition system could handle recursion allowing process infinite number objects finite means. combinatorial infinity property observed language cognitive domains since focus composition itself rather ability learning system solve sophisticated primitive tasks consider tasks require rote memorization namely table lookup tasks. possible strings fixed length table lookup function arbitrary bijective mapping string onto itself. example possible strings consequently distinct mapping tables possible permutation string corresponds distinct output assignment table lookup functions share domain co-domain output function well-formed input function thus generate infinite number functions composition. example apply function compositions forth; abbreviated notation stands tasks attempt make presence composition explicit require models produce output composed table lookup output intermediate steps well. illustrated compositions -bit string tables table also shows present functions order application rather innermost outermost standard mathematical notation. important advantage table lookup tasks clear separation atomic composed tasks allows straightforward evaluation compositional behaviour part learning system. consider learning system mastered mappings presented several input-output pairs composed function behaves compositionally understands mapping composition underlying mappings trouble producing correct output unseen inputs composed mapping i.e. able perform zero-shot generalization. «ncg.» represents atomic task network starts producing output soon character signals input string. output considered correct output string consists correct outcome function application specified input output string terminated dot. similarly figure shows sample episode function composition objective therefore bears similarities traditional language modelling objective experiments paper used randomly-generated atomic tasks based -bit lookup tables composed tasks representing possible compositions atomic tasks. furthermore limit single-composition composed tasks. model used experiments neural network hidden layers recurrent lstm layer units sigmoid layer units input layer model consists concatenated one-hot vector encodings input character output character previous step. architecture motivated compositional solution propose section below. input vocabulary network consists characters specifying type task task codes bits punctuation marks space output layer softmax layer three units three possible output characters «.». step output character highest score selected. model training implemented pytorch. adam algorithm used optimization. details training procedures described separately experiments sections below. first experiment tested whether exist weights character-level give place model demonstrating compositional behaviour. case would show principle described architecture achieve strong segment encodes current atomic task encodes following atomic task using one-hot encoding; segments represent \"call\" stack segment encodes input string using three segment represents index output string i.e. encodes output string output segment stores characters output previous steps current mapping task encoding consists total units. note hidden layer records information provided input string encode actual characters output; particular remember lookup tables left separate layer consisting sigmoid units output layer. units sigmoid layer non-zero weights connections segments segments directly affecting character output step. example consider input «pcgc.». units recurrent layer output values approximately zero beginning episode. network reads input appropriate units change output zero values close one. specifically reading third character seventh unit segment starts output value first mapping performed. reading next character third unit segment activates since second mapping perform next input string encoded segment finally network reads character first unit segment activates signaling network produce first output first mapping next step first output stored segment second unit segment activated signaling second output mapping limit specific maximum number lookup tables finite length strings maximum number possible composition steps simple approach model lookup table compositions finite-state automata instead producing weights directly hand designed encoding scheme recurrent layer encodes state automaton directly supervised output layer conform scheme. encoding scheme learned proceeded training mapping state representations output characters. state solving lookup table compositions needs encode following pieces information stack atomic tasks perform input string current task index output string bits produced current task state information represented recurrent layer form binary code. specifically case units hidden layer divided following segments produced. output bits mapping generated contents segment moved contents moved first four units segment output character produced previous step appended segment furthermore first unit segment activated indicating first output mapping produced. training evaluation. weights network trained phases encoding scheme needs learned prior mapping state representations output characters. first phase network learns right transitions recurrent layer states presence specific input. this generated random pairs input-output strings correct form atomic composed tasks. input-output pair furthermore generated sequence binary vectors length representing target values recurrent layer time step using encoding scheme described above. used vectors direct supervision output recurrent layer trained weights input-to-hidden recurrent connections stochastic gradient descent backpropagation time updating weights episode total training episodes. second phase proceeded train mappings recurrent layer state representations output characters step froze weights input-to-hidden recurrent connections trained connections recurrent sigmoid layers sigmoid output layer. generated random task consisting atomic tasks. sampled episodes atomic tasks trained network using stochastic gradient descent updating weights episode. note need training composed tasks ability produce correct output strings composed tasks follow automatically atomic tasks learned thanks specific pre-training recurrent layer transitions first phase training connections hidden layers. final network evaluated possible inputs atomic associated composed tasks. results. trained network produces correct output across atomic composed tasks cases. note network seen composed tasks training generalization capabitilies purely transition logic recurrent layer connectivity hidden layers. experimental result confirms weightspace implement specific compositional solution sketched above learn provided direct supervision structure automaton. however results imply devised binary encoding natural solution problem practice discover compositional solution provided input/output examples training signal. questions pursued following section. figure average success rate atomic atomic+composed lookup training figure generated five random task sets atomic composed tasks trained randomly initialized networks task set. success rate shown averaged across success rates networks. investigate proportion rnns converge compositional solution purely training input/output examples large random search model initializations train networks standard cross-entropy loss gradient descent atomic composed tasks test zero-shot compositional generalization check rnns batch discovered compositional solution time without external guidance. training. generated random task atomic lookup tasks composed tasks. models architecture previous experiment trained phases first episodes drawn atomic tasks later tasks sampled across atomic composed tasks composed task withheld input strings used evaluation model initialized random weights drawn uniform distribution zero biases trained backpropagation time. used crossentropy loss updated weights episode training model performed asynchronously cpus parallel. shown figure first phase models mastered atomic tasks second mastered composed tasks seen inputs. also explored pure search-based approach networks randomly initialized directly tested without training. network sort behaves better chance level. evaluation. evaluate compositionality trained models zero-shot generalization withheld inputs composed tasks. case test models unseen composed task+input combinations report percentage correctly answered test items generalization performance. baselines. evaluated several random baselines. simplest random-output produces output strings randomly sampling three possible output characters «.». learning expected form output sequence relatively easy network also evaluated baseline random-wellformed-output randomly samples bits appends test items. lastly evaluated baseline random-task-code equivalent model employed experiment whose training input strings composed tasks altered consistent relation task codes underlying tasks. last baseline meant capture kind statistical biases task would captured fully random baseline. baselines random-output random-wellformed-output evaluated times test dataset baseline random-task-codes evaluated trained models. results. figure shows overall distribution runs terms generalization performance. comparison figure shows baseline random-task-codes generalization performance distribution across runs models table shows average generalization performance baselines. runs figure show performance well baselines successful generalization. however also observe tail models generalize well models reach zero-shot accuracy models reach zero-shot accuracy none baseline models achieve performance anywhere close levels. thus conclude rnns trained standard gradient descent methods task involving composition occasionally discover compositional solution allows generalize zero-shot. chances randomly stumble upon however quite slim. first follow-up question whether compositional rnns learned parse language prompts thus interpret «gc» sequence instruction apply lookup table followed lookup table stronger form compositionality akin encounter natural language string composition mirrors meaning composition figure suggests obfuscating prompts longer possible identify atomic tasks involved compositional operation affect overall performance curve. thus even rnns converged compositional solution latter associated arbitrary codes must memorized rather decompositional analysis prompts. next inquire importance curriculum used main experiment started teaching model perform atomic lookups later added compositional tasks. figure suggests that training composed tasks only larger number models drop baseline level generalization hand also considerably larger this observation supported additional experiments network trained subset composed tasks tested unseen composed tasks. network generalize well tasks confirms learning decode prompt structure proportion models learn generalize correctly together previous experiment suggests case successful models fail relate atomic composed task codes. also probably failing exploit knowledge atomic tasks solving composed tasks. success zero-shot generalization models trained solely composed tasks suggests models inducing representation atomic lookups learning composed tasks rather exploiting representations acquired atomic tasks training. finally importantly test whether different initializations properties lead models generalize better others. figure report distribution re-runs successful initializations random search experiment. figure report distribution worst initializations original experiment. surprisingly figures suggest initializations effect related work idea statistical learning systems specifically neural networks capable skill composition around long time particularly domain reinforcement learning natural frame higher-level tasks hierarchical compositions simpler actions domain composition almost always consists temporally concatenating sequences actions thus lacks recursive properties proper function composition discussed section trained solve specific tasks plus controller gating system learns module call point time. modular approach recently greatly extended applied problems visual question answering require proper function composition tasks tackled models much complex table lookup composition models must make strong priori assumptions structure controller modules combined. moreover require direct supervision module sequence applied degree hand-coding module functionality. reasons difficult approaches could scale genuine lifelong learning scenarios faced open-ended skills acquired. promising recent work focuses skill composition. proposed architecture separate skill networks produce embeddings composed differentiable composition function. still system requires separate training skill networks composition function. finally compositional skills sequence-to-sequence recurrent networks recently evaluated framework simple compositional navigation environment showing rnns fail generalization requires systematic compositional skills discussion studied question whether recurrent neural network learn solve function composition task compositionally storing constituent functions combining solve problems zero-shot fashion. specific table-lookup domain considered find theoretically possible learn behave compositionally sense above least finite number compositions. moreover large random search shows certain proportion rnns converged compositional solution indicated successful generalization unseen inputs composed tasks well chance levels. seem however perform weaker form composition rely analyzing composed task prompts suggesting networks represent latter single undecomposable units index specific atomic composed tasks. interestingly results show initializations least range explored experiments little effect final performance networks suggesting instead seemingly minor random factors order task presentations weight updates determine whether path taken model memorization-based compositional. future research would like first gain better understanding compositional strategies induced best models. approach extracting fsas learned rnns i.e. opposite fsa-into-rnn process performed section second would like devise training regimes leading rnns fully compositional solutions stable ways. insight current training regime explicitly reward zero-shot generalization evaluated test time models trained standard cross-entropybased gradient descent large number repetitive examples. gradient-based techniques hard apply generalization objective cannot naturally formulated differentiable terms. thus following experiments plan switch flexible evolutionary techniques using zero-shot generalization held-out compositions fitness criterion. switching evolutionary approach also opens interesting possibilities terms neural network plasticity would like explore architectures grow larger training might encourage modular structures turn favour compositionality time huge power afforded methods come difficulties making converge. informally experimented popular neat algorithm applied lookup tables able solve even atomic tasks. third future work take advantage full potentials table lookup domain. include testing generalization compositions working longer strings testing comprehension prompt language evaluating zero-shot compositions instead zero-shot inputs only. finally would like explore extent results obtained domain generalize compositional problems acknowledgments thank allan jabri rahma chaabouni earlier versions code used experiments. also thank angeliki lazaridou tomas mikolov brenden lake david lopez-paz josé hernández-orallo klemen simonic armand joulin alexander miller inspiration ideas feedback. giles clifford miller dong chen hsing-hen chen guo-zheng yee-chun lee. learning extracting finite state automata secondorder recurrent neural networks. neural computation ronghang jacob andreas marcus rohrbach trevor darrell kate saenko. learning reason end-to-end module networks visual question answering. proceedings iccv. venice italy justin johnson bharath hariharan laurens maaten fei-fei lawrence zitnick ross girshick. clevr diagnostic dataset compositional language elementary visual reasoning. proceedings cvpr. honolulu brenden lake marco baroni. generalization without systematicity compositional skills sequence-to-sequence recurrent networks. https //arxiv.org/abs/.. tomas mikolov martin karafiát lukás burget cernocký sanjeev khudanpur. recurrent neural network based language model. proceedings interspeech. makuhari japan marvin minsky. society mind. simon schuster york. richard montague. universal grammar. theoria steven piantadosi richard aslin. compositional reasoning early eric schulz josh tenenbaum david duvenaud maarten speekenbrink samuel gershman. probing compositionality intuitive functions. proceedings nips. barcelona spain andrea soltoggio kenneth stanley sebastian risi. born learn inspiration progress future evolved plastic artificial neural networks. http//arxiv.org/abs/..", "year": 2018}