{"title": "Transfer Learning for Speech Recognition on a Budget", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.", "text": "end-to-end training automated speech recognition systems requires massive data compute resources. explore transfer learning based model adaptation approach training models constrained memory throughput training data. conduct several systematic experiments adapting wavletter convolutional neural network originally trained english german language. show technique allows faster training consumer-grade resources requiring less training data order achieve accuracy thereby lowering cost training models languages. model introspection revealed small adaptations network’s weights sufficient good performance especially inner layers. automated speech recognition task translating spoken language text real-time. recently end-to-end deep learning approaches surpassed previously predominant solutions based hidden markov models. influential paper amodei used convolutional neural networks recurrent neural networks redefine state art. however amodei also highlighted shortcomings deep learning approach. performing forward backward propagation complex deep networks reasonable amount time requires expensive specialized hardware. additionally order large number parameters deep network properly needs train large amounts audio recordings. time recordings need transcribed hand. data adequate quantities currently available languages english. propose approach combining methodologies address shortcomings. firstly simpler model lower resource footprint. secondly apply technique called transfer learning significantly reduce amount non-english training data needed achieve competitive accuracy task. investigate efficacy approach specific example adapting cnn-based end-to-end model originally trained english recognize german speech. particular freeze parameters lower layers retraining upper layers german corpus smaller english counterpart. expect approach yield following three improvements. taking advantage representation learned english model lead shorter training times compared training scratch. relatedly model trained using transfer learning requires less data equivalent score german-only model. finally layers freeze fewer layers need back-propagate training. thus expect decrease memory usage since maintain gradients layers. paper structured follows. section gives overview transfer learning approaches tasks. details implementation wavletter model trained found section data used preprocessed described section short introduction performed experiments section present discuss results section followed conclusion section annotated speech data sufficient quantity quality train end-to-end speech recognizers scarce languages english. nevertheless demand high-quality method known transfer learning machine learning technique enhancing model’s performance data-scarce domain cross-training data domains tasks. several kinds transfer learning. predominant applied heterogeneous transfer learning involves training base model multiple languages simultaneously. achieves competitive results still requires large amounts data yield robust improvements terms much data needed effective retraining much promising type transfer learning called model adaptation technique first train model languages retrain parts another language unseen first training round. parameters learned first language serve starting point similar effect pre-training. schultz applied technique first learning multilayer perceptron multiple languages relatively abundant data english getting competitive results languages like czech vietnamese much data available. method presented paper differs schultz force representation compressed bottleneck features result output pre-trained network. idea freezing certain layers another approach differs. reasons amodei train network using many gpus complexity. uses convolutional recurrent units stacked many layers. recently much simpler architecture called wavletter proposed collobert model sacrifice accuracy faster training. relies entirely loss function handle aligning audio transcription sequences network consists convolutional units. resulting shorter training time lower hardware requirements make wavletter solid basis transfer learning experiments. since general structure network described collobert specify optimizer used. tried several conventional gradient descent optimizers achieved best convergence adam hyperparameters slightly adapted defaults given kingma used learning rate collobert note choice activation function inner convolution layers seem matter. chose rectified linear units activation function shown work well acoustic models weights initialized xavier uniformly introduced glorot bengio test time decoding performed using beam search algorithm based kenlm decoding procedure follows tensorflow implementation based beam scored using hyperparameters derived using local search optimized yield best combined word error rate letter error rate librispeech validation set. weight language model chose weight multiplied number vocabulary words transcription wvalid word implemented keras language model beam search done tensorflow introspection numpy source code found https//github.com/transfer-learning-asr/transferlearning-asr. innovations collobert introduction autosegcriterion loss function. authors reported mainly improving model’s throughput negligible effect compared connectionist temporal classification loss introduced graves since currently publicly available implementation loss function decided stay existing tensorflow implementation loss instead. german models trained several corpora taken bavarian archive speech signals well dataset described radeck-arneth referred radeck overall total hours training data slightly third english corpus. additional quantitative information regarding corpus well available references given table information kind recording contained corpus given table also important point corpora pose additional challenges speech recognition like partially intoxicated people recordings telephone different dialects. german corpus split training test sets. grouped audio speakers used groups testing. therefore speaker appears training test ensuring results overfitting certain speakers. exceptions procedure corpora used exclusively training obtaining split based speakers trivial here; used testing consists recordings speakers german second language strong foreign accents hours size; radeck used original splits. also rely text corpora kenlm decoding step. english corpus provided -gram model based training transcriptions used like original wavletter implementation. german corpus n-gram model came preprocessed version german wikipedia european parliament proceedings parallel corpus training transcriptions. validation test sets carefully excluded. preprocessing since english model trained data sampling rate german speech data needed brought format convolutional filters could operate timescale. data resampled khz. preprocessing done using librosa consisted applying short-time fourier transform obtain power level spectrum features audio described able reproduce results reliably. transfer learning experiments based model experiments assumed model already given transfer learning task performed. table quantitative information corpora used train german model. references individual corpora given available. size number speakers refer subsets used test reported best transfer learning model model scratch training. control commands answering questions answer last hour? phonetically balanced sentences stories buttergeschichte nordwind sonne sentences train query task numbers phonetically balanced sentences free-form responses questions phonetically balanced sentences numbers nordwind sonne free dialogue free retelling enkel grossvater dialogues appointment scheduling collobert that spectrum features mel-scaled directly cnn. originally parameters window length stride number components adapted window length wnew equals fourier transform window samples order follow convention using power-of-two window sizes. stride snew order achieve overlap successive frames. observed results many components limited window length. therefore decreased parameter nnew generation spectrograms normalized mean standard deviation input sequence. individual recordings german corpora longer seconds removed memory limitations. could solved instead splitting audio files using word alignments provided chose since loss data incurred simply ignoring overly long files negligible. corpora sizes given table removal said sequences. excluded invalid samples radeck corpus truncated audio well samples empty transcriptions. german corpora. means gradient calculated weights layers gradient descent applied update usual. process freezing layers visualized figure transfer training performed based original weights well random initialization comparison. except changing training data german corpora introduce four class labels ¨a¨o¨uß addition original labels. initial weights biases final softmax layer labels zero. additionally baseline performance wavletter based german trained model scratch german training corpora. experiments used batch size training well evaluation. initially hypothesized transfer learning could give three benefits reduced computing time lower memory requirements smaller required amount german speech data. addition that find structural similarities languages task. subsequent sections first report general observations evaluate hypothesis based performed experiments analyze learned weights using introspection techniques. report overall test scores scores test form wers lers. finally discuss language specific assumptions required experiments transfer learning perform languages. retaining reinitializing weights? transfer learning training performed could either continue training existing weights reinitialize them. reusing existing weights might lead stochastic gradient descent stuck local minimum reinitializing take longer converge. compared speed training methods. seen figure using existing weights much faster without decrease quality. reduced computing time given languages share common features pronunciation lower layers contain common features reused transferring model different language. therefore subsequently froze layers original english model choosing different experiment. figure learning curves hours training different numbers frozen layers. note decreased time process batch training models higher allows iterate training data often amount time. eventually help beat model trained fewest dataset iterations still time achieves lowest loss. bigger need backpropagate fewer layers therefore training time step decreases almost monotonically figure despite boost training time experiments show loss almost always smaller given point time smaller figure little german speech data required hypothesized little training data required transfer learning task. additionally using whole hours data available also tried even scarce variant. order prevent overfitting used transfer learning model experiments. seen figure model trained hours using hours audio almost equal using complete training data. longer training causes overfitting. using hours training data problem occurs even earlier. conclude even though training hours works well hours audio beyond overfitting appears nevertheless. compare best transfer learning model german model trained scratch figure able huge improvements terms computing time required achieving loss. conclude good weight starting configuration another language’s beneficial. model introspection applying transfer learning interest much model needs adapted portions model shared different languages. insights differences compared learned parameters english model adapted german model well different points time training. since output layers models number output features excluded layer comparison. first investigated distribution weights corresponding changes english adapted model visualized left side figure plot shows fraction lower memory requirements matter long training takes given resources many researchers also limited memory disposal. experiments performed single geforce titan graphics card layers freeze fewer layers need backpropagate through. therefore memory requirements lower. batch size forward propaweights layer lying respective range values. weights used log-scale fraction weights bin. observed weights highest absolute values input topmost layer. indicates transformations middle layers smaller outer ones. moreover weights layer distributed mean value close zero small variance. similar distributions reasonable compare weights differences following. models minor changes weight distributions supports assumption transfer learning performing well english model suitable model adapted german. since adaptation german explainable based distributions investigated differences individual weights. therefore determined absolute distance weights shown figure right side. plot visualize distribution weight changes. observed small changes therefore log-scale used again. figure right side shows analysis transfer learning model early training well final model four days. early phase weights adapted little maximum difference final model weights changed additionally observed weights changed middle layers earlier progressing training input layer experiences changes. higher variability outer layers observed finally looked changes individual filters. large number neurons provide complete filters layers supplement. present findings selected neurons input layer showed wellinterpretable patterns. weights filters differences english german model shown figure shows neurons interpreted detectors short percussive sounds high pitched noise bottom neurons might detect rising falling pitch vowels. four filters upper left differs english german maximum difference supports detecting percussive sounds german language considerably stronger pronunciation corresponding letters english. hand bottom filters experienced less change supports related vocal detection since differences pronunciation english german speakers. overall test accuracy test lers wers scores consistent differences loss performed experiments. hours training best transfer learning model therefore mean test samples. graphemes. thus types features clearly matter most. german english many phonemes graphemes common. apparent success transfer learning approach greatly facilitated similarities. languages share much terms features. anticipate approach less effective pairs. means expect adaptation less similar language require data training time. suspect differences grapheme inventories cause effects differences phonemes. mapping phonological features graphemes adapted different orthography. contrast differences phoneme inventories require changes features learned lower layers network. moreover could differences importance specific features. instance vowels common potentially important transfer learning sharing many consonants vowels experience higher variability pronunciation. time drastic differences orthography could probably trigger stronger change weights lower network layers. expect transfer learning approach encounter strong difficulties sharing knowledge english logographic language like mandarin chinese. despite difficulties using weights pre-trained asr-network reasonable initialization random weights. basic audio features shared languages. therefore even different language pairs expect transfer learning decrease necessary amount training data time. able show transfer learning using model adaptation improve speed learning hours training data available. given english model trained german model outperforms german baseline model trained scratch amount training time. thus little time approach allows training figure differences specific filters input layer. neurons chosen based particular patterns. triplet images shows weight differences corresponding weights german english model original wavletter network report improvements kenlm integration. table compared decoding performed kenlm scored beam search greedy decoding german corpora. better models. showed english model’s weights good starting configuration allow transfer learning model reach smaller training losses comparison weight reinitialization. less memory available freezing lower layers allows train batches less instead still performing similar hours training. model introspection determined lower upper layers contrast layers center need change thoroughly order adapt language. identified several interesting directions future work. test accuracy showed word compounds challenging dialects pose difficulties little training data available. memory consumption could reduced caching representation needs forward propagation. open source version loss would enable faster training. finally future research investigate well transfer learning approach generalizes applying distinct languages. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. http//tensorflow.org/. dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos erich elsen jesse engel linxi christopher fougner tony awni hannun billy patrick legresley libby sharan narang andrew sherjil ozair ryan prenger jonathan raiman sanjeev satheesh david seetapun shubho sengupta wang zhiqian wang chong wang xiao dani yogatama zhan zhenyao zhu. deep speech end-to-end speech recognition english mandarin. corr abs/.. http//arxiv.org/abs/.. dongpeng chen brian kan-wing mak. multitask learning deep neural networks lowresource speech recognition. ieee/acm trans. audio speech language processing http//dx.doi.org/./taslp... ronan collobert christian puhrsch gabriel synnaeve. wavletter end-to-end convnet-based speech recognition system. corr abs/.. http//arxiv.org/abs/.. christoph draxler florian schiel. three corpora bavarian archive speech signals first step towards distributed web-based third international conference recording. language resources evaluation gonzles rodriguez manual pages alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber. connectionist temporal classification labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning. pages kenneth heafield ivan pouzyrevsky jonathan clark philipp koehn. scalable modified kneserney language model estimation. proceedings annual meeting association computational linguistics. sofia bulgaria pages georg heigold vincent vanhoucke alan senior patrick nguyen ranzato matthieu devin jeffrey dean. multilingual acoustic models using distributed deep neural networks. international conference acoustics speech signal processing ieee pages kate knill mark gales anton ragni language independent shakti rath. speech recogunsupervised acoustic models annual nition keyword spotting. international speech communication association singapore september pages http//www.iscaspeech.org/archive/interspeech .html. brian mcfee colin raffel dawen liang daniel p.w. ellis matt mcvicar eric battenberg oriol nieto. librosa audio music signal analysis python. proceedings python science conference. pages vassil panayotov guoguo chen daniel povey sanjeev khudanpur. librispeech corpus based public domain audio books. international conference acoustics speech signal processing ieee pages stephan radeck-arneth benjamin milde arvid lange evandro gouvˆea stefan radomski m¨uhlh¨auser chris biemann. open source german distant speech recognition corpus acoustic model. international conference text speech dialogue. springer international publishing pages florian schiel christian heinrich sabine barf¨usser. alcohol language corpus first public corpus alcoholized german speech. language resources evaluation ngoc thang tanja schultz. multilingual multilayer perceptron rapid language adaptation fr´ed´eric across language families. bimbot christophe cerisara c´ecile fougeron guillaume gravier lori lamel franc¸ois pellegrino pascal perrier editors interspeech. isca pages", "year": 2017}