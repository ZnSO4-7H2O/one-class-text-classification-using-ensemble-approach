{"title": "Convolutional Neural Networks for Text Categorization: Shallow  Word-level vs. Deep Character-level", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015), on the eight datasets with relatively large training data that were used for testing the very deep character-level CNN in Conneau et al. (2016). Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in Conneau et al., though the results should be interpreted with some consideration due to the unique pre-processing of Conneau et al. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster.", "text": "paper reports performances shallow word-level convolutional neural networks earlier work eight datasets relatively large training data used testing deep characterlevel conneau ﬁndings follows. shallow word-level cnns achieve better error rates error rates reported though results interpreted consideration unique pre-processing shallow word-level uses parameters therefore requires storage deep character-level cnn; however shallow word-level computes much faster. text categorization task labeling documents many important applications sentiment analysis topic categorization. recently several variations convolutional neural networks shown achieve high accuracy text categorization references therein) comparison number methods including linear methods long state art. long-short term memory networks also shown perform well task rivaling sometimes exceeding cnns however cnns particularly attractive since simplicity parallel processing-friendly nature training testing cnns made much faster lstm achieve similar accuracy therefore cnns potential scale better large training data. focus studies report high performances categorizing long documents although studies report higher accuracy previous work respective datasets clear compare lack direct comparison. deep char-cnn shown perform well larger training data perform relatively poorly smaller training data; e.g. underperformed linear methods trained documents. shallow word-cnn shown perform well using training sets mostly smaller used results imply shallow word-cnn likely outperform deep char-cnn trained relatively small training sets used shallow word-cnn untested training sets large used hence purpose report testing shallow word-cnns datasets used direct comparison results deep char-cnns reported limitation work work experiments limited shallow word-cnn provide error rate results deep cnns proposed cite results. although natural assume error rates reported well represent best performance deep char-cnns achieve note documents clipped padded became characters long know pre-processing affected model accuracy. experiment word-cnn handle variable-sized documents variable-sized merit making ﬁxed-sized though reduce size vocabulary reduce storage requirements. considering that emphasize work intended rigorous comparison word-cnns char-cnns; instead regarded report shallow wordcnn performance eight datasets used referring results state-of-the-art performances. proposed deep char-cnns showed best performing models produced higher accuracy shallower models previous deep char-cnns best architecture consisted following character embedding dimensions. convolution layers number feature maps fully-connected layers hidden units each following convolution layers. following three methods downsampling halve temporal size setting stride convolution layer k-max pooling max-pooling stride downsampling done whenever number feature maps doubled. types word-cnn proposed illustrated figure straightforward application text involves training tv-embedding produce additional input base model. models tv-embedding produce higher accuracy provided sufﬁciently large amounts unlabeled data tv-embedding learning available. discussed shallow word-cnn regarded special case general framework jointly trains linear model non-linear feature generator consisting ‘text region embedding pooling’ text region embedding loose term function converts regions text vectors preserving information relevant task interest. component-wise nonlinear function max) input represents text region either concatenation one-hot vectors words region representation region weight matrix bias vector trained. note concatenation onehot vectors interpreted summing position-sensitive word vectors representation region interpreted summing position-insensitive word vectors. thus sense region embedding internally implicitly includes word embedding opposed external figure shallow word-level cnns. oval computation form takes place input parameters trained component-wise nonlinearity typically max. base model input one-hot representation text region ﬁrst train tv-embedding two-view embedding learning objectives produce additional input base model. explicit word embedding layer convolution layer e.g. makes concatenation word vectors. also supplementary material representation power analysis. illustrated figure applied text regions every location document pooling aggregates resulting region vectors document vector used features linear classiﬁer. experiments word-cnn without tv-embedding reported below one-hot representation used ﬁxed concatenation one-hot vectors vocabulary frequent words dimensionality region embedding ﬁxed one-hot vectors k-dimensional out-of-vocabulary word converted zero vector region embedding produced -dimensional vectors region. region size chosen based previous work performed max-pooling pooling units setting sentiment analysis datasets choosing others. models described also served base models word-cnn tv-embedding described next. word-cnns tv-embedding training word-cnns tv-embedding done steps shown figure first train region tv-embedding form above two-view embedding learning objective ‘predict adjacent text regions based text region training done unlabeled data. provides deﬁnition theoretical analysis tv-embeddings. next tv-embedding produce additional input base model train labeled data. model easily extended multiple tv-embeddings which example uses distinct vector representation region region embedding function ﬁnal model written output tv-embedding indexed applied corresponding text region. tv-embedding training done using unlabeled data additional resource; therefore proposed models semi-supervised models. experiments reported below lack standard unlabeled data tested datasets trained tv-embeddings labeled training data ignoring labels; thus resulting models supervised ones. trained four tv-embeddings four distinct one-hot representations text regions representation region size bag-of-{}-gram representation region size make representation tv-embedding used vocabulary frequent words make bag-of-{}-gram representation used vocabulary frequent {}-grams. dimensionality tv-embeddings unless speciﬁed otherwise dimensionality thus note dimensionality internal vectors comparable deep char-cnn shown below. rest setting base model above. two-step approaches another two-step approach word-cnns studied ﬁrst step pre-training word embedding layer followed convolution layer. potential advantage tv-embedding learning learn complex information word embedding report experimental results shallow word-cnns comparison results reported experiments reproduced using code available riejohnson.com/cnn_download.html. eight datasets used summarized table sogou news dbpedia ontology yelp amazon reviews. ‘.p’ names review datasets indicates labels either positive negative ‘.f’ indicates labels represent number stars. yahoo contains questions answers ‘yahoo answers’ website. datasets classes balanced. sogou consists romanized chinese. others english though contain characters languages small proportions. experiment shallow word-cnns also converted upper-case letters lower-case letters. unlike handled variable-sized documents variable-sized without shortening padding; however limited vocabulary size words {}-grams described above. perspective size complete word vocabulary largest training limited words frequency less comparison vocabulary sounds rather small covers text ama.p appears sufﬁcient obtaining good accuracy. datasets held data points training validation data. models trained using training minus validation data model selection done based performance validation data. tv-embedding training done weighted square loss minimized without regularization target regions represented vectors data weights negative sampling effect achieved. tv-embeddings ﬁxed ﬁnal training labeled data. training labels done follows. loss softmax minimized. optimization done mini-batch momentum mini-batch size number epochs ﬁxed learning rate reduced multiplying epochs layers weights initialized gaussian distribution zero mean standard deviation initial learning rate treated hyper parameter. regularization done applying dropout input layer regularization term parameter layer weights. table data statistics. error rates ‘depth’ counts hidden layers weights longest path. reported results several linear methods copied best results. reported results deep char-cnn three downsampling methods copied best results. word-cnn results results. best results shown bold font respectively. performance results error rates table show error rate results shallow word-cnn comparison best results deep char-cnn reported best results linear models reported dataset best results shown bold second best results shown italic font. datasets shallow word-cnn tv-embeddings performs best. second best performer shallow word-cnn without tv-embedding ama.f whereas deep char-cnn underperforms traditional linear models training data relatively small shallow word-cnns without tv-embedding clearly outperform datasets. observe that previous work additional input produced tv-embeddings substantial improvements. model size computation time table observe that compared deep char-cnn shallow word-cnn parameters computes much faster. although table shows computation time error rates particular dataset observation datasets. shallow word-cnn parameters number parameters mostly depends vocabulary size large word-cnn small char-cnn nevertheless computation shallow word-cnn made much faster deep char-cnn three reasons. first implementation handle sparse data efﬁciently computation shallow word-cnn depend vocabulary size. example concatenation one-hot vectors dimensionality computation time depends since need multiply nonzero elements weights second character-based methods need process times text units word-based methods; compare rows average length words characters table third deeper network less parallel processing-friendly since many layers processed sequentially. reduce dimensionality tv-embedding number parameters reduced half small degradation accuracy shown table error rate results -dim tv-embedding table model size computation time. ‘time’ elapsed time testing yelp.f test data using tesla excludes preprocessing input vector generation including one-hot vector manipulation word-cnn. error rates also yelp.f. shallow word-cnn parameters computes faster deep char-cnn. information deep char-cnn except ‘time’. processing time depends implementation test time deep char-cnn measured using implementation. described clipped padded documents documents became characters long. shallow word-cnn computes much faster deep char-cnn. deep charcnn needs process text units many characters words document many layers need processed sequentially. practical advantage shallow word-cnn. shallow word-cnns parameters therefore require storage drawback storage-tight situations. reducing number and/or dimensionality tv-embeddings reduces number parameters though comes expense small degradation accuracy.", "year": 2016}