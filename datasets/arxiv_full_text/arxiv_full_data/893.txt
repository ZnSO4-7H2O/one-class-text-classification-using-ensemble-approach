{"title": "Anomaly Detection on Graph Time Series", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "In this paper, we use variational recurrent neural network to investigate the anomaly detection problem on graph time series. The temporal correlation is modeled by the combination of recurrent neural network (RNN) and variational inference (VI), while the spatial information is captured by the graph convolutional network. In order to incorporate external factors, we use feature extractor to augment the transition of latent variables, which can learn the influence of external factors. With the target function as accumulative ELBO, it is easy to extend this model to on-line method. The experimental study on traffic flow data shows the detection capability of the proposed method.", "text": "paper variational recurrent neural network investigate anomaly detection problem graph time series. temporal correlation modeled combination recurrent neural network variational inference spatial information captured graph convolutional network. order incorporate external factors feature extractor augment transition latent variables learn inﬂuence external factors. target function accumulative elbo easy extend model on-line method. experimental study traﬃc data shows detection capability proposed method. datasets become large complex often important interesting know stands data learn general behavior. anomaly detection important branch data mining discover rare occurrences datasets. ﬁeld wide applications ﬁnance security health care enforcement many others. past decades large body research work anomaly detection detecting network failure network intrusion inspecting card telecommunications fraud classifying email spam monitoring data-center detecting malware/spyware image/video surveillance anomaly detection studied variety data domains including high-dimensional data uncertain data streaming data network data time series data diﬀerent types data always require techniques diﬀerent domains. inspecting anomalies time series needs examine behavior data across time. thus several models proposed statistics literature includes autoregressive integrated moving average autoregressive moving average cumulative statistics vector autoregression exponentially weighted moving average work focus multi-dimensional time series streams temporal dependencies highly non-linear data analyzed online way. yamanishi present smartsifter algorithm time series streams employs online discounting learning algorithm incrementally learn probabilistic mixture model decay factor account anomalies. online discounting methods large number methods dynamically maintained cluster models computing outliers data streams rising applications social network wearable devices sensor networks analysis structured time series becomes important. work also incorporate graph structures data anomaly detection. proposed eigenvector-based method anomaly detection graph streams. similar method also proposed akoglu spot anomalous points time many agents agent network change behavior deviates norm. however methods learn graph behavior time interval cannot work well time series long complex repeating patterns. paper combine variation inference recurrent neural network model graph time series detect anomalies based likelihood. rnns employed wide range tasks inherit ﬂexibility plain neural networks. includes universal approximation capabilities since rnns capable approximating measurable sequence sequence mapping shown turing complete however internal transition structure standard entirely deterministic. recent evidence complex sequences modeled performances rnns dramatically improved uncertainty included hidden states authors propose extend variational autoencoder recurrent framework modeling complex sequences called variational proposed time series model based vrnn. order incorporate structural information graphical convolution neural network applied extract features graphical data. k-localized ﬁlter incorporate utilize local information graph. practice time series inﬂuenced external features considered outliers detection. model conditioned external features extra input. experiments human wearable senor data traﬃc data show detection capability proposed model. rnns discrete-time statespace models trainable specialized weight adaptation algorithms. input variable-length sequence recursively processed. processing symbol maintains internal hidden state operation timestep formulated deterministic state transition function parameter rnns model sequences based parameterization factorization joint sequence probability distribution product conditional probabilities that work function realized long short-term memory since state transition performed deterministic modeling high-dimensional structured time series variability embedded hidden states cannot learned expressed. that’s conditional output probability density source variability. necessary introduce stochasticity hidden state transition. analyzing highly structured time series hidden state rnns expressive enough capture state transition dynamics. previous work introduced extra latent variables model output functions diﬀerent work following work makes prior distribution latent random variable timestep dependent preceding inputs hidden state improve representational power model. diﬀerent previous work detect anomalies based conditional probability time series conditioned previous data. however diﬃcult compute probability complex time series. recently variational autoencoder shown powerful tool approximate intractable complex posterior data space. introduces latent random variables designed capture variations underlie observed variables typically models conditional probability highly ﬂexible function approximator neural network makes inference posterior intractable. thus uses variational approximation posterior introduces approximate posterior gaussian mean variance modeled output neural networks prior assumed simple standard gaussian distribution. training process maximize elbo yield optimal selection parameters generative model inference model based re-parameterizing trick formulate rewrite work model based variational recurrent neural network recurrent extension vae. extend anomaly detection graph time series graph laplacian transform. moreover order better model practical situation incorporate external features extra input. graph time series always demonstrate strong spatial dependency. node strongly inﬂuenced neighbors. example highways sensors installed every miles traﬃc adjacent sensors highly correlated. moreover wearable sensors human body always dependent neighboring sensors dependency diﬀerent diﬀerent activities. however vrnn cannot explicitly model spatial dependencies. work vrnn augmented modeling spatial dependency eﬃcient way. order model spatial dependency vertex domain hidden state transition certain node incorporate hidden states neighboring nodes weighted correlation coeﬃcients. however quite time-consuming consider pairwise dependencies every vertex large graph. diﬃcult express meaningful translation operator vertex domain therefore transform graph time series vertex domain spectral domain graph laplacian transform. assume graph time series deﬁned undirected connected graph ﬁnite vertices edges rn×n time series time signal deﬁned nodes graph i-th wij. graph laplacian operator one-step diﬀusion signal deﬁned graph laplacian matrix deﬁned normalized version d−/w d−/. diagonalizing laplacian matrix diag ﬁlter enough since laplacian operator local working -hop neighborhoods. power laplacian operator supported exactly k-hop neighbors representing signals graph diﬀerent scales. since power laplacian matrix expensive compute apply chebyshev polynomial expansion following model applied recurrent setting. timestep diﬀerent previous work prior latent variables dependent hidden state makes output function incorporate temporal structure time series. have transition function parameterized realized lstm cell capture long short-term temporal dependencies. order capture spatial dependencies modeled graph spectral ﬁlter chebyshev polynomial expansion function ˜ωktkx. know hidden state dependent i.e. z≤t. distributions deﬁne respectively. denote external feature time holiday meteorological data extra input vector directly added mean latent variables inﬂuences variance σzt. based gaussian assumption approximate posterior function input hidden state below learning objective function training. maximizing ﬁrst train model time series data without anomalies. optimization problem solved adam algorithm. optimal generative inference model learned simultaneously. testing phase time detect anomaly based elbo bound below diﬀerent previous work locate anomalies graph likelihood ratio test statistics people likelihood ratio test compare models special case often occurs testing whether simplifying assumption model valid model parameters assumed related. compared models null model alternative model separately ﬁtted data log-likelihood recorded test statistics twice negative diﬀerence log-likelihoods parameter changing observed data best; denotes supreme function ﬁnds maximizer simplicity assume mean variance proportion anomalous degree test calculated below denotes cumulative density function chi-square distribution; degree freedom means number free parameters null model alternative model respectively. nodes larger given threshold likely anomalous. example suppose certain node time follows gaussian distribution below. likelihood null model lnull order achieve likelihood alternative model lalter according equations experimental study beijing taxi flows experiment city beijing partitioned grid based longitude meteorological data t-th time interval inﬂow outﬂow regions denoted tensor represent amount taxi traﬃc ﬂows always inﬂuenced external factors weather holidays. types weather conditions sunny rainy. holidays include public holidays china total number ﬁgures show inﬂuence chinese spring festival thunderstorm beijing i.e. grid map. within .mph. examples traﬃc ﬂows weekends usually lower working days. strong wind make people stay home reduce traﬃc. implementation one-hot binary vectors represent weekday holiday weather ﬂoat numbers represent temperature windspeed. dataset contains traﬃc ﬂows ranging time intervals i.e. mar. jun. nov. apr. traﬃc every region scaled data point shows traﬃc minutes. ﬁrst data points used training rest testing. since external factors already given assume training doesn’t anomalies. anomalies manually added testing set. anomalies categorized global anomaly local anomaly. global anomaly always outstanding global perspective signiﬁcant diﬀerence normal values covers relatively wide range. local anomalies bound seasonal minimum maximum appear paper shows application variational inference recurrent neural networks time series anomaly detection. comprehensive experiments prove proposed model competitive performance anomaly detection graph time series. spectral ﬁlter better learn spatial information variational recurrent neural networks model temporal correlation. multi-variable gaussian model sensitive enough outliers. diﬀerent previous work based likelihood ratio test model detect anomalous time points also localize anomalous nodes graph. future plan apply model detect video outliers.", "year": 2017}