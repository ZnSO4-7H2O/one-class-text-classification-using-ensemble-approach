{"title": "Deep Reinforcement Learning with a Combinatorial Action Space for  Predicting Popular Reddit Threads", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.", "text": "introduce online popularity prediction tracking task benchmark task reinforcement learning combinatorial natural language action space. speciﬁed number discussion threads predicted popular recommended chosen ﬁxed window recent comments track. novel deep reinforcement learning architectures studied effective modeling value function associated actions comprised interdependent sub-actions. proposed model represents dependence sub-actions bi-directional lstm gives best performance across different experimental conﬁgurations domains also generalizes well varying numbers recommendation requests. paper concerned learning policies sequential decision-making tasks system takes actions given options characterized natural language goal maximizing longterm reward. speciﬁcally consider tasks combinatorial action space action multiple interdependent sub-actions. problem combinatorial natural language action space arises many applications. example real-time news feed recommendation user want read diverse topics interest action computer agent would consist news articles similar topics advertisement placement action selection sevwork consider reddit popularity prediction similar newsfeed recommendation different respects. first goal make recommendations based individual’s preferences instead based anticipated long-term interest level broad group readers target community. second predict rather detect popularity. unlike individual interests community interest level often immediately clear; time level interest starts take off. here goal recommendation system identify track written documents real time attempting identify updates become keep reader leading edge. premise user’s bandwidth limited limited number things recommended several possibilities. experimental work discussion forum text recommendations correspond recent posts comments assessing interest based community response observed likes positive reactions comments. training purposes community response measured time much later original post publication. problem well-suited reinforcement learning paradigm since reward immediately known system needs learn mechanism estimating future reactions. different typical reinforcement learning action space combinatorial since action corresponds comments chosen larger candidates. sub-action written comment challenges associated problem include potentially high computational complexity combinatorial action space development framework estimating long-term reward combination sub-actions characterized natural language. here focus second problem exploring different deep neural network architectures effort efﬁciently account potential redundancy and/or temporal dependency different sub-actions relation state space. sidestep computational complexity issue working task number combinations large reducing costs random sampling. main contributions paper. first propose novel reinforcement learning task states combinatorial actions deﬁned natural language introduced section task based comment popularity prediction using data reddit discussion forum serve benchmark social media recommendation trend spotting. second contribution development novel deep reinforcement learning architecture handling combinatorial action space associated natural language. prior work related task deep reinforcement learning reviewed section details models baseline architectures described section experimental results section show proposed methods outperform baseline models bidirectional lstm effective characterizing combined utility sub-actions. brief summary ﬁndings open questions section registered users initiate post people respond comments either original post associated comments. together comments original post form discussion tree grows comments contributed. show discussions tend hierarchical topic structure i.e. different branches discussion reﬂect narrowing higher level topics. reddit discussions grouped different domains called subreddits according different topics themes. depending popularity subreddit post receive hundreds comments. comments associated positive negative votes registered users combined karma score used measure popularity. example reddit discussion tree given figure scores boxes mark current karma comment quite common lower karma comment lead children popular comments future note karma scores dynamic changing readers react evolving discussion eventually settling discussion trails off. real-time comment recommendation system eventual karma comment immediately available prediction popularity based text comment context prior comments subtree comments current time window. popularity prediction tracking reddit setting used paper studying reinforcement learning model long-term rewards combinatorial action space. time step state corresponds collection comments previously recommended. system aims automatically picking lines discussion follow comments given window combinatorial action. thread popularity tracking thought proxy task news scientiﬁc article recommendation. advantages documents relatively short long-term reward characterized reddit voting scores makes task easier work algorithm development larger related tasks. work consider comments associated threads discussion currently following limit number possible sub-actions time step assumption prior context needed interpret comments. words recommendation focus comments subtrees previously recommended comments. typically would expect interdependencies comments made window fall subtree correspond reply parent. addition temporal dependency since sub-action comment other. dependencies affect combined utility sub-actions. according experiments performance signiﬁcantly worse learn myopic policy compared reinforcement learning feature set. shows long-term dependency indeed matters illustrated figure serves justiﬁcation reinforcement learning appropriate approach modeling popularity discussion thread. large discrete state/action spaces. early work td-gammon used neural network approximate state value function recent advances deep learning inspired signiﬁcant progress combining deep learning reinforcement learning natural language processing reinforcement learning applied successfully dialogue systems generate natural language converse human user also interest mapping text instructions sequences executable actions extracting textual knowledge improve game control performance recently narasimhan studied task text-based games deep q-learning framework. proposed separate deep network handling natural language actions model q-values state-action interaction. nogueira also proposed goal-driven navigation task languagebased sequential decision making. narasimhan applied reinforcement learning acquiring incorporating external evidence improve information extraction accuracy. study present reddit popularity tracking differs text-based reinforcement learning tasks language state action spaces unconstrained quite rich. dulac-arnold also investigated problem large discrete action spaces. wolpertinger architecture proposed reduce computational complexity evaluating actions. combinatorial action space large discrete method directly apply case because possible actions changing different states. addition work differs focus modeling combined action-value function rather reducing computational complexity. work targets structured action space includes actor-critic algorithm actions real-valued parameters factored markov decision process certain independence assumptions next-state component subaction. bandits setting guestrin considered diversiﬁcation multi-item recommendation methodology limited using linear approximation hand-crafted features. task explored paper detecting tracking popular threads discussion somerelated topic detection tracking differs goal track topics based frequency rather based reader response. thus work closely related popularity prediction social media online news. studies explored variety deﬁnitions popularity including volume comments response blog posts news articles number twitter shares news articles number reshares facebook retweets twitter rate posts related source rumor difference number reader votes posts comments reddit discussion forums advantage working reddit data positive negative reactions accounted karma score. prior work reddit task explored similar involves choosing relatively high karma comments time-limited rather directly predicting comment karma. prior work popularity prediction used supervised learning; ﬁrst work frames tracking topics social media deep reinforcement learning. action-text) denote state action spaces respectively. here assume chosen given candidates. case described natural language. given state-text action-texts agent aims select best action order maximize long-term reward. environment state updated according probability agent receives reward particular transition. deﬁne action-value function expected return starting taking action comments tracked time step denoted previously tracked comments well post considered state initialize post. action taken total comments ct··· ctn} appear nodes subtree agent picks comments tracked next time step thus have ct··· ctn} time taking action state reward accumulated karma scores i.e. comments mt+. note reward signal used online training model deployment scores used evaluation metric. satisfying call proposed method drrn-sum architecture shown figure similarly drrn networks embed state actions separately. however different sub-actions keep network parameters tied. also layer dimension pairwise interaction function sub-actions. case linear additive interaction inner product bilinear operation equation equivalent computing interaction between state embedding action embedding action embedding obtained linearly summing sub-action embeddings. sub-actions strong correlation independence assumption invalid result poor estimation example people interested total information stored combined action content redun··· dancy sub-actions expect architecture generate embedding comment. bidirectional long shortterm memory used combine sequence comment embeddings. bidirectional lstm larger capacity nonlinear structure expect capture details embeddings sub-actions combine action embedding. note proposed methods handle varying value drrn baselines need ﬁxed training testing. discussion trees separate training testing sets texts seen agent training testing domain different discussions. episode depending whether training/testing simulator randomly picks discussion tree presents agent current state comments. q-function alternatives real-time setting clear action affect next state furthermore future expected reward. action consists comments making modeling q-values difﬁcult. handle large state space mnih proposed deep q-network case large action space state action representations input deep neural network. shown deep reinforcement relevance network i.e. separate deep neural networks modeling state embedding action embedding performs better per-action well variants dealing natural language action spaces include linear padqn drrn. concatenate subactions/comments form action representation. linear pa-dqn take input concatenation state action representations model single q-value using linear function approximations. drrn consists pair dnns state-text embedding action-text embeddings used compute pairwise interaction function simple alternative approach utilizing combinatorial structure compute embedding model value sub-action picking particular sub-action pairwise interaction state subt) represents expected accumuaction. approach assuming long-term rewards associated sub-actions independent other. speciﬁcally greedily picking topics genres. experiments order long enough discussion threads ﬁlter discussion trees fewer comments. subreddit randomly partition data online training data testing basic subreddit statistics shown table report random policy performances heuristic upper bound performances table table upper bound performances obtained using stabilized karma scores ofﬂine constructed tree structure. mean standard deviation obtained independent runs. text preprocessing remove punctuation lowercase capital letters. state comment bag-of-words representation vocabulary networks. vocabulary contains frequent words; out-of-vocabulary rate terms q-learning agent fully-connected neural networks used text embeddings. network hidden layers nodes model parameters initialized small random numbers. \u0001-greedy used exploration-exploitation keep throughout online training testing. pick discount factor online training experience replay memory size tuples experience replay episodes generated stored ﬁrst-in-ﬁrst-out fashion multiple epochs trained model. minibatch stochastic gradient descent implemented batch size learning rate kept constant proposed methods compared three baseline models linear per-action drrn. linear pa-dqn state comments concatenated input. drrn state comments sent separate deep neural networks. however baselines explicitly model values associated comment combined form action value. drrn baseline proposed methods inner product pairwise interaction function. figure provide learning curves different models askscience subreddit online learning. experiment curve obtained averaging independent runs error bars also shown. models start random performance converge approximately experience replays. drrn-sum converges fast baseline models better converged performance. drrnbilstm converges slower methods best converged performance. train models training model parameters apply test models predict action take reward shown evaluation. test performance averaged episodes report mean standard deviation independent runs. askscience multiple settings results shown table drrn-sum drrn-bilstm consistently outperform baseline methods. drrnbilstm performs better larger probably greater chance redundancy combining sub-actions. averaged episodes report mean standard deviation independent runs. ﬁndings consistent askscience. since different subreddits different karma scores distributions language style suggests algorithms apply different text genres. actual model deployment possible scenario users different requests. example user agent provide discussion threads limited reading time agent provide discussion threads day. baseline models need train separate models different k’s. proposed methods hand easily handle varying test whether performance indeed generalizes well train proposed models askscience test shown table compared proposed models speciﬁcally trained generalized test performance indeed degrades expected. however many cases proposed methods still outperform three baselines trained speciﬁcally k’s. shows proposed methods generalize varying even trained particular value table show anecdotal example state sub-actions. sub-actions strongly correlated redundant information. combining second sub-action compared choosing ﬁrst sub-action alone drrn-sum drrn-bilstm predict relative increase action-value respectively. since sub-actions highly redundant hypothesize drrn-bilstm better drrn-sum capturing interdependency sub-actions. state text cosmological phenomena strongly suspect occur universe isn’t enough happened yet? comments white dwarf stars eventually stop emitting light become black dwarfs. quite white dwarfs cool black dwarfs. described natural language task useful language studies. develop novel deep q-learning architectures better model state-action value function combinatorial action space. proposed drrn-bilstm method performs better across different experimental conﬁgurations domains also generalizes well scenarios user request changes number tracked. work represents ﬁrst step towards addressing popularity prediction tracking problem. performance system beats several baselines still falls short oracle result. prior work shown timing important factor predicting popularity proposed models would beneﬁt incorporating information. another variant might consider short-term reactions comment update window. would also interest explore implementations backtracking sub-action space order recommend comments selected earlier become highly popular. lastly important study principled solutions handling computational complexity combinatorial action space.", "year": 2016}