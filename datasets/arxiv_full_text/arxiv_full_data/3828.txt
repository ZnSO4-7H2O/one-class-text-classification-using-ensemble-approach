{"title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "tag": ["cs.CL", "cs.NE"], "abstract": "In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.", "text": "paper propose effective biasing attention mechanism sequence-to-sequence neural machine translation model towards well-studied statistical word alignment models. show novel guided alignment training approach improves translation quality real-life ecommerce texts consisting product titles descriptions overcoming problems posed many unknown words large type/token ratio. also show meta-data associated input texts topic category information signiﬁcantly improve translation quality used additional signal decoder part network. novel features bleu score system product title improves even larger quality gains obtained domain adaptation general domain system e-commerce data. developed system also performs well iwslt speech translation task ensemble four variant systems outperforms phrase-based baseline bleu absolute. systems shown reach state-of-theart translation quality tasks established research community iwslt speech translation task paper also apply approach e-commerce data user-generated product titles descriptions items sale. data different newswire texts typically considered research community. titles particular short contain many brand names often translated also product feature values speciﬁc abbreviations jargon. also vocabulary size large large variety product types many words observed training data once. time data provided additional metainformation item used context perform topic/domain adaptation improved translation quality. ﬁrst glance established phrase-based statistical approaches well-suited ecommerce data translation. phrase-based approach singleton unambiguous words phrases usually translated correctly. also since alignment source target words available possible transfer certain entities source sentence generated target sentence in-context without translating them. entities include numbers product speciﬁcations brand names samsung lenovo. training entities replaced placeholders reduce vocabulary size. however approaches powerful capturing context beyond phrase boundaries shown better exploit available training data. also successfully adapt domain limited amount parallel training data available also previous research shown difﬁcult obtain translation quality improvements topic adaptation phrase-based data sparseness large number topics relevant disambiguating alternative translations solving known problems. contrast expected better solve topic adaptation problem using additional meta-information extra signal neural network. best knowledge ﬁrst work additional information text topic embedded vector space used directly inﬂuence decisions. system attention mechanism introduced important decoding well restoration placeholder content insertion unknown words right positions target sentence. improve estimation soft alignment propose viterbi alignments model additional source knowledge training. additional alignment information helps current system bias attention mechanism towards viterbi alignment. paper structured follows. overview related work section propose novel approach section improve translation quality combining worlds phrase-based statistical word alignment neural attention mechanism. section describe detail topic information beneﬁt nmt. section section describes domain adaptation approach. experimental results presented section paper concluded discussion outlook section neural machine translation mainly based using recurrent neural networks grasp long term dependencies natural language. system trained end-to-end basis maximize conditional probability correct translation given source sentence using attention mechanism large vocabularies techniques reported achieve comparable translation quality state-of-art phrase-based translation systems. approaches based encoder-decoder architecture input sentence ﬁrst encoded ﬁxed-length representation recurrent neural network decoder generates sequence target words. since ﬁxedlength representation cannot give enough information decoding sophisticated approach using attention mechanism proposed approach neural network learns attend different parts source sentence improve translation quality. since source target language vocabularies neural network limited rare words problem deteriorates translation quality signiﬁcantly. rare word replacement technique using soft alignment proposed gives promising solution problem. encoder-decoder architecture insertion unknown words output highly rely quality attention mechanism thus becomes crucial part nmt. research done reﬁne proposed global local attention-based models used biases fertility symmetric bilingual structure improve attention model mechanism. research topic adaptation closely related work performed features proposed added log-linear model phrase-based system. here topic information part input system. another difference primarily work human-labeled topics whereas topic distribution inferred automatically data. translating e-commerce content faced situation product titles descriptions manually translated resulting small in-domain parallel corpus large general-domain parallel corpus available. situations domain adaption techniques used phrase-based systems addition diverse models using different features techniques trained ensemble decoder used combine together make robust model. approach used outperform state-of-art phrase-based system approach evaluation. length target sentence length source sentence ﬁxed-length vector encode source sentence hidden state time step nonlinear function approximate word probability. attention mechanism used ﬁxed-length replaced variable-length representation weighted summary sequence annotations contains information whole input sentence strong focus parts surrounding word then context vector deﬁned function calculate score t-th target word aligning i-th word source sentence. alignment model used calculate similarity previous state bi-directional state experiments took idea global attention model still keep order proposed calculate product encoder state last decoder state instead current decoder state. observe attention model works better concatenation experiments. alignments especially increasing length input sentence many out-of-vocabulary words placeholders. translation lead disordered output word repetition. contrast statistical phrase-based system decoder explicit information candidates current word recurrent step attention weights rely previously generated word decoder/encoder state depicted figure target word used compute attention weights. previous word out-ofvocabulary placeholder information provides calculating attention weights current word neither sufﬁcient reliable anymore. leads incorrect target word prediction error propagates future steps feedback loop. problem even larger case e-commerce data number oovs placeholders considerably higher. improve estimation soft alignment propose viterbi alignments model additional source knowledge training. therefore ﬁrstly extract viterbi alignments trained giza++ toolkit bias attention mechanism. approach optimize decoder cost divergence attention weights alignment connections generated statistical alignments. multi-objective optimization task expressed single-objective means linear combination loss functions original alignment-guided loss. refers training sentence pair denotes total number sentence pairs training corpus. paper name negative log-likelihood decoder cost distinguish alignment cost. using encoderdecoder architecture additional source information training system translation. particular meta-information help disambiguate alternative translations word different meaning. choice right translation often depends category. example word skin translated differently categories mobile phone accessories make-up. outside e-commerce world similar topic information available form e.g. tags keywords given document also used word sense disambiguation topic adaptation. general document belong multiple topics. here propose feed meta-information recurrent neural network help generate words appropriate given particular category topic. topic representation idea represent topic information d-dimensional vector number topics. since sentence belong multiple topics normalize topic vector elements decoder inﬂuence proposed target word distribution. conditional probability given topic membership vector written used approximate probability distribution. implementation introduce intermediate readout layer build function feed-forward network depicted figure topic-aware decoder decoder feed topic membership vector readout layer recurrent step enhance word selection. shown figure topic membership vector decoder additional input besides source target sentences alignment cost introduce alignment cost penalize attention mechanism consistent statistical word alignment. represent pre-trained statistical alignments matrix refers probability word target sentence aligned word source sentence. case multiple source words aligning target word norattentionbased matrix attention weights shape semantics propose penalize based divergence matrices training divergence function cross entropy mean square error gmse equation shown figure comes statistical alignment feeding guided-alignment additional input penalize attention mechanism. training optimize compound loss function regard parameters before. guided-alignment training inﬂuences attention mechanism generate alignment closer viterbi alignment advantage unchanged parameter space model complexity. training done assume generate robust alignment itself need feed alignment matrix input evaluation. indicated equation weights decoder cost alignment cost balance weight ratio. performed experiments analyze impact different weight settings translation quality. concatenation original transformation matrix topic transformation matrix adding topic readout layer input equivalent adding additional topic vector original readout layer output. assuming one-hot category vector equivalent retrieving speciﬁc column matrix hence name additional vector topic embedding regarded vector representation topic information. quite similar word embedding analyze similarity different topics figure readout layer depicted figure merges information last state previous word embedding well current context generate output. seen shallow network consists max-out layer fully-connected layer softmax layer. trained small amounts data attention-based neural network approach always produce reliable soft alignment. problem gets worse sentence pairs available training getting longer. solve problem extracted bilingual sub-sentence units existing sentence pairs used addiwr concatenation original transformation matrix output readout layer embedding last target word yt−; refers last decoder state. weights bias linear transformation respectively. rearrange formula tional training data. units exclusively aligned other words within source sub-sentence aligned words within corresponding target sub-sentence vice versa. alignment determined standard approach boundaries sub-sentence units used punctuation marks including period comma semicolon colon dash etc. simplify bilingual sentence splitting used standard phrase pair extraction algorithm phrase-based minimum/maximum source phrase length tokens respectively. long phrase pairs extracted algorithm kept started ended punctuation mark started/ended sentence; source target side. bootstrapped training merged training data extracted suboriginal sentence units neural training algorithm extended training set. since extracted bilingual sub-sentence units generally showed good correspondence source target constraints described above expectation units repeated training data stand-alone training instances would guide attention mechanism become robust make easier neural training algorithm better correspondences between difﬁcult source/target sentence parts. also short long training instances expected make neural translation quality less dependent input length. e-commerce english-to-french translation task limited amount in-domain parallel training data beneﬁt large amounts general-domain training data followed method described ﬁrst trained baseline model english-french data epochs best result development continued training model in-domain training epochs. contrast however used vocabularies frequent source/target words in-domain data in-house english-tofrench e-commerce translation task. part data preprocessing tokenized lowercased corpora well replaced numbers product speciﬁcations special symbols placeholders num. keep placeholders training preserve content markups dev/test sets restore using attention mechanism. content inserted generated placeholders target side based attention mechanism beam search best translation make sure placeholder content used once. using mechanism also pass words target side tasks evaluate systems system variants using case-insensitive bleu scores held-out development test data using single human reference translation. iwslt german-to-english task mapped topic keywords talk training/dev/test evaluation campaign release general topics politics environment education others. sentences talk share topic talk belong several topics. instead using ofﬁcial iwslt dev/test data aside talks development/test respectively. talks used test sentences highest probability relating particular topic full corpus statistics iwslt data sets obtained given table table corpus statistics iwslt e-commerce translation tasks. rate calculated preprocessing placeholders like etc. largely decrease rate e-commerce test sets. e-commerce data e-commerce english-to-french task used product category fashion electronics topic information training contained product titles product descriptions test contained product titles. title description sentence assigned category. statistics e-commerce data sets given table implemented neural translation model python using blocks deep learning library based open-source mila translation project. compared implementation baseline system englishto-french machine translation task obtained similar bleu score ofﬁcial test reported implemented topic-aware algorithm guided alignment training bootstrapped training model. trained separate models various feature combinations. also created ensemble different models obtain best translation results. experiments word embedding size used two-layer bi-directional encoder layer decoder cell dimension selected frequent german words english words vocabularies iwslt task frequent english/french words e-commerce task. optimization objective function performed using adadelta algorithm beam size dev/test beam search translation. training implementation stochastic gradient descent batch size saving model parameters certain number epochs. saved around consecutive model parameters. selected best parameter according established evaluation measures bleu -ter development set. model selection evaluated best model test set. report test bleu scores table table titan gpus experiments ubuntu linux training converges less hours iwslt talk task around hours ecommerce task. beam search test tasks takes around minutes exact time depends vocabulary size beam size. m¨ochte ihnen heute morgen gerne meinem projekt kunst aufr¨aumen erz¨ahlen. want clean morning project art. would like talk today project clean. would like talk morning project tidying art. unsere kollegen tufts verbinden modelle diese durch tissue engineering erzeugten knochen sehen krebs sich einem teil k¨orpers n¨achsten verbreiten k¨onnte. noaa colleagues combined models models like tissue generated bones bones cancer could spread part body next distribution. colleagues tufts using models like tissue-based engineered bones cancer could spread part body next part. colleagues tufts mixing models like tissue-engineered bone cancer might spread part body next. replacing human-labeled topic one-hot vectors size lda-predicted topic distribution vectors dimension readlayer neural network deteriorated bleu scores signiﬁcantly. attribute data sparseness problems training dimension product titles. german-to-english task also observed quality improvements using human-labeled topic information described figure here extracted topic embedding different experiments show cosine distance figure it’s straightforward different experiments topic tends share similar representation continuous embedding space. time closer topic pairs like politics issues tend shorter distance other. examples improved german-to-english translations human-labeled topic information used shown table tested different approaches topic information best since topic information affect alignment word selection etc. naive approach insert pseudo topic word beginning sentence bias context sentence certain topic. also tried topic vectors different origin read-out layer network. used topics predicted automatically latent dirichlet analysis human-labeled topics feed network shown figure results e-commerce task table show category information pseudo topic word carry enough semantic syntactic meaning comparison real source words positive effect target words predicted decoder. bleu score system even baseline human-labeled categories reliable able positively inﬂuence word selection decoder sigvintage ollech wajs early bird diver watch excellent vintage ollech wajs d´ebut oiseau montre plong´ee excellent montre plong´ee vintage ollech wajs early bird excellent source reference montre plong´ee vintage ollech wajs early bird excellent source reference ampliﬁcateur puissance pour audiophile holman mod`ele fabriqu´e cambridge massachussets holman model audiophile power amplifer made cambridge mass holman modle audiophile power fabricant d’ampli made cambridge mass l’ampliﬁcateur puissance audiophile holman mod`ele fabriqu´ee cambridge mass system description phrase-based system phrase-based system topic vectors bootstrapping guided alignment topic vectors bootstrapping topic vectors bootstrapping guided alignment costs. analyzed relation weight ratio ﬁnal result table besides ﬁxing cost ratios training also apply heuristic adjust ratio training progressing. approach high value alignment cost beginning decay weight every epoch ﬁnally eliminating inﬂuence alignment time. approach helps iwslt task e-commerce task. assume alignment talk sentences seems easier learn alignment product titles translations. also analyzed effect using different loss functions calculating alignment divergence difference squared error cross-entropy large shown table since cross-entropy function consistent form decoder cost decided experiments. extracted attention weights marked connection highest score hard alignment word. drew alignment figure compare baseline alignmentguided nmt. seen graph guided alignment training truly improves overall results overall results e-commerce translation task iwslt task shown table table respectively. observed consistency tasks sense feature improves bleu/ter results task also beneﬁcial other. comparison trained phrase-based models using moses toolkit translation tasks. used standard moses features including -gram trained target side bilingual data word-level phrase-level translation probabilities well distortion model maximum distortion stronger phrase-based baseline included additional features -gram operation sequence model e-commerce task challenging high number words placeholders observed translation output many errors related incorrect attention weights. improve attention mechanism applied guided alignment bootstrapiwslt task baseline behind phrase-based system e-commerce task thus obtained improvements smaller product title translations. observed topic information less helpful bootstrapping guided alignment learning. combined them reached bleu score phrase-based system finally combined four variant systems create ensemble resulted bleu score surpassing phrase-based translation model bleu absolute. presented novel guided alignment training model utilizes model viterbi alignments guide attention mechanism. approach shown experimentally bring consistent improvements translation quality e-commerce spoken language translation tasks. also tasks proposed novel utilizing topic meta-information ping. boosted translation performance. adding topic information increased bleu score selected four best model parameters various experiments make ensemble system improved bleu score following experiment pre-trained model parallel data guided alignment technique continued training e-commerce data several epocs described section performing domain adaptation. approach proved extremely helpful giving increase absolute bleu. finally also applied ensemble methods variants domain-adapted models increase bleu score bleu higher baseline system bleu behind bleu score state-of-the-art phrasebased baseline. table shows examples ensemble system better phrasebased system despite slightly lower corpuslevel bleu score. fact detailed analysis sentence-level bleu scores showed translation titles ranked higher translation shown improve bleu scores. also showed improvements using domain adaptation continuing training out-of-domain system in-domain parallel data. future would like investigate effectively make abundant monolingual data human-labeled product category information available envisioned e-commerce application.", "year": 2016}