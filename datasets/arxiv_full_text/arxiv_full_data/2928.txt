{"title": "End-to-End United Video Dehazing and Detection", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "The recent development of CNN-based image dehazing has revealed the effectiveness of end-to-end modeling. However, extending the idea to end-to-end video dehazing has not been explored yet. In this paper, we propose an End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal consistency between consecutive video frames. A thorough study has been conducted over a number of structure options, to identify the best temporal fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and Detection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with a video object detection model. The resulting augmented end-to-end pipeline has demonstrated much more stable and accurate detection results in hazy video.", "text": "paper ﬁlls blank cnn-based video dehazing innovative integration important merits uniﬁed model inherit spirit training end-toend model directly regresses clean images hazy inputs without intermediate step. proven outperform results multi-stage pipelines; embrace video setting explicitly considering embed temporal coherence neighboring video frames restoring current frame. extensive architecture study identify promising temporal fusion strategy interpretable dehazing viewpoint well aligned previous ﬁndings call proposed model end-toend video dehazing network better evd-net considered pre-processing subsequent high-level computer vision task therefore jointly train concatenated pipeline optimized high-level task performance presence haze. using video object detection task example build augmented end-to-end united video dehazing detection network achieve much stable accurate detection results hazy video. recent development cnn-based image dehazing revealed effectiveness end-to-end modeling. however extending idea end-to-end video dehazing explored yet. paper propose end-to-end video dehazing network exploit temporal consistency consecutive video frames. thorough study conducted number structure options identify best temporal fusion strategy. furthermore build end-to-end united video dehazing detection network concatenates jointly trains evd-net video object detection model. resulting augmented end-to-end pipeline demonstrated much stable accurate detection results hazy video. removal haze visual data captured wild attracting tremendous research interests profound application values outdoor video surveillance trafﬁc monitoring autonomous driving principle generation hazy visual scene observations follows known physical model estimation physical parameters i.e. atmospheric light magnitude transmission matrix become core step solving haze removal inverse problem recently prosperity convolutional neural networks many efforts paid cnn-based single image dehazing among them dehazenet mscnn focused predicting important parameter transmission matrix image inputs using cnns generating clean images physical model. lately aod-net ﬁrst model introduce light-weight end-to-end dehazing convolutional neural network reformulating physical formula. however limited amount efforts exploring video dehazing realistic scenario either traditional statistical approaches cnns. figure evd-net structure options consecutive frames input i-level fusion input frames concatenated feeding ﬁrst layer; k-level fusion input frames ﬁrst processed separately column concatenated layer estimation; j-level fusion output images concatenated. number methods take advantages natural image statistics priors predict separately hazy image often inaccurate estimation either tend bring many artifacts nonsmoothness unnatural color tones contrasts. many cnnbased methods employ tool regress estimated using empirical methods able estimate notably design ﬁrst completely end-to-end dehazing model based re-formulating directly generates without intermediate step integrated variable shown figure aod-net architecture composed modules k-estimation module consisting convolutional layers estimate followed clean image generation module estimate abovementioned methods designed single-image dehazing without taking account temporal dynamics video. comes video dehazing majority existing approaches count post processing correct temporal inconsistencies applying single image dehazing algorithms frame-wise. proposes inject temporal coherence cost function clock ﬁlter speed-up. jointly estimates scene depth recovers clear latent images foggy video sequence. presents imageguided depth-edge-aware smoothing algorithm reﬁne transmission matrix uses gradient residual minimization recover haze-free images. designs spatio-temporal optimization real-time video dehazing. experiments show relatively simple straightforward video dehazing approaches even able outperform sophisticated cnn-based single image dehazing models. observation reminds utility temporal coherence must coupled advanced model structures further boost video dehazing performance. recent years witnessed growing interest modeling video using cnns wide range tasks super-resolution deblurring classiﬁcation style transfer investigates variety structure conﬁgurations video similar attempts made digging different connectivity options video classiﬁcation. proposes ﬂexible formulation placing spatial alignment network frames. introduces trained end-to-end learn accumulating information across frames video deblurring. video style transfer incorporates shortterm long-term coherences also indicates superiority multi-frame methods single-frame ones. fusion conv input frame ﬁrst convolutional layers separately concatenation output l-th layer words multi-frame information fused towards generating parameter current frame based underlying assumption object depths global atmospheric light transmit smoothly across neighboring frames. j-level fusion fusing output level. equivalent feed frame separate k-estimation module outputs concatenated right before clean image generation module. fuse frame-wise predictions made corresponds fusing output level. training video-based deep model often hassle. proves well-trained singlecolumn deep model images could provide high-quality initialization training multi-column model videos splitting convolutional weights fusion step. follow strategy training aod-net ﬁrst initialize different evd-net architectures evd-net. unity brings power optimizing dehazing detection end-to-end pipeline video beyond video restoration purpose dehazing many low-level restoration enhancement techniques commonly employed pre-processing improve performance high-level computer vision tasks presence certain visual data degradations. pioneering works single-image cases demonstrated formulating low-level high-level tasks uniﬁed pipeline optimizing convincingly boost performance. best knowledge methodology validated video cases yet. outdoor surveillance autonomous driving object detection video widely desirable whose performance known heavily suffer existence haze. example autonomous vehicles rely light detection ranging sensor model surrounding world video camera records analyzes interprets objects visually create maps. however haze interfere laser light lidar sensor fail subsequent algorithms. paper investigate brand-new joint optimization pipeline video dehazing video object detection. beyond dehazing part detection part take account temporal coherence well reduce ﬂickering detection results. evd-net design video-adapted version faster r-cnn verify effectiveness recognizing possibility plugging video detection models. ﬁrst convolutional layers classical singleimage faster r-cnn model split three parallel branches input previous current next frames respectively. concatenated second condeep video dehazing model recognizing proposed methodology applied extending deep image dehazing models video e.g. main problem lies strategy temporal fusion. well-justiﬁed fact video processing jointly considering neighboring frames predicting current frame beneﬁt many image restoration classiﬁcation-type tasks speciﬁcally video dehazing case object depth global atmospheric light hardly slowly changed moderate number consecutive frames implying great promise exploiting multiframe coherence video dehazing. fusion strategy video dehazing three structure options enlightened analysis investigate three different strategies fuse consecutive frames. simplicity show architecture input frames example namely previous current next frames goal predict clean version current frame clearly number past future frames accommodated. compared figure three different types fusion structures available evd-net i-level fusion fusing input level. input frames concatenated along ﬁrst dimension ﬁrst convolutional layer applied. corresponds directly fusing image features pixel level running single-image dehazing model fused image. k-level fusion fusing k-estimation. specifically term following structure k-level k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames k-level fusion conv frames finally uniting evd-net mf-faster r-cnn gives rise evdd-net naturally displays interesting locally-connected tree-like structure subject joint optimization. figure plots instance evdd-net low-level temporal window size frames high-level temporal window size frames leading overall temporal window size frames. ﬁrst feed consecutive frames evdd-net part. predicting -frame groups stride size three dehazed results corresponding frames generated. mf-faster r-cnn part fuse detection results frame essentially tree-like structure comes two-step utilization temporal coherence neighboring frames level high level. conﬁdent tree-like structure extensive reference values future deep pipelines seek jointly optimize low-level high-level tasks. rgb-d dataset captures varied visual scenes. depth information reﬁned ﬁlling algorithm split training consisting videos frames non-overlapping testing called testset consisting rest relatively short video clips total frames. training evd-net momentum decay parameters respectively batch size adopt mean square error loss shown well aligned ssim visual quality. thanks light-weight structure evd-net takes epochs converge. fusion structure comparison ﬁrst compare performances three fusion strategies evd-net different conﬁguration parameters testset shown table performance k-level fusion superior i-level fusion j-level fusion psnr ssim albeit number network parameters j-level fusion much two. moreover among conﬁgurations k-level fusion using input frames k-level fusion conv performs best. using input frames performance increases reaches overall peak k-level fusion conv chosen default conﬁguration evd-net. testing frames observe performance gets saturated sometimes hurt since relevance far-away frames current frame decay fast. quantitative comparison compare evd-net testset variety state-of-the-art single image dehazing methods including automatic atmospheric light recovery boundary constrained context regularization fast visibility restoration non-local image dehazing dark-channel prior mscnn dehazenet color attenuation prior aod-net also compare recently proposed video dehazing approach real-time dehazing based spatio-temporal table demonstrates promising performance margin evd-net others terms psnr ssim. compared second best approach aod-net evd-net gains advantage psnr ssim showing beneﬁts temporal coherence. compared video-based stmrf notice remarkable performance psnr ssim. qualitative visual quality comparision figure shows comparison results consecutive frames number image video dehazing approaches realworld hazy video test video taken city road constituting challenging heavy haze scenario. without figure comparisons detection results real-world hazy video sample frames. note third fourth ﬁfth columns results visualized dehazing results. temporal coherence single image dehazing approaches tend produce temporal inconsistencies jaggy artifacts. results especially visually unpleasing. mscnn well stmrf fail fully remove haze e.g. building areas dehazenet tends darken global light. aod-net produces reasonably good results sometimes cannot ensure temporal consistencies illumination color tones. evd-net gives rise visually pleasing detail-preserving temporally consistent dehazed results among all. figure shows comparison example synthetic data three consecutive frames selected testset comparing ground-truth seen evd-net preserves details color tones best. datasets implementation training evddnet lack hazy video datasets object detection labels driven create synthetic training set. synthesize hazy videos various haze levels subset ilsvrc dataset based atmospheric scattering model estimated depth using method evdd-net trained using frames hazy videos two-category object detection problem tested frames another hazy videos several real-world hazy videos also used evaluation. training evdd-net evidently beneﬁts highquality initialization trained evd-net plus mf-faster rcnn model initialized splitting ﬁrst convolutional layers similar found directly end-to-end training parts could lead sufﬁciently good results observe video-based pipeline involves much parameters thus difﬁcult train end. besides initialization also two-step training strategy evdd-net ﬁrst tune fullyconnected layers high-level detection part evd-net iterations tune entire concatenated pipeline another iterations. comparison baselines evdd-net compared baselines original faster r-cnn single image-based trained hazefree images; re-trained faster r-cnn obtained retraining original faster r-cnn hazy image dataset; iii) faster r-cnn simple concatenation separately trained evd-net original faster r-cnn models; jaod-faster r-cnn stateresults analysis table presents mean average precision approaches main evaluation criterion. also display category-wise average precision references. comparing ﬁrst columns verify object detection algorithms trained conventional visual data generalize well hazy data. directly placing evd-net front mf-faster rcnn fails outperform retrained faster-rcnn although surpasses original faster-rcnn margin. notice coincides earlier observations degradation contexts naive concatenation low-level high-level models often cansufﬁciently boost high-level task performance low-level model simultaneously bring recognizable details artifacts. performance jaod-faster r-cnn promising slightly outperforms retrained faster-rcnn. however results often show temporally ﬂickering inconsistent detections. evdd-net achieves signiﬁcantly boosted baselines. evddnet another successful example closing loop low-level high-level tasks based well-veriﬁed assumption degraded image correctly restored also good identiﬁability. figure shows group consecutive frames object detection results approach real-world hazy video sequence. evdd-net able produce accurate temporally consistent detection results. speciﬁc scene evdd-net approach correctly detect four cars throughout four displayed frames especially rightmost hardly recognizable even human eyes. owing temporal regularizations low-level high-level parts evddnet. video results found youtube. paper proposes evd-net ﬁrst cnn-based fully end-to-end video dehazing model thoroughly investigates fusion strategies. furthermore evd-net concatenated jointly trained video object detection model constitute end-to-end pipeline called evddnet detecting objects hazy video. evd-net evdd-net extensively evaluated synthetic realworld datasets verifyt dramatic superiority dehazing quality detection accuracy. future work aims strengthen video detection part evdd-net.", "year": 2017}