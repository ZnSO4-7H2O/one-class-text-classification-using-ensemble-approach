{"title": "Local Distance Metric Learning for Nearest Neighbor Algorithm", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Distance metric learning is a successful way to enhance the performance of the nearest neighbor classifier. In most cases, however, the distribution of data does not obey a regular form and may change in different parts of the feature space. Regarding that, this paper proposes a novel local distance metric learning method, namely Local Mahalanobis Distance Learning (LMDL), in order to enhance the performance of the nearest neighbor classifier. LMDL considers the neighborhood influence and learns multiple distance metrics for a reduced set of input samples. The reduced set is called as prototypes which try to preserve local discriminative information as much as possible. The proposed LMDL can be kernelized very easily, which is significantly desirable in the case of highly nonlinear data. The quality as well as the efficiency of the proposed method assesses through a set of different experiments on various datasets and the obtained results show that LDML as well as the kernelized version is superior to the other related state-of-the-art methods.", "text": "distance metric learning successful enhance performance nearest neighbor classifier. cases however distribution data obey regular form change different parts feature space. regarding that paper proposes novel local distance metric learning method namely local mahalanobis distance learning order enhance performance nearest neighbor classifier. lmdl considers neighborhood influence learns multiple distance metrics reduced input samples. reduced samples denoted prototypes preserve local discriminative information much possible. proposed lmdl kernelized easily significantly desirable case highly nonlinear data. quality well efficiency proposed method assessed different experiments various datasets. obtained results show ldml well kernelized version superior related state-ofthe-art methods. keywords. distance metric learning local information nearest neighbor discrimination kernel space. learning proper distance function data points plays crucial role many domains specifically machine learning domain ultimate goal distance metric learning methods keep similar samples close together putting away dissimilar points. since decision making process nearest neighbor algorithm based distances computed learning proper distance metric significantly improve performance popular classification algorithm. besides distance metric learning plays vital role many machine learning tasks learning mahalanobis distance metric global distance metric gets lots attention efficiency simplicity solving complicated problems case multimodal data distributions however global metric work properly cannot satisfy constraints global point view. capturing local information data points considered solution cope deficiency. first group methods tries learn multiple distance metrics second group tries learn metric imposing local constraints order capture local information. huge number constraints unusual today’s real-world tasks second group methods face many obstacles optimization processes leading employ many relaxations order ease problem. hand risk overfitting considered main drawback first group. besides employ objective function closely related notion decision rule all. said this paper proposes supervised distance metric learning method avoiding aforementioned shortages. first place lmdl selects small samples namely prototypes learns mahalanobis distance metric prototype based closely related objective function proposed ref. attracted interest furthermore proposed learning procedure adjusts prototype positions order minimize objective function well. selecting prototypes mitigates risk overfitting preserving notion locality. additionally develop kernelized version proposed method namely klmdl desirable case highly nonlinear data structures. lmdl also considered extension learns weight vector prototype lmdl learns mahalanobis distance metric prototype. moreover method able used kernel space well. changes turn method powerful dealing real-world problems. performed number different evaluations experimental results clearly demonstrate lmdl well klmdl significantly increases predictive performance related state-of-the-art metric learning methods. nutshell major contribution paper summarized follows rest paper organized follows. section provides brief history related distance metric learning methods. section presents lmdl well klmdl details. section presents discusses experimental results. finally section concludes paper. section summarizes brief history supervised distance metric learning methods related work. work proposed xing apparently considered first attempt learn mahalanobis distance metric uses positive semi-definite formulation maximize distances different-class samples keeping same-class samples close together. neighborhood component analysis uses relative non-convex objective function decision rule trying minimize estimation expected leave-one-out error projection space. mcml uses convex formulation order collapse same-class samples single point. although mcml benefits convex formulation suffers high computational complexity. popular mahalanobis distance metric learning methods lmnn considering local information imposing constraints local manner. lmnn proposed refs. learn global metric based semi-definite programing accordingly suffer high computational complexity. following lmca global metric learning method proposed ref. trying maximize jeffery divergence multivariate gaussian distributions derived local pairwise constraints. however fails take account local information number constraints rises considerable level. recently harandi proposed framework jointly learns lower dimensional mapping space global distance metric. parametric local metric learning learns local metric point based linear combination predefined basis metrics. however plml depends manifold assumption suffers large number parameters. contrast plml generative local metric learning learns independent metric point generative process minimizes expected error assumptions class distributions. following sparse compositional metric learning uses rank-one matrices construct metric point therefore reduces number parameters compared lddm local metric learning method learning multiple distance metrics metric exemplar point optimizing local compactness well local discrimination. similar mentioned local methods however lddm suffers excessive number parameters putting risk overfitting. recently proposed metric learning method based eigenvalue decomposition used global local views. although mlev faster methods enforces orthogonality constraints learned metric thus performing dimensionality reduction metric learning all. contrast aforementioned methods proposed technique concurrently benefit following advantages making powerful metric classification tasks inspiring idea work. first objective function closely related decision rule. second notion prototype learn local mahalanobis metric prototype order capture local discriminative information preventing risk overfitting. third iteratively adjust position prototypes find best position them. finally propose kernelized version method desirable highly nonlinear datasets. given training points \u0000={…} ∈ℝ\u0000×\u0000 {…\u0000} defines corresponding class label ultimate goal learn mahalanobis metrics ⱳ={\u0000}\u0000\u0000\u0000\u0000 \u0000∈ℝ\u0000\u0000×\u0000 positive semi-definite matrix corresponds member randomly selected prototypes ᵽ={…} \u0000\u0000∈ℝ\u0000×\u0000 \u0000≪\u0000. setting vectors matrices respectively denoted boldface lowercase letters. also compact representation parameters suppose ∈ℝ×\u0000 matrix column represents vectorized form similarly \u0000∈ℝ\u0000×\u0000 matrix column holds column \u0000∈ℝ\u0000×\u0000 point using notations squared mahalanobis distance prototype point \u0000∈ℝ\u0000\u0000×\u0000 symmetric matrix defined prototype. order minimize \u0000=\u0000\u0000 \u0000\u0000∈\u0000 \u0000=\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 sigmoid function \u0000\u0000\u0000\u0000∈ᵽ respectively nearest same-class nearest different-class prototypes i.e. \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 argmin argmin accordingly respectively corresponding mahalanobis metrics parameter defines slope sigmoid function large behaves respectively. order update parameters adadelta rule extension adagrad rule requires manual tuning learning rate. furthermore appears robust noisy gradient information different model architecture choices various data modalities selection hyperparameters. algorithm shows iteration algorithm visits updates metrics highest impact prediction sample represent words algorithm updates sample gets closer nearest same-class prototype i.e. updating gets nearest different-class prototype i.e. concurrently same-class different-class nearest prototypes modified moves toward gets away proposed lmdl several fascinating advantages. first setting algorithm needs much smaller number parameters rank representation gives stems fact update steps weighted distance ratio windowed \u0000\u0000\u0000\u0000\u0000\u0000. means that learning algorithm sensitive outliers whose number selected prototypes class effective generalization lmdl well klmdl. small true nature class distribution captured. hand large generalization power reduce problem overfitting would inevitable. although proposed optimization problem convex metrics learned solution experimental results confirm consistently comparable computed globally-optimal methods. //input number prototypes slope sigmoid small constant //output intitialize randomly \u0000\u0000\u0000=\u0000\u0000 \u0000\u0000\u0000\u0000=\u0000 \u0000\u0000=∞ \u0000=\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000=\u0000 \u0000\u0000=\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000=\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000=\u0000\u0000= \u0000\u0000\u0000≠\u0000 calculate \u0000\u0000\u0000\u0000\u0000\u0000\u0000∇\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000∇\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000∇\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000=use adadelta rule find corresponding learning rates \u0000\u0000\u0000\u0000=\u0000\u0000 \u0000−\u0000\u0000\u0000\u0000⨀∇\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000=\u0000\u0000 \u0000−\u0000\u0000\u0000\u0000⨀∇\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000=\u0000\u0000−\u0000\u0000=⨀∇\u0000=\u0000 \u0000\u0000\u0000\u0000\u0000=\u0000\u0000−\u0000\u0000\u0000⨀∇\u0000\u0000\u0000 \u0000\u0000\u0000\u0000=\u0000 \u0000=\u0000\u0000\u0000\u0000\u0000\u0000 although linear metric learning techniques applied properly many cases insufficient case nonlinear data i.e. data complex nonlinear decision boundary. kernel methods suitable choice cases. idea kernel space implicitly project original data higher dimensional space kernel function learn distance metric space. implicit projection makes probable linearly separate data space. following paragraphs illustrate kernelized version proposed method namely klmdl making powerful metric learning method deal highly nonlinear structures. start using fact column space subset column space i.e. hence possible reconstruct linear combination columns i.e. \u0000∈ℝ\u0000×\u0000. replacing rewrite follows \u0000\u0000=\u0000\u0000\u0000\u0000 column matrix \u0000=\u0000∈ℝ\u0000×\u0000 \u0000\u0000∈ℝ\u0000×\u0000 matrix defined prototype. vividly shows distance function replace inner product suitable kernel function e.g. gaussian kernel setting need learn instead required derivatives learning procedure proceed non-kernelized version. worthy note size large therefore make hard calculate matrix however always datasets. experiments reported here parameter number prototypes class selected empirically. however cases parameters fold cross validation values {\u0000\u0000\u0000…\u0000}. rest parameters initialized first experiment visually explores discrimination power lmdl. consider three artificial datasets classes eight classes five classes respectively depicted figures prototypes class algorithm learn lmdl’s parameters. visualize results fact learning mahalanobis distance metric equivalent learn projection matrix. hence learned metrics project data learned spaces. figures respectively show results projected data clearly illustrate discrimination power lmdl. besides remarkable growth leave-one-out nn’s accuracies another evidence proving power discrimination lmdl. figure shows potential ability klmdl highly nonlinear dataset e.g. two-class artificial dataset consisting instances drawn concentric circles. figure shows klmdl achieved much better discrimination lmdl. atasets projected space using proposed methods. achieved results shown figure figure shows class-data separation dramatically acceptable. asserts methods eligible tasks dimensionality reduction. based results overall discriminative capacity klmdl higher lmdl. example case australian dataset klmdl yields linearly separable transformation lmdl fails make separation. number different values allowed features. table shows average test error rate followed standard deviation methods standard deviations reported results borrowed authors. according results reported table lmdl klmdl show superior performance compared rest methods. results friedman test confirm assertion. order reliable comparisons results finner’s post procedure reported tables respectively lmdl klmdl. finner test post procedure determine whether hypothesis mean comparison could rejected examine quality proposed methods high-dimensional spaces perform another experiment five high-dimensional datasets. table provides summary datasets compares results terms errors. case mnist isolet news datasets training test sets defined. rest datasets -fold obtain results. case news remove document headers well stopwords. besides remove words appear less documents documents. that randomly select documents category resulting documents. results seen lmdl klmdl achieve satisfactory results higher comparable competing methods. case isolet mnist proposed methods better significantly worse cases. results glml fails learn appropriate metrics datasets fundamental generative assumption restrictive often valid. contrast proposed methods hold restrictive assumptions. hence applicable variety domains. paper presents local mahalanobis distance learning enhance performance algorithm similarity local similar points enlarged local dissimilar points reduced much possible. lmdl considers neighborhood influence learns multiple distance metrics reduced samples denoted prototypes. prototype mahalanobis metric trying increase local discrimination much possible. objective function closely related nearest neighbor error rate order adjust prototypes’ metrics well positions. furthermore kernelized version proposed method also developed handle non-linear datasets. performed variety experiments synthetic real-world datasets results demonstrate proposed method performs competitively compared related state-of-the-art distance metric learning methods. villegas paredes simultaneous learning discriminative projection prototypes nearest-neighbor classification computer vision pattern recognition cvpr ieee conference hajizadeh taheri jahromi nearest neighbor classification locally weighted distance imbalanced data int. comput. commun. eng. vol. garcía fernández luengo herrera advanced nonparametric tests multiple comparisons design experiments computational intelligence data mining experimental analysis power inf. sci. vol.", "year": 2018}