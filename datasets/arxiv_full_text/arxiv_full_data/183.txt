{"title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "abstract": "The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.", "text": "abstract—the increasing complexity deep learning architectures resulting training time requiring weeks even months. slow training part vanishing gradients gradients used back-propagation extremely large weights connecting deep layers extremely small shallow layers results slow learning shallow layers. additionally also shown highly non-convex problems deep neural networks proliferation high-error curvature saddle points slows learning dramatically paper attempt overcome problems proposing optimization method training deep neural networks uses learning rates speciﬁc layer network adaptive curvature function increasing learning rate curvature points. enables speed learning shallow layers network quickly escape high-error curvature saddle points. test method standard image classiﬁcation datasets mnist cifar imagenet demonstrate method increases accuracy well reduces required training time standard algorithms. deep neural networks extremely successful past years achieving state performance large number tasks image classiﬁcation face recognition sentiment analysis speech recognition etc. spot general trend papers results tend better amount training data increases along increase complexity deep network architecture. however increasingly complex deep networks take weeks months train even high-performance hardware. thus need efﬁcient methods training deep networks. deep neural networks learn high-level features performing sequence non-linear transformations. training data composed data points corresponding labels {bi}n consider -layer network activation function denote weights layer trying learn i.e. denotes weights nodes ﬁrst layer second layer denotes weights nodes second layer third layer. learning problem speciﬁc example formulated following optimization problem activation function non-linear mapping traditionally sigmoid tanh function. recently rectiﬁed linear units max{ become popular tend easy train yield superior results problems non-convex objective usually minimized using iterative methods hope converging good local minima. iterative schemes generate additive updates parameters form appropriately chosen update. notice slightly different notation standard optimization literature incorporate step size learning rate within done help describe optimization algorithms easily following sections. thus denotes update parameters comprises search direction step size learning rate controls large step take direction. since size training data deep networks usually order millions billions data points exact computation gradient feasible. rather gradient often estimated using single data point small batch data points. basis stochastic gradient descent widely used method training deep nets. requires manually selecting initial learning rate designing update rule learning rate decreases time performance however sensitive choice update leading adaptive methods automatically adjust learning rate system learns descent methods used train deep networks additional problems introduced. number layers network increases gradients propagated back initial layers small. dramatically slows rate learning initial layers slows convergence whole network recently also shown high-dimensional non-convex problems deep networks existence local minima high error relative global minima exponentially small number dimensions. instead problems exponentially large number high error saddle points curvature gradient descent methods general move away saddle points following directions negative curvature. however curvature small negative eigenvalues steps taken become small thus slowing learning considerably. learning rates layer tend increase curvature points. enables method quickly escape high-error low-curvature saddle points occur abundance deep network. section review popular gradient methods successful deep networks. section describe optimization algorithm. finally section compare method standard optimization algorithms datasets like mnist cifar imagenet. stochastic gradient descent still remains widely used methods large-scale machine learning largely ease implementation. updates parameters deﬁned equations learning rate decreased time iterates approach local optimum. standard learning rate update given many modiﬁcations basic gradient descent algorithm proposed. popular method convex optimization literature newton’s method uses hessian objective function determine step size unfortunately number parameters increases even moderate size computing hessian becomes computationally expensive. thus many modiﬁcations proposed either improve ﬁrst-order information approximate hessian objective classical momentum method technique increases learning rate parameters gradient consistently points direction decreasing learning rate parameters gradient changing fast. thus update equation keeps track previous updates parameters exponential decay nesterov’s accelerated gradient ﬁrst order method better convergence rate gradient descent certain situations. method predicts gradient next iteration changes learning rate current iteration based predicted gradient. thus gradient higher next step would increase learning rate current iteration would slow down. recently showed method thought momentum method update equation follows carefully designed random initialization using particular type slowly increasing schedule method reach high levels performance used deep networks rather using single learning rate parameters recent work shown using learning rate speciﬁc parameter much successful approach. method gained popularity adagrad uses following update rule denominator norm gradients previous iterations. scales global learning rate shared parameters give parameterspeciﬁc learning rate. disadvantage adagrad accumulates gradients previous iterations continues grow throughout training. shrinks learning rate parameter inﬁnitesimally small limiting number iterations useful training. method builds adagrad attempts address above-mentioned disadvantages adadelta adadelta accumulates gradients previous time steps using exponentially decaying average squared gradients. prevents denominator becoming inﬁnitesimally small ensures parameters continue updated even large number iterations. also replaces global learning rate exponentially decaying average squares parameter updates previous iterations. method shown perform relatively well used train deep networks much less sensitive choice hyper-parameters. however perform well methods like adagrad terms accuracy vanishing gradients phenomenon shallow network layers tend much smaller gradients deep layers sometimes differing orders magnitude layer next previous work optimization deep networks methods either keep global learning rate shared parameters adaptive learning rate speciﬁc parameter. method exploits following observation parameters layer gradients similar magnitudes thus efﬁciently share common learning rate. layer-speciﬁc learning rates used accelerate layers smaller gradients. another advantage approach avoiding computation large numbers parameter-speciﬁc learning rates method remains computationally efﬁcient. finally mentioned section avoid slowing learning high-error curvature saddle points also want method take large steps curvature points. learning rate k-th iteration standard optimization method. case would given equation adagrad would global learning rate equation propose modify follows denotes learning rate parameters l-th layer k-th iteration denotes vector gradients parameters l-th layer k-th iteration. thus gradients layer determine learning rate layer. also important note gradients previous iterations thus save storage. equation gradients layer large equation reduces using normal learning rate however gradients small likely near curvature point. thus equation scales learning rate ensure initial layers network learn faster escape higherror curvature saddle points quickly. note that unlike adagrad uses distinct learning rate parameter different learning rate layer shared weights layer. additionally adagrad modiﬁes learning rate based entire history gradients observed weight update layer’s learning rate based gradients observed weights speciﬁc layer current iteration. thus scheme avoids storing gradient information previous iterations computing learning rates parameter; therefore less computationally memory intensive compared adagrad. proposed layer speciﬁc learning rates also works well large scale datasets like imagenet adagrad fails converge good solution. proposed method used existing learning rate optimization technique uses global provides layer-speciﬁc learning rate escapes saddle points quickly without sacriﬁcing computation memory usage. show section using adaptive learning rates existing optimization techniques almost always improves performance standard datasets. proposed method used existing optimization technique uses global learning rate. helps getting layer-speciﬁc learning rate well helps escaping saddle points quicker little computational overhead. show section using adaptive learning rates existing optimization techniques almost always improves performance standard datasets. present image classiﬁcation results three standard datasets mnist cifar imagenet mnist contains handwritten digit images training handwritten digit images testing. cifar contains classes images class. imagenet contains million color images different classes. caffe implement method. caffe provides optimization methods stochastic gradient descent nesterov’s accelerated gradient adagrad. fair comparison state-of-the-art methods adaptive layer-speciﬁc learning rate method optimization methods. experiments demonstrate effectiveness algorithm convolutional neural networks datasets. cifar global learning rate provided caffe. since method always increases layer-speciﬁc learning rate based global learning rate start slightly smaller learning rate make learning less aggressive imagenet experiment. initialized learning rate used experiments done imagenet. mnist architecture lenet experiments mnist. present results using proposed layer-speciﬁc learning rates stochastic gradient descent nesterov’s accelerated gradient method adagrad mnist dataset. since methods converge quickly dataset present accuracy loss ﬁrst iterations. table shows table mean error rate mnist different iterations stochastic gradient descent nesterov’s accelerated gradient adagrad layer speciﬁc adaptive versions shown table. method times mean standard deviation reported. fig. cifar data plots showing accuracies comparing adagrad adaptive layer-wise learning rates. plot show results step learning rate iterations well iterations. mean accuracy standard deviation method times. observe proposed layer-speciﬁc learning rate consistently better nesterov’s accelerated gradient stochastic gradient descent adagrad. experiments proposed method also attains maximum accuracy like stochastic gradient descent nesterov’s accelerated gradient adagrad. cifar cifar convolutional neural network layers feature maps convolution kernels followed pooling layers. another convolution layer feature maps convolution kernel followed pooling layer. finally fully connected layer hidden nodes soft-max logistic regression layer. convolution layer relu non-linearity applied. architecture speciﬁed caffe. ﬁrst iterations learning rate dropped factor iterations. dataset observe ﬁnal error loss method consistently lower adagrad step down adaptive method reaches lower accuracy nag. note using optimization method improvement mean accuracy sgd. even step learning rate iterations obtain accuracy better iterations signiﬁcantly cutting required training time fig. since method converges much faster used possible perform step learning rate even earlier potentially reducing training time even further. although adagrad perform well cifar default parameters observe improvement mean ﬁnal accuracy signiﬁcant speed-up training time. imagenet implementation alexnet caffe deep convolutional neural network architecture comparing method optimization algorithms. alexnet consists convolution layers followed fully connected layers. details regarding architecture found paper since alexnet deep neural network signiﬁcant complexity suitable apply method network architecture. shows results using method sgd. observe method obtains signiﬁcantly greater accuracy lower loss iterations. further also able reach maximum accuracy validation iterations achieved iterations resulting reduction training time. given large model takes week train properly signiﬁcant reduction. loss also consistently lower across iterations. existing model perform step factor every iterations. order analyze method performs reduce number training iterations vary number training iterations speciﬁc learning rate performing step down. table shows ﬁnal accuracy iterations method. although ﬁnal accuracy drops slightly decrease number iterations perform step learning rate clearly evident method achieves better accuracy sgd. note report top- class accuracy. since caffe implementation alexnet architecture data augmentation techniques results slightly lower reported socher perelygin chuang manning potts recursive deep models semantic compositionality sentiment treebank proceedings conference empirical methods natural language processing citeseer hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. glorot bordes bengio deep sparse rectiﬁer networks proceedings international conference artiﬁcial intelligence statistics. jmlr w&cp volume vol. preprint arxiv. duchi hazan singer adaptive subgradient methods online learning stochastic optimization journal machine learning research vol. fyodorov williams replica symmetry breaking condition exposed random matrix calculation landscape complexity journal statistical physics vol. polyak some methods speeding convergence iteration methods ussr computational mathematics mathematical physics vol. nesterov method solving convex programming problem convergence rate soviet mathematics doklady vol. sutskever martens dahl hinton importance initialization momentum deep learning proceedings international conference machine learning fig. imagenet data plot comparing stochastic gradient descent adaptive layer-wise learning rates. consistent improvement accuracy loss regular method across iterations. used optimization method global learning rate. method uses gradients layer compute adaptive learning rate layer. aims speed convergence parameters curvature saddle point region. layer-speciﬁc learning rates also enable method prevent slow learning initial layers deep network usually caused small gradient values.", "year": 2015}