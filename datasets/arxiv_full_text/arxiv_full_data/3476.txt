{"title": "A Latent Variable Model for Two-Dimensional Canonical Correlation  Analysis and its Variational Inference", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Describing the dimension reduction (DR) techniques by means of probabilistic models has recently been given special attention. Probabilistic models, in addition to a better interpretability of the DR methods, provide a framework for further extensions of such algorithms. One of the new approaches to the probabilistic DR methods is to preserving the internal structure of data. It is meant that it is not necessary that the data first be converted from the matrix or tensor format to the vector format in the process of dimensionality reduction. In this paper, a latent variable model for matrix-variate data for canonical correlation analysis (CCA) is proposed. Since in general there is not any analytical maximum likelihood solution for this model, we present two approaches for learning the parameters. The proposed methods are evaluated using the synthetic data in terms of convergence and quality of mappings. Also, real data set is employed for assessing the proposed methods with several probabilistic and none-probabilistic CCA based approaches. The results confirm the superiority of the proposed methods with respect to the competing algorithms. Moreover, this model can be considered as a framework for further extensions.", "text": "describing dimension reduction techniques means probabilistic models recently given special attention. probabilistic models addition better interpretability methods provide framework extensions algorithms. approaches probabilistic methods preserving internal structure data. meant necessary data ﬁrst converted matrix tensor format vector format process dimensionality reduction. paper latent variable model matrix-variate data canonical correlation analysis proposed. since general analytical maximum likelihood solution model present approaches learning parameters. proposed methods evaluated using synthetic data terms convergence quality mappings. also real data employed assessing proposed methods several probabilistic none-probabilistic based approaches. results conﬁrm superiority proposed methods respect competing algorithms. moreover model considered framework extensions. applied different applications probabilistic dimension reduction models offer many beneﬁts including handling missing outlier data automatic selection number projection vectors extending standard dimension reduction methods complex ones mixture models non-linear models tipping bishop presented probabilistic model principal component analysis called probabilistic showed relation could found projections extracted method maximum likelihood solution restricted factor analysis model lawrence proposed dual model ppca extended nonlinear case gaussian processes bach jordan presented probabilistic interpretation canonical correlation analysis called probabilistic method latent variable model used describe gaussian random vectors. recently different variations pcca also proposed mentioned methods assume input data features described vectors. however many applications encounter data intrinsic structure matrix tensor. example face recognition task pixels image considered features matrix structure image processing gabor functions frequently used feature extraction outputs matrix structure traditional dimension reduction approaches structures usually broken features concatenated long vector. however leads small sample size problem increase computational cost large matrices therefore recent years attention paid algorithms data transformation preprocessing step. two-dimensional general rank approximation matrix two-dimensional among ﬁrst examples algorithms. probabilistic interpretation -or-more-dimensional subspace feature extraction techniques also active research ﬁeld. presented decoupled bayesian tensor analysis model could reduce dimensions tensor data automatically determine appropriate dimensions applying matrix variate distributions probabilistic higher-order matrix-variate data introduced variational expectation-maximization applied learning parameters model. another probabilistic model dpca proposed zhao called bilinear ppca extend previous models deﬁning three separate model noises called column common noise. bpcca formulated proposed model two-stage representation parameters model computed maximum likelihood estimation safayani introduced probabilistic dcca models columns rows images called left right probabilistic models deﬁned learning parameters assumed parameters right probabilistic model known observations projected corresponding latent spaces. parameters left probabilistic model estimated using algorithm. similar procedure parallel left probabilistic model parameters right probabilistic model learned. procedure repeated convergence. approach assumed columns observation matrix independent probability distribution constructed producting probability corresponding columns. also existence models rows columns pair data mapped single latent space. causes model generative model. paper probabilistic model matrix data assumption presented. model random matrices related latent variable matrix-variate normal distribution. since closed-form solution learning parameters model approaches proposed. ﬁrst approach called unilateral matrix-variate assumes latent variables projected side assumption model parameters estimated using algorithm. second approach simple assumption ﬁrst approach considered hidden variables written sides model named bilateral matrix-variate learn parameters model algorithm based variational proposed. learning algorithm posterior distribution estimated using matrix-variate normal distribution lower bound log-likelihood maximized respect variational parameters mean matrix covariance matrices matrix-variate normal distribution. proposed algorithms initially assessed using synthetic data convergence analysis also accuracy mapping matrices evaluated nir-vis face database results compared competing algorithms dcca pcca pdcca rest paper organized follows section matrix-variate normal distribution well dcca pcca brieﬂy reviewed. proposed method presented section iii. section devoted assessing proposed methods using syntectic real data. finally paper concluded section since two-dimensional probabilistic models deal random matrices variables convenient matrix-variate distributions model them. general matrix-variate distributions generalization multivariate distributions matrix-variate normal distribution famous deﬁned follows irm×n random matrix irm×n mean matrix irm×m irn×n column covariance matrices respectively denotes trace matrix also shown follows kronecker product canonical correlation analysis method seeks relationships multivariate sets variables maps data common subspace correlation data sets maximized. example assumes random vectors. ﬁnds linear mappings following criteria maximized observation random vector latent vector multivariate normal distribution zero mean identity covariance matrix residual noise vector follows multivariate normal distribution expectation zero covariance matrix mean vector random vector model conditioned latent variable independent. shown following distributions result arbitrary matrices directions also iterative algorithm based proposed maximize reason zn|n missing values optimization algorithm complete log-likelihood follows index observation variable irmj×nj irmj×nj observed variable residual noise matrix respectively ird×d latent matrix variable irmj×nj indicates means corresponding observed variable irmj×d irnj×d left right projection matrices respectively. latent noise matrix variables following distributions operator vectorize input matrix concatenating columns observed projection matrix kronecker product right left projection matrices similarly noise covariance matrix decompose right left noise covariance matrices. therefore number free parameters model much less model pcca likelihood function observed data i.e. could obtained integrating latent variable matrix-variate distributions general case also posteriori distribution follow matrix-variate normal distribution. therefore propose approaches learning parameters. ﬁrst simplify model assuming projection matrix posteriori distribution derived based matrix-variate normal distribution solution based algorithm provided. call approach unilateral matrix variate model. another approach call bilateral matrix variate model consider model general case estimate posterior distribution using parametric matrix-variate normal distribution lower bound log-likelihood maximized using variational algorithm approaches discussed here. case assume equation left right projection matrices exist. maximizing likelihood function need estimate posterior latent variable given observed variables however general matrix-variate formulation posterior. therefore employ variational-em algorithm maximizing lower-bound likelihood function. variational-em parameterised variational distribution chosen estimate posteriori distribution parameters optimised. here consider following parametric distribution form matrix-variate normal ird×d mean matrix ird×d ird×d column covariance matrices respectively. variational parameters optimised maximized lower bound data log-likelihood function written natural probabilistic projection. here similar ppca represent observed data low-dimensional space using mean posterior distributioni.e. however bmvcca estimates posterior distribution consider mean seen equation applied whenever observation matrices. case observation matrix consider low-dimensional representation average random matrix training dataset. similar formula obtained generating synthetic data ﬁrst sample element projection matrices ir×| uniform distribution range zero one. pair observed data latent matrix residual matrices ir×| sampled respectively observed data ir×} generated using equation generate samples procedure bmvcca algorithm compute frobenius norm projection matrix different iterations algorithm. fig. plots difference computed norms successive iterations. seen algorithm converges iterations. section want mean distribution latent variables conditioned observed matrices compute distance true latent variables. similar previous section generate pairs data ir×} ir×| vectors come uniform distribution range zero data sampled scalar sampled distribution residual matrices ir×| effect number training data repeat experiment different training samples ranging samples. fig. illustrates result observed accuracy estimation improved increased training size. noted experiment applicable latent space. fact obtained latent space leads true latent space rotation general solution obtaining rotation matrices. shown whenever subspace restricted rotational matrix true learned latent variables equal scaling factor magnitude removed normalizing latent spaces. however sign cannot removed procedure canceling sign scaling factor compute euclidean distance true latent space positive negative sign learned subspace minimum chosen. analyzing learned projections umvcca section examine learned projection matrices umvcca using synthetic data. generate pairs observation data ir×} procedure similar previous section. however time right projection vectors ir×| exist. then umvcca learned projection vectors obtained. fig. plots true learned projection vectors. observed without regarding sign scaling factor true learned vectors similar. analyze performance proposed algorithms nir-vis .this dataset contains visible near infrared face images. data collected four different sessions session visible infrared images taken subject participated session. unique person-session subjects dataset face images subject. however subjects image also either visible infrared images. therefore select person-session experiments. label considered subjects involved sessions. select face images train data face images test data. faced images cropped eyes images coordinates. face images gray scaled resized fig. shows several samples dataset. image reconstruction experiment ﬁrst project pairs visible infrared images lowdimensional subspace using bmvcca reconstruct original images. fig. depicts images visible infrared spectrum persons well corresponding low-dimensional subspace reconstructed images. here compressed representation obtained equation observed reconstructed images similar original images. convergence fig. demonstrates euclidean distance consecutive projection matrices learning bmvcca. observed iterations distance goes zero. also fig. depicts plot lower bound logarithm likelihood respect different iterations. observed algorithm converges iterations. face recognition task face recognition train model pairs visible infrared images project train test data low-dimensional space. compute euclidean distance projected test image projected train images choose label train image least distance label test image. table expresses corresponding formula representing images low-dimensional space method. moreover addition mentioned criteria based euclidean distance projected test image train images utilize following probabilistic criteria test image. equation calculates conditional probability test image given training images selects label corresponding probable image test data label. call approach probabilistic test abbreviate ptest tables. table compares error rate different algorithms different number features low-dimensional space. umvcca number features dimension reduced feature space therefore select appropriate number obtained features closed number features corresponding column. example number features ﬁrst second column table respectively. umvcca consider value respectively columns produces features. since pcca methods suffer small sample size problem leads singular matrices. ﬁrst project data lower dimension using applying corresponding algorithms. therefore cannot project data features algorithms consequently place dash corresponding column. observed form table bmvcca ptest criteria outperforms algorithms signiﬁcantly. paper probabilistic model proposed works matrix-variate data image matrices. iterative approach learning parameter presented. ﬁrst approach called unilateral matrix variate model restricted mapping latent matrix side learning method based expectation maximization introduced. approach called bilateral matrix variate latent matrix mapped sides posterior distribution estimated using variational matrixvariate distribution variational parameters estimated using variational expectation maximization. proposed algorithms evaluated using syntectic real data. results indicated algorithm converges iteration. also comparison based algorithms showed proposed algorithm better performance terms recognition accuracy. model extended complex models mixture models nonlinear models bayesian framework. tipping bishop mixtures probabilistic principal component analyzers neural computation vol. zhao efﬁcient model selection mixtures probabilistic hierarchical ieee transactions cybernetics vol. bach jordan probabilistic interpretation canonical correlation analysis klami virtanen kaski bayesian canonical correlation analysis mach. learn. res. vol. apr. michaeli wang livescu nonparametric canonical correlation analysis http//arxiv.org/abs/. sarvestani boostani ff-skpcca kernel probabilistic canonical correlation analysis applied intelligence daugman uncertainty relation resolution space spatial frequency orientation optimized two-dimensional visual cortical ﬁlters choi two-dimensional canonical correlation analysis ieee signal processing letters vol. z.-h. c.-r. zhao two-dimensional canonical correlation analysis application small sample size face recognition matrix-variate higher-order probabilistic projections data min. knowl. discov. vol. gupta nagar matrix variate distributions. press vol. zhao kwok bilinear probabilistic principal component analysis neural networks learning systems ieee transactions", "year": 2017}