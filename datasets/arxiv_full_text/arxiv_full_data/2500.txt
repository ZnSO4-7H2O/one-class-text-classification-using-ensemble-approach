{"title": "Value Iteration with Options and State Aggregation", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This paper presents a way of solving Markov Decision Processes that combines state abstraction and temporal abstraction. Specifically, we combine state aggregation with the options framework and demonstrate that they work well together and indeed it is only after one combines the two that the full benefit of each is realized. We introduce a hierarchical value iteration algorithm where we first coarsely solve subgoals and then use these approximate solutions to exactly solve the MDP. This algorithm solved several problems faster than vanilla value iteration.", "text": "paper presents solving markov decision processes combines state abstraction temporal abstraction. speciﬁcally combine state aggregation options framework demonstrate work well together indeed combines full beneﬁt realized. introduce hierarchical value iteration algorithm ﬁrst coarsely solve subgoals approximate solutions exactly solve mdp. algorithm solved several problems faster vanilla value iteration. finding solutions discrete discounted markov decision processes important problem reinforcement learning. basic problem obtain optimal policy overall discounted reward obtained follow policy within maximized. work work optimal policy directly compute optimal value function instead. approach take paper modify well-known value iteration algorithm basic idea keep iterating bellman optimality equation. well-known converge optimal value function. framework conceptually based natural extension bellman optimality equation matrix models take place vector value functions. order solve large problems table-lookup algorithms practical sheer number states must loop over. hence need state abstraction. work chose aggregation nicely integrated framework modiﬁed bellman optimality equation. algorithms based single-step models primitive actions impractical long solution paths require many iterations hence need temporal abstraction. solve problem options construct option models used interchangeably models primitive actions. knowledge ﬁrst paper algorithm using options value iteration efﬁciently solves medium-sized mdps unlike prior work demonstrate modest improvement runtime performance well signiﬁcant reduction number iterations. also ﬁrst convergent vi-style algorithm options combined framework state abstraction yielding better results either idea alone. furthermore algorithm based principled extension bellman equation. emphasise algorithm converges optimal value function although approximate solutions subgoals solutions used inputs solve original exactly regardless choice subgoals. state aggregation consider actions; action probability transition matrix deﬁned vector expected rewards state element corresponding state deﬁned aggregate respectively. state aggregation approximation solving original replaced solving much smaller aggregate computing dpaφ dra. solution computed known algorithm. convergent matrices deﬁne valid mdp. gives value function terms aggregate states. binary termination condition tells whether option terminates state discuss models options primitive actions. model consists transition matrix vector expected rewards primitive action deﬁned section options analogous meaning. expected total discounted reward given corresponds matrix multiplication i.e. block matrices also block matrix corresponding ﬁrst executing option deﬁned deﬁned paper assume action already given matrix format. introduce similar format value functions. value function represented vector length ﬁrst index values state subsequent indices. value function corresponding ﬁrst executing option deﬁned evaluating states element vector state action model. matlab notation i.e. element vector matrix give brief survey known approaches hierarchical learning. stress approach novel reasons compose macro-operators run-time ﬁxed hierarchy. done date except work options introduced generalizations bellman equation versions use. include state abstraction slowing resulting algorithm produced decrease iteration count required solve provide better solution time. approaches include using macro-operators gain speed planning deterministic systems only. prior work hexq largely orthogonal focuses hierarchy discovery describe algorithm given subgoals. work portable options discusses ﬁxed options hierarchy. maxq also involves pre-deﬁned controller hierarchy combining temporal state abstraction tried before diﬀerently work. abstraction-via-statistical-testing approach works transfer learning options constructed original solved. u-tree approach guarantee convergence mdps. modiﬁed lisp approach uses ﬁxed option hierarchy policy obtained optimal given hierarchy i.e. optimal policy without hierarchy constraint. begin describing table-lookup algorithm computing value function mdp. similar described previous work here termination implemented diﬀerent intuitive way. start plain proceed complicated variants. matlab notation described follows state hence equivalent plain however serves important stepping stone introducing subgoals next. assume moment interested maximizing overall reward. instead want reach arbitrary conﬁguration states deﬁned subgoal vector length entry deﬁnes value associate reaching state show later picking subgoals judiciously improve speed overall algorithm. update subgoal following execute state iteration converges model corresponds policy reaching subgoal however policy executes continually stop state high subgoal value reached. introducing possibility termination state ﬁrst determine subgoal considered reached make next step. two-stage process given below. first compute termination condition state according following equation. note optimization linear function therefore either conceptually update thought converting non-binary subgoal speciﬁcation binary termination condition computed deﬁne diagonal matrix diag well matrix follows. here identity matrix. summarizes termination condition behaves like model states terminate like identity model states this deﬁne actual update executed state iterating many times obtain tend every state states high values subgoal elements speciﬁed units rewards i.e. algorithm non-terminating states state particular value subgoal value subgoal exceeds opportunity loss reward way. terminating states model still make step according induced policy speed algorithm introduction initiation sets. case instead selecting action possible actions select action allowed actions given state formally boolean vector ‘true’ entries action allowed state ‘false’ otherwise. equation becomes following. beneﬁt using initiation sets considering irrelevant actions whole algorithm becomes much faster. defer deﬁnition initiation sets used section finally solve several subgoals simultaneously. current state every model every iteration compute next iteration models. denote subgoals k-th iteration models trying solve subgoals deﬁne models allowed itera}. gives rise update given below tion i.e. solving several subgoals simultaneously improve algorithm immediate availability partial solution every subgoal leads faster convergence. words feature used construct macro-operator hierarchy time algorithm. contrast many approaches hierarchy ﬁxed algorithm run. section given aggregation matrix disaggregation matrix convert action transition matrix expected reward vector aggregate using matrix model notation becomes follows. main idea algorithm following deﬁne subgoal solve action original problem macro-action gaining speed. repeat many subgoals. solving subgoals fast small aggregate state space. precise pick subgoal approximation architecture compress actions compressed actions according eqs. gives model solving subgoal aggregate state space. want model help solve original mdp. however cannot directly since model deﬁned respect aggregate state space size need convert model deﬁned original state space size model also valid i.e. correspond sequence actions. idea make following transformation aggregate model compute option aggregate state space up-scale option original state space construct onestep model construct long-term model concretely ﬁrst compute option corresponding model ˜m∞. option consists policy termination condition obtain termination condition using aggregate states. policy obtained greedily aggregate state wherever option terminate rows identity matrix wherever does. need model takes step towards subgoal want takes way. therefore continually evaluate option exponentiating model matrix producing model still rows identity matrix option terminates therefore correspond valid combination primitive actions. solve problem compute according following equation contains rows option terminate rows dictated option policy does. guarantees valid combination primitive actions added action treated like action. value iteration using extended action original actions subgoal models corresponding subgoal faster using original actions alone even factoring time used compute subgoal models proof outline. addition subgoal macro-operators action change ﬁxpoint value iteration macro-operators construction compositions original actions. supplement existing work formal proof general proposition. observation tells algorithm always exactly solve computing worst thing happen subgoal macro-operators useless i.e. resulting value iteration take many iterations without them. looking eqn. best compress actions. seem using linear features better expressive easier come speciﬁcally consider following compressing actions alternative deﬁne approximation architecture modelling value functions sequence converge optimal value function. begin deﬁning projection operator compresses table-lookup model model works linear features transition reward components separately; also column treated independently others. semantics such column make entry corresponding k-th column close possible feature number next state index current state similarly picked approximate expected next reward state. words linear dynamical system models one-step dynamics features markov chain corresponding action. might hope linear dynamical system could used much compressed state aggregation composed time again algorithm diverge. argument given shows cannot arbitrary features distributions hand framework based suﬀer described divergent behaviour. also depend distribution states meaning less parameter algorithm. applied approach three domains taxi hanoi -puzzle. case compared several variants including approach combining state aggregation options. vanilla considered algorithms based figure summarises solution times domain; details given following domain-speciﬁc subsections. however stress beforehand algorithm produced speeed-up domains tried. taxi prototypical example problem combines spatial navigation additional variables. denote number states number aggregate states state sink state. ﬁrst experiment four algorithms computing optimal value function. combination using state aggregation options. consider using neither aggregation options model iteration complexity practice sparsity. fuel pump. iteration complexity n)). sparsity becomes on). algorithm needs iterations less converge subgoals allow make jumps. however increased cost iteration time required converge increased. look version aggregation options. aggregate states. original state states taking taxi position ignoring variables. sink state gets mapped aggregate sink state proceed stages. first actions compressed then problem solved using model smaller state-space. takes iterations fast small value function obtain greedy model initialization iteration takes iterations less original algorithm. consider ﬁnal version beneﬁts aggregation options combined. again algorithm consists stages. first compressed actions compute models getting subgoals. requires iterations; complexity macro-actions need original four actions moving sensible movement locations. algorithm takes iterations converge. run-time i.e. speed-up times model results four versions chosen diagonal matrix entries left principal eigenvector state transition matrix turns that matrix spectrum within unit circle leads convergent algorithm. however problem approach takes account long-term eﬀects actions. instance two-dimensional grid-world action goes right distribution non-zero along rightmost edge grid-world. practice seldom ergodic information non-recurrent part lost. cannot lead good overall solutions mdp. summarized ﬁgure also constructed stochastic version problem probability staying original state moving. results qualitatively similar ﬁgure speed-up combining options aggregation greater times. stress main result. deterministic case replace many iterations many iterations followed iterations. stochastic problems replace many iterations many iterations followed iterations. second experiment digression main thrust paper tried diﬀerent approach aggregation framework compute approximate value function gaining speed. actions compressed deﬁned simply apply process gives value function deﬁned aggregate state space upscale value function original states using original problem. consider aggregate states aggregation happens eliminating fuel variable leaving others intact. algorithm used given applied compressed actions. takes iterations converge iterations learned value function corresponds policy ignores fuel never visits pump otherwise enough fuel transports passenger intended. shown important principle aspect system feel solution ignore eliminate still approximate solution. beneﬁt speed-up. case respect solving original using plain smallest disk moving disk remaining pegs. known problem takes iterations converge. speed iteration introduced following state abstraction. sub-problems size ...r first solve problem disks i.e. abstraction considers position smallest disks ignoring rest. three subgoals placing disks pegs. then obtained three models subgoals solve sub-problem size ignoring disks except three smallest ones. again three subgoals. proceed solve problem disks. subgoal need iterations note however time complexity algorithm subgoals still exponential whereas number iterations linear iteration need iterate whole state space exponential. stochastic version run-times model plain computing optimal value function options aggregation. -puzzle well-known planning community. subgoal shown ﬁgure ‘a’‘b’ denote groups tiles. subgoal consists arranging tiles group correct place matrix original conﬁguration tiles mapped onto tile marked group belongs using subgoal alone result speed-up used notion initiation sets trained subgoal iterations obtained model able reach subgoal starting states upscaled model time model initiation containing states subgoal reachable. iteration used plain value iteration extended initiation sets. intuition behind initiation sets makes sense subgoal already part state space close thus obtained total run-time seconds amounts speed-up plain value iteration. results ﬁgure introduced bellman optimality equations facilitate options. equations combined state aggregation sound therefore applied solution mediumsized mdps. ﬁrst algorithm combining options state abstraction guaranteed converge. notable proposed approaches notably based linear features known diverge even small problems. finally shown experimentally beneﬁts options state aggregation realized applied together.", "year": 2015}