{"title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D  View Synthesis", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.", "text": "important problem graphics vision synthesize novel views object single image. particularly challenging partial observability inherent projecting object onto image space ill-posedness inferring object shape pose. however train neural network address problem restrict attention speciﬁc object categories gather ample training data. paper propose novel recurrent convolutional encoder-decoder network trained end-to-end task rendering rotated objects starting single image. recurrent structure allows model capture long-term dependencies along sequence transformations. demonstrate quality predictions human faces multi-pie dataset dataset chair models also show ability disentangle latent factors variation without using full supervision. numerous graphics algorithms established synthesize photorealistic images models environmental variables commonly known rendering. time recent advances vision algorithms enable computers gain form understanding objects contained images classiﬁcation detection segmentation caption generation name few. approaches typically deduce abstract representations image pixels. however long-standing problem graphics vision automatically synthesize novel images applying intrinsic transformations subject input image. artiﬁcial intelligence perspective viewed answering questions object appearance view angle illumination changed action taken. synthesized images perceived humans photo editing evaluated machine vision systems game playing agent vision-based reinforcement learning paper consider problem predicting transformed appearances object rotated single image. general ill-posed problem loss information inherent projecting object image space. classic geometry-based approaches either recover object model multiple related images i.e. multi-view stereo structure-from-motion register single image known object category prior model e.g. faces resulting mesh used re-render scene novel viewpoints. however meshes intermediate representations methods limited particular object categories vulnerable image alignment mistakes easy generate artifacts during unseen texture synthesis. overcome limitations propose learning-based approach without explicit model recovery. observed rotations similar objects trained model better infer true pose shape texture object make plausible assumptions potentially ambiguous aspects appearance novel viewpoints. thus learning algorithm relies mappings euclidean image space underlying nonlinear manifold. particular view synthesis cast pose manifold traversal desired rotation decomposed sequence small steps. major challenge arises long-term dependency among multiple rotation steps; identifying information original input must remembered along entire trajectory. furthermore local rotation step must generate correct result data manifold subsequent steps also fail. closely related image generation task considered paper problem invariant recognition involves comparing object images different viewpoints poses dramatic changes appearance. shepard metzler mental rotation experiments found time taken humans match objects different views increased proportionally angular rotational difference them. humans rotating mental images steady rate. inspired mental rotation phenomenon propose recurrent convolutional encoder-decoder network action units model process pose manifold traversal. network consists four components deep convolutional encoder shared identity units recurrent pose units rotation action inputs deep convolutional decoder rather training network model speciﬁc rotation sequence provide control signals time step instructing model move locally along pose manifold. rotation sequences varying length. improve ease training employed curriculum learning similar used sequence prediction problems intuitively model learn make one-step rotation learning make series rotations. main contributions work summarized follows. first novel recurrent convolutional encoder-decoder network developed learning apply out-of-plane rotations human faces chair models. second learned model generate realistic rotation trajectories control signal supplied step user. third despite trained synthesize images model learns discriminative view-invariant features without using class labels. weakly-supervised disentangling especially notable longer-term prediction. related work transforming autoencoder introduces notion capsules deep networks tracks presence position visual features input image. models apply afﬁne transformations rotations images. address similar task rendering object appearance undergoing rotations convolutional network architecture lieu capsules incorporate action inputs recurrent structure handle repeated rotation steps. predictive gating pyramid developed time-series prediction learn image transformations including shifts rotation multiple time steps. task related time-series prediction formulation includes control signal uses disentangled latent features uses convolutional encoder decoder networks model detailed images. ding taylor proposed gating network directly model mental rotation optimizing transforming distance. instead extracting invariant recognition features shot model learns perform recognition exploring space relevant transformations. similarly model explore space rotation object image setting control signal time step recurrent network. problem training neural networks generate images studied dosovitskiy proposed convolutional network mapping shape pose transformation labels images generating chairs. able control factors variation generate high-quality renderings. also generate chair renderings paper model adds several additional features deep encoder network distributed representations appearance pose recurrent structure long-term prediction. contemporary work inverse graphics network also adds encoding function learn graphics codes images along decoder similar chair generating network. model uses deep convolutional encoder extract image representations apply modiﬁcations these re-render. model differs train recurrent network perform trajectories multiple transformations control signal input step deterministic feed-forward training rather variational auto-encoder framework related line work disentangling latent factors variation generate natural images. bilinear models separating style content developed shown capable separating handwriting style character identity also separating face identity pose. disentangling boltzmann machine applies idea augment restricted boltzmann machine partitioning hidden state distinct factors variation modeling higher-order interaction. multi-view perceptron employs stochastic feedforward network disentangle identity pose factors face images order achieve view-invariant recognition. encoder network also trained learn disentangled representation images extracting graphics code factor. latent factors variation discovered disentangled using novel hidden unit regularizer. work also loosely related deepstereo\" algorithm synthesizes novel views scenes multiple images using deep convolutional networks. recurrent convolutional encoder-decoder network section describe model formulation. given image object goal synthesize rotated views. inspired recent success convolutional networks mapping images high-level abstract representations synthesizing images graphics codes base model deep convolutional encoder-decoder networks. example network structure shown figure encoder network used convolution-relu layers stride -pixel padding dimension halved convolution layer followed fullyconnected layers. bottleneck layer deﬁne group units represent pose desired transformations applied. group units represent change transformations named identity units. decoder network symmetric encoder. increase dimensionality ﬁxed upsampling found ﬁxed stride- convolution upsampling worked better max-pooling unpooling switches applying transformations encoder pooling switches would general match switches produced target image. desired transformations reﬂected action units. used -of- encoding encoded clockwise rotation encoded noop encoded counter-clockwise rotation. triangle indicates tensor product taking input pose units action units producing transformed pose units. equivalently action unit selects matrix transforms input pose units output pose units. action units introduce small linear increment pose units essentially model local transformations nonlinear pose manifold. however order achieve longer rotation trajectories simply accumulate linear increments action units two-step clockwise rotation pose units fall manifold resulting predictions. overcome problem generalize model recurrent neural network shown capture long-term dependencies wide variety sequence modeling problems. essence recurrent pose units model step-by-step pose manifold traversals. identity units shared across time steps since assume training sequences preserve identity changing pose. figure shows unrolled version model. perform encoding ﬁrst time step transformations carried latent space; i.e. model predictions time step next time step input. training objective based pixel-wise prediction time steps training sequences figure unrolled recurrent convolutional network learning rotate objects. convolutional encoder decoder abstracted represented vertical rectangles. apiq sequence actions fidpxpiqq produces identity features invariant time steps fposepxpiq apiq produces transformed pose features time step gp¨¨q image decoder producing image given output fidp¨q fposep¨¨¨q xpiq i-th image ypitq i-th training image target step curriculum training trained network parameters using backpropagation time adam optimization method effectively train recurrent network found beneﬁcial curriculum learning gradually increase difﬁculty training increasing trajectory length. appears useful sequence prediction recurrent networks domains well section show increasing training sequence length improves model’s image prediction performance well pose-invariant recognition performance identity features. also longer training sequences force identity units better disentangle pose. identity units need used predict ˝-rotated ˝-rotated image training units cannot pick pose-related information. model learn disentangled features without explicitly regularizing achieve effect. necessary gradient clipping. experiments carry experiments achieve following objectives. first examine ability model synthesize high-quality images face complex objects wide range rotational angles. second evaluate discriminative performance disentangled identity units cross-view object recognition. third demonstrate ability generate rotate novel object classes interpolating identity units query objects. datasets multi-pie. multi-pie dataset consists face images people. images captured viewpoints illumination conditions different sessions. evaluate model rotating faces select subset multi-pie covers viewpoints evenly neutral illumination. face image aligned manually annotated landmarks eyes nose mouth corners cropped pixels. images ﬁrst people training remaining people testing. chairs. dataset contains chair models made publicly available aubry chair model rendered azimuth angles elevation angles ﬁxed distance virtual camera. subset chair models experiments selected dosovitskiy order remove nearduplicate models low-quality models. crop rendered images small border resize common size pixels. also prepare binary masks subtracting white background. images ﬁrst models training remaining models test set. figure view synthesis multi-pie. panel ﬁrst shows ground truth second third rows show re-renderings -step clockwise rotation input image -step counter-clockwise rotation input image respectively. network architectures training details multi-pie. encoder network multi-pie dataset used convolution-relu layers stride -pixel padding followed fully-connected layer ˆˆ´ˆˆ´. number identity pose units respectively. decoder network symmetric encoder. curriculum training procedure starts single-step rotation model call rnn. prepare training samples pairing face images person captured session adjacent camera viewpoints. example xpiq mapped ypiq action apiq xpiq mapped ypiq action apiq xpiq mapped ypiq action apiq face images ending viewpoints one-way rotation feasible. train network using adam optimizer ﬁxed learning rate epochs. since viewpoints person session schedule curriculum training stages call respectively. sample training sequences ﬁxed length allow clockwise counter-clockwise rotations. example input image xpiq mapped pypiq ypiq ypiq ypiqq corresponding angles action inputs prsrsrsrsq. stage initialize network parameters previous stage ﬁne-tune network ﬁxed learning rate additional epochs. chairs. encoder network chairs used three convolution-relu layers stride -pixel padding followed fully-connected layers decoder network symmetric except fully-connected layers branches image mask prediction layers. mask prediction indicates whether pixel belongs foreground background. adopted idea generative found beneﬁcial training efﬁciency image synthesis quality. tradeoff parameter applied mask prediction loss. train single-step network parameters ﬁxed learning rate epochs. schedule curriculum training call rnn. note curriculum training stops reached limit memory. since images chair model rendered viewpoints evenly sampled easily prepare training sequences clockwise counterclockwise t-step rotations around circle. similarly network parameters current stage initialized previous stage ﬁne-tuned learning rate epochs. ﬁrst examine re-rendering quality models novel object instances seen training. multi-pie dataset given input image test possible views encoder produces identity units pose units decoder renders images progressively ﬁxed identity units action-driven recurrent pose units t-steps. examples shown figure longest rotations i.e. clockwise counter-clockwise rnn. high-quality renderings generated smooth transformations adjacent views. characteristics faces gender expression eyes nose glasses also preserved rotation. also compare model state-of-the-art morphable model face pose normalization figure observed model produces stable renderings morphable model sensitive facial landmark localization. advantages morphable model preserves facial textures well. chair dataset synthesize rotated views novel chairs test set. given chair image certain view deﬁne action sequences; progressive clockwise rotation another counter-clockwise rotation. challenging task compared rotating faces complex shapes chairs large rotation angles since previous methods tackle exact chair re-rendering problem k-nearest-neighbor method baseline comparisons. baseline implemented follows. ﬁrst extract features vgg- chair images. test chair image k-nearest neighbors training comparing features. retrieved top-k images expected similar query terms style pose given desired rotation angle synthesize rotated views test image averaging corresponding rotated views retrieved top-k images training pixel level. tune value namely achieve best performance. examples shown figure model shapes well preserved clear boundaries rotated views different input appearance changes smoothly adjacent views consistent style. figure view synthesis -step rotations chairs. panel compare synthesis results model baseline ﬁrst panels belong chair different starting views last panels another chair starting views. input images marked boxes. note conceptually learned network parameters different stages curriculum training used process arbitrary number rotation steps. model works well ﬁrst rotation step produces degenerate results second step. trained two-step rotations generates reasonable results third step. progressively seem generalize well chairs longer predictions measure quantitative performance mean squared error figure result best retrievals obtains comparable model model signiﬁcantly outperforms relative improvement. cross-view object recognition experiment examine compare discriminative performance disentangled representations cross-view object recognition. multi-pie. create gallery/probe splits test set. split face images view e.g. collected gallery rest views probes. extract features identity units rnns test images probes matched gallery cosine distance. considered success matched gallery image identity probe. also categorize probes split measuring angle offsets gallery. particular angle offsets range recognition difﬁculties increase angle offsets. demonstrate discriminative performance learned representations also implement convolutional network classiﬁer. architecture connecting encoder identity units -way softmax output layer parameters learned training ground truth class labels. features extracted layer softmax layer used perform cross-view object recognition above. figure compares average success rates rnns standard deviations splits angle offset. success rates drop angle offset success rates keep improving general curriculum training rnns best results achieved rnn. expected performance reduces phenomenon demonstrates model gradually learns pose/viewpoint-invariant representations face recognition. without using class labels model achieves competitive results cnn. chairs. experimental setup similar multi-pie. total azimuth views chair instance. view create gallery/probe split splits. extract features identity units rnn. probes split sorted angle offsets gallery images. note experiment particularly challenging chair matching ﬁne-grained recognition task chair appearances change signiﬁcantly rotations. also compare model instead training scratch pre-trained vgg- extract features chair matching. success rates shown figure performance drops quickly angle offset greater signiﬁcantly improves overall success rates especially large angle offsets. notice standard deviations large around angle offsets views contain information chair shapes views performance variations. interestingly performance vgg- surpasses model angle offset greater hypothesize phenomenon results symmetric structures chairs. vgg- trained mirroring data augmentation achieve certain symmetric invariance model explore structure. demonstrate disentangling property model pose units extracted input images repeat cross-view recognition experiments. mean success rates shown table turns better identity units perform worse pose units perform. identity units achieve near-perfect recognition multi-pie pose units obtain mean success rate close random guess classes. class interpolation view synthesis experiment demonstrate ability model generate novel chairs interpolating existing ones. given chair images view different instances encoder network used compute identity units pose units pose pose interpolated zpose zpose recurrent decoder network render rotated views. example interpolations four chair instances shown figure interpolated chairs present smooth stylistic transformations pair input classes unique stylistic characteristics also well preserved among rotated views figure chair style interpolation view synthesis. given four chair images view test presents renderings style manifold traversal ﬁxed view column presents renderings pose manifold traversal ﬁxed interpolated identity. conclusion paper develop recurrent convolutional encoder-decoder network demonstrate effectiveness synthesizing views unseen object instances. multi-pie dataset database chair models model predicts accurate renderings across trajectories repeated rotations. proposed curriculum training gradually increasing trajectory length training sequences yields better image appearance discriminative features poseinvariant recognition. also show trained model could interpolate across identity manifold chairs ﬁxed pose traverse pose manifold ﬁxing identity. generative disentangling chair identity pose emerged recurrent rotation prediction objective even though explicitly regularize hidden units disentangled. future work includes introducing actions proposed model rotation handling objects embedded complex scenes handling one-to-many mappings transformation yields multi-modal distribution future states trajectory. acknowledgments work supported part n--- career iis- cmmi-. thank nvidia donating tesla gpu.", "year": 2016}