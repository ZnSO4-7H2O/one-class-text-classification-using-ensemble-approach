{"title": "Learning Graphical Models from a Distributed Stream", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A current challenge for data management systems is to support the construction and maintenance of machine learning models over data that is large, multi-dimensional, and evolving. While systems that could support these tasks are emerging, the need to scale to distributed, streaming data requires new models and algorithms. In this setting, as well as computational scalability and model accuracy, we also need to minimize the amount of communication between distributed processors, which is the chief component of latency. We study Bayesian networks, the workhorse of graphical models, and present a communication-efficient method for continuously learning and maintaining a Bayesian network model over data that is arriving as a distributed stream partitioned across multiple processors. We show a strategy for maintaining model parameters that leads to an exponential reduction in communication when compared with baseline approaches to maintain the exact MLE (maximum likelihood estimation). Meanwhile, our strategy provides similar prediction errors for the target distribution and for classification tasks.", "text": "denote probability event random variable denote domain shorthand random variable clear context. random variables denote joint distribution denote possible assignments deﬁnition bayesian network directed acyclic graph nodes edges represents random variable. denote parents nondescendants denote variables descendants random variables obey following condition conditionally independent nondescendants given lemma consider distributed system sites. given randomized distributed algorithm distcounter continuously maintains distributed counter property deﬁnition consider bayesian network denote joint distribution given approximation factor \u0001-approximation joint probability distribution that assignment values given additional parameter distribution -approximation \u0001-approximation probability least simple solution maintain parameters maintain counter exactly times coordinator. approach coordinator always joint distribution communication cost subtlety duplicate terms arising different leads terms product independent other. simplify cases maintain separate distributed counters counters using approximation error counters leads communication cost proportional jiki since number counters needed note maintain counter independently parent wasteful since tracking event. utilizing special structure better maintain copy counter accurate approximation factor objective assignment subset random variables given usual assignment maximizes probability given class maxy interested approximate version formulation given communication cost uniform nonuniform. reason networks used cardinalities random variables quite similar. words different equation similar values kis. makes approximation factors uniform nonuniform", "year": 2017}