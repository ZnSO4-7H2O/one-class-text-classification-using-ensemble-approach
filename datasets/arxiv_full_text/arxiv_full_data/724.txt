{"title": "Quadratically constrained quadratic programming for classification using  particle swarms and applications", "tag": ["cs.AI", "cs.LG", "cs.NE", "math.OC"], "abstract": "Particle swarm optimization is used in several combinatorial optimization problems. In this work, particle swarms are used to solve quadratic programming problems with quadratic constraints. The approach of particle swarms is an example for interior point methods in optimization as an iterative technique. This approach is novel and deals with classification problems without the use of a traditional classifier. Our method determines the optimal hyperplane or classification boundary for a data set. In a binary classification problem, we constrain each class as a cluster, which is enclosed by an ellipsoid. The estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem. The optimization problem is solved in distributed format using modified particle swarms. Our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of SVM.", "text": "particle swarm optimization used several combinatorial optimization problems. work particle swarms used solve quadratic programming problems quadratic constraints. approach particle swarms example interior point methods optimization iterative technique. approach novel deals classiﬁcation problems without traditional classiﬁer. method determines optimal hyperplane classiﬁcation boundary data set. binary classiﬁcation problem constrain class cluster enclosed ellipsoid. estimation optimal hyperplane between clusters posed quadratically constrained quadratic problem. optimization problem solved distributed format using modiﬁed particle swarms. method advantage using direction towards optimal solution rather searching entire feasible region. results iris pima wine thyroid datasets show proposed method works better neural network performance close svm. class algorithms originated minimizing maximizing function satisfying constraints history optimization function constraints linear problem known linear programming early published algorithms solving linear programming given dantzig popularly known simplex method number dimensions constraints increased solving using simplex method became hard. inability simplex method could solve polynomial time. khachiyan proposed ellipsoid algorithm alternative simplex method proved could reach solution iteratively polynomial time practical infeasible condition ellipsoid algorithm evolution several interior barrier point methods. well known interior point methods karmarkar’s method proposed narendra karmarkar binary classiﬁcation active research areas machine learning several ways train binary classiﬁer. class labels data stored retrieved classiﬁcation using approach nearest neighbor hyperplane learnt classiﬁcation training neural network always optimal vapnik others formulated problem classiﬁcation optimization. method known support vector machines sequential minimal optimization technique solves optimization problem svms decision trees bagging boosting techniques also used binary classiﬁcation nearest neighbor method involve modeling reduce storage training data hand neural network model data objective function estimate hyperplane used classiﬁcation. neural network approach objective function least squares quadratic nature minimized given data set. hyperplane obtained neural network optimal since depends number layers weights used train network. uses quadratic programming formulation linear constraints minimizing objective function several variants even though objective function used neural network quadratic programming problem constraints linear. model linear constraints quadratic constraints objective function becomes quadratically constrained quadratic programming paper binary classiﬁcation posed qcqp problem novel solution proposed using particle swarm optimization advantages approach solves qcqp problem without need gradient estimation. paper organized follows qcqp described background section solution quadratically constrained quadratic programming using particle swarms described sec. proposed method compared khachiyan’s karmarkar’s algorithms linear programming neural networks quadratic programming sec. experiments results. section concludes paper suggestions future work. particle swarm optimization proposed optimizing weights space neural network applied numerous applications optimizing non-linear functions evolved simulating bird ﬂocking schooling. advantages simple conception easy implement. particles deployed search space particle evaluated optimization function. best particle chosen directing agent rest. velocity particle controlled particle’s personal best global best. movement particles reach global best. movement particles adapted genetic algorithms evolutionary programming. ...xk} particles deployed search space optimization function number particles ...vk} velocities respective particles. particles. simple update follows weight previous velocity; constants random values varied iteration. personal best value particle global best value among particles. updated velocity particle iteration velocity value iteration. position particle iteration. novelty paper application particle swarms solving qcqp problem. particle swarms deployed within ellipsoid determine function evaluated particle search space. particle minimum considered closest point particle swarm shown figure ease representation. stochastic evolutionary algorithm takes several generations reach optimal value performance depends initialization. velocity update equation algorithm modiﬁed including function addition evaluation function restricts particle moving away actual course towards global best position. addition provides advantage computation. however also constrains figure ellipsoid particle point inside point boundary nearest point outside dotted lines connecting point points shown. desired direction movement also shown arrow required reach point constants random value varied iteration. value chosen term take particle outside ellipsoid. random point surface sphere dimension randomly varying radius. forms craziness term. velocity vector update equation term relating minimization modiﬁed includes direction minimization. improves convergence rate thus reduces computation time. arrow shown figure direction minimizing function given assumption point known shortest path particle swarms placed search space region point shortest path determined. order evaluate objective function need determine point region region sets particle swarms region placed within search spaces respective regions. objective function evaluated based particles present regions. every iteration best position particle region used known point shortest path velocity update equation particles region. objective function reaches optimal value several generations. process optimization particles often search space. limit particles within search space inspect every tentative position update whether particle lying within search space carrying check qcqp constraints. intended position particle going violate constraint position updated. words particles likely search space redeployed back previous positions. proposed solution used control system problems optimization sensor networks collaborative data mining based multiple agents gradient projection general consensus problem solved using proposed method multiple agents need reach common goal consensus distributed optimization discussed ‘consensus sharing’ using alternating direction method multipliers admom uses agent constraint. solvers used solving distributed format table presents pseudo code algorithm. parameters ﬁxed values randomly varying parameters updated iteration. position velocity personal best global best particle stored. maximum number iterations algorithm speciﬁed experiments size swarm used algorithm proposed algorithm implemented matlab. section diﬀerent optimization problems quadratic constraints solved using proposed algorithm. reliability algorithm tested linear quadratic programming problems. figure plot evaluated value number iterations diﬀerent algorithms problem given around iterations algorithms except karmarkar’s reach value close solution. observe karmarkar algorithm takes iterations reach solution since length direction vector decreases monotonically. solution problem problem solved using khachiyan’s ellipsoid algorithm karmarkar’s algorithm method. figure shows value function iteration typical independent experiment iterations. length vector karmarkar’s method scaled variable values used namely evaluation function. value increases algorithm reaches optimal value less number iterations. table shows error value function algorithms iteration shown figure error values ellipsoid methods less algorithm values variables values small vector addition position keeps particles within region value varied since scales neighboring search space particle. results diﬀerent values shown figure carried independent runs problem diﬀerent values average minimum maximum standard deviation error value tabulated table error values less compared indicating neighboring search space scaled higher value error value ﬁtness function becomes better fewer number iterations. several variants applied classiﬁcation problems generally deployed rule-based classiﬁcation. suitable classiﬁer chosen classiﬁcation; example neural network parameters like network weights tuned using particle swarms. discrete versions swarms take ﬁnite values here swarms learn rule classifying test samples. method binary classiﬁcation posed figure hyperplanes obtained neural networks method shown synthetic dataset classes. hyperplane estimated method closely aligned obtained linear kernel. hyperplanes arrived neural networks optimal. figure shows simulated data classes synthesized using gaussian distributions means covariance matrix generic decision boundary binary classiﬁcation problem hypersurface. classes data linearly separable thus reducing hypersurface hyperplane. equation hyperplane kind data given matlab programming platform used implement test method also neural network comparison. trained linear kernel neural network synthetic data. hyperplanes learnt neural network shown figure single layer perceptron algorithm epochs used training neural network algorithm used training linear kernel explain determination optimal hyperplane binary classiﬁcation method. values sample mean covariance classes calculated simulated samples. mahalanobis distance determined mean value class data points class closest data point class label found. closest point used reference boundary search space class. process repeated class boundary ﬁrst class also determined. boundaries ellipsoidal regions formed estimated means classes centers. posed qcqp problem formulated estimated covariance matrices normalized boundary points algorithm implemented placing particle swarms near mean value classes evaluating optimization function. shortest path closest point boundary estimated. optimized using proposed algorithm. intersection ellipsoidal regions assumed zero eliminate situation particles reach diﬀerent solutions. closest points boundaries determined perpendicular bisector line joining closest points hyperplane. calculated hyperplane binary classiﬁcation shown figure observe optimal hyperplane calculated algorithm method closely placed hyperplanes optimal. estimated weight bias values method tabulated table estimated weight values normalized thyroid. four datasets chosen based consideration minimal number datasets maximum coverage diﬀerent types attributes further number classes case maximum number hyperplanes obtained datasets limited three. main characteristics datasets tabulated table crossvalidation performance datasets compared linear kernel neural network gsvm. gsvm estimates classiﬁcation hyperplane similar proposed approach developed fundamentals optimizing problem. gsvm modiﬁes bias obtained moving hyperplane geometric mean recall precision improved. iris dataset consists four diﬀerent measurements samples iris ﬂower. samples three diﬀerent species iris forming dataset. features whose values available dataset length width leaves petals diﬀerent iris plants. three species linearly separable other whereas third linearly separable rest species. classiﬁcation task determine species iris plant given -dimensional feature vector. wine dataset contains diﬀerent physical chemical properties three diﬀerent types wines derived three diﬀerent strains plants. physical properties colour intensity integer values whereas chemical properties phenol content real values. feature vector dimensions total samples. classiﬁcation task determine type wine given values content thirteen physical chemical components. pima dataset contains eight diﬀerent parameters measured adult females pima indian heritage. again integer valued number pregnancies. certain parameters serum insulin real valued. two-class classiﬁcation problem identifying normal diabetic subjects given -dimensional feature vector input. dataset contains distinct databases diﬀerent dimensions. particular database chosen study contains diﬀerent parameters measured individuals. variables binary values others real values. classiﬁcation task assign individual classes given -dimensional feature vector input. eigen value decomposition performed dataset covariance matrix. dataset projected eigen vectors. scaling performed component projected dataset unit variance. step performed independently subsets used cross-validation stage. project subset samples dimensions. first direction vector joining sample means classes. equation vector projection vector sample means classes- respectively. second direction eigen vector corresponding largest eigen value covariance matrix subset. datasets separate training test samples; hence perform cross-validation. cross-validation small subset used testing remaining used training samples. fold cross-validation split datasets subsets testing others training time rotate subsets. equation used estimation hyperplane turn used classify test samples. trials performed average cross-validation errors reported table performance method close variants superior neural network. notice iris wine datasets cross-validation error obtained technique better achieved linear kernels also neural network. pima thyroid datasets cross-validation error high high degree correlation. developed classiﬁcation method optimization algorithm solving qcqp problems. novelty method application particle swarms evolutionary technique optimization. results indicate approach possible method solving general qcqp problems withgradient estimation. shown results algorithm quadratic constraints evaluating diﬀerent optimization functions. issue based methods computational complexity need parameter tuning. number function evaluations linearly increases number particles employed number iterations carried out. future intend learn multiple hyperplanes placing multiple kernels class evaluating performance multiple-kernel learning algorithms. hyperplanes estimated diﬀerent kernels reduce cross-validation error pima thyroid datasets.", "year": 2014}