{"title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "tag": ["cs.IR", "cs.CL", "cs.LG", "cs.NE", "stat.CO", "stat.ML"], "abstract": "The F-measure or F-score is one of the most commonly used single number measures in Information Retrieval, Natural Language Processing and Machine Learning, but it is based on a mistake, and the flawed assumptions render it unsuitable for use in most contexts! Fortunately, there are better alternatives.", "text": "david m.w. powers beijing university technology china flinders university australia technical report kit-- computer science engineering mathematics flinders university figure theoretic interpretation f-measure. stolen rijsbergen f-measure corresponds number items white intersection divided average size groups real predicted items. fallout since constant. indeed relationship quite clear precision tp/pp fallout fp/rn ppâ€“tp. however makes tradeoff relationship quite different although shannon information conveyed precision figure comparison receiver operating characteristics equal informedness isobars precision-recall arithmetic harmonic geometric mean isobars using multiclassifier fusion facial expression recognition cohn-kanade image set. true rate results shown anger disgust fear happiness sadness surprise based -fold cross validation showing number images real class. blue isobars equal cost equal score =constant prx). break-even lines also shown curves corresponding bias=prevalence informedness=kappa=correlation precision=recall=accuracy appropriate constraint variance similar positives negatives near decision boundary). appropriate linear logarithmic reciprocal scalings used permit isobars linear x-axis fallout precision y-axis reciprocal recall x-axis reciprocal precision prf. note classes chance recall precision without distributional data. therefore axes representing reciprocal recall precision truncate level represents better relative naive baseline chance level. average angle transition predicting positives predicting negatives threshold approximates default weighting measures across curves. curve smoother thus tuning costs seems appropriate prx. sharper elbow less tuning costs affect optimum. entwisle d.m.w. powers present statistics evaluation parsers joint conferences methods language processing computational natural language learning d.m.w. powers recall precision versus bookmaker international conference cognitive science pp.- expanded form flinders university technical report sie-- full paper d.m.w. powers evaluation precision recall f-measure informedness markedness correlation journal machine learning technologies international workshop inductive logic programming relevant context p.a. flach geometry space understanding machine learning metrics isometrics international conference machine learning", "year": 2015}