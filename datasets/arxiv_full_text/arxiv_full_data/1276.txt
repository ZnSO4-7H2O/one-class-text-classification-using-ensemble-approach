{"title": "On Lyapunov exponents and adversarial perturbation", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper, we would like to disseminate a serendipitous discovery involving Lyapunov exponents of a 1-D time series and their use in serving as a filtering defense tool against a specific kind of deep adversarial perturbation. To this end, we use the state-of-the-art CleverHans library to generate adversarial perturbations against a standard Convolutional Neural Network (CNN) architecture trained on the MNIST as well as the Fashion-MNIST datasets. We empirically demonstrate how the Lyapunov exponents computed on the flattened 1-D vector representations of the images served as highly discriminative features that could be to pre-classify images as adversarial or legitimate before feeding the image into the CNN for classification. We also explore the issue of possible false-alarms when the input images are noisy in a non-adversarial sense.", "text": "accompanying blog associated authors goodfellow papernot posit adversarial examples hard defend hard construct theoretical model adversarial example crafting process contend idea attacking machine learning easier defending background shall look closely speciﬁc type defense motivate relevance method within framework. prominent approach emerges literature adversarial defenses crafting pre-detection ﬁltering systems inputs might potentially adversarial. authors posit adversarial examples drawn distribution legitimate samples thus detected using statistical tests. authors train separate binary classiﬁer ﬁrst classify input image legitimate adversarial perform inference passed images. approaches authors assume dnns classify accurately near small manifold training data synthetic adversarial samples data manifold. applying dropout test time ascertain conﬁdence adversariality input image. paper would like disseminate model-agnostic approach towards adversarial defense dependent purely quasi-time-series statistics input images discovered rather serendipitous fashion. goal present method propose fool-proof adversarial ﬁlter instead draw attention communities towards chance discovery feel worthy inquiry. order facilitate reproducibility results eﬀective criticism duly open-sourced implementation well annotated jupyter-notebook shared following location https//github.com/vinayprabhu/lyapunov defense paper would like disseminate serendipitous discovery involving lyapunov exponents time series serving ﬁltering defense tool speciﬁc kind deep adversarial perturbation. state-of-the-art cleverhans library generate adversarial perturbations standard convolutional neural network architecture trained mnist well fashion-mnist datasets. empirically demonstrate lyapunov exponents computed ﬂattened vector representations images served highly discriminative features could pre-classify images adversarial legitimate feeding image classiﬁcation. also explore issue possible false-alarms input images noisy non-adversarial sense. recent past plethora defenses adversarial attacks proposed. include safetynet adversarial training label smoothing defensive distillation feature-squeezing name few. also ongoing kaggle contest underway exploring novel defenses adversarial attacks. evinced recent spurt papers written topic defenses proposed quelled novel attack exploits weakness defense. authors queried could concoct strong defense combining multiple defenses showed ensemble weak defenses suﬃcient providing strong defense adversarial examples able craft. dynamical system deﬁne lyapunov exponents corresponding large-time behavior system. characteristic exponents numerically quantify sensitivity initial conditions emanate ergodic theory diﬀerentiable dynamical systems introduced eckmann ruelle simply initial state time series slightly perturbed characteristic exponent represent exponential rate perturbation increases time. detailed understanding characteristic exponents would like refer user given ﬁnite-length ﬁnite-precision time series sequence stage embed-tangent maps-qr decomposition based recipe introduced used numerically compute lyapunov exponents. numerical recipe implemented many time-series analysis packages background serendipitous discovery investigating usage metrics time-series analysis literature identify adversarial attacks axis-wise mobile-phone motion sensor data ;example accelerometer gyroscope realized procedure could easily extended vectorized images notice change sign lyapunov exponents true image adversarial images. time-series analysis existence least positive lyapunov exponent interpreted strong indicator chaos. introduce adversarial noise prevalence positive lyapunov exponents becomes prevalent. contains example image associated adversarially perturbed counterparts lyapunov exponents. showcased scatter-plot ﬁrst lyapunov exponents clear clustering between legitimate examples adversarial examples. adversarial examples inside cluster legitimate examples target class true class. detection also performing viewpoint outlier detection train one-class classiﬁer inlier images’ lyapunov exponents used training. shows heatmap features train test sublegitimate adversarial). received attacker rejection rate true acceptance rate feature engineering hyper-parameter tuning. showcases scatterplot train test samples along decision boundaries found isolation forest algorithm. eﬀect noise-models image lyapunov exponents shown seen ’localvariance gaussian’ noise discernible eﬀect lyapunov exponents hence chosen model choice perturb legitimate images with. added training datasets noisy images re-train isoforest classiﬁer. results shown false alarm rate drops attacker rejection rate almost unchanged noise selected distribution norm distances among adversarial perturbations classiﬁer retains ability distinguish adversarially perturbed randomly perturbed images. previous sections used -class isolation forest classiﬁer learn inlier unmodiﬁed images. experiment test whether classiﬁer trained positive negative data generated using known adversarial attacks outperform -class classiﬁer. train logistic regression model unmodiﬁed mnist images data generated using untargeted attacks previous section. evaluate model validation consisting natural images images modiﬁed using left-out attack. logistic model able achieve near-perfect performance four attacks. model fails perform data generated using jsma achieving auroc score attacks model reaches auroc score exact auroc scores curves shown figure detect adversarial perturbations attacks carlini-wagner-l attack. consider fast gradient sign method jacobian saliency attack deepfool attack presented madry consider targeted untargeted versions attack applicable. default parameters cleverhans library wherever possible. cleverhans provide default value values referenced original paper describing attack. trained ﬁrst lyapunov exponents isolation forest poorly attacks. however accuracy improves signiﬁcantly train four-dimensional data. cases train using natural mnist images inlier set. true negative paper sought disseminate serendipitous discovery entailing usage lyapunov exponents model-agnostic tool used pre-ﬁlter input images potentially adversarially perturbed. shown validity idea defensing images adversarially perturbed using carlini-wager-l attack procedure across datasets namely mnist fashionmnist. used latest version cleverhans library open-sourced code ensure repeatability results presented here. currently investigating potential across various datasets attacks. learning-based gait biometric recognition adversarial perturbations. cvpr workshop bright dark sides computer vision challenges opportunities privacy security", "year": 2018}