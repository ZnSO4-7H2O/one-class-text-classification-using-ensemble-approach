{"title": "A Novel Model of Working Set Selection for SMO Decomposition Methods", "tag": ["cs.LG", "cs.AI"], "abstract": "In the process of training Support Vector Machines (SVMs) by decomposition methods, working set selection is an important technique, and some exciting schemes were employed into this field. To improve working set selection, we propose a new model for working set selection in sequential minimal optimization (SMO) decomposition methods. In this model, it selects B as working set without reselection. Some properties are given by simple proof, and experiments demonstrate that the proposed method is in general faster than existing methods.", "text": "abstract— process training support vector machines decomposition methods working selection important technique exciting schemes employed ﬁeld. improve working selection propose model working selection sequential minimal optimization decomposition methods. model selects working without reselection. properties given simple proof experiments demonstrate proposed method general faster existing methods. huge interest support vector machines excellent generalization performance wide range problems. work training svms solve follow quadratic optimization problem. slow convergence. better method working selection reduce number iterations hence important research issue. methods proposed solve problem reduce time training svms paper propose model select working set. model specially selects without reselection. another word selected tested selected following working selection. experiments demonstrate model general faster existing methods. paper organized following. section give literature review decomposition method existing working selection discussed. method working selection presented section iii. section experiments corresponding analysis given. finally section concludes paper. notable effects taken training svms unlike optimization methods update whole vector iteration decomposition method modiﬁes subset iteration. iteration variable indices split \"working set\" complement then subproblem variables solved thereby leaving values remaining variables unchanged. method leads small sub-problem minimized iteration. extreme case sequential minimal optimization restricts working elements. comparative tests algorithms done platt indicates often much faster better scaling properties. sequential minimal optimization proposed platt extreme case decomposition algorithm size working restricted two. method named algorithm keerthi improved performance algorithm training svms take account situation kernel matrices non-positive deﬁnite pai-hsuen chen introduce algorithm restrict proof. fig. illustration frequency reselection. using dataset kernel method rbf. abscissa indicates indices ordinate indicates number times certain picked training process ﬁrst phenomenon lots selected training process. using \"shrinking\" samples \"shrunk\" training procedure thus never selected optimized. time notice another interesting phenomenon several selected optimize problem again others remain untouched. kind reselection necessary? investigating phenomena above limit reselection could effectively reduce time training svms acceptable effect ability generalization. thus propose method working selection certain selected once. optimization problem method described following iteration selected method considers accurate second order information available optimization sample words working selected optimized samples examined anymore following selection. relationship sets shown fig. currently popular select working \"maximal violating pair\" call short. working selection ﬁrst proposed used example software libsvm instead using ﬁrst order approximation used method consider accurate second order information proposed call using check possible decide experiments indicate full check reduce iterations using much butfor linear kernel sometimes positive semideﬁnite possible +kjj −kij moreover existing kernel functions inner product vectors even positive semi-deﬁnite. occur proof firstly selected once means chosen optimization value never modiﬁed before; secondly initialized beginning algorithm. thus values selected proof firstly algorithm terminates sample left certain optimization conditions reached; secondly samples selected deleted available set. thus worst situation iterations samples left active algorithm terminates. including nine binary classiﬁcation regression problems investigated various settings. large classiﬁcation problems also taken account. select splice delve archive problems german.numer heart australian statlog collection problems fourclass transformed two-class set. datasets diabetes breast-cancer machine learning repository dataset problems compiled platt \"adult\" dataset. problems also platt problem ijcnn ﬁrst problem ijcnn challenge different svms parameters kernel parameters affect training time difﬁcult evaluate methods every parameter setting. fair comparison experimental procedure rong-en used without shrinking. different cache size first cache allows whole kernel matrix stored computer memory. second allocate memory cache miss happen kernel evaluations needed. second setting simulates training large-scale sets whose kernel matrices cannot stored. comparison cross validation accuracy classiﬁcation first grid method applied. cross validation accuracy compared \"parameters selection\" \"ﬁnal training\". test various situations concerning commonly used kernels without shrinking technique mb/kb cache. table show cross validation accuracy wss-wr method almost same. speciﬁcally |accuracywss-wr accuracywss datasets accuracy outperform wss-wr. also several datasets wss-wr performs even accurate besides great improvement made number iterations well consumption time. kernel give ﬁgures showing results \"parameter selection\" \"ﬁnal training\" steps respectively. separate ﬁgure situations without/with shrinking present ratios using wss-wr using number iterations independent cache size. \"parameter selection\" step time parameters summed calculating ratio. general \"ﬁnal training\" step fast timing result accurate. hence repeat step times obtain reliable timing values. fig. present obtain ratios general less conclude using wss-wr general better using caching shrinking techniques wss-wr according analysis experimental results wss-wr model caching shrinking techniques much effects decomposition method. data table viii certify conclusion. experiments large classiﬁcation datasets next experiment large classiﬁcation sets handled similar procedure. parameter selection time consuming adjust \"parameter selection\" procedure -point searching. cache size experiments employ sigmoid kernel methods along sigmoid kernel general leads worst ratio wss-wr outperforms datasets \"parameter selection\" step \"ﬁnal training\" steps. unlike training time wss-wr increase amount memory caching drops. property indicates wss-wr useful situation datasets large kernel matrices stored enough memory. shrinking technique libsvm introduced pai-hsuen chen make decomposition method faster. view table viii experiments regression large classiﬁcation problems shrinking technique almost shorten training time using wss-wr. wss-wr table gives iteration time ratios. comparing results among small problems safely draw conclusion ratios time iteration large datasets less small ones especially situation small size cache. analysis choose datasets breast-cancer made evaluations using diversity kernel methods fig. compare convergence wss-wr illustrations breast-cancer omitted short. fig. convergence rates wss-wr exactly beginning procedure. addition wss-wr terminated soon reaches optimum contrary hold objective value make insigniﬁcant progress much time consumed. thus reasonable conclude wsswr much efﬁcient. analyzing available working selection methods interesting phenomena proposed working selection model–working selection without reselection subsequently full-scale experiments given demonstrate wss-wr outperforms almost datasets \"parameters selection\" \"ﬁnal training\" step. then discussed features model analyzing results experiments. theoretical study convergence wss-wr continually improve model future work. work support national high technology research development program china grant no.aaz. thanks chihchung chang chih-jen powerful software libsvm. joachims making large-scale support vector machine learning practical advances kernel methods support vector machines schölkopf burges press cambridge platt fast training support vector machines using sequential minimal optimization advances kernel methods support vector learning bernhard schölkopf christopher burges press cambridge j.c.platt using sparseness analytic speed training support vector machines advances neural information processing systems m.s.kearns d.a.cohn eds. press cambridge michie spiegelhalter taylor machine learning neural statistical classiﬁcation prentice data available http//www.ncc.up.pt/liacc/ml/statlog/datasets.html.", "year": 2007}