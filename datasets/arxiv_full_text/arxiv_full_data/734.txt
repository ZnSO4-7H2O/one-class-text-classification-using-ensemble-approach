{"title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream  Data", "tag": ["cs.AI", "cs.LG", "cs.NE", "68T05", "I.2.6"], "abstract": "In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels.  We verified our method in extended MNIST data set and not MNIST data set. We set the experiment to simulate virtual drift, real drift, and hybrid drift event and we demonstrated how our CNNELM adaptability works. Our proposed method works well and gives better accuracy, computation scalability, and concept drifts adaptability compared to the regular ELM and CNN. Further researches are still required to study the optimum parameters and to use more varied image data set.", "text": "data data continuously generated distribution keep changes overtime. challenges online stream data known concept drift. paper proposed adaptive convolutional method enhancement convolutional neural network hybrid extreme learning machine model plus adaptive capability. method aimed concept drift handling. enhanced convolutional hiererchical features representation learner combined elastic parallel supervised classifier. propose adaptive os-elm concept drift adaptability classifier level matrices concatenation ensembles concept drift adaptability ensemble level proposed adaptive cnnelm flexible works well classifier level ensemble level current methods proposed work either levels. experiment simulate virtual drift real drift hybrid drift event demonstrated cnnelm adaptability works. proposed method works well gives better accuracy computation scalability concept drifts adaptability compared regular cnn. researches still required study optimum parameters varied image data set. online data stream learning emerging research area shown importance data era. volumes data continuously generated devices softwares internet higher incoming rate time bound. knowledge mining needs special machine learning techniques learn large volumes data timely fashion. techniques also need scalable deployment online real-time scenario capable overcome uncertainty data representations. problem concept drift input and/or output concepts stationary uncertain data distribution. uncertainties perceived increase class overlapping additional comprehensive features feature space makes deterioration classifiers. occurs. common handling methods based classifier ensemble ensemble methods combined decision classifier members however ensemble methods difficult manage complexities handling many types consecutive drifts techniques offer promising avenue automated feature extraction data streaming approaches. deep learning many variants deep belief network hinton deep boltzmann machine salakhutdinov hinton stacked denoising autoencoders vincent convolutional neural network lecun many others. traditional machine learning methods i.e. extreme learning machine support vector machine multi-layer perceptron neural network hidden markov model able handle stream data directly although worked successfully classification problem many areas. shallow methods need good feature representation assume data available. feature engineering focused constructing features essential element machine learning. however data rapidly growing within dynamic assumptions handy-crafted feature engineering difficult traditional methods need modified. popular approach combines deep learning method unsupervised feature representation learner supervised classifier traditional machine learning methods. handle concept drift online stream data. named adaptive cnn-elm studied acnnelm scheme concept drift either changes number feature inputs named virtual drift number classes named real drift consecutive drift occurred time named hybrid drift recurrent context named adaptive os-elm works single classifier handling single classifier aos-elm combines simultaneously many strategies solve many types simple platform. section section describes proposed methods. focus empirical experiments prove methods mnist not-mnist image classification task section section discusses conclusions challenges future directions. subset input data time initialization stage. xx...x subset input data next sequential time. subset different number quantity. corresponding label data subscript font shows drift type. i.e. concept virtual drift event replaced concept symbolized concept real drift event replaced concept concept recurrently shuffled composition symbolized extreme learning machine works based generalized pseudoinverse iterative learning single hidden layer feedforward neural network architecture compared neural networks including used random value parameters. however used fixed random value hidden nodes parameters used iterative generalized pseudoinverse optimization process used iterative gradient descent optimization process smooth weight parameters. fast accurate online sequential named online sequential extreme learning machine sequential learning phase. sequential learning computed previous function. filled initial training data filled incremental training data output weights approximated solving another approach elastic extreme learning machine parallel based mapreduce framework solve large sequential training data parallel way. first transform process intermediate matrix multiplications training data portion. second reduce aggregate process result. finally corresponding output weights obtained centralized computing using result reduce/aggregate process. therefore learning efficient rapidly massive training data computation os-elm address possibility hidden nodes increased training. os-elm ceos-elm parallel method address concept drift issues; i.e. number attributes number classes data changed. categorized os-elm ceos-elm parallel non-adaptive sequential elm. paper developed parallelization framework integrate different slfn consists convolution sub-sampling layers feed forward architecture. first successful deep architecture keep characteristics traditional excellent performance spatial visual classification benefit comparing another deep learning methods using fewer parameters layer followed fully connected standard multilayer neural network many variants architectures literature basic common building blocks convolutional layer pooling layer fully connected layer larger inputs). additive bias activation function applied feature either pooling layer. layer densely connected layers supervised learning learning errors propagated back previous layers using optimization method finally update kernel weight parameters bias. getting beneficial hardware parallel implementation krizhevsky showed large deep capable achieving record breaking results highly challenging dataset using purely supervised learning. however network size still limited mainly amount memory available current gpus huang explained theories valid fully connected architecture also actually valid local connections named local receptive fields similar kernel term. huang proposed elm-lrf connections input layer hidden nodes randomly generated following continuous different types probability distributions. according huang random convolutional hidden nodes type local receptive fields. different type hidden nodes keeps essence iterative output weights calculation. convolution layers pooling layers high level features abstraction input images. then abstracted features classified classifier. pang sequential learning approach. results give dc-elm compared elm-lrf mnist regular samples training data. handwritten digit recognition. used automatic feature extractor replace original classification layer cnn. cnn-elm achieved error rate lower alone. trained original converged. last layer replaced complete classification without iteration. experiments used regular mnist data proposed integration os-elm. learning method called orthogonal bipolar vector also well applied analyzing neural networks learned combining algorithms. experimental results demonstrate proposed method conduct network learning faster rate conventional cnn. addition used solve local minima overfitting problems. experiment used norb dataset mnist handwritten digit standard dataset cifar- dataset. recognition rate mnist testing data virtual drift refers changes distribution incoming data changes). changes incomplete partial feature representation current data distribution. trained model built additional data environment without overlapping actual class boundaries. combined using form voting ensemble approach integrate results individual classifiers unified predicted result improve accuracy robustness single classifiers proposed general hybrid adaptive ensemble learning framework proposed ensemble based uses cross-validation scheme build classifiers ensemble. different certain requirement needs replace entirely outdated concept either sudden gradually another requirement needs handle concepts come alternately thus hard combine simultaneously many types requirement complex drift solutions hybrid drift simple ensemble platform. adaptive ensemble approach practical flexible member designed adaptive need recall previous training data moreover another simple approach using single classifier named meta-cognitive os-elm developed based weighted os-elm mos-elm used additional weighting matrix control adaptivity however works concept replacement only. according interpolation theory point view learning principle theory input weight bias hidden nodes parameters independent training samples learning environment randomization. independence initial training stage also sequential training stages. thus adjust input weight bias pair according universal approximation theory inspired related works aos-elm real drift capability modifying output matrix zero block matrix concatenation change size matrix dimension without changing norm value. zero block matrix means previous knowledge concept. approximate complex decision boundary long output weights kept minimum number output classes increased. hybrid drift consecutive drifts scenario occur drift event. scenario requires modification hidden nodes parameters scenario requires modification output weight modifications independence thus modified event. grachten proposed adapting strategies single classifier system based common adaptive approaches reuse reuse model upon change task replace task training data task training data. reset ignore previous representations learned begin task learning randomly initialized model. further grachten categorized follows. figure schematic overview grachten al.’s experiment single data set; training methods grey rounded boxes represent models circles data instances document shapes evaluation method white rounded expansion based average system error training recognition rate performance. incremental learning training data handled branches addition original network keep unchanged. acnn used structure global expansion meet average error criteria local expansion expand network structure. zhang used face database. model recognition rate increased using local expansion. however zhang discuss concept drift handling. i.e. number layers number feature maps layer. researchers constructed compared candidate performance best one. studies tried hardware acceleration speed performance comparison discovery. however zhang discuss concept drift handling. used idea global expansion proposed method. deployed integration architecture becomes models acnnelm- acnnelm model works based aos-elm concept drift adaptability classifier level. acnnelm- model combines acnnelms aggregated matrices concatenation ensemble handle concept drift adaptability ensemble level. acnnelm- used matrices adjustment padded using zero block matrices recomputing acnnelm- enhanced model matrices concatenation aggregation result individual model let’s additional features concept assigned model concatenated together last layer models single matrix compute larger column size previous used method change needs adjusted also need adjust previous padding zero block matrix let’s additional output classes expansion concept. thus need modify layer adjusting column dimension matrix zero block matrix column adjustment only. idea based matrix multiplication actually decomposable. used re-compose members ensemble seems elm. idea needs minimum members need additional multi-classifier strategies enhanced concatenation concept handling. let’s additional features concept assigned cnn-elm hybrid model concatenated learning result without disturbing cnn-elm hybrid model. using simple model flexibilities reuse reset strategy recurrent sudden concept drift handling. dataset successful research simulate stream data. mnist common data data machine learning fact accepted standard give excellent result. mnist data balanced data contains numeric divided examples training data separated examples testing data however simulate stream data regular mnist adequate. reason developed extended mnist data larger training examples adding types image noises extended mnist data finally examples training data examples testing data. not-mnist harder task mnist. not-mnist dataset foolish images bulatov explained logistic regression stacked autoencoder fine-tuning gets accuracy whereas approach gives divided numeric alphabet symbol data including many foolish images. challenge not-mnist numeric not-mnist alphabet many similarities class class class class another similar foolish images. used single precision double precision computation paper. double precision accuracy improvement matrix inverse computation single precision. matlab using double precision default better accuracy single precision. however using single precision memory saving especially parallelization stream data. unfortunately papers mention precision method used. want achieve accuracy improvement precision computation factor. benchmark compared accuracy performance adaptive os-elm. guarantee performance sequential training data unless find best hidden nodes parameter regularization scalar using double precision extended mnist testing accuracy using single precision experiment not-mnist numeric testing accuracy using single precision experiment guarantee larger hidden nodes gives better accuracy performance using single precision. better using larger number epoch. better scalability sequential learning data needs longer time iteration improve performance epoch= testing accuracy learning time perspective time iterations equivalent build models individual sequentially. experiment studied concatenation ensemble concept better performance standalone model retain performance well even standalone model performance seems decreased. using c-s-c-s nodes model used hidden nodes elm. multiplicated models hidden nodes compared hidden nodes regular elm. experiment verified handling method acnnelm-. first used single model trained mnist concept. model trained next mnist concept. build additional attributes. last layers combined modify layer. experiment verified handling method acnnelm-. first picked previous cnn-elm models mnist concept. then mnist concept build cnn-elm models using additional c-s-c-s kernel size= hidden nodes concatenated models concatenation ensemble compared current previous performance cnn-elm model dependencies another model tested models classes testing data set. experiment verified handling method acnnelm-. first used single model trained mnist concept. model continued next mnist mnist concept without building model. repeat experiment not-mnist data set. experiment verified handling method acnnelm-. first build acnnelm- models mnist concept classes then mnist concept build cnn-elm models using classes concatenated models concatenation ensemble compared current previous performance matrices concatenation need adjust model model experiment verified handling method acnnelm-. first used single model trained mnist concept. model continued training next mnist concept time building model additional attributes. proposed method gives better adaptive capability classifier level ensemble level acnnelm- better computation scalability performance accuracy acnnelm- result aggregation ensemble benefit. iterations random weight kernel assignment error backpropagation optimization decay parameters larger layers larger feature dimension. also need investigate implement spatial recognition i.e. posed based human action recognition gradient descent optimum kernel weight dropout dropconnect regularization decay parameters. improve performance believe optimum parameters also work well cnnelm.", "year": 2016}