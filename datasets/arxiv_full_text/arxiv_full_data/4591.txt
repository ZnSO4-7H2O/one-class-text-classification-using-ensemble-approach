{"title": "Incorporating Expressive Graphical Models in Variational Approximations:  Chain-Graphs and Hidden Variables", "tag": ["cs.AI", "cs.LG"], "abstract": "Global variational approximation methods in graphical models allow efficient approximate inference of complex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture models. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are chain graphs, which capture distributions that are partially directed. The second class of models are directed graphs (Bayesian networks) with additional latent variables. Both classes allow representation of multi-variable dependencies that cannot be easily represented within a Bayesian network.", "text": "global variational approximation models allow efficient approximate plex posterior distributions choice approximating tradeoff complexity procedure paper consider variational classes models richer standard bayesian networks els. such classes allow find better tradeoffs spectrum approximations. models elwin graphs capture distributions partially directed tional latent variables. tion multi-variable represented tiona approximation. substruc­ terior distribution tures random variables. idea generalized various factored forms bayesian networks markov networks jaakkola jordan plore another direction improving mean field proximation. approximations teriors. structured mixture approximation trade-off accuracy computational structured adding structure increase improve methods order achieve greater accuracy given available computational approx­ imation results enhance range approximating butions increase ability trade-off accuracy complexity. central task using probabilistic ference. model decompose task. general problem np-hard structures bounded tree width) allow efficient inference. model intractable variational rithms attempt approximate observation remaining variables tractable define lower bound likelihood parameters define adapted trying maxi­ mize lower bound. imations. current structured networks markov networks approximating tions. classes models different expres­ sive power. provide uniform treatment examining chain graphs expressive includes ative procedure updates parameters update parame­ iteration. monotonic increase ters according lower bound local maximum. every consequence parents concave function assignment parameters {bx;ju; dom}. there­ fore stationary parameters. follows fact second order partial derivatives determined ilies complexity important probabilities terms equation need computed. need consider conditional given independent denoted values bayesian network determine independencies using separation i.e. terms form ignored update equations since change parameters constant factor absorbed zu;. therefore reduce amount computations factors depend given follows chain graph approximations well known classes distributions represented markov networks bayesian net­ works equivalent. best tractable bayesian networks distributions proximation ibility choosing approximating general class probability ture dependency markov networks dependency tured neither them. consider bayesian network. form posterior concrete example consider network fig­ when observe value create pendencies rior distribution network represented chain graph before however since depend value constant thus absorbed normalizing general structure solution remains similar simpler case bayesian networks analogue lemma write chain-graph similar form includes additional terms. case terms bayesian network easily identify depend value focus computation these. straightforward ideas bayesian networks omit details. adding hidden variables structured improving mean field approximation. jaakkola jordan proposed another improving mean field approximation mixture component represented tribution. emerges fact many cases posterior tribution i.e. several distinct gions domain distribution relatively high probability distribution mean field approximation mode. mixture distribution lower bound likelihood tion unfortunately ward manner would help since extra hidden vari­ ables introduces leave opti­ mization original overcame problem introducing transformation likelihood imations work figure recall structured network modeled approximating network three independent presented here correlations hidden variables. variable time slices maintained edges within time slice maintained preserve tion first term average without ment arises second term. given structure approximating network without lower bound improved ferent configurations defined achieve mal. using extra hidden variable configuration ofthe mutual information examples evaluate synthetic works general variables trolled panded rameters hyperparameter skewed distributions. lihood observation repeated works sampled combination structured represent mixture type considered ponents imation iterations seems con­ verge runs. avoid local maxima tried different best scoring comparison shows simple mean field methods. mixtures mean field improve still ponents tions network examples light differences xample differences acknowledgements thank elidan tommy kaplan iftach nachman matan ninio anonymous referees part earlier israeli friedman also supported harry sherman senior lectureship science. experiments reported funded basic equipment grant.", "year": 2013}