{"title": "Simultaneous Dempster-Shafer clustering and gradual determination of  number of clusters using a neural network structure", "tag": ["cs.AI", "cs.NE", "I.2.3; I.2.6; I.5.3"], "abstract": "In this paper we extend an earlier result within Dempster-Shafer theory [\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc. Seventh Int. Conf. Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU'98)] where several pieces of evidence were clustered into a fixed number of clusters using a neural structure. This was done by minimizing a metaconflict function. We now develop a method for simultaneous clustering and determination of number of clusters during iteration in the neural structure. We let the output signals of neurons represent the degree to which a pieces of evidence belong to a corresponding cluster. From these we derive a probability distribution regarding the number of clusters, which gradually during the iteration is transformed into a determination of number of clusters. This gradual determination is fed back into the neural structure at each iteration to influence the clustering process.", "text": "paper extend earlier result within dempstershafer theory several pieces evidence clustered ﬁxed number clusters using neural structure. done minimizing metaconﬂict function. develop method simultaneous clustering determination number clusters iteration neural structure. output signals neurons represent degree corresponding probability distribution regarding number clusters gradually iteration transformed determination number clusters. gradual determination back neural structure iteration inﬂuence clustering process. paper develop neural network structure simultaneous clustering evidence within dempstershafer theory gradual determination number clusters. clustering done minimizing metaconﬂict function. studied problem concerns situation reasoning multiple events handled independently. clustering process separate evidence clusters handled separately. earlier paper developed method based clustering neural network structure ﬁxed number clusters. used structure neural network learning done weights network. instead weights directly method using conﬂict dempster’s rule input. clustering computational complexity compared previous method based iterative optimization although clustering performance equally good. order improve clustering performance hybrid methods also developed here idea gradual determination number clusters developed integrated neural structure clustering process. methodology developed ﬁnding posterior probability distribution concerned number clusters. based ﬁnal clustering result iterative optimization metaconﬂict function partial speciﬁcations nonspeciﬁc pieces evidence uncertain respect events referring here approach expanded include gradual determination number clusters. instead using ﬁnal clustering result partial speciﬁcations incremental clustering states iteration neural network output signal neuron represent degree piece evidence belongs corresponding cluster. yields probability distribution number clusters. using change entropy output signals neural structure iteration gradually transform probability distribution determination number clusters. result gradual determination back neural network iteration influence clustering process. early stages iteration number clusters vary slowly change gradual determination number clusters changes conflict clusters. latter stages iteration converges fixed number clusters. section describe problem hand section continue neural structure simultaneous clustering gradual determination number clusters. first describe approach clustering neural structure. secondly focus gradual determination number clusters structure simultaneously computational clustering performance investigated section finally section draw conclusions. receive several pieces evidence different separate events pieces evidence mixed want arrange according event referring thus partition pieces evidence subsets subset refers multiple events. metaconflict derived plausibility partitioning correct conflict viewed piece metalevel evidence partitioning evidence subsets definition. metaconﬂict function since elements different subsets frame always partition clusters global minimum metaconﬂict function equal zero. evidence –element remaining elements –element etc. zero conﬂict every cluster. reason choose problem minimum metaconﬂict zero makes good test example evaluating performance. minimizing metaconﬂict function using neural structure choose architecture minimizes sum. thus make change function want minimize. take logarithm minus metaconﬂict function change minimizing minimizing sum. change minimization follows becomes piece evidence metalevel. particular event. ﬁgure subsets denoted conﬂict pieces evidence combined dempster’s rule denoted here thirteen pieces evidence partitioned four subsets. number subsets uncertain also domain conﬂict conﬂict current hypothesis number subsets prior belief. partition simply allocation pieces evidence different events. since events anything other analyze separately. uncertain event pieces evidence referring problem. could impossible know directly different pieces evidence referring event. know subset not. problem problem organization. evidence different events want analyze unfortunately mixed facing problem separating them. create additional piece evidence subset proposition adequate partition. simple frame discernment metalevel {adp adp} short adequate partition. proposition take value equal conﬂict combination within subset pieces evidence regarding subset reason partition original evidence. confuse original evidence call evidence metalevel evidence combination take place metalevel ﬁgure iteration iteration sixteen different states neural network neurons. left right convergence clustering pieces evidence unknown number clusters every fourth iteration. snapshot iteration columns represent possible cluster rows represent piece evidence. linear dimension square proportional output voltage neuron represent degree piece evidence belongs cluster. ﬁnal state output voltage output voltages piece evidence represented clustered cluster output voltage study calculations taking place neural network iteration. terminology hopﬁeld tank input voltages weighted input signals neuron output voltages output signal neuron inhibition terms negative weights. previous input voltage previous iteration plus gain factor times terms. ﬁrst term output voltages neurons column weighted data-term inhibition times weight conflict −log plus global inhibition second term output voltages neurons weighted row-inhibition global gradual determination number clusters. neuron column gradual determination factor weighted domain-inhibition plus global inhibition. last terms excitation bias minus previous input voltage nmn. entire different minimal number clusters iteration. note longer talking individual clusters derived support regarding actual number clusters. newly created pieces evidence combined prior probability distribution problem speciﬁcation. domain dependent distribution. without prior probability distribution nothing hold clustering together could many clusters pieces evidence allowed neural structure. trials chosen distribution constant. thus viewing piece evidence subset support existence subset able derive posterior probability distribution concerned question many subsets are. ﬁgure plot projected iterationprobability axes. notice initial drop alternatives especially clusters well early rise three four cluster alternatives. iterations cluster alternative still preferable choice quickly preferable choice becomes four clusters. trying cluster evidence four making gradual determination number clusters process several steps. first idea piece evidence subset supports existence subset degree piece evidence supports anything iteration uncertain cluster pieces evidence belongs. therefore every piece evidence cluster weighted output voltage cluster. weighted pieces evidence cluster combined. degree result combination supports anything entire frame degree pieces evidence taken together supports existence thus calculated possible clusters iterations. problem ﬁgure notice basic probability clusters drops fast ﬁrst iterations remains remainder iterations. also basic probability cluster drops initially comes back iteration process ﬁnish high. create type evidence exchanging propositions previous ones conjunctions terms proposition type evidence form here probability conjunctions length previous pieces evidence awarded focal element piece evidence supports proposition ﬁgure gradual determination posterior probability distribution ﬁgures plotted. ﬁrst iteration identical gradually determination takes place ﬁnal iteration exact determination clusters made. ﬁgure observe even though exact determination clusters take place ﬁnal iteration situation pretty clear long before. iterations respectively giving clustering comparison done using previous described pieces evidence different random basic probability assignments runs. ﬁgure metaconﬂict iterations runs resulting clusters shown dashed line. ﬁgure conﬂict cluster shown. notice tendency conﬂict drops cluster time. quickly conﬂict drops cluster ﬁve. initially high conﬂict cluster also drops within ﬁrst iterations. remainder iteration concerns mainly clusters three four. cluster three holds middle iteration towards remaining conﬂict situated cluster one. clusters internal conﬂicts become large necessary reclaim ﬁfth cluster. observed dramatic shift around iteration. shift remains preferable throughout remainder iteration. sooner later becomes necessary determine many clusters are. basis making determination probability distribution ﬁgure obvious choice highest probability decide ﬁnal iteration must avoid making early determination. seen ﬁgure early choice based maximum probability likely false. however neither wait ﬁnal iteration determine number clusters. must give neural iteration good opportunity converge right problem. idea accept probability distribution ﬁgure make gradual determination number clusters throughout iterative process ﬁnal last iteration. measuring traveled initial state ﬁnal state convergence reached. initially output voltages neurons scattered maximally among different choices ﬁnal state scattering all. normalized shannon entropy thus good measure traveled towards ﬁnal state entropy zero ﬁnal state. entropy calculated pieces evidence clusters runs clustering pieces evidence unknown number clusters four runs ended using clusters runs used clusters used four clusters. clusters runs result average clustering performance. number clusters ﬁxed would seen higher average conﬂict runs. here interpreted need additional cluster. since propositions evidence different subsets frame never possible conﬂict free partitioning evidence four clusters. reason partition occurred once basic probability assignments attached propositions random number propositions conﬂict assignments close zero numeric conﬂict still small number might found acceptable compared additional cluster. assignments could never four clusters test. measure different number subsets. poor clustering might yield clusters good lower conﬂict cluster. reason compare performance runs resulted clusters clustering unknown number clusters clustering known clusters. four resulting clusters. considered four best runs ten. compare four best runs clustering known clusters. also compare four runs random assignments number clusters known table expected conﬂict approach higher actual numbers quite small. best criteria good clustering performance conﬂict cluster piece evidence. tabulated table example mean conﬂict cluster piece evidence case clustering unknown number clusters. compared average numeric conﬂict conﬂicting pieces evidence. demonstrated possible neural network structure perform simultaneous clustering dempster-shafer evidence gradual determination number clusters unknown. found computational clustering performance almost good clustering ﬁxed number clusters. approach advantageous since clustering process performed. fast dempster-shafer clustering using neural network structure proc. seventh int. conf. information uncertainty knowledge-based systems université sorbonne paris france july editions paris schubert cluster-based speciﬁcation techniques dempster-shafer theory symbolic quantitative approaches reasoning uncertainty proc. european university conf. fribourg switzerland july springer-verlag berlin schubert neural network iterative optimization hybrid dempster-shafer clustering proc. eurofusion int. conf. data fusion great malvern october d.w. computation decisions biol. cybern. bergsten svensson schubert applying data mining machine learning techniques submarine intelligence analysis proc. third int. conf. data mining newport beach august aaai press menlo park schubert cluster-based speciﬁcation techniques dempster-shafer theory evidential intelligence analysis multiple target tracks ph.d. thesis trita− royal institute technology sweden isrn kth/na/r−−/−−se issn isbn −−−. schubert cluster-based speciﬁcation techniques dempster-shafer theory evidential intelligence analysis multiple target tracks communications", "year": 2003}