{"title": "Dualing GANs", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its maximin formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this 'dualing GAN' act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.", "text": "generative adversarial nets promising technique modeling distribution samples. however well known training suffers instability nature maximin formulation. paper explore ways tackle instability problem dualizing discriminator. start linear discriminators case conjugate duality provides mechanism reformulate saddle point objective maximization problem generator discriminator ‘dualing gan’ concert. demonstrate extend intuition non-linear formulations. gans linear discriminators approach able remove instability training gans nonlinear discriminators approach provides alternative commonly used training algorithm. generative adversarial nets among others like variational auto-encoders auto-regressive models promising technique modeling distribution samples. empirical evidence shows gans able learn generate images good visual quality unprecedented resolution recently research interest gans better understand properties training process. training gans viewed duel discriminator generator. players instantiated deep nets. generator required produce realistic-looking samples cannot differentiated real data discriminator. turn discriminator good possible tell samples apart real data. complexity optimization problem training gans notoriously hard usually suffers problems mode collapse vanishing gradient divergence. training procedures unstable sensitive hyperparameters. number techniques proposed address issues empirically justiﬁed theoretically motivated tremendous amount recent work together wide variety heuristics applied practitioners indicates many questions regarding properties gans still unanswered. work provide another perspective properties gans aiming toward better training algorithms cases. study paper motivated alternating gradient update discriminator generator employed training gans. form update source instability known diverge even simple problems ideally discriminator optimized optimality objective deterministic function generator. case optimization problem would much easier solve. motivates idea dualize parts objective offering mechanism better optimize discriminator. interestingly dual formulation provides direct relationship objective maximum mean-discrepancy framework discussed restricted linear discriminators optimal discriminator solving dual formulation permits derivation optimization algorithm monotonically increases objective. moreover non-linear discriminators apply trust-region type optimization techniques obtain accurate discriminators. work brings table additional optimization techniques beyond stochastic gradient descent; hope encourages researchers pursue direction. generative training interested modeling sampling unknown distribution given datapoints example images. gans generator network parameterized maps samples drawn simple distribution e.g. gaussian uniform samples data space separate discriminator parameterized maps point data space probability real sample. discriminator trained minimize classiﬁcation loss typically cross-entropy generator trained maximize loss. sets real data samples noise samples using cross-entropy loss results following joint optimization problem adhere formulation ﬁxed batch samples clarity presentation also point process adapted stochastic optimization setting later paper well supplementary material. solve maximin optimization problem ideally want solve optimal discriminator parameters argminw case program given reformulated maximization using maxθ however typical training alternates gradient updates usually step round. case objective maximized generator instead. objective always upper bound correct objective since optimal maximizing upper bound guarantee maximizing correct objective leads instability. therefore many practically useful techniques proposed circumvent difﬁculties original program deﬁnition presented log)) update order avoid vanishing gradients early stages training discriminator strong. technique combined approach follows keep elegant formulation program speciﬁed main idea ‘dualing gans’ represent discriminator program minw using dual maxλ hereby dual objective w.r.t. dual variables. instead gradient descent update solve dual instead. results maximization problem maxθ maxλ using dual beneﬁcial reasons. first note lower bound objective optimal discriminator parameters staying dual domain guaranteed optimization w.r.t. makes progress terms original program. second dual problem usually involves much smaller number variables therefore solved much easily primal formulation. provides opportunities obtain accurate estimates discriminator parameters turn beneﬁcial stabilizing learning generator parameters following start studying linear discriminators extending technique training non-linear discriminators. also cross-entropy classiﬁcation loss emphasize convex loss functions e.g. hinge-loss applied equivalently. linear discriminator start linear discriminators linear scoring function i.e. discriminator /x)]. here indicates real data generated sample characterizes probability generated versus real data sample. require scoring function linear differentiable features used place formulation. substituting linear scoring function objective given results following program also added l-norm regularizer note program presented convex discriminator parameters hence equivalently solve dual domain discussed following claim proof provided supplementary material. claim dual program task given reads follows remarks intuitively considering last terms program given claim well constraints assigning weights close half many data points many artiﬁcial samples possible. carefully investigating ﬁrst part generated λzigθ. note resembles moment matching property obtained maximum likelihood models. importantly objective also resembles maximum mean discrepancy framework empirical squared estimated generative models learn minimize objective like generative moment matching networks therefore included framework using ﬁxed proper scaling ﬁrst term. combining result obtained claim training objective generator yields task maxθλ training gans linear discriminators. hence instead searching saddle-point strive maximizer task presumably easier. price restriction linear discriminators fact every randomly drawn artiﬁcial sample dual variable λzi. non-stochastic optimization setting optimize ﬁxed sets data samples {xi} randomizations {zi} easy design learning algorithm gans linear discriminators monotonically improves objective based line search. although approach practical large data sets property convenient smaller scale data sets. addition linear models favorable scenarios know informative features want discriminator attention optimizing mini-batches introduce data samples {xi} randomizations {zi} every iteration. supplementary material show corresponds maximizing lower bound full expectation objective. since dual variables vary mini-batch next need solve newly introduced dual variables reasonable accuracy. small minibatch sizes commonly used deep learning literature like calling constrained optimization solver solve dual problem quite cheap. used ipopt solves dual problem good accuracy negligible time; solvers also used lead improved performance. utilizing log-linear discriminator reduces model’s expressiveness complexity. therefore propose methods alleviate restriction. general non-linear discriminators non-convex scoring functions parameterized deep net. non-convexity makes hard directly convert problem dual form. therefore approach training gans non-convex discriminators based repeatedly linearizing dualizing discriminator locally. ﬁrst sight seems restrictive however show speciﬁc setup technique recovers gradient direction employed regular training mechanism providing additional ﬂexibility. consider locally approximating primal objective around point using model function phrase update w.r.t. discriminator parameters search step i.e. indicates current iteration. order guarantee quality approximation introduce trust-region constraint speciﬁes trust-region size. concretely search step solving given generator parameters rather optimizing objective stochastic gradient descent instead employ model function algorithm outlined fig. proceeds ﬁrst performing gradient ascent w.r.t. generator parameters afterwards step solving program given apply step repeat. different model functions result variants algorithm. choose model function identical program given hard solve. therefore following propose model functions found useful. ﬁrst based linearization cost function recovers step employed gradient-based discriminator updates standard training. second based linearization score function keeping loss function intact; second approximation hence accurate larger region. many models exist leave exploration space future work. cost function linearization local approximation cost function constructed using ﬁrst order taylor approximation model function appealing step algorithm outlined fig. i.e. minimization model function subject trust-region constraints speciﬁed analytically computable solution step size like common specify step size standard training. mentioned before using ﬁrst order taylor approximation model recovers direction employed standard training. value parameters ﬁxed adapted; supplementary material details. importantly using ﬁrst order taylor approximation model choice. choices fairly obvious quadratic approximation present another intriguing option following. score function linearization instead linearizing entire cost function demonstrated previous part choose linearize score function locally around approximation keeps nonlinearities surrogate loss function intact therefore expect accurate linearization whole cost function already linear linearization score function introduces approximation error formulation naturally reduced discussion presented sec. non-negligible errors introduced linearizing whole cost function case. general non-linear discriminators however analytic solution computed program given using model. nonetheless model function fulﬁlls convex exploiting convexity derive dual trust-region optimization problem presented following claim. proof included supplementary material. model function claim dual program mins s.t. combining dual formulation maximization generator parameters results maximization opposed search saddle point. however unlike linear case possible design algorithm guaranteed monotonically increase cost function culprit step algorithm outlined fig. adapts model every iteration. intuitively program illustrated claim aims choosing dual variables weighted means derivatives well scores match. note program searches direction opposed searching weights hence term −cwk inside squared norm. practice ipopt solve dual problem. form dual ill-conditioned linear case. solution found ipopt sometimes contains errors however found errors generally tolerable affect performance models. figure show learning curves samples models architecture optimized dual space primal space iterations. samples shown different points training well despite similar sample qualities demonstrate drastically different training behavior. typical setup loss oscillates clear trend whereas dual setup loss monotonically increases shows much smaller oscillation. sample quality nicely correlated dual objective training. section empirically study proposed dual algorithms. particular show stable monotonic training linear discriminators study properties. nonlinear gans show good quality samples compare standard training methods. experiments done three datasets dataset composed gaussians mnist cifar- overall results show proposed approaches work across range problems provide good alternatives standard training method. explore dual linear discriminator synthetic dataset generated sampling points mixture gaussians well mnist dataset. experiments show proposed dual algorithm training stable; dual variables used extra informative signal monitoring training process; features matter train good generative models even linear discriminators good features. experiments compare proposed dual standard training generator discriminator models. discussion linear discriminators presented sec. works feature representation place long differentiable allow gradients simple -gaussian dataset features based sample training points. mnist dataset convolutional neural concatenate hidden activations layers features. dual formulation single hyper-parameter found algorithm sensitive experiments. used adam ﬁxed learning rate figure training gans linear discriminators simple -gaussians dataset. showing typical runs compared methods training curves samples single experiment left dual full batch middle dual minibatch right standard minibatch. real data dataset drawn blue generated samples green. below distribution training dual experiments histogram x-value intensity depicts frequency values ranging momentum optimize generator. additional experimental details results included supplementary material. stable training main results illustrating stable training provided fig. show learning curves well model samples different points training. dual standard minibatches size synthetic dataset extra experiment full-batch training. curves stable monotonic increase dual objective contrasted standard gan’s spiky training curves. synthetic data increasing minibatch size leads signiﬁcantly improved stability. supplementary material include extra experiment quantify stability proposed method synthetic dataset. sensitivity hyperparameters sensitivity hyperparameters another important aspect training stability. successful training typically requires carefully tuned hyperparameters making difﬁcult non-experts adopt generative models. attempt quantify sensitivity investigated robustness proposed method hyperparameter choice empirically showed proposed method less sensitive choice hyperparameters. -gaussians mnist datasets randomly sampled hyperparameter settings ranges speciﬁed table compared learning using proposed dual standard gan. -gaussians dataset evaluated performance models well model samples covered modes. deﬁned successfully covering mode samples falling within distance standard deviations center gaussian. dual linear succeeded experiments standard succeeded demonstrating method signiﬁcantly easier train tune. mnist mean inception scores proposed method training respectively. detailed breakdown mode coverage inception score found figure distribution training dual formulation allows monitor training process unique perspective monitoring dual variables fig. shows evolution distribution training synthetic dataset. begining training side generator good encouraged small minimize moment matching cost. generator improves attention devoted entropy term dual objective start converge value table ranges hyperparameters sensitivity experiment. randint means samples drawn uniformly distributed integers closed interval similarly rand real numbers. enr) shorthand exp) used hyperparameters commonly explored log-scale. generator architectures -gaussians dataset tried -layer fully-connected networks hidden units. mnist tried -layer fully-connected networks hidden units dcgan-like architecture without batch normalization. figure results hyperparameter sensitivity experiment. -gaussians dataset x-axis represents number modes covered. mnist x-axis represents discretized inception score. overall proposed dual results concentrate signiﬁcantly mass right side demonstrating better robustness hyperparameters standard gans. comparison different features qualitative differences learned models different features observed fig. general information features carry data better learned generative models are. mnist even random features linear discriminators learn reasonably good generative models. hand results also indicate features hard learn good models. leads nonlinear discriminators presented below discriminator features learned together last layer necessary complicated problems domains features potentially difﬁcult engineer. next assess applicability proposed technique non-linear discriminators focus training models mnist cifar-. discussed sec. discriminator non-linear approximate discriminator locally. therefore monotonic convergence guarantees. however better approximation optimization discriminator expect proposed dual work better standard gradient based training cases. since training sensitive hyperparameters make comparison fair tuned parameters standard gans approaches extensively compare best results each. fig. show samples generated models learned using different approaches. visually samples proposed approaches standard gans. extra quantitative metric performance computed inception score cifar- table inception score surrogate metric highly depends network architecture. figure samples dual linear using pretrained random features mnist. column shows different features utilizing layers convnet successive single layers network. table inception score different training methods. since score depends classiﬁer used code well small convnet cifar- classiﬁer evaluation scores computed using samples. pair scores ﬁnal models. gans known unstable results sometimes cherry-picked. bottom pair scores averaged across models sampled different iterations training stopped improving. therefore computed score using classiﬁer proposed seen table score cost linearization competitive standard gans. training curves also score linearization best terms approximating objective score linearization cost linearization oscillate less standard gans. thorough review research devoted generative modeling beyond scope paper. section focus gans review related work discussed throughout paper. dual formulation reveals close connection moment-matching objectives widely seen many models. related objective used deep generative models proposed range techniques improve training including usage feature matching. similar techniques also common style transfer addition these moment-matching objectives common exponential family models common works ﬁxed moments. wasserstein objective proposed training also thought form moment matching features part discriminator adaptive. main difference dual linear discriminators forms adaptive moment matching adapt weighting features optimizing non-parametric dual parameters works mostly adopt parametric model adapt features. duality also studied understand improve training. pioneered work uses duality derive training objectives divergences. also used duality derive practical objective training gans distance metrics. compared previous work instead coming objectives instead used duality original objective better optimize discriminator. beyond already discussed range techniques developed improve extend training e.g. name few. figure nonlinear discriminator experiments mnist training curves showing primal objective approximation discriminator accuracy. showing typical runs compared methods conclusion conclude introduced ‘dualing gans’ framework considers duality based formulations duel discriminator generator. using dual formulation provides opportunities train discriminator better. helps remove instability training linear discriminators also adapted framework non-linear discriminators. dual formulation also provides connections techniques. particular discussed close link moment matching techniques showed cost function linearization non-linear discriminators recovers original gradient direction standard gans. hope results spur research direction obtain better understanding objective intricacies.", "year": 2017}