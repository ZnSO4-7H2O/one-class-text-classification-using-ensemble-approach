{"title": "CSGNet: Neural Shape Parser for Constructive Solid Geometry", "tag": ["cs.CV", "cs.AI"], "abstract": "We present a neural architecture that takes as input a 2D or 3D shape and induces a program to generate it. The in- structions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this task that rely on primitive detection are inherently slow since the search space over possible primitive combi- nations is large. In contrast, our model uses a recurrent neural network conditioned on the input shape to produce a sequence of instructions in a top-down manner and is sig- nificantly faster. It is also more effective as a shape detec- tor than existing state-of-the-art detection techniques. We also demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.", "text": "figure shape parser produces compact program generates input shape. input image shape program underlying parse tree primitives combined boolean operations. bottom input voxelized shape induced program resulting shape execution. shapes constituent modeling primitives operations within framework constructive solid geometry modeling seen figure poses number challenges. first number primitives operations shapes i.e. output constant dimensionality case pixel arrays voxel grids ﬁxed point sets. second order operations matters. figure demonstrates example complex object created boolean operations combine simpler objects. performs small change e.g. swap operations resulting object becomes entirely different. aspect shape modeling process could thought visual program i.e. ordered modeling instructions. finally challenge would like learn efﬁcient parser generates compact program without relying vast number shapes annotated programs target domain. present neural architecture takes input shape induces program generate instructions program based constructive solid geometry principles i.e. boolean operations shape primitives deﬁned recursively. bottom-up techniques task rely primitive detection inherently slow since search space possible primitive combinations large. contrast model uses recurrent neural network conditioned input shape produce sequence instructions top-down manner signiﬁcantly faster. also effective shape detector existing state-of-the-art detection techniques. also demonstrate network trained novel datasets without ground-truth program annotations policy gradient techniques. recent years growing interest generative models shapes especially deep neural networks image shape priors however current methods limited generation low-level shape representations consisting pixels voxels points. human designers hand rarely model shapes collection individual elements. example vector graphics modeling packages shapes often created higher-level primitives parametric curves basic shapes well operations acting primitives boolean operations deformations extrusions reason choosing higher-level primitives incidental. describing shapes possible primitives operations highly desirable designers since compact makes subsequent editing easier perhaps better capturing aspects human shape perception view invariance compositionality symmetry tackle challenges designed memory-enabled network architecture given target image shape target shape generates program generate train network created large synthetic dataset automatically generated programs. networks trained dataset however lead poor generalization applied domains. adapt models domains without program annotations employ policy gradient techniques reinforcement learning literature. combining parser rendering engine allows parser receive feedback based visual difference target shape generated shape. thus parser network trained minimize difference. contributions follows. first show proposed architecture efﬁcient effective inferring programs shapes across number domains. second show parser learned using reinforcement learning techniques novel datasets withprogram annotations. third show parser better faster shape detector state-of-the detection approaches rely bottom-up cues. conjecture parser jointly reasons presence ordering parsing unlike detector. work primarily related neural program induction methods. secondly also related vision-asinverse-graphics approaches well neural networkbased methods predict shape primitives parameters procedural graphics models. below brieﬂy overview prior methods explain differences work. neural program induction. method inspired recent progress neural network-based methods infer programs expressed high-level language solve task. methods often employ variants recurrent neural networks whose parameters trained predict desired program outputs given exemplar inputs answers questions involving complex arithmetic logical semantic parsing operations context visual reasoning several authors proposed architectures produces programs composed functions perform compositional reasoning input image. also incorporate execution engine produces result program neural module network. contrast method aims produce generative program consisting shape modeling functions match target image. objects input image data i.e. perform analysis-bysyntesis kulkani proposed sampling-based probabilistic inference estimate parameters stochastic graphics models representing space hypothesized scenes given input image. shape grammars alternatively used analysis-bysynthesis image parsing frameworks disadvantage modeling long-range dependencies parsing task often speciﬁc particular shape class recent approaches employ convnets infer parameters objects whole scenes similar trend observed graphics applications convnets used input images partial shapes procedural model parameters detects objects scenes employing network producing object proposals network predicts whether object proposed segment along various object attributes. eslami uses recurrent neural network attends object time scene learns appropriate number inference steps recover object counts identities poses. contrast parsing images scenes collection objects parameters. instead parse input images shapes sequence modeling operations primitives match target image. setting space outputs much larger order operations visual programs matter. deal complexity combination supervised pretraining reinforcement learning reward design post-optimization modeling parameters described next section. neural primitive ﬁtting. tulsiani proposed volumetric convolutional network architecture predicts ﬁxed number cuboidal primitives describe input shape. better handle variable number primitives instead proposed lstm-based architecture predicts boxes given input depth images. also deriving geometrically interpretable explanations shapes terms primitives. however network limited predicting single type primitives also outputs modeling operations acting them words supports signiﬁcantly richer modeling paradigm. program used geometrically describe input shape also directly edited manipulate desired. finally ellis proposed neural network architecture extract various hand-drawn primitives images grouped latex programs. program synthesis posed constraint satisfaction problem computationally expensive take hours figure neural shape parser consist three main parts. firstan encoder takes shape input outputs ﬁxed dimensional vector using cnn. second decoder maps encoded vector sequence modeling instructions representing visual program. third program processed rendering engine renders program ﬁnal shape pi’s shape primitives predicted network. union intersection subtraction modeling operations shape primitives. bottom-up parsing. work related approaches shape parsing using grammars applied articulated objects represented using tree-structured grammars human bodies. however approaches often shallow grammars accurate bottom-up proposals guide parsing. context primitive detection challenging shapes appear highly distorted boolean operations applied parse trees tend deeper. result bottom-up parsing becomes computationally expensive since complexity scales exponentially program length. goal parser produce sequence instructions given input shape. parser implemented encoder-decoder using neural network modules shown figure encoder takes input image produces encoding using convolutional neural network decoder takes input produces probability distribution programs represented sequence instructions. decoders implemented using recurrent neural networks employ gated recurrent units widely used sequential prediction tasks generating natural language speech. overall network written space programs efﬁciently described according grammar. example constructive solid geometry instructions consist drawing primitives performing boolean operations described grammar following production rules rule indicates possible derivations non-terminal symbol separated symbol. start symbol chosen deﬁned modeling operations shapei primitive chosen basic shapes different positions scales orientations etc. instructions written standard post-ﬁx notation. given input parser network generates program minimizes reconstruction error shape produced executing program target shape. note programs valid hence network must also learn generate grammatical programs. supervised learning target programs available architecture trained standard supervised learning techniques. training data case consists shape program pairs implementation produces categorical distribution good choice baseline expected value returns starting produce baseline network learn minimizing mean square error reward rewards must designed achieve goals encourage visual similarity generated program target encourage shorter programs. goals conﬂict since always possible produce perfect reconstruction using long program short program poor reconstruction. describe strategy balancing goals worked well problem. shaping function rendering engine. since invalid programs zero reward maximum length constraint programs encourages network produce shorter programs achieve high rewards. maximum length experiments. function shapes exponent higher values encourages close zero. found provides good trade-off program length visual similarity. inference greedy decoding beam search estimating likely program given input intractable using rnns. instead usually employs greedy decoder picks likely instruction time step. alternate beam search procedure maintains k-best likely sequences time step. experiments report results varying beam sizes. instructions every time step. similarly ground-truth program written sequence instructions length program parameters learned maximize log-likelihood ground truth instructions learning policy gradients without target programs minimize reconstruction error shape obtained executing program target. however directly minimizing error using gradientbased techniques possible since output space discrete execution engines typically differentiable. policy gradient techniques reinforcement learning literature instead used case. concretely parser represents policy network used sample program conditioned input shape reward measures similarity generated image obtained executing program target shape estimated. setup want learn network parameters maximize expected rewards programs sampled predicted distribution across images sampled distribution often tractable compute expectation since space programs large. hence expectation must approximated. popular reinforce algorithm computes monte-carlo estimate sampling programs policy program disys obtained sampling instructions tribution every time step till stop symbol sampled. reward calculated executing program sampling based estimates typically high variance reduced visually-guided reﬁnement parser produces program discrete primitives. however reﬁnement done directly optimizing position shape primitives maximize reward. reﬁnement step keeps program structure primitive type ﬁxed uses heuristic algorithm optimize parameters using feedback rendering engine. dataset shapes primitives search space relatively small algorithm converges local minima iterations consistently improves results. describe experiments different datasets exploring generalization capabilities parser network ﬁrst describe datasets automatically generated dataset shapes based synthetic generation programs shapes mined ground-truth programs available logo images mined also ground-truth programs also available. discuss qualitative quantitative results datasets. primitives speciﬁed type square circle triangle locations circumscribing circle radius canvas size triangles assumed upright equilateral. synthetic dataset created sampling random programs containing different number primtives grammar constraining distribution various primitive types operation types uniform. also ensure duplicate programs exist dataset. primitives rendered binary images programs executed canvas pixels. samples dataset shown figure table provides details size training validation test splits. three binary operations used case. three basic solids denoted ‘sp’ sphere ‘cu’ cube ‘cy’ cylinder. represents center primitive voxel grid. speciﬁes radius sphere cylinder also speciﬁes size cube. height cylinder. primitives rendered voxel grids programs executed volumetric grid size used sampling method described synthetic dataset resulting programs. shape samples dataset shown figure table lists dataset statistics. table comparison supervised neural parser network nearest neighbor baseline synthetic dataset using chamfer distance metric. results shown different beam sizes decoding. nearest neighboring approach program retrieved ﬁnding corresponding image training dataset similar test image. table compares csgnet baseline using chamfer distance between test target predicted shapes. parser able outperform method. would expect would perform well size training large. however results indicate compositional parser better capturing shape variability still signiﬁcant dataset. results also shown increasing beam sizes decoding consistently improves performance. figure also shows programs retrieved generated program number examples test split synthetic dataset. input shape represented pixel voxel occupancy grid respectively. encoder based image-based convnet case inputs volumetric convnet case inputs. output encoder passed input gru-based decoder every program step. hidden state units passed fully-connected layers converted probability distribution program instructions classiﬁcation layer. unique instructions corresponding different primitive types discrete locations sizes boolean operations stop symbol. unique instructions different types primitives sizes locations plus boolean modeling operations stop symbol. training synthetic dataset sample images rendered programs variable length training dataset upto primitives operations. details architecture encoder decoder provided supplementary material. supervised learning adam optimizer learning rate dropout non-recurrent network connections. reinforcement learning stochastic gradient descent momentum learning rate dropout. implementation based pytorch source code become publicly available upon paper acceptance. figure comparison performance synthetic dataset. input image nn-retrieved image top- prediction csgnet best result top- beam search predictions csgnet. evaluation synthetic shapes. perform supervised learning train csgnet training split synthetic dataset evaluate performance test split different beam sizes. compare baseline retrieves program training split using evaluation shapes. dataset report results test split conditions training network synthetic data training network synthetic data also ﬁne-tuning training split dataset policy gradient. table comparison various approaches shape dataset. csgnet trained supervision comparable approach reinforcement learning dataset signiﬁcantly improves results. results shown different beam sizes decoding. visually guided reﬁnement testing also improves results signiﬁcantly. logos. here experiment logo dataset described section outputs induced programs parsing input logos shown figure general method able parse logos primitives well performance degrade long programs required generate them contain shapes different used primitives. figure results logo dataset. shows target logos second shows output shapes csgnet last column shows inferred primitives. circle primitives shown outlines triangles green squares blue. evaluation synthetic csg. finally show approach extended shapes. setting train d-cnn based network based synthetic dataset explained section input parser voxelized shapes grid. output program rendered high-resolution polygon mesh figure show pairs input voxel grids output shapes test split dataset. results indicate method promising inducing correct programs also advantage accurately reconstructing voxelized surfaces high-resolution surfaces. figure comparison performance dataset. left column right column input image retrieved image top- prediction csgnet supervised learning mode top- prediction csgnet ﬁne-tuned best result beam search csgnet ﬁne-tuned reﬁning results using visually guided search best beam result refer section details. table shows quantitative results dataset. ﬁrst compare baseline. since goal output program dataset ground-truth programs available retrieves program synthetic dataset. list performance csgnet trained supervised manner synthetic dataset. beam search performance variant improves compared importantly training reinforcement learning training split dataset improves results signiﬁcantly outperforms approach considerable margin. also shows advantage using advantage training shape parser without ground-truth programs. note directly training network using alone yield good results suggests two-stage learning important. finally optimizing best beam search program visually guided reﬁnement yielded results smallest chamfer distance figure shows comparison rendered programs various examples test split dataset variants network. visually guided reﬁnement beam search stage-learned network qualitatively produces results best match input image. accuracy measured standard evaluation protocols object detection report mean average precision primitive type using overlap threshold predicted true bounding intersection-over-union. table compares parser network faster r-cnn approach. parser clearly outperforms faster r-cnn detector squares triangles category. larger beam search also produce slighly better results circle detection. interestingly parser considerably faster faster r-cnn tested gpu. circle square triangle mean speed method faster r-cnn csgnet csgnet table detectors synthetic shape dataset. also report detection speed measured images/second nvidia gpu. believe work represents ﬁrst step towards automatic generation modeling programs given target visual content believe quite ambitious hard problem. demonstrated results generated programs various domains including logos binary shapes shapes well analysis-by-synthesis application context shape primitive detection. might argue images shapes method parsed relatively simple structure geometry. however would also like point even ostensibly simple application scenario method demonstrates competitive even better results stateof-the-art object detectors importantly problem generating programs trivial solve based experiments combination memoryenabled networks supervised strategies along beam local exploration state space seemed necessary produce good results. future work challenging research direction would generalize approach longer programs much larger spaces parameters modeling operations. another promising direction would explore combine bottom-up proposals top-down approaches parsing shapes. acknowledgments acknowledge support masstech collaborative grant funding umass cluster. figure input voxelized shape summarization steps program induced csgnet form intermediate shapes final output created executing induced program. successful program induction shape requires predicting correct primitives also correct sequences operations combine primitives. evaluate shape parser primitive detector allows directly compare approach bottom-up object detection techniques. particular compare state-of-the-art object detector faster r-cnn based vgg-m network trained using boundingbox primitive annotations based synthetic training dataset. test time detector produces bounding boxes associated class scores. models trained evaluated pixel images. also experimented bottom-up approaches primitive detection based hough transform rule-based approaches. however experiments indicated faster r-cnn considerably better. fair comparison obtain primitive detections csgnet trained synthetic dataset obtain detection scores sample programs using beam-search decoding score primitive fraction times appears across beam programs. represents monte-carlo estimate", "year": 2017}