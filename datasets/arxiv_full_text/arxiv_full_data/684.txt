{"title": "Empirical learning aided by weak domain knowledge in the form of feature  importance", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Standard hybrid learners that use domain knowledge require stronger knowledge that is hard and expensive to acquire. However, weaker domain knowledge can benefit from prior knowledge while being cost effective. Weak knowledge in the form of feature relative importance (FRI) is presented and explained. Feature relative importance is a real valued approximation of a feature's importance provided by experts. Advantage of using this knowledge is demonstrated by IANN, a modified multilayer neural network algorithm. IANN is a very simple modification of standard neural network algorithm but attains significant performance gains. Experimental results in the field of molecular biology show higher performance over other empirical learning algorithms including standard backpropagation and support vector machines. IANN performance is even comparable to a theory refinement system KBANN that uses stronger domain knowledge. This shows Feature relative importance can improve performance of existing empirical learning algorithms significantly with minimal effort.", "text": "standard hybrid learners domain knowledge require stronger knowledge hard expensive acquire. however weaker domain knowledge benefit prior knowledge cost effective. weak knowledge form feature relative importance presented explained. feature relative importance real valued approximation feature’s importance provided experts. advantage using knowledge demonstrated iann modified multilayer neural network algorithm. iann simple modification standard neural network algorithm attains significant performance gains. experimental results field molecular biology show higher performance empirical learning algorithms including standard backpropagation support vector machines. iann performance even comparable theory refinement system kbann uses stronger domain knowledge. shows feature relative importance improve performance existing empirical learning algorithms significantly minimal effort. empirical learning methods dominant methods supervised learning problems. methods dependent significant amount training data training time perform well. furthermore shown learning algorithms simply refine knowledge provided inductive bias algorithm. learner learns already knows better accuracy. therefore providing prior knowledge greatly improves performance. learner’s learned model also usually much comprehensible learner takes existing knowledge account extensive research combine prior knowledge learning algorithms focl rapture kbann ksvm different systems different forms prior knowledge. many propositional logic kbann. eccentric forms prior knowledge also used derivatives instances tangentprop certainty factors rapture however hybrid systems prevalent real world applications. main reason high cost difficulty obtaining domain knowledge prior knowledge needed systems deep extensive; fact called theory refinement systems; essentially refine existing domain theory. domain theories sufficient classify instances many cases. means much resources expertise required acquire deep prior knowledge. deep prior knowledge available cost effective many problem domains. however propose incorporating weaker/shallow form domain knowledge benefit learning providing best plain empirical hybrid theory refinement systems. weaker knowledge-based learners following capabilities firstly superior plain empirical systems terms classification accuracy given training set. secondly equally accurate empirical systems smaller training training time. moreover domain knowledge acquirable much less expertise wider problem types. paper introduces feature relative importance form weaker/shallow prior knowledge. idea weight features according importance contribution classification. already feature selection ranking algorithms available e.g. however algorithms preprocessors select modify dataset discarding irrelevant features. however cannot concluded dimensions unimportant different subset features yield predictive accuracy. actual learning algorithms take importance attributes account feature selection consider features equally important. knowledge importance used guide search space achieve faster better learning. feature relative importance represents importance features real valued normalized weight range. human experts provide feature importance much easier stronger domain theories. knowledge acquisition also cheaper faster. present iann algorithm demonstrate benefit empirical learning show superior performance compared ordinary multilayer neural networks. iann simple modification standard neural network learning procedure attains much higher performance. briefly iann algorithm uses initialize uses modified form backpropagation train network. subsequent section iann applied real world domain molecular biology specifically problem recognizing eukaryotic splice junctions promoter genes. performance analyzed compared many popular learning algorithms well hybrid learning systems. shown iann performs better plain empirical systems neural networks support vector machines. fact iann performs comparable kbann uses much stronger prior knowledge domain. brief outline paper section provides explanation section describes iann section describes experimental results. section concludes paper. describe notation used paper. used refer feature represents features data learning model neural network weights represented weight connection going unit unit learning rate input unit unit layerk units layer first hidden layer; inputs units input network units. feature importance provided experts algorithm. must define mean importance knowledge. importance defined terms feature’s influence trained learner. trained learner represented real valued scoring function input vector. many measures feature importance provided feature selection ranking domain e.g. sensitivity analysis firm saliency generally depends domain; different definitions appropriate depending one's goals. definition based define term dependency used define feature importance. exact definition vary based learning model. general definition definition dependency expected score learner’s score function conditional feature given input vector normalized approximation feature importance made human experts. define fri. definition feature relative importance normalized approximation actual importance feature relative feature learning problem feature relative importance defined important aspect importance that ratio. knowledge comes variation importance absolute value importance itself. feature features value equivalent feature value features. long ratio maintained highly important unimportant features value arbitrarily. weak knowledge apart importance straight forward relationship features actual learned model deduced fri. human experts point importance feature easily forms prior knowledge. less costly knowledge acquisition step theory refinement systems. example problem suitable weather conditions play presented explain used represent expert’s knowledge feature’s importance. values also chosen long difference maintained features. important features highest values less-important features lower values. subsequent section present iann algorithm uses domain knowledge. empirical learner domain knowledge normally used tailor bias otherwise provided randomization experimentation. feed forward neural networks provided bias mainly three ways network topology initial weights training algorithm used network topology usually experimentation; training algorithm popularly used backpropagation network weights normally initialized randomly. iann modifies normal multilayer perceptrons modifying weight initialization step training rule. currently able find tailor network architecture actually improves performance. importance weak domain knowledge provide insight network topology. result topology still must selected users traditional ways must fully connected. want modify using question importance feature affects particular network? answer question dependency neural networks defined. know definition dependency output scoring function conditional particular feature. output function single hidden layer neural network thus neural network real valued non-linear function dependency corresponds partial derivative output function respect definition dependency feature neural network partial derivative respect feature value feature computation dependency straight forward linear learner perceptron dependency feature simply weight. however number layers increase dependency gets distributed weights higher layers finding straight forward dependency becomes difficult. derivative thus derivative depends current input network well network weights. shown computation derivative np-complete trying particular formulation influence neural networks plausible choice. instead assumption higher dependency feature means higher average absolute weight first hidden layer directly connected input. thus come following intuition forms basis algorithm intuition feed forward network feature relative importance feature proportional average absolute weight trained network. clearly intuition correct several cases. relationship importance weight gets complex layers added neural network gets expressive power. moreover weights necessarily proper measure importance. fact multiplying feature inputs positive scalar dividing associated weight scalar importance corresponding feature changed arbitrarily. however experimentation shown intuition works many cases. viable assumption. intuition suppose feature higher value total weight first network layer directly connected inputs; layer features direct role. therefore focus iann layer. training rule weight initialization step changed iann ordinary mlp. explained next subsections. based intuition features higher values higher total weights also active training; features weight change training. iann provides chance important layers change weights providing higher learning rate. general backpropagation training rule follows iann provides learning rate proportional value. done multiplying value delta weight. training rule changes units first layer. however rule extended units setting hidden output layer units. simplifies implementation training rule. updated training rule becomes rule important features converge local optimum faster unimportant features take time change weights. however unimportant features proportionally less weight trained network intuition; means smaller learning rate sufficient convergence. moreover weight initialization also higher probability smaller weights unimportant features. make fewer changes weights. weight initialization iann similar normal except first hidden layer; done important features larger weight less important features lower weight. usually done simply setting weight perturbing random number. however testing shown ineffective. fact that generates general pattern weight hidden layer units; larger weights important unit weights smaller weights unimportant units. intuition says average absolute weight proportional weights units. power hidden layers comes variation initial weights. thus simple weight initialization method sacrifices variation also performance. developed method weight initialization based intuition also improved performance. first random number features selected units first hidden layer. connection features unit value randomly sign. feature weights unit random number like ordinary multilayer perceptrons range half selected features. range value selected features range features ensures higher weight features greater weights layers randomly like normal neural networks. iann modifies first layer weight initialization procedure. procedure ensures features value initial weight hidden layer units all. variation initial weights maintained important features higher average initial weights. features promoted demoted based others random weights. section reports experimental results using iann attains significant higher performance simple modification. benefit using weak prior knowledge. real world problems analysis experimented compared datasets several popular learning algorithms standard backpropagation first dataset promoter recognition dataset. promoter short sequence precedes gene sequence distinguished nonpromoter. input sequence nucleotides dataset instances positive negative examples. second dataset splice-junction determination. class problem; task determine three categories specified sequence belongs exon/intron borders intron/exon borders neither. input also sequence nucleotides. dataset instances selected randomly population percentage split classes neither datasets available ftp//ftp.cs.wisc.edu/machine-learning/shavlik-group/datasets/ weka implementations empirical learning algorithms used experiments. examples. employed -fold cross validation methodology datasets. prior knowledge propositional rules also present datasets. constructed datasets using rules basis recognize important features. features antecedent number rules given higher importance. given important features based importance; features mentioned rules given importance distribution also similar found topology network single unit hidden layer promoter dataset units splice-junction dataset. topology generalizes well better than others tried search topology space. ordinary backpropagation network topology iann kbann used propositional rules domain theory. linear kernel found effective. iann ordinary network trained epochs validation used. apparent results iann outperforms empirical algorithms unexpectedly even outperforms kbann datasets. iann outperforms standard backpropagation learner even though iann simple adjustment standard algorithm. advantage prior knowledge clearly evident splice-junctions learning curve figure splice junction full population instances used experiment. training examples randomly selected population remaining instances became test examples. error rate ordinary backpropagation kbann plotted comparison knowledge weak knowledge strong knowledge learner. results kbann backpropagation acquired earlier experiment kbann learns fastest fewer examples beginning. however iann quickly reaches kbann within training examples continues better. interesting trend knowledge aided algorithms reach optimum performance error rate actually climbs. backpropagation follows decreasing trend throughout reaching error rate iann achieves best performance examples. proposed simple efficient procedure incorporating feature importance neural network learning. performance learner shows feature importance aided learners achieve superior performance ordinary empirical learners even compare stronger knowledge based learners without extra cost deep domain theory. approach incorporating feature importance learners worthy development. possible future applications maybe areas expert knowledge readily available scarcity training data well. iann used domains fewer training data expert knowledge. furthermore modifications existing popular empirical learners also developed utilize feature importance. currently assumed feature importance knowledge provided experts almost correct extent. however accuracy questionable performance actually degrade. learning algorithm developed correct knowledge training examples. current machine learning algorithms rely much training examples. incorporating domain knowledge improvement. proposed method stepping stone towards goal domains deep knowledge available cost effective. simard victoni lecun denker tangent prop-a formalism specifying selected invariances adaptive network. advances neural information processing systems morgan kaufmann. pazzani brunk silverstein knowledge-intensive approach learning relational concepts. proceedings eighth international workshop machine learning mahoney jeffrey mooney raymond combining symbolic neural learning revise probabilistic theories. proceedings machine learning workshop integrated learning real domains fung mangasarian shavlik knowledge-based support vector machine classifiers. proceedings sixteenth conference neural information processing systems towell shavlik noordewier refinement approximate domain theories knowledge-based neural networks. proceedings eighth national conference artificial intelligence noordewier towell shavlik training knowledge-based neural networks recognize genes sequences. advances neural information processing systems morgan kaufmann", "year": 2010}