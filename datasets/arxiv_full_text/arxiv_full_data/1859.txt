{"title": "Counterfactual Learning for Machine Translation: Degeneracies and  Solutions", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Counterfactual learning is a natural scenario to improve web-based machine translation services by offline learning from feedback logged during user interactions. In order to avoid the risk of showing inferior translations to users, in such scenarios mostly exploration-free deterministic logging policies are in place. We analyze possible degeneracies of inverse and reweighted propensity scoring estimators, in stochastic and deterministic settings, and relate them to recently proposed techniques for counterfactual learning under deterministic logging.", "text": "counterfactual learning natural scenario improve web-based machine translation services ofﬂine learning feedback logged user interactions. order avoid risk showing inferior translations users scenarios mostly exploration-free deterministic logging policies place. analyze possible degeneracies inverse reweighted propensity scoring estimators stochastic deterministic settings relate recently proposed techniques counterfactual learning deterministic logging. machine translation recently become commodity service offered free online translation google microsoft integrated e-commerce platforms social media commercial settings facilitate collection user feedback quality machine translation output either form explicit user rating indirect signal inferred interaction user translated content. user feedback form user clicks displayed shown valuable signal response prediction online advertising gold mine free user feedback exploited area machine translation. recent research proposed bandit structured prediction online learning machine translation weak user feedback predicted translations instead costly manually created reference translations. scenario investigated works still removed real world applications commercial machine translation besides fact previous research conﬁned simulated feedback online bandit learning unrealistic commercial settings additional latency desire ofﬂine testing system updates deployment. natural solution would exploit counterfactual learning reuses existing interaction data predictions made historic system different target system. however online learning ofﬂine learning logged data plagued problem exploration prohibitive commercial systems since means show inferior translations users. effectively results deterministic logging policies lack explicit exploration making application off-policy methods theoretically questionable. lawrence recently showed bandit learning machine translation possible even deterministic logging. proposed application techniques doubly-robust policy evaluation learning weighted importance sampling ofﬂine learning deterministically logged data presented evidence simulation experiments conﬁrmed conjecture techniques effectively serve smooth deterministic components. purpose paper give formal account possible degeneracies standard inverse propensity scoring technique reweighted variant stochastic deterministic logging goal clearer understanding effectiveness techniques proposed lawrence following give short overview methods developed lawrence formalize problem counterfactual learning bandit structured prediction follows denotes structured input space denotes possible structured output input denotes reward function quantifying quality structured outputs. data denoted tuples inputs logging policy produced output logged corresponding reward case stochastic logging propensity score logged addition. using inverse propensity scoring approach importance sampling achieves unbiased estimate expected reward parametric target policy case deterministic logging outputs logged propensity historical system. results empirical risk minimization empirical reward maximization without correction sampling bias logging policy lawrence call equation deterministic propensity matching objective propose ﬁrst modiﬁcation weighted importance sampling objective reweighted deterministic propensity matching objective lawrence present modiﬁcations equation incorporation direct reward estimation method proposed doubly-robust estimator regression-based reward model trained logged data scalar allows optimize estimator minimal variance deﬁne doubly controlled empirical risk minimization objective setting yields objective called ˆvdc. setting recovers standard stochastic doubly-robust estimator ˆvˆc optimal scalar parameter derived easily taking derivative variance term leading learning algorithms lawrence deﬁned applying stochastic gradient ascent update rule objective functions deﬁned above. gradients shown table experiments reported lawrence policy distribution estimators exhibit degenerate behavior maximized simply setting logged outputs probability i.e. case irrespective whether data logged stochastically deterministically obviously undesired probability reward outputs raised. abbreviation theorem maxπ ˆvips maxπ ˆvdpm degenerate behavior described theorem ﬁxed using reweighting results deﬁning probability distribution reweighting increasing probability reward output takes away probability mass higher reward output. decreases value estimator thus avoided learning. however ips+r dpm+r still behave degenerate manner show following. deﬁne dmax contains tuples receive highest reward δmax observed assume δmax leading cardinality dmax least one. show estimators maximized simply setting probability least tuple dmax value higher leaving tuples dmax probabilities setting probability tuples d\\dmax clearly undesired outputs reward close δmax receive probability furthermore learning goal easy achieve since degenerate estimator needs concerned lowering probability tuples d\\dmax long tuple dmax probability want prove following theorem theorem maxπ ˆvips+r maxπ ˆvdpm+r dmax d\\dmax employing stochastic gradient ascent ips+r dpm+r prevented reaching degenerate state performing early stopping validation set. however cannot control happens probability mass freed lowering probability logged output. freed probability mass could allocated outputs receive lower reward logged output would create system worse logging system. estimators successfully solve problem. direct reward predictor takes whole output space account thus assigns rewards structured output. objective increased probability outputs high estimated reward increased decreased outputs estimated reward. happen high reward outputs ones maximal reward considered even outputs seen training log. shift probability mass unseen data high estimated reward desired property learning. completeness report experimental evidence lawrence provide show effectiveness proposed techniques. report application counterfactual learning domain-adaptation setup machine translation. model trained using out-of-domain data using hierarchical phrase-based machine translation framework based linear learner. model given in-domain data translate outputs logged together persentence bleu score true reference simulates reward signal. experiments conducted language pairs. ﬁrst german-to-english baseline system trained concatenation europarl corpus common crawl corpus news corpus. target domain represented corpus containing transcribed talks. second language pair french-to-english. out-of-domain system trained europarl corpus target domain news corpus. shown table deterministic logging best results obtained combining reweighting double control method. relations algorithms even absolute improvements quite similar stochastic logging. extended discussion lawrence presented analysis possible degenerate behavior counterfactual learning scenarios. analyzed degeneracies standard inverse propensity scoring method weighted variant stochastic deterministic logging. analysis facilitates clearer understanding doubly robust learning techniques serve avoid degeneracies techniques even allow perform counterfactual learning deterministic logging. lawrence also discuss possible implicit exploration effect stochastic selection inputs. phenomenon recently given formal account bastani analyzed formally paper. open question application techniques proposed lawrence machine translation neural networks. example necessity normalize probabilities full logged data creates memory bottleneck makes difﬁcult transfer reweighting approach neural networks.", "year": 2017}