{"title": "Hierarchical Representations for Efficient Architecture Search", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.", "text": "explore efﬁcient neural architecture search methods show simple powerful evolutionary algorithm discover architectures excellent performance. approach combines novel hierarchical genetic representation scheme imitates modularized design pattern commonly adopted human experts expressive search space supports complex topologies. algorithm efﬁciently discovers architectures outperform large number manually designed models image classiﬁcation obtaining top- error cifar- transferred imagenet competitive best existing neural architecture search approaches. also present results using random search achieving less top- accuracy cifar- less imagenet whilst reducing search time hours hour. discovering high-performance neural network architectures required years extensive research human experts trial error. image classiﬁcation task concerned state-ofthe-art convolutional neural networks going beyond deep chain-structured layout towards increasingly complex graph-structured topologies combinatorial explosion design space makes handcrafted architectures expensive obtain also likely suboptimal performance. recently surge interest using algorithms automate manual process architecture design. goal described ﬁnding optimal architecture given search space validation accuracy maximized given task. representative architecture search algorithms categorized random weights prediction monte carlo tree search evolution reinforcement learning among reinforcement learning approaches demonstrated strongest empirical performance far. architecture search computationally intensive evaluation typically requires training neural network. therefore common restrict search space reduce complexity increase efﬁciency architecture search. various constraints used include growing convolutional backbone skip connections linear sequence ﬁlter banks directed graph every node exactly predecessors work constrain search space imposing hierarchical network structure allowing ﬂexible network topologies level hierarchy. starting small primitives convolutional pooling operations bottom level hierarchy higher-level computation graphs motifs formed using lower-level motifs building blocks. motifs hierarchy stacked multiple times form ﬁnal neural network. approach enables search algorithms implement powerful hierarchical modules change motifs propagated across whole network immediately. analogous modularized design patterns used many handcrafted architectures e.g. vggnet resnet inception comprised building blocks. case hierarchical architecture discovered evolutionary random search. evolution neural architectures studied sub-task neuroevolution topology neural network simultaneously evolved along weights hyperparameters. beneﬁts indirect encoding schemes multi-scale representations historically discussed gruau kitano stanley stanley despite pioneer studies evolutionary random architecture search investigated larger scale image classiﬁcation benchmarks recently work shows power simple search methods substantially enhanced using well-designed search spaces. experimental setup resembles zoph architecture found using reinforcement learning obtained state-of-the-art performance imagenet. work reveals random evolutionary methods seen less efﬁcient scale achieve competitive performance task combined powerful architecture representation whilst utilizing signiﬁcantly less computational resources. introduce hierarchical representations describing neural network architectures. show competitive architectures image classiﬁcation obtained even simplistic random search demonstrates importance search space construction. present scalable variant evolutionary search improves results ﬁrst describe representations neural architectures architecture represented single directed acyclic graph primitive operations. move hierarchical representations smaller graph motifs used building blocks form larger motifs. primitive operations discussed sect. consider family neural network architectures represented single-source single-sink computation graph transforms input source output sink. node graph corresponds feature directed edge associated primitive operation transforms feature input node passes output node. here number nodes graph merge operation combining multiple feature maps experiments implemented depthwise concatenation. alternative option element-wise addition less ﬂexible requires incoming feature maps contain number channels strictly subsumed concatenation resulting immediately followed convolution. idea hierarchical architecture representation several motifs different levels hierarchy lower-level motifs used building blocks construction higher-level motifs. consider hierarchy levels level contains motifs. highest-level contains single motif corresponding full architecture lowest level primitive operations. recursively deﬁne m-th motif level composition hierarchical architecture representation therefore deﬁned determined network structures motifs levels bottom-level primitives. assembly process illustrated fig. applicable primitives stride convolved feature maps padded preserve spatial resolution. convolutional operations followed batch normalization relu activation number channels ﬁxed constant note convolutions larger receptive ﬁelds channels expressed motifs primitives. indeed large receptive ﬁelds obtained stacking convolutions chain structure wider convolutions channels obtained merging outputs multiple convolutions depthwise concatenation. evolutionary search neural network architectures performed treating representations sect. genotypes. ﬁrst introduce action space mutating hierarchical genotypes well diversiﬁcation-based scheme obtain initial population describe tournament selection random search sect. distributed implementation sect. sample target non-primitive level sample target motif target level. sample random successor node target motif. sample random predecessor node target motif. replace current operation randomly sampled uniform distributions respective domains. notably mutation process powerful enough perform various modiﬁcations target motif good initial coverage search space non-trivial architectures also helps avoid additional bias introduced handcrafted initialization routines. fact strategy ensures initial architectures reasonably well-performing even without search suggested random sample results table evolutionary search algorithm based tournament selection starting initial population random genotypes tournament selection provides mechanism pick promising genotypes population place mutated offspring back population. repeating process quality population keeps reﬁned time. always train model scratch ﬁxed number iterations refer training evaluation single model evolution step. genotype highest ﬁtness among entire population selected ﬁnal output ﬁxed amount time. tournament formed random genotypes sampled current effective population among individual highest ﬁtness value wins tournament. selection pressure controlled tournament size population size case. remove genotypes population allowing grow time maintaining architecture diversity. evolution algorithm similar binary tournament selection used recent large-scale evolutionary method also investigated random search simpler strategy sufﬁciently explored literature alternative evolution. case population genotypes generated randomly ﬁtness computed genotype done evolution genotype highest ﬁtness selected ﬁnal output. main advantage method parallel entire population substantially reducing search time. distributed implementation asynchronous consisting single controller responsible performing evolution genotypes workers responsible evaluation. parties access shared tabular memory recording population genotypes ﬁtness well data queue containing genotypes unknown ﬁtness evaluated. speciﬁcally controller perform tournament selection genotype whenever worker becomes available followed mutation selected genotype insertion ﬁtness evaluation worker pick unevaluated genotype whenever available assemble architecture carry training validation record validation accuracy architectures trained scratch ﬁxed number steps random weight initialization. rely weight inheritance though incorporating system possible. note architecture evolution synchronization required workers fully occupied. experiments proposed search framework learn architecture convolutional cell rather entire model. reason would like able quickly compute ﬁtness candidate architecture transfer larger model achieved using less cells ﬁtness computation cells full model evaluation. similar approach recently used architecture search carried entirely cifar- training split sub-sets training validation images. candidate models trained training subset evaluated validation subset obtain ﬁtness. search process over selected cell plugged large model trained combination training validation sub-sets accuracy reported cifar- test set. note test never used model selection used ﬁnal model evaluation. also evaluate cells learned cifar- large-scale setting imagenet challenge dataset figure image classiﬁcation models constructed using cells optimized architecture search. top-left small model used architecture search cifar-. top-right large cifar- model used learned cell evaluation. bottom imagenet model used learned cell evaluation. cifar- experiments model consists convolution channels followed groups learned convolutional cells group containing cells. cell insert separable convolution stride channels last cell group stride channels otherwise. purpose convolutions control number channels well reduce spatial resolution. last cell followed global average pooling linear softmax layer. ﬁtness computation smaller model shown fig. trained using momentum steps starting learning rate reduced steps. batch size weight decay value employ standard training data augmentation crop randomly sampled image followed random horizontal ﬂipping. evaluation performed full size image. note variance. found variance optimization non-negligible believe reporting important performing fair comparison assessing model capabilities. training cifar models observed standard deviation using exact setup. solution adopted compute ﬁtness average accuracy training-evaluation runs. evaluation learned cell architecture cifar- larger model shown fig. larger model trained steps starting learning rate reduced steps. rest training settings used ﬁtness computation. report mean standard deviation computed training-evaluation runs. evaluation ilsvrc imagenet challenge dataset architecture similar used cifar following changes. input image passed convolutional layers channels stride each. followed groups convolutional cells ﬁrst group contains single cell remaining three groups cells momentum steps starting learning rate reduced steps. batch size weight decay auxiliary losses weight averaging label smoothing path dropout empirically found effective training augmentation consists random crops horizontal ﬂips brightness contrast changes. report single-crop top- top- error ilsvrc validation set. figure fitness number parameters evolution step hierarchical representations. left ﬁtness genotype generated evolution step. middle maximum ﬁtness across genotypes generated evolution step. right number parameters small cifar- model constructed using genotype generated evolution step. evolution hierarchical genotypes steps using workers. initial size randomly initialized population later grows result tournament selection mutation hierarchical representation three levels level- motifs graph nodes level- motif graph nodes. level- motif followed convolution number channels motif input reduce number parameters. representation used graph nodes achieve comparable number edges. evolution process visualized fig. left plot shows ﬁtness genotype generated step evolution ﬁtness grows fast initially plateaus time. middle plot shows best ﬁtness observed evolution step. since ﬁrst steps correspond random initialization mutation starts that best architecture found step corresponds output random search architectures. fig. shows number parameters small network constructed using genotype produced step. notably genotypes achieve higher ﬁtness cost larger parameter count. thus also consider parameter-constrained variant genotype genotypes number parameters ﬁxed threshold permitted; threshold chosen genotype similar number parameters hierarchical one. setting hierarchical genotypes achieve similar ﬁtness. demonstrate improvement ﬁtness hierarchical architecture correlated improvement accuracy corresponding large model trained till convergence plot relative accuracy improvements fig. figure accuracy improvement course evolution measured respect ﬁrst random genotype. small model model used ﬁtness computation evolution large model model evolved cell architecture deployed training evaluation. architecture search time concerned takes hour compute ﬁtness architecture single using gpus thus takes hour perform random search architectures days evolutionary search steps. signiﬁcantly faster days using gpus reported days using gpus reported flat repr-n random architecture flat repr-n random search flat repr-n evolution flat repr-n parameter-constrained evolution hier. repr-n random architecture hier. repr-n random search hier. repr-n random search hier. repr-n evolution first note randomly sampled architectures already perform surprisingly well attribute representation power architecture spaces. second random search architectures achieves competitive results cifar- imagenet remarkable considering took hour carry out. demonstrates well-constructed architecture representations coupled diversiﬁed sampling simple search form simple strong baseline architecture search. best results achieved using evolution hierarchical representations classiﬁcation error cifar- test improved channels imagenet validation achieve top- classiﬁcation error top- error. results context state tables achieve best published results cifar- using evolutionary architecture search also demonstrate competitive performance compared best published methods cifar- imagenet. imagenet model parameters comparable inception-resnet-v larger nasnet-a model resnet- pre-activation wide resnet-- dropout densenet densenet-bc metaqnn block-qnn-a nasnet-a evolving genetic large-scale evolution smash evolutionary search hier. repr. evolutionary search hier. repr. table classiﬁcation error cifar- test obtained using state-of-the-art models well best-performing architecture found using proposed architecture search framework. existing models grouped handcrafted architectures architectures found using reinforcement learning architectures found using random evolutionary search. table classiﬁcation error imagenet validation obtained using state-of-the-art models well best-performing architecture found using framework. evolved hierarchical cell visualized appendix shows architecture search discovered number skip connections. example cell contains direct skip connection input output nodes connected motif turn contains direct connection input output. cell also contains several internal skip connections motif presented efﬁcient evolutionary method identiﬁes high-performing neural architectures based novel hierarchical representation scheme smaller operations used building blocks form larger ones. notably show strong results obtained even using simplistic search algorithms evolution random search coupled well-designed architecture representation. best architecture yields state-of-the-art result frdric gruau l’universite claude bernard lyon diplome doctorat jacques demongeot examinators michel cosnard jacques mazoyer pierre peretto darell whitley. neural network synthesis using cellular encoding genetic algorithm. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning risto miikkulainen jason liang elliot meyerson aditya rawal fink olivier francon bala raju arshak navruzyan nigel duffy babak hodjat. evolving deep neural networks. arxiv preprint arxiv. esteban real sherry moore andrew selle saurabh saxena yutaka leon suematsu quoc alex kurakin. large-scale evolution image classiﬁers. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei imagenet large scale visual recognition challenge. international journal computer vision christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition", "year": 2017}