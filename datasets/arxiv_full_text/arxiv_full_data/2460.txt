{"title": "Large-scale Validation of Counterfactual Learning Methods: A Test-Bed", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The ability to perform effective off-policy learning would revolutionize the process of building better interactive systems, such as search engines and recommendation systems for e-commerce, computational advertising and news. Recent approaches for off-policy evaluation and learning in these settings appear promising. With this paper, we provide real-world data and a standardized test-bed to systematically investigate these algorithms using data from display advertising. In particular, we consider the problem of filling a banner ad with an aggregate of multiple products the user may want to purchase. This paper presents our test-bed, the sanity checks we ran to ensure its validity, and shows results comparing state-of-the-art off-policy learning methods like doubly robust optimization, POEM, and reductions to supervised learning using regression baselines. Our results show experimental evidence that recent off-policy learning methods can improve upon state-of-the-art supervised learning techniques on a large-scale real-world data set.", "text": "ability perform effective off-policy learning would revolutionize process building better interactive systems search engines recommendation systems e-commerce computational advertising news. recent approaches off-policy evaluation learning settings appear promising paper provide real-world data standardized test-bed systematically investigate algorithms using data display advertising. particular consider problem ﬁlling banner aggregate multiple products user want purchase. paper presents test-bed sanity checks ensure validity shows results comparing state-of-the-art off-policy learning methods like doubly robust optimization poem reductions supervised learning using regression baselines. results show experimental evidence recent off-policy learning methods improve upon state-of-the-art supervised learning techniques large-scale real-world data set. effective learning methods optimizing policies based logged user-interaction data potential revolutionize process building better interactive systems. unlike industry standard using expert judgments training learning methods could directly optimize user-centric performance measures would require interactive experimental control like online algorithms would subject data bottlenecks latency inherent testing. recent approaches off-policy evaluation learning settings appear promising highlight need accurately logging propensities logged actions. paper provide ﬁrst public dataset contains accurately logged propensities problem batch learning bandit feedback data criteo leader display advertising space. addition providing data propose evaluation methodology running blbf learning experiments standardized test-bed allows research community systematically investigate blbf algorithms. i=}. encodes system logs collected denotes input system denotes output predicted system number encoding observed online metric output predicted; elaborate deﬁnitions logged dataset next section. since past research blbf limited availability appropriate dataset hope test-bed spur research several aspects blbf off-policy evaluation including following training objectives learning algorithms regularization mechanisms blbf; improved model selection procedures effective tractable policy classes speciﬁed task algorithms scale massive amounts data. rest paper organized follows. section describe standardized test-bed evaluation off-policy learning methods. then section describe sanity checks used dataset ensure validity applied generally gathering data off-policy learning evaluation. finally section show results comparing state-of-the-art off-policy learning methods like doubly robust optimization poem reductions supervised learning using regression baselines. results show ﬁrst time experimental evidence recent off-policy learning methods improve upon state-of-the-art supervised learning techniques largescale real-world data set. create test-bed using data display advertising similar kaggle challenge hosted criteo compare prediction algorithms. however paper build clickthrough conversion prediction models bidding real-time auctions instead consider problem ﬁlling banner aggregate multiple products user want purchase. part system takes place bidding agent auction. context many banner types differ number products contain layout shown figure task choose products display knowing banner type order maximize number clicks. task thus different kaggle challenge. setting choosing best products banner easily gather exploration data placement products banner randomized without incurring prohibitive cost unlike search exploration much costly logging policy uses randomization aggressively different uniformly random policy. banner type corresponds different look feel banner banner differ number products size geometry background color data shown call ﬁxed attributes. banner types also dynamic aspects form pagination animation. examples shown figure throughout paper label positions banner type left right bottom. thus left position. user impression denote user context number slots banner type candidate pool products context product pair described features input system encodes pc{φ pc}. logging policy stochastically selects products construct banner ﬁrst computing non-negative scores candidate products using plackett-luce ranking model propensity chosen banner propensities hand counterfactually evaluate banner-ﬁlling policy unbiased using inverse propensity scoring following logged committing single feature encoding single produces scores entire duration data collection. record feature vector products candidate record selected products sampled plackett-luce model propensity; record click/no click location banner. impression represented lines cardinality ﬁrst line header containing summary information. note ﬁrst {nbslots} candidates correspond displayed products ordered position features. display features context features banner type features constant candidate products given impression. unique quadruplet feature correspond unique banner type. product features based similarity and/or complementarity candidate products historical products seen user advertiser’s website. also included interaction terms features directly dataset limit amount feature engineering required good policy. features numerical features categorical. categorical features multi-valued means take value product note example increasing time allowing temporal slices evaluation although enforce test-bed. importantly non-clicked examples sub-sampled aggressively reduce dataset size kept random sub-sample them. needs account learning evaluation evaluator provide test-bed accounts sub-sampling. work-horse counterfactual evaluation inverse propensity scoring requires accurate propensities crude approximation produces estimates variance scales roughly range inverse propensities. table report number impressions average largest inverse propensities partitioned {nbslots}. constructing conﬁdence intervals importance weighted estimates like often appeal asymptotic normality large sample averages however inverse propensities large relative number samples asymptotic normality assumption probably violated. simple statistical tests detect issues inaccurately logged propensities arithmetic harmonic tests however require candidate actions available impression ﬁxed priori. scenario context-dependent candidate precludes running tests propose general class diagnostics detect systematic biases issues propensity-logged datasets. estimate diagnostic computable case since require datapoints sub-sampling. following straightforward modiﬁcation sub-sampled data-points instead. again eeoi∼pr hence numerator expectation normalizing constant expectation ratios expectations equal expectation ratio expect small bias estimate easy show estimate asymptotically consistent. finally consider estimating again eeoi∼pr numerator expectation denominator. again expect estimate small bias remain asymptotically consistent. computable variant self-normalized estimator simply uses computable deﬁnition ˆrsnips family policies parametrized diagnose expected behavior estimates policy behaves like uniformly random ranking policy probability probability behaves like logging policy. formally impression context possible actions logged action probability choosing policy vary away policy looks different logging policy logged impressions. tables report conﬁdence interval assuming asymptotic normality different choices also report ips-estimated clickthrough rates policies standard error ﬁnally self-normalized ips-estimates pick policies differ logging policy estimated variance estimates increases. moreover control variate systematically under-estimated. caution rely single pointestimate snips often provide better bias-variance trade-off estimates fail catastrophically variance high systematic under-estimation moreover high-variance situations constructed conﬁdence intervals reliable clearly computed intervals. based sanity checks focus evaluation set-up section -slot case. estimates based importance sampling considerable variance number slots increases. would thus need tens millions impressions estimate slot-ﬁlling policies high precision. limit risks people over-ﬁtting variance querying away logging policy propose following estimates policy table diagnostics ips-estimated clickthrough rates different policies evaluated slices trafﬁc k-slot banners interpolates logging policy uniform provided evaluation software alongside dataset online. learning algorithms must reason bias/variance explicitly reliably achieve better estimated ctr. consider -slot banner ﬁlling task deﬁned using dataset. slice trafﬁc modeled logged contextual bandit problem small number arms. slice randomly divided train-validate-test split. following methods benchmarked code accompanying dataset release. methods linear policy class πlin differ training objectives. hyperparameters chosen maximize validation test-set estimates reported table random policy picks uniformly random display. regression reduction supervised learning predicts every candidate action. number training epochs regularization lasso learning rate hyper-parameters. directly optimizes evaluated training split. implementation uses reduction weighted one-against-all multi-class classiﬁcation employed hyperparameters regression approach. combines regression method using doubly robust estimator perform policy optimization. uses reduction weighted one-against-all multi-class classiﬁcation uses hyper-parameters. poem directly trains stochastic policy following counterfactual risk minimization principle thus reasoning differences variance estimate hyperparameters variance regularization regularization propensity clipping number training epochs. results learning experiments summarized table details speciﬁcs experiment setup visit dataset website. differences random numbers compared table computed subset expect conﬁdence intervals overlap. regression approach loosely corresponds predicting candidate using supervised machine learning substantially improved using many recent off-policy learning algorithms effectively logged propensities. also note limited hyper-parameter tuning performed methods like poem instance poem conceivably improved employing doubly robust estimator. leave algorithm-tuning future work. paper introduced standardized test-bed systematically investigate off-policy learning algorithms using real-word data. presented test-bed sanity checks ensure validity showed results comparing state-of-the-art off-policy learning methods poem regression baselines -slot banner ﬁlling task. results show experimental evidence recent off-policy learning methods improve upon state-of-the-art supervised learning techniques large-scale real-world data set. results presented -slot banner ﬁlling tasks. several dimensions setting challenging interesting relevant off-policy learning problems data collected future work.", "year": 2016}