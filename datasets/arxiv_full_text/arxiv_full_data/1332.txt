{"title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural  Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDAR-video inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.", "text": "abstract—the ability simultaneously leverage multiple modes sensor information critical perception automated vehicle’s physical surroundings. spatio-temporal alignment registration incoming information often prerequisite analyzing fused data. persistence reliability multi-modal registration therefore stability decision support systems ingesting fused information. lidar-video systems like many driverless cars common example keeping lidar video channels registered common physical features important. develop deep learning method takes multiple channels heterogeneous data detect misalignment lidarvideo inputs. number variations tested ford lidar-video driving test data discussed. best knowledge multi-modal deep convolutional neural networks dynamic real-time lidar-video registration presented. navigation situational awareness optionally manned vehicles requires integration multiple sensing modalities light detection ranging video could easily extended modalities including radio detection ranging shortwavelength infrared global positioning system spatio-temporal registration information multimodal sensors technically challenging right. many tasks pedestrian object detection tasks make multiple sensors decision support methods rest assumption proper registration. approaches lidar-video instance build separate vision lidar feature extraction methods identify common anchor points both. alternatively generating single feature lidar video optical enables system capture mutual information among modalities efﬁciently. ability dynamically register information available data channels perception related tasks alleviate need anchor points sensor modalities. auto-registration prerequisite need operating multimodal information conﬁdence. deep neural networks lend seamless manner data fusion time series data. challenges modalities share signiﬁcant mutual information features generated fused information provide insight neither input alone effect version whole greater it’s parts. autonomous navigation places signiﬁcant constraints speed perception algorithms ability drive decision making real-time. though computationally intensive train implemented dcnn easily within real-time frame rates could accommodate standard rates fps. research deep neural networks focused algorithmic improvements novel applications signiﬁcant beneﬁt applied researchers sometimes appreciated. automated feature generation dnns enables create mutli-modal systems less overhead. need domain experts hand-crafted feature design lessened allowing rapid prototyping testing. generalization auto-registration across multiple assets clearly path explored. paper main contributions formulation image registration problem fusion modalities different sensors namely lidar video optical performance evaluation deep convolutional neural networks various input parameters kernel ﬁlter size different combinations input channels fusion patch-level image-level predictions generate alignment frame-level. experiments conducted using publicly available dataset ford university michigan dcnn implementation executed nvidia tesla cores compute power tflops paper organized following sections section describes introduction motivation work; section provides survey related work; problem formulation along dataset description preprocessing explained section iii; section gives details dcnn setup different experiments; section describes experiments post-processing steps visualizing qualitative results; ﬁnally section summarizes paper concludes future research thrusts. great amount published various multi-modal fusion methods common approaches taken generate features interest modality separately create decision support mechanism aggregates features across modalities. spatial alignment required across modalities lidar-video ﬁlter methods required ensure proper inter-modal registration. ﬁlter methods leveraging lidar images often geometric nature make projections different data spaces. automatic registration video lidar widely researched topic decade application real-time autonomous navigation makes challenging problem. majority registration algorithms based feature matching. geometric features like corners edges extracted detected vanishing points line segments shadows feature based approaches generally rely dense point cloud additional knowledge relative position gps/inertial navigation system another approach used video lidar auto-registration reconstruct point cloud video sequences using structure motion performing registration registration difﬁcult computationally expensive compared registration. deep neural networks analyze multi-modal sensor inputs increased sharply last years including audio-video image/text image/depth lidar-video best knowledge multi-modal deep neural networks dynamic lidar-video registration presented. common challenge data fusion methods deciding level features differing sensor streams brought together. deep neural network approach similar traditional data fusion methods train dnns independently sensor modalities high-level outputs networks inputs subsequent aggregator could also dnn. analogous earlier example learning features process identifying common geometric features. possible however apply dnns agnostic view enabling uniﬁed features learned across multi-modal data. cases input channels aren’t differentiated. unsupervised methods including deep boltzmann machines deep auto-encoders learning joint representations successful. deep convolutional neural networks enable similar agnostic approach input channels. signiﬁcant difference target data required train classiﬁers. approach chosen automating registration lidar-video optical-ﬂow combining d/d/d data representations respectively learn uniﬁed model across many able detect correct misalignment among sensors different kinds critical decision support systems operating fused information streams. work dcnns implemented detection small spatial misalignments lidar video frames. methodology directly applicable temporal registration well. lidar-video data collected driverless chosen multi-modal fusion test case. lidar-video common combination providing perception capabilities many types ground airborne platforms including driverless cars xsens mti-g sensor consisting accelerometer gyroscope magnetometer integrated receiver static pressure sensor temperature sensor. measures co-ordinates vehicle also provides velocity rate turn. dataset generated vehicle driving around ford research campus downtown michigan. data includes feature rich downtown areas well featureless empty parking lots. shown figure divided data training testing sections respectively. chosen manner minimizes likelihood contamination training testing. this direction light source never testing training sets. area navigation mobile robots optical widely used estimate egomotion depth maps reconstruct dynamic scene depth segment moving objects optical provides information scene dynamics expressed estimate velocity pixel consecutive frames denoted motion ﬁeld frames measured motion pixel brightness pattern changes image brightness camera object motion. describes algorithm computing optical images used preprocessing step. figure shows example optical computed using consecutive frames ford lidar-video dataset. including optical input channels imbue dcnn information dynamics observed across time steps. video frame timestep inputs model consist channels data ranging channels. channels consist grayscale information video horizontal vertical components optical depth information lidar data modality reshaped ﬁxed size values partitioned patches prescribed stride. patch stacked across channels effectively generating vector dimensions. different preprocessing parameters denoted patch size stride number input channels preprocessing repeated times number offset classes. offset class video optical channels kept static depth channel lidar moved offset simulating misalignment video lidar sensors. order accurately detect misalignment lidar video sensor data threshold limit information available channel. lidar data regions sparsity hence lidar patches variance dropped ﬁnal dataset. leads elimination majority foreground patches data reducing size training testing approximately figure shows class elliptically distributed offsets figure shows patch stacked across different channels. models auto-registration dcnns trained classify current misalignment lidar-video data streams predeﬁned offsets. dcnns probably successful deep learning model date ﬁelded applications. fact algorithm shares weights training phase results fewer model parameters efﬁcient training. dcnns particularly useful problems local structure important object recognition images temporal information voice recognition. alternating steps convolution pooling generates features multiple scales turn imbues dcnn’s scale invariant characteristics. model shown figure consists pairs convolution-pooling layers estimates offset lidar-video inputs time step. patch within timestep variants lidar-videooptical inputs offset predetermined amounts. outputs softmax layer thereby providing offset classiﬁcation value patch frame. described section iii-c patches stacked across different channels provided input dcnn. channels rgbluv used majority experiments whereas channels required rgbl grluv experiments. ﬁrst convolutional layer uses ﬁlters size stride pixel padding pixels edges. following pooling layer generates input data second convolutional layer. layer uses ﬁlters size stride pixel padding pixels edges. second pooling layer similar ﬁrst used generate input size third convolutional layer uses ﬁlters size stride padding previous convolutional layer. third pooling layer similar conﬁguration previous pooling layers connects output softmax layer labels corresponding classes. dcnn described trained using stochastic gradient descent mini-batch size epochs. dcnn conﬁgured rectiﬁed linear units train several times faster equivalents tanh connections nvidia kepler series gpus flops/watt efﬁcient used drive real-time image processing capabilities gpus consist cores on-board device memory deep learning applications targeted gpus previously implementations compute memory bound. stacking channels results vector suitable single instruction multiple datapath architecture gpus. time training batch size caches memory utilization gpu’s memory high. also results experiments successfully single instead partitioning different layers multiple gpus. experiments elliptically distributed offsets lidar-video data considered. lidar data displaced along ellipse major axis pixels minor axis pixels rotated clockwise x-axis shown figure separate training testing sets generated different tracks shown figure offsets lidar data. training testing tracks never seen regions also different lighting conditions. preprocessing step described section iii-c results patches testing training extracted images respectively. single frame level prediction. sample histogram patch level predictions show figure color patch frame color corresponding predicted class shown figure table lists inputs parameters explored ranked order increasing accuracy. averaged values across diagonal confusion matrix determine image level patch level accuracy. patch level accuracy patches individual performance testing images. classiﬁcation patches belonging single time-step voted predict shift image level accuracy. table ﬁrst columns show results different number ﬁlter combinations convolutional layers ﬁxed number ﬁlters input channels rgbluv. observed image patch level accuracy decreased increase number ﬁlters. experiments shown columns ﬁlter size increased number ﬁlters constant observed channels rgbluv ﬁlter size gave best image level accuracy column shows results experiment dropping optical channels. image patch level accuracy decreased case indicating optical contributed signiﬁcantly towards image registration. remaining experiments utilized grayscale information instead produced best results image patch level accuracy respectively. table shows using information consecutive frames performance increases signiﬁcantly. paper proposed deep learning method lidar-video registration. demonstrated effect ﬁlter size number ﬁlters different channels. also showed advantage using temporal information optical grayscale. next step taking work forward complete development deep auto-registration method ground aerial platforms requiring priori calibration ground truth. aerospace applications particular present noisier data increased number degrees freedom. extension methods simultaneously register snoek worring gemert j.-m. geusebroek smeulders challenge problem automated detection semantic concepts multimedia proceedings annual international conference multimedia. thrun google’s driverless talk wang neumann robust approach automatic registration aerial images untextured aerial lidar data computer vision pattern recognition cvpr ieee conference correa automatic registration lidar optical imagery using depth stereo computational photography ieee international conference ieee mastin kepner fisher automatic registration lidar optical images urban scenes computer vision pattern recognition cvpr ieee conference ieee ding lyngbaek zakhor automatic registration aerial imagery untextured lidar models computer vision pattern recognition cvpr ieee conference june frueh sammon zakhor automated texture mapping city models oblique aerial imagery data processing visualization transmission dpvt proceedings. international symposium sept stamos chen wolberg zokai integrating automated range registration multiview geometry photorealistic modeling large-scale scenes international journal computer vision vol. available http//dx.doi.org/./s--- zhao nister alignment continuous video onto point clouds computer vision pattern recognition cvpr proceedings ieee computer society conference vol. june ii–ii. provost deep learning robust feature generation audiovisual emotion recognition acoustics speech signal processing ieee international conference", "year": 2014}