{"title": "On the Limits of Learning Representations with Label-Based Supervision", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning.", "text": "jiaming song russell stewart shengjia zhao stefano ermon computer science department stanford university {tsongstewartrzhaosjermon}cs.stanford.edu advances neural network based classiﬁers transformed automatic feature learning pipe dream stronger routine expected property practical systems. since emergence alexnet every winning submission imagenet challenge employed end-to-end representation learning utility good representations transfer learning representation learning become important distinct task supervised learning. present distinction inconsequential supervised methods state-of-the-art learning transferable representations recent work shown generative models also powerful agents representation learning. representations learned generative methods ever rival quality supervised competitors? work argue afﬁrmative information theoretic perspective generative models greater potential representation learning. based several experimentally validated assumptions show supervised learning upper bounded capacity representation learning ways certain generative models generative adversarial networks not. hope analysis provide rigorous motivation exploration generative representation learning. feature learning discriminative models observations drawn distribution labels obtained deterministic mapping assume operating domain exists good features would like learn features emerge suitable weights deep neural network thus must compete exponentially large random features. goal learn good features dataset process using neural network perform certain tasks. analyze feature learning process parameterizing state network according features already learned. investigate marginal value learning additional feature. thus learned features {fi}k− propose measure ease learning k-th feature according reduction entropy labels feature improve supervised learning performance. concept simply encodes intuition features easier learn pertain directly task hand aligns well information gain feature selection metric random forests genomic studies willing believe learnability feature corresponds signal observe following upper bound potential learning features using labeled supervision. learn features signal features must least features must hard learn mini∈{...k} signal h/k. thus independent size dataset upper bound capacity model learn large number features. remainder paper refer phenomenon feature competition. tested feature competition hypothesis following experiment mnist digits. input image contained side side digits randomly selected mnist dataset denoted denote respective ground truth labels ﬁrst trained feature detector inputs using left label supervision. given train logistic regression classiﬁer predict right label inputs keeping ﬁxed. uncorrelated task difﬁcult value learning features original task. here show even features contain information sufﬁciency features decreases conditional mutual information features makes harder learn. consider separate networks. concatenation used perform supervised learning tasks. ﬁrst phase completely corrupt left digit probability assign right digit label label left digit probability pretraining task able calculate signal learning features function given ﬁxed increasing make features easier learn without changing relationship however shown figure ﬁxed increasing decrease test performance second phase. suggests ability learn high quality features decreases ability learn features thus direct competition features supervised learning. supervised learning bounded capacity feature extraction entropy labels unsupervised learning? section show family unsupervised learning methods generative adversarial networks impacted feature competition limited assumptions. gans generator network generates samples discriminator network attempts distinguish samples real data. presented labeled dataset classes balanced assume already learned features discriminator cannot separate samples features. indicates pr··· fk−) dataset thus discriminator state confusion. measure motivation learning feature measure similarity distributions feature real generated samples. identically distributed almost everyg thus generated distribution match real data distribution along positive signal learn. importantly lower bound dependence previously learned features fk−. discriminator state confusion feature competition. shown theoretically class generative models gans limited upper bound feature learning signal discriminative models cnns. empirically test implications revisiting two-digit experiment section digits selected completely random consider four frameworks feed forward convolutional traditional recently proposed wasserstein autoencoder. four frameworks architecture output neurons second layer. results shown table demonstrate spite absence labels features learned three generative models considered including gans wgans useful subsequent task learning recognize right digit. identiﬁed upper bound capacity supervised models extract features depends size dataset rather quality labels. results suggest great promise future feature extraction unlabeled approaches. chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems nguyen yosinski bengio dosovitskiy clune. plug play generative networks conditional iterative generation images latent space. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems longer independent fk+l− equation analogous supervised learning setting simply trying learn features fk+l given previous features fk+l− optimize ﬁxed objective deﬁned features fk−. learned features learned features motivated learn proper distribution feature minimize however quantity still smaller even assume learns correct distribution incentive becomes notice mutual information exactly motivation discriminator learn features fk+l. implies continue allow learn feature time case attempt balance gans loss statistics catch step; hand advantage cause suffer feature competition challenge less incentive learn features should. obvious method counter balancing gans loss statistics; although suffers feature competition learning fk+l− catch learns multiple features consecutively makes confused again however promoting strategy trained whenever loss exceeds predetermined value believe principled approaches tackle problem valuable training gans.", "year": 2017}