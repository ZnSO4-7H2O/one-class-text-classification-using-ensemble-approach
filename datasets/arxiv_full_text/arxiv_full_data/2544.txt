{"title": "A Subsequence Interleaving Model for Sequential Pattern Mining", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms.", "text": "paper introduce alternate probabilistic perspective subsequence mining develop generative model database conditioned patterns. then following shannon’s theorem length optimal code database model simply negative logarithm probability. allows search patterns best compress database withdesigning speciﬁc coding scheme. approach call interesting sequence miner novel sequential pattern mining algorithm able eﬃciently mine relevant sequential patterns database rank using associated measure interestingness. makes novel probabilistic model sequences based generating sequence interleaving group subsequences. learned component subsequences patterns returns. approach based probabilistic machine learning brings variety beneﬁts namely probabilistic model allows declaratively incorporate ideas types patterns would useful; easily compose model types probabilistic models literature; able bring bear powerful tools inference optimization probabilistic machine learning. inference model involves approximate optimization non-monotone submodular objective subject submodular coverage constraint. necessary partition function intractable construct directly however show eﬃciently computed using suitable lower bound. sequential patterns model inferred eﬃciently using structural expectation maximization framework knowledge ﬁrst expectation-maximization scheme subsequence mining problem. real-world datasets returns notably diverse patterns recent methods gokrimp retaining similar quality. diverse patterns suggest especially suitable manual examination exploratory data analysis. qualitatively mined patterns highly correlated extremely relevant e.g. representing phrases dear concepts reproducing kernel hilbert space. broadly perspective potential open wide variety future directions modelling approaches combining sequential pattern mining methods hierarchical models topic models nonparametric bayesian methods. abstract recent sequential pattern mining methods used minimum description length principle deﬁne encoding scheme describes algorithm mining compressing patterns database. present novel subsequence interleaving model based probabilistic model sequence database allows search compressing patterns without designing speciﬁc encoding scheme. proposed algorithm able eﬃciently mine relevant sequential patterns rank using associated measure interestingness. eﬃcient inference model direct result structural expectation-maximization framework expectation-step takes form submodular optimization problem subject coverage constraint. show synthetic real world datasets model mines sequential patterns spuriousness redundancy high interpretability usefulness real-world applications. furthermore demonstrate quality patterns approach comparable better than existing state sequential pattern mining algorithms. sequential data pose challenge exploratory data analysis large data sets sequences diﬃcult visualise. applications healthcare click streams bioinformatics source code common approach sequential pattern mining identify patterns commonly occur subsequences sequences data. natural family approaches sequential pattern mining mine frequent subsequences closed frequent subsequences suﬀer well-known problem pattern explosion list frequent subsequences typically long highly redundant diﬃcult understand. recently researchers introduced methods prevent problem pattern explosion based minimum description length principle methods deﬁne encoding scheme describes algorithm compressing sequence database based library subsequence patterns search patterns lead best compression database. methods provide theoretically principled approach results better patterns frequent subsequence mining performance relies designing sequential pattern mining ﬁrst introduced agrawal srikant context market basket analysis number algorithms frequent subsequence including preﬁxspan spade spam frequent sequence mining suﬀers pattern explosion huge number highly redundant frequent sequences retrieved given minimum support threshold low. address mining frequent closed sequences i.e. subsequences frequency bide algorithm however even mining frequent closed sequences fully resolve problem pattern explosion. refer interested reader chapter survey frequent sequence mining algorithms. attempt tackle problem modern approaches sequence mining used minimum description length principle sequences best summarize data. gokrimp algorithm directly mines sequences best compress database using mdl-based approach. goal gokrimp essentially cover database sequences possible dictionary-based description length used gokrimp favours encoding schemes cover long frequent subsequences database. fact ﬁnding compressing sequence database strongly related maximum tiling problem i.e. ﬁnding tile largest area binary transaction database. sqs-search also uses sequences summarize data best small informative sequences achieve best compression mined directly database. uses encoding scheme explicitly punishes gaps assigning zero cost encoding non-gaps higher cost encoding larger gaps items pattern. eﬀective mining informative patterns text canhandle interleaving patterns unlike gokrimp signiﬁcant drawback certain datasets e.g. patterns generated independent processes frequently overlap. related work mannila meek proposed generative model sequences ﬁnds partial orders describe ordering relationships items sequence database. sequences generated selecting subset items partial order learned inclusion probability arranging compatible random ordering. unlike model allow gaps generated sequences sequence generated single partial order unrealistic assumption practice. also existing research probabilistic models sequences especially using markov models. gwadera variable order markov model identify statistically signiﬁcant sequences. stolcke omohundro developed structure learning algorithm hmms learns number states topology. landwehr extended hmms handle ﬁxed number hidden processes whose outputs interleave form sequence. wood developed sequence memoizer variable order markov model pitman-yor process prior. also nevill-manning witten infer contextfree grammar sequences using sequitur algorithm. section formulate problem identifying interesting sequences useful explaining sequence database. first deﬁne preliminary concepts notation. item element universe indexes symbols. sequence simply ordered list items sequence subsequence another sequence denoted exist integers sequence database merely list sequences further sequence supported sequence sequence database note deﬁnition sequence contains single item important popular sequence type multiset generalization allows elements occur multiple times i.e. speciﬁc multiplicity example multiset element occurs twice multiplicity problem formulation work infer interesting subsequences database sequences interesting mean patterns useful helping human analyst understand important properties database interesting subsequences reﬂect important patterns data suﬃciently concise non-redundant suitable manual examination. criteria inherently qualitative reﬂecting fact goal data mining build human insight understanding. quantify criteria operationalize notion interesting sequence sequences best explain underlying database probabilistic model sequences. speciﬁcally generative model i.e. model starts interesting subsequences generates sequence database goal infer likely generating under chosen generative model. want model simple possible powerful enough capture correlations items sequences. simple model follows iteratively sample subsequences randomly interleave form database sequence associate subsequence probability sample indicator variable bernoulli include however wish include subsequence sequence need sampling multiplicity simplest change generating distribution bernoulli e.g. categorical sample multiplicity categorical vector probabilities entry multiplicity deﬁne generative model formally next section. generative model note easily extend algorithm mine sequences sets items extending subsequence operator handle general ‘sequences’. sequences could generated interleaving together subsequences complicated fact multiset contain multiple occurrences subsequence makes eﬃcient computation impractical. however turns compute straightforward upper bound since clearly bounded possible permutations items subsequences bound attained contains distinct singleton sequences without repetition. formally conveniently gives non-trivial lower-bound posterior which want maximize posterior precisely want. moreover lower bound acts additional penalty strongly favouring non-redundant sequences np-hard problem general impractical solve directly practice. however show viewed special case maximizing submodular function subject submodular constraint approximately solved using greedy algorithm submodular function optimization. strictly speaking notion submodular function applicable sets however consider following generalization multisets deﬁne function speciﬁc case multiset supported interesting sequences i.e. sequences s.t. multiplicity given maximum number occurrences partition deﬁne |∪s∈cs|. re-state find non-overlapping multiset covering maximizes i.e. maximized. note construction. clearly monotone submodular multiset coverage function show non-monotone submodular. submodular observe discussed previous section propose simple directed graphical model generating database sequences interesting sequences. generative story model independently sequence database interesting sequence decide independently number times included i.e. sample multiplicity categorical note never need construct practice since require cardinality inference show next section eﬃciently compute approximation |p|. however sample eﬃciently merging subsequences time follows splice elements order randomly chosen points example generate sequences could course learn transition distribution subsequences model choose want force model explain sequential dependencies data. inference given interesting sequences denote vector sequences similarly denote list assuming fully determined evident generative model probability generating database sequence |πs| length evaluates otherwise. intuitively helps think inﬁnite vector augmented kleene star operator that example generate sequence calculating normalization constant problematic count number possible distinct maximizing posterior therefore problem maximizing submodular function subject submodular coverage constraint approximately solved applying greedy approximation algorithm greedy algorithm builds multiset covering repeatedly choosing sequence maximizes proﬁt adding covering divided number items covered covering |s|. order minimize time spent solving problem cache sequences coverings database sequence needed. note good theoretical guarantees approximation ratio achieved greedy algorithm maximizing monotone submodular function subject coverage constraint problem maximizing non-monotone submodular function subject coverage constraint best knowledge studied literature. however submodular optimization problem extension weighted cover problem greedy algorithm natural indeed observe good performance practice. learning given interesting sequences consider case variables model unknown. case hard algorithm parameter estimation latent variables. hard-em algorithm case merely simple layer inference algorithm suppose database sequences multisets supported interesting sequences hard algorithm given algorithm initialize natural choice simply support sequence. infer sequences using structural i.e. candidate sequence improves optimal value problem averaged across database sequences. interestingly implicit regularization eﬀects here. firstly observe candidate added model corresponding term added log-likelihood database sequences support. large sequence databases amounts signiﬁcant penalty candidates practice. secondly observe last term acts additional penalty strongly favouring non-redundant sequences. estimate maximum beneﬁt including candidate must carefully choose initial value avoid getting stuck local optimum. infer good force candidate explain database sequences supports initializing update probability corresponding actual usage inferred coverings. given interesting sequences corresponding probabilities along database sequences iteration structural algorithm given algorithm below. occasionally hard-em algorithm assign zero probability singleton sequences cause greedy algorithm able fully cover database sequence using interesting sequences case simply re-seed necessary singletons. finally practice store candidates rejected structural-em check potential candidate eﬃciency. candidate generation structural-em algorithm requires method generate candidate sequences considered inclusion interesting sequences possibility would algorithm recursively suggest larger sequences starting singletons however preliminary experiments found eﬃcient method. reason take slightly diﬀerent approach recursively combine interesting sequences highest support ﬁrst candidate generation algorithm likely propose viable candidate sequences earlier practice heuristic works well. inferred model variables able rank retrieved sequences natural rankings employ strengths weaknesses. obvious approach rank sequence according probability model however disadvantage strongly favouring frequent sequences rare ones issue would like avoid. alternative rank retrieved sequences according interestingness model ratio database sequences explain database sequences support. think interestingness measure necessary sequence model higher interestingness supported database sequences sequence explains. thus interestingness provides balanced measure probability expense missing frequent sequences explain database sequences support. deﬁne interestingness formally follows. close well-known connection probabilistic modelling minimum description length principle used gokrimp particularly nice explanation). given probabilistic model single database sequence shannon’s theorem optimal code model encode using approximately bits. ﬁnding patterns maximizes probability data also ﬁnding patterns minimize description length. conversely encoding scheme implicitly deﬁnes probabilistic model. given encoding scheme assigns database sequence string bits deﬁne optimal code interpreting previous subsequence mining methods terms implicit probabilistic models provides interesting insights methods. encoding database sequence used interpreted probabilistic model analog similar rameter optimization step need performed every iteration fact eﬃcient suggest several candidate sequences optimizing parameters. operations database sequences algorithm trivially parallelizable perform m-steps hard structural algorithms parallel. along additional terms correspond description lengths indicating presence absence gaps usage sequence additionally contains explicit penalty encoding patterns encourages smaller number patterns. probabilistic model interpreted prior distribution patterns. also prior distribution content patterns similar unigram model encourages patterns contain common elements. addition description length used gokrimp also cost penalizes sequences large gaps. gokrimp employs greedy heuristic compressing sequence empty sequence iteratively extended frequent item statistically dependent contrast iteratively extends sequences frequent sequence candidate generation step enables quickly generate large candidate sequences consider performing statistical test sequence extending sequence however proved computationally prohibitive. interleaving. cannot mine subsequences interleaved thus struggles datasets consist mainly interleaved subsequences gokrimp handles interleaving using pointer scheme explicitly encodes location subsequence within database. partition function allows handle interleaving subsequences withneeding explicitly encode positions also serves additional penalty number elements subsequences used explain database sequence. penalties. gokrimp explicitly punish gaps sequential patterns. adding penalty would require trivial modiﬁcation algorithm namely updating cost function algorithm pursue observe excellent results without encoding patterns. gokrimp contain explicit penalty term description length pattern database corresponds prior distribution patterns. experiments practice explicit prior distribution necessary good results. would possible incorporate trivial change algorithm particular computing score improvement candidate structural step. encoding pattern absence. also observe that view mdl-type method presence pattern also absence explicitly encoded result implicit penalty adding many patterns model need code table would serve explicit penalty greater model complexity. section perform comprehensive quantitative qualitative evaluation ism. synthetic datasets show returns list sequential patterns largely non-redundant contains spurious correlations scales linearly number sequences dataset. real-world datasets show ﬁnds patterns consistent interpretable highly relevant problem hand. moreover show able mine patterns achieve good accuracy used binary features real-world classiﬁcation tasks. datasets real-world datasets numerical evaluation alice dataset consists text lewis carrol’s alice wonderland tokenized sentences using stanford document preprocessor stop words punctuation deliberately retained. gazelle dataset consists sequences clickstream data e-commerce website used kdd-cup competition jmlr dataset consists abstracts journal machine learning research previously used evaluation gokrimp algorithms sequence list stemmed words text stop words removed. sign dataset list american sign language utterances utterance contains number gestural grammatical ﬁelds last datasets listed table ﬁrst introduced evaluate classiﬁcation accuracy mined sequential patterns used features. datasets converted time interval sequences sequences items considering start unique interval distinct items ordering items according time. results dataset iterations priority queue size candidates. runtime number non-singleton sequential patterns returned given right-hand side table also investigated scaling number sequences database increases using model trained sign dataset section generate synthetic sequence databases various sizes. iterations databases figure generating patterns gokrimp returned many patterns generating. top-k patterns mined bide contained successively less generating patterns increased. intention draw conclusions performance algorithms experimental setup naturally favours ism. instead compare patterns gokrimp real-world data next sections. pattern redundancy sets sequential patterns returned gokrimp bide actually are. suitable measure redundancy single sequence minimum edit distance mined sequences set. averaging distance across sequences obtain average inter-sequence distance similarly also calculate average number sequences containing mined sequences provides anmeasure redundancy. finally also look number unique items present mined sequences gives indication diverse gokrimp bide datasets table report results three aforementioned redundancy metrics non-singleton sequential patterns algorithm table average sequences larger inter-sequence distance smaller number containing sequences larger number unique items clearly demonstrating less redundant gokrimp bide. predictably bide sequences redundant average inter-sequence distance classiﬁcation accuracy property patterns mined data usefulness real-world applications. keeping previous work focus classiﬁcation tasks important applications pattern mining algorithms. speciﬁcally consider task classifying sequences database using mined sequential patterns binary features. therefore performed -fold cross validation using support vector machine classiﬁer classiﬁcation datasets table top-k patterns mined gokrimp bide features. used linear classiﬁer libsvm library default parameters. additionally used top-k frequent singleton patterns baseline classiﬁcation tasks. resulting plots classiﬁcation accuracy datasets algorithms given figure patterns mined perform best exhibiting highest classiﬁcation accuracy four datasets closely followed gokrimp performs surprisingly well considering struggles return patterns. three consistently outperform bide singletons baseline exhibit similar performance other. therefore conclude sequential patterns mined indeed useful real-world applications. pattern interpretability figure precision recall algorithm synthetic database using top-k patterns threshold. note single point top-left gokrimp near zero precision recall. plotted curve -point interpolated precision. scaling linear expected. experiments performed machine intel xeon .ghz cpus ram. evaluation criteria evaluate along gokrimp bide according following criteria spuriousness assess degree spurious correlation sequence-cover formulation algorithm naturally favours adding sequences model whose items co-occur sequence database. would therefore expect largely avoid suggesting sequences uncorrelated items return meaningful patterns. verify case validate inference procedure check able recover sequences used generate synthetic database. obtain realistic synthetic database sampled sequences generative model trained sign dataset able measure precision recall algorithm i.e. fraction mined patterns generating fraction generating patterns mined respectively. figure shows precision-recall curve gokrimp bide using top-k mined sequences threshold. clearly able mine algenerating patterns almost patterns mined generating despite fact generated database contain many subsequences present original dataset nature ‘subsequence interleaving’ generative model. provides good validation ism’s inference procedure underlying generative model also demonstrates returns spurious patterns. comparison returned small table average inter-sequence distance average containing sequences unique items non-singleton sequences returned algorithms datasets. larger inter-sequence distances smaller containing sequences indicate less redundancy. returned less non-singleton sequences. jmlr dataset compare top- non-singleton patterns mined gokrimp bide table immediately obvious table bide patterns almost exclusively permutations frequent items uninformative. reason omit bide consideration next dataset. patterns mined gokrimp informative containing technical concepts support vector machine commonly used phrases state art. alice dataset compare twenty- non-singleton patterns mined gokrimp ﬁrst three columns table time clearly patterns mined considerably informative. contain collocated words phrases mock turtle dear correlated words spoke head well correlated punctuation gokrimp hand mine collocated words spurious punctuation stop words e.g. prepending nouns commas phrases. illustrate notable diﬀerence also show top- non-singleton patterns exclusive algorithm last three columns table clearly gokrimp least informative exclusive patterns predominantly combinations stop words punctuation mostly prepends appends informative exclusive patterns punctuation stop words whereas algorithm returns purely correlated words. note particular struggles return patterns balanced parentheses since punishes large gaps cannot handle interleaving patterns enclose. really power statistical model underlying able discern spurious punctuation genuine phrases. parallel dataset finally consider synthetic dataset demonstrates ability handle interleaving patterns. following generate synthetic dataset item sequence generated independent parallel processes i.e. process generates item possible items order. step generator chooses random generates item using process sequence length sequence split sequences length dataset know figure linear classiﬁcation accuracy using top-k sequences returned algorithm binary features. shows consistently good performance comparable gokrimp. mined sequences containing mixture items diﬀerent processes spurious. enables calculate recall i.e. fraction processes present true patterns mined algorithm. plot recall top-k patterns mined gokrimp figure ﬁrst-k patterns mined gokrimp able mine true patterns processes returns patterns processes. paper taken probabilistic machine learning approach subsequence mining problem. presented novel subsequence interleaving model called interesting sequence miner infers subsequences best compress sequence database without design encoding scheme. demonstrated eﬃcacy approach synthetic real-world datasets showing returns diverse patterns previous approaches retaining comparable quality. future would like extend approach many promising application areas well considering advanced techniques parallelization.", "year": 2016}