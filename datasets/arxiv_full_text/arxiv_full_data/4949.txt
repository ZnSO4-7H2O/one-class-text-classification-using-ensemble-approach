{"title": "A First Empirical Study of Emphatic Temporal Difference Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper we present the first empirical study of the emphatic temporal-difference learning algorithm (ETD), comparing it with conventional temporal-difference learning, in particular, with linear TD(0), on on-policy and off-policy variations of the Mountain Car problem. The initial motivation for developing ETD was that it has good convergence properties under off-policy training (Sutton, Mahmood and White 2016), but it is also a new algorithm for the on-policy case. In both our on-policy and off-policy experiments, we found that each method converged to a characteristic asymptotic level of error, with ETD better than TD(0). TD(0) achieved a still lower error level temporarily before falling back to its higher asymptote, whereas ETD never showed this kind of \"bounce\". In the off-policy case (in which TD(0) is not guaranteed to converge), ETD was significantly slower.", "text": "paper present ﬁrst empirical study emphatic temporaldifference learning algorithm comparing conventional temporaldifference learning particular linear on-policy off-policy variations mountain problem. initial motivation developing good convergence properties -policy training also algorithm on-policy case. on-policy off-policy experiments found method converged characteristic asymptotic level error better achieved still lower error level temporarily falling back higher asymptote whereas never showed kind bounce. off-policy case guaranteed converge) signiﬁcantly slower. consider problem learning value function markov decision process given policy. agent environment interact discrete time steps environment state agent selects action result environment emits reward next state st+. states represented agent feature vectors seek parameter vector inner product policy selecting future actions. fact actions selected alternate policy training called on-policy whereas policies different training called off-policy. consider special case emphatic temporal difference learning algorithm bootstrapping complete discounting ∀s). studying methods complete bootstrapping suitable case differences maximized. approaches methods behave similarly point become equivalent setting algorithm step size parameter. followon trace according update time step emphasized de-emphasized. obtained removing ﬁrst equation. different even on-policy case always thorough explanation section show although initial motivation developing good convergence properties off-policy training also different algorithm on-policy training. emphasize difference present simple example convergent on-policy training long known converges constant value on-policy training surprisingly assured converge varying even on-policy training. recently presented counterexample state dependent on-policy convergent. example simple markov decision process consisting states system simply moves state another cycle. process starts states equal probability. shown below matrix problem positive deﬁnite. moreover eigenvalues matrix negative real parts thus diverges case. experimental study used variation mountain control problem form prediction problem. original mountain problem -dimensional space position velocity three actions full throttle forward full throttle backward throttle. episode starts around bottom hill reward time steps pasts goal hill ends episode. task undiscounted. variation mountain problem ﬁxed target policy always push towards direction velocity push direction velocity call variation mountain problem ﬁxed-policy mountain testbed. performance measure used estimation mean squared value error reﬂects mean squared difference true value function estimated value function weighted according often state visited state space following behavior policy state distribution change steps taken stationary distribution achieved limit. agent started state followed target policy termination times time return computed recorded. returns averaged result used true value state value function learning algorithm’s estimation value function state shown applied on-policy on-policy methods ﬁxed-policy mountain testbed. created many instances method changing step size parameter. approximate value function problem used tile coding tilings tiles each. algorithm instance initialized weight vector episodes. whole process repeated runs. produce learning curves instance methods computed error measure episode averaged runs. figure also performed parameter study asymptotic performance methods. averaged error last episodes computed average standard error runs. figure parameter study computed episode terminal state reached averaged last episodes. done different value standard errors different instances shown error bars ﬁgure visible small sizes. compare performance on-policy on-policy ﬁrst need understand errors changed number episodes increased. etd’s error decreasing function number episodes sufﬁciently small values however showed bounce reaching error temporarily falling back higher asymptotic error. depth asymptotic level bounce depend duration did. smaller later bounce result took episodes converge smaller values figure outperformed terms asymptotic performance. instances smaller values step size converge within episodes. figure conﬁrm converged smaller values repeated experiments episodes computed error measure. error measure changed instances converged within episodes. light dark blue curves figure show performance different instances method episodes respectively. obvious instances converge instances larger values step size did. also applied off-policy off-policy ﬁxed-policy mountain testbed. case target policy policy on-policy case behavior policy choose random action time according target policy time. different instances method created different step size parameters. instance method episodes whole process repeated runs. learning curves off-policy case presented figure parameter study results figure parameter study computed episode terminal state reached averaged last episodes. done different value standard errors different instances shown error bars ﬁgure. visible small sizes. analogous on-policy case method advantages disadvantages. achieved better asymptotic performance whenever converged. compared could take advantage using larger values step size thus converged signiﬁcantly faster etd’s step size values small control method’s high variance larger step size range converged; however converged short range step size similar on-policy study showed bounce every value step size performed ﬁrst systematic empirical study emphatic temporal difference learning method showed used problem relatively large state space promising results. although originally proposed off-policy method also used reliable on-policy algorithm. according results seems slow off-policy case; however achieves better asymptotic performance on-policy off-policy cases. spite fact experiments limited variation mountain problem believe observations lead better understanding methods. yu’s counter example along experimental results motivate study on-policy off-policy method. authors thank huizhen insights speciﬁcally providing counterexample. gratefully acknowledge funding alberta innovates technology futures natural sciences engineering research council canada.", "year": 2017}