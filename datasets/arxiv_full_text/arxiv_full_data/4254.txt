{"title": "The Shape of Art History in the Eyes of the Machine", "tag": ["cs.AI", "cs.CV"], "abstract": "How does the machine classify styles in art? And how does it relate to art historians' methods for analyzing style? Several studies have shown the ability of the machine to learn and predict style categories, such as Renaissance, Baroque, Impressionism, etc., from images of paintings. This implies that the machine can learn an internal representation encoding discriminative features through its visual analysis. However, such a representation is not necessarily interpretable. We conducted a comprehensive study of several of the state-of-the-art convolutional neural networks applied to the task of style classification on 77K images of paintings, and analyzed the learned representation through correlation analysis with concepts derived from art history. Surprisingly, the networks could place the works of art in a smooth temporal arrangement mainly based on learning style labels, without any a priori knowledge of time of creation, the historical time and context of styles, or relations between styles. The learned representations showed that there are few underlying factors that explain the visual variations of style in art. Some of these factors were found to correlate with style patterns suggested by Heinrich W\\\"olfflin (1846-1945). The learned representations also consistently highlighted certain artists as the extreme distinctive representative of their styles, which quantitatively confirms art historian observations.", "text": "machine classify styles art? relate historians’ methods analyzing style? several studies shown ability machine learn predict style categories renaissance baroque impressionism etc. images paintings. implies machine learn internal representation encoding discriminative features visual analysis. however representation necessarily interpretable. conducted comprehensive study several state-of-the-art convolutional neural networks applied task style classiﬁcation images paintings analyzed learned representation correlation analysis concepts derived history. surprisingly networks could place works smooth temporal arrangement mainly based learning style labels without priori knowledge time creation historical time context styles relations styles. learned representations showed underlying factors explain visual variations style art. factors found correlate style patterns suggested heinrich w¨olfﬂin learned representations also consistently highlighted certain artists extreme distinctive representative styles quantitatively conﬁrms historian observations. style central discipline history. word style used refer individual manner someone makes something example rembrandt’s style painting. style also refers groups works similar typology characteristics impressionist style high renaissance style. historians identify characterize deﬁne styles based evidence physical work itself combination analysis cultural historical features time place made. although style know exists still central agreed upon theory style comes about changes. best scholars history written persuasively importance style discipline concomitant difﬁculty deﬁning explaining changes connoisseurship proven effective means detect styles various artists differentiate style categories distinctions larger movements periods. recent research computer vision machine learning shown ability machine learn discriminate different style categories renaissance baroque impressionism cubism etc. reasonable accuracy however classifying style machine interests historians. instead important issues machine learning tell characteristics style identiﬁed patterns sequence style changes. ability machine classify styles implies machine learned internal representation encodes discriminative features visual analysis paintings. however typical machine uses visual features interpretable humans. limits ability discover knowledge results. study’s emphasis understanding machine achieves classiﬁcation style internal representation uses achieve task representation related history methodologies identifying styles. achieve understanding utilized formulations style pattern style change history theory heinrich w¨olfﬂin w¨olfﬂin’s comparative approach formal analysis become standard method history pedagogy. w¨olfﬂin chose separate form analysis discussions subject matter expression focusing visual schema works visible world crystallized certain forms w¨olfﬂin identiﬁed pairs works demonstrate style differences comparison contrast exercises focused principles features. w¨olfﬂin used method differentiate renaissance baroque style visual principles linear/painterly planar/recessional closed form/open form multiplicity/unity absolute clarity/relative clarity. w¨olfﬂin posited form change pattern differentiation style types changes come certain sequences. advances computer vision machine learning availability comprehensive datasets images positioned approach history predictive science relate means determining questions style machine results. manovich argued computational methods providing radical shift scale fact continues humanities’ traditional methodologies nearly impossible apply empirically test w¨olfﬂin’s methods style differentiation analysis developments computer science. human would assemble number examples needed prove value methods ﬁnding discriminative features. could anyone amass dataset necessary demonstrate usefulness model processing style description sequencing beyond immediate examples renaissance baroque pairs. chose w¨olfﬂin’s theory emphasis formal discriminative features compare/contrast logic system qualities make conducive machine learning. today historians wide variety methods solely focused form type analysis paper w¨olfﬂin’s approach useful. deep convolutional neural networks recently played transformative role advancing artiﬁcial intelligence evaluated large number state-of-the-art deep convolutional neural network models variants them trained classify styles. focused increasing interpretability learned presentation forcing machine achieve classiﬁcation reduced number variables without sacriﬁcing classiﬁcation accuracy. analyzed achieved representations linear nonlinear dimensionality reduction activation space visualization correlation analysis time w¨olfﬂin’s pairs. used collection digitized paintings train validate test models. utilized sets digitized paintings visualization correlation analysis achieved representations. particular used variants alexnet vggnet resnet originally developed task object categorization imagenet challenge raised state task introduced. adapted networks classifying style classes. study included varying training strategies varying network architecture data augmentation strategies. trained predict styles based noisy discrete style labels without given notion time machine encoded history smooth chronology. learned representation explained based handful factors. ﬁrst modes variations aligned concepts linear painterly planer recessional suggested heinrich w¨olfﬂin quantitatively explain variance history temporal progression correlates radially across modes importance results show selected historian’s theories style change quantiﬁably veriﬁed using scientiﬁc methods. results also show style appears subjective issue computationally modeled objective means. figure modes variations activation subspace showing smooth temporal transition correlation w¨olfﬂin’s concepts. first second modes variations alexnet+ model paintings color-coded date creation. ﬁrst mode seems correlate ﬁgurative dominant till impressionism non-ﬁgurative distorted ﬁgures abstract dominate century styles. another interpretation dimension reﬂects w¨olfﬂin’s concept plane recession axis correlates w¨olfﬂin’s concept plane recession pcc. vertical axis correlates linear painterly concept angular coordinate exhibits strong correlation time correlation w¨olfﬂin’s concepts. contrast typical object classiﬁcation images problem style classiﬁcation different challenges highlight here. style necessarily correlated subject matter corresponds existence certain objects painting. style mainly related form correlated features different levels medium high-level. result necessary networks perform better extracting semantic concepts object categories would perform well style classiﬁcation. literature object classiﬁcation deeper networks shown perform better since facilitate richer representations learned different levels features. know deeper network necessarily better style classiﬁcation domain. remains something discover empirical study. however challenge context style classiﬁcation lack images scale magnitude similar imagenet largest publicly available dataset order images. limitation copyright issue integral domain art. moreover collecting annotation domain hard since requires expert annotators typical crowd sourcing annotators qualiﬁed. another fundamental difference styles lend discrete mutually exclusive classes supervised machine learning. style transition time typically smooth style labels after-the-fact concepts imposed historians sometimes centuries later. paintings elements belongs multiple styles therefore necessarily identiﬁable unique style class. paintings also come wide variety sizes. canvas span couple hundreds square feet small couple square inches. aspect ratio vary significantly well. typical requires input resized ﬁxed size retinal array. bound introduce geometric distortions affect composition loss details related surface texture carrying essential information brush strokes. composition brush strokes essential identify style. paper address solving issues. mainly report behavior studied model style classiﬁcation despite limitation address issues. training-testing trained validated tested networks using paintings publicly available wikiart dataset. collection images paintings artists ranging ﬁfteenth century contemporary artists. several prior studies style classiﬁcation used subsets dataset originally wikiart style classes. purpose study reduced number classes classes merging ﬁne-grained style classes small number images example merged cubism analytical cubism synthetic cubism cubism class. table shows list style classes used mapping original wikiart classes. also ignored symbolism style class since wikiart collection class contains various paintings different styles erroneously labeled symbolism. general visual inspection notice style labels wikiart collection noisy accurate. however remains largest publicly available collection. available collections might cleaner labels typically much smaller size purpose training deep network. excluded collections images sculptures photography. total number images used training validation testing images. split data training validation test sets visualization used another smaller dataset containing images paintings artchive dataset analyze visualize representation. previous researches used dataset denote dataset paper visualization dataset wikiart collection much bigger size dataset contains better representation important works western artists. therefore mainly visualize analyze learned representation. excluded collections images sculptures images containing partial details paintings. also collected historian’s rating annotations w¨olfﬂin’s pairs paintings data correlation analysis. visualization also used painting wikiart dataset visualization analysis representations. included paintings date annotations purposes visualization temporal correlation. chosen visualization sets complementary properties. artchive visualization represents knowledge inﬂuential important artists wikiart arbitrarily chosen based available public domain. artchive dataset lacks paintings century broad sampling early paintings post-war century. contrast wikiart dataset lacking certain works historians would consider important nevertheless densely samples broad range styles. addition wikiart dataset large bias towards century post wwii works. performed comprehensive comparative study several deep convolutional networks adapted task style classiﬁcation. purpose paper report results three main networks alexnet vggnet resnet well variants them. models originally developed task object recognition imagenet challenge raised state introduced. non-expert reader brieﬂy summarize main features models. deep convolutional networks general consist sequence layers artiﬁcial neural units different types. convolutional layers apply learned ﬁlters location input image. convolutional layers interleaved pooling layers aggregate responses convolution layers. typically sequence convolution pooling layers results re-representing visual information image responses large number learned ﬁlters applied wider wider regions image information propagates deeper network. ﬁlters learned tuned machine response task hand. finally responses passed sequence fully connected layers acts classiﬁer. alexnet architecture consists consecutive convolution layers interleaved pooling layers followed three fully connected layers resulting nodes representing object classes imagenet. convolutional ﬁlters different sizes layer starting ﬁlters reduced ﬁlter size following layers. contrast vggnet adapted architecture ﬁxed size ﬁlters deeper sequence convolutional layers. residual networks introduced shortcut connection convolution layers outputs later layers results much deeper architectures reaching layers. study general models ﬁnal softmax layer originally designed classes imagenet removed replaced layer softmax nodes style class. modes training evaluated training models scratch wikiart data described above; using pre-trained model imagenet data ﬁnetuned wikiart data. fine-tuning standard practice adapting well-performing pre-trained models different domain. large number nodes fully connected layer allows representation project data high dimensional space classiﬁcation would easy without forcing similar paintings across styles come closer representation. increase interpretability representation force network achieve classiﬁcation lower dimension representation. achieve this training network fully connected layers added reduced number nodes. reduced dimensional layers force representation smaller number degrees freedom turn forces paintings across styles come closer representation based similarity. particular added layers nodes models models ﬁne-tuned adjust weights layers. shown later adding dimensionality reduction layers affect classiﬁcation accuracy. experiments showed gradually reducing number nodes fully connected layers forced network achieve smoother interpretable representation. important emphasize achieve effect without reducing accuracy; layers added network trained models ﬁne-tuned. training full architecture extra layers whether scratch ﬁne-tuned typically doesn’t result converging similar accuracy. analyzed modes variations activation layer learned representation using principle component analysis also analyzed activation space using independent component analysis analyzed nonlinear manifold activations laplacian eigen embedding chose techniques widely used representatives linear nonlinear dimensionality reduction techniques providing different insight learned representation. also performed correlation analysis dimensions activation space time well ground truth w¨olfﬂin’s concepts. table shows classiﬁcation accuracy different models using pre-training ﬁnetuning training scratch. cases pre-trained ﬁne-tuned networks achieved signiﬁcantly better results counterparts trained scratch surprising consistent several models adapted ﬁne-tuned pre-trained networks different domains. however learned ﬁlters network trained scratch domain signiﬁcantly different ones typically learned imagenet typically shows gabor-like blob-like ﬁlters. figure shows visualization ﬁlters alexnet trained imagenet compared ﬁlters trained wikiart style classiﬁcation. hard interpret ﬁlters trained style classiﬁcation observe oriented-edge-like ﬁlters except horizontal edge ﬁlter. emphasizes difference nature problems suggests better performance ﬁne-tuned models could out-performed sufﬁcient data available train style-classiﬁcation network scratch data only. table comparison classiﬁcation different models different training methodologies trained scratch pre-trained fine-tuned network alexnet alexnet+ adding reduced layers vggnet conv layers layers vggnet+ adding reduced layers resnet figure comparison ﬁlters learned object classiﬁcation style classiﬁcation. left typical ﬁlters alexnet trained imagenet object classiﬁcation. center right filters alexnet trained style classiﬁcation using ﬁlters. increasing depth network added accuracy alexnet convolutional layers resnet convolutional layers. case learning scratch increasing depth improve results resnet layers performed worse alexnet convolution layers. vggnet convolutional layers performed better alexnet. increasing depth vggnet improve results. limited gain performance increase depth conjunction difference learned ﬁlters suggests shallow network might sufﬁcient style classiﬁcation along better ﬁlter design. experiments showed adding extra fully connected layers gradually reducing number nodes forces networks achieve smoother interpretable representation. large number nodes fully connected layers allows representation project data high dimensional space classiﬁcation would easy without necessarily enforcing similar paintings across styles come closer representation based similarity. quantiﬁed phenomenon examining dimensionality subspace activation visualization dataset using measures number components needed preserve variance. variance retained ﬁrst dimensions. also evaluated accuracy expanded models added reduced layers resulted loss accuracy. cased added layers enhanced accuracy table shows adding reduced-dimension layers effectively consistently reduced dimensionality subspace data preserving classiﬁcation accuracy. reduction signiﬁcant alexnet dimensions retained variance compared dimensions cases ﬁne-tuned learned scratch networks respectively around variance retained ﬁrst dimension. interestingly representation achieved vggnets already reduced dimension subspaces compared subspace number principle components cumulatively retaining variance. retained variance percentage variance retained ﬁrst principle components. resnet layers. number nodes last pooling layer. figure cumulative retained variance ﬁrst dimensions activation space last layer left models adding dimensionality reductions layers. right original models. alexnet resnet. however adding reduced dimension layers signiﬁcantly lowered subspace dimension improving classiﬁcation accuracy maximum reduction subspace dimensionality resnet dimension subspace retaining variance reduced variance ﬁrst dimensions. figure shows cumulative retained variance ﬁrst dimensions activation subspace models adding reduced dimensions layers. deﬁne activation space given fully connected layer output layer prior rectiﬁed linear functions. particular paper show analysis activation last reduced dimension fully connected layer prior ﬁnal classiﬁcation layer consists nodes. activations rectiﬁed linear functions networks. learned representation machine shows underlying factors explain characteristics different styles history. using principle component analysis fewer modes variations explain variance visualization studied models additional reduced fully connected layers. resnet vggnet number modes respectively. networks ﬁrst modes variations explained variance various models visualization moreover clear visualizations linear nonlinear embedding activation manifold various models dated prior plane consistent results achieved analyzing painting wikiart data found subspaces dimensions retain variance activation alexnet+ vggnet+ resnet+ respectively consistency results studied networks datasets imply existence small number underlying factors explaining representation intrinsic property history artifact particular dataset model. learned representations show smooth transition time. figure shows paintings visualization projected ﬁrst modes variations activations last reduced dimension fully connected layer different networks. paintings color-coded based date creation. plots consistently show networks learned smooth temporal representations reﬂect historical progress style. despite fact networks trained images discrete style labels. information provided painting created style took place artist created painting styles related despite lack information learned representations clearly temporally smooth reﬂect high level correlation time. figure first modes variations different learned representations temporal correlations. every point painting color-coded year creation. dataset vs-i. data arranged plot radial clock-wise around center make complete circle projection starting renaissance ending abstract art. progress following plot clock-wise italian northern renaissance bottom baroque neo-classicism romanticism reaching impressionism followed post impressionism expressionism cubism. cycle completes abstract art. explanation network arranged representation become clearer next show results nonlinear activation manifold analysis. quantiﬁed correlation time different ways. measure pearson correlation coefﬁcients principle components well dimension nonlinear manifold embedding. table shows temporal correlation using ﬁrst dimensions ﬁrst embedding dimension different models. also computed correlation radial coordinate painting projection ﬁrst principle components– time. temporal correlation obtained representation consistent among different networks. taking ﬁne-tuned alexnet+ example ﬁrst dimensions strong correlation time pearson correlation coefﬁcients respectively. linear combination dimensions convex weights proportional pearson correlation coefﬁcients results single dimension correlations time cycle around plot renaissance till abstraction suggests angular coordinate around plot would reﬂect correlation time. fact angular coordinates time. strongest temporal correlation case resnet radial correlations. conclusion consistent among networks tested. conclusion shows style changes smoothly time proves noisy discrete style labels enough networks recover temporal arrangement mainly visual similarities encoded learned representation continuous activation space. strong temporal correlation also exists modes variations visualization set-ii. however case necessarily ﬁrst modes variation ones highest temporal correlation. bias dataset towards century work. table shows time dimensions three tuned networks. table analysis modes variations activations paintings wikiart collection three models correlation time time-correlated modes highlighted bold. visualizing modes variations reveals interesting ways networks able consistently capture evolution characteristics styles. take ﬁne-tuned alexnet+ example however similar results noticed models. ﬁrst mode seems correlate ﬁgurative dominant till impressionism non-ﬁgure distorted ﬁgures abstract dominates century styles. another interpretation dimension reﬂects w¨olfﬂin’s concept planar recession axis correlates w¨olfﬂin’s concept planar recession pearson correlation coefﬁcient. lesser degree horizontal axis correlates closed open linear painterly quantitative correlation clearly noticed looking sampled paintings shown figure horizontal axis characterized planar open linear form right recession closed painterly left table correlation w¨olfﬂin’s concepts. pearson correlation coefﬁcient ﬁrst dimensions ﬁrst dimensions activation space w¨olfﬂin’s concepts. concepts maximum correlation dimension shown. second mode variations pearson correlation coefﬁcient linear painterly concept. clearly smooth transition linear form renaissance bottom towards painterly form baroque extreme case painterly impressionism top. transition back linear form abstract styles. projecting data dominant modes variations aligned plane recession linear painterly gives explanation representation correlates time radial fashion. correlation ﬁrst modes variations concepts planar recession linear painterly consistent among representations learned tested networks whether pre-trained trained scratch. cases ﬁrst mode variations correlates concept planar recession second mode correlates linear painterly. table shows pearson correlation coefﬁcients concepts. full correlation results dimensions shown appendix. fourth ﬁfth dimensions alexnet+ representation spread away strongly renaissance baroque styles styles perspective ﬁfth dimension particular correlates relative absolute clarity unity multiplicity open closed painterly linear form bottom consistent w¨olfﬂin’s theory since suggested exactly contrast concepts highlight difference renaissance baroque. figure renaissance style appears baroque appears bottom ﬁgure impressionism cubism half plot since share many concepts baroque. fourth dimension seems separate impressionism cubism right abstraction left. figure separation renaissance baroque fourth ﬁfth modes variations show separation renaissance baroque. correlation modes variations w¨olfﬂin’s concepts conﬁrms w¨olfﬂin’s hypothesis. looking dimensions embedded activation manifold notice orthogonal planes characterizing prior plane spans renaissance baroque orthogonal plane spans impressionism post-impressionism. figure representation could discover distinctive artists style. ﬁrst three modes variations vggnet activations. distinctive artists representing style identiﬁed representation pulled away cloud center. factorization activation space using independent component analysis maximally independent axes show alignment styles three modes variations network activation works wikiart collection interestingly modes variations explaining data correlate w¨olfﬂin’s concepts. learned representations ﬁrst modes variation always close-to-zero linear correlation w¨olfﬂin’s concepts. notable example ﬁfth dimension embedded activation manifolds separates impressionism post-impressionism almost zero linear correlation w¨olfﬂin’s pairs implies separation styles interpretable terms w¨olfﬂin’s pairs. visualizing different representations shows certain artists consistently picked machine distinctive representatives styles extreme points along dimensions aligned style. visible ﬁrst three modes variations representation learned vggnet shown figure-a retains variance besides observable temporal progress representation separates certain styles certain artists within style distinctively cloud center non-orthogonal axes. northern renaissance yellow ellipse majority paintings sticking eyck d¨urer. baroque black ellipse represented rubens rembrandt velzquez. orange ellipse impressionism base pissarro caillebotte manet least painterly type ending monet renoir painterly spike. circles post-impressionism particular dominated gogh c´ezanne forms base spike cubism light blue ellipse. spike dominated picasso braque gris; goes abstract cubist works. interestingly representation separates rousseau marked green ellipse mainly dominated work. consistent results seen plotting embedding activation whole paintings wikiart dataset collection seen figure f-h. applying independent component analysis activation space results ﬁnding non-maximally independent non-gaussian orthogonal axes. figure shows factorization network activation space. here subspace retains variance factorized ica. results maximally independent components shown figure also show components time axis figure components aligned speciﬁc styles components contrast several styles factorization facilitates quantitatively discovering artists represent style works appear extreme points axes style. also highlights styles opposite along axes independent component analysis transforms activation embedding space transformed data become statistically independent much possible achieve ﬁnding non-orthogonal basis original activation space maximizing independence criteria. since algorithm used experiment limitation capture over-completeness implies existence greater number tails dimension space experiment found axes aligned styles. however still results show merits interesting historical facts provide quantitatively measurement them works artists representative figure independent component factorization activation network representation achieved ica. independent component time correlation w¨olfﬂin’s concepts. components aligned speciﬁc styles components contrast several styles fact representation highlights certain representative artist artists style among hundreds thousands paintings many artists style emphasizes quantitatively signiﬁcance particular artists deﬁning styles belong using nonlinear dimensionality reduction activation allows achieve embedding activation manifold visualization data sets reveals interesting observations well. particular used local linear embedding similar techniques based building graph data points point connected nearest neighbors using graph embed data dimensional space visualized. case data points activation given painting goes deep network measured last fully connected layer results embedding activation manifold. controlling number neighbors painting constructing graph allows controlling connections paintings. parameter small number results accentuating distinctive paintings trends. parameter large number allow paintings connect capture overall trend. ﬁrst dimension representation models exhibits strong temporal correlation shown table example ﬁrst dimension alexnet+ representation time resnet+ time. also similar modes variations ﬁrst dimensions shows strong correlation w¨olfﬂin’s concepts planar recession linear painterly respectively table looking representation learned alexnet+ example notice surface embedded activation manifold different styles arranged learned representation reﬂects temporal relation well stylistic similarity temporal relation between styles linear historical order since elements styles appear later styles. example linear temporal progress renaissance baroque romanticism realism impressionism. however also direct connection renaissance post-impressionism century styles. explained since linear planar concepts later styles echo elements renaissance style. interesting connection captured representation arrangement c´ezanne’s work connects impressionism cubism. clearly figures c´ezanne’s work acting bridge impressionism side cubism abstract side. historians consider c´ezanne ﬁgure style transition towards cubism development abstraction century art. bridge c´ezanne’s painting learned representation quite interesting quantiﬁable connection data metaphorical term. branching post-impressionism c´ezanne’s work clearly separates post impressionist expressionist works towards top. branch continues evolve connects early cubist works picasso braque well abstract works kandinsky. figures show another interesting connection renaissance modern captured learned representation. despite fact structure reﬂects smooth temporal progression interesting outlier progression. particular high renaissance northern renaissance mannerist paintings stick renaissance cluster left connect late early centuries. frequent similarity works across time results pulling inﬂuential works order placing closer inﬂuenced. ﬁgure works stick renaissance cluster left connect modernity mainly dominated paintings el-greco paintings d¨urer. among paintings el-greco signiﬁcantly stick laoco¨on saint ildefonso view toledo piet`a. also works raphael mantegna michelangelo group well. accentuate representation reducing number neighbors painting constructing manifold. small neighborhood construction results lining distinctive paintings thin structures accentuate trends. results version shown figure interesting connections discovered activation manifold. example activation manifold alexnet+. paintings color coded date surface activation manifold showing smooth transition time. transitions movements important connections. accentuated version manifold highlighting major styles renaissance baroque impressionism cubism abstract. c´ezanne’s bridge branching postfigure impressionism c´ezanne’s work clearly separates post-impressionist expressionist works towards top. branch continues evolve till connects early cubist works picasso braque well abstracts works kandinsky. thumbnails without labels plot c´ezanne. connection renaissance modern styles marked outliers temporal progress patterns certain works el-greco d¨urer raphael mantegna michelangelo. figure visualization distinct styles clearly accentuated among twenty styles machine learned renaissance baroque impressionism cubism abstract. show representative paintings styles order raphael’s catherine rembrandt’s self-portrait monet’s water lilies clouds braque’s violin candlestick malevich’s square c´ezanne’s bridge el-greco/d¨urer’s bridge accentuated representation well. interestingly c´ezanne’s connection el-greco/d¨urer connection appears consistently various representations learned different networks however manifested different forms. another interesting observation cubism appears unique style sticks singularity representations. paper presented result comprehensive study training various deep convolutional neural networks purpose style classiﬁcation. mainly emphasized analyzing learned representation understanding machine achieves style classiﬁcation. different representations learned machine using different networks shared striking similarities many aspects. clear small numbers factors explain figure orthogonal planes characterize prior learned representation. renaissance-baroque plane seems orthogonal impressionism-post-impressionism plane embedding activation manifold ﬁne-tuned alexnet+ model shown. different view dimensions shown. dimensions span renaissance-baroque differences. dimensions. dimension spans impressionism-post-impressionism differences. interestingly dimension small correlation w¨olfﬂin’s concepts. correlation w¨olfﬂin’s concepts results also indicate learned representation shows smooth temporal transition between styles machine discovered without notion time given training. interestingly studying modes variations representation showed radial temporal progress quantiﬁable correlation time starting renaissance baroque progressing impressionism post impression cubism closing loop century styles abstract coming back close renaissance. studying correlation modes variations w¨olfﬂin’s suggested pairs found that consistently learned models ﬁrst mode variation correlating concept plane recession second mode variation correlates concept linear painterly. correlation explains radial temporal progress loop closure renaissance century styles since share linearity planarity form. also studied activation manifold different representation learned machine also reveal smooth temporal progress captured representation well correlation w¨olfﬂin’s concepts. studying activation manifolds different neighborhood structure allowed discover different interesting connections history quantiﬁable role c´ezanne’s work bridge impressionism cubism-abstract art. another interesting connection connection renaissance modern styles expressionism abstract-expressionism works el-greco d¨urer raphael others. visualizing different representations shows certain artists consistently picked machine distinctive representatives styles belong extreme points along dimensions aligned style. example distinctively representative artists eyck d¨urer northern renaissance raphael italian renaissance rembrandt rubin baroque monet impressionism c´ezanne gogh post impressionism rousseau nave-primitivism picasso braque cubism malevich kandinsky abstract. quite known historians machine discovered highlighted artists amongst many others style distinctive representatives styles without prior knowledge quantiﬁable way. networks presented colored images therefore ability learn whatever features suitable discriminate styles might include compositional features contrast light dark color composition color contrast detailed brush strokes subject matter related concepts. particular networks pre-trained object categorization datasets might suggest potential bias towards choosing subject-matter-related features classiﬁcation. however visualizing learned representations reveals learned representations rule subject matter basis discrimination. clear noticing loop closure renaissance style dominated religious subject matter modern century styles abstract others. contrast loop closure suggests basis discrimination related concepts related form suggested w¨olfﬂin. implication networks ability recover smooth temporal progression history absence temporal cues given training absence temporal constraints putting paintings style closer achieve classiﬁcation suggests visual similarity main factor forces smooth temporal representation results study highlight potential role data science machine learning play domain history approaching history predictive science discover fundamental patterns trends necessarily apparent individual human eye. study also highlights usefulness re-visiting formal methods history pioneered historians w¨olfﬂin data science using tools computer vision machine learning. finally study offers insights characteristics functions style historians conﬁrming existing knowledge empirical providing machine-produced patterns connections exploration.", "year": 2018}