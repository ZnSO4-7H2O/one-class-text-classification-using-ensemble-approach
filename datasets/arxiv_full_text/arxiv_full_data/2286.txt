{"title": "Interpretable Deep Convolutional Neural Networks via Meta-learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for \"algorithmic fairness\" also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the deep CNN model, which leads to reliable interpretations.", "text": "interpretability requirement many applications crucial decisions made users relying model’s outputs. recent movement algorithmic fairness also stipulates explainability therefore interpretability learning models. successful contemporary machine learning approaches deep neural networks produce models highly non-interpretable. attempt address challenge proposing technique called cnn-inte interpret deep convolutional neural networks meta-learning. work interpret speciﬁc hidden layer deep model mnist image dataset. clustering algorithm two-level structure meta-level training data random forest base learning algorithms generate meta-level test data. interpretation results displayed visually diagrams clearly indicates speciﬁc test instance classiﬁed. method achieves global interpretation test instances without sacriﬁcing accuracy obtained original deep model. means model faithful deep model leads reliable interpretations. fast development sophisticated machine learning algorithms artiﬁcial intelligence gradually penetrating number brand ﬁelds unprecedented speed. outstanding problems hampering progress interpretability challenge. challenge arises models build machine learning algorithms used humans decision making particularly decisions subject legal consequences and/or administrative audits. human decision makers operating circumstances accept professional legal responsibility ensuing decisions assisted machine comprehend models. learning generally true areas like criminal justice health care terrorism detection education system ﬁnancial markets. trust model decision makers need ﬁrst understand model’s behavior evaluate reﬁne model using domain knowledge. even areas like book movie recommendations automated aids explanations recommendation error made could increase trust reliance systems. furthermore european general data protection regulation forthcoming june stipulates explainability automatically made decisions concerning individuals includes decisions made assisted machine learning models. hence growing demand interpretability machine learning algorithms. paper deﬁne interpretability model ability provide visual textual presentation connections input features output predictions. realize goal interpretability usually approaches. design algorithm inherently interpretable achieving competitive accuracy complex model. examples decision trees decision lists decision sets etc. disadvantage approach trade interpretability accuracy easy learn interpretable model expressing complex process high accuracy. approach sacriﬁce accuracy takes opposite approach ﬁrst builds highly accurate model without worrying interpretabilty subsequently uses separate re-representation techniques assist user understanding behavior algorithm. techniques could aforementioned relatively simple interpretable algorithms explain behavior complex model reasons given classiﬁer treated black classiﬁes given instance particular e.g. lime beta trepan deep learning methods lately successful image processing natural language processing. could categorized representation learning approach learns reﬁned features could improve model’s generalization ability. deep learning however highly noninterpretable. implement tool helps user understand hidden layers deep model work classify examples. results expressed graphs indicates sequential separations true class hypothesis. main contributions method follows compared lime provides local interpretation speciﬁc regions feature space method provides global interpretation test instances whole feature space. compared models apply inherently interpretable algorithms e.g. method advantage compromising accuracy model interpreted. produces reliable interpretation. resolve problems trusting prediction trusting model methods proposed explain individual predictions understand model’s behavior respectively local interpretable model-agnostic explanations submodular pick lime main idea lime inherently interpretable models interpret complex models locally. designed objective function minimize unfaithfulness complexity although stated paper objective function could interpretable models sparse linear models paper. based individual explanations generated lime design submodular pick algorithm sp-lime explain model whole picking number representative non-redundant instances. suggested coverage precision effort used evaluate results model interpretation. although lime achieves high precision effort coverage clear. words lime able explain speciﬁc prediction made using weights local model can’t indicate local region explanation faithful. solve problem anchor local interpretable model-agnostic explanations method introduced alime if-then rules used instead using weights linear model explain speciﬁc prediction idea based decision sets algorithm if-then rules easy comprehend good coverage. pointed trade interpretability accuracy machine learning algorithms terms inherently interpretable models rule-based models e.g. decision trees decision lists often preferred balance factors. decision lists usually considered interpretable decision trees if-then-else statements hierarchy structure. structure reduces extent interpretability interpret additional rule previous rules reasoned about. also rules list applied much narrow feature spaces makes multi-class classiﬁcation difﬁcult minority classes deserves equally good rules. motivates proposal decision sets algorithm produces isolated ifrules rule could independent prediction. realize this objective function takes account interpretability accuracy showed solving objective function nphard problem ﬁnds near-optimal solutions however decision set’ accuracy approaches random forest expressive power catches decision tree. black explanations transparent approximations introduced different lime aims local interpretation beta framework attempts produce global interpretation classiﬁer treated black classiﬁers. based previous work decision sets authors designed framework level decision sets taking account ﬁdelity unambiguity interpretability interactivity level structure outer if-then rules neighborhood descriptors inner if-then rules decision logic rules similar objective function built near-optimal solutions found. methodology could classiﬁed post-hoc interpretation trained model given main task interpret method close second approach mentioned fourth paragraph introduction section also different many ways. first model interpreted treated black directly interpret hidden layers deep cnn. second compared lime local interpretability method achieves global interpretability. similar lime also provide qualitative interpretation graphs visualize results. method interprets deep meta-learning ﬁrst brieﬂy introduces deep meta-learning discuss framework details. section introduces deep model going interpret. implement program using tensorflow tensorboard function draw structure deep construct fig. deep advanced machine learning algorithm image classiﬁcation. takes advantage two-dimensional structure input images. uses ﬁlters ﬁlter pixels input images generate higher level representations learnt model order improve performance. three major components deep convolutional layer pooling layer fully connected layer deep model usually stack layers. convolutional layer ﬁlter used compute products pixels input image speciﬁc position values ﬁlter producing single value output feature map. convolution operation completed ﬁlter slided across width height input image. following convolutional layer activation function often rectiﬁed linear unit applied inject nonlinearities model speed training process. following relu pooling layer non-linear down-sampling layer. common algorithm pooling pooling algorithm. algorithm sub-region previous feature turned single maximum value region. pooling reduces computation controls overﬁtting. order calculate predicted class performing pooling feature needs ﬂattened feed fully connected layer. last layer output layer softmax classiﬁer applied prediction. structure deep model designed illustrated fig. placeholder represents interface input training data. reshape needed ﬁrst convert input one-dimensional image data dimensional data. experiment mnist dataset input features converted two-dimensional image. model series convolutional layer followed pooling layer conv-pool-convpool followed fully connected layer fully connected network susceptible suffer overﬁtting dropout operation applied aims reduce operation probability parameter keep speciﬁc neuron probability adam optimizer rather standard stochastic gradient descent optimizer used train model modifying variables reducing loss. output layer neurons represents class meta-learning ensemble learning method learns results base classiﬁers. two-level structure algorithms used ﬁrst level called base-learners algorithm second level called meta-learner. base-learners trained original training data. meta-learner trained predictions base classiﬁers true class original training data. training meta-learner class-combiner strategy applied here predictions includes predicted class understand meta-learning algorithm intuitively fig. illustrates simpliﬁed training process meta-learning numbers represents four steps training. step base learning algorithms trained training data. step validation dataset used test trained classiﬁers step predictions generated step true labels validation dataset used train meta-learner. finally step meta-classiﬁer produced whole metalearning training process completed. training process accomplished test process much easier execute. fig. presents simpliﬁed test process step test data applied base classiﬁers generate predictions combined true labels test data comprises meta-level test data step. step ﬁnal predictions generated testing meta-level classiﬁer predictions step accuracy could calculated. framework named cnn-inte stands convolutional neural network interpretation. similar meta-learning different ways. work interpret ﬁrst fully connected layer deep model illustrated fig. training process shown fig. step original training data used train model. step parameters generated step used calculate values activations ﬁrst fully connected layer step clustering algorithm used cluster data generated step number groups deﬁne factors henceforth. step data belonging factors clustered generating number clusters assigned unique step grouped together training features meta-level using labels original training data label meta-learner. step features original training data step used train number random forests assume training data numbers instances layer neurons. labels training data ln}. deep model trained training instance calculate activations hidden neuron layer. hence obtain matrix size construct meta-level training data clustering algorithm cluster matrix along hidden layer axis several factors fk}. within factors cluster data again time along axis instances. clustering results instance belongs instance present technical details cnn-inte training process provide pseudo code algorithm line initialization algorithm. line activations clustered factors number clusters clustering algorithm lines clustering algorithm applied factors generate sets numbers. lines uses generated numbers true labels original training data train meta-learner till training process done yet. still need generate base models used test process. lines uses features original training data numbers train base models. output training process would meta-lever classiﬁer base models m··· mk}. fig. example illustrates process. example hidden neurons training instances. number clusters ﬁrst second level clustering hence matrix size ﬁrst clustered factors horizontally. factor activations clustered three clusters vertically e.g. clustered numbers cluster corresponding numbers factor according fig. hence meta-level training data combined corresponding training labels original training data used train meta-learner. meta-learner used decision tree inherently interpretable algorithm. tree structure provides excellent visual explanation predictions. test process meta-model exactly meta-learning test process shown fig. test process original test data test base classiﬁers generated meta-level training process obtain meta-level test data’s features. base-learner applied random forest. number base models equal number factors. hence base models mk}. example three factors lead three base models. training data ﬁrst base model corresponding would ti−li represents features original training instance. obtain base models original test data test produce meta-level test data. data feed trained decision tree model interpret individual test predictions. dataset mnist database handwritten digits extracted examples training data examples test data. examples represents images pixels ﬂattened features. experiments performed tensorflow platform. first need train nice deep model. ﬁrst reshape input training data images size training data every epoch expensive requires lots resources computer lead termination program. apply stochastic training ﬁrst epoch select minibatch training data perform optimization batch; loop batches randomize training data start epoch. experiment epoch batch size stochastic training cheap achieves similar performance using whole training data every epoch. mini-batch ﬁrst convolutional layer apply ﬁlters size generates feature maps. ﬁrst pooling layer apply ﬁlters size stride size second convolutional layer ﬁlters size ﬁrst convolutional layer. second pooling layer parameters previous one. immediately pooling layer ﬁrst fully connected layer. number neurons layer reduce overﬁtting also dropout parameter means neuron’s output probability dropped. last layer second fully connected layer neurons neuron outputs probability corresponding digits test accuracy trained model test data comes part setting interpretation. deﬁne interpretability model ability provide visual textual presentation connections input features output predictions. ﬁrst feed trained fully connected layer original training data would produce data size cluster several factors. clustering algorithm applied k-means algorithm number factors equal number clusters level. hence turned list size representing data belonging factor. second level clustering factor k-means algorithm cluster number clusters. number experiment number classes original training data hence cluster assigned unique number belonging training instances true labels original training data train decision tree algorithm. limitation space unable show structure trained decision tree here. maximum depth decision tree although deeper decision tree would generate better accuracy makes harder interpret many tree levels. obtain test data decision tree ﬁrst original training data’s feature features factor labels train corresponding random forest algorithm generating base models. random forest number trees maximum nodes finally original test data test trained base models. generated predictions become features meta-level test data sizes using meta-level test data trained decision tree produces accuracy tree depth=. value comparable test accuracy trained deep model noted decision tree’s accuracy could improved increasing depth tree tuning related parameters. interpret deep model’s behavior test data intend diagrams generated tool cnn-inte examine individual predictions test data. hence provide qualitative interpretations visually. arbitrarily selected test instances correctly classiﬁed decision tree test instance wrongly classiﬁed. noted tool could used test instances globally limited three cases provide. details selected test instances shown table represents features metalevel test data label test label original test data pred prediction generated decision tree meta-level test data. true true represents correctly classiﬁed instances wrong wrongly classiﬁed instance. order examine classiﬁcation process visually check feature values according trained structure decision tree plot graphs activations corresponding true label hypothesis. interpretation result instance true shown fig. true label instance classes could regarded hypothesis graphs hypothesis fig. represents examination feature values corresponding different factors different levels trained decision tree e.g. ﬁrst represents root level decision tree. since depth decision tree rows all. column stands query test instance belongs corresponding hypothesis nodes visited. take column hypothesis example goal label test instance extract activations corresponding satisﬁes condition draw graph activations belongs label= label= check graph evaluate data corresponding true class could separated hypothesis. answer hypothesis represented blue points overlaps true class shown points. hence need query trained decision tree further. values factors need check row; row; row; row. process noticed true class hypothesis class successfully separated points corresponding true label left. therefore don’t need examine that’s graph displayed. highlight graph green rectangles ﬁnal results separable vice versa. idea applied hypothesis. also draw graphs instances true wrong fig. fig. respectively. work present interpretation tool cnn-inte interprets hidden layer deep model learned hidden layer classiﬁes test instances. although show results ﬁrst fully connected layer read-out layer approach could applied hidden layers. interpretation realized ﬁnding relationships original training data trained hidden layer metalearning. used two-level k-means clustering algorithm meta-level training data random forests base models generating meta-level test data. visual results generated program clearly indicate test instance truly wrongly classiﬁed checking overlaps corresponding activations. future work plan initiate quantiﬁcation interpreted results. experiments things tricky setting number clusters k-means algorithm. future plan replace k-means algorithm dbscan doesn’t need specifying number clusters. stated decision sets seems better option decision tree inherently interpretable algorithm also plan replace decision tree decision sets. last least would quite meaningful apply tool real world applications interpretations demanded either training data hidden layer hidden layer predictions. authors acknowledge support province nova scotia dalhousie university natural sciences engineering research council canada create program grant. herlocker konstan riedl explaining collaborative ﬁltering recommendations proceedings conference computer supported cooperative work –december dzindolet peterson pomranky pierce beck. role trust automation reliance int. hum.-comput. stud. vol. pp.– lakkaraju bach leskovec interpretable decision sets joint framework description prediction proceedings sigkdd international conference knowledge discovery data mining august t.ribeiro s.singh c.guestrin trust you? explaining predictions classiﬁer proceedings sigkdd international conference knowledge discovery data mining august t.ribeiro s.singh c.guestrin nothing else matters modelagnostic explanations identifying prediction invariance conference neural information processing systems chan stolfo experiments multistrategy learning meta-learning proceedings second international conference information knowledge management december montavon samek k.r. m¨uller methods interpreting understanding deep neural networks digital signal processing nair hinton rectiﬁed linear units improve restricted boltzmann machines proceedings international conference machine learning", "year": 2018}