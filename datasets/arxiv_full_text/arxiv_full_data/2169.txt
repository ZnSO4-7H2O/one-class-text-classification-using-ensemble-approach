{"title": "Reinforcement Learning Algorithm Selection", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.OC"], "abstract": "This paper formalises the problem of online algorithm selection in the context of Reinforcement Learning. The setup is as follows: given an episodic task and a finite number of off-policy RL algorithms, a meta-algorithm has to decide which RL algorithm is in control during the next episode so as to maximize the expected return. The article presents a novel meta-algorithm, called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection. Under some assumptions, a thorough theoretical analysis demonstrates its near-optimality considering the structural sampling budget limitations. ESBAS is first empirically evaluated on a dialogue task where it is shown to outperform each individual algorithm in most configurations. ESBAS is then adapted to a true online setting where algorithms update their policies after each transition, which we call SSBAS. SSBAS is evaluated on a fruit collection task where it is shown to adapt the stepsize parameter more efficiently than the classical hyperbolic decay, and on an Atari game, where it improves the performance by a wide margin.", "text": "paper formalises problem online algorithm selection context reinforcement learning. setup follows given episodic task ﬁnite number off-policy algorithms meta-algorithm decide algorithm control next episode maximize expected return. article presents novel meta-algorithm called epochal stochastic bandit algorithm selection principle freeze policy updates epoch leave rebooted stochastic bandit charge algorithm selection. assumptions thorough theoretical analysis demonstrates near-optimality considering structural sampling budget limitations. esbas ﬁrst empirically evaluated dialogue task shown outperform individual algorithm conﬁgurations. esbas adapted true online setting algorithms update policies transition call ssbas. ssbas evaluated fruit collection task shown adapt stepsize parameter efﬁciently classical hyperbolic decay atari game improves performance wide margin. reinforcement learning machine learning framework optimising behaviour agent interacting unknown environment. practical problems dialogue robotics trajectory collection costly sample efﬁciency main performance indicator. consequently applying problem must carefully choose advance model representation optimisation technique parameters. facing complexity choice domain expertise sufﬁcient. confronted cost data popular trial error approach shows limits. develop online learning version algorithm selection smith-miles kotthoff consists testing several algorithms task selecting best given time. clarity throughout whole article algorithm selector called meta-algorithm algorithms available meta-algorithm called portfolio. meta-algorithm maximises objective function return. beyond sample efﬁciency objective online approach besides addresses four practical problems online rl-based systems. first improves robustness algorithm fails terminate outputs aberrant policy dismissed others selected instead. second convergence guarantees empirical efﬁciency united covering empirically efﬁcient algorithms slower algorithms convergence guarantees. third enables curriculum learning shallow models control policy early stages deep models discover best solution late stages. four allows deﬁne objective function return. fair algorithm selection implies fair budget allocation algorithms equitably evaluated compared. order comply requirement reinforcement algorithms portfolio assumed off-policy trained every trajectory regardless algorithm controls section provides unifying view algorithms allows information sharing algorithms whatever state representations optimisation techniques. also formalises problem online selection off-policy algorithms. next section presents epochal stochastic bandit novel meta-algorithm addressing online off-policy problem. principle divide time-scale epochs exponential length inside algorithms allowed update policies. epoch algorithms therefore constant policy stochastic multi-armed bandit charge strong pseudo-regret theoretical guaranties. thorough theoretical analysis provides esbas upper bounds. then section empirically evaluates esbas dialogue task shown outperform individual algorithm conﬁgurations. afterwards section esbas initially designed growing batch setting adapted true online setting algorithms update policies transition call ssbas. evaluated fruit collection task shown adapt stepsize parameter efﬁciently classical hyperbolic decay q*bert running several different network size depth parallel allows improve ﬁnal performance wide margin. finally section concludes paper prospective ideas improvement. goal section enable information sharing algorithms even though considered black boxes. propose share trajectories expressed universal format interaction process. reinforcement learning consists learning trial error control agent behaviour stochastic environment time step agent performs action perceives environment signal called observation receives reward bounded rmin rmax. figure illustrates framework. interaction process markovian agent internal memory. article problem assumed episodic. introduce time scales different notations. first deﬁne meta-time time scale meta-time corresponds meta-algorithm decision i.e. choice algorithm generation full episode controlled policy determined chosen algorithm. realisation called trajectory. second rl-time deﬁned time scale inside trajectory rl-time corresponds triplet composed observation action reward. denote space trajectories. trajectory collected meta-time formalised sequence triplets t∈|ετ| |ετ| length trajectory objective given discount factor =|ετ| generate trajectories high discounted cumulative reward also called return noted γt−rτ since bounded return also bounded. trajectory meta-time denoted {ετ}τ∈t sub-trajectory rl-time called history rl-time written |ετ|. history records happened episode rl-time )t∈t ensemble trajectory sets undetermined size goal algorithm policy yields optimal expected returns. algorithm viewed black takes input trajectory outputs policy παd. consequently algorithm formalised follows high level deﬁnition algorithms allows share trajectories algorithms trajectory sequence observations actions rewards interpreted algorithm decision process state representation. instance algorithms classically rely deﬁned explicit implicit state space representation thanks projection then trains policy παdt trajectories projected state space representation. off-policy optimisation techniques compatible approach numerous literature well post-treatment state alternative decision process off-policy algorithm used. algorithms deﬁned black boxes considered meta-algorithms indifferent algorithms compute policies granted satisfy off-policy assumption. online learning approach tackled article different algorithms experienced evaluated data collection. since boils classical exploration/exploitation trade-off multi-armed bandit used combinatorial search evolutionary algorithm meta-learning online problem off-policy novel deﬁne follows pseudo-code formalises online setting. meta-algorithm deﬁned function trajectory selection algorithm meta-algorithm queried meta-time |dτ−|+ input ouputs algorithm controlling policy πσdτ− generation trajectory stochastic environment. ﬁnal goal optimise cumulative expected return. expectation rewards obtained trajectories eµαd eπαd condensed notation expected return policy trained trajectory algorithm equation transforms cumulative expected return nested expectations. outside expectation assumes meta-algorithm ﬁxed averages trajectory generation corresponding algorithms policies. inside expectation assumes policy ﬁxed averages possible trajectories stochastic environment. nota bene three levels decision meta-algorithm selects algorithm computes policy control. paper focus meta-algorithm level. order evaluate meta-algorithms formulate additional notations. first optimal expected return deﬁned highest expected return achievable policy algorithm portfolio second every algorithm portfolio deﬁne canonical metaalgorithm i.e. meta-algorithm always selects algorithm absolute pseudo-regret deﬁnes regret loss controlled trajectory optimal policy worth noting optimal meta-algorithm unlikely yield null regret large part absolute pseudo-regret caused sub-optimality algorithm policies trajectory still limited size. indeed absolute pseudo-regret considers regret selecting optimal policy takes account pseudo-regret selecting best algorithm pseudo-regret algorithms ﬁnding optimal policy. since metaalgorithm interfere training policies ought account pseudo-regret related latter. related schweighofer doya meta-learning tune ﬁxed algorithm order observed animal behaviour different problem ours. cauwet teytaud problem solved portfolio composed online algorithms. main limitation works relies fact on-policy algorithms used prevents sharing trajectories among algorithms meta-learning speciﬁcally eligibility trace parameter also studied white white wang study learning process algorithms selects best learning faster task. work related batch intuitive solve problem consider algorithms arms multi-armed bandit setting. bandit meta-algorithm selects algorithm controlling next trajectory objective function constitutes reward bandit. prediction expert advice minimise regret best expert predeﬁned experts. experts learn time performances evolve hence sequence expert rewards non-stationary. exponential weight algorithms designed prediction expert advice sequence rewards experts generated oblivious adversary. approach extended competing best sequence experts adding update weights forgetting factor proportional mean reward combining concept drift detector allesiardo féraud exponential weight algorithms extended case rewards generated sequence stochastic processes unknown means stochastic bandit algorithm extended case switching bandits using discount factor window forget past garivier moulines class switching bandit algorithms designed experts learn hence evolve time step. esbas description solve offpolicy problem propose novel meta-algorithm called epochal stochastic bandit nonstationarity induced algorithm learning stochastic bandit cannot directly select algorithms. instead stochastic bandit choose ﬁxed policies. comply constraint meta-time scale divided epochs inside algorithms policies cannot updated algorithms optimise policies epochs start policies constant inside epoch. consequence since returns bounded epoch problem rigorously cast independent stochastic k-armed bandit |p|. esbas meta-algorithm formally sketched pseudo-code embedding auer stochastic k-armed bandit meta-algorithm takes input algorithms portfolio. meta-time scale fragmented epochs exponential size. epoch lasts meta-time steps that meta-time epoch starts. beginning epoch esbas meta-algorithm asks algorithm portfolio update current policy. inside epoch policy never updated anymore. beginning epoch instance reset run. whole epoch selects meta-time step algorithm control next trajectory. based several assumptions three theorems show esbas absolute pseudo-regret expressed function absolute pseudo-regret best canonical algorithm esbas shortsighted pseudo-regret. also provide upper bounds esbas short-sighted pseudo-regret. full theoretical analysis found supplementary material section provide intuitive overlook results. table numerically reports bounds two-fold portfolio depending nature algorithms. must read line. according ﬁrst column order magnitude esbas short-sighted pseudo-regret bounds displayed second column third fourth columns display esbas absolute pseudo-regret bounds also depending order magnitude regarding short-sighted upper bounds main result appears last line algorithms converge policies different performance esbas logarithmically converges best obtained summing gaps. means algorithms almost equally good goes beyond threshold distinguishability. threshold structurally impossibility determine better algorithm interpreted cauwet apart takes budget issue. meta-time necessary distinguish evaluation arms however esbas particularly designed tasks impossible update policy every transition episode. policy update costly real-world applications dialogue systems growing batch setting preferred esbas practical efﬁciency therefore illustrated dialogue negotiation game involves players system user goal agreement among alternative options. dialogue option players private agree player considered fully empathetic uniformly drawn cost one. details experiment found supplementary material section c... learning algorithms using fitted-q iteration linear parametrisation \u0001β-greedy exploration epoch number. several algorithms differing state space representation considered simple fast simple- fast- n-ζ{simple/fast/simple-/fast-} constant-µ. section full descriptions. algorithms esbas playing stationary user simulator built imitation learning real-human data. results averaged runs. performance ﬁgures plot curves algorithms individual performance esbas portfolio control σesbas function epoch performance average return problem. ratio ﬁgures plot average algorithm selection proportions esbas epoch. deﬁne relative pseudo regret difference esbas absolute pseudo-regret absolute pseudo-regret best canonical meta-algorithm. relative pseudo-regrets conﬁdence interval trajectory. extensive numerical results provided table supplementary material. figures plot typical curves obtained esbas selecting portfolio learning algorithms. figure esbas curve tends reach less best algorithm point expected. surprisingly figure reveals algorithm selection ratios strong favour another time. indeed variance trajectory collection makes simple better runs end. esbas proves efﬁcient selecting best algorithm unexpectedly obtains negative relative pseudo-regret figures plot typical curves obtained esbas selecting portfolio constituted learning algorithm algorithm deterministic stationary policy. esbas succeeds remaining close best algorithm epoch saves return value selecting constant algorithm overall yields regret using best algorithm. esbas also performs well larger portfolios learners negative relative pseudo-regrets even algorithms average almost selected uniformly figure reveals. individual present different ratios depending quality trained policies. esbas also offers curriculum learning importantly early policies avoided. algorithms constant policy improve time full reset k-multi armed bandit urges esbas unnecessarily explore underachieving algorithm. easy circumvent drawback knowledge reset arms. operating learning algorithm start outperforming constant esbas simply neither exploits explores constant algorithm anymore. without reset constant algorithms esbas’s learning curve follows perfectly learning algorithm’s learning curve outperforms constant algorithm achieves strong negative relative pseudo-regrets. again interested reader refer table supplementary material numerical results. section propose adapt esbas true online setting algorithms update policies transition. stochastic bandit trained sliding window last selections. even though arms stationary window guarantee eventually forgetting oldest pulls. algorithm called ssbas sliding stochastic bandit despite lack theoretical convergence bounds demonstrate domains different meta-optimisation tasks ssbas exceptionally well outperforming algorithms portfolio wide margin. goal demonstrate ssbas perform efﬁcient hyperparameter optimisation simple tabular domain gridworld problem goal collect fruits placed corner fast possible. episodes terminate fruits collected transitions. objective function used optimise stochastic bandit longer return time spent collect fruits agent possible positions nonterminal fruits conﬁgurations resulting states. action reward function mean eating fruit otherwise. reward function corrupted strong gaussian white noise variance portfolio composed q-learning algorithms varying learning rates linearly annealing -greedy exploration. selection ratios displayed figure show ssbas selected algorithm highest learning rate ﬁrst stages enabling propagate efﬁciently reward signal visited states then overtime preferentially chooses algorithm learning rate less sensible reward noise ﬁnally ssbas favours algorithm ﬁnest learning rate million episodes ssbas enables save half transition episode average compared best ﬁxed learning rate value transitions worst ﬁxed learning rate portfolio also compared efﬁciency linearly annealing learning rate ssbas performs steps average linearly annealing learning rate algorithm still performs steps steps. investigate deep arcade learning environment precisely game q*bert goal step block. similar level starts. later levels needs step twice block even later stepping blocks cancel colour change. used three different settings instances small uses setting described mnih large uses setting mnih ﬁnally huge uses even larger network known reach near-human level performance q*bert. ssbas instance runs algorithms different random initialisations setting. disclaimer contrarily experiments curve result single improvement might aleatory. indeed training long ssbas needs train models parallel. computationally-efﬁcient solution might architecture osband figure reveals ssbas experiences slight delay keeping touch best setting performance initial learning phase surprisingly ﬁnds better policy single algorithms portfolio ones reported previous articles. observe large setting surprisingly worst q*bert task implying difﬁculty predict model efﬁcient task. ssbas allows select online best one. article tackle problem selecting online off-policy algorithms. problem formalised follows ﬁxed portfolio algorithms meta-algorithm learns performs best task hand. fairness algorithm evaluation granted fact algorithms learn off-policy. esbas novel meta-algorithm proposed. principle divide meta-time scale epochs. algorithms allowed update policies start epoch. policies constant inside epoch problem cast stochastic multi-armed bandit. implementation detailed theoretical analysis leads upper bounds regrets. esbas designed growing batch setting. limited online setting required many real-world applications updating policy requires resources. experiments ﬁrst negotiation dialogue game interacting human data-built simulated user. settings esbas demonstrates efﬁciency select best algorithm also outperforms best algorithm portfolio thanks curriculum learning variance reduction similar ensemble learning. then esbas adapted full online setting algorithms allowed update transition. meta-algorithm called ssbas empirically validated fruit collection task performs efﬁcient hyper-parameter optimisation. ssbas also evaluated q*bert atari game achieves substantial improvement single algorithm counterparts. interpret esbas/ssbas’s success reliably outperforming best algorithm portfolio result four following potential added values. first curriculum learning esbas/ssbas selects algorithm ﬁtted data size. property allows instance shallow algorithms data deep algorithms collected lot. second diversiﬁed policies esbas/ssbas computes experiments several policies. diversiﬁed policies generate trajectories less redundant therefore informational. result policies trained trajectories efﬁcient. third robustness algorithm fails ﬁnding good policies soon discarded. property prevents agent repeating obvious mistakes. four last adaptation course algorithm best average given task given meta-time. depending variance trajectory collection necessarily train best policy run. esbas/ssbas meta-algorithm tries selects algorithm best run. properties inherited algorithm selection similarity ensemble learning wiering hasselt uses vote amongst algorithms decide control next transition. instead esbas/ssbas selects best performing algorithm. regarding portfolio design mostly depends available computational power sample ratio. practical implementations recommend limit highly demanding algorithms paired several faster algorithms take care ﬁrst learning stages algorithms diverse regarding models hypotheses etc. adding algorithms similar adds inertia likely distinguishable esbas/ssbas. detailed recommendations building efﬁcient portfolio left future work. references robin allesiardo raphaël féraud. drift detection switching bandit problem. proceedings ieee international conference data science advanced analytics ieee marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research sébastien bubeck nicolò cesa-bianchi. regret analysis stochastic nonstochastic multiarmed bandit problems. foundations trends machine learning álvaro fialho luis costa marc schoenauer michele sebag. analyzing bandit-based adaptive operator selection mechanisms. annals mathematics artiﬁcial intelligence aurélien garivier eric moulines. upper-conﬁdence bound policies switching bandit problems springer berlin heidelberg berlin heidelberg isbn ---. ./----_. http//dx.doi.org/./ ----_. hatim khouzaimi romain laroche fabrice lefevre. optimising turn-taking strategies reinforcement learning. proceedings annual meeting special interest group discourse dialogue hatim khouzaimi romain laroche fabrice lefèvre. reinforcement learning turn-taking management incremental spoken dialogue systems. proceedings international joint conference artiﬁcial intelligence jialin olivier teytaud. meta online learning experiments unit commitment problem. proceedings european symposium artiﬁcial neural networks computational intelligence machine learning volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature osband charles blundell alexander pritzel benjamin roy. deep exploration bootstrapped dqn. proceedings advances neural information processing systems jane wang kurth-nelson dhruva tirumala hubert soyer joel leibo rémi munos charles blundell dharshan kumaran matt botvinick. learning reinforcement learn. corr abs/. designation reinforcement learning time rl-time meta-algorithm time meta-time action taken rl-time observation made rl-time reward received rl-time action observation lower bound values taken upper bound values taken trajectory collected meta-time size ﬁnite set/list/collection ensemble integers comprised space trajectories discount factor decision process return trajectory objective function trajectory collected meta-time history rl-time policy optimal policy algorithm ensemble trajectory sets state space algorithm trajectory state space projection algorithm trajectory policy learnt algorithm trajectory algorithm portfolio size portfolio meta-algorithm algorithm selected meta-algorithm meta-time expected value conditionally expected return trajectories controlled policy optimal expected return canonical meta-algorithm exclusively selecting algorithm absolute pseudo-regret functions asymptotically dominated constant number stochastic k-armed bandit algorithm epoch index parameter algorithm short-sighted pseudo-regret best another index second best algorithm second best epoch rounding closest integer esbas meta-algorithm functions asymptotically dominating dominated best meta-algorithm among canonical ones first section section figure figure figure section section section section section section section section section section section section section section section section section section section section section section section equation equation section section deﬁnition section theorem section section pseudo-code deﬁnition theorem theorem theorem theorem theorem table theorem designation player system player user player option agree disagree cost booking/selecting option player uniform distribution final state reached trajectory immediate reward received system player dialogue dialogue consisting proposing option dialogue consisting asking player repeat said dialogue consisting accepting proposition dialogue consisting ending dialogue sentence error rate system listening user speech recognition score normal distribution centre variance refprop last proposed option symbol refprop askrepeat accept enddial seru scoreasr refinsist refnewprop refprop best option proposed accept φasr φdif φnoise simple fast simple- fast- n--simple n--fast accept last understood option proposition \u0001-greedy exploration function epoch features algorithm constant feature always equal feature equal last recognition score cost feature equal difference cost proposed targeted options rl-time feature noise feature φasr φdif φasr φdif} φasr φdif φasrφdif φtφasr φdif φasr φdif φasrφdif φasr φdif φnoise} φasr φdif φnoise} φasr φdif φnoise φasrφdif φtφnoise φasrφt φasr φdif non-learning algorithm average performance number noisy features added feature probability conditionally first section section section section section section section section section section section section section section section section section section section section section section section section section section section section section theoretical aspects algorithm selection reinforcement learning general epochal stochastic bandit algorithm selection particular thoroughly detailed section. proofs theorems provided sections recall formalise absolute pseudo-regret deﬁnition provided section deﬁnition absolute pseudo-regret algorithm’s expected return optimal expected return theoretical analysis hindered fact directly inﬂuences return distribution also trajectory distribution therefore policies learnt algorithms next trajectories indirectly affect future expected returns. order allow policy comparison based relation trajectory sets derived from analysis relies assumptions. assumption algorithms train better policies larger trajectory average whatever algorithm controlled additional trajectory assumption states algorithms off-policy learners additional data cannot lead performance degradation average. algorithm off-policy could biased speciﬁc behavioural policy would therefore transgress assumption. assumption algorithm trains better policy trajectory another remains same average collecting additional trajectory algorithm assumption states performance relation policies trained trajectory sets preserved average adding another trajectory whatever behavioural policy used generate assumptions theorem provides upper bound order magnitude function worst algorithm portfolio. veriﬁed meta-algorithm theorem absolute pseudo-regret bounded worst algorithm absolute pseudo-regret order magnitude contrarily name theorem suggests meta-algorithm might worse worst algorithm order magnitude. proof rather complex intuitive result because order control possible outcomes needs translate selections algorithm meta-algorithm canonical meta-algorithm σα’s view. esbas intends minimise regret choosing best algorithm given meta-time short-sighted intend optimise algorithms learning. deﬁnition short-sighted pseudo-regret immediate best expected return algorithm selected expected return best algorithm epoch algorithm smallest null exist i.e. non-null epoch noted regret null. several upper bounds order magnitude easily deduced theorem depending order magnitude corollaries section table generally section discussion. short-sighted pseudo-regret optimality depends meta-algorithm itself. instance poor deterministic algorithm might optimal meta-time yield information implying situation meta-time thus meta-algorithm exclusively selects deterministic algorithm would achieve short-sighted pseudo-regret equal selecting algorithms long efﬁcient. theorem necessary step towards absolute pseudo-regret analysis. absolute pseudo-regret decomposed absolute pseudo-regret best canonical meta-algorithm regret always selecting best algorithm potentially learning fast short-sighted regret regret gaining returns granted best algorithm. decomposition leads theorem provides upper bound absolute pseudo-regret function best canonical meta-algorithm short-sighted pseudo-regret. ﬁrst introduce fairness assumption. fairness budget distribution formalised cauwet property stating every algorithm portfolio much resources others terms computational time data. issue online problems since algorithm selected data therefore must advanced one. circumvent issue select equally online setting goal precisely select best algorithm often possible. answer require algorithms portfolio learning off-policy i.e. without bias induced behavioural policy used learning dataset. assuming algorithms learn off-policy allow information sharing cauwet algorithms. share trajectories generate. consequence assume every algorithm least selected ones learn trajectory set. therefore control unbalance directly lead unfairness algorithms performances algorithms learn equally trajectories. however unbalance might still remain exploration strategy instance algorithm takes beneﬁt exploration chosen chosen another algorithm. analysis purposes theorem assumes fairness assumption trajectory better another training given algorithm algorithms. successive median elimination upper conﬁdence bound conditions examples appropriate satisfying conditions stated theorems again table generally section discussion bounds. esbas practical efﬁciency illustrated dialogue negotiation game involves players system user goal agreement among alternative options. dialogue option players private uniformly drawn agree player considered fully empathetic one. cost result players come agreement system’s immediate reward dialogue state reached player dialogue agreed option; players fail agree ﬁnal immediate reward ﬁnally player misunderstands agrees wrong option system gets cost selecting option without reward successfully reaching agreement −νps players turn starting randomly other. four possible actions. first refprop player makes proposition option option previously proposed player player refuses second askrepeat player asks player repeat proposition. third accept player accepts option understood proposed player. ends dialogue either whether understood proposition right not. four enddial player want negotiate anymore ends dialogue null reward. understanding speech recognition system assumed noisy sentence error rate probability seru error made system understands random option instead actually pronounced. order reﬂect human-machine dialogue asymmetry simulated user always understands system says sers adopt khouza+e−x generate speech recognition conﬁdence scores scoreasr player understood right option otherwise system therefore portfolio algorithms action restrained parametric actions refinsist refprop option lastly proposed system; refnewprop refprop preferred askrepeat accept⇔ accept last understood option proposition enddial. learning algorithms using fitted-q iteration linear parametrisation \u0001β-greedy exploration epoch number. algorithms differing state space representation considered simple state space representation four features constant feature last recognition score feature φasr difference cost proposed option next .t+. φasr φdif φt}. best option φdif ﬁnally rl-time feature constant-µ algorithm follows deterministic policy average performance without exploration learning. constant policies generated simple- learning predeﬁned batch limited size. experiments esbas parameter consider epochs. ﬁrst second epochs last meta-time steps lengths double epoch total meta-time steps many trajectories. algorithms esbas playing stationary user simulator built imitation learning realhuman data. results averaged runs. performance ﬁgures plot curves algorithms individual performance esbas portfolio control σesbas function epoch performance average return reinforcement learning return equals γ|\u0001|rps negotiation game. ratio ﬁgures plot average algorithm selection proportions esbas epoch. deﬁne relative pseudo regret difference esbas absolute pseudo-regret absolute pseudo-regret best canonical meta-algorithm. relative pseudo-regrets well gain chosen worst algorithm portfolio provided table relative pseudo-regrets conﬁdence interval trajectory. several results show that practice assumptions transgressed. firstly observe assumption transgressed. indeed states trajectory better another given algorithm it’s algorithms. still assumption infringement seem harm experimental results. even seems help general assumption consistent curriculum learning inconsistent adaptation property advanced subsection states algorithm might best another runs. secondly off-policy reinforcement learning algorithms exist practice state space representations distort off-policy property however experiments reveal obvious bias related off/on-policiness trajectory algorithms train three networks built similar fashion relu activations layer except output layer linear rmsprop optimizer uniform initialisation. hyperparameters used training also equal ones presented table hereinafter huge ﬁrst convolution layer kernel stride second convolution layer kernel stride third convolution layer kernel stride followed dense layer size ﬁnally output layer also dense. portfolio simple- fast- simple n--simple- simple n--simple simple- n--simple- all- constant-. all- constant-. all- constant-. all- all--simple constant-. all--simple constant-. all--simple constant-. all--simple fast simple- simple- constant-. simple- constant-. simple- constant-. simple constant-. simple constant-. simple constant-. all- all--n- constant-. all- all--n- constant-. all- all--n- constant-. all- all--n- all--simple all--n--simple *n--simple *n--simple *n--simple- simple- constant-. simple- constant-. simple- constant-. simple- constant-. simple- constant-. table esbas pseudo-regret epochs compared best worst algorithms portfolio function algorithms portfolio character used separate algorithms. all- means four learning algorithms described section simple fast simple- fast-. all--n- means four algorithms additional feature noise. finally all--simple means simple simple- all--n--simple means n--simple n--simple-. second column redder colour worse esbas achieving comparison best algorithm. inversely greener colour number better esbas achieving comparison best algorithm. number neither green means difference portfolio best algorithm insigniﬁcant performing good. already achievement esbas good best. third column bluer cell weaker worst algorithm portfolio. notice positive regrets always triggered weak worst algorithm portfolio. cases esbas allow outperform best algorithm portfolio still credited fact dismissed efﬁciently weak algorithms portfolio. proof seem reader rather complex intuitive loose result algorithm selection algorithms selects tricky. instance selecting algorithm collected trajectory sets contains misleading examples implies following unintuitive inequality always true eµαdσ order control possible outcomes needs translate selections algorithm σα’s view. note optimal constant algorithm selection horizon necessarily optimal algorithm selection might exist probably exists constant algorithm selection yielding smaller pseudo-regret. esbas absolute pseudo-regret ρσesbas decomposed pseudo-regret followed optimal constant algorithm selection pseudo-regret selected algorithm highest return i.e. pseudo-regret trajectory pseudo-regret immediate optimal return sub∗ learnt algorithm trajectory sub∗ trajectory subset obtained removing trajectories generated algorithm first line equation rewritten follows point equation evaluate size sub∗ side assumption fairness states algorithm learns fast another history. asymptotically optimal algorithm therefore consider sets |subα|= last trajectory generated since esbas stochastic bandit regret o/∆) guarantees algorithms eventually selected inﬁnity times know recall stochastic bandit algorithm assumed guarantee best algorithm least times high probability show time longest stochastic bandit lasts least βτ−; meta-time spent epoch equal βτ−; meta-time spent epoch either case meta-time spent epoch higher thus esbas guaranteed best algorithm least times high probability result", "year": 2017}