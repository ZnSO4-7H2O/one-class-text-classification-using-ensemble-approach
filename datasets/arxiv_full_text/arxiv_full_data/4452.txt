{"title": "Hierarchical Bayesian image analysis: from low-level modeling to robust  supervised learning", "tag": ["cs.CV", "stat.ML"], "abstract": "Within a supervised classification framework, labeled data are used to learn classifier parameters. Prior to that, it is generally required to perform dimensionality reduction via feature extraction. These preprocessing steps have motivated numerous research works aiming at recovering latent variables in an unsupervised context. This paper proposes a unified framework to perform classification and low-level modeling jointly. The main objective is to use the estimated latent variables as features for classification and to incorporate simultaneously supervised information to help latent variable extraction. The proposed hierarchical Bayesian model is divided into three stages: a first low-level modeling stage to estimate latent variables, a second stage clustering these features into statistically homogeneous groups and a last classification stage exploiting the (possibly badly) labeled data. Performance of the model is assessed in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.", "text": "within supervised classiﬁcation framework labeled data used learn classiﬁer parameters. prior that generally required perform dimensionality reduction feature extraction. preprocessing steps motivated numerous research works aiming recovering latent variables unsupervised context. paper proposes uniﬁed framework perform classiﬁcation low-level modeling jointly. main objective estimated latent variables features classiﬁcation incorporate simultaneously supervised information help latent variable extraction. proposed hierarchical bayesian model divided three stages ﬁrst low-level modeling stage estimate latent variables second stage clustering features statistically homogeneous groups last classiﬁcation stage exploiting labeled data. performance model assessed speciﬁc context hyperspectral image interpretation unifying standard analysis techniques namely unmixing classiﬁcation. context image interpretation numerous methods developed extract meaningful information. among them generative models received particular attention strong theoretical background great convenience offer term interpretation ﬁtted models compared model-free methods deep neural networks. methods based explicit statistical modeling data allows task-speciﬁc model derived either general models implemented solve generic tasks gaussian mixture model classiﬁcation task-speciﬁc classiﬁcation-like models different ways reach interpretable description data respect particular applicative non-semantic issue. instance analyzing images task-speciﬁc models recovering latent structures classiﬁcation probably common interpret data whatever application ﬁeld interest undeniable appeal motivated simplicity resulting output. simplicity induces appreciable possibility beneﬁting training data relatively cost. indeed general experts generally produce ground-truth equivalent expected results classiﬁcation amount data. supervised approach allows priori knowledge easily incorporated improve quality inferred classiﬁcation model. nevertheless supervised methods signiﬁcantly inﬂuenced size training representativeness reliability moreover extent modeling pixelwise data single descriptor appear somehow limited. reason user-deﬁned classes often refer rather vague semantic meaning possible large intra-class variability. overcome issues simultaneously facing theoretical limitations expected classiﬁer ability generalization approach consists preceding training stage feature extraction feature extraction techniques whether parametric nonparametric also great advantage simultaneously signiﬁcantly reducing data volume handled well dimension space training subsequently conducted. unfortunately generally conducted separate manner classiﬁcation task i.e. without beneﬁting prior knowledge available training data. thus possible strategy consider features selecting relevant ones appropriate optimization schemes observation illustrates difﬁculty incorporating ground-truthed information feature extraction step generally latent structure analysis. versatility data description producing expert ground-truth degrees accuracy ﬂexibility would timeconsuming thus prohibitive. example research problem important well-documented source separation recent attempts made incorporate supervised knowledge provided end-user nonetheless latent structure analysis offer relevant meaningful interpretation data since various conceptual structured knowledge inferred incorporated modeling. particular dealing measurements provided sensor task-related biophysical considerations guide model derivation typically case spectral mixture analysis conducted interpret hyperspectral images whose pixel measurements modeled combinations elementary spectra corresponding physical elementary components contribution paper lies derivation uniﬁed framework able perform classiﬁcation latent structure modeling jointly. first framework primary advantage recovering consistent high level image descriptions explicitly conducting hierarchical image analysis. moreover improvements results associated methods expected thanks complementarity approaches. ground-truthed training data limited driving high level analysis i.e. classiﬁcation task. indeed also makes possible inform level analysis i.e. latent structure modeling usually beneﬁt well prior knowledge. hand latent modeling inferred data level description used features classiﬁcation. direct expected side effect explicit dimension reduction operated data classiﬁcation finally proposed hierarchical framework allows classiﬁcation robust corruption ground-truth. mentioned previously performance supervised classiﬁcation questioned reliability training dataset since generally built human expert thus probably corrupted label errors resulting ambiguity human mistakes. reason problem developing classiﬁcation methods robust label errors widely considered community purinteraction high level models handled non-homogeneous markov random ﬁelds mrfs probabilistic models widely-used describe spatial interactions. thus used derive prior model within bayesian approach particularly well-adapted capture spatial dependencies latent structures underlying images example chen proposed mrfs perform clustering. proposed framework incorporates instances ensuring consistency high level modeling consistency external data available prior knowledge classical spatial regularization. remaining article organized follows. section presents hierarchical bayesian model proposed unifying framework conduct low-level high-level image interpretation. markov chain monte carlo method derived section sample according joint posterior distribution resulting model parameters. then particular illustrative instance proposed framework presented section hyperspectral images analyzed dual scope unmixing classiﬁcation. finally section concludes paper opens research perspectives work. order propose unifying framework offering multi-level image analysis hierarchical bayesian model derived relate observations task-related parameters interest. model mainly composed three main levels. ﬁrst level presented section takes care low-level modeling achieving latent structure analysis. second stage assumes data samples divided several statistically homogeneous clusters respective latent structures. identify cluster memberships samples assigned discrete labels priori described non-homogeneous markov random ﬁeld combines terms ﬁrst related potential potts-mrf promote spatial regularity between neighboring pixels; second term exploits labels higher level promote coherence cluster classiﬁcation labels. clustering process detailed section finally last stage model explained section allows high-level labels estimated taking advantage availability external knowledge ground-truthed expert-driven data akin conventional supervised classiﬁcation task. whole model dependence summarized directed acyclic graph figure low-level task aims inferring r-dimensional latent variable vectors appropriate representing respective d-dimensional observation vectors subspace lower dimension original observation space i.e. task also include estimation function additional parameters function relating unobserved observed variables. denoting pmatrices gathering respectively observation latent variable vectors relation expressed general statistical formulation stands statistical model e.g. resulting physical approximation considerations flat deterministic function used deﬁne latent structure possible additional nuisance parameters. applicative contexts aimed work model function flat separable respect measurements assumed conditionally independent leading factorization worth noting statistical model explicitly lead derivation particular form likelihood function involved bayesian model. choice latent structure related function flat application-dependent directly chosen end-user. conventional choice consists considering linear expansion observed data orthogonal basis spanning space whose dimension lower original one. orthogonal space priori ﬁxed even learnt dataset itself e.g. leveraging popular nonparametric methods principal component analysis case model interpreted probabilistic counterpart latent variables would correspond factor loadings. similar linear latent factors low-rank models widely advocated address source separation problems nonnegative matrix factorization typical illustration assuming additive white centered gaussian statistical model linear latent function flat generic model particularly instanced identity matrix matrix spanning signal subspace variance gaussian error considered nuisance parameter. besides popular class gaussian models formulation allows noise statistics handled within linear factor modeling required approximation envisaged beyond conventional euclidean discrepancy measure provided different perspective generic formulation statistical latent structure also result thorough analysis complex physical processes underlying observed measurements resulting speciﬁc richer physics-based latent models sake generality latent structure speciﬁed rest manuscript except section linear gaussian model deeply investigated illustration particular applicative context. regularize latent structure analysis model complemented clustering step higher level bayesian hierarchy. besides another objective clustering stage also bridge lowhigh-level data interpretations namely latent structure analysis classiﬁcation. clustering performed assumption latent variables statistically homogeneous allocated several clusters i.e. identities belonging cluster supposed distributed according distribution. identify membership observation assigned cluster label number clusters. formally unknown latent vector thus described following prior elements matrix introduced latter account connection cluster class revealing hidden interaction clustering classiﬁcation. high value tends promote association cluster sample belongs class interaction encoded matrix coefﬁcients unknown thus motivates estimation matrix reach interpretation matrix coefﬁcients terms probabilities interdependency dirichlet distribution elected prior nonnegativity sum-to-one constraints imposed coefﬁcients deﬁning column allows interpreted probability vectors. choice prior furthermore motivated properties resulting conditional posterior distribution demonstrated later section present work hyperparameters chosen equal resulting uniform prior corresponding simplex deﬁned probability constraints. obviously additional prior knowledge interaction between clustering classiﬁcation available hyperparameters adjusted accordingly. last stage hierarchical model deﬁnes classiﬁcation rule. stage unique discrete class label attributed sample. task seen high-level sense deﬁnition classes motivated semantic meaning. classes speciﬁed end-user thus class gather samples signiﬁcantly dissimilar observation vectors even dissimilar latent features. clustering stage introduced earlier also allows mixture model derived classiﬁcation task. indeed class tends union several clusters identiﬁed clustering stage providing hierarchical description dataset. conventional well-admitted setup supervised classiﬁcation considered. setup means partial ground-truthed dataset available subset samples. follows denotes subset observation indexes ground-truth available. particularity proposed model lies prior cluster labels nonhomogeneous markov random field used prior model promote distinct behaviors potentials. ﬁrst local non-homogeneous potential parametrized k-by-j matrix promotes consistent relationships cluster labels classiﬁcation labels number classes. classiﬁcation labels associated high-level interpretation precisely investigated third stage hierarchy section pursuing objective analyzing images second potential associated potts-mrf granularity parameter promote piecewise consistent spatial regularity cluster labels. prior probability thus deﬁned ground-truth provides expected classiﬁcation labels observations indexed conversely index unlabeled samples ground-truth available noted moreover proposed model assumes ground-truth corrupted class labeling errors. consequence provide classiﬁcation robust possible errors classiﬁcation labels dataset estimated even associated observations indexed classiﬁcation process labels estimated observations indexed necessarily equal labels provided expert external knowledge. prior probability assigning given label proportion samples class hyperparameter stands conﬁdence given i.e. ground-truth label pixel case conﬁdence total parameter tends leads deterministic manner. however realistic applicative context ground-truth generally provided human experts contain errors example ambiguities simple mistakes. possible proposed model example level conﬁdence allows reestimate class label labeled thus correct provided ground-truth. mean robustness classiﬁcation label errors improved. similarly prior model advocated prior model classiﬁcation labels non-homogeneous composed potentials. again potts-mrf potential granularity parameter used promote spatial coherence classiﬁcation labels. potential non-homogeneous exploits supervised information available under form ground-truth particular attends ensure consistency estimated ground-truthed labels samples indexed moreover classiﬁcation labels associated indexes prior probability belong given class proportion class observed setting assumes expert representative whole scene analyzed term label proportions. assumption veriﬁed proposed modeling easily adjusted accordingly. mathematically formal description summarized following conditional prior probability given classiﬁcation label infer parameters hierarchical bayesian model introduced previous section mcmc algorithm derived generate samples according joint posterior distribution interest computed according following hierarchical structure pppp θk}. note that conciseness nuisance parameters implicitly marginalized hierarchical structure. marginalization straightforward nuisance parameters also explicitly included within model jointly estimated. bayesian estimators parameters interest approximated using samples. minimum mean square error estimators parameters approximated empirical averages carry sampling strategy conditional posterior distributions various parameters need derived. importantly ability drawing according distributions required. posterior distributions detailed follows. latter result motivates dirichlet distribution prior thus worth noting interpreted byproduct proposed model describes intrinsic dataset structure. allows practitioner overview distribution samples given class various clusters also possibly identify origin confusions several classes. again clustering step allows disparity semantic classes mitigated. intraclass variability results emerging several clusters subsequently agglomerated classiﬁcation stage. practice burn-in period proposed gibbs sampler avoid highly intensive computations cluster labels sampled according columns interaction matrix sampled according words burn-in period certain spatial regularization imposed cluster labels interaction matrix sampled according approximation conditional posterior distribution. burn-in period granularity parameter results removing spatial regularization cluster labels. thus convergence reached conditional posterior distribution reduces iteraction matrix properly sampled according exact conditional posterior distribution. similarly cluster labels classiﬁcation labels sampled evaluating conditional probabilities computed possible labels. however cases need considered sampling classiﬁcation label depending availability groundtruth label corresponding pixel. precisely i.e. pixel accompanied corresponding ground-truth conditional probabilities written denotes matrix whose column removed {p|zp number observations whose cluster class labels respectively indicator function probability simplex ensures implies sampling according conditional distribution would require compute partition function straightforward. partition function indeed possible conﬁgurations strategy would consist precomputing partition function appropriate grid alternatives could likelihood-free metropolis hastings algorithm auxiliary variables pseudolikelihood estimators however strategies remain high computational cost precludes practical applicative scenarii encountered real-world image analysis. besides partition function reduces words partition function constant spatial regularization induced taken account. case conditional posterior distribution following dirichlet distribution problem endmembers signatures well proportions nonnegative. moreover speciﬁcally reach close description pixel measurements abundance coefﬁcients interpreted concentrations different materials spatial position. nevertheless complementary classes methods considered jointly limited number works proposed hierarchical bayesian model offers great opportunity design uniﬁed framework methods conducted jointly. spectral unmixing perfectly suitable envisaged low-level task model described section abundance vector provides biophysical description pixel seen vector latent variables corresponding pixel. classiﬁcation step related semantic description pixel. low-level clustering tasks general framework described respectively sections speciﬁed follows classiﬁcation task directly implemented section bayesian model low-level interpretation according conventional linear mixing model pixel spectrum observed spectral bands approximated linear mixtures elementary signatures i.e. matrices endmember signatures abundance vectors noise respectively. work endmember spectra assumed priori known previously recovered hyperspectral images using endmember extraction algorithm assumption matrix formulation deﬁned straightforwardly interpreted particular instance low-level interpretation choosing latent function flat linear mapping flat statistical model gaussian probability density function parametrized variance denotes classiﬁcation label vector whose element removed. conversely i.e. pixel assigned ground-truth label conditional posterior probability reads proposed general framework introduced previous sections instanced speciﬁc application namely analysis hyperspectral images. hyperspectral imaging earth observation receiving increasing attention last decades particular signal/image processing literatures keen interest scientiﬁc community easily explained richness information provided images. indeed generalizing conventional red/green/blue color imaging hyperspectral imaging collects spatial measurements acquired large number spectral bands. pixel associated vector measurements referred spectrum characterizes macroscopic components present pixel. classiﬁcation spectral unmixing welladmitted techniques analyze hyperspectral images. mentioned earlier similarly numerous applicative contexts classifying hyperspectral images consists assigning discrete label pixel measurement agreement predeﬁned semantic description image. conversely spectral unmixing proposes retrieve elementary components called endmembers respective proportions called abundance pixel associated spatial distribution endmembers scene spectral unmixing cast blind source separation nonnegative matrix factorization task particularity spectral unmixing also known spectral mixture analysis microscopy literature lies speciﬁc constraints applied spectral unmixing. finally prior distribution cluster mean chosen dirichlet distribution dir. prior induces soft non-negativity sum-to-one constraints indeed constraints generally admitted describe abundance coefﬁcients since represent proportions/concentrations. work constraint directly imposed abundance vectors rather mean vectors since synthetic data used assess performance proposed analysis model algorithm. hyperspectral images synthetically generated according following hierarchical procedure. first cluster maps generated potts-markov mrfs. then corresponding classiﬁcation maps chosen artiﬁcially merging clusters deﬁne class. abundance vectors given cluster randomly drawn dirichlet distribution parametrized speciﬁc mean cluster. finally pixel measurements generated using linear mixture model real endmembers signatures spectral bands extracted spectral library. linearly mixed pixels corrupted gaussian noise resulting signal-tonoise ratio snr= distinct images referred image image represented figure considered. ﬁrst -pixel image composed endmembers clusters classes. second hyperspectral image -pixel image consists endmembers clusters classes. figure represents abundance vectors pixel probabilistic simplex image three clusters clearly identiﬁable class represented blue also clearly divided clusters. estimated jointly parameters interest. precisely variance assigned conjugate inversegamma prior non-informative jeffreys hyperprior chosen associate hyperparameter mean vector covariance matrix associated cluster. gaussian assumption equivalent considering high-level class mixture gaussian distributions abundance space. covariance matrices chosen diag−. shows latent vector associated pixel belonging cluster distributed according multivariate gaussian distribution included bayesian model choosing conjugate inverse-gamma prior distributions denote respectively estimated actual matrices abundance vectors. moreover accuracy estimated classiﬁcation maps measured conventional cohen’s kappa. results reported table show obtained rmse signiﬁcantly different models. moreover comparison processing times shows small computational overload required proposed model. noticed experiment conducted ﬁxed number iterations proposed mcmc algorithm second scenario considered training includes label errors. corrupted training generated tuning varying probability assign incorrect label possible labels equiprobable. probability varies step. context conﬁdence classiﬁcation ground-truth equal results averaged trials setting compared results obtained using mixture discriminant analysis conducted either directly pixel spectra either abundance vectors estimated proposed model. resulting classiﬁcation performances depicted figure function results show proposed model performs well even training highly corrupted figure classiﬁcation accuracy measured cohen’s kappa function label corruption proposed model abundance vectors measured reﬂectance shaded areas denote intervals corresponding standard deviation computed trials. moreover already explained another advantage proposed model interesting by-products provided method. illustration figure presents interactions matrices estimated image. ﬁgure clearly possible identify structure various classes hierarchical exploit high-level information. pixels associated classiﬁcation labels located upper quarters images used training conﬁdence classiﬁcation ground-truth value pixels additionally values potts-mrf granularity parameters selected case eches model images subsequently classiﬁed using estimated abundance vectors clustering maps following strategy proposed performance spectral unmixing task evaluated using root global mean square error associated abundance estimation values provide meaningful interpretation image. figure presents colored composition hyperspectral image expert ground-truth obtained results terms clustering classiﬁcation quantitative results term classiﬁcation accuracy computed summarized table note performance measure unmixing step provided since abundance groundtruh available real dataset. additionally robustness respect expert mislabeling ground-truth training dataset evaluated compared performance obtained state-of-the-art random forest classiﬁer. errors expert ground-truth randomly generated process used previous experiment synthetic data conﬁdence ground-truth equal pixels corruption rate maximum conﬁdence. parameters classiﬁer optimized using cross-validation training set. classiﬁcation accuracy measured cohen’s kappa presented figure function corruption rate training set. results proposed method seems perform favorably compared classiﬁer. worth noting prominent method classify remote sensing data robustness noise labeled data well-documented property classiﬁcation technique paper proposed bayesian model perform jointly low-level modeling robust classiﬁcation. hierarchical model capitalized markov random ﬁelds promote coherence various levels deﬁning model namely clustering conducted latent variables low-level modeling estimated class labels estimated class labels expert partial label provided supervised classiﬁcation. proposed model speciﬁcally designed result classiﬁcation step robust labeling errors could present expert ground-truth. simultaneously offered opportunity correct mislabeling errors. model particularly instanced particular application real hyperspectral image finally proposed strategy implemented analyze real -pixel hyperspectral image acquired within framework multiscale mapping ecosystem services high spatial resolution hyperspectral lidar remote sensing imagery project image composed spectral bands endmembers extracted using widely-used vertex component analysis algorithm associated expert ground-truth classiﬁcation made classes experiment upper half expert ground-truth provided training data proposed method. conﬁdence training pixels account imprecision expert ground-truth. granularity parameters proposed parameters since figure real muesli image. classiﬁcation accuracy measured cohen’s kappa function label corruption proposed model random forest shaded areas denote intervals corresponding standard deviation computed trials. aims conducting hyperspectral image unmixing classiﬁcation jointly. numerical experiments conducted ﬁrst synthetic data real data. results demonstrate relevance accuracy proposed method. richness resulting image interpretation also underlined results. future works include generalization proposed model handle fully unsupervised low-level analysis tasks. instantiations proposed model applicative contexts also considered.", "year": 2017}