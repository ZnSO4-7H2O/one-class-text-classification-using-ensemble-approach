{"title": "Learning Invariant Representations with Local Transformations", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.", "text": "learning invariant representations important problem machine learning pattern recognition. paper present novel framework transformationinvariant feature learning incorporatfeaing linear ture learning algorithms. example present transformation-invariant restricted boltzmann machine compactly represents data weights transformations achieves invariance feature representation probabilistic pooling. addition show transformation-invariant feature learning framework also extended unsupervised learning methods autoencoders sparse coding. evaluate method several image classiﬁcation benchmark datasets mnist variations cifar- stl- show competitive superior classiﬁcation performance compared state-of-the-art. furthermore method achieves state-of-theart performance phone classiﬁcation tasks timit dataset demonstrates wide applicability proposed algorithms domains. recent years unsupervised feature learning algorithms emerged promising tools learning representations data particular important problem learn invariant representations robust variability high-dimensional data since enable machine learning systems achieve good generalization performance using small number labeled training examples. context several feature learning algorithms proposed learn invariant representations speciﬁc transformations using customized approaches. example convolutional feature learning methods achieve shift-invariance exploiting convolution operators. another example denoising autoencoder learn features robust input noise trying reconstruct original data hidden representation perturbed data. however learning invariant representations respect general types transformations still challenging problem. paper present novel framework transformation-invariant feature learning. focus local transformations approximated linear transformations incorporate linear transformation operators feature learning algorithms. example present transformation-invariant restricted boltzmann machine generative model represents input data combination transformed weights. case transformation-invariant feature representation obtained probabilistic pooling hidden units transformations. addition show extensions transformationinvariant feature learning framework unsupervised feature learning algorithms autoencoders sparse coding. experiments evaluate method variations mnist dataset show algorithm signiﬁcantly outperform baseline restricted boltzmann machine underlying transformations data well-matched considered model. furthermore method learn features much robust wide range local transformations results highly section formulate novel feature learning framework learn invariance linear transformations based rbm. begin section describing transformation operator. notation formulate transformationinvariant restricted boltzmann machine learn invariance transformations. speciﬁcally given transformation matrices equation impose softmax constraint hidden units unit activated probabilistic pooling allows obtain feature representation invariant linear transformations. precisely suppose input matches ﬁlter given aninput transformed version tirbm transformation matrix matches transformed ﬁlter i.e. therefore competitive performance visual recognition tasks cifar- stl- datasets. addition method also achieves state-of-the-art performance phone classiﬁcation tasks timit dataset demonstrates wide applicability proposed algorithms domains. rest paper organized follows. provide preliminaries section section introduce proposed transformation-invariant feature learning algorithms. section review previous work invariant feature learning. then section report experimental results several datasets. section concludes paper. paper present general framework learning locally-invariant features using transformations. presentation restricted boltzmann machine main example. ﬁrst describe below followed novel extension restricted boltzmann machine bipartite undirected graphical model composed visible hidden layers. assuming binary-valued visible hidden units energy function joint probability distribution given follows figure feature encoding tirbm. here denote image patches shaded pattern inside reﬂects shaded patterns transformed ﬁlters show corresponding original ﬁlters ﬁlters selected probabilistic pooling across transformations shown arrows illustration assumed also existence identity transformation. section discuss design transformation matrix ease presentation assume transformations extended cases straightforwardly. further assume case here; discuss general cases later. image transformations rotation scaling contribution input coordinates output coordinate computed bilinear interpolation. since pre-computed usually sparse equation computed eﬃciently. emphasize transformation-invariant feature learning framework limited energybased probabilistic models extended unsupervised learning methods well. compared regular tirbm learn diverse patterns keeping number parameters small. speciﬁcally multiplying transformation matrix viewed increasing number ﬁlters factor without signiﬁcantly increasing number parameters parameter sharing. addition pooling local transformations ﬁlters learn invariant representations transformations. similar training stochastic gradient descent train tirbm. gradient loglikelihood approximated contrastive divergence taking gradient energy function respect model parameters. sparseness feature representation often desirable property. following al.’s approach extend model sparse tirbm adding following regularizer given besides translation however learning invariant features types image transformations extensively studied. contemporary work ours kivinen williams proposed transformation equivariant boltzmann machine shares similar mathematical formulation models infer best matching ﬁlters transforming using linear transformation matrices. however model motivated global equivariance main purpose work learn locally-invariant features useful classiﬁcation tasks. thus rather considering algebraic group transformation matrices focus variety local transformations include rotation translation well scale variations. furthermore eﬀectively address boundary eﬀects highly problematic scaling translation operators forming non-square matrix rather zeropadding. addition present general framework transformation-invariant feature learning show extensions based autoencoder sparse coding. overall argument strongly supported state-of-the-art performance image audio classiﬁcation tasks. another related work feature learning methods topographic maps also learn invariant representations compared methods model compact fewer parameters train since factors weights transformations. addition given number parameters model represent diverse variable input patterns topographic ﬁlter maps. observed learning zero-padded squared transformation matrices showed signiﬁcant boundary effect visualization ﬁlters often resulted signiﬁcantly worse classiﬁcation performance. similar standard sparse coding optimize parameters alternately optimizing ﬁxing other. speciﬁcally solved using orthogonal matching pursuit therefore refer algorithm transformationinvariant orthogonal matching pursuit researchers made signiﬁcant eﬀorts develop invariant feature representations. example rotationscale-invariant descriptors sift shown great success many computer vision applications. however image descriptors usually demand domain-speciﬁc knowledge signiﬁcant amount hand-crafting. alternative approach several unsupervised learning algorithms proposed learn robust feature representations automatically sensory data. example denoising autoencoder learn robust features trying reconstruct original data hidden representations randomly perturbed data generated distortion processes adding noise multiplying zeros randomly selected coordinates. among types transformations relating temporally spatially correlated data translation extensively studied context unsupervised learning. speciﬁcally convolutional training popular methods encourages shift-invariance during feature learning. example convolutional deep belief network first veriﬁed performance algorithm variations handwritten digit dataset assuming transformation information given. mnist variation datasets tested mnist-rot mnist-rot-back-image further evaluate diﬀerent types transformations created four additional datasets contain scale translation variations without random background examples shown figure speciﬁc transformations. example considered equally-spaced rotations rot-bgimg datasets. similarly scale scale-bgrand datasets generated scaletransformation matrices followed generation process described http//www.iro.umontreal.ca/~lisa/twiki/bin/view. cgi/public/mnistvariations create customized scaled translated digits. example randomly selected scale-level uniformly number pixel shifts horizontal vertical directions without making truncation foreground pixels. datasets random background randomly added uniform noise background pixels. figure samples handwritten digit datasets transformations rotation scaling translation. learned ﬁlters mnist-rot dataset sparse tirbm sparse respectively. vertical stride pixels. classiﬁcation trained ﬁlters sparse rbms sparse tirbms used softmax classiﬁer. used examples training examples validation examples test set. reported table method consistently outperformed baseline method datasets. results suggest tirbms learn better representations foreground objects transforming ﬁlters. worth noting error rates mnist-rot mnistrot-back-image datasets also signiﬁcantly lower best published results obtained stacked denoising autoencoders qualitative evaluation visualize learned ﬁlters mnist-rot dataset trained sparse tirbm sparse respectively. ﬁlters learned sparse tirbms show much clearer pen-strokes learned sparse rbms partially explains figure visualization ﬁlters trained tirbms natural images. trained ﬁlters used nine translations step size pixel rotations step size radian two-level scale transformations step size pixels respectively. handwritten digit recognition previous section assumed prior information global transformations image dataset. assumption enabled proposed tirbms achieve signiﬁcantly better classiﬁcation performance baseline method since data-speciﬁc transformation information encoded tirbm. however natural images reasonable assume global transformations complex image structures. fact recent literature suggests level invariance local transformations leads improved performance classiﬁcation. viewpoint makes sense learn representations local receptive ﬁelds invariant generic image transformations require data-speciﬁc prior information. hateren dataset baseline model learns many similar vertical edges shifted pixels whereas methods learn diverse patterns including diagonal horizontal edges shown figure results suggest tirbms learn diverse sets ﬁlters reminiscent eﬀects convolutional training however model much easier train convolutional models handle generic transformations beyond translations. evaluated image classiﬁcation tasks using datasets. first tested widely used cifar dataset composed training testing examples categories. rather learning features local image patches keeping channels. suggested coates used ﬁxed ﬁlter size determined receptive ﬁeld size depending types transformations. then after unsupervised training tirbm used convolutional feature extraction scheme also following coates al.. speciﬁcally computed pixel patch densely extracted stride pixel averaged patch-level activations quadrants image. eventually procedure yielded k-dimensional feature vectors image l-regularized linear svm. performed -fold cross validation determine hyperparameter comparison baseline model separately evaluated sparse tirbms single type transformation using shown table single type transformation tirbms brought signiﬁcant performance gain baseline sparse rbms. classiﬁcation performance improved combining diﬀerent types transformations single model. addition also report classiﬁcation results obtained using tiomp- unsupervised training. experiment used following two-sided soft thresholding encoding function constant threshold crossvalidated. result observed improvement baseline method using ﬁlters supports argument transformation-invariant feature learning framework eﬀectively transferred unsupervised learning methods. finally increasing number ﬁlters obtained better results previously published results using single-layer models well using deep networks. channels. followed unsupervised training classiﬁcation pipeline cifar-. reported table consistent improvements classiﬁcation accuracy incorporating various transformations learning algorithms. finally achieved accuracy using ﬁlters competitive best published single layer result accuracy timit dataset. following generated -dimensional mfcc features used contiguous frames input patch. tirbms applied three temporal translations stride frame. first compared classiﬁcation accuracy using linear evaluate performance gain coming unsupervised learning algorithms following experimental setup reported table tirbm showed improvement sparse well sparse coding sparse ﬁltering. next setting used rbf-kernel extracted features concatenated mfcc features. used default kernel width experiments performed cross-validation values. shown table combining mfcc tirbm features eﬀective resulted improvement classiﬁcation accuracy baseline mfcc features. increasing number tirbm features able beat best published results timit phone classiﬁcation tasks using hierarchical lm-gmm classiﬁer work proposed novel feature learning algorithms achieve invariance predeﬁned transformations. method handle general transformations experimentally showed learning invariant features transformations leads strong classiﬁcation performance. future work plan work learning transformations data combine algorithm. automatically learning transformation matrices data able learn robust features potentially lead signiﬁcantly better feature representations.", "year": 2012}