{"title": "Natural Language Multitasking: Analyzing and Improving Syntactic  Saliency of Hidden Representations", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. The representations change significantly when translation and part-of-speech decoders are added. The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure. Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way.", "text": "train multi-task autoencoders linguistic tasks analyze learned hidden sentence representations. representations change signiﬁcantly translation part-of-speech decoders added. decoders model employs better clusters sentences according syntactic similarity representation space becomes less entangled. explore structure representation space interpolating sentences yields interesting pseudo-english sentences many recognizable syntactic structure. lastly point interesting property models difference-vector sentences added change third sentence similar features meaningful way. representation learning opened doors many creative neural networks learn generate music extract artistic style painting apply arbitrary photograph computational linguistics progress made neural machine translation speechto-text many applications. however creative algorithms write poetry mimic author even develop ﬁctional languages sparse non-existent. since good representations crucial creative tasks examine ways improve learned representations develop ways measure linguistic quality. analyze improvements syntactic capabilities model relate learned hidden representations. syntax clustering experiment shows representation space multi-task models easily separable disentangled regions single-task models. result simple sentence features added subtracted representation space. work focus optimizing single task error forcing representations contain useful information structured analyzable way. train several sequence sequence models increasing number decoders. decoder distinctive linguistic task. compare sentence representations models learned explore representations different sentences relate other. sutskever long short-term memory networks encode arbitrary sequence vector decode back sequence. achieve results neural machine translation competitive statistical machine translation models objective several tasks models. luong extend sutskever al.’s model three multi-task settings one-to-many many-to-one many-to-many. work shows translation performance beneﬁt parsing image caption tasks. kind improvement adding related tasks also subject work. niehues note many linguistic resources enabled commonly used models. train translation models jointly part-of-speech named-entity recognition tasks show translation tagging beneﬁt shared information. research rooted idea focus learned representations rather training objectives. alternative bag-of-words feature many natural language processing models mikolov propose paragraph vector represent sentences paragraphs whole documents. feature outperforms bag-of-word models text classiﬁcation sentiment analysis tasks. could extended deal larger pieces text work focuses sentence-level. developed multi-task deep neural network multi-domain classiﬁcation information retrieval learns general semantic representations useful tasks demonstrating advantage multi-task learning. artetxe note large parallel corpora training models scarce. introduce novel system solely relies monolingual data still learning translate languages. vinyals develop generative model connects image processing natural language generation. model takes image input generates english sentence describes content image. complex task relies good well-generalized representations exploring work well. model architecture learn representations autoencoder operates stages encoder transforms data code hidden layer decoder tries reconstruct original input. decoder modiﬁed learn task reconstruction translating input different language. text sequential nature long short term memory recurrent networks encoders decoders. extend basic sequence sequence autoencoder model adding multiple decoders perform separate linguistic tasks. first encoder lstm consumes sequence characters updates internal state. whole input sequence read encoder state contains information entire sequence. state dense layer call representation layer output real vector speciﬁed dimension. analysis perform section refers output layer. next representation vector decoder lstm generate output sequences corresponding tasks. single-task model learn representations training data useful objective hand. since supports multiple decoders model architecture forces representations contain useful information objective. adding linguistic tasks decoders make representations salient linguistic perspective change properties whole representation space meaningful analyzable way. four different decoders. replicating decoder’s task reconstruct input sequence. german french decoders attempt translate input sentence german french respectively. last decoder learns words input sequence part-of-speech tags verb noun adjective. figure shows architecture multi-task autoencoder model. train models three tasks replication translation part-of-speech-tagging require multilingual corpus sentences correspond other. transcripts european parliament sessions suitable corpus aligned english german french sentences. replication task uses english sentences input target. training data decoders created using python nltk module figure architecture multi-task autoencoder models. four different decoders replicating translation german translation french part-of-speech tagger encoder decoders lstms. fully connected layers transform ﬁxed-length representation vectors variable length state vectors decoder. english text european parliament corpus. subset dataset contains million sentences million used training set. remaining million sentences form test set. training example tuple whose size depends model conﬁguration. example single-task model uses -tuples whereas multi-task rep-de-pos-model uses -tuples number available training examples model multi-task models trained larger tuples therefore training data. account imbalance train reference models fewer training examples compare performance main models train full dataset. train models different decoders representation layer sizes. table shows perplexities reached decoder trained models. perplexity measures close generated sequence target sequence deﬁned exponential cross entropy sequences. model name simply lists decoders representation layer size. encoder decoder lstms neurons. experimented different numbers layers neurons representation layer sizes. generally results sensitive choices thus refrained ﬁne-tuning models. achieved perplexities competitive state-of-the-art models. however work focuses learned representations decoder losses. perplexities reached reference models mentioned indistinguishable main models thus listed separately. quality learned representations measured? goal pursue models highest possible decoder accuracy. instead interested representations capture linguistic aspect input language. order compare learned representations different models examine well cluster syntactically similar sentences. sentence prototypes different syntactic structures example placeholders nouns verbs adjectives adverbs. following list sentence prototypes used +vs. ever +vs. still +ving +ving would rather without +ns. order often like +ns. whitespace sentence prototype randomly populated common english words times. syntax sentence category similar identical others category different sentences categories. sentences models. record every resulting representation pair input sentence. using k-means clustering cluster representation-sentence pairs representation space. resulting cluster count many sentences prototype contains. yields list this shows content clusters sentences type type since sentences cluster type cluster assigned cluster sentence category ﬁve. however sentences type falsely assigned cluster. therefore error cluster errors clusters clustering error quality measure experiment. since k-means clustering nondeterministic algorithm times. table shows best-of- clustering errors. starting left single-task model highest clustering error. german french translation decoders syntax clustering error decreases. adding part-of-speech tagging reduces clustering error even more. finally combining translation part-of-speech tagging reduces clustering error zero. means syntactically different sentence prototypes perfectly separated representation space. surprising model performs better others least humans classical algorithms correct tagging requirement preparative step syntax analysis. distinctive advantage model others indicates neural language models beneﬁt related linguistic tasks. clear separable clusters sentences suggests aspects syntax disentangled multi-task representation space. figure shows syntax clusters worst best performing models visualized using t-sne. clearly using syntactically relevant tasks helps model learn clearly separated representations syntax. trained several reference models fewer training examples account different numbers decoders thus different amount effective training data although clustering errors differ clear trend models cluster worse others better fewer examples note perplexities models achieved comparable reference models none models overﬁt training data. sentence representations models generate high-dimensional space. clustering experiments show data points sentences training spaced evenly vector space rather form clusters manifolds. seeing clearly models cluster sentences according syntax question arises lies sentences? precisely sentences representations sentence corresponds points along straight line table shows example outputs decoder rep-de-pos- rep-de- models. shown section rep-de-pos- signiﬁcantly lower clustering error seen also produces plausible sentences fewer non-words interpolation representation space. unsurprisingly every point representation space corresponds correct english sentence. overlook non-words interpolated sentences change syntax understood degree. however linear interpolation seem enough explore shape representation space. work investigating manifolds could yield useful results generative creative algorithms. example properties would representation space rep-de-pos- want? meat need? true? nomists told items. course hotels drug itselve. consumer took speed follows mr... consumer must therefore informed gmos. tolerate policy religious repression. threaten insist regarding representation. dossil ecoprat south direct-resprest and. must knowled alro economic objarimar represent. sant techni asplig poor correads report’sed. interest porals often martensrespre. still want recoursion viruccuroning responsibly. insade sarame places outso requirponts resel. issue returning ideology rrrengeing reply. strongly matic poorer ’fragen presumers’ thre. systemic policy religious reprovement. systematic policy religious repression. systematic policy religious repression. rep-de- want? nath affend? whin shakin weaknes fan? cust hesitage chear thembox. consumer encautant quote. consumer botted binds consumer must therefore informed gmos tolerate policy religious repression. word embedding spaces special property differences embedded words correspond direction axis semantic grammatical interpretation. example difference vector queen king might roughly equal woman raises question whether sentence representations similar properties. simple assumption would difference-vector cats good pets. dogs good pets. canceled part good pets roughly point dogs cats. adding difference-vector sentence contains dogs result sentence dogs replaced cats. rep-de- model fails task shown table however arithmetic works better model performed best syntax clustering experiment shown table arithmetic worked ﬁrst three sentences failed last two. results representative require rigorous investigation cherry pick examples. fact representation arithmetic works number small examples shows learned representations much complex mere symbol probabilities. since experiment considers word occurrence order remains open whether semantic component phenomenon. adding specialized semantic tasks models could improve results. trained several multi-task autoencoders linguistic tasks analyzed learned sentence representations. representations change signiﬁcantly translation part-of-speech tagging decoders added. decoders model uses better cluster sentence representations according syntactic similarity. indicates space becomes separable disentangled tasks added. explored structure representation space interpolating sentences yields interesting pseudo-english sentences many recognizable syntactic structure. finally point interesting property models’ representations difference-vector sentence representations added change third sentence similar features meaningful way. call process representation arithmetic since allows adding subtracting sentence features sentences. future want better understanding shape representation space. interpolating inside manifold data populates could enable creative algorithms produce grammatical sentences sampling inside manifold. perhaps representation arithmetic property made robust adding semantic tasks decoders. behavior could made predictable representation space would useful properties generative models semantic features could transferred sentences. constrained latent space since focus language generation. nevertheless plan experimenting variational autoencoders inherent capability disentangle latent space might enable better disentanglement semantics syntax. references leon gatys alexander ecker matthias bethge. image style transfer using convolutional neural networks. ieee conference computer vision pattern recognition cvpr pages niehues eunah cho. exploiting linguistic resources neural machine translation using multi-task learning. proceedings second conference machine translation pages xiaodong jianfeng xiaodong deng kevin ye-yi wang. representation learning using multi-task deep neural networks semantic classiﬁcation information retrieval. naacl conference north american chapter association computational linguistics human language technologies pages oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural image caption generator. ieee conference computer vision pattern recognition cvpr boston june pages edward loper steven bird. nltk natural language toolkit. proceedings acl- workshop effective tools methodologies teaching natural language processing computational linguistics volume etmtnlp pages association computational linguistics tomas mikolov ilya sutskever chen gregory corrado jeffrey dean. distributed representations words phrases compositionality. annual conference neural information processing systems nips pages", "year": 2018}