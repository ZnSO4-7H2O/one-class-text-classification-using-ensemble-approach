{"title": "Multimodal Word Distributions", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.", "text": "word embeddings provide point representations words containing useful semantic information. introduce multimodal word distributions formed gaussian mixtures multiple word meanings entailment rich uncertainty information. learn distributions propose energy-based max-margin objective. show resulting approach captures uniquely expressive semantic information outperforms alternatives wordvec skip-grams gaussian embeddings benchmark datasets word similarity entailment. model language must represent words. imagine representing every word binary one-hot vector corresponding dictionary position. representation contains valuable semantic information distances between word vectors represent differences alphabetic ordering. modern approaches contrast learn words similar meanings nearby points vector space large datasets wikipedia. learned word embeddings become ubiquitous predictive tasks. vilnis mccallum recently proposed alternative view words represented whole probability distribution instead deterministic point vector. speciﬁcally model word gaussian distribution learn mean covariance matrix data. approach generalizes deterministic point embedding fully captured mean vector gaussian distribution. moreover full distribution provides much richer information however since gaussian distribution mode learned uncertainty representation overly diffuse words multiple distinct meanings order model assign density plausible semantics moreover mean gaussian pulled many opposing directions leading biased distribution centers mass mostly around meaning leaving others well represented. paper propose represent word expressive multimodal distribution multiple distinct meanings entailment heavy tailed uncertainty enhanced interpretability. example mode word ‘bank’ could overlap distributions words ‘ﬁnance’ ‘money’ another mode could overlap distributions ‘river’ ‘creek’. contention ﬂexibility critical qualitatively learning meanings words optimal performance many predictive tasks. particular model word mixture gaussians learn parameters mixture model using maximum margin energy-based ranking objective energy function describes afﬁnity pair words. analytic tractability gaussian mixtures inner product probability distributions hilbert space known expected likelihood kernel energy function additionally propose transformations numerical stability initialization resulting robust straightforward scalable learning procedure capable training corpus billions words days. show model able automatically discover multiple meanings words signiﬁcantly outperform alternative methods across several tasks word similarity entailment made code available http//github.com/ benathi/wordgm implement model tensorﬂow past decade explosion interest word vector representations. wordvec arguably popular word embedding uses continuous words skipgram models conjunction negative sampling efﬁcient conditional probability estimation popular approaches feedforward recurrent neural network language models predict missing words sentences producing hidden layers word embeddings encode semantic information. employ conditional probability estimation techniques including hierarchical softmax noise contrastive estimation different approach learning word embeddings factorization word cooccurrence matrices glove embeddings matrix factorization approach shown implicit connection skip-gram negative sampling levy goldberg bayesian matrix factorization columns modeled gaussians explored salakhutdinov mnih provides different probabilistic perspective word embeddings. exciting recent work vilnis mccallum propose gaussian distribution model word. approach signiﬁcantly expressive typical point embeddings ability represent concepts entailment distribution word encompass distributions sets related words however unimodal distribution approach cannot capture multiple distinct meanings much like deterrecent work also proposed deterministic embeddings capture polysemies example cluster centroid context vectors adapted skip-gram model algorithm learn multiple latent representations word neelakantan also extends skip-gram multiple prototype embeddings number senses word determined non-parametric approach. learns topical embeddings based latent topic models word associated multiple topics. another related work nalisnick ravi models embeddings inﬁnite-dimensional space embedding gradually represent incremental word sense complex meanings observed. probabilistic word embeddings recently begun explored shown great promise. paper propose best knowledge ﬁrst probabilistic word embedding capture multiple meanings. gaussian mixture model allows highly expressive distributions words. time retain scalability analytic tractability expected likelihood kernel energy function training. model training procedure harmonize learn descriptive representations words superior performance several benchmarks. section introduce gaussian mixture model word representations present training method learn parameters gaussian mixture. method uses energy-based maximum margin objective wish maximize similarity distributions nearby words sentences. propose energy function compliments model retaining analytic tractability. also provide critical practical details numerical stability hyperparameters initialization. mean vectors represent location component word akin point embeddings provided popular approaches like wordvec. represents component probability component covariance matrix containing uncertainty information. goal learn model parameters corpus natural sentences extract semantic information words. gaussian component’s mean vector word represent word’s distinct meanings. instance component polysemous word ‘rock’ represent meaning related ‘stone’ ‘pebbles’ whereas another component represent meaning related music ‘jazz’ ‘pop’. figure illustrates word embedding model difference multimodal unimodal representations words multiple meanings. learning training objective {µwi σwi} draws inspiration continuous skip-gram model word embeddings trained maximize probability observing word given another nearby word. procedure follows distributional hypothesis words occurring natural contexts tend semantically related. instance words ‘jazz’ ‘music’ tend occur near another often ‘jazz’ ‘cat’; hence ‘jazz’ ‘music’ likely related. learned word representation contains useful semantic information used perform variety tasks word similarity analysis sentiment classiﬁcation modelling word analogies preprocessed input complex system statistical machine translation. gaussian mixture embedfigure ding component corresponds distinct meaning. gaussian component represented ellipsoid whose center speciﬁed mean vector contour surface speciﬁed covariance matrix reﬂecting subtleties meaning uncertainty. left show examples gaussian mixture distributions words gaussian components randomly initialized. training right component word ‘rock’ closer ‘stone’ ‘basalt’ whereas component closer ‘jazz’ ‘pop’. also demonstrate entailment concept distribution general word ‘music’ encapsulates words ‘jazz’ ‘rock’ ‘pop’. bottom gaussian embedding model words multiple meanings ‘rock’ variance learned representation becomes unnecessarily large order assign probability meanings. moreover mean vector words pulled clusters centering mass distribution region certain meanings. sample objective consists pairs words sampled sentence corpus nearby word within context window length instance word ‘jazz’ occurs sentence listen jazz music’ context words negative context word obtained random sampling. call term partial energy. observe term captures similarity meaning word meaning word total energy equation possible pairs partial energies weighted accordingly mixture probabilities term −µfi−µgj)−µfi− µgj) explains difference mean vectors semantic pair semantic uncertainty pairs term importance relative terms inverse covariance scaling. observe loss function section attains value relatively high. high values achieved component means across different words close together sentations). high energy also achieved large values washes importance mean vector difference. term serves regularizer prevents covariances pushed high expense learning good mean embedding. ative sampling contrasts product positive context pairs negative context pairs. energy function measure similarity distributions discussed section max-margin ranking objective used gaussian embeddings vilnis mccallum pushes similarity word positive context higher negative context margin eθ)) objective minimized mini-batch stochastic gradient descent respect parameters {µwi σwi} mean vectors covariance matrices mixture weights multimodal embedding word sampling word sampling scheme wordvec balance importance frequent words rare words. frequent words ‘the’ ‘to’ meaningful relatively less frequent words ‘dog’ ‘love’ ‘rock’ often interested learning semantics less frequently observed words. subsampling improve performance learning word vectors technique discards word probability generate negative context words word type sampled according distribution distorted version unigram distribution also serves diminish relative importance frequent words. subsampling negative distribution choice proven effective wordvec training energy function vector representations words usual choice similarity measure product vectors. word representations distributions instead point vectors therefore need measure reﬂects point similarity also uncertainty. time components learn signals word occurrences equally. training progresses semantic representation mixture becomes clear term ξij’s predominantly higher terms giving rise semantic pair related. negative divergence another sensible choice energy function providing asymmetric metric word distributions. however unlike expected likelihood kernel divergence closed form distributions gaussian mixtures. introduced model multi-prototype embeddings expressively captures word meanings whole probability distributions. show combination energy objective functions proposed section enables learn interpretable multimodal distributions unsupervised training describing words multiple distinct meanings. representing multiple distinct meanings model also reduces unnecessarily large variance gaussian embedding model improved results word entailment tasks. learn parameters proposed mixture model train concatenation datasets ukwac wackypedia discard words occur fewer times corpus results vocabulary size words. word sampling scheme described section similar wordvec negative context word positive context word. training obtain learned parameters {µwi pi}k word treat mean vector embedding mixture component covariance matrix representing subtlety uncertainty. perform qualitative evaluation show embeddings learn meaningful multi-prototype representations compare existing models using quantitative evaluation word similarity datasets word entailment. unless stated otherwise experiment components model results discussion section primarily consider spherical case computational efﬁciency. note diagonal spherical covariances energy computed efﬁciently since matrix inversion would simply require computation instead full matrix. empirically found diagonal covariance matrices become roughly spherical training. indeed relatively high dimensional embeddings sufﬁcient degrees freedom mean vectors learned covariance matrices need asymmetric. therefore perform evaluations spherical covariance models. models used evaluation dimension context window unless stated otherwise. provide additional hyperparameters training details supplementary material since word embeddings contain multiple vectors uncertainty parameters word following measures generalizes similarity scores. measures pick component pair maximum similarity therefore determine meanings relevant. natural choice similarity score expected likelihood kernel inner product distributions discussed section metric incorporates uncertainty covariance matrices addition similarity mean vectors. metric measures maximum similarity mean vectors among pairs mixture components distributions ||µfi|| ||µgj|| corresponds matching meanings similar. gaussian embedding maximum similarity reduces usual cosine similarity. banks mouth river river conﬂuence waterway downstream upstream dammed banks banking banker banks bankas citibank interbank bankers transactions table nearest neighbors based cosine similarity mean vectors gaussian components gaussian mixture embedding gaussian embedding notation denotes mixture component word minimum euclidean distance cosine similarity popular evaluating embeddings. however training objective directly involves euclidean distance opposed product vectors wordvec. therefore also consider clidean metric qualitative evaluation table show examples polysemous words nearest neighbors embedding space demonstrate trained embeddings capture multiple word senses. instance word ‘rock’ could mean either ‘stone’ ‘rock music’ meanings represented distinct gaussian component. results mixture gaussians model conﬁrm hypothesis observe component ‘rock’ related component related similarly word bank component representing river bank component representing ﬁnancial bank. contrast table gaussian embeddings mixture component nearest neighbors polysemous words predominantly related single meaning. instance ‘rock’ mostly neighbors related rock music ‘bank’ mostly related ﬁnancial bank. alternative meanings polysemous words well represented embeddings. numerical example cosine similarity ‘rock’ ‘stone’ gaussian representation vilnis mccallum much lower cosine similarity component ‘rock’ ‘stone’ multimodal representation. cases word single popular meaning mixture components fairly close; instance component ‘stone’ close reﬂects subtle variations meanings. general mixture give properties heavy tails interesting unimodal characterizations uncertainty could described single gaussian. embedding visualization interactive visualization part code repository https//github.com/benathi/ wordgmvisualization allows realtime queries words’ nearest neighbors components. notation similar table token represents component word instance link search bank obtain nearest neighbors river confluence waterway indicates component ‘bank’ meaning ‘river bank’. hand searching bank yields nearby words banking banker indicating component close ‘ﬁnancial bank’. also visualization unimodal comparison link. addition embedding link gaussian mixture model mixture components learn three distinct meanings. instance three components ‘cell’ close indicating distribution captures concept ‘cellphone’ ‘jail cell’ ‘biological cell’ respectively. limited number words meanings model generally offer substantial performance differences model hence display results compactness. evaluate embeddings several standard word similarity datasets namely simlex wordsim- ws-s ws-r mturk dataset contains list word pairs human score related similar words are. calculate spearman correlation labels scores generated embeddings. spearman correlation rank-based correlation measure assesses well scores describe true labels. correlation results shown table using scores generated expected likelihood kernel maximum cosine similarity maximum euclidean distance. show results gaussian mixture model compare performance wordvec original gaussian embedding vilnis mccallum note model unimodal gaussian embedding also outperforms original model differs model hyperparameters initialization multi-prototype model also performs better skip-gram gaussian embedding methods many datasets namely ws-r best maximum cosine similarity yields performance datasets; however minimum euclidean distance better metric datasets results consistent single-prototype multi-prototype models. also compare results wordsim- multi-prototype embedding method huang neelakantan shown table observe singleprototype model competitive compared models huang even without using corpus stop words removed. could auto-calibration importance covariance learning decrease importance frequent words ‘the’ ‘to’ etc. moreover multi-prototype model substantially outperforms model huang mssg model neelakantan wordsim- dataset. compare method multiprototype models huang tian chen mssg model note chen model uses external lexical source wordnet gives extra advantage. many metrics calculate scores spearman correlation. maxsim refers maximum cosine similarity. avesim average cosine similarities respect component probabilities. table model performs best among single-prototype models either vector dimensions. model performs competitively compared multiprototype models. scws gain ﬂexibility moving probability density approach appears dominate effects using multiprototype. examples surpass multi-prototype structure table spearman correlation word similarity datasets. models denote wordvec skip-gram gaussian embedding gaussian mixture embedding measures denote maximum cosine similarity expected likelihood kernel minimum euclidean distance. underline similarity metric best score. dataset boldface score best performance across models. correlation scores taken vilnis mccallum correspond cosine distance. table spearman’s correlation wordsim datasets word gaussian mixture embeddings well multi-prototype embedding huang mssg model neelakantan huang* trained using data stop words removed. models dimension except mssg still outperformed model. table spearman’s correlation dataset scws. show results single prototype multi-prototype sufﬁx refers single multiple prototype models respectively. important good performance probabilistic representation. note models also avgsimc metric uses context information yield better correlation report numbers using avgsim maxsim existing models comparable performance maxsim. motivation gaussian mixture embedding model word uncertainty accurately gaussian embeddings overly large variances polysemous words gaussian mixture model indeed reduce variances component words. instance observe word rock much higher variance dimension compared gaussian components rock also next section desirable quantitative behavior word entailment. word entailment evaluate embeddings word entailment dataset baroni lexical entailment words denoted means instances entailment dataset contains positive pairs table entailment results models window size maximum cosine similarity maximum negative divergence. calculate best average precision best score. cases outperforms describing entailment. generate entailment scores word pairs best threshold measured average precision score identiﬁes negative versus positive entailment. maximum cosine similarity minimum dikl envergence tailment scores. minimum divergence similar maximum cosine similarity also incorporates embedding uncertainty. addition divergence asymmetric measure suitable certain tasks word entailment relationship unidirectional. instance imply indeed aircraft vehicle imply vehicle aircraft since aircraft vehicles vehicles aircraft. difference versus distinguishes word distribution encompasses andistribution demonstrated figure table shows results model versus gaussian embedding model observe trend models window size metric yields improvement cosine similarity. addition generally outperforms multi-prototype model estimates meaning uncertainty better since longer constrained unimodal leading better characterizations entailment. hand gaussian embedding model suffers overestimatating variances polysemous words results less informative word distributions reduced entailment scores. introduced model represents words expressive multimodal distributions formed gaussian mixtures. learn properties mixture proposed analytic energy function combination maximum margin objective. resulting embeddings capture different semantics polysemous words uncertainty entailment also perform favorably word similarity benchmarks. elsewhere latent probabilistic representations proving exceptionally valuable able capture nuances face angles variational autoencoders subtleties painting strokes infogan moreover classically deterministic deep learning architectures actively generalized probabilistic deep models full predictive distributions instead point estimates signiﬁcantly expressive representations similarly probabilistic word embeddings capture range subtle meanings advance state art. multimodal word distributions naturally represent belief words single precise meanings indeed shape word distribution express much semantic information point representation. future multimodal word distributions could open doors suite applications language modelling whole word distributions used inputs probabilistic lstms decision functions uncertainty matters. part effort explore different metrics distributions divergences would natural choice order embeddings model entailment properties. would also informative explore inference number components mixture models word distributions. approach could potentially discover unbounded number distinct meanings words also distribute support word distribution express highly nuanced meanings. alternatively could imagine dependent mixture model distributions words evolving time covariates. could also build types supervised language models constructed fully leverage rich information provided word distributions. references maruan al-shedivat andrew gordon wilson yunus saatchi zhiting eric xing. learning scalable deep kernels recurrent structure. arxiv preprint arxiv. marco baroni raffaella bernardi ngoc-quynh chung-chieh shan. entailment word eacl level distributional semantics. conference european chapter association computational linguistics avignon france april pages http//aclweb.org/anthologynew/e/e/e-.pdf. marco baroni silvia bernardini adriano ferraresi eros zanchetta. wacky wide collection large linguistically processed web-crawled corpora. language resources evaluation https//doi.org/./s---. chen chen duan rein houthooft john schulinfogan ilya sutskever pieter abbeel. interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems annual conference neural information processing systems december barcelona spain. pages xinxiong chen zhiyuan maosong sun. uniﬁed model word sense representation disambiguation. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group acl. pages http//aclweb.org/anthology/d/d/d-.pdf. ronan collobert jason weston. uniﬁed architecture natural language processing deep neural networks multitask learning. machine learning proceedings twenty-fifth international conference helsinki finland june pages chunyuan changyou chen yunchen qinliang lawrence carin. scalable bayesian learning recurrent neural networks language modeling. arxiv preprint arxiv. michael gutmann aapo hyv¨arinen. noisecontrastive estimation unnormalized statistical models applications natural image statistics. journal machine learning research halawi gideon dror evgeniy gabrilovich yehuda koren. large-scale learning word relatedness constraints. sigkdd international conference knowledge discovery data mining beijing china august pages eric huang richard socher christopher manning andrew improving word representations global context multiple word prototypes. annual meeting association computational linguistics proceedings conference july jeju island korea volume long papers. pages http//www.aclweb.org/anthology/p-. thorsten joachims. optimizing search engines using proceedings eighth clickthrough data. sigkdd international conference knowledge discovery data mining july edmonton alberta canada. pages omer levy yoav goldberg. neural word embedding implicit matrix factorization. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada. pages finkelstein evgeniy gabrilovich yossi matias ehud rivlin zach solan gadi wolfman eytan ruppin. placing search context concept revisited. trans. inf. syst. kira radinsky eugene agichtein evgeniy gabrilovich shaul markovitch. word time computing word relatedness using temporal semantic analysis. proceedings international conference world wide web. pages ruslan salakhutdinov andriy mnih. bayesian probabilistic matrix factorization using markov chain machine learning proceedings monte carlo. twenty-fifth international conference helsinki finland june pages https//doi.org/./.. tian hanjun jiang bian zhang enhong chen tie-yan liu. probabilistic model learning multi-prototype word embeddings. coling international conference computational linguistics proceedings conference technical papers august dublin ireland. pages http//aclweb.org/anthology/c/c/c.pdf. andrew wilson zhiting ruslan salakhutdinov eric xing. stochastic variational deep kernel learning. advances neural information processing systems. pages andrew gordon wilson zhiting ruslan salakhutdinov eric xing. deep kernel learning. proceedings international conference artiﬁcial intelligence statistics. pages tomas mikolov anoop deoras daniel povey luk´as burget cernock´y. strategies training large scale neural network language ieee workshop automatic speech els. recognition understanding asru waikoloa december pages https//doi.org/./asru... tomas mikolov martin karaﬁ´at luk´as burget cernock´y sanjeev khudanpur. recurrent neuinterspeech network based language model. annual conference international speech communication association makuhari chiba japan september pages cernock´y sanjeev khudanpur. extensions recurrent neural network language model. proceedings ieee international conference acoustics speech signal processing icassp prague congress center prague czech republic. pages https//doi.org/./icassp... tomas mikolov ilya sutskever chen gregory corrado jeffrey dean. distributed representations words phrases compositionality. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states.. pages andriy mnih geoffrey hinton. scalable hiadvances erarchical distributed language model. neural information processing systems proceedings twenty-second annual conference neural information processing systems vancouver british columbia canada december pages frederic morin yoshua bengio. hierarchical probabilistic neural network language model. proceedings tenth international workshop artiﬁcial intelligence statistics aistats bridgetown barbados january arvind neelakantan jeevan shankar alexandre passos andrew mccallum. efﬁcient non-parametric estimation multiple embeddings word vector space. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group acl. pages http//aclweb.org/anthology/d/d/d-.pdf. jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings conference empirical methods natural language processing emnlp maxij ξij. model hyperparameters training details loss function margin batch size initialize word embeddings uni] expectation variance mean zero initialize dimension diagonal matrix constant value also initialize mixture scores initial probabilities equal among components. threshold negative sampling recommended value wordvec skip-gram large datasets. also separate output embeddings addition input embeddings similar wordvec implementation word sets distributions gaussian mixture. given pair word context input distribution output distribution context optimize parameters trained input distributions ﬁnal word representations. mini-batch asynchronous gradient descent adagrad performs adaptive learning rate parameter. also experiment adam corrects bias adaptive gradient update adagrad proven popular recent neural network models. however found much slower adagrad gradient computation model relatively fast complex gradient update algorithm adam becomes bottleneck optimization. therefore choose adagrad allows better scale large datasets. linearly decreasing learning rate optimization constraint stability optimize since component diagonal vector constrained positive. similarly constrain probability optimizing unconstrained scores using softmax esik function convert scores probability loss computation numerically unstable elements diagonal covariances small therefore small term log(df constant becomes addition observe small would result eξij machine precision. order", "year": 2017}