{"title": "Bi-Objective Nonnegative Matrix Factorization: Linear Versus  Kernel-Based Models", "tag": ["stat.ML", "cs.CV", "cs.LG", "math.OC"], "abstract": "Nonnegative matrix factorization (NMF) is a powerful class of feature extraction techniques that has been successfully applied in many fields, namely in signal and image processing. Current NMF techniques have been limited to a single-objective problem in either its linear or nonlinear kernel-based formulation. In this paper, we propose to revisit the NMF as a multi-objective problem, in particular a bi-objective one, where the objective functions defined in both input and feature spaces are taken into account. By taking the advantage of the sum-weighted method from the literature of multi-objective optimization, the proposed bi-objective NMF determines a set of nondominated, Pareto optimal, solutions instead of a single optimal decomposition. Moreover, the corresponding Pareto front is studied and approximated. Experimental results on unmixing real hyperspectral images confirm the efficiency of the proposed bi-objective NMF compared with the state-of-the-art methods.", "text": "abstract—nonnegative matrix factorization powerful class feature extraction techniques successfully applied many ﬁelds namely signal image processing. current techniques limited single-objective problem either linear nonlinear kernelbased formulation. paper propose revisit multi-objective problem particular bi-objective objective functions deﬁned input feature spaces taken account. taking advantage sum-weighted method literature multi-objective optimization proposed bi-objective determines nondominated pareto optimal solutions instead single optimal decomposition. moreover corresponding pareto front studied approximated. experimental results unmixing real hyperspectral images conﬁrm efﬁciency proposed bi-objective compared state-of-the-art methods. provides parts-based representation nonnegative data entries becoming versatile technique plenty applications opposed dimensionality reduction approaches e.g. principal component analysis vector quantization linear discriminant analysis based additivity contributions bases approximate original data. decomposition model often yields tractable physical interpretation thanks sparse nonnegative obtained representation input data. many real world applications beneﬁt virtues including hyperspectral unmixing face facial expression recognition gene expression data blind source separation spectral clustering name few. approximates high-rank nonnegative input matrix nonnegative low-rank ones. consequence provides decomposition suitable many signal processing data analysis problems particular hyperspectral unmixing problem. indeed hyperspectral image cube consists images scene scrutiny corresponding ground scene light certain wavelength reﬂected. namely reﬂectance spectral wavelength range available pixel. assumed spectral mixture pure materials called endmembers. hyperspectral unmixing problem consists extracting endmembers estimating abundance endmember every pixel obviously physical interpretation requires nonnegativity abundances endmember spectrums. linear model since viewed input spectral approximated linear combination basis spectrums. estimate decomposition objective function minimization deﬁned euclidean space so-called input space difference input matrix product estimated ones usually measured either frobenius norm generalized kullback-leibler divergence objective functions often augmented including different regularization terms fisher constraint learning local features sparseness constraint intuitive easily interpretable decompositions temporal smoothness spatial decorrelation regularization minimum dispersion regularization unmixing accuracy objective functions also raised practical standpoints e.g. ℓ-norm robustness outliers missing data bregman divergence fast computational performance many studies shown limits linear decomposition opposed nonlinear research activities concentrated linear works considered nonlinear case. attempt extent linear models nonlinear scope several kernel-based proposed within framework offered kernel machines employing nonlinear function kernel-based methods mainly data higher dimensional space existing linear techniques performed transformed data. kernel trick enables estimation inner product pair mapped data reproducing kernel hilbert space so-called feature space without need knowing explicitly neither nonlinear function resulting space. example linear technique performed kernel matrix whose entries consist inner products input data calculated kernel function. kernel-based techniques presented follow similar scheme share additive assumption originated convex approach proposed basis matrix represented convex combination mapped input data feature space worth noting objective function frobenius norm residual kernel matrix factorization above-mentioned kernel-based methods. however although input data remainder paper organized follows. ﬁrst revisit conventional kernel-based nmf. differences input feature space optimization discussed section iii. section present proposed bi-objective framework. section demonstrates efﬁciency proposed method unmixing real hyperspectral images. conclusions future works reported section residual error measured input vector anten input space optimization operated two-block coordinate descent scheme alternating elements keeping elements matrix ﬁxed. straightforward generalization nonlinear form proposed within framework offered kernel machines. following present kernel-based recently proposed worth noting variants also investigated including ones studied however variants suffer pre-image problem making derivations study difﬁcult; details. matrix nonnegative nonnegativity mapped data guaranteed. severe disadvantage obtained bases feature space reverse mapping input space difﬁcult. indeed needs solve pre-image problem obstacle inherited kernel machines difﬁculties circumvented deﬁning model feature space optimized directly input space. paper revisit framework discover nonlinearity input matrix. section ii-b details. either linear conventional formulation nonlinear kernel-based formulation well variations tackling single-objective optimization problem. essence underlying assumption known prior linear model dominates nonlinear vice versa input data study. obtain prior information given input data practical real-world applications. moreover possible fusion linear nonlinear models reveals latent variables closer ground truth single model considered alone. independently framework combination linear model nonlinear ﬂuctuation recently studied chen richard honeine where former nonlinearity depends spectral content deﬁned post-nonlinear model latter. multiple-kernel learning approach studied bayesian approach investigated so-called residual component analysis. methods share major drawback consist estimating abundances nonlinear model endmembers need extracted preprocessing stage using conventional linear technique opposed separation optimization problems provides elegant framework solving jointly unmixing problem namely estimating endmembers abundances. best knowledge previous studies combine linear nonlinear models within framework. paper study bi-objective optimization problem performs simultaneously input feature spaces combining linear kernel-based models. ﬁrst objective function optimize stems conventional linear second objective function deﬁned feature space derived kernel-based model. case conﬂicting objective functions exists rather nondominated noninferior pareto optimal solutions opposed unique decomposition dealing exclusively objective function. order acquire pareto optimal solutions investigate sum-weighed method literature multi-objective optimization ease integrated proposed framework. moreover propose approximate corresponding pareto front. multiplicative update rules derived resulting sub-optimization problem case feature space induced gaussian kernel. convergence algorithm discussed well initialization stopping criteria. recently several works investigating combination linear model often advocated physical model additive nonlinear ﬂuctuation determined kernel-based term. model takes form belongs nonlinear feature space. several models proposed deﬁne nonlinearity outlined here. nonlinearity depends exclusively endmembers additive ﬂuctuation relaxed considering convex combination socalled multiple kernel learning. recently abundances incorporated nonlinear model post-nonlinear model studied bayesian approach used so-called residual component analysis. another model proposed context supervised learning. approaches consider endmembers already known estimated using linear techniques vertex component analysis nonlinearity investigated within abundances ant. opposed approaches method considered paper investigates also estimation endmembers nonlinear relation respect functions namely input feature space sense ill-deﬁned problem. indeed possible general common solution optimal objective functions. opposed single-objective optimization problems main focus would decision solution space namely space consider nonlinear function maps columns matrix well columns matrix input space feature space associated norm denoted corresponding inner product feature space form evaluated using so-called kernel function kernel machines. examples kernel functions gaussian polynomial kernels. difference linear nonlinear cases illustrated fig. linear sample approximated linear combination elements namely minimizing euclidean distance input nonlinear case using kernel-based formalism optimization considered feature space minimizing models corresponding optimization problems distinct ill-posed nonlinear optimization problem; details. shown literature investigating pre-image problem demonstrated recently unknown coefﬁcients depend making worth noting difference linear nonlinear case inherited framework kernel machines; drawback spans also multiple kernel combination several kernels focus paper work extends wide class kernel methods providing framework optimize input feature spaces shown next. ﬁxed value weight problem called suboptimization problem. solution suboptimization problem pareto optimal original bi-objective problem proven general case. solving suboptimization problem spread values weight obtain approximation pareto front. obvious model breaks single-objective conventional extreme case leads kernel optimization problem closed-form solution drawback inherited optimization problems nonnegativity constraints. moreover objective function nonconvex nonlinear making optimization problem difﬁcult solve. following propose iterative techniques purpose. noteworthy mention yields approximate optimal solution whose objective vector approximates point pareto front. substituting expressions given aggregated objective function becomes although nonconvex subproblem matrix ﬁxed convex. similar algorithms apply two-block coordinate descent scheme namely alternating elements keeping elements matrix ﬁxed. derivative respect entries bi-objective optimization problem brings focus objective space namely space objective vector belongs. beyond bi-objective optimization problem multi-objective optimization widely studied literature. taking advantage literature study solving bi-objective optimization problem revisit following deﬁnitions context pareto optimal given solution global pareto optimal dominated solution decision space objective vector corresponding pareto optimal cannot improved space without degradation space. various multi-objective optimization techniques proposed successfully applied engineering ﬁelds e.g. evolutionary algorithms sum-weighted algorithms ε-constraint method normal boundary intersection method name few. survey references therein methods multiobjective optimization. among existing methods sumweighted scalarization method always popular since straightforward easily implement. sum-weighted technique converts multi-objective problem single-objective problem combining multiple objectives. conditions objective vector corresponding latter’s optimal solution belongs convex part multi-objective problem’s pareto front. thus changing weights among objectives appropriately pareto front original problem approximated. drawbacks sum-weighted method reside nonconvex part pareto front unattainable even convex part front uniform spread weights frequently result uniform spread pareto points pareto front pointed nevertheless sum-weighted method practical view complexity problem nonconvex illposed np-hard. following formulation introduced previous section propose minimize bi-objective function nonnegativity matrices decision solution size corresponds entries unknown matrices following sum-weighted method widely-used approach tackle multi-objective expression gradient subtraction nonnegative terms i.e. nonnegative entries. stepsize corresponding obtain following multiplicative update division multiplication element-wise. proposed algorithm tolerates strictly positive matrices initial matrices. simple uniform distribution positive quadrant shown good initialization experiments. advantage algorithms stricter initial conditions required. instance proposed hyperspectral unmixing problem constrained minimum volume constrained initialize columns endmember matrix randomly chosen pixels image study. given weight stopping criterion two-fold either stationary point attained preset maximum number iterations reached. therefore algorithm stops n-th iteration respectively. therefore multiplicative update rules imply part karush-kuhn-tucker conditions. however conditions state necessary conditions local minimum. concerning nonconvex problem studied problem non-unique kktpoints local minimum guaranteed. similar multiplicative-type update rules proposed proposed algorithm lacks guaranteed optimality property since convergence stationary point always correspond local minimum. also discussions around convergence conventional independently theoretical lack convergence show next proposed algorithm provides relevant results also outperforms state-of-the-art methods. stepsize parameters balance rate convergence accuracy optimization differently depending iteration rectiﬁcation follow guarantee nonnegativity entries convergence slow sensitive stepsize value pointed seung following spirit latter paper provide multiplicative update rules proposed bi-objective nmf. without loss generality restrict presentation case gaussian kernel second objective function valid kernels corresponding multiplicative update rules derived using similar procedure. gaussian kernel incorporating expression gaussian kernel gradient objective function respect becomes multiplicative update rule elaborated using socalled split gradient method trick decomposes section performance proposed algorithm bi-objective demonstrated unmixing well-known hyperspectral images. approximation pareto front proposed comparison state-of-theart unmixing methods conducted. ﬁrst image depicted fig. urban image acquired hyperspectral digital imagery collection experiment sensor. left part pixels taken original pixels’ image. data consists channels covering bandwidth .µm. recommended clean bands high-snr interest. according ground truth provided studied area mainly composed grass tree building road. second image sub-image pixels selected well-known cuprite image acquired airborne visible/infrared imaging spectrometer data collected contiguous spectral bands wavelength ranging .µm. removal noisy bands spectral bands remain. investigated area known dominated three materials muscovite alunite cuprite. experiments conducted employing weight implies model varying gradually nonlinear gaussian conventional linear weight multiplicative update rules given applied maximum iteration number nmax initial matrices generated using uniform distribution. choose appropriate bandwidth gaussian kernel ﬁrst apply single objective gaussian images using candidate considering reconstruction error input feature space urban image cuprite image investigated since determine whole pareto front unrealistic nonlinear multi-objective optimization problem target approximate pareto front discrete points concept pareto optimal pareto front strict proposed algorithm solver suboptimization problem guaranteeing local minimum mention global minimum. obstacles inherited nonconvexity nonlinearity kernel-based problem. case pareto optimal pareto front refer actually candidate pareto optimal approximation pareto front respectively approximate pareto front discrete points operate follows value weight obtain solution algorithm proposed section iv-c; evaluating objective functions solution single point objective space. approximated pareto front urban cuprite images shown fig. fig. evolution objectives aggregated objective function evaluated solution obtained weight shown fig. urban image fig. cuprite image. images study solutions generated dominated since solutions pareto front outperform them respect objectives. reveals neither conventional linear regarding sum-weighted approach minimizer suboptimization problem proven pareto optimal original multi-objective problem i.e. corresponding objective vector belongs pareto front objective space practice obtain dominated solutions urban cuprite images respectively. phenomenon however surprising since exist multiple pareto optimal solutions problem objectives conﬂicting other claimed possible explanation could applied numerical optimization scheme weak convergence method failure solver ﬁnding global minimum urban image shown fig. fig. obtained solutions pareto optimal within objectives-conﬂicting interval regarding cuprite image observed fig. fig. objectives-conﬂicting interval pareto optimal solutions found using fact obtained solutions local pareto optimal dominated global pareto optimal pointed that nonconvex problem global solver generates global pareto optimal local pareto optimal interest front global pareto optimal. illustrated fig. fig. even distribution weight lead even spread solutions approximated pareto front. moreover nonconvex part pareto front cannot attained using weight. exactly case fig. fig. trivial nonconvex part approximated pareto front probably resulted nonoptimal solution suboptimization problem. main drawbacks sum-weighted method. nevertheless obtained approximation pareto front high value. hand provides pareto optimal solutions instead single decomposition. hand insight trade-off objectives reveals underlying linearity/nonlinearity data study illustrated following section. section study performance method unmixing problem hyperspectral imagery. unmixing performance evaluated metrics introduced ﬁrst reconstruction error input space deﬁned fig. illustration approximated pareto front objective space urban cuprite images. objective vectors non-dominated solutions marked approximate part pareto front; objective vectors dominated solutions marked blue. nonlinear gaussian best studied images. contrary pareto optimal solutions result points pareto front provide feasible nondominated decompositions decision maker i.e. user. worth noting apply sum-weighted method posteriori method different pareto optimal solutions generated makes ﬁnal comprise among optimal solutions. alternatively priori method speciﬁes weight advance generate solution. details. convex combination certain data points. kernel convex-nmf kernel semi-nmf based nonnegative least squares essentially kernelized variants convexnmf alternating nonnegativity constrainted least squares active method respectively discussed experiments also conducted kernel methods adopting gaussian kernel. nonlinear based constructing mercer kernels introduced addresses nonlinear problem using self-constructed gaussian kernel nonnegativity embedded bases coefﬁcients preserved. embedded data ﬁnally factorized conventional nmf. particular note reconstruction error feature space calculated aforementioned kernel-based methods since pre-images mapped endmember required computing reconstruction error input space cannot exploited. unmixing performance respect reconstruction errors input feature spaces compared aforementioned unmixing approaches demonstrated table table iii. observed proposed method pareto optimal solution outperforms unmixing problem comprises estimation endmembers corresponding abundance maps. existing techniques either extract endmembers estimate abundances methods enable simultaneous estimations e.g. variants. brieﬂy present unmixing algorithms used comparison. most-known endmember extraction technique vertex component analysis based linear mixture model presumes existence endmembers within image analysis. seeks inﬂate simplex enclosing spectra. endmembers vertices largest simplex. technique applied endmember extraction jointly three abundance estimation techniques fcls k-hype gbm-snmf. fully constrained least squares algorithm least square approach using linear mixture model abundances estimated considering nonnegativity sum-to-one constraints. nonlinear unmixing model abundance estimation considered nonlinear term described kernel-based model so-called linear-mixture/nonlinear-ﬂuctuation model generalized bilinear model formulated parameters optimized using semi-nonnegative matrix factorization consider nmf-based techniques capable estimate endmembers abundances jointly. minimum dispersion constrained includes dispersion regularization conventional integrating sum-to-one constraint pixel’s abundance fractions minimization variance within endmember. problem solved exploiting alternate projected gradient scheme. convex nonnegative matrix factorization basic matrix restricted span input data sample viewed estimated endmembers corresponding abundance maps proposed method shown fig. fig. urban image different areas better recognized pareto optimal compared solutions linear gaussian nmf. regarding cuprite image linear recognizes three regions; whereas pareto optimal gaussian able distinguish three regions. however abundance maps gaussian appear overly sparse compared counterpart pareto optimal solution. also noticed endmembers extracted linear spiky even zero-parts thus meeting poorly real situation. paper presented novel bi-objective nonnegative matrix factorization exploiting kernel machines decomposition performed simultaneously input feature space. multiplicative update rules derived. performance method demonstrated unmixing well-known hyperspectral images. resulting pareto fronts analyzed. future work extending approach include objective functions deﬁned input feature space. considering simultaneously several kernels consequence several feature spaces also investigation. honeine kallas kernel non-negative matrix factorization without pre-image problem ieee workshop machine learning signal processing reims france sep. flierl ruan kleijn graph-preserving sparse nonnegative matrix factorization application facial expression recognition ieee transactions systems cybernetics part cybernetics vol. feb. cichocki zdunek s.-i. amari algorithms nonnegative matrix factorization applications blind source separation ieee international conference acoustics speech signal processing vol. ding relationships among various nonnegative matrix factorization methods clustering proceedings sixth international conference data mining ser. icdm washington ieee computer society chen cichocki nonnegative matrix factorization temporal smoothness and/or spatial decorrelation constraints laboratory advanced brain signal processing riken tech. huck guillaume blanc-talon minimum dispersion constrained nonnegative matrix factorization unmix hyperspectral data ieee transactions geoscience remote sensing vol. jun. kanade robust norm factorization presence outliers missing data alternative convex programming ieee computer society conference computer vision pattern recognition vol. lebanon park fast bregman divergence using taylor expansion coordinate descent. proceedings sigkdd international conference knowledge discovery data mining. chen richard honeine nonlinear estimation material abundances hyperspectral images ℓ-norm spatial regularization ieee transactions geoscience remote sensing vol. available http//ieeexplore.ieee.org/stamp/stamp.jsp?arnumber= nguyen chen richard honeine theys supervised nonlinear unmixing hyperspectral images using pre-image method concepts imaging optical statistical models eds. mary theys aime ser. publications series. sciences vol. ngom kernel non-negative matrix factorization application microarray data analysis ieee symposium computational intelligence bioinformatics computational biology diego may. chen richard honeine estimating abundance fractions materials hyperspectral images ﬁtting post-nonlinear mixing model proc. ieee workshop hyperspectral image signal processing evolution remote sensing jun. nonlinear unmixing hyperspectral images based multikernel learning proc. ieee workshop hyperspectral image signal processing evolution remote sensing jun. altmann dobigeon mclaughlin j.-y. tourneret residual component analysis hyperspectral images application joint nonlinear unmixing nonlinearity detection. nascimento bioucas dias vertex component analysis fast algorithm unmix hyperspectral data ieee transactions geoscience remote sensing vol. apr. honeine kallas kernel nonnegative matrix factorization without curse pre-image ieee transactions pattern analysis machine intelligence preprint. kallas honeine richard francis amoud nonnegativity constraints pre-image pattern recognition kernel machines pattern recognition vol. scholkopf mika burges knirsch muller ratsch smola input space versus feature space kernel-based methods ieee transactions neural networks vol. zitzler thiele multiobjective evolutionary algorithms comparative case study strength pareto approach ieee transactions evolutionary computation vol. nov. dennis closer look drawbacks minimizing weighted sums objectives pareto generation multicriteria optimization problems structural optimization vol. brub gendreau potvin exact ε-constraint method bi-objective combinatorial optimization problems application traveling salesman problem proﬁts european journal operational research vol. dennis normal-boundary intersection method generating pareto surface nonlinear multicriteria optimization problems siam journal optimization vol. lant´eri theys richard mary regularized split gradient method nonnegative matrix factorization ieee international conference acoustics speech signal processing miao endmember extraction highly mixed data using minimum volume constrained nonnegative matrix factorization ieee transactions geoscience remote sensing vol. mar. gonzales zhang accelerating lee-seung algorithm non-negative matrix factorization department computational applied mathematics rice university tech. rep. halimi altmann dobigeon tourneret nonlinear unmixing hyperspectral images using generalized bilinear model ieee transactions geoscience remote sensing vol. nov. honeine richard geometric unmixing large hyperspectral images barycentric coordinate approach ieee transactions geoscience remote sensing vol. jun. heinz chang fully constrained least squares linear spectral mixture analysis method material quantiﬁcation hyperspectral imagery ieee transactions geoscience remote sensing vol. mar. yokoya chanussot iwasaki nonlinear unmixing hyperspectral data using semi-nonnegative matrix factorization ieee transactions geoscience remote sensing vol. feb. park nonnegative matrix factorization based alternating nonnegativity constrained least squares active method siam journal matrix analysis applications vol. jul. born liaoning china received degrees mathematics applied mathematics economics xi’an jiaotong university xi’an degree systems optimization security university technology troyes troyes france. currently working toward ph.d. degree university technology troyes research interests include hyperspectral image analysis. paul honeine born beirut lebanon october received dipl.-ing. degree mechanical engineering m.sc. degree industrial control faculty engineering lebanese university lebanon. received ph.d. degree systems optimisation security university technology troyes france postdoctoral research associate systems modeling dependability laboratory since september assistant professor university technology troyes france. research interests include nonstationary signal analysis classiﬁcation nonlinear statistical signal processing sparse representations machine learning. particular interest applications sensor networks biomedical signal processing hyperspectral imagery nonlinear adaptive system identiﬁcation. co-author best paper award ieee workshop machine learning signal processing. past years published peerreviewed papers.", "year": 2015}