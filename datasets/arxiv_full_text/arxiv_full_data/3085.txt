{"title": "On the Lagrangian Biduality of Sparsity Minimization Problems", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Recent results in Compressive Sensing have shown that, under certain conditions, the solution to an underdetermined system of linear equations with sparsity-based regularization can be accurately recovered by solving convex relaxations of the original problem. In this work, we present a novel primal-dual analysis on a class of sparsity minimization problems. We show that the Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of the sparsity minimization problems can be used to derive interesting convex relaxations: the bidual of the $\\ell_0$-minimization problem is the $\\ell_1$-minimization problem; and the bidual of the $\\ell_{0,1}$-minimization problem for enforcing group sparsity on structured data is the $\\ell_{1,\\infty}$-minimization problem. The analysis provides a means to compute per-instance non-trivial lower bounds on the (group) sparsity of the desired solutions. In a real-world application, the bidual relaxation improves the performance of a sparsity-based classification framework applied to robust face recognition.", "text": "recent results compressive sensing shown that certain conditions solution underdetermined system linear equations sparsity-based regularization accurately recovered solving convex relaxations original problem. work present novel primal-dual analysis class sparsity minimization problems. show lagrangian bidual sparsity minimization problems used derive interesting convex relaxations bidual minimization problem -minimization problem; bidual minimization problem enforcing group sparsity structured data ∞minimization problem. analysis provides means compute per-instance non-trivial lower bounds sparsity desired solutions. realworld application bidual relaxation improves performance sparsitybased classiﬁcation framework applied robust face recognition. last decade seen renewed interest problem solving underdetermined system equations rm×n regularizing solution sparse i.e. non-zero entries. speciﬁcally aims least number nonzero entries solves linear system problem known -minimization problem intended seek entry-wise sparsity known np-hard general. compressive sensing literature shown solution often obtained solving tractable linear program namely -minimization research area broad applications included sparse error correction compressive imaging image denoising restoration face recognition name few. addition enforcing entry-wise sparsity linear system equations notion group sparsity attracted increasing attention recent years case assumes matrix underlying structure grouped blocks enforcing group sparsity exploits problem’s underlying structure improve solution’s interpretability. example sparsity-based classiﬁcation framework applied face recognition columns vectorized training images human faces naturally grouped blocks corresponding different subject classes vectorized query image entries represent coefﬁcients linear combination training images reconstructing group sparsity lends naturally problem since desirable images smallest number subject classes reconstruct subsequently classify query image. furthermore problem robust face recognition considered interesting modiﬁcation known cross-and-bouquet model represents possible sparse error corruption observation argued model solved group sparsity problem coefﬁcients would group. however problem trivial solution would smallest possible group sparsity. hence necessary regularize entry-wise sparsity effect considers mixture previous cases aims enforce entrywise sparsity well group sparsity number non-zero blocks reconstruction error also sparse. mixed sparsity minimization problem posed controls tradeoff entry-wise sparsity group sparsity. counting norm optimization problems also np-hard general. hence several recent works focused developing tractable convex relaxations problems. case group sparsity relaxation involves replacing p-norm xkp. relaxations p-norm also used mixed sparsity case work interested deriving analyzing convex relaxations general sparsity minimization problems. entry-wise case main theoretical understanding link original np-hard problem convex relaxation given simple fact -norm convex surrogate -norm. however group sparsity case similar relaxation produces family convex surrogates i.e. whose value depends raises question whether preferable value relaxation group sparsity minimization problem? fact consider following important question present optimization-theoretic framework based lagrangian duality deriving convex relaxations sparsity minimization problems. speciﬁcally introduce class equivalent optimization problems derive lagrangian duals original np-hard problems. consider lagrangian dual lagrangian dual optimization problem term lagrangian bidual primal problem. show lagrangian biduals convex relaxations original sparsity minimization problems. importantly show lagrangian biduals problems correspond minimizing -norm ∞-norm respectively. since lagrangian duals linear programs duality lagrangian duals corresponding lagrangian biduals. therefore bidual based convex relaxations interpreted maximizing lagrangian duals original sparsity minimization problems. provides interpretations relaxations sparsity minimization problems. moreover since lagrangian dual minimization problem provides lower bound optimal value primal problem show optimal objective value convex relaxation provides non-trivial lower bound sparsity true solution primal problem. follows derive lagrangian bidual mixed sparsity minimization problem generalizes entry-wise sparsity group sparsity cases speciﬁcally derive lagrangian bidual following optimization problem given unique ﬁnite solution exists constant absolute values entries less namely note unique solution might possible choose ﬁnite-valued upper bounds solutions. case ﬁnite-valued viewed regularization term desired solution. effect consider following modiﬁed version introduce constraint chosen described ensure optimal values same. primal problem. frame equivalent optimization problem introduce notation. entry-based sparsity indicator namely otherwise. also introduce group-based sparsity indicator vector whose entry denotes whether block contains non-zero entries namely otherwise. express constraint introduce matrix }n×k entry belongs block otherwise. finally denote positive component negative component respectively given deﬁnitions reformulated order obtain lagrangian dual function need minimize respect notice coefﬁcients non-zero minimization i.e. respect unbounded below. effect constraints coefﬁcients equal form constraints dual variables. next consider minimization respect since entry takes values optimal value minimizes given notice made changes going first replaced constraints constraint eliminated second introduced variables encode operator objective function lagrangian bidual. consider lagrangian dual referred lagrangian bidual veriﬁed lagrangian dual given notice going discrete valued variables relaxed take real values given noting represented x+−x− conclude constraint solution satisﬁes moreover given relaxed take real values optimal values respectively. hence eliminate constraints replacing optimal values. veriﬁed solving equivalent solving problem section ﬁrst describe properties biduality framework general. focus important results special cases entry-wise sparsity group sparsity. theorem optimal value lagrangian bidual lower bound optimal value np-hard primal problem proof. since duality linear program lagrangian dual optimal values lagrangian dual lagrangian bidual same. moreover know optimal value primal minimization problem always bounded optimal value lagrangian dual hence required result. remark since original primal problem np-hard note duality between primal dual non-zero general. moreover notice increase optimal value primal unchanged optimal value bidual decreases. hence duality increases increases. preferably equal primal∞ possible estimate accurately practice. therefore interest analyze effect taking conservative estimate i.e. choosing large value follows show taking conservative estimate equivalent dropping constraint bidual. results entry-wise sparsity minimization notice substituting optimization problem reduces entry-wise sparsity minimization problem hence lagrangian bidual m-regularized entry-wise sparsity problem precisely well-known -norm relaxation framework therefore provides interpretation relaxation remark -norm minimization problem lagrangian bidual -norm minimization problem solving equivalent maximizing dual note solution derive non-trivial lower bound primal objective function precisely sparsity desired solution. speciﬁcally theorem conclude following result corollary results group sparsity minimization notice substituting optimization problem reduces group sparsity minimization problem hence lagrangian bidual group sparsity problem convex ∞-norm relaxation p-min problem words biduality framework selects ∞-norm entire family p-norms convex surrogate p-norm. finally theorem show solution obtained minimizing ∞-norm provides lower bound group sparsity. corollary p∞-norm seems interesting choice computing lower bound group sparsity compared p-norms ﬁnite example consider case p-norm equivalent -norm. assume consists single block several columns maximum number non-zero blocks denote solution -minimization problem possible construct examples hence unclear general solutions obtained minimizing p-norms ﬁnite-valued help provide lower bounds group sparsity. present experiments evaluate bidual framework minimizing entry-wise sparsity mixed sparsity. present experiments synthetic data show framework used compute non-trivial lower bounds entry-wise sparsity minimization problem. consider face recognition problem compare performance bidual-based ∞-norm relaxation -norm relaxation mixed sparsity minimization. boxplots provide concise representation results’ statistics. bottom edge boxplot values indicates maximum minimum values. bottom extents indicate percentile mark. mark indicates median crosses outside boxes indicate potential outliers. entry-wise sparsity. explore practical implications corollary synthetic experiments. randomly generate entries gaussian distribution unit variance. sparsity varied steps solve using corollary compute lower bounds true sparsity i.e. repeat experiment times sparsity level figure shows boxplots bounds computed experiments. ﬁrst analyze lower bounds computed figure explained section bounds expected tight duality gap. notice extremely sparse solutions maximum computed bounds close true sparsity diverges sparsity reduces. median value bounds much looser median also diverges sparsity reduces. furthermore computed lower bounds seem grow linearly function true sparsity. similar trends observed figures respectively. expected discussion section bounds become loose increases. figure results computing lower bounds true entry-wise sparsity obtained trials. bounds computed solving using corollary notice expected discussion section bounds tight duality become looser increases. theory would like per-instance certiﬁcates-of-optimality computed solution lower bound equal true sparsity nonetheless note ability compute per-instance non-trivial lower bound sparsity desired solution important step forward respect previous approaches require pre-computing optimality conditions equivalence solutions -norm -norm minimization problems. performed similar experiment group sparsity case observed bidual framework able provide non-trivial lower bounds group sparsity also. mixed sparsity. evaluate results mixed sparsity minimization sparsity-based face recognition problem columns represent training images face classes a··· represents query image. assume subset pixel values query image corrupted disguised. hence error image space modeled sparse error term uncorrupted image. linear representation query image forms following linear system equations identity matrix. goal sparsity-based classiﬁcation minimize group sparsity sparsity dominant non-zero coefﬁcients reveal membership ground-truth observation experiments solve solving following optimization problem notice reduces solving special case problem i.e. bidual relaxation mixed sparsity problem conservative estimate experiments compare solutions obtained using evaluate algorithms subset dataset manually aligned frontal face images size male female subjects i.e. individual contributes un-occluded training images un-occluded testing images occluded testing images. hence training images testing images. compute number non-zero blocks coefﬁcient estimated testing image number blocks whose energy greater speciﬁed threshold. results experiments presented figure solution obtained gives better group sparsity however sparser error estimated number non-zero entities solution i.e. number non-zero blocks plus number nonzero error entries lower solution obtained using rather obtained using however primal mixed-sparsity objective value lower solution obtained using figure comparison mixed sparsity solutions present boxplots group sparsity entry-wise sparsity differences calculated better group sparsity sparse error compare classiﬁcation results obtained solutions computed experiments. classiﬁcation consider non-zero blocks assign query image block i.e. subject class gives least residual akxk. results presented table notice classiﬁcation results obtained better obtained using since classiﬁcation un-occluded images already good using classiﬁcation gives minor improvement case. however tangible improvement noticed classiﬁcation occluded images. therefore classiﬁcation general better obtained considered state-of-the-art sparsity-based classiﬁcation presented novel analysis several sparsity minimization problems allows interpret several convex relaxations original np-hard primal problems equivalent maximizing lagrangian duals. pivotal point analysis formulation mixedinteger programs equivalent original primal problems. derived biduals sparsity minimization problems techniques also used easily derive convex relaxations sparsity minimization problems interesting result biduality framework ability compute per-instance certiﬁcate optimality providing lower bound primal objective function. contrast previous research aims characterize either subset solutions conditions perfect sparsity recovery using convex relaxations cases conditions either weak hard verify. importantly conditions needed precomputed opposed verifying correctness solution run-time. lieu this hope proposed framework prove important step towards per-instance veriﬁcation solutions. speciﬁcally interest future explore tighter relaxations veriﬁcation solutions. research supported part muri wnf--- mast-cta wnf--- cns- cns- grant views conclusions contained document authors interpreted representing ofﬁcial policies either expressed implied army research laboratory u.s. government. u.s. government authorized reproduce distribute government purposes notwithstanding copyright notation herein.", "year": 2012}