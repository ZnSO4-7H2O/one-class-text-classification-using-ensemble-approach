{"title": "Fraternal Dropout", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.", "text": "recurrent neural networks form important class architectures among neural networks useful language modeling sequential prediction. however optimizing rnns known harder compared feed-forward neural networks. number techniques proposed literature address problem. paper propose simple technique called fraternal dropout takes advantage dropout achieve goal. speciﬁcally propose train identical copies different dropout masks minimizing difference predictions. regularization encourages representations rnns invariant dropout mask thus robust. show regularization term upper bounded expectation-linear dropout objective shown address difference train inference phases dropout. evaluate model achieve state-of-the-art results sequence modeling tasks benchmark datasets penn treebank wikitext-. also show approach leads performance improvement signiﬁcant margin image captioning semi-supervised tasks. recurrent neural networks like long short-term memory networks gated recurrent unit popular architectures sequence modeling tasks like language generation translation speech synthesis machine comprehension. however harder optimize compared feed-forward networks challenges like variable length input sequences repeated application transition operator time step largely-dense embedding matrix depends vocabulary size. optimization challenges rnns application batch normalization variants successful counterparts feed-forward networks although considerably provide performance gains. similarly naive application dropout shown ineffective rnns therefore regularization techniques rnns active area research. address challenges zaremba proposed apply dropout nonrecurrent connections multi-layer rnns. variational dropout uses dropout mask throughout sequence training. dropconnect applies dropout operation weight matrices. zoneout similar spirit dropout randomly chooses previous time step hidden state instead using current one. similarly substitute batch normalization layer normalization normalizes hidden units within sample zero mean unit standard deviation. recurrent batch normalization applies batch normalization unshared mini-batch statistics time step merity merity hand show activity regularization temporal activation regularization also effective methods regularizing lstms. another recent regularizing rnns similar spirit approach take involves minimizing difference hidden states original auxiliary network serdyuk paper propose simple regularization based dropout call fraternal dropout minimize equally weighted prediction losses identical copies lstm different dropout masks regularization difference predictions networks. analytically show regularization objective equivalent minimizing variance predictions different i.i.d. dropout masks; thus encouraging predictions invariant dropout masks. also discuss regularization related expectation linear dropout π-model laine aila activity regularization merity empirically show method provides non-trivial gains related methods explain furthermore ablation study dropout powerful regularization neural networks. usually effective densely connected layers suffer overﬁtting compared convolution layers parameters shared. reason dropout important regularization rnns. however dropout training inference phase since latter phase assumes linear activations correct factor expected value activation would different addition prediction models dropout generally vary different dropout mask. however desirable property cases would ﬁnal predictions invariant dropout masks. such idea behind fraternal dropout train neural network model encourages variance predictions different dropout masks small possible. specifically consider model denoted takes input denotes prediction model input sample model parameters. corresponding time step loss value overall input-target sample pair fraternal dropout simultaneously feed-forward input sample identical copies share parameters different dropout masks time step yields loss values time step given analytically showed expected error model’s expected prediction dropout masks prediction using average mask upper bounded. based result propose explicitly minimize difference speciﬁcally achieved feed-forwarding input twice network withdropout mask minimizing main network loss along regularization term speciﬁed goal minimize network loss along expected difference prediction individual dropout mask prediction expected dropout mask. note regularization objective upper bounded expectation-linear dropout regularization shown proposition ˜reld. result shows minimizing objective indirectly minimizes regularization term. finally indicated above apply target loss network dropout. fact ablation studies back-propagating target loss network makes optimizing model harder. however setting simultaneously backpropagating target loss networks yields performance gain well convergence gain. believe convergence faster regularization network weights likely target based updates back-propagation case. especially true weight dropout since case dropped weights updated training iteration. laine aila propose π-model goal improving performance classiﬁcation tasks semi-supervised setting. propose model similar except apply target loss networks time-dependent weighting function intuition case leverage unlabeled data using minimize difference prediction between copies network different dropout masks. further also test model supervised setting fail explain improvements obtain using regularization. note case analytically show minimizing regularizer equivalent minimizing variance model predictions furthermore also show relation regularizer expectation linear dropout section study effects target based loss networks used π-model. applying target loss networks leads signiﬁcantly faster convergence. finally bring attention temporal embedding claimed better version π-model semi-supervised learning) intractable natural language processing applications storing averaged predictions time steps would memory exhaustive ﬁnal note argue supervised case using time-dependent weighting function instead constant value needed. since ground truth labels known observed problem mentioned laine aila network gets stuck degenerate solution large earlier epochs training. note much easier search optimal constant value true case opposed tuning time-dependent function. similarity π-model makes method related semi-supervised works mainly rasmus sajjadi since semi-supervised learning primary focus paper refer laine aila details. another address train evaluation mode dropout perform monte carlo sampling masks average predictions evaluation used feed-forward networks. technique work well rnns. details experiments found appendix. case language modeling test model benchmark datasets penn tree-bank dataset wikitext- dataset preprocessing speciﬁed mikolov moses tokenizer koehn datasets awd-lstm -layer architecture described merity call baseline model. number parameters model used million compared million case larger vocabulary size larger embedding matrix. apart differences architectures identical. fraternal dropout simply regularization baseline model. word level penn treebank inﬂuenced melis goal make sure fraternal dropout outperforms existing methods simply extensive hyper-parameter grid search rather regularization effects. hence experiments leave vast majority hyper-parameters used baseline model unchanged i.e. embedding hidden states sizes gradient clipping value weight decay values used dropout layers however changes necessary ﬁnal change hyper-parameters alter non-monotone interval used nonmonotonically triggered averaged optimizer polyak juditsky mandt melis grid search obtain evaluate model using perplexity metric compare results obtain existing state-of-the-art results. results reported table approach achieves state-of-the-art performance compared existing benchmarks. conﬁrm gains robust initialization experiments baseline model different seeds dataset compute conﬁdence intervals. average best validation perplexity minimum value equals test perplexity respectively. score beats ordinal dropout minimum values. also perform experiments using fraternal dropout grid search hyper-parameters leads improvements performance. details experiment found section word level wikitext- case wikitext- language modeling task outperform current state-of-the-art using perplexity metric signiﬁcant margin. lack computational power single training procedure fraternal dropout dataset larger ptb. experiment best hyper-parameters found dataset wt). ﬁnal results presented table also apply fraternal dropout image captioning task. well-known show tell model baseline emphasize image captioning task image encoder sentence decoder architectures usually learned together. since want focus beneﬁts using fraternal dropout rnns frozen pretrained resnet- model image encoder. means results directly comparable state-of-the-art methods however report results original methods readers baseline performs well. ﬁnal results presented table argue task smaller values optimal image captioning encoder given information beginning hence variance consecutive predictions smaller unconditioned natural language processing tasks. fraternal dropout beneﬁts mainly averaging gradients different mask hence updating weights frequently. section goal study existing methods closely related expectation linear dropout π-model laine aila activity regularization merity experiments ablation studies apply single layer lstm hyper-parameters model architecture melis relation expectation-linear dropout discussed section perform experiments study difference performance using regularization versus regularization addition also study modiﬁcation applies target loss copies lstms similar finally also evaluate baseline model without regularizations. learning dynamics curves shown figure regularization performs better terms convergence compared methods. terms generalization similar baseline eldm much worse. interestingly looking train validation curves together eldm seems suffering optimization problems. since π-model laine aila similar algorithm study difference performance π-model qualitatively quantitatively establish advantage approach. first single layer lstm -layer awd-lstm task check model compares case language modeling. results shown figure model converges signiﬁcantly faster π-model. believe happens back-propagate target loss networks leads weights getting updated using target-based gradients often. batch size truncated back-propagation time steps constant zero state provided initial state probability learning rate multiplied whenever validation performance improve ever epochs weight dropout hidden hidden matrix dropout every word mini-batch probability embedding dropout output dropout gradient clipping weight decay input embedding size input/output size lstm embedding size embedding weights tied coefﬁcient π-model hence focus experiment evaluate difference performance target loss backpropagated networks additionally tuning function instead using constant coefﬁcient infeasible. figure ablation study train validation perplexity word level modeling single layer lstm curves study learning dynamics baseline model π-model expectation-linear dropout expectation-linear dropout modiﬁcation fraternal dropout converges faster regularizers comparison generalizes par. even though designed algorithm speciﬁcally address problems rnns fair comparison compare π-model semi-supervised task goal. speciﬁcally cifar- dataset consists images classes. following usual splits used semi-supervised learning literature thousand labeled thousand unlabeled samples training thousand labeled samples validation thousand labeled samples test set. original resnet- architecture. grid search dropout rates leave rest hyperparameters unchanged. additionally check importance using unlabeled data. results reported table algorithm performs π-model. unlabeled data used fraternal dropout provides slightly better results compared traditional dropout. table ablation study accuracy altered cifar- dataset resnet- based models. algorithm performs π-model. unlabeled data used traditional dropout hurts performance fraternal dropout provides slightly better results. means methods beneﬁcial lack data additional regularizing methods. figure ablation study train validation perplexity word level modeling single layer lstm curves study learning dynamics baseline model temporal activity regularization prediction regularization activity regularization fraternal dropout converges faster generalizes better regularizers comparison. encapsulates term along product term perform experiments conﬁrm gains approach regularization alone. similar argument goes objective. grid search include hyper-parameters mentioned merity regularization furthermore also compare regularization regularizes regularization. based grid search pick best model validation regularizations additionally report baseline model without four mentioned regularizations. learning dynamics shown figure regularization performs better terms convergence generalization compared methods. average hidden state activation reduced regularizer described applied conﬁrm models trained fraternal dropout beneﬁt nt-asgd ﬁne-tuning step however time-consuming practice since different hyper-parameters used additional part learning procedure probability obtaining better results extensive grid search higher. hence experiments ﬁne-tuning procedure implemented ofﬁcial repository present importance ﬁne-tuning table table ablation study candidate hyper-parameters possible used grid search comparing fraternal dropout expectation linear dropout. uniform distribution interval ﬁnite sets value drawn equal probability. table ablation study fraternal dropout expectation linear dropout comparison. perplexity penn treebank validation dateset. fraternal dropout robust different hyperparameters choice twice much runs ﬁnished performing better baseline model perform extensive grid search baseline model subsection trained either fraternal dropout expectation linear dropout regularizations contrast performance methods. experiments without ﬁne-tuning dataset. dropout rates randomly altered multiplied value drawn uniform distribution interval rest hyper-parameters drawn shown table subsection regularizers deactivated. together experiments. results presented table perform better baseline model instead uses regularizers. hence conﬁrm previous ﬁnding better. however found previously smaller model subsection convergence faster eld. additionally fraternal dropout robust different hyper-parameters choice paper propose simple regularization method rnns called fraternal dropout acts regularization reducing variance model predictions across different dropout masks. show model achieves state-of-the-art results benchmark language modeling tasks along faster convergence. also analytically study relationship regularization expectation linear dropout perform number ablation studies evaluate model different aspects carefully compare related methods qualitatively quantitatively. authors would like acknowledge support following agencies research funding computing support nserc cifar ivado. would like thank rosemary philippe lacaille thoughts comments throughout project. would also like thank stanisław jastrz˛ebski† evan racah† useful discussions. philipp koehn hieu hoang alexandra birch chris callison-burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens moses open source toolkit statistical machine translation. proceedings annual meeting interactive poster demonstration sessions association computational linguistics david krueger tegan maharaj jános kramár mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio hugo larochelle aaron courville zoneout regularizing rnns randomly preserving hidden activations. arxiv preprint arxiv. césar laurent gabriel pereyra philémon brakel ying zhang yoshua bengio. batch normalized recurrent neural networks. acoustics speech signal processing ieee international conference ieee mehdi sajjadi mehran javanmardi tolga tasdizen. regularization stochastic transformations perturbations deep semi-supervised learning. advances neural information processing systems nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. corr abs/. http//arxiv.org/abs/. well known address train evaluation mode dropout perform monte carlo sampling masks average predictions evaluation used feed-forward networks. since fraternal dropout addresses problem would like clarify straight-forward feasible apply mc-eval rnns. feed-forward networks average output prediction scores different masks. however case rnns perform evaluation problematic. follows online averaging consider ﬁrst make prediction time step using different masks averaging prediction score. output feed input time step different masks time step generate output time step order rnns work also need feed previous time hidden state time step would average hidden states different masks time step hidden space general highly nonlinear clear averaging space good strategy. approach justiﬁed. besides strategy whole extremely time consuming would need sequentially make predictions multiple masks time step. sequence averaging let’s consider different mask time want generate sequence average prediction scores compute argmax actual generated sequence. case notice guaranteed predicted word time step averaging predictions would lead next word feed time step output input time step example different dropout masks probability time step outputs probability time step outputs averaged prediction score followed argmax result prediction would incorrect. similar concern applies output predictions varying temporal length. nonetheless experiments dataset using mc-eval start simple comparison compares fraternal dropout averaged mask awd-lstm -layer baseline single ﬁxed mask call model performs much worse fraternal dropout. hence would hard model practice single sample inaccurate. also check mc-eval larger number models ﬁnal results worse baseline uses averaged mask. comparison also evaluate note ﬁne-tuning used experiments. table appendix monte carlo evaluation. perplexity penn treebank word level language modeling task using monte carlo sampling fraternal dropout average mask. variance prediction accumulates among time steps rnns since share parameters time steps value step. feed-forward networks layers usually share parameters hence want different values different layers simple alleviate problem apply regularization term pre-softmax predictions value layers. however believe limit possible gains. best performing architectures usually kind dropout often high dropout rates however true feed-forward networks. instance resnet architectures often dropout seen paper unlabeled data used regular dropout hurts performance using fraternal dropout seems improve little. mentioned before image recognition tasks experiment something would temporarily fraternal augmentation hence force given neural network predictions different augmentations.", "year": 2017}