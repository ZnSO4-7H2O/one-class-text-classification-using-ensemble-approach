{"title": "Fast nonparametric clustering of structured time-series", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.", "text": "progression. high cost measurement limited temporal resolution underlying system usually dictates small number time points measurement process subject technical biological variation. groups data occur naturally time series taken different patients clinical trial thus grouping patient number; measurements taken development related species subject replicated experiments. envisage level grouping might different patients measurements taken different hospitals developmental time series taken different laboratories using different technologies. whilst inference tractable inference requires numerical procedure gibbs sampling variational approaches. whilst variational methods widely acknowledged faster sampling need even faster inference methods. particular faster inference allows exploration larger datasets given computational resources avoids common practice applying crude ﬁltering reduce size data prior modelling. propose fast inference scheme based recent work hensman novel derivation variational bayes amalgamates several ideas literature. first construct kl-corrected bound marginal likelihood objective function depends approximate distribution clustering variables. model parameters marginalised constructing lower bound conditional likelihood explicitly parameterised optimization. makes parameter-space optimization signiﬁcantly smaller. reduced parameter space make riemann structure approximation derive natural gradient closely linked vbem procedure. using approximate geometrical conjugacy manifold implement conjugate natural gradient approach outperforms vbem free-form optimization. gaussian process methods applied gene expression time-series several aims infer transcription regulation dynamic differential expression recently proposed hierarchical gaussian processes modelling gene expression showed simple proposal abstract—in publication combine bayesian nonparametric models gaussian process dirichlet process innovation model introduce variation prior enables model structured time-series data i.e. data containing groups wish model interintra-group variability. innovation model implementation fast collapsed variational inference procedure enables optimize variational approximation signiﬁcantly faster standard approaches. biological time series application show model better captures salient features data leading better consistency existing biological classiﬁcations associated inference algorithm provides signiﬁcant speed-up em-based variational inference. consider problem modelling clustering structured time series. turn tools ﬁeld bayesian nonparametrics using gaussian processes model time series dirichlet processes model clusterings. model constructed follows. given data partitioned disjoint groups construct hierarchical model using model group single additional model prior mean whole set. general behaviour groups governed last deviations group mean behaviour also modelled envisage many applications groupings sub-divisions model using additional layers hierarchy. further construct model level partition unknown apriori i.e. case clustering using dirichlet process prior base distribution atom becomes prior mean hierarchical allows perform inference clusters hierarchically grouped data. model inspired analysis gene-expression time-series data. previous models clustering time-series using models failed account structure data previously proposed inference procedures scale well. biological application values gene expression measured regular irregular time intervals spanning phenomenon development disease hensman n.d. lawrence department computer science shefﬁeld institute translational neuroscience university shefﬁeld rattray faculty life science university manchester much improved modelling time-series various applications. park choi also proposed method based hierarchical gaussian processes. presentation conceptually similar objective saving computation performing gaussian process regression behseta proposed hierarchical gaussian process model application neuronal poisson-process intensities. clustering gene expression time series application attracted interest. analysis time series clusters important tool exploring understanding gene networks whilst incorporating knowledge timeseries model potential improve ability method discern clusters. dunson proposed dp-gp model much like ours make important additions. dunson’s model series functions drawn dp-gp prior observation assigned functions. however dunson makes structure model observations differ latent function draws white noise. model describe genes differ latent function describe structure experiment replications. rasmussen ghahramani also presented method combined using method used gating approach produce mixture experts model different aims presented here. outside nonparametric framework medvedovic described hierarchical clustering model approach novel applying structure model time series within clustering. cooke proposed based clustering approach gene expression replicates used clustering estimate level noise experiment. richer model explicitly accounts replicate structure experiment. method inferring structure clustering model also offers improvement aforementioned approaches. show inference method considerably faster usual variational method widely acknowledged faster gibbs sampling approach adopted dunson agglomerative clustering method proposed cooke suffer signiﬁcant scalability issues ﬁrst round agglomeration needs compute marginal likelihoods models every pair genes. shall derive collapsed variational procedure inference clustering. collapsed variational bayes latent variable method sung share many properties approach details. method improves considering riemannian structure collapsed problem relating vbem method. derive extremely efﬁcient gradient methods apply conjugate gradient algorithms account riemman structure collapsed problem. previously honkela proposed natural gradient based variational methods without connection collapsed approach unable achieve speed-up standard vbem. related model uses kl-corrected approach overlapping mixtures gaussian processes model series latent functions assumed observation assigned objective tracking. model reduced removing structured element well prior. used free-form variational optimization shall show much slower approach. section brieﬂy review gaussian process regression introduce notation. extend model structured time series introducing notation mixture models. gaussian process regression perhaps widely applied bayesian nonparametric methods particularly since publication rasmussen williams idea place prior space functions bayesian inference update one’s belief function observing noisy realisations function ﬁnite number points. speciﬁed mean function covariance function mean function often assumed zero everywhere covariance function takes parametric form. publication make square exp{ t)}. parameters covariance function control type function permitted variance function controlled parameter length-scale function publication several separate covariance functions subscripts identify function parameters belong vector collect appropriate covariance function parameters. prior functions written covariance matrix constructed covariance function regression setting usually presented noise corrupted version assuming noise gaussian writing interpretation function gaussian process prior covariance observe directly. conjugate relationship construct structured models using i.i.d white noise gaussian process functions. assignment variables ﬁnding cluster parameters mixing proportions variational procedure better method since avoids pitfalls maximum likelihood estimation however methods require speciﬁcation number components using dirichlet process prior mixing proportions avoids problem selecting number components model. also offers convenient inference procedure gibbs sampling though paper focus variational approach. dirichlet process seen inﬁnite mixture model normal mixture model atoms correspond gaussian densities number clusters allowed tend inﬁnity. general case used inﬁnite mixture models simply gaussian densities. mixing proportions clusters controlled concentration parameter propose dirichlet process gaussian process mixture model using stick breaking construction follows. space functions mapping space base distribution concentration draw series atoms associated stick-breaking lengths independently thus atom function drawn construction similar provided dunson though innovated using additional structure model also propose novel inference procedure based variational bayes. construction clustering functional data hierarchical developed previous section. group data associated single atom variables {zn}n varies atom independent draw another atom becomes mean function hierarchical described above. applications levels hierarchy unknown groups highest level known groups simple extend model series levels known unknown groupings each depending application. consider time series wish model. groups data {yn}n taken times {tn}n example group could represent experimental replication different conditions. model latent function governs time series denote gp)). given draw group data drawn gp)). additional covariance account correlated structure noise. possible marginalise latent function thus introduce covariance groups using conjugate properties noise discussed above. covariance amongst groups depends group index write compound covariance function mixture models involve assignment data clusters well inference properties cluster. popular gaussian mixture model involves assignment data vector gaussian components whilst simultaneously inferring mean covariance components. algorithm gaussian mixture models widely known treats assignment data clusters latent variable problem estimates cluster means variances maximising likelihood. algorithm alternates inference latent variables maximisation likelihood respect parameters. cluster parameters treated random variables inference performed computing joint posterior distribution latent variables cluster parameters. variational approach inference mixture model achieved approximating posterior factorising distribution updated using vbem algorithm. using conjugate priors vbem algorithm alternates computing optimal approximating distribution fig. hierarchical gaussian process model gene expression single gene drosophila development. pane y-axis represents normalised gene-expression level x-axis represents time hours. left-most frame shows posterior distribution function subsequent frames represent biological replicates model hierarchical groups. right-most pane represents hypothetical future replicate. posterior means represented solid lines shaded areas represent standard deviations. steepest ascent direction riemann manifold given natural gradient model related mixture models show necessary information-geometric quantities computed closed form without expensive matrix inverse hindered previous approaches demonstrate empirically algorithm converges faster vbem free-form approach l´azaro-gredilla titsias variables inﬁnite dimension single unitary element approximate posterior shall truncate number components selecting truncation parameter select adopt merge-split approach applied maximum likelihood clustering also metropolis-hastings step collapsed gibbs sampler brieﬂy formalise notations model summarising table dp-gp construction described above whose atoms denote suppose groups data wish cluster. gene expression data expression genes necessarily gathered time vectors same simplifying exposition somewhat. values function times gathered vector denote collection {fi}∞ collect stick breaking lengths similarly {vi}∞ here vectors {yn}n represent data wish cluster sub-groupings concatenated. simpliﬁes model illustrated figures accordingly deﬁne likelihood usual mixture form variational bayes method probabilistic inference posterior distribution approximated using simpler distribution. usual assumption posterior factorises yields tractable lower bound marginal likelihood serves objective optimization. factorising assumption leads naturally vbem algorithm factors updated turn. recently various forms collapsed variational bayes proposed speciﬁc models variables marginalised analytically. hensman showed many schemes equivalent. also proposed riemann optimization scheme similar honkela showed vbem fact steepest ascent algorithm upon riemannian manifold. thus introducing geometrically conjugate gradient directions serves increase speed convergence. consider application ideas dp-gp model. dirichlet process mixture models consider here kurihara considered forms collapsed stickbreaking prior. although approach differs derivation marginalised stick breaking lengths before making variational approximation show similar expressions. further collapse parameters model aside cluster allocation variables. leads greater simpliﬁcation computing collapsed variables must factorise prior examining graphical representation model figure latent variables observed model would d-separate appropriately. shall variational distribution introduce variational parameters truncation level ensure valid distribution shall re-parameterize softmax function. also assist computation natural gradients shall show. important difference variational method vbem procedure introduce parameterisation distributions collapsed variables analytically marginalised problem. ﬁrst step jensen’s inequality derive lower bound data likelihood conditioned variables shall collapsing fig. graphical models representation hierarchical gaussian process clustering method. gaussian processes represented inﬁnite self-connected plates hyper-parameters concentration shown solid dots. right-hand plate represents dirichlet process left hand plate represents independent data groups clustered. inner plate represents single level structure clustering indexed functions represented fig. graphical models representation hierarchical gaussian process clustering method variables collapsed using standard methodology. model enables d-separation test shows approximating distribution latent variables remainder model marginalise analytically. integral separates integral follows easily completing square using gaussian identity. integral also straightforward reveals relations previous studies collapsed stick breaking priors ﬁrst step deriving collapsed bound select variables used parameterisation collapsed problem done using d-separation test. given observed variables treating variables wish parameterise observed fisher information matrix parameterisation given diag matrix singular over-parameterised nature softmax function makes computing natural gradient problematic. kuusela suggests omitting ﬁrst element writing though avoided follows. variable thus natural gradient computed simply dividing matrix inversions required. since often contains many elements close zero division cause numerical problems. avoided considering chain-rule used compute gradients respect i=k+ znk. kurihara al’s approach stick breaking lengths marginalised expression variational approximation made leading make variational approximation tractable ﬁrst order taylor expansion ptsb used around point approximate ’marginalisation’ leads similar expression description allocates data group latent function collection allocation variables latent function realisations function points collection realised function values group observed data collection observed data stick-breaking length construction collection stick-breaking lengths concentration parameter stick breaking process posterior probability assigning datum component effective number assignment component approximate posterior presented kl-corrected bound natural gradient dirichlet process mixture hierarchical gaussian processes. following discuss relations optimization approach vbem method. furthermore vbem procedure effectively gradient method taking unit length steps natural gradient direction coordinates turn important result natural gradient kl-corrected bound mean ﬁeld bound means recover exactly vbem algorithm taking unit steps natural gradient direction kl-corrected bound. kl-corrected bound brought following advantages simpler derive lower-dimensional space optimization. surprisingly compact representation complicate optimization natural gradient steps kl-corrected bound effect full steps mean-ﬁeld bound perhaps important advantage kl-corrected bound enables conjugate gradient descent preformed easily. needs consider conjugate computations small number variables. conjugate gradient step fails improve bound reverting unit step natural gradient direction recover vbem update again conjugate part algorithm ’restarted’. merge-split approach suggested mixture models using mcmc approach current solution reinitialised either re-deﬁning clusters cluster new. collapsed nature kl-corrected bound particularly helpful performing merge-split since parameters deal with. since lower bound marginal likelihood also natural method accepting proposed moves depending whether increase bound. φnk. increase truncation parameter adding cluster move half probabilistic mass cluster φnk. optimising convergence accept move bound increases. found empirically merge procedures necessary optimization naturally managed merge clusters appropriately. simply removed empty clusters appropriate. also make re-ordering move re-order solution largest cluster ﬁrst increases bound prior. note collapsed parameterisation model makes procedures simple implement matrix containing deal with columns deleted added moved adjusted arbitrarily long bound log-likelihood increases. present application model variational inference procedure three data sets. experiments initialised allocation clusters randomly. effect covariance function hyper-parameters play quite strong role results. initialised using following rules thumb length-scales initialised half span input data; top-level variance hierarchical variance noise account data variance. optimization hyper-parameters interleaved variational optimization. unless otherwise stated dirichlet process concentration parameter ﬁxed synthetic data demonstrate model generated synthetic data follows. selected time points randomly region deﬁned clusters evaluating sine function uniformly randomised phase randomised frequency around randomly selected data cluster interval datum cluster selected correlated offset mean cluster using randomised sine functions added small amount i.i.d. noise. data illustrated figure drosophila development present results clustering data kalinka paper gene expression species drosophila presented measured hour intervals embryonic development. data contains natural structure aside structure across species experiments performed replicate. pools embryos used measurements taken every hours pool. pools measured time points. hierarchical model replicate structure accounting correlated differences replicates. level hierarchy clustering genes. using method similar kalaitzis lawrence eliminate silent genes selected genes clustering. gp-dp construct without structured model poor ﬁnding clusters since cannot account correlations amongst signals using noise model attempts introduce extra components model account variance. gaussian mixture model also fails infer correct structure without prior knowledge signal correlations unable separate groups. clusters inferred model shown figure optimize parameters covariance functions interleave standard riemannian method standard optimization covariance functions parameters keeping variational distributions ﬁxed. using synthetic data simple trial examine sensitivity method initial conditions covariance function parameters. created initial conditions drawing parameters values standard log-normal distribution optimized models using riemannian procedure interleaved standard conjugate-gradient optimization parameters merge-split routine. cases optimal structure discovered; remaining cases clusters conﬂated reﬂected lower bound marginal remaining structure inferred correctly. cases lengthscales variances covariance functions estimated correctly decimal places best solution. results clustering drosophila data shown supplementary material. example structure inferred single cluster illustrated figure hierarchical model allows biologically meaningful clusterings possible without structure. example ﬁrst three clusters appear similar signals second cluster rise slightly faster ﬁrst. using online david tool found cellular component gene ontology terms enriched differently each. ﬁrst cluster ccgo terms extracelular matrix extracelluar region part proteinaceous extracellular matrix p-value belows ccgo terms plasma second cluster membrane part cell junction similar p-values. third cluster whose signals arrive slightly later still also enriched genes intrinsic membrane also showed enrichment cell adhesion biological adhesion. comparative purposes also applied code clustering data without hierarchical structure. similar spirit proposed dunson though maintain variational framework. model variation cluster mean modelled independent gaussian noise. result many clusters used model data. noise measurement process simply i.i.d. model enforces varying sensitivities microarray system well true biological variation genes mean genes activated similar pathways thus similar temporal patterns vary correlated manner. summarise results models table fig. cluster allocation diagrams synthetic data. each data indexed vertically clusters index horizontally white square indicates allocation datum cluster grey squares represent uncertain allocations. hierarchical model ﬁnds correct structure confusion ﬁrst second cluster. non-structured gp-dp fails correctly account structure data uses many clusters model variance. gaussian mixture model unable discern clusters correctly. periodic clustering advantage using models clustering incorporate prior information model. gossan gene expression mouse cartilage measured four hour intervals duplicate following light-dark cycle. genes corresponding circadian rhythms identiﬁed ﬁtting sine functions data. propose clustering model circadian genes. periodic covariance function based projection matern covariance drawing functions dp-gp construct periodic nature. next layer hierarchical structure uses standard covariance i.i.d. noise. model genes able share periodic component deviation periodicity accounted gene-by-gene basis. clusters inferred model shown figure analysis discovered cluster structure showed reﬂected known groupings established clock genes well providing insights regulation cartilage speciﬁc genes circadian clock. details gossan tested model synthetic data compared gp-dp construct without hierarchical structure using i.i.d. noise model difference clustered signals dirichlet process gaussian mixture model attempts infer covariance structure data. comparison concentration parameter give correct number clusters priori inferred using scheme each. figure shows ground truth cluster allocation inferred cluster allocation each. figure model correctly inferred correct structure true clusters confused. fig. synthetic data shown clustering formation inferred hierarchical model. pane represents cluster data assigned cluster represented thin lines joining observations. posterior means standard deviations shown solid lines shaded areas. note figure omits additional structure datum modelled whose prior mean common cluster. fig. example structure inferred within single cluster drosophila data set. function governs behaviour cluster shown bottom left panel. represents single gene cluster left-most pane representing inferred function gene subsequent panes representing inferred function individual replicates. bottom shows predictive distribution hypothetic extra gene cluster. even accounting extra parameters required difference extremely signiﬁcant. table model data uses twice variance noise model hierarchical structure plots discovered clusters found supplementary material clear nonhierarchical version discovers many small clusters similar proﬁles. fig. convergence method drosophila dataset using random restarts. restarts applied methods. conjugate riemann method uses hestenes-steifel conjugacy manifold whist vbem procedure effectively steepest descent. types optimization show plateaus objective function conjugate method quickly escapes these whilst vbem move according local gradient becomes stuck. compare riemann procedure vbem test effectiveness merge-split approach kernel parameters sensible values found using several optimization runs. ﬁnds superior solutions empirically found occurred algorithm vbem procedure steepest ascent method riemann manifold becomes stuck plateaus little gradient. illustrated figure conjugate riemann method vbem pause levels likelihood nature algorithm allows pass solution whilst vbem algorithm stuck. monitor effects ability algorithm good solution restarts data sets using initial conditions without using merge-split approach. monitored restarts many times algorithm came good solution divided total time taken restarts number runs found solution. statistic asses well algorithms perform speeding convergence also escaping plateaus discussed. results shown table iii. effect expedited convergence using riemann approach ampliﬁed optimising hyperparameters using merge-split method. practise necessary variational optimization many times interleaved merge-split trials optimization hyperparameters. since riemann procedure often ﬁnds better local solutions variational parameters need many split-procedures good global optimum. optimising hyper-parameters interleaved parameters riemann procedure also particularly effective ﬁnding local solutions quickly. presented method clustering structured time series. method based hierarchical gaussian process. simple idea allows combine related time series groups natural way. introduced clustering model allows infer groupings modelling structure within sub-groups. inspired gene expression time series applied model three datasets. showed gaussian process methodology allows incorporate knowledge problem model periodicity shared timeseries function. model many applications clustering time series currently exploring application motion capture data. performed inference model using recent modiﬁcation variational bayes. provided speed improvement also allowed extremely simple implementations merge-split approach. related collapsed expression used collapsed variational bayes showed compute natural gradient softmax-parameterised variables derivation wide application clustering models. blei jordan. variational inference dirichlet process mixtures. bayesian analysis cooke savage kirk darkins wild. bayesian hierarchical clustering microarray time series data replicates outlier measurements. bioinformatics gossan zeef hensman hughes bateman rowley little piggins rattray boot-handford circadian clock murine chondrocytes regulates genes controlling aspects cartilage homeostasis. arthritis rheumatism hoffman blei wang paisley. stochastic variational inference. arxiv preprint arxiv. honkela girardot gustafson furlong lawrence rattray. model-based method transcription factor target identiﬁcation limited data. proceedings national academy sciences honkela raiko kuusela tornio karhunen. approximate riemannian conjugate gradient learning ﬁxed-form variational bayes. journal machine learning research kalaitzis lawrence. simple approach ranking differentially expressed gene expression time courses gaussian process regression. bioinformatics kalinka varga gerrard preibisch corcoran jarrells ohler bergman tomancak. gene expression divergence recapitulates developmental hourglass model. nature fig. eight clusters found mouse cartilage data using periodic clustering. case information shared genes group must captured periodic fortunate enough given period rhythm advance enforced light-dark cycle. although effects data case wish clustering thus modelled gene-by-gene basis kuusela raiko honkela karhunen. gradient-based algorithm competitive variational neural netbayesian mixture gaussians. works ijcnn international joint conference pages ieee stegle denby cooke wild ghahramani borgwardt. robust bayesian two-sample test detecting intervals differential gene expression microarray time series. journal computational biology sung ghahramani bang. latent-space variational bayes. pattern analysis machine intelligence ieee transactions newman welling. collapsed variational bayesian inference algorithm latent dirichlet information processing allocation. systems", "year": 2014}