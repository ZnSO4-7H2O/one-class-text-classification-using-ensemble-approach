{"title": "Bit-pragmatic Deep Neural Network Computing", "tag": ["cs.LG", "cs.AI", "cs.AR", "cs.CV"], "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two, and b) hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronication scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5].", "text": "hardware typically uses either -bit ﬁxed-point quantized -bit numbers bit-parallel compute units. since actual precision requirements vary considerably across layers typical hardware ends processing excess bits processing inner products unless values processed layer need full value range afforded hardware’s representation excess bits signiﬁcant positions least signiﬁcant positions need zero contribute ﬁnal outcome. bit-parallel compute units performance beneﬁt processing excess bits. recent work stripes uses serial-parallel multiplication avoid processing zero preﬁx sufﬁx bits yielding performance energy beneﬁts. represents neurons using pre-speciﬁed layer precisions. given neuron represented bits synapse represented example -bits processes bit-serially cycles cycle multiplied accumulating result running sum. takes cycles compute product ideally improve performance compared -bit ﬁxed-point bit-parallel hardware processing neurons synapse pairs parallel. abundant parallelism convolutional layers makes possible. avoids processing ineffectual sufﬁx tail bits neurons one-size-ﬁts-all representation conventional bit-parallel hardware still processes many ineffectual neuron bits time zero multiplied synapse adds nothing ﬁnal output neuron. ineffectual bits introduced conventional positional number representation. multiplications could avoided would take even less time calculate product improving energy performance. section shows state-of-the-art image classiﬁcation networks show neuron synapse products ineffectual using respectively -bit ﬁxed-point -bit quantized representations. work presents pragmatic accelerator whose goal process essential bits input neurons. subsumes since avoids processing non-essential bits regardless position also obviates need determine priori speciﬁc precision requirements layer. employs following four techniques on-the-ﬂy conversion neurons storage representation propose pragmatic architecture exploits improving performance energy efﬁciency. source ineffectual computations best understood context conventional multipliers generate internally multiple terms products multiplicand powers added together produce ﬁnal product runtime many terms zero generated multiplicand combined zero-bits multiplicator. conventional bit-parallel multipliers calculate terms parallel reduce individual product latency calculates non-zero terms using on-theﬂy conversion multiplicator representation explicit list powers bit-parallel multplicand/bit-serial multiplicator processing units. exploits sources ineffectual computations aforementioned zero product terms result lack explicitness multiplicator representation excess representation precision used multiplicants multiplicators e.g. measurements demonstrate convolutional layers straightforward variant improves performance dadianao accelerator similarly improves energy efﬁciency average compared dadn str. improved cross lane synchronization scheme boosts performance improvements dadn. finally pragmatic beneﬁts persist even -bit quantized representation deep neural networks become state-ofthe-art technique many recognition tasks object speech recognition dnn’s high computational demands today practical deploy given availability commodity graphic processing units exploit natural parallelism dnns. need even sophisticated dnns demands even higher performance energy efﬁciency motivating special purpose architectures state-of-the-art dadiannao power limiting modern high-performance designs achieving better energy efﬁciency essential enable advances dnns comprise pipeline layers processing time spent convolutional layers work targets. layers perform inner products neurons synapses multiplied pairs resulting products added produce single output neuron. typical convolutional layer performs hundreds inner products accepting hundreds thousands neuron synapse pairs. ﬁxed-precision hardware. additional ineffectual bits appear positions result positional number representation. total ineffectual bits processed generating ineffectual terms. number could represented explicit list three constituent powers representation require bits thus undesirable storage coupled abundant parallelism present dnns layers provides opportunity revisit hardware design improving performance energy efﬁciency. rest section motivates pragmatic measuring fraction non-zero bits neuron stream stateof-the-art dnns three commonly used representations estimating performance improvement possible processing non-zero neuron bits. table reports essential content neuron stream state-of-the-art dnns commonly used ﬁxed length representations -bit ﬁxed-point dadiannao -bit quantized tensorﬂow essential content average number non-zero bits measurements presented representation neuron values non-zero neurons accelerators skip zero neurons ﬁxed-point representations recently proposed considering neurons essential bit-content ﬁxed-point quantized representations respectively. measurements consistent neuron values following normal distribution centered ﬁltered rectiﬁer linear unit function even considering non-zero neurons essential content remains well next section show many non-zero valued neurons suggesting potential exists improve performance energy efﬁciency approaches target zero valued neurons. results suggest signiﬁcant number ineffectual terms processed conventional ﬁxed-length hardware. stripes tackles excess precision exploiting variability numerical precision dnns requirements increase performance processing neurons bit-serially. pragmatic’s goal also exploit lack explicitness. next section show pragmatic potential greatly improve performance even compared stripes. estimate pra’s potential section compares number terms would processed various computing engines convolutional layers state-of-the-art dnns aforementioned baseline neuron representations. -bit fixed-point representation following computing engines considered baseline representative dadn using -bit ﬁxed-point bit-parallel units hypothetical enhanced baseline skip zero valued neurons positional number quantized) explicit representation essential bits only bit-serial neuron/bit-parallel synapse processing idea borrowed adapted aforementioned representation judicious simd lane grouping maintain wide memory accesses avoid fragmenting enlarging multi-mb on-chip synapse memories computation re-arrangement reduce datapath area. evaluated variants maintain wide memory accesses highly-parallel simd-style computational units. introduces additional dimension upon software improve performance energy efﬁciency controlling neuron values judiciously order reduce essential content maintaining accuracy. work explores alternative software explicitly communicates many preﬁx sufﬁx bits discard layer. experimental measurements state-of-the-art dnns demonstrate straightforward variant boosts average performance convolutional layers state-of-the-art dadn accelerator compared performance improvement alone. pragmatic’s average energy efﬁciency dadn area overhead another variant boosts performance dadn expense additional area. software guidance accounts performance beneﬁts. assume p-bit bit-parallel multiplier using straightforward implementation shift algorithm multiplier computes terms product adds produce ﬁnal result. terms calculated concurrently reduce latency hardware arrangement sources ineffectual computations result from excess precision lack explicitness figure shows example illustrating sources bit-parallel multiplier using -bit unsigned ﬁxed-point number fractional integer bits. requires bits -bit bit-parallel multiplier zero-extend preﬁx sufﬁx bits. example number terms processed bit-parallel baseline ideal impractical bit-parallel engine skips zero neurons pra. interest space since subsumes considered. pragmatic’s potential beneﬁts signiﬁcant even -bit quantized representation. average skipping zero valued neurons would eliminate terms whereas pragmatic would remove terms. section corroborated past observations that many neuron values zero close half computations performed traditionally needed numerical precision properly adjusted showed less computations really needed average -bit ﬁxedpoint -bit quantized representations respectively essential neuron bits processed. finally software boost opportunities savings communicating layer precisions. section illustrates idea behind pragmatic simpliﬁed example. purposes discussion sufﬁces know layer typically hundreds thousands neurons multiplied corresponding synapse synapses reused several times. section iv-a describes relevant computations detail. bit-parallel unit figure multiplies neurons respective synapses adder reduces products. unit reads neuron synapse bits respectively single cycle. result sources inefﬁciency manifest here represented using bits instead respectively eop. even bits contain zero loe. result four ineffectual terms processed using standard multipliers derived shift cnvlutin practical design skip zero value neurons ﬁrst layer avoids ideal softwaretransparent pra-fp processes essential neuron bits ideal pra-red software communicates advance many preﬁx sufﬁx bits zeroed layer figure reports number terms normalized dadn multiplication accounted using equivalent number terms equivalently additions dadn layer using precision bits number essential neuron bits pra-fp pra-red. example number additions counted would dadn cvn+ could -bit ﬁxed-point representation pra-fp pra-red. average reduces number terms compared dadn skipping zero valued neurons could reduce practical practice cvn. pra-fp ideally reduce number additions average software provided precisions layer pra-red reduces number additions average. potential savings robust across dnns remaining dnns pra-red. -bit quantized representation figure shows relative figure representative tackles eop. cycle unit processes neuron hence takes three cycles compute convolution neurons represented using bits each slowdown bit-parallel engine. match throughput bit-parallel engine figure takes advantage synapse reuse processes multiple neurons groups parallel. example neurons combined synapses shown. starting least signiﬁcant position cycle neuron anded corresponding synapse. results added reduction tree result accumulated shifted bit. since speciﬁc neuron values could represented using bits would need cycles process products compared cycles needed bit-parallel system speedup. however stripes still processes ineffectual terms. example ﬁrst cycle terms zero added adder tree wasting computing resources energy. figure shows simpliﬁed engine. example neurons longer represented vectors bits vectors offsets essential bits. example neuron represented neuron value would represented outof-band shown indicates neuron’s end. shifter neuron uses offsets effectively multiply corresponding synapse respective power passing adder tree. result processes non-zero terms avoiding ineffectual computations loe. example would process neuron synapse pairs single cycle speedup bit-parallel engine. work presents pragmatic modiﬁcation state-of-the-art dadiannao accelerator. accordingly section provides necessary background information section iv-a reviews operation convolutional layers section iv-b overviews dadn processes convolutional layers. convolutional layer processes produces neuron arrays arrays real numbers. layer applies ﬁlters sliding window fashion using constant stride produce output array. input array contains neurons. ﬁlters contains synapses also real numbers. output neuron array dimensions depth equals ﬁlter count. ﬁlter corresponds desired feature goal layer determine input neuron array features appear. accordingly constituent array along dimension output neuron array corresponds feature. calculate output neuron layer applies ﬁlter window ﬁlter-sized subarray input neuron array. respectively input output neurons synapses ﬁlter output neuron position given layer applies ﬁlters repeatedly different windows positioned along dimensions using constant stride output neuron window ﬁlter. accordingly output neuron array dimensions dimension e.g. n...n. bricks denoted origin element subscript e.g. term pallet refers bricks corresponding adjacent using stride windows along dimensions e.g. nb...nb denoted number neurons brick bricks pallet design parameters. pragmatic demonstrated modiﬁcation dadiannao accelerator proposed chen figure shows dadn tile processes ﬁlters concurrently calculating neuron synapse products ﬁlter total products cycle. cycle tile accepts synapses ﬁlter total synapses input neurons. tile multiplies synapse neuron whereas neuron multiplied synapses ﬁlter. tile reduces products single partial output neuron ﬁlter total partial output neurons tile. dadn chip comprises tiles processing different ﬁlters cycle. accordingly cycle whole chip processes neurons synapses producing partial output neurons. internally tile synapse buffer provides synapses cycle synapse lane input neuron buffer provides neurons cycle neuron lanes neuron output buffer accepts partial output neurons cycle. tile’s datapath neural functional unit neuron lane paired synapse lanes ﬁlter. synapse neuron lane pair feed multiplier adder tree ﬁlter lane reduces ﬁlter products partial sum. ﬁlter lanes produce partial cycle total partial output neurons nfu. full window processed resulting sums non-linear activation function produce ﬁnal output neurons. multiplications reductions needed cycle implemented multipliers synapse lane sixteen -input adder trees ﬁlter lane. dadn’s main goal minimizing off-chip bandwidth maximizing on-chip compute utilization. avoid fetching synapses off-chip dadn uses edram tile total edram. inter-layer neuron outputs except initial input ﬁnal output stored shared central edram neuron memory connected broadcast interconnect nbin buffers. off-chip accesses needed reading input image synapses layer writing ﬁnal output. processing starts reading external memory ﬁrst layer’s ﬁlter synapses input image. synapses distributed input stored cycle input neuron brick broadcast units. units reads synapse bricks produces partial output neuron brick stores nbout. computed output neurons stored nbout back nbins processing next layer. loading next synapses external memory overlapped processing current layer necessary. section presents pragmatic architecture. section describes pra’s processing approach section describes organization. sections present optimizations respectively improve area performance. simplicity description assumes speciﬁc values various design parameters performance matches dadn conﬁguration section iv-b worst case. process essential bits input neurons. converts on-the-ﬂy input neuron representation contains essential bits processes essential neuron full -bit synapse cycle. since processes neuron bits serially take cycles produce product neuron synapse. always match exceed performance bit-parallel units dadn processes neurons concurrently exploiting abundant parallelism convolutional layers. remaining section describes turn appropriate neuron representation calculates terms multiple terms processed concurrently maintain performance dadn worst case pra’s units supplied necessary neurons input neuron representation starts input neuron representation straightforward identify next essential cycle. representation explicit list oneffsets constituent powers two. example neuron would represented implementation described herein neurons stored -bit ﬁxed-point converted on-the-ﬂy representation broadcast tiles. single oneffset processed neuron cycle. oneffset represented -bit value single indicates neuron. example represented npra worst case bits input neuron would hence representation would contain oneffsets. boosting compute bandwidth dadn match dadn’s performance needs process number effectual terms cycle. dadn tile calculates neuron synapse products cycle terms. terms practice ineffectual guarantee always performs well dadn process terms cycle. time assume neurons contain number essential bits processing multiple neurons parallel units complete time thus proceed next neurons sync. next section relax constraint. since processes neurons bits serially produces term neuron synapse pair thus needs process pairs concurrently. choice neuron synapse pairs process concurrently adversely affect complexity performance. example could force increase capacity width increase width ineffective unit underutilization given commonly used layer sizes. fortunately possible avoid increasing capacity width keeping units utilized dadn. speciﬁcally tile read synapse bricks equivalent neuron bits dadn’s tiles speciﬁcally dadn tile processes synapse bricks concurrently ﬁlter. however differently dadn synapse bricks combined neuron brick processed bit-parallel combines synapse brick neuron bricks windows processed bit-serially. neuron bricks combined synapse bricks. neuron bricks form pallet enabling synapse brick combined all. example single cycle title processing ﬁlters could combine combine example illustrates approach allows synapse combined neuron window whereas dadn synapse combined neuron only. total essential neuron bits processed cycle given synapses windows processes neuron synapse pairs terms cycle producing partial output neurons ﬁlter partial output neuron bricks cycle. supplying input neuron synapse bricks thus assumed input neurons number essential bits. assumption neuron lanes complete processing terms time allowing move next neuron pallet next synapse bricks step. allows reuse str’s approach fetching next pallet single-ported brieﬂy unit stride neurons would typically stored adjacent rows thus fetched cycles. stride neurons spread multiple rows thus multiple cycles needed fetch all. fortunately fetching next pallet overlapped processing current one. accordingly takes access next pallet current pallet requires cycles process next pallet begin processing cycles. performance lost waiting practice highly unlikely neurons number essential bits. general neuron lane left unrestricted advance different rate. worst case neuron lane needing neurons necessary. straightforward design oneffsets -bits shifter accepts -bit synapse shift positions producing -bit output. finally adder tree accepts -bit inputs. section presents enhanced design requires narrower components improving area energy. dispatcher reads neuron bricks expected tiles. oneffset generator converts neurons on-the-ﬂy oneffset representation broadcasts oneffset neuron cycle total oneffsets titles. fetching assembling neuron bricks akin fetching words stride cache structure. section discussed take multiple cycles depending stride alignment initial neuron brick. uses dispatcher design neuron bricks collected oneffset generators operate parallel locate communicate next oneffset neuron. straightforward -bit leading detector sufﬁcient. latency oneffset generators dispatcher readily hidden pipelined desired overlapping processing tiles. shift performed stages smaller shifts thus shift synapses different offsets decompose offsets sums common term e.g. accordingly processing rearranged using stage processing ﬁrst stage uses synapse speciﬁc offsets second stage common across synapses offset arrangement used reduce width synapse shifters adder tree sharing common shifter adder tree figure shows. design parameter deﬁnes number bits controlling synapse shifters. meaning design process oneffsets differ less single cycle. reduces size synapse shifters reduces size adder tree support terms bits only. section vi-b shows design reduces area shifters adder trees largest components pip. figure shows example illustrating handle combination oneffsets. section vi-b studies impact cost performance. different neuron brick thus breaking pra’s ability reuse synapse brick. undesirable impractical would require partitioning replicating unrelated synapses could read cycle would also increase complexity bandwidth. fortunately complexities avoided palletlevel neuron lane synchronization neuron lanes wait essential bits ﬁnish proceeding next pallet. approach matter bits essential neuron many exist. since unlikely pallets contain neuron essential terms improve performance dadn. section vi-b show practice approach improves performance dadn str. section discuss ﬁner-grain synchronization schemes lead even better performance. however intervening sections detail pra’s design. figure shows pragmatic tile architecture comprises array pragmatic inner product units processes neuron oneffset window corresponding synapse j-th ﬁlter. speciﬁcally pips along i-th receive synapse brick belonging i-th ﬁlter pips along j-th column receive oneffset neuron neuron brick belonging j-th window. necessary neuron oneffsets read nbin placed dispatcher oneffset generators units section explains. every cycle nbin sends oneffsets window lane. pips column receive oneffsets corresponding neurons single window. tile starts process neuron pallet synapses read synapse lanes dadn stored synapse registers pip. synapse oneffsets processed pips next section describes. pragmatic inner-product unit figure shows internals. every cycle synapses combined corresponding oneffsets. oneffsets controls shifter effectively multiplying synapse power two. shifted synapses reduced adder tree. gate synapse supports injection null terms fig. -stage shifting. modiﬁed pip. example processing three -bit synapse neuron pairs oneffset generator reads neuron values produces three oneffests cycle. cycle control logic shared amortized across entire column pips compares oneffsets processed ﬁrst cycle example picks lowest indicated circle. minimum oneffset controls second stage shifter. control subtracts offset three oneffsets. difference oneffset long less controls corresponding ﬁrst level shifter. ﬁrst cycle shifters values shifter bottom stalled given able handle shift cycle oneffsets minimum controls stage shifter control ﬁrst-level shifters. cycle ﬁrst third neurons still oneffsets process. computation ﬁnishes cycle last oneffset third neuron controls shifters. fig. per-column synchronization example extra synapse register array capable processing windows parallel. numbers brick show ﬁrst brick’s index bricks ﬁrst second window. second maximum count oneffsets neurons respectively. numbers registers indicate index corresponding bricks i.e. synapse register containing stores synapses corresponding neurons bricks indexes cycles thicker lines indicate registers loaded wires used. pallet neuron lane synchronization scheme section many possible synchronization schemes. finer-grain neuron lane synchronization schemes possible leading higher performance albeit cost. section presents column neuron lane synchronization appealing scheme that section vi-c shows enhances performance little additional cost. column operates independently pips along column wait neuron essential bits proceeding next neuron brick. since pips along column operate sync process synapse bricks read using existing interface. however given different columns operate out-of-sync would read higher number times become bottleneck. concerns different columns need perform independent reads port common connecting array repeat accesses increase energy already major contribution energy consumption. concerns addressed follows access proceed cycle thus column need wait collisions occur. need extra read port extra wires array. sram registers synapse registers introduced front holding recently read synapse bricks. since columns eventually need synapse bricks temporarily buffering avoids fetching repeatedly synapse read stays columns copied policy guarantees accessed number times dadn. however stalls incur column able store synapses reads figure shows example. section vi-c evaluates design. since neuron lane advances independently worst case dispatcher need fetch independent neuron bricks different pallet. dispatcher buffer pallets avoid rereading would worst require pallet buffer. however given number ssrs restricts apart columns since section vi-c shows sufﬁcient pallet buffer dispatcher needed. enables additional dimension upon hardware software attempt boost performance energy efﬁciency controlling essential neuron value content. work investigates software guided approach precision requirements layer used zero number preﬁx sufﬁx bits output layer. using proﬁling method judd software communicates precisions needed layer meta-data. hardware trims output neurons writing using gates precision derived masks. performance area energy efﬁciency pragmatic compared dadn stripes stateof-the-art accelerators. dadn fastest bit-parallel accelerator proposed date processes neuron regardless values improves upon dadn exploiting layer precision requirements dnns. cnvlutin improves upon dadn skipping zero-valued neurons however stripes shown outperform rest section organized follows section vi-a presents experimental methodology. sections vi-b vi-c explore design space considering respectively single-stage shifting conﬁgurations column synchronization. section vi-d reports energy efﬁciency best conﬁguration. section vi-e analyzes contribution software provided precisions. finally section vi-f reports performance designs using -bit quantized representation. systems modelled using methodology consistency. custom cycle-accurate simulator models execution time. computation scheduled designs reuse synapses thus read energy. estimate power area designs synthesized synopsis design compiler tsmc library. nbin nbout sram buffers modelled using cacti edram area energy modelled destiny compare layer numerical representation requirements reported table found using methodology judd conﬁgurations studied exploit software provided precisions section v-f. section vi-e analyzes impact information overall performance. performance measurements convolutional layers account overall execution time dadn affect execution time remaining layers. performance figure shows performance variants relative dadn. systems labelled number bits used operate ﬁrst-stage synapse shifters e.g. synapse shifters -bit prab able shift four positions -bit prab single-stage pragmatic prasingle sections v-a– whose synapse shifters shift positions second stage shifter. prasingle improves performance average dadn compared average improvement str. performance improvements dadn vary vggm. expected -stage variants offer slightly lower performance prasingle however performance prab prab always within prasingle. even prab include synapse shifters outperforms average. given oneffsets prab accommodate minimum nonzero oneffset cycle second level shifter. area power table shows absolute relative dadn area power. area measurements reported unit excluding nbin nbout memory blocks whole chip comprising units memory blocks. since dominate chip area area area overheads given performance advantage area power overheads justiﬁed. prab particularly appealing overall area cost base power performance average. accordingly restrict attention conﬁguration rest evaluation. performance figure reports performance prab column synchronization function number ssrs section v-e. stripes pragmatic relative dadn. conﬁguration praxr refers conﬁguration using ssrs. boosts performance average close even prar ideally possible pra∞r figure shows energy efﬁciency various conﬁgurations pragmatic. energy efﬁciency simply efﬁciency system relative base deﬁned ratio ebase/enew energy required base compute convolution layers new. selected networks efﬁcient dadn. power overhead prasingle speedup resulting circuit less efﬁcient dadn. prab reduces power overhead maintaining performance yielding efﬁciency prar yields best efﬁciency dadn. conﬁgurations studied thus used software provided layer precisions reduce essential content. require precisions operate. table shows fraction performance beneﬁts software guidance prar best conﬁguration studied. results demonstrate that would outperform architectures even without software guidance average software guidance improves performance estimate section ideal figure reports performance dadn conﬁgurations using -bit quantized representation used tensorﬂow quantization uses bits specify arbitrary minimum maximum limits layer neurons synapses separately maps available -bit improve energy efﬁciency cambricon ﬁrst instruction architecture deep learning minerva highly automated software hardware co-design approach targeting ultra low-voltage highly-efﬁcient accelerators eyeriss power real-time accelerator exploits zero valued neurons memory compression energy reduction efﬁcient inference engine exploits efﬁcient neuron synapse representations pruning greatly reduce communication costs improve energy efﬁciency boost performance avoiding certain ineffectual computations targets fully-connected layers shown efﬁcient dadn layers less efﬁcient convolutional layers. aforementioned accelerators bit-parallel units. work demonstrated pragmatic modiﬁcation dadn computation units potentially general approach could compatible aforementioned accelerator designs. investigation interesting future work. newer network architectures like googlenet rely less fully connected layers work used dadn energy efﬁcient high performance baseline. proﬁling used determine precision requirements neural network hardwired implementation exploited general purpose hardware application domains. example brooks exploit preﬁx bits turn parts datapath improving energy. park similar approach trade image quality improved energy efﬁciency. neither approach directly improves performance. best knowledge pragmatic ﬁrst accelerator exploits layer precision requirements dnns also essential information content neuron values. work targeted high-performance implementations pragmatic’s core approach applicable hardware accelerators. warden low-precision matrix multiplication. girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation corr vol. abs/. values linearly resulting interval. representation higher ﬂexibility better utilization reduced precision approach stripes since range doesnt symmetrical limits dont powers still allowing straightforward multiplication values. limit values maximum minimum neuron values layer quantization uses recommended rounding mode. figure reports performance relative dadn performance prasingle prab prar beneﬁts persist nearly prar measuring area energy designs left future work however absolute area energy needed lower narrower representation. moreover given tile logic occupy relatively less area whole chip given account signiﬁcant area energy overall overheads designs dadn lower measured -bit ﬁxed-point conﬁgurations. acceleration deep learning active area research yielded numerous proposals hardware acceleration. dadiannao facto standard high-performance acceleration interest space section restricts attention methods either directly related dadn follow value-based approach acceleration pragmatic falls category accelerators. value-based accelerators exploit properties values processed improve performance energy beyond possible exploiting computation structure alone. cnvlutin stripes accelerators already discussed compared work. pudiannao hardware accelerator supports seven machine learning algorithms including dnns shidiannao camera-integrated power accelerator exploits integration reduce communication overheads esmaeilzadeh blem amant sankaralingam burger dark silicon multicore scaling proceedings annual international symposium computer architecture isca albericio judd hetherington aamodt jerger moshovos cnvlutin ineffectual-neuron-free deep neural network computing ieee/acm international conference computer architecture chen yu-hsin krishna tushar emer joel vivienne eyeriss energy-efﬁcient reconﬁgurable accelerator deep convolutional neural networks ieee international solid-state circuits conference isscc digest technical papers reagen whatmough adolf rama hernndez-lobato g.-y. brooks minerva enabling lowpower highly-accurate deep neural network accelerators international symposium computer architecture judd albericio hetherington aamodt moshovos stripes bit-serial deep neural network computing proceedings annual ieee/acm international symposium microarchitecture micro- chen zhou zhou teman feng zhou chen pudiannao polyvalent machine learning accelerator proceedings twentieth international conference architectural support programming languages operating systems asplos pudiannao. fasthuber chen ienne feng chen temam shidiannao shifting vision processing closer sensor acm/ieee annual international symposium computer architecture june shidiannao. hwang sung real-time phoneme recognition vlsi using feed-forward deep neural networks ieee international conference acoustics speech signal processing brooks martonosi dynamically exploiting narrow width operands improve processor power performance proceedings international symposium high performance computer architecture hpca ieee computer society park choi dynamic bit-width adaptation approach trade image quality computation energy ieee transactions large scale integration systems vol.", "year": 2016}