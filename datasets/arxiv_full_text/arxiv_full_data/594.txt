{"title": "Very Deep Convolutional Networks for Text Classification", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.", "text": "dominant approach many tasks recurrent neural networks particular lstms convolutional neural networks. however architectures rather shallow comparison deep convolutional networks pushed state-of-the-art computer vision. present architecture text processing operates directly character level uses small convolutions pooling operations. able show performance model increases depth using convolutional layers report improvements state-ofthe-art several public text classiﬁcation tasks. best knowledge ﬁrst time deep convolutional nets applied text processing. goal natural language processing process text computers order analyze extract information eventually represent information differently. want associate categories parts text structure text differently convert form preserves part content level granularity processing range individual characters subword units words whole sentences even paragraphs. terest research community systematically applied tasks. however neural networks shown good results many tasks seems reached level outperform state-of-the-art large margin observed computer vision speech recognition. convolutional neural networks short convnets successful computer vision. early approaches computer vision handcrafted features used instance scale-invariant feature transform followed classiﬁer. fundamental idea convnets consider feature extraction classiﬁcation jointly trained task. idea improved years particular using many layers convolutions pooling sequentially extract hierarchical representation input. best networks using layers many approaches consider words basic units. important step introduction continuous representations words. word embeddings state-of-the-art nlp. however less clear best represent sequence words e.g. whole sentence complicated syntactic semantic relations. general sentence faced local long-range dependencies. currently mainstream approach consider sentence sequence tokens process recurrent neural network tokens usually processed sequential order left right expected memorize whole sequence internal states. popular successful variant certainly lstms love show however episodes ﬁrst season shows ﬁrst eight. hope release another contains episodes still somewhat enjoyable. jing jing zhou cheng what look buying laptop? best brand what’s reliable?weight dimensions important you’re planning travel laptop. something least ram. good brand easy site build custom laptop. huber many works shown ability lstms model long-range dependencies applications e.g. name few. however argue lstms generic learning machines sequence processing lacking task-speciﬁc structure. well known fully connected hidden layer neural network principle learn realvalued function much better results obtained deep problem-speciﬁc architecture develops hierarchical representations. means search space heavily constrained efﬁcient solutions learned gradient descent. convnets namely adapted computer vision compositional structure image. texts similar properties characters combine form n-grams stems words phrase sentences etc. believe challenge develop deep architectures able learn hierarchical representations whole sentences jointly task. paper propose deep architectures many convolutional layers approach goal using layers. design architecture inspired recent progress computer vision particular paper structured follows. previous attempts convnets text processing. summarize previous works next section discuss relations differences. architecture described detail section evaluated approach several sentence classiﬁcation tasks initially proposed tasks experimental results detailed section proposed deep convolutional network shows signiﬁcantly better results previous convnets approach. paper concludes discussion future research directions deep approach nlp. large body research sentiment analysis generally sentence classiﬁcation tasks. initial approaches followed classical stage scheme extraction features followed classiﬁcation stage. typical features include bag-of-words ngrams tf-idf. techniques compared convnets corpora experiments. recently words characters projected low-dimensional space embeddings combined obtain ﬁxed size representation input sentence serves input classiﬁer. simplest combination element-wise mean. usually performs badly since notion token order disregarded. another class approaches recursive neural networks. main idea external tool namely parser speciﬁes order word embeddings combined. node left right context combined using weights shared nodes state node classiﬁer. recurrent neural network could considered special case recursive combination performed sequentially usually left right. last state used ﬁxed-sized representation sentence eventually combination hidden states. first works using convolutional neural networks appeared subsequently applied sentence classiﬁcation discuss techniques detail below. otherwise stated approaches operate words projected high-dimensional space. rather shallow neural proposed convolutional layer followed pooling layer time. ﬁnal classiﬁer uses fully connected layer drop-out. results reported data sets particular stanford sentiment treebank similar system proposed using convolutional layers. important difference also introduction multiple temporal k-max pooling layers. allows detect important features sentence independent speciﬁc position preserving relative order. value depends length sentence position layer network. ﬁrst perform sentiment analysis entirely character level. systems convolutional layers followed three fully connected classiﬁcation layers. convolutional kernels size used well simple max-pooling layers. another interesting aspect paper introduction several large-scale data sets text classiﬁcation. experimental setting character level information also proposed character embeddings word combined operation jointly used word embedding information shallow architecture. parallel work proposed based hierarchical attention network document classiﬁcation perform attention ﬁrst sentences document words sentence. architecture performs well datasets whose samples contain multiple sencomputer vision community combination recurrent convolutional networks architecture also investigated goal best worlds e.g. idea recently applied sentence classiﬁcation convolutional network layers used learn highlevel features serve input lstm. initial motivation authors obtain performance networks signiﬁcantly fewer parameters. report results close even outperform convnets data sets. summary aware work uses vgg-like resnet-like architecture deeper convolutional layers sentence classiﬁcation. deeper networks tried reported improve performance. sharp contrast current trend computer vision signiﬁcant improvements reported using much deeper networks namely layers even layers remainder paper describe deep convolutional architecture report results corpora able show performance improves increased depth using convolutional layers. overall architecture network shown figure model begins look-up table generates tensor size contain embeddings characters. ﬁxed seen dimension input text. ﬁrst apply layer convolutions size followed stack temporal convolutional blocks. inspired philosophy resnets apply design rules output temporal resolution layers number feature maps temporal resolution halved number feature maps doubled. helps reduce memory footprint network. networks contains pooling operations resulting levels feature maps output convolutional blocks tenp size number down-sampling operations. level convolutional network resulting tensor seen high-level representation input text. since deal padded input text ﬁxed size constant. however case variable size input convolutional encoder provides representation input text depends initial length representations text vectors variable size valuable namely neural machine translation particular combined attention model. figure temporal convolutions kernel size feature maps denoted temp conv fully connected layers linear projections denoted -max pooling stride means temporal maxpooling kernel size stride previous applications convnets architecture rather shallow combines convolutions different sizes e.g. spanning tokens. motivated fact convolutions extract n-gram features tokens different n-gram lengths needed model shortlong-span relations. work propose create instead architecture uses many layers small convolutions stacking layers convolutions results span tokens network learn itself best combine different -gram features deep hierarchical manner. architecture fact seen temporal adaptation network also investigated kind resnet shortcut connections namely identity convolutions classiﬁcation tasks work temporal resolution output convolution blocks ﬁrst down-sampled ﬁxed dimension using k-max pooling. means network extracts important features independently position appear sentence. resulting features transformed single vector input three layer fully connected classiﬁer relu hidden units softmax outputs. number output neurons depends classiﬁcation task number hidden units experiments. drop-out fully connected layers temporal batch normalization convolutional layers regularize network. convolutional block sequence convolutional layers followed temporal batchnorm layer relu activation. kernel size temporal convolutions padding temporal resolution preserved steadily increasing depth network adding convolutional layers feasible thanks limited number parameters small convolutional ﬁlters layers. different depths overall architecture obtained varying number convolutional blocks pooling layers temporal batch normalization applies kind regularization batch normalization except activations mini-batch jointly normalized temporal locations. mini-batch size feature maps temporal size standard deviations related batchnorm algorithm taken terms. table number conv. layers depth. work explored four depths networks deﬁne number convolutional layers. depth network obtained summing number blocks ﬁlters block containing convolutional layers. figure network blocks type resulting depth adding ﬁrst convolutional layer sums depth convolutional layers. depth thus increased decreased adding removing convolutional blocks certain number ﬁlters. best conﬁgurations observed depths described table also give number parameters convolutional layers. computer vision community availability large data sets object detection image classiﬁcation fueled development architectures. particular made possible compare many different architectures show beneﬁt deep convolutional networks. present results eight freely available large-scale data sets introduced cover several classiﬁcation tasks sentiment analysis topic classiﬁcation news categorization number training examples varies number classes comprised considerably lower computer vision consequence example induces less gradient information make harder train large architectures. also noted tasks ambiguous particular sentiment analysis difﬁcult clearly associate grained labels. equal numbers examples class training test sets. reader referred details construction data sets. table summarizes best published results corpora aware thesaurus data augmentation preprocessing except lower-casing. nevertheless still outperform best convolutional neural networks data sets. main goal work show possible beneﬁcial train deep convolutional networks text encoders. data augmentation improve results even further. investigate future research. following settings used experiments. found best initial experiments. following processing done character level atomic representation sentence pixels images. dictionary consists following characters abcdefghijklmnopqrstuvwxyz -;.?’\"/| %ˆ&*˜‘+=<>{} plus special padding space unknown token total tokens. input text padded ﬁxed size larger text truncated. character embedding size training performed using mini-batch size initial learning rate momentum follow training procedure initialize convolutional layers following epoch took minutes depth minutes depth took epoches converge. implementation done using torch experiments performed single nvidia gpu. unlike previous research convnets text processing temporal batch norm without dropout. section evaluate several conﬁgurations model namely three different depths three different pooling types main contribution thorough evaluation networks increasing depth using architecture small temporal convolution ﬁlters different types pooling shows significant improvement state-of-the-art conﬁgurations achieved text classiﬁcation tasks pushing depth convolutional layers. deep architecture works well data sets particular even small depths. table shows test errors depths type pooling convolution stride k-max pooling temporal max-pooling. smallest depth model already performs better zhang’s convolutional baselines biggest data sets yelp full yahoo answers amazon full polarity. important decrease classiﬁcation error observed largest data amazon full million training samples. table best published results previous work. zhang best results thesaurus data augmentation technique yang hierarchical methods particularly adapted datasets whose samples contain multiple sentences. depth improves performance. increase network depth test errors decrease data sets types pooling going depth amazon full reduces error rate absolute. since test composed samples test samples classiﬁed correctly. improvements especially large data sets signiﬁcant show increasing depth useful text processing. overall compared previous state-of-the-art best architecture depth max-pooling test error compared represents gain absolute accuracy. signiﬁcant improvements obtain data sets compared zhang’s convolutional models include data augmentation technique. max-pooling performs better pooling types. terms pooling also max-pooling performs best overall close convolutions stride signiﬁcantly superior k-max pooling. models outperform state-of-the-art convnets. obtain state-of-the-art results data sets except ag’s news sogou news smallest ones. however deep architecture closer stateof-the-art ngrams tf-idf data sets signiﬁcantly surpass convolutional models presented observed previous work differences accuracy shallow deep models signiﬁcant large data sets still perform well small data sets getting closer convolutional state-of-the-art results small data sets. deep models even perform well ngrams ngrams-tfidf respectively sentiment analysis task yelp review polarity ontology classiﬁcation task dbpedia data set. results yang outperform model yahoo answers dataset probably linked fact model task-speciﬁc datasets whose samples contain multiple sentences like hierarchical attention mechanism apply well documents going even deeper degrades accuracy. shortcut connections help reduce degradation. described gain accuracy increase depth limited using standard convnets. depth increases much accuracy model gets saturated starts degrading rapidly. degradation problem attributed fact deep models harder optimize. gradients backpropagated deep networks vanish momentum able converge correct minimum loss function. overcome degradation model resnet model introduced shortcut connections convolutional blocks allow gradients easily network evaluate impact shortcut connections increasing number convolutions layers. present adaptation resnet model case temporal convolutions text table shows evolution test errors yelp review full data without shortcut connections. looking column without shortcut observe degradation problem original resnet article going layers test error rate increases using shortcut connections observe improved results network layers training test errors network less prone underﬁtting without shortcut connections. shortcut connections give better results network deep able reach state-of-the-art results them. plan explore adaptations residual networks temporal convolutions think milestone going deeper nlp. residual units better adapted text processing task help training even deeper models text processing left future research. exploring models text classiﬁcation tasks classes sounds promising. note important difference classiﬁcation tasks discussed work imagenet latter deals classes thus much information back-propagated network gradients. exploring impact depth temporal convolutional models categorization tasks hundreds thousands classes would interesting challenge left future research. presented architecture follows design principles operate lowest atomic representation text i.e. characters deep stack local operations i.e. convolutions max-pooling size learn high-level hierarchical representation sentence. architecture evaluated eight freely available large-scale data sets able show increasing depth convolutional layers steadily improves performance. models much deeper previously published convolutional neural networks outperform approaches data sets. best knowledge ﬁrst time beneﬁt depths shown convolutional neural networks nlp. eventhough text follows human-deﬁned rules images seen signals environment images small texts similar properties. texts also compositional many languages. characters combine form n-grams stems words phrase sentences etc. similar properties make comparison computer vision natural language processing proﬁtable believe future research invest making text processing models deeper. work ﬁrst attempt towards goal. paper focus deep convolutional neural networks sentence classiﬁcation tasks. applying similar ideas sequence processing tasks particular neural machine translation left future research. needs investigated whether also beneﬁt deeper convolutional encoders.", "year": 2016}