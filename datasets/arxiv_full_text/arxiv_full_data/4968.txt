{"title": "ACCNet: Actor-Coordinator-Critic Net for \"Learning-to-Communicate\" with  Deep Multi-agent Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "Communication is a critical factor for the big multi-agent world to stay organized and productive. Typically, most previous multi-agent \"learning-to-communicate\" studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm, which can not generalize to changing environment or large collection of agents.  In this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework for solving \"learning-to-communicate\" problem. The ACCNet naturally combines the powerful actor-critic reinforcement learning technology with deep learning technology. It can efficiently learn the communication protocols even from scratch under partially observable environment. We demonstrate that the ACCNet can achieve better results than several baselines under both continuous and discrete action space environments. We also analyse the learned protocols and discuss some design considerations.", "text": "nized productive. typically previous multi-agent learning-to-communicate studies predeﬁne communication protocols technologies tabular reinforcement learning evolutionary algorithm cannot generalize changing environment large collection agents directly. paper propose actor-coordinator-critic framework solving multi-agent learning-to-communicate problem. accnet naturally combines powerful actor-critic reinforcement learning technology deep learning technology. learn communication protocols even scratch partially observable environments. demonstrate accnet achieve better results several baselines continuous discrete action space environments. also analyse learned protocols discuss design considerations. communication important factor multi-agent world stay organized productive. applications individual agent limited capability particularly critical multiple agents learn communication protocols work collaborative example data routing congestion detection trafﬁc management learning evolutionary algorithm cannot generalize changing environment large collection agents directly. argue ﬁeld requires in-depth studies technologies. recently researchers seen success deep marl i.e. combination deep learning multi-agent reinforcement learning many applications self-play two-player pong multi-player starcraft however work either assume full observability environment lack communication among multiple agents. naturally paper answer question learn multiagent communication protocols even scratch partially observable distributed environments help deep marl? consider setting multiple distributed agents fully cooperative goal maximize shared discounted rewards partially observable environment. full cooperation means agents receive independent contributions. partially observable environments mean agent observe underlying markov states must learn effective communication protocols. fact problem setting exactly modelled dec-pomdp-com extension dec-pomdp considering communication. novelty communication bandwidth limited. limited communication bandwidth common setting recent learningto-communicate studies traditional cooperative agents share sensations learned policies even training episodes suitable real-world applications communication takes much bandwidth. opinion limited communication bandwidth meanings. hand message speciﬁc timestep transported using packets take much bandwidth. hand valuable message necessary reduce bandwidth requirement. message comes time time intermittent time task-speciﬁc. achieve former limited bandwidth suggest deep neural networks compress message message dimension packets needed transporting message controlled. latter introduce corresponding methods based gating mechanism token mechanism another paper space limitation. propose actor-coordinator-critic framework combines powerful actor-critic technology technology. accnet paradigms. ﬁrst ac-cnet learns communication protocols among actors help coordinator keeps critics independent. however actors ac-cnet inevitably need communication even execution impractical special situations second a-ccnet learns communication protocols among critics help coordinator keeps actors independent. actors independent cooperate even without communication a-ccnet trained well. note that actor critic different agents services agent. explore proposed accnet different partially observable environments. experiments show that ac-cnet a-ccnet achieve good results simple multi-agent environments; complex environments a-ccnet better generalization ability performs almost like ideal fully observable models. best knowledge ﬁrst work investigate multi-agent learning-to-communicate problem based deep actor-critic architecture partially observable environment. reinforcement learning machine learning approach solve sequential decision making problem. timestep agent observes state takes action receives feedback reward environment observes state st+. goal learn policy i.e. mapping state action maximize expected discount cumulative future reward model-free algorithms divided three groups actor-only methods directly learn parameterized policy generate continuous action suffer high variance estimation policy gradient. criticmethods variance temporal difference learning estimate q-value policy derived using greedy action selection i.e. maxa usually used discrete action ﬁnding computationally intensive continuous action space. actor-critic methods jointly learn preserve advantages actor-only critic-only methods. schematic structure actor-critic methods shown figure functions reinforce other correct actor gives high rewarding trajectory updates critic towards right direction; correct critic picks good action actor reinforce. mutual reinforcement behavior helps actor-critic methods avoid local minima similar work openai released time. another concurrent work oxford also uses similar idea. explicitly address learning-to-communicate problem afﬁrm other’s methods results mutually. comparison accnet related studies shown table converge faster particular on-policy methods follow recent policy sample trajectory training speciﬁcally actor uses stochastic policy action selection actor critic updated based following td-error stochastic policy gradient theorem learn communication protocols efﬁciently critical success multiagent systems. previous work predeﬁne communication protocols others technologies tabular evolutionary algorithm cannot generalize changing environment large collection agents directly point out. recently end-to-end differentiable communication channel embedded deep neural network proven useful learning communication protocols. generally protocols optimized simultaneously network optimized. work instance method relevant studies include commnet dial bicnet commnet single network designed agents. input concatenation current states agents. communication channels embedded network layers. agent sends hidden state communication message current layer channel. averaged message agents sent next layer speciﬁc agent. however single network communication channel layer easy scale dial trains single network individual agent. timestep agent outputs message input agents next timestep. learn communication protocols also pushes gradients agent another communication channels. however message delayed timestep environment non-stationary multi-agent situation. commnet dial based discrete action. bicnet based actor-critic methods continuous action. uses bi-directional recurrent neural networks communication channels. approach allows single agent maintain internal state share information collaborators relevant excellent studies include limited researchers veriﬁed possibility learning communication protocols among agents. nevertheless providing general framework ease learning communication protocols among agents. straightforward approach build communication channel actors keep critics independent. shown figure coordinator communication channel used coordinating actors generate coordinated actions call paradigm ac-cnet. speciﬁcally agent encodes local state local message sends coordinator generates global communication signal agent considering messages agents. global signal encoding local messages expect catch global information system. integrated state concatenation local state global signal input actor-critic model. whole ac-cnet trained original actor-critic model. agents generate actions shared global knowledge even without communication training? execution? answer ﬁrst glance also think fortunately machine learning fascinating property prediction model trained auxiliary data model trained need longer kept. move communication among actors critics actors independently take actions according speciﬁc states execution auxiliary critics training need longer kept. fact possible actor-critic methods. however actor-only critic-only methods unsuitable task training execution mechanisms methods exactly same. shown figure coordinator communication channel used coordinating critics generate better estimated q-values call paradigm a-ccnet. speciﬁcally actor a-ccnet actor original actor-critic model shown figure critics communicate coordinator generate estimated q-values. compared ac-cnet communication occurs among actors communication signal encode local state a-ccnet communication among critics state action encoded communication signal. expect accnet generate better policies conﬁrmed experiments. besides designs critic. critic uses global signal generate q-values directly critic combines global signal local message generate q-values. designs actors generate actions independently without communication execution. ac-cnet critics independent update agent based equation like updating single actor-critic agent. difference need push gradients actors coordinator communication channels communication protocols also optimized simultaneously. generally speaking injective function. besides using discrete continuous action separately natural. however discrete action function states without knowing actions equation longer true. primary insight accnet agent knows states actions agents environment could treated stationary regardless changing policies. formally equation always keeps true agent indexed changing policies coma maddpg a-ccnet share similar idea accelerating training help critics executing real environment based actors. research purposes different. coma aims solving credit assignment problem multi-agent cooperative environments. maddpg wants investigate cooperation competition among agents. proposed accnet tries provide general framework ease learning communication protocols among agents even scratch. speciﬁcally coma based stochastic policy gradient theorem reinforce algorithm. uses counterfactual baseline centralised critic address multi-agent credit assignment problem. however experiments discrete action space environments assume critic entire game screen. maddpg extends ddpg multi-agent environments. authors verify method suitable cooperative competitive tasks. however experiments limited continuous action space environments. coma maddpg address learning-to-communicate problem explicitly states actions agents directly without considering communication cost. nevertheless afﬁrm other’s methods results mutually. fully cooperative discrete action continuous action parti. observable distri. agents limited bandwith indep. execution also deal competitive tasks well. address credit assignment problem well. section test proposed accnet continuous discrete action space environments. environments partial observable multiple distributed fully cooperative agents. continuous action space environment problem deﬁnition. continuous action space environment focus network routing domain problem modiﬁed currently internet made many networks. network shown figure several edge routers. edge routers combined ingress-egress router pair i-th ie-pair input demand available paths used deliver ingress-router egress-router. path made several links link belong several paths. l-th link transmission capacity link utilization ratio know high link utilization ratio dealing burst trafﬁc want good trafﬁc splitting policy jointly ie-pairs across available paths minimize maximum link utilization ratio network. setting. design following elements. state. current trafﬁc demand static network topology information available. also encode estimated link utilization ratio state. speciﬁcally local state fully-connected controller agents controlled fully-connected actor-critic network learn trafﬁc splitting policy. communication channel embedded network without bandwidth limitation. model worst situation model seen ideal situation. besides mentioned before design kinds critics a-ccnet critics share critic separately learns following models fc-sep fc-sha ac-cnet a-ccnet-sep a-ccnet-sha. experiment results. environment care convergence ratio independent experiments maximum link utilization ratio i-th bottleneck link convergence. results shown table space limitation results threeie supplementary material. models high mlui simple twoie topology. a-ccnet better performance ac-cnet ind. even similar performance ideal fully observable model. complex fiveie topology performances ac-cnet drop severely a-ccnet still keep ability performing almost like ideal model. reason a-ccnet global information models a-ccnet communication among critics local state action encoded communication signal communication signal ac-cnet encode local state exchange information all. case information means environment could seen stationary illustrated equation example packets transmitted) agent emit large message value agent usually emits small message value. action value agent splits trafﬁc agent split trafﬁc underused. besides agent wider range state value message value action value generated agent also wider agent. sophisticated coordinated behaviors critical marl systems stay organized. discrete action space environment problem deﬁnition. consider trafﬁc junction problem modiﬁed shown figure four cars deriving -way junction road. generated reaches destination edge grid. simulation classiﬁed failure location overlaps occurred timesteps. target learn driving policy failure rate setting. elements commnet. state. cars know location driving direction. cannot cars. represent local state one-hot vector {location direction}. action. possible actions gassing cell route experiment results. table shows results task. training models episodes commnet proposed a-ccnet lower commnet baselines. training episode increases a-ccnet lower higher models cannot results. communication message analysis. special driving policy left right always brake make space car. illustrate emitted messages different cars policy figure messages braking gassing naturally separated. type messages also separated different cars accnet distinguish them. besides gassing message diverse braking message. reason braking positions position grid road needs different gassing message. know design choices important success realworld applications. example experience replay frame skipping target network reward clipping asynchronous training auxiliary task even methods batch normalization attention mechanism skip connection widely adopted section brieﬂy present design choices used accnet hoping researchers conﬁrm usefulness environments. note design choices need studied. embedded communication channel. suggest deep neural networks encode communication message ﬁnal message dimension controlled independent dimension original information. importantly communication channel embedded deep neural networks communication protocols learned even scratch end-to-end differentiable network optimized. concurrent experience replay experience replay beneﬁcial single-agent except making collection process training data efﬁcient also break correlation among sequential training data accelerate convergence models. however point necessary disable experience replay marl non-concurrent property local experiences sampled independently agent. propose address problem. generally speaking samples concurrent experiences collected timestep agents trained concurrently. fact replay method name cer. refer readers paper details. current episode experience replay traditional experience replay methods uniformly sample batch experiences replay buffer training examples model updating. introduce prioritized experience replay based magnitude td-error accelerate learning. proposed ceer seen time-prioritized replay method. ceer keeps experiences current episode temporary buffer combines experiences main replay buffer training examples episode. preliminary experiments show effectiveness method. detailed analyses found supplementary material. disabled experience replay discrete action space environments. training discrete action space environments non-stable matter replay method used. footnote explain phenomenon extent research needed. full-information activation function sensitive continuous action. ideally policy one-to-one function mapping state optimal action neural network approximate meet one-to-one mapping requirement throw away information state layer network otherwise similar states encoded identical hidden vector mapped optimal action suggest sigmoid etc. rather relu activation functions sensitive continuous action space applications. similarly suggest relu discrete action space applications action ﬁnite similar states often correspond optimal action applications. preliminary experiments show relu based models generate averagely optimal action exactly optimal action. analyses found supplementary material. centralized coordinator. single point failure? accnet fully distributed agent special designed agents coordinator. addition a-ccnet need coordinator execution centralized training common setting marl systems proposed accnet born combined abilities deep models actorcritic reinforce models general framework learn communication protocols scratch fully cooperative partially observable marl problems matter action space continuous discrete. specially a-ccnet concrete implementation accnet make training marl systems stationary previous methods supported mathematical equation experimental results various environments. another attractive advantage a-ccnet need communication execution still keeps good generalization ability. future work efforts following important challenging problems. make training discrete action space marl systems stationary. special experience replay method powerful tool problem. make communication signals sparse. paper deep neural networks compress original communication messages. method achieve spatial-sparsity i.e. dimension communication signals limited values around zero. another time-sparsity i.e. communication signals come intermittently. although concepts borrowed sparse autoencoder useful real-world distributed marl systems. gating mechanism token mechanism useful achieving time-sparsity. introduce methods formally future. authors would like thank xiangyu weichen chao quanbin wang yiping song anonymous reviewers insightful comments. work supported national natural science foundation china grant no.. contact author zhen xiao.", "year": 2017}