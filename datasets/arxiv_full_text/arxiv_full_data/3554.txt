{"title": "Deep Gradient Compression: Reducing the Communication Bandwidth for  Distributed Training", "tag": ["cs.CV", "cs.DC", "cs.LG", "stat.ML"], "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.", "text": "large-scale distributed training requires signiﬁcant communication bandwidth gradient exchange limits scalability multi-node training requires expensive high-bandwidth network infrastructure. situation gets even worse distributed training mobile devices suffers higher latency lower throughput intermittent poor connections. paper gradient exchange distributed redundant propose deep gradient compression greatly reduce communication bandwidth. preserve accuracy compression employs four methods momentum correction local gradient clipping momentum factor masking warm-up training. applied deep gradient compression image classiﬁcation speech recognition language modeling multiple datasets including cifar imagenet penn treebank librispeech corpus. scenarios deep gradient compression achieves gradient compression ratio without losing accuracy cutting gradient size resnet- deepspeech .mb. deep gradient compression enables large-scale distributed training inexpensive commodity gbps ethernet facilitates distributed training mobile. large-scale distributed training improves productivity training deeper larger models synchronous stochastic gradient descent widely used distributed training. increasing number training nodes taking advantage data parallelism total computation time forward-backward passes size training data dramatically reduced. however gradient exchange costly dwarfs savings computation time especially recurrent neural networks computation-to-communication ratio low. therefore network bandwidth becomes signiﬁcant bottleneck scaling distributed training. bandwidth problem gets even worse distributed training performed mobile devices federated learning training mobile devices appealing better privacy better personalization critical problem mobile devices suffer even lower network bandwidth intermittent network connections expensive mobile data plan. deep gradient compression solves communication bandwidth problem compressing gradients shown figure ensure loss accuracy employs momentum correction local gradient clipping gradient sparsiﬁcation maintain model performance. also uses momentum factor masking warmup training overcome staleness problem caused reduced communication. empirically veriﬁed deep gradient compression wide range tasks models datasets image classiﬁcation language modeling speech recognition experiments demonstrate gradients compressed without loss accuracy order magnitude higher previous work researchers proposed many approaches overcome communication bottleneck distributed training. instance asynchronous accelerates training removing gradient synchronization updating parameters immediately node completed back-propagation gradient quantization sparsiﬁcation reduce communication data size also extensively studied. gradient quantization quantizing gradients low-precision values reduce communication bandwidth. seide proposed -bit reduce gradients transfer data size achieved speedup traditional speech applications. alistarh proposed anapproach called qsgd balance trade-off accuracy gradient precision. similar qsgd developed terngrad uses -level gradients. works demonstrate convergence quantized training although terngrad examined cnns qsgd examined training loss rnns. also attempts quantize entire model including gradients. dorefa-net uses -bit weights -bit gradients. gradient sparsiﬁcation strom proposed threshold quantization send gradients larger predeﬁned constant threshold. however threshold hard choose practice. therefore dryden chose ﬁxed proportion positive negative gradient updates separately heaﬁeld proposed gradient dropping sparsify gradients single threshold based absolute value. keep convergence speed gradient dropping requires adding layer normalization. gradient dropping saves gradient exchange incurring loss bleu score machine translation task. concurrently chen proposed automatically tunes compression rate depending local gradient activity gained compression ratio around fully-connected layers convolutional layers negligible degradation top- accuracy imagenet dataset. compared previous work pushes gradient compression ratio whole model require extra layer normalalgorithm gradient sparsiﬁcation node input dataset input minibatch size node input number nodes input optimization function input init parameters w··· reduce communication bandwidth sending important gradients gradient magnitude simple heuristics importance gradients larger threshold transmitted. avoid losing information accumulate rest gradients locally. eventually gradients become large enough transmitted. thus send large gradients immediately eventually send gradients time shown algorithm encode function packs -bit nonzero gradient values -bit lengths zeros. insight local gradient accumulation equivalent increasing batch size time. loss function want optimize. synchronous distributed performs following update training nodes total equation shows local gradient accumulation considered increasing batch size length sparse update interval iterations gradient sent. learning rate scaling commonly used technique deal large minibatch. automatically satisﬁed equation learning rate batch size canceled out. without care sparse update greatly harm convergence sparsity extremely high example algorithm incurred loss accuracy cifar dataset shown figure momentum correction local gradient clipping mitigate problem. momentum correction momentum widely used place vanilla sgd. however algorithm doesn’t directly apply momentum term since ignores discounting factor sparse update intervals. ﬁrst term local gradient accumulation training node accumulation result larger threshold pass hard thresholding sparse function encoded sent network second term. similarly line algorithm accumulation result gets cleared mask sparse function. equation compared equation leads loss convergence performance. illustrated figure equation drives optimization point point local gradient accumulation equation goes point gradient sparsity high update interval dramatically increases thus signiﬁcant side effect harm model performance. avoid error need momentum correction equation make sure sparse update equivalent dense update equation regard velocity equation gradient second term equation considered vanilla gradient local gradient accumulation proved effective vanilla section therefore locally accumulate velocity instead real gradient migrate equation approach equation ﬁrst terms corrected local gradient accumulation accumulation result used subsequent sparsiﬁcation communication. simple change local equation equation shown figure refer migration momentum correction. tweak update equation doesn’t incur hyper parameter. beyond vanilla momentum also look nesterov momentum appendix similar momentum sgd. local gradient clipping gradient clipping widely adopted avoid exploding gradient problem method proposed pascanu rescales gradients whenever l-norms exceeds threshold. step conventionally executed gradient aggregation nodes. accumulate gradients iterations node independently perform gradient clipping locally adding current gradient previous accumulation explained appendix scale threshold current node’s fraction global threshold nodes identical gradient distributions. practice local gradient clipping behaves similarly vanilla gradient clipping training suggests assumption might valid real-world data. delay update small gradients updates occur outdated stale. experiments parameters updated every iterations gradient sparsity quite long compared number iterations epoch. staleness slow convergence degrade model performance. mitigate staleness momentum factor masking warm-up training. momentum factor masking mitliagkas discussed staleness caused asynchrony attributed term described implicit momentum. inspired work introduce momentum factor masking alleviate staleness. instead searching momentum coefﬁcient suggested mitliagkas simply apply mask accumulated gradients momentum factor equation warm-up training early stages training network changing rapidly gradients diverse aggressive. sparsifying gradients limits range variation model thus prolongs period network changes dramatically. meanwhile remaining aggressive gradients early stage accumulated chosen next update therefore outweigh latest gradients misguide optimization direction. warm-up training method introduced large minibatch training helpful. warm-up period less aggressive learning rate slow changing speed neural network start training also less aggressive gradient sparsity reduce number extreme gradients delayed. instead linearly ramping learning rate ﬁrst several epochs exponentially increase gradient sparsity relatively small value ﬁnal value order help training adapt gradients larger sparsity. shown table momentum correction local gradient clipping improve local gradient accumulation momentum factor masking warm-up training alleviate staleness effect. gradient sparsiﬁcation local gradient accumulation four techniques make deep gradient compression help push gradient compression ratio higher maintaining accuracy. validate approach three types machine learning tasks image classiﬁcation cifar imagenet language modeling penn treebank dataset speech recognition librispeech corpus. hyper-parameter introduced deep gradient compression warm-up training strategy. experiments related rise sparsity warm-up image classiﬁcation studied resnet- cifar alexnet resnet- imagenet. cifar consists training images validation images classes imagenet contains million training images validation images classes train models momentum following training schedule gross wilber warm-up period epochs epochs cifar epochs epochs imagenet dataset. language modeling penn treebank corpus dataset consists training validation test words vocabulary select mikolov adopt -layer lstm language model architecture hidden units layer tying weights encoder decoder suggested inan using vanilla gradient clipping learning rate decays improvement made validation loss. warm-up period epoch epochs. speech recognition dataset contains training test utterances librispeech corpus contains hours reading speech deepspeech architecture without n-gram language model multi-layer following stack convolution layers train -layer lstm hidden units layer -layer hidden units layer librispeech nesterov momentum gradient clipping learning rate anneals every epoch. warm-up period epoch epochs. ﬁrst examine deep gradient compression image classiﬁcation task. figure top- accuracy training loss resnet- cifar nodes. gradient sparsity learning curve gradient dropping worse baseline gradient staleness. momentum correction learning curve converges slightly faster accuracy much closer baseline. momentum factor masking warm-up training techniques gradient staleness eliminated baseline gradient dropping deep gradient compression baseline gradient dropping deep gradient compression baseline gradient dropping deep gradient compression baseline gradient dropping deep gradient compression scaling large-scale dataset figure show learning curve resnet- gradient sparsity accuracy fully matches baseline. interesting observation top- error training sparse gradients decreases faster baseline training loss. table shows results alexnet resnet- training imagenet nodes. compare gradient compression ratio terngrad alexnet deep gradient compression gives better compression terngrad loss accuracy. resnet- compression ratio slightly lower slight increase accuracy. language modeling figure shows perplexity training loss language model trained nodes gradient sparsity training loss deep gradient compression closely match baseline validation perplexity. table deep gradient compression compresses gradient slight reduction perplexity. speech recognition figure shows word error rate training loss curve -layer lstm dataset nodes gradient sparsity learning curves show improvement acquired techniques deep gradient compression image network. table shows word error rate performance librispeech test dataset test-clean contains clean speech test-other noisy speech. model trained deep gradient compression gains better recognition ability clean noisy speech even gradients size compressed implementing requires gradient top-k selection. given target sparsity ratio need pick largest millions weights. complexity number gradient elements propose sampling reduce top-k selection time. sample gradients perform top-k selection samples estimate threshold entire population. number gradients exceeding threshold expected precise threshold calculated already-selected gradients. hierarchically calculating threshold signiﬁcantly reduces top-k selection time. practice total extra computation time negligible compared network communication time usually hundreds milliseconds several seconds depending network bandwidth. performance model proposed perform scalability analysis combining lightweight proﬁling single training node analytical communication modeling. all-reduce communication model density sparse data doubles every aggregation step worst case. however even considering effect deep gradient compression still signiﬁcantly reduces network communication time implied figure figure shows speedup multi-node training compared single-node training. conventional training achieves much worse speedup gbps gbps ethernet nonetheless deep gradient compression enables training gbps ethernet competitive conventional training gbps ethernet. instance training alexnet nodes conventional training achieves speedup gbps ethernet speedup achieved gbps ethernet. comparison figure deep gradient compression beneﬁts even communication-to-computation ratio model higher network bandwidth lower. deep gradient compression compresses gradient wide range cnns rnns. achieve compression without slowing convergence employs momentum correction local gradient clipping momentum factor masking warm-up training. propose hierarchical threshold selection speed gradient sparsiﬁcation process. deep gradient compression reduces required communication bandwidth improves scalability distributed training inexpensive commodity networking infrastructure. chia-yu chen jungwook choi daniel brand ankur agrawal zhang kailash gopalakrishnan. adacomp adaptive residual gradient compression data-parallel distributed training. arxiv preprint arxiv. nikoli dryden jacobs moon brian essen. communication quantization dataparallel training deep neural networks. proceedings workshop machine learning high performance computing environments ieee press priya goyal piotr doll´ar ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. arxiv preprint arxiv. awni hannun carl case jared casper bryan catanzaro greg diamos erich elsen ryan prenger sanjeev satheesh shubho sengupta adam coates deep speech scaling end-to-end speech recognition. arxiv preprint arxiv. jakub koneˇcn`y brendan mcmahan felix peter richt´arik ananda theertha suresh dave bacon. federated learning strategies improving communication efﬁciency. arxiv preprint arxiv. vassil panayotov guoguo chen daniel povey sanjeev khudanpur. librispeech corpus based public domain audio books. acoustics speech signal processing ieee international conference ieee benjamin recht christopher stephen wright feng niu. hogwild lock-free approach parallelizing stochastic gradient descent. advances neural information processing systems cong feng chunpeng yandan wang yiran chen terngrad ternary gradients reduce communication distributed deep learning. advances neural information processing systems eric xing qirong jinliang seunghak zheng pengtao abhimanu kumar yaoliang petuum platform distributed machine learning data. ieee transactions data shuchang zhou yuxin zekun xinyu zhou yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. arxiv preprint arxiv. practice training node performs forward-backward pass different batches sampled training dataset network model. gradients nodes summed optimize models. synchronization step models different nodes always training. aggregation step achieved ways. method using parameter servers intermediary store parameters among several servers nodes push gradients servers servers waiting gradients nodes. gradients sent servers update parameters nodes pull latest parameters servers. method perform all-reduce operation gradients among nodes update parameters node independently shown algorithm figure paper adopt latter approach default. algorithm distributed synchronous node input dataset input minibatch size node input number nodes input optimization function input init parameters {w··· training recurrent neural network gradient clipping perform gradient clipping locally adding current gradient algorithm denote origin threshold gradients l-norm ||g|| thrg threshold local gradients l-norm ||gk|| thrgk. algorithm deep gradient compression vanilla momentum node input dataset input minibatch size node input momentum input number nodes input optimization function input initial parameters {w··· algorithm deep gradient compression nesterov momentum node input dataset input minibatch size node input momentum input number nodes input optimization function input initial parameters {w···", "year": 2017}