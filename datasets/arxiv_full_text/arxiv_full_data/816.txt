{"title": "Reverse Curriculum Generation for Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "cs.NE", "cs.RO"], "abstract": "Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in reverse, gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.", "text": "abstract many relevant tasks require agent reach certain state manipulate objects desired conﬁguration. example might want robot align assemble gear onto axle insert turn lock. goal-oriented tasks present considerable challenge reinforcement learning since natural reward function sparse prohibitive amounts exploration required reach goal receive learning signal. past approaches tackle problems exploiting expert demonstrations manually designing task-speciﬁc reward shaping function guide learning agent. instead propose method learn tasks without requiring prior knowledge obtaining single state task achieved. robot trained reverse gradually learning reach goal start states increasingly goal. method automatically generates curriculum start states adapts agent’s performance leading efﬁcient training goal-oriented tasks. demonstrate approach difﬁcult simulated navigation ﬁne-grained manipulation problems solvable state-of-the-art reinforcement learning methods. reinforcement learning powerful learning technique training agent optimize reward function. reinforcement learning demonstrated complex tasks locomotion atari games racing games robotic manipulation tasks however many tasks hard design reward function easy maximize yields desired behavior optimized. ubiquitous example goal-oriented task; tasks natural reward function usually sparse giving binary reward task completed sparse reward create difﬁculties learning-based approaches hand non-sparse reward functions tasks might lead undesired behaviors example suppose want seven robotic learn align assemble gear onto axle place ring onto shown fig. complex precise motion required align ring slide bottom makes learning highly impractical binary reward used. hand using reward function based distance center ring bottom leads learning policy places ring next agent never learns needs ﬁrst lift ring carefully insert shaping reward function efﬁciently guide policy towards desired solution often requires considerable human expert effort experimentation correct shaping function task. another source prior knowledge demonstrations requires expert intervention. work avoid reward engineering demonstrations exploiting insights. first easier reach goal states nearby goal states nearby agent already knows reach goal. second applying random actions state leads agent feasible nearby states much harder reach goal. understood requiring minimum degree reversibility usually satisﬁed many robotic manipulation tasks like assembly manufacturing. take advantage insights develop reverse learning approach solving difﬁcult manipulation tasks. robot ﬁrst trained reach goal start states nearby given goal state. then leveraging knowledge robot trained solve task increasingly distant start states. start states automatically generated executing short random walk previous start states reward still require training. method learning reverse growing outwards goal inspired dynamic programming methods like value iteration solutions easier sub-problems used compute solution harder problems. paper present efﬁcient principled framework performing reverse learning. method automatically generates curriculum initial positions learn achieve task. curriculum constantly adapts learning agent observing performance step training process. method requires prior knowledge task providing single state achieves task contributions paper include formalizing novel problem deﬁnition ﬁnding optimal start-state distribution curriculum-based approaches manually designed schedules explored supervised learning split particularly complex tasks smaller easier-to-solve sub-problems. particular type curriculum learning explicitly enables learner reject examples currently considers hard type adaptive curriculum mainly applied supervised tasks practical curriculum approaches rely pre-speciﬁed task sequences general frameworks proposed generate increasingly hard problems although implementations concretize idea tackle preliminary tasks similar line work uses intrinsic motivation based learning progress obtain developmental trajectories focus increasingly harder tasks nevertheless method requires iteratively partitioning full task space strongly limits application ﬁne-grain manipulation tasks like ones presented work recent work curriculum assumes baseline performances several tasks given uses gauge tasks hardest require training however framework handle ﬁnite sets tasks requires task learnable own. hand method trains policy generalizes continuously parameterized tasks shown perform well even sparse rewards allocating training effort tasks hard current performance agent. closer method adaptively generating tasks train interesting asymmetric selfplay strategy recently proposed contrary approach aims generate train tasks appropriate level difﬁculty asymmetric component method lead biased exploration concentrating subset tasks appropriate level difﬁculty authors experiments suggests. problem time-oriented metric hardness lead poor performance continuous state-action spaces typical robotics. furthermore approach designed exploration bonus single target task; contrast deﬁne problem efﬁciently optimizing policy across range start states considered relevant improve generalization approach understood sequentially composing locally stabilizing controllers growing tree stabilized trajectories backwards goal state similar work done tedrake viewed funnel takes start states goal state series locally valid policies unlike methods approach require dynamic model system. counterpart closer approach work bagnell policy search algorithm spirit traditional dynamic programming methods proposed learn non-stationary policy learn done last time-step back learn previous time-step nevertheless require stronger assumption access baseline distributions approximate optimal state-distribution every time-step. idea directly inﬂuencing start state distribution accelerate learning markov decision process already drawn attention past. kakade langford studied idea exploiting access ‘generative model’ allows training policy ﬁxed ‘restart distribution’ different originally speciﬁed mdp. properly chosen proven improve policy training ﬁnal performance original start state distribution. nevertheless practical procedure given choose distribution don’t consider adapting start state distribution training researchers proposed expert demonstrations improve learning model-free algorithms either modifying start state distribution uniform among states visited provided trajectories biasing exploration towards relevant regions method works without expert demonstrations compare lines research. consider general problem learning policy leads system speciﬁed goal-space start state sampled given distribution. section ﬁrst brieﬂy introduce general reinforcement learning framework formally deﬁne problem statement. deﬁne discrete-time ﬁnite-horizon markov decision process tuple state action transition probability distribution bounded reward function start state distribution horizon. learn stochastic policy parametrized maximizes objective expected return es∼ρr. expected reward starting starting denotes whole trajectory policy search methods iteratively collect trajectories on-policy improve current policy work propose instead different start-state distribution every training iteration learning speed. learning progress still evaluated based original distribution convergence desirable required optimal policy start distribution also optimal long support coincide. case approximately optimal policies bounds performance derived advocated rajeswaran important able train agent achieve goal large start states agent trained would much robust agent trained single start state could recover undesired deviations intended trajectory. therefore choose start states feasible points wide area around goal. hand goal space robotics ﬁne-grained manipulation tasks deﬁned small states around desired conﬁguration discussed above sparsity reward function makes learning extremely difﬁcult algorithms approaches like reward shaping difﬁcult time-consuming engineer task. following subsection introduce three assumptions rest paper describes leverage assumptions efﬁciently learn achieve complex goal-oriented tasks directly sparse reward functions. work study exploit three assumptions hold true wide range practical learning problems assumption arbitrarily reset agent start state beginning trajectories. assumption least state provided assumption markov chain induced taking uniformly sampled random actions communicating class including start states given goal state ﬁrst assumption considered previously deemed considerably weaker assumption access full transition model mdp. assumption improve learning mdps require large exploration already demonstrated kakade langford nevertheless propose concrete procedure choose distribution sample start states order maximally improve objective case combining assumption assumption able reset state critical method initialize start state distribution concentrate around goal space beginning learning.for second assumption note assume access state goal region; require description full region trajectories leading finally assumption ensures goal reached relevant start states start states also reached goal; assumption satisﬁed many robotic problems interest long major irreversibilities system. next sections detail automatic curriculum generation method based continuously adapting start state distribution current performance policy. demonstrate value method challenging robotic manipulation tasks. wide range goal-oriented problems reaching goal overwhelming majority start states requires prohibitive amount on-policy undirected exploration. hand usually easy learning agent reach goal states nearby goal state therefore learning states fast agent perceive strong signal even indicator reward introduced section agent knows reach goal nearby states train even states bootstrap already acquired knowledge. reverse expansion inspired classical methods like value iteration policy iteration although case assume knowledge transition model environments high-dimensional continuous actionstate spaces. following subsections propose method leverages assumptions previous section idea reverse expansion automatically adapt start state distribution generating curriculum start state distributions used tackle problems unsolvable standard methods. policy gradient strategies well suited robotic tasks continuous high dimensional action-spaces nevertheless applying directly original poorly tasks sparse rewards long horizons like challenging manipulation tasks. goal reached start states reward received policy cannot improve. therefore propose adapt distribution start states sampled train policy analogously held postulate goal-oriented environments strong learning signal obtained training start states agent reaches goal sometimes always. call start states good starts. formally training rmin iteration would like sample unif. unfortunately sampling uniformly intractable. nevertheless least beginning training states nearby goal state then iterations training start states likely completely mastered others still need training. good starts follow reasoning states nearby remaining rest section describe effective sampling feasible nearby states layout full algorithm. robotic manipulation tasks complex contacts constraints applying noise state-space yield many unfeasible start states example even small random perturbations joint angles seven degree-of-freedom generate large modiﬁcations end-effector position potentially placing infeasible state intersects surrounding objects. reason concept nearby states might unrelated euclidean distance states. instead understand proximity terms likely reach state taking actions mdp. therefore choose generate states certain seed state applying noise action space. means exploit assumption reset system state execute short brownian motion rollouts horizon taking actions method generating nearby states detailed procedure total sampled states large enough nnew desired states startsnew obtained subsampling extend directions around input states starts. states visited rollouts guaranteed feasible used start states keep training policy. generic algorithm detailed algorithm ﬁrst initialize policy good start states list starts given goal state perform iter training iterations algorithm choice train pol. case perform iterations trust region policy optimization on-policy method could used. every iteration start state distribution uniform list start states obtained sampling nnew start states nearby ones good starts list starts nold start states replay buffer previous good starts startsold. already shown held replay buffer important feature avoid catastrophic forgetting. technically check states starts execute trajectories states estimate expected returns considerably increases sample complexity. instead trajectories collected train estimate save list rews. used select good start states next iteration picking ones rmin rmax. found heuristic give good enough estimate drastically decrease learning performance overall algorithm. method keeps expanding region state-space policy reach goal reliably. samples heavily nearby start states need training mastered avoiding start states receive reward current policy. thanks assumption brownian motion used generate start states eventually reach start states therefore method improves metric deﬁned sec. performance policy target start state distribution improve training focusing training good starts speed learning? brownian motion good generate good starts previous good starts? figure task images. source code videos performance obtained algorithm available here http//bit.ly/reversecurriculum point-mass maze point-mass agent must navigate within goal position g-shaped maze target start state distribution seek reach goal uniform feasible positions maze. maze quadruped robot must navigate center mass within goal position u-shaped maze target start state distribution seek reach goal uniform feasible positions inside maze. ring robot must learn place ring tight-ﬁtting round peg. task complete ring within bottom tall peg. target start state distribution seek reach goal uniform feasible joint positions center ring within bottom peg. insertion robot must learn insert key-hole. task completed distance three reference points extremities corresponding targets order reach target robot must ﬁrst insert speciﬁc orientation rotate degrees clockwise push forward rotate degrees counterclockwise. target start state distribution seek reach goal uniform feasible joint positions within key-hole. figure uniform sampling curves show average return policies learned trpo without modifying start state distribution. green blue curves correspond method ablation exploiting idea modifying start state distribution every learning iterations. approaches perform consistently better across board. case point-mass maze navigation task fig. observe uniform sampling high variance policies learn perform well side goal ant-maze experiments fig. also show considerable slow-down learning speed using plain trpo although effect less drastic start state distribution smaller space. complex manipulation tasks shown fig. probability reaching goal uniform sampling around ring task task. success probabilities correspond reliably reaching goal nearby positions ring already initialized close ﬁnal position. none learned policies trained original learn reach goal distant start states. hand methods succeed reaching goal wide range away start states. underlying training algorithm evaluation metric same. conclude training different start state distribution improve training even allow all. figure applying algorithm modify start state distribution considerably improves learning ﬁnal performance original mdp. elements involved improvement ﬁrst backwards expansion goal second concentration training efforts good starts. test relevance second element ablate method running samplenearby procedure states policy trained previous iteration. words select function algorithm replaced identity returning starts independently rewards rews obtained last training iteration. resulting algorithm performance shown brownian starts blue curve figures expected method still better modifying start state distribution slower learning running samplenearby around estimated good starts. evaluate upper bound beneﬁt provided idea sampling good starts. mentioned sec. would ideally like sample start states unif intractable. instead evaluate states brownian motion nearby states approximate evaluate much approximation hinders learning exhaustively sampling states lower dimensional point-mass maze task. every iteration samples states uniformly state-space empirically estimates return rmin rmax}. exhaustive rejects ones sampling method orders magnitude expensive terms sample complexity would practical use. particular easier point-mass maze task. performance shown brown curve fig. called oracle training states sampled manner improves learning rate ﬁnal performance. thus approximation using states leads loss performance beneﬁt greatly reduced computation time. finally compare another generating start states based asymmetric self-play method sukhbaatar basic idea train another policy alice proposes start states learning policy bob. seen method performs poorly pointmass maze task investigation shows alice often gets stuck local optimum leading poor start states suggestions bob. original paper method demonstrated discrete action spaces multi-modal distribution alice maintained; even settings authors observed alice easily stuck local optima. problem exacerbated moving continuous action spaces deﬁned unimodal gaussian distribution. detailed analysis failure modes appendix evaluate running procedure samplenearby around good starts yields good starts running samplenearby previously visited states. clearly seen figs. robotic manipulation tasks. propose method automatically adapt start state distribution agent trained performance original problem efﬁciently optimized. leverage three assumptions commonly satisﬁed simulated tasks tackle hard goal-oriented problems state methods cannot solve. limitation current approach generates start states grow single goal uniformly outwards cover original start state distribution unif. nevertheless target start states goal prior knowledge would interesting bias generated start distributions towards desired start distribution. promising future line work combine present automatic curriculum based start state generation goal generation similar classical results planning observed videos ﬁnal policy manipulation tasks agent learned exploit contacts instead avoiding them. therefore learning based aspect presented method huge potential tackle problems classical motion planning algorithms could struggle with environments non-rigid objects uncertainties task geometric parameters. also leave future work combine curriculum-generation approach domain randomization methods obtain policies transferable real world. references schulman moritz levine jordan abbeel. high-dimensional continuous control using generalized advantage estimation. international conference learning representation popov heess lillicrap hafner barth-maron vecerik lampe tassa erez riedmiller. data-efﬁcient deep reinforcement learning dexterous manipulation. arxiv preprint arxiv bengio vinyals jaitly shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems schmidhuber. power training increasingly general problem solver continually searching simplest still unsolvable problem. frontiers psychology srivastava steunebrink stollenga schmidhuber. continually adding self-invented problems repertoire first experiments powerplay. ieee international conference development learning epigenetic robotics subramanian isbell thomaz. exploration demonstration interactive reinforcement learning. proceedings international conference autonomous agents multiagent systems kuffner lavalle. rrt-connect efﬁcient approach single-query path planning. ieee international conference robotics automation volume pages ieee describe hyperparemeters used method. iteration generate start states append seed states total start states. subsample nnew start states. appended nold sampled start states states used initialize agent train policy. brownian motion rollouts horizon timesteps actions taken random sampled standard normal distribution method well baselines train multi-layer perceptron gaussian policy trpo implemented rllab trpo step-size baseline. tasks train batch size timesteps. experiments maximum horizon time steps maze experiments maximum horizon episode ends soon agent reaches goal state. deﬁne goal ball around goal state ball radius ring tasks point-mass maze task ant-maze task. deﬁnition rmin rmax discount factor optimization order encourage policy reach goal fast possible. performance metric tasks reach speciﬁed goal region start states feasible within certain distance goal region. therefore evaluate progress need collect trajectories starting states uniformly sampled pointmass maze navigation task straight forward designer give concrete description feasible space uniformly sample nevertheless trivial uniformly sample feasible start states robotics tasks. particular state space joints angles angular velocities physical constraints contactrich environments given geometries task. therefore uniformly sampling angular bounds mostly yields unfeasible states part end-effector intersecting objects scene. order approximate uniformly sampling assumption provided feasible goal state simply samplenearby procedure initialized starts large long time horizons large aggregated state data-set saved samples used proxy evaluate performance algorithm. figures show sampled start states data sets used evaluate ring task task. data sets available project website future reproducibility benchmarking. given quasi-static nature tasks considered generate initial angle positions initial velocities zero. generating initial velocities fairly simple extension approach leave future work. although method able train policies sparse rewards policy optimization steps train kind reward shaping available. extend already using discount factor motivates policies reach goal soon possible. similar reward modulations could included take account energy penalties reward shaping prior knowledge. example robotics tasks considered paper goal deﬁned terms reference state hence seems natural distance state additional penalty guide learning. however found modiﬁcation actually improve training. start states near goal policy learn reach goal simply indicator reward introduced section states away distance goal actually useful metric guide policy; hence distance reward actually guides policy updates towards suboptimal local optimum leading poor performance. fig. ring task much affected additional reward whereas task suffers considerably reward added. case maze navigation task observe applying trpo directly original incures high variance across learning curves. observed policies learned perform well certain side goal. reason learning algorithm batch on-policy method; therefore beginning learning uniformly sampling state-space might give batch trajectories reach goal hence likely come side goal. case algorithm update policy direction everywhere wrongly extrapolating successful trajectories received. less likely happen trajectories batch collected different start state distribution concentrates uniformly around goal better learning progress curves show. section compare performance method asymmetric self-play approach sukhbaatar although approach learns faster uniform sampling baseline gets stuck local optimum fails learn reach goal start-states point-mass maze task. explained above part reason method gets stuck local optimum alice represented unimodal gaussian distribution common representation policies continuous action spaces. thus alice’s policy tends converge moving single direction. original paper problem somewhat mitigated using discrete action space multi-modal distribution alice maintained. however even case authors original paper also observed alice tends converge local optimum difﬁculty alice reward function sparse inherently difﬁcult optimize. alice’s reward deﬁned time takes alice reach given start state goal time takes return goal start state. based reward optimal policy alice nearest state know return goal; lead large value small value theory lead automatic curriculum start-states bob. however practice sometimes bob’s policy might improve faster alice’s. case learned return goal many start states much faster alice reach start states goal. cases would hence thus alice’s rewards sparse hence difﬁcult alice’s policy improve leading locally optimal policy alice. reasons observed alice’s policy often getting stuck alice unable start-states propose already know reach goal from. implemented simple environment illustrates issues. environment synthetic reach goal state within radius goal. states within reach goal time proportional distance state goal; words states sg|/vb distance state goal bob’s speed. states goal know reach goal thus maximum possible value. setup illustrated figure region shown designates area within goal e.g. states knows reach goal. ﬁrst iteration alice random policy iterations training alice converged policy reaches location outside states knows reach goal states alice receives maximum reward large low. note also observe unimodal nature alice’s policy; alice converged policy proposes small states among possible states would receive similar reward. figure simple environment illustrate asymmetric self-play areas indicate states knows reach goal. blue points start-states proposed alice iteration point synthetically increase corresponding situation learns reach goal larger states. however alice’s policy already converged reaching small states optimal bob’s previous policy. states alice receives reward described above return states quickly goal thus alice receive reward signal able improve policy. hence alice’s policy remains stuck point able states propose simple case could attempt perform various hacks situation e.g. artiﬁcially increasing alice’s variance resetting alice random policy. however note that real example learning increasingly complex policy alice would need learn equally complex policy states cannot succeed from; hence simple ﬁxes would sufﬁce overcome problem. fundamentally asymmetric nature selfplay alice creates situation alice difﬁcult time learning often gets stuck local optimum unable improve.", "year": 2017}