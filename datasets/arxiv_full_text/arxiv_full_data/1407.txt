{"title": "Theory II: Landscape of the Empirical Risk in Deep Learning", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least in the case of the most successful Deep Convolutional Neural Networks (DCNNs), practitioners can always increase the network size to fit the training data (an extreme example would be [1]). The most successful DCNNs such as VGG and ResNets are best used with a degree of \"overparametrization\". In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove in the regression framework the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The argument that relies on the use of Bezout theorem is rigorous when the RELUs are replaced by a polynomial nonlinearity (which empirically works as well). As described in our Theory III [2] paper, the same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the most robust zero-minimizer. We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN's empirical loss surface, which might not be as complicated as people commonly believe.", "text": "previous theoretical work deep learning neural network optimization tend focus avoiding saddle points local minima. however practical observation that least case successful deep convolutional neural networks practitioners always increase network size training data successful dcnns resnets best used degree overparametrization. work characterize theory experiments landscape empirical risk overparametrized dcnns. ﬁrst prove regression framework existence large number degenerate global minimizers zero empirical error argument relies bezout theorem rigorous relus replaced polynomial nonlinearity described theory paper minimizers degenerate thus likely found furthermore select higher probability robust zero-minimizer. experimentally explored visualized landscape empirical risk dcnn cifar- entire training process especially global minima. finally based theoretical experimental results propose intuitive model landscape dcnn’s empirical loss surface might complicated people commonly believe. least three main parts theory deep neural networks. ﬁrst part approximation deep neural networks avoid curse dimensionality’ second part landscape minima empirical risk general global local minima? third part generalization generalize well despite standard overparametrization deep neural networks paper focus second part landscape empirical risk. theoretical analyses study nonlinear system equations corresponding critical points gradient loss zero minimizers corresponding interpolating solutions. equations functions representing network’s output contain relu nonlinearity. consider \u0001approximation norm using polynomial approximation corresponding legendre expansion. invoke bezout theorem conclude large number zero-error minima zero-error minima highly degenerate whereas local non-zero minima exist degenerate. case classiﬁcation zero error implies existence margin region dimensions around zero error. visualizations experimental explorations theoretical results indicate degenerate global minima loss surface dcnn. however unclear rest landscape look like. gain knowledge this visualize landscape entire training process using multidimensional scaling. also probe locally landscape diﬀerent locations perturbation interpolation experiments. simple model landscape summarizing theoretical experimental results propose simple baseline model landscape empirical risk shown figure conjecture loss surface dcnn complicated commonly believed. least case overparametrized dcnns loss surface might simply collection basins following interesting properties every basin reaches global minima. basins rugged perturbation noise leads slightly diﬀerent convergence path. despite perhaps locally rugged basin relatively regular overall landscape average model within basin gives model whose error roughly average errors original models. interpolation basins hand signiﬁcantly raise error. might good properties basin local minima encounter local minima cifar even training batch gradient descent without noise. assume deep network convolutional type overparametrization weights data points since successful deep networks used. conditions show imposing zero empirical error provides system equations large number degenerate solutions weights. equations polynomial weights coeﬃcients reﬂecting components data vectors system equations underdetermined assumed overparametrization. global minima degenerate many dimensions likely found local minima less degenerate. although generalization focus work minima likely generalize better sharp minima might reason overparametrized deep networks seem overﬁt. figure landscape empirical risk overparametrized dcnn simply collection basins. proﬁle view basin top-down view basin example landscape empirical risk example perturbation small perturbation move model current basin re-training converges back bottom basin. perturbation large re-training converges another basin. example interpolation averaging models within basin tend give error average models averaging models basins tend give error higher models. example optimization trajectories correspond figure section following theoretical analysis landscape empirical risk based assumptions assume network overparametrized typically using several times parameters data points. practice even data augmentation always make model larger achieve overparametrization without sacriﬁcing either training generalization performance. section assumes regression framework. study many solutions weights lead perfect prediction training labels. classiﬁcation settings solutions subset solutions. function realized deep neural network polynomial relu units replaced univariate polynomial. course relu approximated within desired norm polynomial. well-determined case bezout theorem provides upper bound number solutions. number distinct zeros would equal product degrees equations. since system equations usually underdetermined many equations data points unknowns expect inﬁnite number global minima form regions zero empirical error. equations inconsistent still many global minima squared error solutions systems equations similar form. assume simplicity equations particular compositional form degree approximating equation determined desired accuracy approximating relu activation univariate polynomial degree number layers argument based relus approximation estimating number zeros qualitative since good approximation imply good approximation bezout theorem number zeros. notice however could abandon completely approximation approach consider number zeros relus replaced order univariate polynomial. argument would strctly apply relu networks would still carry weight types networks polynomial activation relus behave empirically similar way. supporting material provide simple example network associated equations exact zeros. system underconstrained polynomial equations degree general many constraints data points much larger number unknown weights w··· solutions system inconsistent happens linear combination equations otherwise inﬁnitely many complex solutions solutions algebraic dimension least underdetermined system chosen random dimension equal probability one. even non-degenerate case bezout theorem suggests many solutions. layers degree polynomial equations datapoints bezout upper bound zeros weights even number real zero corresponding zero empirical error much smaller number still enormous cifar situation high mentioned several cases expect absolute zeros exist zero empirical error. equations inconsistent seems likely global minima similar properties exist. figure convert deep network polynomial function using polynomial nonlinearity. long nonlinearity approximates relu well polynomial performs similarly relu net. theory applies rigorously polynomial net. polynomial equations weights higher order case zero-minimizers. course satisﬁed degenerate zeros empirical error also additional non-degenerate solutions. empirical work described analyze classiﬁcation problem cross entropy loss. theoretical analyses regression framework provide lower bound number solutions classiﬁcation problem. unless mentioned otherwise trained -layer deep convolutional neural network cifar-. layers convolutional layers stride pooling performed. batch normalizations used hidden layers. shifting scaling parameters used. data augmentation performed training ﬁxed parameters dcnn. multidimensional scaling core visualization approach multidimensional scaling record large number intermediate models process several training schemes. model high dimensional point number dimensions number parameters. strain-based algorithm applied points corresponding points found dissimilarity matrix points similar high-dimensional points possible. minus cosine distance used dissimilarity metric. robust scaling weights usually normalized bns. euclidean distance gives qualitatively similar results though. show figure optimization trajectories stochastic gradient descent throughout entire optimization process training dcnn cifar-. trajectories follow mini-batch approximations training loss surface. although trajectories noisy collected points along trajectories provide good visualization landscape empirical risk. show visualization based weights layer results layers qualitatively similar shown appendix. figure train -layer convolutional network cifar- stochastic gradient descent divide training process stages. stage perform parallel sgds learning rate epochs resulting parallel trajectories denoted diﬀerent colors. trajectories stage start ﬁnal model trajectory previous stage. trajectories stage start perturbed version perturbation performed adding gaussian noise weights layer standard deviation times layer’s standard deviation. general observe running trajectory almost always leads slightly diﬀerent convergence path. plot results layer weights collected throughout training epochs stage number ﬁgure represents model collected procedures. points space generated algorithm pairwise distances optimized reﬂect distances original high-dimensional space. results stages quite cluttered. applied separate stages also plot stage separately example. trajectories stages plotted appendix. figure visualizing exact training loss surface using batch gradient descent dcnn trained cifar- scratch using batch gradient descent numbers training errors. corresponds randomly initialized models epoch create branch perturbing model adding gaussian noise layers. standard deviation gaussian denotes standard deviation weights layer respectively. also interpolate models branches main trajectory epoch epoch. interpolated models evaluated entire training performance. first surprisingly stuck local minima indicating good properties landscape. test error solutions found somewhat worse found much worse another interesting observation training proceeds amount perturbation less able lead drastically diﬀerent trajectory. nevertheless perturbation almost always leads least slightly diﬀerent model. local neighborhood main trajectory seems relatively contain many good solutions supporting theoretical predictions. also intriguing interpolated models reasonable performance. results based weights layer results layers similar shown appendix. next visualize figure exact training loss surface training models using batch gradient descent addition training also performed perterbation interpolation experiments described figure main trajectory branches interpolated models together provides good visualization landscape empirical risk. perform detailed analyses several locations landscape. especially would like check global minima ﬂat. train -layer dcnn cifar- epochs epochs batch gradient descent performed close global minima possible. next select three models learning trajectory model epoch model epoch inal ﬁnal model epochs epochs bgd. results shown figure shown appendix. figure verifying ﬂatness global minima layerwise perturb weights model inal adding gaussian noise standard deviation standard deviation weights. perturbation continue training model epochs gradient descent procedure performed times resulting curves shown ﬁgures. training test classiﬁcation errors losses shown visualization trajectories shown trajectories converge diﬀerent solutions. visualization trajectory addition show confusion matrices converged models appendix verify indeed diﬀerent models. similar experiments found appendix. section propose simple baseline model landscape empirical risk consistent theoretical experimental ﬁndings. case overparametrized dcnns recapitulation main observations theoretically show large number global minimizers zero empirical error. minimizers degenerate. regardless stochastic gradient descent batch gradient descent small perturbation model almost always leads slightly diﬀerent convergence path. earlier perturbation training process diﬀerent ﬁnal model would interpolating nearby convergence paths lead another convergence path similar errors every epoch. interpolating distant models lead raised errors. observe local minima even training bgd. simple model consistent observations. ﬁrst-order characterization believe landscape empirical risk simply collection basins global minima. illustrations provided figure decreases hypervolume parameter space decreases example). amount scaling dimension volume shrinks much faster number dimension increases linear decrease dimension hypervolume decreases exponential function number dimensions. number dimensions number parameters volume shrinks incredibly fast. leads phenomenon observe experimentally whenever perturb model adding signiﬁcant noise loss almost always never down. larger perturbation error increases. reasons simple local landscape hyper-basin volume lower loss area small randomly perturbing point almost chance getting there. larger perturbation likely much higher loss area. nevertheless plausible variants model explain experimental ﬁndings. figure show alternative model call basin-fractal. model elegant also consistent observations. diﬀerence simple basins basin-fractal basin-fractal able walls models within basin. since fractal walls present levels errors. moment discovered walls models trajectories lead diﬀerent found signiﬁcant walls perturbation interpolation experiments. ﬁrst order model landscape would collection simple basins. nevertheless basin-fractal elegant perhaps walls loss areas noticed. another surprising ﬁnding basins that seem smooth local minima. even training batch gradient descent encounter local minima. trained long enough small enough learning rates always gets classiﬁcation error negligible cross entropy loss. deep learning references start hinton’s backpropagation lecun’s convolutional networks course multilayer convolutional networks around least back optical processing neocognitron convolutional neural network trained recognize characters. property compositionality main motivation hierarchical models visual cortex hmax regarded pyramid layers sequence conjunctions disjunctions. provided formal conditions deep networks avoid curse dimensionality. speciﬁcally several papers appeared landscape training error deep networks. techniques borrowed physics spin glasses used suggest existence band local minima high quality measured test error. argument however depends number assumptions rather implausible comments work problem). soudry carmon show mild over-parameterization dropout-like noise training error neural network hidden layer piece-wise linear activation zero every local minimum. results suggest energy landscape deep neural networks easy optimize. less hold practice easy optimize prototypical deep network near-zero loss training set. results shown work data dependent? visualized trajectories case ﬁtting random labels. qualitative diﬀerence results normal labels. safe results least label dependent. check ﬁtting random input data random labels give similar results. generalization? experimentally observed least experiments overparametrization hurt generalization all. conclusions overall characterize landscape empirical risk overparametrized dcnns theoretical analyses experimental explorations. provide simple baseline model landscape account theoretical experimental results. nevertheless ﬁnal model simple hard believe would completely characterize true loss surface dcnn. research warranted.", "year": 2017}