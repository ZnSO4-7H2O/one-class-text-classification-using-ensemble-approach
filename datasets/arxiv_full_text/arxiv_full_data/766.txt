{"title": "Learning in the Machine: Random Backpropagation and the Deep Learning  Channel", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. To better understand random backpropagation, we first connect it to the notions of local learning and learning channels. Through this connection, we derive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP (ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their computational complexity. We then study their behavior through simulations using the MNIST and CIFAR-10 bechnmark datasets. These simulations show that most of these variants work robustly, almost as well as backpropagation, and that multiplication by the derivatives of the activation functions is important. As a follow-up, we study also the low-end of the number of bits required to communicate error information over the learning channel. We then provide partial intuitive explanations for some of the remarkable properties of RBP and its variations. Finally, we prove several mathematical results, including the convergence to fixed points of linear chains of arbitrary length, the convergence to fixed points of linear autoencoders with decorrelated data, the long-term existence of solutions for linear systems with a single hidden layer and convergence in special cases, and the convergence to fixed points of non-linear chains, when the derivative of the activation functions is included.", "text": "abstract random backpropagation variant backpropagation algorithm training neural networks transpose forward matrices replaced ﬁxed random matrices calculation weight updates. remarkable eﬀectiveness spite using random matrices communicate error information completely removes taxing requirement maintaining symmetric weights physical neural system. better understand random backpropagation ﬁrst connect notions local learning learning channels. connection derive several alternatives including skipped adaptive sparse combinations analyze computational complexity. study behavior simulations using mnist cifar- bechnmark datasets. simulations show variants work robustly almost well backpropagation multiplication derivatives activation functions important. follow-up study also low-end number bits required communicate error information learning channel. provide partial intuitive explanations remarkable properties variations. finally prove several mathematical results including convergence ﬁxed points linear chains arbitrary length convergence ﬁxed points linear autoencoders decorrelated data long-term existence solutions linear systems single hidden layer convergence special cases convergence ﬁxed points non-linear chains derivative activation functions included. years question biological plausibility backpropagation algorithm implementing stochastic gradient descent neural networks raised several times. question gained relevance numerous successes achieved backpropagation variety problems ranging computer vision speech recognition engineering high energy physics biology natural sciences well recent results optimality backpropagation however several well known issues facing biological neural networks relation backpropagation include continuous real-valued nature gradient information ability change sign violating dale’s law; need kind teacher’s signal provide targets; need implementing linear operations involved backpropagation; need multiplying backpropagated signal derivatives forward activations time layer traversed; need precise alternation forward backward passes; complex geometry biological neurons problem transmitting error signals precision individual synapses. however perhaps formidable obstacle standard backpropagation algorithm requires propagating error signals backwards using synaptic weights identical corresponding forward weights. furthermore related problem suﬃciently recognized weight symmetry must maintained times learning during early neural development. hard imagine mechanisms biological neurons could create maintain perfect symmetry. however recent simulations surprisingly indicate symmetry required fact backpropagation works less well random weights used backpropagate errors. general goal investigate backpropagation random weights better understand works. foundation better understanding random backpropagation provided concepts local learning deep learning channels introduced thus begin introducing notations connecting concepts. turn leads derivation several alternatives study simulations well known benchmark datasets proceeding formal analyses. size input layer size hidden layer size output layer. assume layers fully connected denote weight connecting neuron layer neuron layer output neuron layer computed transfer functions usually neurons typical exceptions output layer usually monotonic increasing functions. typical functions used artiﬁcial neural networks identity logistic hyperbolic tangent rectiﬁed linear softmax. learning process. general asssume standard error functions squared error case regression identity transfer functions output layer relative entropy case classiﬁcation logistic softmax units output layer although essential point. applied stochastic fashion on-line batch form summing averaging training examples. single example omitting index simplicity standard backpropagation learning rule easily obtained applying chain rule given thus short errors propagated backwards essentially linear fashion using transpose forward matrices hence symmetry weights multiplication derivative corresponding forward activations every time layer traversed. standard random backpropagation operates exactly like backpropagation except weights used backward pass completely random ﬁxed. thus learning rule becomes thus general optimal weights must depend input targets well weights network. learning viewed lossy storage procedure transferring information contained training weights architecture. inputs lower weights leading layer subsumed term thus framework separate channel communicating information inputs deep weights necessary. thus focus feedback information targets contained term which physical neural system must transmitted dedicated channel. depends output target well weights layers fully connected case ways backpropagation process. addition depends also upper derivatives i.e. derivatives activations functions neurons unit layer fully connected case thus general solution critical equations weights outputs targets upper weights upper derivatives. backpropagation shows suﬃcient weights depend upper weights upper derivatives. physical neural system learning rules must also local sense involve variables available locally space time although simplicity focus locality space. thus typically present formalism local learning rule deep layer must form assuming targets local variables layer. among things allows organize stratify learning rules instance considering polynomial learning rules degree forth. deep local learning term describe local learning adaptive layers feedforward architecture. note hebbian learning form local learning deep local learning proposed instance fukushima train neocognitron architecture essentially feed forward convolutional neural network inspired earlier neurophysiological work hubel wiesel however deep local learning information targets propagated deep layers therefore general deep local learning cannot solutions critical equations thus cannot succeed learning complex functions critical equations optimal neural network learning algorithm must capable communicating information outputs targets upper weights deep weights physical neural system communication channel must exist communicate information. deep learning channel learning channel short studied using tools information complexity theory. physical systems learning channel must correspond physical channel leads important considerations regarding nature instance whether uses forward connections reverse direction diﬀerent connections. here focus primarily information coded sent channel. general information outputs targets communicated channel although backpropagation propagates information layer deep layers staged necessary could sent directly deep layer somehow skipping layers above. observation leads immediately skipped variant described next section. also important note principle information form however standard backpropagation shows possible send information synapses impinging onto neuron thus possible learn simpler type information form standard backpropagation uses information upper weights ways through output appears error terms backpropagation process itself. random backpropagation crucially shows information upper weights contained backpropagation process necessary. thus ultimately focus exclusively information simple form ﬁxed random weights. corresponding product presynaptic activity kind backpropagated error information standard special cases. obvious important questions seek full partial answers include kinds forms corresponding tradeoﬀs among forms instance terms computational complexity information transmission? upper derivatives necessary why? output layer processed linear operations i.e. additions multiplications constants standard backpropagation algorithm many possible ones. interested case matrices random. however even within restricted setting several possibilities depending instance whether information progressively propagated layers broadcasted directly deep layers; whether multiplication derivatives forward activations included not; properties matrices learning channel leads several algorithms. following notations srbp skipped random backpropagation backpropagated signal arriving onto layer given random matrix directly connecting output layer layer layer learning channel initialized randomly progressively adapted learning using product corresponding forward backward signals denotes randomly backpropagated error. case forward channel becomes learning channel backward weights. forward weights essential lead improvements speed stability. thus word congruent weights describe case. note ﬁxed random matrices learning channel initialized congruently congruence lost learning sign forward weight changes. srbp introduced information theoretic reasons– happens error information communicated directly?–and facilitate mathematical analyses since avoids backpropagation process. however next sections also show empirically srbp viable learning algorithm practice work even better rbp. importantly simulation results suggest learning synaptic weight information upper derivatives needed. however immediate derivative needed. note suggests another possible algorithm skipped backropagation case training example epoch matrix used feedback channel product corresponding transposed forward matrices ignoring multiplication derivative forward transfer functions layers layer consideration. multiplication derivative forward transfer functions applied layer consideration. another possibility combination srbp learning channel implemented combination long-ranged connections carrying srbp signals short-range connections carrying backpropagation procedure long-range signals available. relevant biology since combinations long-ranged short-ranged feedback connections common biological neural systems. size equivalent srbp. however layers size layer sizes introduce rank constraints information backpropagated diﬀer information propagated srbp. linear non-linear cases network depth equivalent srbp since random matrix. additional variations obtained using dropout multiple sets random matrices learning channel instance averaging purposes. another variation skipped case cascading i.e. allowing backward matrices learning channel pairs layers. note notion cascading increases number weights computations still interesting exploratory robustness point view. number computations required send error information learning channel fundamental quantity which however depends computational model used cost associated various operations. obviously everything else equal computational cost connected total number weights. general primary cost multiplication synaptic weight corresponding signal backward pass. thus easy bulk operations required compute backpropagated signals scale like with note whether biases added separately equivalently implemented adding unit clamped layer change scaling. likewise adding costs associated sums computed neuron multiplications derivatives activation functions change scaling long operations costs within constant multiplicative factor cost multiplications signals synaptic weights. sense computational complexity srbp identical layers size signiﬁcantly diﬀerent otherwise especially taking consideration tapering associated architectures used practice. classiﬁcation problem instance random matrices srbp rank scales like total number neurons rather total number forward connections. thus provided leads eﬀective learning srbp could lead computational savings digital computer. however physical neural system spite savings scaling complexity srbp could same. physical neural system backpropagated signal reached neuron layer still communicated synapse. physical model would specify cost communication. assuming unit cost srbp would require operations across entire architecture. finally full analysis physical system would take account also costs associated wiring possibly diﬀerential costs long short wires instance srbp requires longer wires standard rbp. section simulate various algorithms using standard benchmark datasets. primary focus achieving state-of-the-art results rather better understanding algorithms break down. results summarized table end. several learning algorithms ﬁrst compared mnist classiﬁcation task. neural network architecture consisted inputs four fully-connected hidden layers tanh units followed softmax output units. weights initialized sampling scaled normal distribution training performed epochs using mini-batches size initial learning rate decaying factor update momentum. figure performance algorithm shown training test results adaptive versions random propagation algorithms shown figure results sparse versions shown figure main conclusion general concept robust works almost well performance unaﬀected degrades gracefully random backwards weights initialized diﬀerent distributions even change training. skipped versions algorithms seem work slightly better non-skipped versions. finally used diﬀerent neuron activation functions though multiplying derivative activations seem play important role. figure mnist training test accuracy function epoch nine diﬀerent learning algorithms backpropagation skip random skip random version algorithm error signal multiplied derivative post-synaptic transfer function case layer trained lower layer weights ﬁxed note algorithms diﬀer backpropagate error signals lower layers; layer always updated according typical gradient descent rule. models trained times diﬀerent weight initializations; trajectory mean shown here. figure mnist training test accuracy function training epoch adaptive versions algorithm srbp algorithm simulations adaption slightly improves performance srbp speeds training. arbp algorithm learning rate reduced factor experiments keep weights growing quickly. models trained times diﬀerent weight initializations; trajectory mean shown here. figure mnist training test accuracy function training epoch sparse versions srbp algorithms. experiments diﬀerent levels sparsity controlling expected number non-zero connections sent neuron layer connected backward learning channel. random backpropagation matrix connecting layers created sampling entry using bernoulli distribution element prob softmax outputs sends non-zero connection average neuron hidden layers. compare versions srbp elements matrices initialized standard normal distribution scaled forward weight matrices models trained times diﬀerent weight initializations; trajectory mean shown here. figure mnist post-training accuracy sparse versions srbp algorithm. extreme values sparse srbp fails backward weights zero error signals sent; backward weights neurons given layer receive error signal. performance algorithm surprisingly robust extremes. sparse backward elements matrices learning channel srbp sampled uniform normal distribution non-zero mean performance unchanged. also consistent sparsity experiments above means sampling distributions zero. test validity results performed similar simulations convolutional architecture cifar- dataset speciﬁc architecture based previous work consisted sets convolution max-pooling layers followed densely-connected layer tanh units softmax output layer. input consists -by- pixel -channel images; convolution layer consists tanh channels kernel shape strides; max-pooling layers receptive ﬁelds strides. weights initialized sampling scaled normal distribution updated using stochastic gradient descent mini-batches size momentum learning rate started decreased factor update. training training images randomly translated either direction horizontally vertically ﬂipped horizontally probability examples results obtained convolutional architectures shown figures overall similar obtained mnist dataset. figure cifar- training test accuracy function training epoch nine diﬀerent learning algorithms backpropagation skip random skip random version algorithm error signal multiplied derivative post-synaptic transfer function case layer trained lower layer weights ﬁxed models trained times diﬀerent weight initializations; trajectory mean shown here. figure cifar- training test accuracy sparse versions srbp algorithms. experiments different levels sparsity controlling expected number non-zero connections sent neuron layer connected backward learning channel. random backpropagation matrix connecting layers created sampling entry using bernoulli disotherwise. compare versions srbp elements matrices initialized standard normal distribution scaled forward weight matrices models trained times diﬀerent weight initializations; trajectory mean shown here. mnist baseline no-f adaptive sparse- sparse- sparse- quantized error -bit quantized error -bit quantized error -bit quantized update -bit quantized update -bit quantized update -bit dropout dropout dropout table summary experimental results showing ﬁnal test accuracy algorithms epochs training mnist cifar-. experiments section training repeated times diﬀerent weight initializations; cases mean provided sample standard deviation parentheses. also included quantization results section experiments applying dropout learning channel section distinct work uses quantization reduce computation memory costs. quantization applied forward activations weights; quantization applied backpropagated signal received hidden neuron given quantization occurs error signal backpropagated previous layers quantization errors accumulate. experiments used ﬁxed scale parameter varied width bits. figure shows performance degrades gracefully precision error signal decreases small values; larger values e.g. bits performance indistinguishable unquantized updates -bit ﬂoats. figure mnist training test accuracy function training epoch sparse versions srbp algorithms. experiments diﬀerent levels quantization error signal controlling bitwidth bits according formula given text idea using low-precision weight updates liao recently explored low-precision updates rbp. following experiment investigate robustness srbp low-precision weight updates controlling degree quantization. equation used quantization scale factor reduced since weight updates need small. quantization applied error signals backpropagated hidden layers summing minibatch; previous experiments minibatch updates size non-decaying learning rate momentum term main conclusion even lowprecision updates weights used train mnist classiﬁer accuracy low-precision weight updates appear degrade performance srbp roughly way. figure mnist training test accuracy function training epoch sparse versions srbp algorithms. experiment carried diﬀerent levels quantization weight updates controlling bitwidth bits according formula given text quantization applied example-speciﬁc update summing updates within minibatch. section provide number simple observations provide intuition previous simulation results variations work. observations focused srbp general easier study standard rbp. fact rbps algorithms l-layer parameters follows gradient trained like since random feedback weights used learning layer. words bp=rbp=srbp layer. coherent weights denote weights bottom layer weights layer weights learning channel. then variants weights updates direction gradient. obvious layer ﬁrst layer weights changes given ηciij similar change produced gradient descent ηbiij since assumed coherent. dynamics lower layer exactly gradient direction always orthant gradient thus downhill respect error function. additional examples showing positive necessary eﬀect coherence given section fact srbp seems perform well showing upper derivatives needed. however derivative corresponding layer seem matter. general activation functions considered here derivatives tend thus learning attenuated neurons saturated. ingredient seems matter synapses neurons saturated change synapses neurons saturated fact consider multi-class classiﬁcation problem mnist. elements class tend receive backpropagated signal tend move unison. instance consider beginning learning small random weights forward network. images tend produce less uniform output vector similar thus images class tend produce previous error vector forth. words classes associated roughly orthogonal error vectors. vectors multiplied ﬁxed random matrix srbp tend produce approximately orthogonal vectors corresponding hidden layer. thus backpropagated error signals tend similar within digit class orthogonal across diﬀerent digit classes. beginning learning expect roughly half direction thus conclusion intuitive picture work that random weights introduce ﬁxed coupling learning dynamics forward weights layer weights always follows gradient descent stirs learning dynamic right direction; learning dynamic tends cluster inputs associated response move away similar clusters. next discuss possible connection dropout. first observe equations viewed form dropout averaging equations sense that ﬁxed example compute ensemble average activity units learning channel. ensemble average taken possible backpropagation networks unit dropped stochastically unit layer dropped note view kinds noise choice dropout probabilities vary example; actual dropout procedure. consider adding third type noise symmetric weights backward pass form assume distribution noise could gaussian instance essential. important point noise weight independent noise weights well independent dropout noise units. assumptions shown expected value activity unit backward pass exactly given standard equations equal unit layer words standard backpropagation viewed computing exact average backpropagation processes implemented stochastic realizations backward network three forms noise described above. thus reverse argument consider approximates average averaging ﬁrst kinds noise third where instead averaging random realization weights selected ﬁxed epochs. connection suggests intermediate variants several samples weights used rather single one. finally possible dropout backward pass. forward pass robust dropping neurons fact dropout procedure beneﬁcial apply dropout procedure neurons learning channel backward pass. results simulations reported figure conﬁrm srbp robust respect dropout. general strategy derive precise mathematical results proceed simple architectures complex architectures linear case non-linear case. linear case amenable analysis case srbp equivalent hidden layer layers size. thus study convergence optimal solutions linear architectures increasing kind linear network standard assumptions derive non-linear–in fact polynomial–autonomous ordinary diﬀerential equations average time evolution synaptic weights srbp algorithm. soon variable system non-linear general theory understand corresponding behavior. fact even dimensions problem understanding upper bound number relative position limit cycles system form dx/dt dy/dt polynomials degree open–in fact hilbert’s problem ﬁeld dynamical systems considering speciﬁc systems arising rbp/srbp learning equations must ﬁrst prove systems long-term solution. note polynomial odes long-term solutions must thus result activity input hidden output neuron always therefore weights remain constant equal initial values error also remain constant equal thus assume lower weight never changes remains equal initial value. initial value satisﬁes activity hidden output unit remains equal times thus remains constant equal initial value error remains constant equal always error simple furthermore easy check changing sign corresponds reﬂection a-axis. likewise changing sign corresponds reﬂection origin thus short suﬃcient focus case where case critical points given dp/dt everywhere therefore gradient vector directed towards hyperbola critical points. started region grow monotonically critical point reached error decrease monotonically towards global minimum. da/dt da/dt dp/dt everywhere vector directed towards hyperbola critical points. started region decrease monotonically critical point reached error decrease monotonically towards global minimum. similar situation observed fourth quadrant given starting point system follow trajectory given parabola equation converges critical point da/dt da/dt speciﬁc critical point converges equations must satisﬁed simultaneously leads depressed cubic equation solved using standard formula roots cubic equations. note parabolic trajectories contained upper half plane intersect critical hyperbola point therefore equation single real root. lower half plane parabolas associated trajectories intersect hyperbolas distinct points corresponding real root real roots real roots. multiple roots convergence point trajectory easily identiﬁed looking derivative vector note ﬁgure points critical hyperbolas stable attractors except lower half-plane satisfy shown linearizing system around critical points. thus converges zero attractor. particular always case small sign diverges corresponds unstable critical points described above. constant. finally note many cases instance trajectories upper half plane value along trajectories increases decreases monotonically towards global optimum value. however always case trajectories dp/dt changes sign happen once. adding depth linear chain derivation system case linear architecture notational simplicity denote forward weights random weights learning channel case aaai learning equations figure vector ﬁeld linear case correspond horizontal axis correspond vertical axis. critical points correspond hyperbolas critical points ﬁxed points global minima error functions. arrows colored according value dp/dt showing critical points unstable. critical points attractors. reversing sign leads reﬂection across a-axis; reversing sign leads reﬂection across axes. global minima error function. along trajectory quadratic function starting point ﬁnal ﬁxed point calculated solving polynomial equation degree seven. proof remains constant thus back linear case architecture inputs replaced likewise remains constant problem reduced case proper adjustments. thus rest section assume depend weights learning channel. critical points correspond global minima error function. critical points also critical points product additional critical points provided hypersurface short da/dt polynomial degree expanding simplifying equation easy leading term negative given therefore using theorem initial conditions converges ﬁnite ﬁxed point. since quadratic function also converges ﬁnite ﬁxed point similarly thus general case system always converges global minimum error function satisfying α−βp hypersurface depends provides additional critical points product shown linearization hypersurface separates stable unstable ﬁxed points. linear chain architecture arbitrary length case denote forward weights denote feedback weights. using derivation previous cases letting gives system theorem except trivial cases starting initial conditions system equation converges ﬁxed point corresponding global minimum quadratic error function. ﬁxed points located points independent weights learning channel correspond global minima error function. additional critical points critical points dependent weights learning channel. small congruent respective feedforward weights dp/dt sign furthermore equation seen leading coeﬃcient negative therefore using theorem initial conditions system must converge ﬁnite ﬁxed point. given initial condition point convergence solved looking nearby roots polynomial degree gradient descent equations comparison gradient descent equations adding width derivation system consider linear architecture notational simplicity weights lower layer weights upper layer random weights learning channel. case theorem except trivial cases starting initial conditions system equation converges ﬁxed point corresponding global minimum quadratic error function. ﬁxed points located hyersurface given global minima error function. along trajectory quadratic polynomial function aﬃne function aj.for starting point ﬁnal ﬁxed point calculated solving polynomial diﬀerential equation degree thus dynamics variables completely determines dynamics variables needs understand behavior variables. addition vertical coupling horizontal coupling variables given equation resulting theorem case autoencoder uncorrelated normalized data system converges ﬁxed point satisfying positive root particular cubic equation. ﬁxed point shows converges converges ﬁxed point error function converges convex function performs gradient descent convex function thus must also approach ﬁxed point. general three-layer linear case derivation system matrix weights lower layer matrix weights upper layer random matrix weights learning channel. case learning equations given note two-layer linear case corresponds classical least square method well understood. general theory threelayer linear case however well understood. section take signiﬁcant step towards providing complete treatment case. main results system deﬁned equation long-term existence convergent thus short system able learn. however alone imply matrix valued functions individually convergent. prove latter special cases like studied previous sections well positive constants since long-term existence note possible increasing would thus would bounded always increasing local maximum point √c/√c implies √c/√c monotonically decreasing. since bounded theorem nonnegative expression convergent. particular also bounded along ﬂow. integrable. thus fact pointwise convergence caa. since full rank call partial convergence. full rank convergent theorem provides evidence general algebraic deﬁned equation might discrete. although moment able prove discreteness general case question separate interest mathematics system deﬁned equation over-determined system algebraic equations. example matrices non-singular system contains equations unknowns. deﬁne koszul complex associated equations using complex given speciﬁc matrices constructive algorithmic determine whether discrete. corresponding system convergent. expected case non-linear networks challenging analyze mathematically. linear case transfer functions identity thus derivatives transfer functions equal thus play role. simulations reported provide evidence non-linear case derivatives activation functions play role srbp. study simple non-linear case provides evidence. consider simple architecture single power function linearity power hidden layer ﬁnal output neuron linear thus overall inputoutput relationship setting instance provides s-shaped transfer function hidden layer setting corresponds linear case analyzed previous section. weights forward network learning channel. excluding usual trivial cases constant depending coupling shows da/dt da/dt therefore general limit cycles possible. critical points given equation depend weight learning channel. thus nontrivial cases hyperbolic function easy least cases system converges ﬁxed point. instance small positive da/dt da/dt derivatives monotonically increas decreases monotonically convergence critical point.thus general system including derivatives forward activations simpler better behaved. fact general theorem. integer polynomial degree leading coeﬃcient negative therefore using theorem system convergent. integer positive roots function proof proceeds similarly proof theorem diﬀerential equation convergent roots however since careful analysis shows converge zero. thus must converge positive root equation training deep architectures backpropagation digital computers useful practical applications become easier ever part creation software packages automatic diﬀerentiation capabilities. convenience however misleading hampers thinking constraints learning physical neural systems merely mimicked digital computers. thinking learning physical systems useful many ways leads notion local learning rules turn identiﬁes fundamental problems facing backpropagation physical systems. first backpropagation local thus learning channel required communicate error information output layer deep weights. second backpropagation requires symmetric weights signiﬁcant challenge physical systems cannot forward channel reverse direction thus requiring diﬀerent pathway communicate errors deep weights. mode communicating information learning channel completely bypasses need symmetric weights using ﬁxed random weights instead. however possibility among many ones harnessing randomness learning channel. derived several variants studied simulations mathematical analyses. additional variants studied followup paper considers additional symmetry issues learning channel architecture symmetric version forward architecture non-linear units learning channel similar non-linear units forward architecture. many variants seem practical role digital simulations often lead slower learning useful future better understand biological neural systems implement neural physical systems silicon substrates. table postsynaptic information required deep synapses optimal learning. represents signal carried deep learning channel postsynaptic term learning rules considered here. diﬀerent algorithms reveal essential ingredients signal simpliﬁed. last function implemented sparse adaptive matrices carry precision signals include non-linear transformations learning channel supervised learning critical equations show principle deep weights must depend training examples weights network. backpropagation shows possible derive eﬀective learning rules form role lower part network subsumed presynaptic activity term signal communicated deep learning channel carries information outputs targets deep synapses. studied kind information must carried signal much simpliﬁed main conclusion composition linear propagation non-linear activation functions ﬁxed slowly varying matrices involved random sparse etc. expected better matrices full rank although gracious degradation opposed catastrophic failure observed matrices deviate slightly full rank case. robustness properties algorithms explanations general principles. provided intuitive formal explanations several properties. mathematical side polynomial learning rules linear networks lead systems polynomial diﬀerential equations. shown several cases corresponding odes converge optimal solution. however polynomial systems odes rapidly become complex results provided useful complete thus providing directions future research. next consider remaining case invertible proportional identity. case take third order derivatives reach conclusion. taking derivatives ﬁrst relations equation matrix s−ξξt trace determinant zero. eigenvalues zero. hand since skew-symmetric matrices proportional identity. result matrix hence identical eigenvalues. eigenvalue then", "year": 2016}