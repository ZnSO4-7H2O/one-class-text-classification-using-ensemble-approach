{"title": "Fine-grained Event Learning of Human-Object Interaction with LSTM-CRF", "tag": ["cs.CV", "cs.AI"], "abstract": "Event learning is one of the most important problems in AI. However, notwithstanding significant research efforts, it is still a very complex task, especially when the events involve the interaction of humans or agents with other objects, as it requires modeling human kinematics and object movements. This study proposes a methodology for learning complex human-object interaction (HOI) events, involving the recording, annotation and classification of event interactions. For annotation, we allow multiple interpretations of a motion capture by slicing over its temporal span, for classification, we use Long-Short Term Memory (LSTM) sequential models with Conditional Randon Field (CRF) for constraints of outputs. Using a setup involving captures of human-object interaction as three dimensional inputs, we argue that this approach could be used for event types involving complex spatio-temporal dynamics.", "text": "abstract. event learning important problems however notwithstanding signiﬁcant research eﬀorts still complex task especially events involve interaction humans agents objects requires modeling human kinematics object movements. study proposes methodology learning complex human-object interaction events involving recording annotation classiﬁcation event interactions. annotation allow multiple interpretations motion capture slicing temporal span; classiﬁcation long-short term memory sequential models conditional randon field constraints outputs. using setup involving captures human-object interaction three dimensional inputs argue approach could used event types involving complex spatio-temporal dynamics. study events long involved many disciplines including philosophy cognitive psychology linguistics computer science gestalt school philosophy characterized events whole processes emerge relations components. cognitive psychologists tulving recognized importance events postulating separate cognitive process called episodic memory. representation events natural language studied many diﬀerent approaches formal logic frames computational linguistics combining perspectives computer science logic linguistics recent work suggests events eﬀectively modeled programs within dynamic logic enabling computer simulations linguistic expressions computer science little consensus events modeled learning. represented atomically i.e. entire events predicted classiﬁcation manner combinations primitive actions i.e. complex event types learned based recognition combined primitive actions. former type event representation quantitative approaches based low-level pixel features qualitative approaches induction relational states among event participants latter approach systems state transition graphical models dynamic bayesian networks ∗this work supported contract defense advanced research projects agency contract wnf--c-. approved public release distribution unlimited. views expressed authors reﬂect oﬃcial policy position department defense u.s. government. would like thank nikhil krishnaswamy keigh discussion input topic. errors mistakes course responsibilities authors.. learning events whole works best human motion signatures running sitting etc. poses problem event types require distinctions spatio-temporal relationships objects. pointed also diﬃcult model events strict orderings subevents especially overlapping relations them. moreover purpose event learning facilitate communication interaction human computational agents robots achieve common goals agents need keep track multiple events time involving themselves humans well surrounding environment. practical point view calls ﬁner-grained treatment event modeling. also case ﬁne-grained analysis events strongly supported theoretical point view. example long known event classiﬁcation needs take account called extra-verbal factors. event types semantically deﬁned base verbal expression running walking need incorporate components expression compositionally objects adjuncts change event type overall verb phrase sentence motivated arguments suggest diﬀerent approach event learning. instead treating events whole programs subevents allow multiple interpretations motion capture slicing temporal span give separate annotation slice. particular event capture annotation tool called ecat employs microsoft kinectr capture sessions performers interacting types objects cube cylinder objects tracked using markers ﬁxed sides facing camera. projected three dimensional space using depth ﬁeld. performers tracked using kinect provides three dimensional inputs performer’s joint points sessions ﬁrst sliced slice annotated textual description using event language. sequence learning algorithms input sequences feature vectors output representation event. main contributions study twofolds. firstly created framework event recording annotation takes account temporal dynamics i.e. diﬀerent interpretations events diﬀerent temporal spans. applying ﬂavor popular sequential learning method lstm accommodates output constraints achieved good performance human-object interaction setup. used three dimensional coordinates bodies tracked kinectrsdk model human kinematics. joint points upper body performers kept concatenated lower parts occluded interact objects table. addition marker detected generate features features objects concatenated vector ﬁxed size sample dataset consists sequence ﬁxed length feature vectors. label mapped textual annotation lstm ﬂavor deep recursive neural network generally solved problem vanishing gradients traditional learning found application wide range problems involving sequential learning hand written recognition speech recognition gesture recognition etc. describe detail lstm implementation provide online access code approach brieﬂy however model passes feature vector linear layer feeding sequence lstm. label requires separate lstm cell depending whether predict label independently combine ﬁnal prediction predict basis outputs last layer variants considered correspondingly lstm-i lstm-w. outputs that object allowed diﬀerent syntactic slots; verb slots none; locative preposition dependent locative none preposition must also none vice versa. edges nodes left side figure show dependencies output labels wish model. however training classifying using full model would diﬃcult especially implemented neural network architecture. modiﬁed model tree-crf structure make model learnable using dynamic programming. complexity algorithm reduced size vocabulary. learning problem thereby changed learning weights along edges tree-crf example locative preposition directionality edges forward direction message passing algorithm used learning lstm-crf natural extension lstm applied constrained outputs. instance used named entities recognition task model constraints labels. learning lstm-w modify term produced outputs lstm followings. training tmax calculated predicted label combination namely below. calculate using message passing tree nodes tree. cross entropy predicted distribution correct output cost training. demonstrate model’s capability learn spatio-temporal dynamics object interactions events collection four action types push pull slide roll along three diﬀerent spatial prepositions used space conﬁgurations objects namely toward away past afterwards session sliced events short segments frames. annotators assigned watch annotate speed annotation event types related original captured types shown selection. instance event type captured session performer pushes toward available event types performer pushes slides toward none. lstm models hidden layer features. methods used combat over-ﬁtting dropout lstm cell probability gradient clipping global norm. network trained mini-batch gradient descent optimization epochs tensorﬂow library. frequent label tagging used baseline study simply predict sample frequent tuple seen training corpus. captured sessions split training testing sets proportion i.e. sessions training sessions testing event type. gives total training samples testing samples. precisions reported averaged runs breakdowns precision label show verb precision lowest high confusion following pairs push roll pull roll slide roll. likely poor tracking result objects rolled. fact capture tool could recognize objects many frames objects roll fast compensated using interpolation. improvement tracking objects however target study. observed signiﬁcant improvement learning using lstm baseline especially coupled crf. moreover also observed reduction invalid outputs used. consider results quite good particularly since sequential learning model used simple fast employ feature engineering method. paper demonstrated methodology provides reasonable learning results complex event types. hope study provide starting point investigations ﬁne-grained event learning. currently learning method requires number objects inputs could overcome incrementally adding object features size feature vector possibly using recursive neural network. regarding annotation framework natural extension spans diﬀerent lengths could annotated appropriate re-sampling methods. leave future research topics.", "year": 2017}