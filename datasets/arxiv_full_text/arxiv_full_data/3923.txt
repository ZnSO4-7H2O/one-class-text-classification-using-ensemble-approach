{"title": "Multiobjective hBOA, Clustering, and Scalability", "tag": ["cs.NE", "cs.AI", "I.2.8; I.2.6; G.1.6; I.5.3"], "abstract": "This paper describes a scalable algorithm for solving multiobjective decomposable problems by combining the hierarchical Bayesian optimization algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and clustering in the objective space. It is first argued that for good scalability, clustering or some other form of niching in the objective space is necessary and the size of each niche should be approximately equal. Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and clustering in the objective space. The algorithm mohBOA differs from the multiobjective variants of BOA and hBOA proposed in the past by including clustering in the objective space and allocating an approximately equally sized portion of the population to each cluster. The algorithm mohBOA is shown to scale up well on a number of problems on which standard multiobjective evolutionary algorithms perform poorly.", "text": "paper describes scalable algorithm solving multiobjective decomposable problems combining hierarchical bayesian optimization algorithm nondominated sorting genetic algorithm clustering objective space. ﬁrst argued good scalability clustering form niching objective space necessary size niche approximately equal. multiobjective hboa described combines hboa nsga-ii clustering objective space. algorithm mohboa diﬀers multiobjective variants hboa proposed past including clustering objective space allocating approximately equally sized portion population cluster. algorithm mohboa shown scale well number problems standard multiobjective evolutionary algorithms perform poorly. pareto-optimal front exploited select solutions appropriate particular application without weigh objectives advance reduce multiple objectives way. number multiobjective evolutionary algorithms proposed past including nondominated sorting genetic algorithm improved strength pareto evolutionary algorithm pareto-archived evolution strategy multiobjective genetic algorithm niched-pareto genetic algorithm multiobjective bayesian optimization algorithm however studies multiobjective evolutionary algorithms focused either whether algorithm could discover wide dense pareto-optimal front practical applications overlook algorithm scalability means time complexity multiobjective evolutionary algorithm grows problem size. recent work shown although proposed multiobjective evolutionary algorithms provide high-quality paretooptimal front isolated problem instances relatively small size time complexity algorithms often grow prohibitively fast algorithms thus scale well. purpose paper present scalable multiobjective evolutionary algorithm solve decomposable multiobjective problems low-order polynomial time. algorithm consists three main ingredients model-building model-sampling replacement procedures hboa nondominated sorting crowding-distance assignment nsga-ii clustering objective space paper starts overview prior work multiobjective estimation distribution algorithms introduction basic concepts used mohboa. section describes mohboa. section describes experiments presents experimental results. finally section summarizes concludes paper. section starts summarizing prior work multiobjective estimation distribution algorithms section discusses basic components multiobjective hierarchical bayesian optimization algorithm hierarchical bayesian optimization algorithm nondominated sorting genetic algorithm k-means algorithm clustering objective space estimation distribution algorithms also called probabilistic model-building genetic algorithms iterated density estimation algorithms replace standard variation operators crossover mutation building probabilistic model selected candidate solutions sampling built model generate solutions. several multiobjective edas proposed past diﬀerent variants edas mixtures univariate tree models multiobjective hboa combining hboa selection replacement mechanisms nsga-ii tests challenging decomposable multiobjective problems indicated without identifying exploiting interactions diﬀerent string positions decomposable problems become intractable using standard variation operators hand mboa mhboa could solve decomposable hierarchical problems relatively eﬃciently. laumanns ocenasek combined mixed selection replacement mechanisms spea since mixed applied problems discrete continuous variables resulting algorithm also applied vectors types variables. algorithm tested knapsack shown dominate nsga-ii spea spea instances. combined real-coded selection procedure nsga-ii sharing intensity measure modiﬁed nsga-ii crowding mechanism. incorporating learning sampling multivariate probabilistic models important step toward competent multiobjective solvers allowed standard multiobjective genetic algorithms nsga-ii spea solve problems necessitate form linkage learning. however primary purpose presented tests examine good coverage pareto-optimal front overlooked scalability. later indicated proposed multiobjective edas scale well decomposable problems without form clustering discussed paper neither standard multiobjective nsga-ii. thierens bosman combined simple edas univariate tree models nondominated tournament selection clustering. clustering used split population subpopulations separate model built subpopulation. clustering another important step toward scalable edas multiobjective evolutionary algorithms. number samples generated model cluster encourage sampling extreme solutions methods described thierens bosman clustering similar paper. however clustering explicitly ensure part pareto-optimal front suﬃciently many candidates population. following section describes hierarchical boa. next multiobjective optimization dishierarchical bayesian optimization algorithm evolves population candidate solutions given problem. ﬁrst population usually generated random. population updated number iterations using basic operators selection variation. selection operator selects better solutions expense worse ones current population yielding population promising candidates. variation operator starts learning probabilistic model selected solutions encodes features promising solutions inherent regularities. hboa uses bayesian networks local structures model promising solutions. variation operator proceeds sampling probabilistic model generate candidate solutions. solutions incorporated original population using restricted tournament replacement ensures useful diversity population maintained long periods time. detailed description hboa found pelikan pelikan multiobjective optimization task solution solutions optimal respect multiple objectives. example want optimize design engine maximize performance well minimize fuel consumption. basic approaches solving multiobjective optimization problems weigh objectives yielding single-objective problem objective consists weighted objectives pareto-optimal front deﬁned solutions improved respect objective expense quality respect least objective; example performance engine design pareto-optimal front could improved expense fuel consumption around pareto optimality easily explained using concept dominance. candidate solution dominates candidate solution better respect least objective worse respect objectives. example engine dominates engine better respect performance well fuel consumption. pareto-optimal front subset candidate solutions dominated candidate solution. primary advantage ﬁnding pareto-optimal front opposed ﬁnding optimum single-objective problem created weighing objectives sometimes diﬃcult impossible weigh objectives appropriately satisfactory solutions. furthermore ﬁnding pareto-optimal front reveals relationship among objectives used decide solutions front best particular problem instance. paper focus discovery pareto-optimal front application hboa advanced evolutionary algorithms single-objective problem straightforward. task solutions across entire pareto-optimal front cover front well possible. nondominated sorting genetic algorithm modiﬁes selection replacement standard genetic algorithms enable discovery wide-spread dense pareto-optimal front. selection operator nsga-ii starts partially sorting population using dominance. first rank assigned subset current population consists solutions dominated solution population. next solutions dominated remaining unranked solutions selected given rank process ranking solutions continues always considering solutions dominated remaining solutions assigning increasing ranks solutions. manner solutions dominated least solutions given lower ranks solutions dominated solutions. respect pareto optimality solutions lower ranks given priority. addition ranking candidate solution assigned crowding distance estimates dense current pareto-optimal front vicinity solution. higher crowding distance solution isolated solution. crowding distance computed rank separately. candidate solutions ﬁrst sorted according objective crowding distance solution computed considering distance nearest neighbors ordering figure pseudocode crowding distance assignment algorithm. compare quality solutions ranks compared ﬁrst. ranks solutions diﬀer solution lower rank better. ranks solutions equal solution greater crowding distance wins. ranks well crowding distances equal winner determined randomly. pseudocode comparison solutions shown figure comparison procedure used standard selection operator tournament truncation selection. original nsga-ii well mohboa tournament selection used. selected population solutions undergoes mutation crossover combined original population form population candidate solutions. nsga-ii uses elitist replacement mechanism combine parent population oﬀspring population form population replacement operator nsga-ii starts merging populations population. ranks crowding distances computed solutions merged population nondominated crowding comparison operator given points k-means clustering splits clusters subsets approximately variance. algorithm proceeds updating cluster centers center deﬁnes cluster. cluster centers initialized randomly advanced algorithms also used initialize centers. iteration consists steps. ﬁrst step point attached closest center second step cluster centers recomputed center center mass points attached algorithm terminates points remain cluster recomputing cluster centers reassigning points newly computed centers. points attached cluster center deﬁne cluster. numbers points diﬀerent clusters diﬀer signiﬁcantly points distributed uniformly clusters even become empty. sometimes necessary rerun k-means several times result best run. decomposable multiobjective problems objectives compete number problem partitions using traditional selection replacement mechanisms necessitates exponentially scaled populations discover entire pareto-optimal front reason behavior niches extremes pareto-optimal front expected exponentially smaller niches middle alleviate problem necessary process diﬀerent parts pareto-optimal front separately allocate suﬃciently large portion population part pareto-optimal front. important note algorithms nsga-ii spea also include mechanisms attempt deal good coverage wide pareto-optimal front. however mechanisms insuﬃcient decomposable multiobjective problems result creating exponentially large niches middle pareto-optimal front eliminating extremes. leads poor scalability supported experimental results shown section thus consists m-dimensional vectors real numbers number objectives. reduce number iterations creation reasonable clusters cluster centers initialized ordering points according objective assigning center i)th point ordering. forcing cluster produce equal number candidate solutions regular coverage paretooptimal front ensured even diﬃcult decomposable multiobjective problems. pseudocode mohboa shown figure like hboa mohboa generates initial population candidate solutions random. population ﬁrst evaluated. similarly evolutionary algorithms iteration starts selection. however instead using standard selection methods mohboa ﬁrst uses nondominated crowding nsga-ii rank candidate solutions assign crowding distances. ranks crowding distances serve basis applying standard selection operators. example binary tournament selection used winner tournament determined ranks crowding distances obtained nondominated crowding. selecting population promising solutions k-means clustering applied population obtain speciﬁed number clusters. usually clusters remain empty thus considered recombination phase. separate probabilistic model built cluster used generate part oﬀspring population. encourage equal coverage entire pareto-optimal front model cluster used generate number candidate solutions. incorporates oﬀspring population original population solution time. oﬀspring solution random subset candidate solutions original population ﬁrst selected. solution closest selected subset. measure distance solutions counting number bits solutions diﬀer might advantageous consider distance metric deﬁned objective space similarly k-means clustering; however using objective function values compute distance seem improve results signiﬁcantly shortly veriﬁed experiments. replaces selected solution better according nsga-ii comparison procedure two-objective test problems used onemax zeromax trap- inverse trap- problem. functions assume binary-string representation. section describes problems discusses diﬃculty. second test problem consists objectives trap- inverse trap-. string positions ﬁrst divided disjoint subsets partitions bits each. partitioning ﬁxed entire optimization algorithm given information partitioning advance. bits partition contribute trap- using fully deceptive -bit trap function deﬁned trap- deceives algorithm away optimum interactions bits partition considered standard crossover operators—such uniform one-point two-point crossover—fail solve trap- unless bits partition located close chosen representation; fact standard crossover operators require exponentially scaled population sizes solve trap- mutation operators require evaluations solve trap- therefore also highly ineﬃcient solving trap-. task maximize function thus optimum inverse trap- located string zeros. figure visualize trap- block bits. inverse trap- also deceives algorithm away optimum interactions bits partition considered. test problems algorithms diﬀerent problem sizes examined study scalability. problem type problem size algorithm bisection used determine minimum population size representative solution point pareto-optimal front independent runs. pareto-optimal front n-bit onemax-zeromax consists solutions unique values objectives whereas pareto-optimal front n-bit trap-invtrap consists solutions unique objective function values. reduce noise bisection method times. thus results problem type problem size algorithm correspond successful runs. algorithm performance measured number evaluations pareto-optimal front completely covered. figure results onemax-zeromax indicate k-means clustering objective space leads dramatic improvement performance umda furthermore indicate performs better elitist replacement nsga-ii multiobjective umda capable solving onemax-zeromax low-order polynomial time. number generations umda mohboa recombination upper-bounded problem size whereas runs standard crossover mutation given generations slower convergence. probability crossover whereas probability ﬂipping mutation focus eﬀects diﬀerent recombination replacement strategies number clusters k-means clustering number unique solutions ﬁnal paretooptimal front number clusters cannot approximated advance obtained automatically using example bayesian information criterion figure shows growth number evaluations problem size onemax-zeromax. results indicate clustering objective space necessary scalable solution onemaxzeromax. furthermore results show based nondominated crowding performs better elitist replacement nsga-ii. finally results indicate umda provides low-order polynomial solution onemax-zeromax. figure shows results trap-invtrap. results show expected trap-invtrap necessitates clustering objective space like onemax-zeromax also eﬀective identiﬁcation exploitation interactions diﬀerent problem variables also called linkage learning. standard crossover umda fail solve problem eﬃciently become intractable already relatively small problems. algorithm mohboa clustering objective space provides best performance scales polynomially problem size. again leads better performance elitism. figure results trap-invtrap indicate decomposable problems necessary include clustering well identify exploit interactions interacting string positions decision variables. furthermore show performs better elitist replacement nsga-ii mohboa capable solving trap-invtrap low-order polynomial time. objective space used compute distance solutions however using objective space still capable ensuring scalable performance clustering objective space used indicating insuﬃcient incorporate niching replacement based distribution solutions matter whether niching method based candidate solutions objective values. paper discussed scalable optimization multiobjective decomposable problems objectives compete diﬀerent partitions problem decomposition. multiobjective hierarchical proposed combines hierarchical nondominated crowding nsga-ii clustering objective space. combining powerful genetic evolutionary algorithms nsga-ii clustering scalable multiobjective optimization algorithm decomposable problems created. problems objectives used experiments conclusions drawn apply problems objectives. experimental results indicate clustering objective space necessary scalable optimization decomposable multiobjective problems. restricted tournament replacement based nondominated crowding appears perform better elitist replacement used nsgaii. furthermore solve arbitrary multiobjective decomposable problems linkage learning must considered eﬀectively identify process diﬀerent subproblems. experiments indicate mohboa solve decomposable multiobjective problems low-order polynomial time whereas compared algorithms fail scale well become intractable already relatively small problems. figure results multiobjective umda using distance metric objective space onemax-zeromax indicate clustering objective space cannot replaced variant choice metric signiﬁcantly aﬀect performance. work partially supported research award research board university missouri. experiments done using hboa software developed martin pelikan david goldberg university illinois urbana-champaign. work also sponsored force oﬃce scientiﬁc research force materiel command usaf grant f--- national science foundation grant dmr-- grant dmr- dept. energy grant defg-er u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation thereon.", "year": 2005}