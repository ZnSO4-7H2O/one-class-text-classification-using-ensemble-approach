{"title": "Denoising Autoencoders for Overgeneralization in Neural Networks", "tag": ["cs.AI", "cs.CV", "cs.LG"], "abstract": "Despite the recent developments that allowed neural networks to achieve impressive performance on a variety of applications, these models are intrinsically affected by the problem of overgeneralization, due to their partitioning of the full input space into the fixed set of target classes used during training. Thus it is possible for novel inputs belonging to categories unknown during training or even completely unrecognizable to humans to fool the system into classifying them as one of the known classes, even with a high degree of confidence. Solving this problem may help improve the security of such systems in critical applications, and may further lead to applications in the context of open set recognition and 1-class recognition. This paper presents a novel way to compute a confidence score using denoising autoencoders and shows that such confidence score can correctly identify the regions of the input space close to the training distribution by approximately identifying its local maxima.", "text": "abstract—despite recent developments allowed neural networks achieve impressive performance variety applications models intrinsically affected problem overgeneralization partitioning full input space ﬁxed target classes used training. thus possible novel inputs belonging categories unknown training even completely unrecognizable humans fool system classifying known classes even high degree conﬁdence. solving problem help improve security systems critical applications lead applications context open recognition -class recognition. paper presents novel compute conﬁdence score using denoising autoencoders shows conﬁdence score correctly identify regions input space close training distribution approximately identifying local maxima. index terms—overgeneralization fooling autoencoder open recognition open world recognition -class recognition conﬁdence score neural networks networks achieved impressive performance variety applications. models class however suffer problem overgeneralization whereby whole input space partitioned target classes speciﬁed training generally lack possibility reject novel sample belonging those. main issue overgeneralization context open recognition open world recognition limited number classes encountered training testing performed larger includes potentially large number unknown classes never observed before. example shown figure linear classiﬁer trained discriminate drawings digits digit present training wrongly classiﬁed general instances classes present training fall partitions input space learnt classiﬁer. problem becomes worse real world applications extremely hard know advance possible categories observed. region meaningful samples input space usually small compared whole space. easily grasped randomly sampling large number points input space example images certain resolution observing chance producing fig. linear classiﬁer trained recognize exclusively pictures digits digit never observed training example wrongly classiﬁed digit example overgeneralization. similar problem ‘fooling’ whereby possible generate images unrecognizable humans nonetheless classiﬁed known classes high conﬁdence example noise-looking picture bottom-left corner classiﬁed digit recognizable sample negligible. discriminative models assign high conﬁdence score random images depending learnt partition input space. indeed observed fooling shown possible generate input samples unrecognizable humans classiﬁed speciﬁc target class high conﬁdence fooling particular lead severe problems applications related safety security. suggested problems mitigated solved using generative models rather learning posterior class label directly learn joint distribution computed. modeling distribution data would give model capability identify input samples belonging know classes reject believed belong unknown ones. apart mitigating problem overgeneralization modeling distribution data would also useful applications novelty outlier detection incremental learning broadening range applications model could used necessary. results work suggest identiﬁcation high-density regions close local maxima data distribution sufﬁcient correctly identify samples belong distribution ones rejected. speciﬁcally possible identify classify critical points data distribution exploiting recent work shown denoising contractive autoencoders reconstruction error tends approximate gradient log-density. measure conﬁdence score computed function gradient. similar approach exploited energy-based generative adversarial networks denoising autoencoder used discriminator model trying determine whether inputs belong training distribution not. indeed reconstruction error autoencoder used compute energy function assigns values points belonging distribution interest high values everywhere else provided regularization used prevent learning identity function corresponding constant energy landscape. problem overgeneralization discriminative models like neural networks serious concern context security model interpretability critical aspect recognition problems limited number classes recognized among larger number unknown ones. simplest build recognition system based given classiﬁer threshold predicted output labels computed example using softmax function turns probability distribution known classes rejecting sample value output model thus treated estimate conﬁdence score classiﬁer. approach however limited help shown sensitive problem fooling another mitigate problem done classical object recognition training positive samples complemented negative samples bundle together instances belonging variety ‘other’ classes. approach however completely solve problem usually affected unbalanced training generally larger amount negatives required correct classiﬁcation. potential amount negatives arbitrarily large problem consists gathering sufﬁcient amount data required approximate actual distribution made even worse fact full negative categories fully known training system. example context object recognition vision high-resolution images represent possible image class majority likely known training. negative training instances problem overgeneralization analysed context ‘open recognition’ formally deﬁned scheirer colleagues framework assumed machine learning models trained ‘known’ classes potentially ‘known unknown’ ones testing however performed larger samples include ‘unknown unknown’ classes never seen training. models developed address problem open recognition focus problem ‘unknown unknown’ classes seminal paper gave ﬁrst formal deﬁnition problem proposed extension algorithm called vs-set machine designed learn envelope around training data using parallel hyperplanes inner separating data origin feature space scheirer colleagues proposed weibull-calibrated algorithm address multi-class open recognition another interesting approach recently applied deep neural networks openmax model works modelling class-speciﬁc distribution activation vectors hidden layer neural network using information recognize outliers. related problem open recognition ‘open world recognition’ novel classes ﬁrst detected learnt incrementally seen extension open recognition ‘unknown unknown’ classes discovered time becoming ‘novel unknowns’. classes labelled potentially unsupervised become ’known’. authors proposed nearest non-outlier algorithm address problem. recognition -classrecognition training performed using samples single class without using negative samples. -class algorithm proposed address problem ﬁtting hyperplane separates data points origin feature space maximizing distance origin. algorithm applied novelty outlier detection variants algorithm like support vector data description also used learn envelope around points dataset systems tried estimate boundaries data computing region minimum volume input space containing certain probability mass finally speciﬁc sub-problem overgeneralization fooling paper nguyen colleagues make particularly interesting remark suggesting generative models less affected problem modeling data generating distribution. also argued however present generative models still limited fully solve problem scale well high-dimensional datasets like imagenet competitive overcomplete output layer model recently proposed mitigate problem fooling. cool works replacing ﬁnal output layer neural network special cool layer constructed replacing outputs classiﬁer normalized example using softmax output interpreted introducing implicit ‘reject’ option probability conﬁdence score proposed here however designed probability estimate. experiments presented here classiﬁer constructed fully connected softmax layer attached hidden layer autoencoder symmetric weights order keep number weights similar equivalent feed-forward benchmark model identical except lack decoder. general keeping autoencoder separate classiﬁer connecting complex ways work well using classiﬁer neural network. case autoencoder classiﬁer kept separate autoencoder used infer information data distribution. pairing systems together however might provide advantages outside scope present work like enabling degree semi-supervised learning. autoencoder improved replacing discriminator ebgan potentially learn better model data. model ﬁrst tested classiﬁcation task help visualize capacity learn region input space training class. three target distributions deﬁned uniform rings thickness inner radius centered three points output unit corresponding speciﬁc target class ones output units target class made compete activation means softmax activation forces learn recognize different parts input space overlapping around region support data generating distribution. network compute conﬁdence score product activation units belonging target class high inputs large number units agrees regions data distribution output units active. solution presented based novel measure conﬁdence correct identiﬁcation data points belonging distribution observed training rejection. ideally conﬁdence score would function probability sample belonging data distribution. computing full distribution however necessary. particular problem simpliﬁed identiﬁcation points belonging data manifold points close local maxima data generating distribution. recently shown denoising contractive autoencoders implicitly learn features underlying data distribution speciﬁcally reconstruction error approximates gradient log-density small corruption noise dec) reconstructed input. larger noise however found work best practice. result proven hold type input noise process reconstruction loss long compatible log-likelihood interpretation similar interpretation suggested reconstruction error regularized autoencoders used deﬁne energy surface trained take small values points belonging training distribution higher values everywhere else thus critical points data distribution correspond points small gradient distribution small reconstruction error indeed points network reconstruct well thus hopefully experienced training managed generalize well. conﬁdence score designed takes high values points data manifold points near local minima energy function corresponding small magnitude gradient log-density data distribution small values everywhere else. note however approach cannot distinguish local minima maxima saddle points thus assign high conﬁdence score small points belonging target distribution. problem addressed scaling computed conﬁdence function favours small negative curvature log-density data distribution computed model presented paper next tested benchmark fooling mnist dataset similar proposed however contrary previous work classiﬁcation accuracy models tested reported ‘thresholded classiﬁcation accuracy’ requirement order counted correctly classiﬁed test samples assigned threshold higher used consider fooling instance valid. metric indeed reported alongside fooling rate model otherwise model trivially caps conﬁdence scores network ﬁxed value lower threshold used consider fooling attempts valid would deﬁnition never fooled. model would however never classify valid sample threshold. metric thus proves useful compare different models varying degrees sensitivity overgeneralization. fooling test performed trying fool target network classify input unrecognizable humans target class fooling instances generated using fooling generator network consisting single layer perceptron sigmoid activation equal number input output units mnist). importantly produces samples values bounded without requiring explicit deﬁnition constraint. fooling target digit attempted performing stochastic gradient descent parameters minimize cross-entropy output network fooled speciﬁc desired target output class. fooling digit attempted trials different random inputs trial consisting parameter updates described ﬁrst test compared three models plain convolutional neural network competitive overcomplete output layer network based system described section built models addition decoder taking activation hidden layer input complete denoising autoencoder used compute conﬁdence score denoising autoencoder trained corruption inputs additive gaussian noise. models trained ﬁxed epochs. fooling attempted different thresholds contrast previous work used comparing models different thresholds indeed give information robustness amplify differences thus improving comparison. table reports results three models splitting denoising autoencoder model separate tests using either separate decoder building decoder symmetric transpose encoder table reports thresholded classiﬁcation accuracy models together original unthresholded one. fooling measured proportion trials produced valid fooling samples within maximum fig. system presented trained classify points sampled three uniform ring distributions. sampling data points target distributions. labeling point input space scaled computed conﬁdence score. regions white assigned conﬁdence scores. labeling point input space without re-scaling output classiﬁer computed conﬁdence score. conﬁdence score scaling bottom estimate curvature log-distribution data conﬁdence score product functions. panel product classiﬁer’s output conﬁdence score. black denotes high values. training distributions shown figure training performed minibatches size using adam optimizer total update steps. shown figure model learned correctly identify support region target distributions. contrary uncorrected classiﬁer partitioned whole space three regions incorrectly labeling points conﬁdence score computed model presented helps system prevent overgeneralization limiting decisions classiﬁer points likely belong target distributions. panel evidences different contributions factors used compute conﬁdence score. measure proximity local extrema data generating distribution modulated remove local minima using function local curvature probability distribution important observe however function reduce computed conﬁdence score valid samples. different types applications beneﬁt inclusion exclusion depending whether importance given correct rejection samples belong training distribution correct classiﬁcation expense partial increase overgeneralization. number updates. average number updates required fool network reported parentheses. full parameters used simulations reported appendix model presented outperformed thresholds also retaining high thresholded classiﬁcation accuracy even high thresholds. previous protocol cross-entropy loss used optimize computed using unscaled output network. difference autoencoders using symmetric versus asymmetric decoder found minimal symmetric autoencoder used remaining experiments number parameters three model kept similar results table however found different reported particular fooling rate cool found signiﬁcantly lower reported well average number updates required fool major contributor difference found rectiﬁed linear units experiments reported here compared sigmoid units original study. shown separate simulations three models used sigmoid activations instead relus ﬁxed fooling threshold case thresholded classiﬁcation accuracy models slightly higher matched signiﬁcant increase fooling rate cool model plain variations protocol could account differences found could different paradigm training slightly different network architecture present work used higher number ﬁlters convolutional layer. next effect learning rate used fooling update steps investigated increasing used previous study used train models expecting higher fooling rate. classiﬁcation threshold indeed plain benchmark network found fooled trials updates based model still never fooled. cool hand signiﬁcantly decreased performance fooling rate finally cool models tested trying fool conﬁdence scores directly rather output classiﬁcation scores contrast threshold used. interestingly cool model never fooled model described fooled trials although requiring large number updates also found adding regularization weights model generation large number instances passing fooling test generated samples actually resembled real digits closely thus could considered examples fooling. shows model heavily regularized capable learning tight boundary around high density regions data generating distribution although cost reducing thresholded accuracy generated samples shown supplementary figure example generated fooling samples reported fig. showing instances main results table plain cool experiment fooling conﬁdence scores directly model. open recognition tested building sequence classiﬁcation problems varying degrees openness based mnist dataset problem consisted training target model limited number ‘known‘ training classes testing full test digits requiring model able reject samples hypothesized belong ‘unknown’ classes. degree openness problem computed similarly fig. comparison plain cool model described paper benchmark open recognition. f-measure computed three models problems created mnist dataset using limited number ‘known’ classes training testing full test example training model requiring model able reject samples hypothesized belong ‘unknown’ classes. higher values openness problem reﬂect smaller number classes used training value openness corresponds case full digits used. curves averaged across runs using different sub-sets digits degree openness except openness digits used training performed once. error bars denote standard deviation. unknown classes seen testing classes experienced training. number training classes varied reﬂecting full range degrees openness offered dataset. ﬁxed number training classes used training models trained repetitions different random subsets digits balance easier harder problems depending speciﬁc digits used exception test using full training once. subsets digits used three models. correct classiﬁcation computed correct identiﬁcation class label conﬁdence score classiﬁcation threshold correct rejection measured either assigning classiﬁcation score classifying input sample classes seen training models trained ﬁxed epochs task. figure reports results experiment. like previous published benchmarks open recognition performance models degree openness computed f-measure harmonic mean precision recall scores averaged across repetitions degree openness. limit open recognition single training class observed training problem -classrecognition explored comparing model presented paper cool -class separate -class-recognition problem created mnist dataset digit. problem models trained using samples selected class tested full test comprising digits. negative samples used training. training performed epochs model problem. figure reports results curve averaged across curves computed -class-recognition problems. based model outperformed area curve compared -class cool. conﬁdence score based denoising autoencoders introduced paper found perform better competing models open recognition -class recognition. system also found signiﬁcantly robust problem fooling state cool model. together results show possible fig. curves averaged -class recognition problems digit mnist three models model described paper -class cool area curve model -class-svm cool. noted comparing results cool model used degree overcompleteness original paper. however tuning parameter particular using higher values achieve higher performance benchmarking tasks used here. also similarly original cool paper fooling attempted output classiﬁer rather directly conﬁdence scores. gives advantage systems conﬁdence score computed complex ways directly dependendent output classiﬁer. however tests presented section iv-b showed system presented signiﬁcantly outperforms models even fooling attempted directly conﬁdence scores. particular case found training denoising autoencoder heavy regularization resulted generated samples resembling real digits thus showing model learnt tight boundary around data manifold. energy-based makes reconstruction error denoising autoencoder compatible interpretation proposed here. particular uses approximated energy function learnt autoencoder take values points belonging training distribution high values everywhere else. seen equation shown reconstruction error denoising autoencoders proportional gradient distribution data. thus small absolute values reconstruction error correspond extrema points distribution limited local maxima also including minima saddle points. fig. good example dynamics system even complex data problem local minima saddle points limited. however case ebgan might learn generate samples regions local minima data distribution desirable. would interesting modify system using function described order correctly isolate local maxima distribution. model presented regularization function used ebgan repelling regularizer adds pulling-away term forces learnt representations maximally different different data points attempting orthogonalize pair samples minibatch stronger regularization might help denoising autoencoder learn tighter representation support data distribution thus improving conﬁdence score improvements performance system achieved separating classiﬁer denoising autoencoder although combining result advantages like adding degree semi-supervised learning regularizing autoencoder. also possible autoencoder built larger pre-trained network ﬁxed training thus learn reconstruct higher-level stable feature vectors rather high-dimensional inputs. overgeneralization signiﬁcant problem neural networks. paper presented novel approach address problem pairing generic classiﬁer denoising contractive autoencoder used compute conﬁdence score assigns high values input vectors likely belong training data distribution. particular recognition input belonging distribution performed taking account gradient density curvature speciﬁc input point using information determine whether lies close local maximum distribution. suggested system presented signiﬁcant applications variety problems. first problem fooling overgeneralization lead issues context security deployed systems makes possible trick models thus make vulnerable malicious exploitation manner similar proposed adversarial examples example could face retina recognition system restricted access critical facility. thus resistance fooling overgeneralization general lead classiﬁers robust likely behave desired. paper also explored natural application overgeneralization-resistant system context open recognition. general model presented could used complex architectures allow incremental continual learning learning recognize regions input space already explored learnt potentially provide different training regimes unexplored parts case samples regions observed future. example applied system allow adding novel target classes even deployed real world without requiring full retraining costly terms compute time required especially large models. seen extreme case open recognition -class recognition proved challenging problem. building systems capable robust -class recognition critical applications context novelty outliers anomaly detection. conclusion developing discriminative models capable capturing aspects data distribution even without computing perfectly prove useful large number practical applications future work topic highly beneﬁcial. system presented address problem shown perform better previously proposed systems benchmarks. training models reported paper -with exception -class-svmperformed stochastic gradient descent cross-entropy loss function using adam algorithm learning rate models tests implemented using python tensorﬂow model used parameters network used symmetric denoising autoencoder inputs size hidden layers size output layer classiﬁer connected hidden layer autoencoder output units. training performed steps minibatches size three target distributions deﬁned uniform rings thickness inner radius centered three points three models compared regular network output layer replaced cool layer network addition decoder connected hidden layer complete denoising autoencoder used compute conﬁdence score supplementary reconstruction loss added cross-entropy loss classiﬁcation task training autoencoder. variants autoencoders tested using symmetric decoder whose weights transpose weights encoder asymmetric independent weights size. {convd convd ullyconnected ullyconnected}. layer followed relu non-linearity except output layer followed softmax. fooling attempted times digit update steps learning rate updating training performed ﬁxed epochs model. training denoising autoencoder done corrupting input samples additive gaussian noise zero mean parameters model variable depending threshold used parameters previous experiments used cool models except model used addition regularization weights -class simulations used kernel used scaling parameter training -class-svm model done using implementation available scikit-learn library nguyen yosinski clune deep neural networks easily fooled high conﬁdence predictions unrecognizable images proceedings ieee conference computer vision pattern recognition markou singh novelty detection review part statistical approaches signal processing vol. vincent larochelle bengio p.-a. manzagol extracting composing robust features denoising autoencoders proceedings international conference machine learning. rifai vincent muller glorot bengio contractive auto-encoders explicit invariance feature extraction proceedings international conference machine learning russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge international journal computer vision vol. abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay scikit-learn machine learning python journal machine learning research vol. giacomo spigler born received b.sc. degree diploma computer engineering university pisa sant’anna school advanced studies respectively m.sc. cognitive science university edinburgh. currently ﬁnishing ph.d. computational neuroscience university shefﬁeld. current research interests include deep neural networks continual learning.", "year": 2017}