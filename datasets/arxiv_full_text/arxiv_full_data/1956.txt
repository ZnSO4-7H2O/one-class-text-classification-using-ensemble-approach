{"title": "Approximate Bayesian Image Interpretation using Generative Probabilistic  Graphics Programs", "tag": ["cs.AI", "cs.CV", "stat.ML"], "abstract": "The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model. Representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately Bayesian inferences about ambiguous real-world images.", "text": "idea computer vision bayesian inverse problem computer graphics long history appealing elegance proved difﬁcult directly implement. instead vision tasks approached complex bottom-up processing pipelines. show possible write short simple probabilistic graphics programs deﬁne ﬂexible generative models automatically invert interpret real-world images. generative probabilistic graphics programs consist stochastic scene generator renderer based graphics software stochastic likelihood model linking renderer’s output data latent variables adjust ﬁdelity renderer tolerance likelihood model. representations algorithms computer graphics originally designed produce high-quality images instead used deterministic backbone highly approximate stochastic generative models. formulation combines probabilistic programming computer graphics approximate bayesian computation depends general-purpose automatic inference techniques. describe applications reading sequences degraded adversarially obscured alphanumeric characters inferring road models vehicle-mounted camera images. probabilistic graphics programs present relies lines probabilistic code supports accurate approximately bayesian inferences ambiguous real-world images. computer vision historically formulated problem producing symbolic descriptions scenes input images usually done building bottom-up processing pipelines isolate portions image associated scene element extract features indicative identity. many pattern recognition learning techniques used build classiﬁers individual scene elements sometimes learn features approach remarkably successful especially problems recognition. bottom-up pipelines combine image processing machine learning identify written characters high accuracy recognize objects large sets possibilities. however resulting systems typically require large training corpuses achieve reasonable levels accuracy difﬁcult build modify. example tesseract system optical character recognition lines c++. small changes underlying assumptions frequently necessitates end-to-end retraining and/or redesign. generative models range image parsing tasks also explored provide appealing avenue integrating top-down constraints bottom-up processing provide inspiration approach take paper. like traditional bottom-up pipelines vision approaches relied considerable problem-speciﬁc engineering chieﬂy design and/or learn custom inference strategies mcmc proposals incorporate bottom-up cues. combinations top-down knowledge bottom processing remarkably powerful. example shown global geometric information signiﬁcantly improve performance bottom-up object detectors. paper propose novel formulation image interpretation problems called generative probabilstic graphics programming generative probabilistic graphics programs share common template stochastic scene generator approximate renderer based existing graphics software highly stochastic likelihood model comparing renderer’s output observed data latent variables control ﬁdelity renderer tolerance image likelihood. probabilistic graphics programs written variant church probabilistic programming language model introduce requires less lines probabilistic code. renderers likelihoods based standard templates written short python programs. unlike typical generative models scene parsing inverting probabilistic graphics programs requires custom inference algorithm design. instead rely automatic metropolis-hastings transition operators provided probabilistic programming system. approximations stochasticity renderer scene generator likelihood models serve implement variant approximate bayesian computation combination produce kind self-tuning analogue annealing facilities reliable convergence. best knowledge gpgp framework ﬁrst real-world image interpretation formulation combine probabilistic programming automatic inference computer graphics approximate bayesian computation; constitutes main contribution. second contribution provide demonstrations efﬁcacy approach image interpretation problems reading snippets degraded adversarially obscured alphanumeric characters inferring road models vehicle mounted cameras. cases quantitatively report accuracy approach representative test datasets compared standard bottom-up baselines extensively engineered. generative probabilistic graphics programs deﬁne generative models images combining four components. ﬁrst stochastic scene generator written probabilistic code makes random choices location conﬁguration main elements scene. second approximate renderer based existing graphics software maps scene control variables image third stochastic likelihood model image data enables scoring rendered scenes given control variables. fourth latent variables control ﬁdelity renderer and/or tolerance stochastic likelihood model. components described schematically figure perform inference execution histories probabilistic graphics programs using uniform mixture generic single-variable metropolis-hastings transitions without custom bottom-up proposals. ﬁrst give general description generative model inference algorithm induced probabilistic graphics programs; later sections describe speciﬁc details application. {si} decomposition scene parts independent priors example text application include binary indicators presence absence glyph along identity parameters including location figure overview generative probabilistic graphics framework. models shares common template stochastic scene generator samples possible scenes according prior latent variables control ﬁdelity rendering tolerance model approximate render based existing graphics software stochastic likelihood model links observed rendered images. scene sampled scene generator according could rendered onto single image would extremely unlikely exactly match data instead requiring exact matches formulation broaden renderer’s output image likelihood latent control variables inference mediates degree smoothing posterior. size rotation. also {xj} decomposition control variables parts priors bandwidths per-glyph gaussian spatial blur kernels variance gaussian image likelihood proposals modify single elements scene control variables time follows |{si}| |{xj}| total number random variables execution. simplicity describe case number bounded beforehand i.e. total priori scene complexity limited. inference step choose random variable index uniformly random. corresponds scene variable propose overall proposal kernel δs−i i)δx corresponds control variable propose kernel associated variable accept reject metropolis-hastings equation implement probabilistic graphics programs variant church probabilistic programming language. metropolis-hastings inference algorithm provided default system; custom inference code required. context generative probabilistic graphics formulation algorithm makes implicit ideas approximate bayesian computation methods approximate bayesian inference complex generative processes using exogenous distance function compare sampled outputs observed data. original rejection sampling formulation samples accepted match data within hard threshold. subsequently combinations mcmc proposed including variants inference threshold value recently extensions introduced hard cutoff replaced stochastic likelihood model formulation incorporates combination insights rendered scenes approximately constrained match observed image tightness match mediated inference factors ﬁdelity rendering stochasticity likelihood. allows image variability unnecessary even undesirable model treated principled fashion. figure four input images captcha corpus along ﬁnal results convergence trajectory typical inference runs. ﬁrst highly cluttered synthetic captcha exhibiting extreme letter overlap. second captcha turbotax third captcha fourth shows example system makes errors runs. probabilistic graphics program originally support rotation needed captchas; adding required additional line probabilistic code. main text quantitative details supplemental material full corpus. developed probabilistic graphics program reading short snippets degraded text consisting arbitrary digits letters. figure representative inputs outputs. program latent scene {si} contains bank variables glyph including whether potential letter present absent scene spatial coordinates size identity rotated renderer rasterizes letter independently applies spatial blur image composites letters blurs result. also applied global blur original training image applying stochastic likelihood model blurred original rendered images. stochastic likelihood model multivariate gaussian whose mean blurry rendering; formally control variables {xj} renderer likelihood consist per-letter beta global image blur rendered image gaussian spatial blur bandwidths xblur rendered beta global image blur original test image xblur test beta standard deviation gaussian likelihood gamma make hard classiﬁcation decisions sample lowest pixel reconstruction error approximate posterior samples. also experimented enabling enumerative gibbs sampling uniform discrete variables probability. probabilistic code model shown figure assess accuracy approach adversarially obscured text developed captcha corpus consisting images widely used websites turbotax e-trade plus additional challenging synthetic captchas high degrees letter overlap superimposed distractors. source text violates underlying assumptions probabilistic graphics program different ways. turbotax captchas incorporate occlusions break strokes within figure inference renderer ﬁdelity signiﬁcantly improves reliability inference. reconstruction errors runs variants probabilistic graphics program text. without sufﬁcient stochasticity approximation generative model strong prior purely deterministic high-ﬁdelity renderer inference gets stuck local energy minima inference renderer ﬁdelity per-letter global blur tolerance image likelihood number letters convergence improves substantially many local minima likelihood escaped course single-variable inference blur variables automatically adjusted support localizing identifying letters. clockwise left input captcha typical local minima correct parse. representative illustrating convergence dynamics result inference renderer’s ﬁdelity. left right show overall probability pixel-wise disagreement number active letters scene per-letter blur variables. inference automatically adjusts blur newly proposed letters often blurred localized identiﬁed accurately. letters captchas include per-letter warping. captchas involve arbitrary digits letters result lack cues word identity best published captcha breaking systems depend observe robust character recognition given enough inference overall character detection rate calibrate difﬁculty corpus also tesseract optical character recognition engine corpus; character detection rate found dynamically-adjustable ﬁdelity approximate renderer high stochasticity generative model necessary inference robustly converge accurate results. aspect formulation viewed kind self-tuning stochastic bayesian analogue annealing signiﬁcantly improves robustness approach. figure illustration dynamics. also developed generative probabilistic graphics program localizing roads single images. important problem autonomous driving. many perception problems robotics clear scene structure exploit also considerable uncertainty scene well substantial image-to-image variability needs robustly ignored. figure example inputs. probabilistic graphics program problem shown figure latent scene comprised height roadway ground plane road’s width lane size offset corner road camera location. prior encodes assumption lanes small relative road road lanes likely visible scene rendered produce surface-based segmentation image assigns input pixel regions {left oﬀroad right oﬀroad road lane}. rendering done scene element sepaassume max-num-glyphs assume is_present assume pos_x assume pos_y assume size_x assume size_y assume rotation assume glyph assume blur assume global_blur assume data_blur assume epsilon assume port external renderer likelihood server. assume load_image assume render_surfaces assume incorporate_stochastic_likelihood assume data figure generative probabilistic graphics program reading degraded text. scene generator chooses letter identity position size rotation random. random variables renderer along bandwidths series spatial blur kernels blur kernels control ﬁdelity rendered image. image returned renderer compared data pixel-wise gaussian likelihood model whose variance also unknown variable. rately followed compositing text program. figure random surface-based segmentation images drawn prior. extensions richer road ground geometries interesting direction future work. experiments used k-means cluster values randomly chosen training image. used clusters build compact appearance model based cluster-center histograms assigning text image pixels nearest cluster. stochastic likelihood incorporates histograms multiplying together appearance probabilities image region probabilities denoted thetar smoothed pseudo-counts drawn gamma distribution. per-region normalizing constant quantized pixel coordinates input image. likelihood model figure shows appearance model histograms random training frame. figure shows extremely noisy lane/non-lane classiﬁcations result appearance model without scene prior; accuracy extremely low. other richer appearance models gaussian mixtures values compatible formulation; simple quantized model chosen primarily simplicity. generic metropolis-hastings strategy inference problem text application. although deterministic search strategies inference could developed particular program less clear build single deterministic search algorithm could work generative probabilistic graphics programs present. figure illustration generative probabilistic graphics road ﬁnding. renderings random samples scene prior showing surface-based image segmentation induced sample. representative test frames kitti dataset maximum likelihood lane/nonlane classiﬁcation images based solely best-performing single-training-frame appearance model geometric constraints clearly needed reliable road ﬁnding. results typical inference results proposed generative probabilistic graphics approach images appearance model histograms best-performing single-training-frame appearance model four region types lane left offroad right offroad road. figure approximate bayesian inference yields samples broad multimodal scene posterior frame violates modeling assumptions reports less uncertainty frame compatible model table report accuracy approach road dataset kitti vision benchmark suite. focus accuracy face visual variability exploit temporal correspondences. test every frame total report lane/non-lane accuracy results maximum likelihood classiﬁcation appearance models well single best appearance model set. posterior samples frame both. reference include performance sophisticated bottom-up baseline system baseline system requires signiﬁcant priori knowledge including intrinsic extrinsic parameters camera rough intial segmentation test image. contrast approach infer aspects scene image data. also show units arbitrary uncentered renderer coordinate system. assume road_width assume road_height assume lane_pos_x assume lane_pos_y assume lane_pos_z assume lane_size assume assume port external renderer likelihood server. assume load_image assume render_surfaces assume incorporate_stochastic_likelihood assume theta_left assume theta_right assume theta_road assume theta_lane assume data assume surfaces also shown approach yield accurate globally consistent interpretations real-world images coherently report posterior uncertainty latent scenes appropriate. core contributions introduction conceptual framework initial demonstrations efﬁcacy. scale inference approach handle complex scenes likely important consider complex forms automatic inference beyond single-variable metropolis-hastings proposals currently use. example discriminatively trained proposals could help fact could trained based forward executions probabilistic graphics program. appearance models derived modern image features texture descriptors going beyond simple quantizations currently could also reduce burden inference improve generalizability individual programs. important note high dimensionality involved probabilistic graphics programming necessarily mean inference impossible. example approximate inference models probabilities bounded away sometimes provably tractable sampling techniques runtimes depend factors dimensionality fact preliminary experiments text program appear show convergence times unknown letters. exploring role stochasticity facilitating tractability important avenue future work. interesting potential generative probabilistic graphics programming avenue provides bringing powerful graphics representations algorithms bear hard modeling inference problems vision. example avoid global re-rendering inference step need represent exploit conditional independencies rendering process. lifting graphics data structures z-buffers probabilistic program might enable this. real-time high-resolution graphics software contains solutions many hard technical problems image synthesis. long term hope probabilistic extensions ideas ultimately become part analogous solutions image analysis. grateful keith bonawitz eric jonas preliminary work exploring feasibility captcha breaking church seth teller bill freeman adelson michael james siegel helpful discussions.", "year": 2013}