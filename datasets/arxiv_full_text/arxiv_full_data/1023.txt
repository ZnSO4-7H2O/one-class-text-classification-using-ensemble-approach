{"title": "Why does deep and cheap learning work so well?", "tag": ["cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through \"cheap learning\" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various \"no-flattening theorems\" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that $n$ variables cannot be multiplied using fewer than 2^n neurons in a single hidden layer.", "text": "show success deep learning could depend mathematics also physics although well-known mathematical theorems guarantee neural networks approximate arbitrary functions well class functions practical interest frequently approximated cheap learning exponentially fewer parameters generic ones. explore properties frequently encountered physics symmetry locality compositionality polynomial log-probability translate exceptionally simple neural networks. argue statistical process generating data certain hierarchical form prevalent physics machine-learning deep neural network eﬃcient shallow one. formalize claims using information theory discuss relation renormalization group. prove various no-ﬂattening theorems showing eﬃcient linear deep networks canaccurately approximated shallow ones without eﬃciency loss; example show variables cannot multiplied using fewer neurons single hidden layer. deep learning works remarkably well helped dramatically improve state-of-the-art areas ranging speech recognition translation visual object recognition drug discovery genomics automatic game playing however still fully understood deep learning works well. contrast gofai algorithms hand-crafted fully understood analytically many algorithms using artiﬁcial neural networks understood heuristic level empirically know certain training protocols employing large data sets result excellent performance. reminiscent situation human brains know train child according certain curriculum learn certain skills lack deep understanding brain accomplishes this. makes timely interesting develop analytic insights deep learning successes goal present paper. improved understanding interesting right potentially providing clues brains work also practical applications. better understanding shortcomings deep learning suggest ways improving make capable make robust throughout paper adopt physics perspective problem prevent application-speciﬁc details obscuring simple general results related dynamics symmetries renormalization etc. exploit useful similarities deep learning statistical mechanics. task approximating functions many variables central applications machine learning including unsupervised learning classiﬁcation prediction illustrated figure example interested classifying faces want neural network implement function feed image represented million greyscale pixels output probability distribution people image might represent. paper focused expressibility eﬃciency speciﬁcally following well-known problem neural networks approximate functions well practice possible functions exponentially larger practically possible networks? example suppose wish classify secrest paper organized follows. tion present results shallow neural networks merely handful layers focusing simpliﬁcations locality symmetry polynomials. section study increasing depth neural network provide polynomial exponential eﬃciency gains even though adds nothing terms expressivity discuss connections renormalization compositionality complexity. summarize conclusions section explore classes probability distributions focus physics machine learning accurately eﬃciently neural networks approximate them. interested probability disages cats diﬀerent coloring size posture viewing angle lighting condition electronic camera noise etc. physics example might interpret element features enabling accurate approximation follows simple physical generative model relatively free parameters example dependence exhibit symmetry locality and/or simple form exponential low-order polynomial. contrast dependence fig. paper follow machine learning convention refers model parameters refers data thus viewing stochastic function computer scientists usually call category discrete parameter vector continuous. neural networks approximate probability distributions. given many samples random vectors unsupervised learning attempts approximate joint probability distribution without making assumptions causality. classiﬁcation involves estimating probability distribution given opposite operation estimating probability distribution given often called prediction causes earlier data time sequence; cases causes example generative model operation sometimes known probability density estimation. note machine learning prediction sometimes deﬁned outputting probability distribution sampling megapixel greyscale images categories e.g. cats dogs. pixel take values possible images wish compute probability depicts cat. means arbitrary function deﬁned list probabilities i.e. numbers atoms universe neural networks merely thousands millions parameters somehow manage perform classiﬁcation tasks quite well. deep learning cheap sense requiring parameters? neural networks perform combinatorial swindle replacing exponentiation multiplisay inputs taking cation values each swindle cuts number parameters times constant factor. show success swindle depends fundamentally physics although neural networks work well exponentially tiny fraction possible inputs laws physics data sets care machine learning also drawn exponentially tiny fraction imaginable data sets. moreover tiny subsets remarkably similar enphysics hamiltonian cheap learning simple gaussian quadratic locality sparsity translationally symmetric convnet computing spin free energy diﬀerence eﬀective theory irrelevant operator relevant operator statisticians refer self-information surprisal statistical physicists refer hamiltonian quantifying energy given parameter table brief dictionary translating physics machine-learning terminology. deﬁnitions transform equation boltzmann form recasting equation useful hamiltonian tends properties making simple evaluate. section also helps understand relation deep learning renormalization. wish investigate well vector-valued function approximated neural net. standard n-layer feedforward neural network maps vectors vectors applying series linear nonlinear transformations succession. speciﬁcally implements vector-valued functions form relatively simple nonlinear operators vectors aﬃne transformations form wix+bi matrices so-called bias vectors popular choices nonlinear operators include means compute hamiltonian vector n-layer neural evaluate desired classiﬁcation probability vector simply adding softmax layer. µ-vector simply becomes bias term ﬁnal layer. approximation becomes exact. words arbitrarily accurate multiplication always achieved using merely neurons. figure illustrates multiplication approximator. approximates polynomial accuracy better furthermore bounded complexity polynomial scaling number multiplications required times factor typically slightly larger addition four neurons required multiplication additional neurons deployed copy variables higher layers bypassing nonlinearity linear copy gates implementing function course trivial implement using simpler version procedure using shift scale input fall tiny range scaling shifting accordingly virtually popular nonlinear activation functions approximate smooth function desired accuracy even using merely single hidden layer. however theorems guarantee accomplished network feasible size following simple example explains cannot diﬀerent boolean functions variables network implementing generic function class requires least bits describe i.e. bits atoms universe fact neural networks feasible size nonetheless useful therefore implies class functions care approximating dramatically smaller. section physics machine learning tend favor hamiltonians polynomials indeed often ones sparse symmetric low-order. therefore focus initial investigation hamiltonians expanded power series accurately approximate multiplication using small number neurons construct network eﬃciently approximating polynomial repeated multiplication addition. using smooth otherwise arbitrary non-linearity applied element-wise. popular logistic sigmoid activation function trick. ules computable function accurately evaluated suﬃciently large network them. nand gates unique particular neuron implementation indeed generic smooth nonlinear activation function universal class functions exactly expressed neural network must invariant composition since adding layers corresponds using output function input another. important classes include linear functions aﬃne functions piecewise linear functions max) polynomials continuous functions smooth functions whose derivatives continuous. according stone-weierstrass theorem polynomials piecewise linear functions approximate continuous functions arbitrarily well. places merely need summary string arbitrary function evaluated simple -layer neural network middle layer uses sigmoid functions compute products equation layer performs sums equation softmax equation seen polynomials accurately approximated neural networks using number neurons scaling either number multiplications required number terms polynomials panacea binary input functions polynomials continuous input coeﬃcients generic polynomial degree variables easily becomes unmanageably large. discuss situations exceptionally simple polynomials sparse symmetric and/or low-order play special role physics machine-learning. hamiltonians show physics random functions tend polynomials order typically degree ranging simplest example course harmonic oscillator described hamiltonian quadratic position momentum. many reasons order polynomials show physics. important ones sometimes phenomenon studied perturbatively case taylor’s theorem suggests away order polynomial approximation. second reason renormalization higher order terms hamiltonian statistical ﬁeld theory tend negligible observe macroscopic variables. fundamental level hamiltonian standard model particle physics many approximations quartic hamiltonian accurate speciﬁc regimes example maxwell equations governing electromagnetism navier-stokes equations governing ﬂuid dynamics alv´en equations governing magnetohydrodynamics various ising inﬁnite series equation thus gets replaced ﬁnite series terms ending term h...nx since possible strings h−parameters equation suﬃce exactly parametrize arbitrary function eﬃcient multiplication approximator multiplied variables time thus requiring multiple layers evaluate general polynomials. contrast vector implemented using merely three layers illustrated figure middle layer evaluates products third layer takes linear combination them. bits allow accurate multiplication approximator takes product arbitrary number bits once exploiting fact product bits trivially determined example product sum-checking implemented using popular choices nonlinear function logistic +e−x satisﬁes sigmoid compute product above shift additional reasons might expect order polynomials. thanks central limit theorem many probability distributions machine-learning statistics accurately approximated multivariate gaussians i.e. form image classiﬁcation tasks often exploit invariance translation rotation various nonlinear deformations image plane move pixels locations. spatial transformations linear functions pixel vector functions implementing convolutions fourier transforms also polynomials. course arguments imply expect order polynomials every application. consider data generated simple hamiltonian discard random variables resulting distribution general become quite complicated. similarly observe random variables directly observe generic functions random variables result generally mess. arguments however might indicate probability encountering hamiltonian described low-order polynomial application might signiﬁcantly higher might expect naive prior. example uniform prior space polynomials degree would suggest randomly chosen polynomial would almost always degree might prior real-world applications. also note even hamiltonian described exactly low-order polynomial would expect corresponding neural network reproduce low-order polynomial hamiltonian exactly practical scenario host possible reasons including limited data requirement inﬁnite weights inﬁnite accuracy failure practical algorithms stochastic gradient descent global minimum cost function many scenarios. looking weights neural network trained actual data good indicator whether underlying hamiltonian polynomial degree not. deepest principles physics locality things directly aﬀect immediate vicinity. physical systems simulated computer discretizing space onto rectangular lattice locality manifests allowing nearest-neighbor interaction. words almost coeﬃcients equation forced vanish total number non-zero coeﬃcients grows linearly binary case equation applies magnetizations take values locality also limits degree greater number neighbors given spin coupled again applicability considerations particular machine learning applications must determined case case basis. certainly arbitrary transformation collection local random variables result non-local collection. certainly cases physics locality still approximately preserved example simple block-spin renormalization group spins grouped blocks treated random variables. high degree accuracy blocks coupled nearest neighbors. locality famously exploited biological artiﬁcial visual systems whose ﬁrst neuronal layer performs merely fairly local operations. whenever hamiltonian obeys symmetry number independent parameters required describe reduced. instance many probability distributions physics machine learning invariant translation rotation. example consider vector pressures measured microphone times assuming hamiltonian describing reduces number parameters cality reduces requiring translational symmetry reduces parameter count taken together constraints locality symmetry polynomial order reduce number continuous parameters hamiltonian standard model physics merely symmetry reduce merely parameter count also computational complexity. example linear vector-valued function mapping variables onto happens satisfy translational symmetry convolution means investigated probability distributions physics computer science applications lent themselves cheap learning accurately eﬃciently approximated neural networks merely handful layers. turn separate question depth i.e. success deep learning properties real-world probability distributions cause eﬃciency improve networks made deeper? question extensively studied mathematical point view mathematics alone cannot fully answer part answer involves physics. argue answer involves hierarchical/compositional structure generative processes together inability eﬃciently ﬂatten neural networks reﬂecting structure. striking features physical world hierarchical structure. spatially object hierarchy elementary particles form atoms turn form molecules cells organisms planets solar systems galaxies etc. causally complex structures frequently created distinct sequence simpler steps. relevant physics image classiﬁcation respectively. examples involve markov chain probability distribution level hierarchy determined causal predecessor alone next step generative hierarchy requires knowledge merely present state also information past present state redeﬁned include also information thus ensuring generative process markov process. physics example cosmological parameters determines power spectrum density ﬂuctuations universe turn determines pattern cosmic microwave background radiation reaching early universe gets combined foreground radio noise galaxy produce frequency-dependent maps recorded satellite-based telescope measures linear combinations diﬀerent signals adds electronic receiver noise. recent example planck satellite datasets contained numbers respectively. generally given data generated statistical physics process must described equation form equation since dynamics classical physics fundamentally markovian classical equations motion always ﬁrst order diﬀerential equations hamiltonian formalism. technically covers essentially data interest machine learning community although fundamental markovian nature generative process data in-eﬃcient description. image classiﬁcation example deliberately contrived over-simpliﬁed pedagogy single signifying determines parameters determining animal’s coloration body shape posture etc. using approxiate probability distributions determine image ray-tracing scaled translated random amounts randomly generated background added. decomposition generative process hierarchy simpler steps helps resolve theswindle paradox introduction although number parameters required describe arbitrary function input data beyond astronomical generative process speciﬁed modest number parameters steps can. whereas specifying arbitrary probability distribution multi-megapixel images requires bits atoms universe information specifying comimage galaxy entire probability distribution deﬁned standard model particle physics parameters together specify process transforming primordial hydrogen galaxies. parameter-counting argument also applied artiﬁcial images interest machine learning example giving simple low-informationcontent instruction draw cute kitten random sample artists produce wide variety images complicated probability distribution colors postures etc. artist makes random choices series steps. even pre-stored information probabilities artists’ brains modest size. note random resulting image typically contains much information generative process creating example simple instruction generate random string bits contains much fewer bits. typical steps generative hierarchy speciﬁed non-astronomical number parameters discussed section plausible neural networks implement steps eﬃciently. deep neural network stacking simpler networks another would implement entire generative process eﬃciently. summary data sets functions care form minuscule minority plausible also eﬃciently implemented neural networks reﬂecting generative process. remainder? data sets functions care about? almost images indistinguishable random noise almost data sets functions indistinguishable completely random ones. follows borel’s theorem normal numbers states almost real numbers string decimals would pass randomness test i.e. indistinguishable random noise. simple parameter counting shows deep learning would fail implement almost functions training would fail useful patterns. thwart pattern-ﬁnding eﬀorts. cryptography therefore aims produces random-looking patterns. although might expect hamiltonians describing human-generated data sets drawings text music complex describing simple physical systems nonetheless expect resemble natural data sets inspired creation much resemble random functions. goal deep learning classiﬁers reverse hierarchical generative process well possible make inferences input output treat hierarchical problem rigorously using information theory. tant role statistics almost century information contained contained suﬃcient statistic. minimal suﬃcient statistic suﬃcient statistic suﬃcient statistic suﬃcient statistics. means suﬃcient exists function illustrated figure thought information distiller optimally compressing data retain information relevant determining discarding irrelevant information. generate random variables well. biology spiking neurons provide good random number generator machine learning stochastic architectures restricted boltzmann machines same. experimentally measured whereas noise involves unobserved microscopic scales. part framework known renormalization group transformation although connection machine learning studied alluded repeatedly signiﬁcant misconceptions literature concerning connection attempt clear ﬁrst review standard working deﬁnition renormalization context statistical physics involving three ingredients vector random variables course-graining operation requirement operation leaves hamiltonian invariant except parameter changes. think microscopic degrees freedom typically physical quantities deﬁned lattice points space. probability distribution speciﬁed hamiltonian parameter vector interpret implementing coarse-graining system. random variable also hamiltonian denoted require functional form original hamiltonian although parameters change. words function since iterated times giving hamiltonian hrn) repeatedly renormalized data. similar case suﬃcient statistics contrary claims literature eﬀective ﬁeld theory renormalization group little idea unsupervised learning patternﬁnding. instead standard renormalization procedures statistical physics essentially feature extractor supervised learning features typically correspond long-wavelength/macroscopic degrees freedom. words eﬀective ﬁeld theory makes sense specify features interested example given data position momenta particles inside mole liquid tasked predicting data whether alice burn ﬁnger touching liquid suﬃcient statistic simply temperature object turn obtained coarse-grained degrees freedom single spin according rule. case might seem could possibly domain onto itself since fewer degrees freedom coarse-graining. hand domain range diﬀer cannot easily talk hamiltonian functional form since renormalized hamiltonian would diﬀerent domain original hamiltonian. physicists around taking limit lattice inﬁnitely large maps inﬁnite lattice inﬁnite lattice. although minimal suﬃcient statistics often diﬃcult calculate practice frequently possible come statistics nearly suﬃcient certain sense explain. even information distillation functions strictly suﬃcient useful long distill relevant information computationally eﬃcient. example possible trade loss mutual information dramatic reduction complexity hamiltonian; e.g. considerably easier implement neural network precisely situation applies physical example described figure hierarchy eﬃcient near-perfect information distillers found numerical cost scaling number inputs parameters respectively. abstractly procedure renormalization ubiquitous statistical physics viewed special case approximate information distillation describe. systematic framework distilling desired information unwanted noise physical theories known eﬀective field theory typically desired information involves relatively large-scale features explicit link renormalization deep-learning consider model natural images. image described intensity ﬁeld -dimensional vector. assume ensemble images described quadratic hamiltonian form parameter vector deﬁnes ensemble images; could imagine ﬁctitious classes images trying distinguish generated hamiltonians form diﬀerent parameter vectors assume function speciﬁed pixels suﬃciently close derivatives well-approximated diﬀerences. derivatives linear operations implemented ﬁrst layer neural network. translational symmetry equation allows implemented convnet. shown coursenentially repeatedly renormalize keeps increasing modest neglect ﬁrst yi’s. would taken arbitrarily large neural network computed neural network ﬁnite bounded size assuming interested classifying data based coarse-grained variables. insuﬃcient statistics still discriminatory power interested discriminating hamiltonians diﬀer ﬁrst example parameters correspond relevant operators physicists signal machine-learners whereas remaining parameters correspond irrelevant operators physicists noise machine-learners. ﬁxed point structure transformation example simple imagine complicated problems ﬁxed point structure various transformations might highly non-trivial. certainly case statistical mechanics problems renormalization methods used classify various phases matters; point renormalization group thought solving pattern-recognition problem classifying long-range behavior various statistical systems. summary renormalization thought type supervised learning large scale properties system considered features. desired features large-scale properties might still expect generalized formalism renormalization provide intuition problem replacing scale transformation transformation. calling procedure renormalization ultimately matter semantics; remains seen whether semantics teeth namely whether intuition ﬁxed points renormalization group provide concrete insight machine learning algorithms. many numerical methods purpose renormalization group eﬃciently accurately evaluate free energy system function macroscopic variables interest temperature pressure. thus sensibly talk accuracy rg-scheme speciﬁed macroscopic variables interested cause composition number simpler functions suppose approximate function eﬃcient neural network reasons given section simply stack networks other obtain deep neural subtlety regarding statements presented multi-scale entanglement renormalization ansatz mera viewed variational class wave functions whose parameters tuned match given wave function closely possible. perspective mera unsupervised machine learning algorithm classical probability distributions many variables replaced quantum wavefunctions. special tensor network structure found mera resulting variational approximation given wavefunction interpretation generating ﬂow. hence example unsupervised learning problem whose solution gives rise ﬂow. possible extra mathematical structure problem generic variational ansatz give rise interpretation vice versa. measured reasonable norm). neuron-eﬃcient ﬂattening dimensions hidden layers less synapse-eﬃcient ﬂattening number non-zero entries weight matrices less lets deﬁne ﬂattening cost network functions specifying factor optimal ﬂattening increases neuron count synapse count respectively. refer results class functions no-ﬂattening theorems since imply ﬂattening comes cost efﬁcient ﬂattening impossible. complete list noﬂattening theorems would show exactly deep networks eﬃcient shallow networks. already interesting progress spirit crucial questions remain. hand shown deep always better least empirically image classiﬁcation tasks hand many functions found ﬂattening cost signiﬁcant. certain deep boolean circuit networks exponentially costly ﬂatten families multivariate polynomials exponential ﬂattening cost constructed focus functions tree-like hierarchical compositional form concluding ﬂattening cost exponential almost functions sobolev space. relu activation function ﬁnds class functions exhibit exponential ﬂattening costs; study tailored complexity measure deep versus shallow relu networks. shows given weak conditions activation function always exists least function implemented -layer network exponential ﬂattening cost. finally study diﬀerential geometry shallow versus deep networks ﬂattening exponentially neuron-ineﬃcient. work elucidating cost ﬂattening various classes functions clearly highly valuable. might suspect network simple questions concerning ﬂattening become entirely trivial successive multiplication diﬀerent matrices equivalent multiplying single matrix eﬀect ﬂattening indeed trivial expressibility case learnability involves nonlinear complex dynamics despite linearity network show eﬃciency linear networks also rich question. neuronal eﬃciency trivially attainable linear networks since hidden-layer neurons eliminated without accuracy loss simply multiplying weight matrices together. instead consider case synaptic eﬃciency many divide-and-conquer algorithms numerical linear algebra exploit factorization particular matrix order yield signiﬁcant reduction complexity. example represents discrete fourier transform fast fourier transform algorithm makes sparse factorization contains non-zero matrix elements instead naive single-layer implementation contains non-zero matrix elements. ﬁrst pointed example depth helps terminology linear no-ﬂattening theorem fully ﬂattening network performs variables increases another important example illustrating subtlety linear networks matrix multiplication. speciﬁcally take input neural network entries matrix output linear exactly implemented -layer linear neural network. amazingly naive algorithm matrix multiplication requires multiplications optimal strassen algorithm requires mean time interesting noﬂattening results obtained even simpler-tomodel context linear neural networks operators replaced identity biases zero simply linear operators every speciﬁed matrix real matics also physics favors certain classes exceptionally simple probability distributions deep learning uniquely suited model. argued success shallow neural networks hinges symmetry locality polynomial log-probability data inspired natural world favors sparse loworder polynomial hamiltonians eﬃciently approximated. arguments particularly relevant explaining success machine-learning applications physics example using neural network approximate many-body wavefunction whereas previous universality theorems guarantee exists neural network approximates smooth function within error cannot guarantee size neural network grow inﬁnity shrinking activation function become pathological. show constructively given multivariate polynomial generic non-linearity neural network ﬁxed size generic smooth activation function indeed approximate polynomial highly eﬃciently. turning separate question depth argued success deep learning depends ubiquity hierarchical compositional generative processes physics machine-learning applications. studying suﬃcient statistics generative process showed inference problem requires approximating compositional function formation interest irrelevant noise hierarchical process mirrors generative process. although compositional functions eﬃciently implemented deep neural network long individual steps generally possible retain eﬃciency ﬂattening network. extend existing no-ﬂattening theorems showing eﬃcient ﬂattening impossible even many important cases involving linear networks. particular prove ﬂattening polynomials exponentially expensive neurons required multiply numbers using single hidden layer task deep network perform using neurons. strengthening analytic understanding deep learning suggest ways improving make capable make robust. promising area prove sharper comprehensive no-ﬂattening theorems placing lower upper bounds cost ﬂattening networks implementing various classes functions. acknowledgements work supported foundational questions institute http//fqxi.org/ rothberg family fund cognitive science grant thank scott aaronson frank yoshua bengio rico jonschkowski tomaso poggio bart selman viktoriya krakovna krishanu sankar boya song helpful discussions suggestions frank section multiplication variables could implemented neural network neurons hidden layer using equation illustrated figure appendix show equation merely special case formula words multiplication variables implemented network neurons hidden layer. also prove appendix best neural network implement n-input multiplication gate using fewer neurons hidden layer. another powerful no-ﬂattening theorem telling polynomials exponentially expensive ﬂatten. example power monomial xx...xn evaluated deep network using neurons arranged deep neural network copies multiplication gate figure arranged binary tree layers contrast functionally equivalent ﬂattened network requires whopping neurons. example deep neural network multiply numbers using neurons shallow requires neurons. since broad class real-world functions well approximated polynomials helps explain many useful neural networks cannot eﬃciently ﬂattened. must show coefi= aijxi)r since must words depend variable since equation combinations signs variables every term canceled another term opposite sign weight opposite sign neural network compute polynomials accurately eﬃciently linear cost using neurons multiplication. example evaluated using neurons arranged binary tree network hidden layers. appendix prove no-ﬂattening theorem demonstrating ﬂattening polynomials exponentially expensive exists neural network implement function using single hidden layer neurons. furthermore smallest possible number neurons network single hidden layer. result compared problems boolean circuit complexity notably question whether circuit depth analogous number layers number gates analogous number neurons. boolean circuit model neural network model allowed neurons/gates unlimited number inputs. constraint deﬁnition gate elements standard universal library analogous constraint particular nonlinear function. note however theorem weaker applying depth includes circuits depth identical taylor expansions terms degree discussed earlier construction product gate inputs exables achieve arbitrary accuracy ﬁrst scaling factors approximately multiplying ﬁnally scaling result. standpoint group theory construction involves representation group acting upon space polynomials variables group generated elements ﬂips sign wherever occurs. then construction corresponds computation i.e. statement monomials linearly dependent. since distinct monomials fact linearly independent contradiction assumption distinct nonzero. conclude full rank therefore concludes proof. b´eny arxiv e-prints saremi sejnowski proceedings national academy sciences http//www.pnas.org/content///.full.pdf http//www.pnas.org/content///.abstract. miles stoudenmire schwab arxiv e-prints", "year": 2016}