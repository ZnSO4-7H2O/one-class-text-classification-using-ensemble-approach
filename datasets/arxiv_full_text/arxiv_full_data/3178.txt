{"title": "Training Support Vector Machines Using Frank-Wolfe Optimization Methods", "tag": ["cs.LG", "cs.CV", "math.OC", "stat.ML"], "abstract": "Training a Support Vector Machine (SVM) requires the solution of a quadratic programming problem (QP) whose computational complexity becomes prohibitively expensive for large scale datasets. Traditional optimization methods cannot be directly applied in these cases, mainly due to memory restrictions.  By adopting a slightly different objective function and under mild conditions on the kernel used within the model, efficient algorithms to train SVMs have been devised under the name of Core Vector Machines (CVMs). This framework exploits the equivalence of the resulting learning problem with the task of building a Minimal Enclosing Ball (MEB) problem in a feature space, where data is implicitly embedded by a kernel function.  In this paper, we improve on the CVM approach by proposing two novel methods to build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast method to approximate the solution of a MEB problem. In contrast to CVMs, our algorithms do not require to compute the solutions of a sequence of increasingly complex QPs and are defined by using only analytic optimization steps. Experiments on a large collection of datasets show that our methods scale better than CVMs in most cases, sometimes at the price of a slightly lower accuracy. As CVMs, the proposed methods can be easily extended to machine learning problems other than binary classification. However, effective classifiers are also obtained using kernels which do not satisfy the condition required by CVMs and can thus be used for a wider set of problems.", "text": "training support vector machine requires solution quadratic programming problem whose computational complexity becomes prohibitively expensive large scale datasets. traditional optimization methods cannot directly applied cases mainly memory restrictions. adopting slightly diﬀerent objective function mild conditions kernel used within model eﬃcient algorithms train svms devised name core vector machines framework exploits equivalence resulting learning problem task building minimal enclosing ball problem feature space data implicitly embedded kernel function. paper improve approach proposing novel methods build svms based frank-wolfe algorithm recently revisited fast method approximate solution problem. contrast cvms algorithms require compute solutions sequence increasingly complex deﬁned using analytic optimization steps. experiments large collection datasets show methods scale better cvms cases sometimes price slightly lower accuracy. cvms proposed methods easily extended machine learning problems binary classiﬁcation. however eﬀective classiﬁers also obtained using kernels satisfy condition required cvms thus used wider problems. support vector machines currently eﬀective methods approach classiﬁcation machine learning problems improving traditional techniques like decision trees neural networks number applications svms deﬁned optimizing regularized risk functional training data cases leads classiﬁers outstanding generalization performance optimization problem usually formulated large convex quadratic programming problem naive implementation requires space time number examples complexities prohibitively expensive large scale problems major research eﬀorts hence directed towards scaling algorithms large datasets. typically dense structure hessian matrices involved traditional optimization methods cannot directly applied train large datasets. problem usually addressed using active method iteration small number variables allowed change non-linear svms problems essentially equivalent selecting subset training examples called working prominent example category methods sequential minimal optimization variables selected optimization time main disadvantage methods generally exhibit slow local rate convergence closer gets solution slowly approaches solution. moreover performance results practice sensitive size active select active variables implementation details like caching strategy used avoid repetitive computations kernel function model based attempts scale methods consist adapting interior point methods classes qp.. large-scale problems however resulting rank kernel matrix still high handled eﬃciently reformulation objective function sampling methods reduce number variables problem combination small svms using ensemble methods also explored. looking eﬃcient methods approach proposed task learning classiﬁer data transformed problem computing minimal enclosing ball ball smallest radius containing points. equivalence obtained adopting slightly diﬀerent penalty term objective function imposing mild conditions kernel used svm. recent advances computational geometry demonstrated algorithms capable approximating degree accuracy iterations independently number points dimensionality space ball built adopting algorithms tsang colleagues devised core vector machine demonstrating method compares favorably traditional software including example software cvms start solving optimization problem small subset data proceed iteratively. iteration algorithm looks point outside approximation obtained far. point exists added previous subset data deﬁne larger optimization problem solved obtain approximation meb. process repeated points outside current approximating ball found within prescribed tolerance. cvms hence need resolution sequence optimization problems increasing complexity using external numerical solver. order eﬃcient solver able solve problem warm-start avoid full storage corresponding gram matrix. experiments ref. employ variant second-order proposed paper study novel algorithms exploit formalism cvms need resolution sequence qps. algorithms based frank-wolfe optimization framework introduced recently studied method approximate solution problem convex optimization problems deﬁned unit simplex. algorithms used obtain solution arbitrarily close optimum time considerably simpler cvms. idea replace nested optimization problem solved iteration approach linearization objective function current feasible solution exact line search direction obtained linearization. consequently iteration becomes fairly cheaper iteration require external numerical solver. similar cvms algorithms incrementally discover examples become support vectors model looking optimal weights process. however second proposed algorithms also endowed ability explicitly remove examples working used iteration procedure thus potential compute smaller models. theoretical side algorithms guaranteed succeed iterations arbitrary addition second algorithm exhibits asymptotically linear rate convergence research originally motivated framework computational geometry optimization problem training svm. however major advantage proposed methods approach possibility employ kernels satisfy conditions required obtain equivalence optimization problems. example popular polynomial kernel allow cvms training method. since optimal kernel given application cannot speciﬁed priori capability training method work valid kernel function important feature. adaptations handle general kernels recently proposed contrast algorithms used mercer kernel without changes theory implementation. siﬁcation sets already used show improvements cvms second-order experimental results suggest that long minor loss accuracy acceptable algorithms signiﬁcantly improve actual running times algorithm. statistical tests conducted assess signiﬁcance conclusions. addition experiments conﬁrm eﬀective classiﬁers also obtained kernels fulﬁll conditions required cvms. article organized follows. section presents brief overview svms problem computing treated problem. section describes approach. section introduce proposed methods. section presents experimental setting numerical results. section closes article discussion main conclusions research. section present overview support vector machines discuss conditions problem building models treated minimal enclosing ball problem feature space. pattern classiﬁcation problem consider training data {xi} often coinciding called input space instance associated given category ck}. pattern termed hypothesis associate instances models risk functional assessing ability correctly predict category instances procedure dataset mapped given hypothesis achieving risk. context machine learning called learning algorithm hypothesis space induction principle rest paper focus problem computing model designed binary classiﬁcation problems. extension models handle multiple categories accomplished several ways. possible approach corresponds several binary classiﬁers separately trained joined multi-category decision function. well known approaches type one-versus-the-rest classiﬁer trained separate class rest; one-versus-one diﬀerent binary svms used separate possible pair classes; ddag one-versus-one classiﬁers organized directed acyclic graph decision structure previous experiments svms show frequently obtains better performance terms accuracy training time another type extension consists reformulating optimization problem underlying method directly address multiple categories. details methods. highly non-linear svms build linear model original space high-dimensional product feature space original data embedded mapping space feature space related means called kernel function allows compute products directly input space. precisely explicit computation mapping would computationally infeasible thus avoided binary classiﬁcation problems common approach associate positive label examples ﬁrst class negative label examples belonging class. approach allows real valued hypotheses sgn) sgn)). since linear function ﬁnal necessarily imply small test error. implementation induction principle guaranteeing good classiﬁcation performance instances problem addressed svms building concept margin given training pair margin deﬁned reliable prediction model pattern note example misclassiﬁed note also large margin pattern suggests robust decision respect changes parameters decision function estimated training sample margin attained given prediction mechanism full training deﬁned minimum margin whole sample mini∈i implements measure worst classiﬁcation performance training since regularity conditions large margin leads theoretical guarantees good performance decision instances decision function maximizing margin training data thus obtained solving however without constraint size solution maximin problem exist hand even norm separating hyperplane guaranteeing positive margin training pattern need exist. case example high noise level causes large overlap classes. case hyperplane maximizing performs poorly prediction mechanism determined entirely misclassiﬁed examples theoretical results guaranteeing good classiﬁcation accuracy unseen patterns longer hold standard approach deal noisy training patterns allow possibility examples several reformulations problem found literature. particular formulations two–norm penalized instead article particularly interested soft margin one–norm. l-svm proposed mangasarian formulation paper focus l-svm model described above. formulation mainly motivated eﬃciency adopting slightly modiﬁed functional eqn. exploit framework introduced solve learning problem easily explain next subsection. drawback constraints problem explicitly depend images training examples mapping practice avoid explicit computation mapping convenient derive wolfe equal otherwise. contrast problem depends training examples images products using kernel function hence obtain problem deﬁned entirely original data explain l-svm formulation introduced previous paragraphs lead eﬃcient algorithms extract classiﬁers data. pointed ﬁrst generalized l-svm equivalently formulated problem certain feature space computation ball smallest radius containing image computing entire dataset frequently called coreset concept going explore next sections. immediately notice deep similarity problems diﬀerence presence linear term objective function latter. linear term neglected mild conditions kernel function suppose fulﬁlls following normalization condition puting labelled data equivalent computing feature points mapping satisﬁes condition possible implementation mapping turn mapping associated original mercer kernel used svm. classiﬁer implements well-known d-th order polynomial kernel longer constant thus equivalence longer holds. complex constructions required extend optimization framework svms using diﬀerent kernel functions problem general large dense obtaining numerical solution large expensive matter kind numerical method decides employ. taking account practice approximate solution within given tolerance convenient modify priori objective instead meb) compute approximate sense speciﬁed following deﬁnition. dimension cardinality provided. particular coreset iterations. denote starting given iteration deﬁned union point furthest algorithm computes stops brk+) contains note computation performed means kernel evaluations spite lack explicit representation found included index set. finally reduced corresponding approximate coreset solved. algorithm main sources computational overhead computation furthest point linear solution optimization subproblem step complexity former step made constant independent suitable sampling techniques issue return later. regards optimization step cvms adopt method variables selected optimization iteration known cost iteration high method require large number iterations order satisfy reasonable stopping criteria regards initialization computation simple choice suggested consists choosing case center radius .za−zb respectively. initialize eﬃcient strategy implemented example code subset training points indices solver. practice suggested enough also larger initial guesses long rapidly compute initial meb. deﬁned points gaining strictly positive dual weight process corresponding indices. directions vertex feasible set. constraint ensures feasibility denotes i-th vector canonical basis index corresponding largest component updating proved procedure converges globally drawback however often exhibits tendency stagnate near solution. intuitively suppose solutions boundary later detailed improvement quantiﬁed terms rate convergence algorithm thus number iterations expected fulﬁll stopping conditions. practice tendency stagnate near solution lead later iterations wasting computational resources making minimal progress towards optimal function value. would thus desirable obtain stronger result convergence rate guarantees speed algorithm deteriorate approaching solution. paragraph describes technique geared precisely towards aim. essentially previous algorithm enhanced introducing alternative search directions known away steps. basic idea that instead moving towards vertex maximizing linear approximation move away vertex minimizing iteration choice options made choosing best ascent direction. whole procedure known modiﬁed frank-wolfe algorithm sketched follows ruling away steps zero stepsize. away step cannot taken already zero. linear convergence proved assuming lipschitz continuity strong concavity strict complementarity solution. proof result provided problem weaker assumptions. important note assumptions particular satisﬁed formulation l-svm aforementioned linear convergence property holds problems considered paper. particular uniqueness solution implied strong concavity required. gist essentially that small neighborhood solution forced perform away steps face containing reached happens ﬁnite number iterations. starting point algorithm behaves unconstrained optimization method proved converges linearly practice step selects index input point maximizing distance exactly done procedure. computation distances carried cvms using regards step shown regards initialization deﬁned exactly procedure. subsequent iterations formula update immediately follows updating indeed indices strictly positive components plus αki∗ introduction sequence {ik} algorithm makes evident structure output algorithm consider performing away steps weight nearest point current center reduced. course since weight point allowed drop zero search performed only. again optimal stepsize determined closed form consider solution problem known nonzero components correspond points lying boundary exact meb. therefore makes sense remove model points near center away step performed chosen supremum search interval αk+j∗ corresponding example removed current coreset moreover it’s hard step chooses perform away step whenever steps respectively. finally step shows that standard steps performing away step analytical formula update expression follows easily writing objective function λej∗ substantially shown convergence analysis exists point optimization path algorithm away steps performed. algorithm needs remove useless examples correctly identify optimal support vector set. stage algorithm converges linearly optimum value objective function. contrast standard algorithm possess explicit ability eliminate spurious patterns index tends slow getting near solution. methods studied paper originally motivated recent advances computational geometry eﬃcient algorithms address problem however major advantage proposed methods e.g. approach theory implementation algorithms applied without changes train svms using kernels satisfy condition imposed obtain equivalence problem optimization problem methods designed maximize diﬀerentiable concave function bounded convex polyhedron. objective function problem concave constraints coincides unit simplex. proposed methods thus applied directly solve without regard theoretical results global convergence algorithms still hold. addition since strict complementarity usually holds problems results imply still converges linearly optimum. note also constant makes diﬀerence normalized kernels still added objective function case non-normalized kernels since simply ignored optimizing implementation designed handle normalized kernels thus directly used mercer kernel. apparent geometrical interpretation underlying algorithms needs reformulated problem longer equivalent problem computing meb. however easy show search direction procedure iteration still index corresponds largest component similarly away direction explored iterate still index corresponds smallest component constraints problem coincides addition approximate solution produced proposed algorithms feasible. thus sequence strictly increasing converges optimum immediately evident however whether stopping condition used within algorithms guarantees method solution neighborhood optimum show indeed case. simplicity notation convenient write explicitly target meb-svms matrix form matrix entries ˜kij yiyjk yiyj matrix whose columns feature vectors note normalized kernel non-normalized kernels instead viewed arbitrary constant added objective function ˜kα. test classiﬁcations methods discussed several classiﬁcation problems. show that long minor loss accuracy acceptable frank-wolfe based methods able build l-svm classiﬁers considerably smaller time compared cvms turn proven faster traditional software. especially evident large-scale problems capability construct classiﬁer signiﬁcantly reduced amount time useful. discussing several implementation issues compare performance studied algorithms several classical datasets. experiments include scalability tests diﬀerent collections problems increasing size assess capability frank-wolfe based methods eﬃciently solve problems increasingly large size. results found subsecs. subsection present additional experiments problems studied statistical signiﬁcance results presented analyzed section separate test performed subsection study inﬂuence penalty parameter training algorithm. finally subsection present experiments showing capability methods handle wider family kernel function respect cvms. highlight purpose paragraph improve accuracy training time algorithms. detailed commentary obtained results summarizes expands conclusions closes section subsection detail below datasets used section widely used literature. selected cover large variety respect number instances number dimensions number classes. cases training testing sets standard obtained public repositories like others indicate dataset descriptions. exceptions rule datasets pendigits kdd-pc. cases testing obtained random sampling original collection items. examples selected test instances employed training. problem specify tab. number training points input space dimension number classes indicate number examples test used evaluate accuracy classiﬁers never employed training parameter tuning. case multi-category classiﬁcation problems adopted one-versus-one approach method used extend cvms beyond binary classiﬁcation usually obtains best performance terms accuracy training time according hence cases also report size mmax largest binary subproblem size mmin smallest binary subproblem decomposition. states postal service envelopes. extended version usps-ext ﬁrst appeared show large-scale capabilities cvms. original version downloaded extended edge discovery data mining cup. data connection records network obtained simulating wide variety normal accesses intrusions military network. problem detect diﬀerent types accesses network identifying fraudulent ones. version randomly selected whole data. choice kernel best-known family kernels admitted cvms frequently used practice particular choice large experiments presented demonstrate advantage framework software. however subsection present results showing capability methods handle polynomial kernel satisfy conditions required cvms. relatively small datasets pendigits usps parameter determined together parameter svms using -fold cross-validation logarithmic grid ﬁrst collection values correspond parameter second parameter large-scale datasets determined using default method employed cvms average squared distance among training patterns. parameter determined logarithmic grid using validation consisting randomly computed fraction training set. stress paper determine optimal value parameters ﬁne-tuning algorithm test problems seek best possible accuracy. intent compare performance presented methods analyze behavior manner consistent theoretical analysis necessary perform experiments conditions given dataset. optimization problem solved algorithm. reason deliberately avoided using diﬀerent training parameters comparing diﬀerent methods. speciﬁcally parameters tuned using method obtained values used algorithms discussed paper furthermore since value parameter signiﬁcant inﬂuence running times devote speciﬁc subsection evaluate eﬀect parameter diﬀerent training algorithms. random sampling techniques computing i.e. evaluating training points requires number kernel evaluations order cardinality large complexity quickly become unacceptable ruling possibility solve large scale classiﬁcation problems reasonable time. sampling technique called probabilistic speedup proposed overcome obstacle. practice distance computed random subset identiﬁed index small constant cardinality overall complexity thereby reduced order major improvement previous estimate since generally main result technique relies fig. report results concerning accuracies training times speedups support vector sizes obtained datasets. series monotonically increasing number training patterns grows approximately number training patterns ﬁrst dataset speed-up method respect cvms measured training time algorithm training time method measured seconds. similarly speed-up method respect cvms measured training time method. depicted fig. proposed methods slightly less accurate cvms. training time contrast scales considerably better methods number training patterns increases. speed-ups actually always greater shows methods indeed build classiﬁers faster cvms. importantly speed-up monotonically increasing ranging times faster times faster case algorithm times faster almost times faster case method. suggests improvements proposed method cvms becomes signiﬁcant size training grows. fig. depicts accuracies speed-ups obtained adult datasets. like datasets collection created purpose analyzing scalability methods number training patterns grows approximately rate speed-up methods computed previous section. results obtained experiment conﬁrm proposed methods tend faster cvms dataset grows. cvms actually faster cases corresponding smallest versions sequence. ﬁfth version series. speed-ups obtained method experiment moderate collection. however time exhibits also better test accuracies cvms. finally algorithm faster also accurate cvms classiﬁcation problem. table test accuracy proposed algorithms baseline method cvm. statistics correspond mean standard deviation obtained repetitions experiment. protein dataset repetition carried signiﬁcantly longer training times. table running times proposed algorithms baseline method cvm. statistics correspond mean standard deviation obtained repetitions experiment. protein dataset repetition carried signiﬁcantly longer training times. paragraph devoted verify statistical signiﬁcance results obtained above. adopt guidelines suggested ﬁrst conduct multiple test determine whether hypothesis algorithms perform equally rejected not. then conduct separate binary tests compare performances algorithm other. binary tests adopt wilcoxon signed-ranks test method. multiple test non-parametric friedman test. demsar recommends tests safe alternatives classical parametric t-tests compare classiﬁers multiple datasets. main hypothesis paper algorithms faster cvm. also observed slightly less accurate. therefore design binary tests algorithms tab. regards comparison proposed methods apparent advantage terms running time other. seems however accurate thus conduct two-tailed test running times adopt one-tailed test testing accuracy. equally fast faster equally accurate accurate equally fast faster equally accurate accurate equally fast running times diﬀerent equally accurate less accurate tab. report values tests statistics calculated datasets used paper. critical values rejection null hypothesis given signiﬁcance level obtained several books here tab. report p-values corresponding test. note case p-values lower therefore commonly used signiﬁcance levels conclude signiﬁcant diﬀerences terms time accuracy among algorithms. table summarizes reproducibility concerns p-values computed using statistical software wilcoxon signed-ranks test exact p-values preferred asymptotic ones. pratt method handle ties employed default. case friedman test iman davenport’s correction adopted suggested conclusions binary tests. note main hypothesis paper conﬁrmed. time algorithms faster less accurate. previous sections seen however loss accuracy usually lower running time order magnitudes better. regards comparison proposed algorithms cannot conclude diﬀerence training time statistically signiﬁcant. however conclude accurate last observation stresses relevance work extension results presented previous experiments shown parameter used svms handle noisy patterns signiﬁcant impact training time required build classiﬁer hence conduct experiments datasets study eﬀect detail. figs. show training times accuracies obtained shuttle kdd-pc pendigits reuters datasets changing value results conﬁrm general eﬀect parameter training time grows algorithms become faster. however training times proposed methods time signiﬁcantly lower cvms independently value parameter used svm. solving classiﬁcation problem using svms requires select kernel function. since optimal kernel given application cannot speciﬁed priori capability training method work family kernels important feature. order show proposed methods obtain eﬀective models even kernel satisfy conditions required cvms conduct experiments using homogeneous second order polynomial kernel here parameter estimated inverse average squared distance among training patterns. parameter determined usual using validation grid search values test never used determine svms meta-parameters. note however purpose section determine optimal choice kernel function considered problems. results presented merely indicative capability methods handle wide family kernels thus allowing greater ﬂexibility building classiﬁer. tab. summarizes results obtained datasets used section. test accuracies training times comparable obtained using gaussian kernel. noted algorithm cannot used train using kernel selected experiment thus incorporate methods table. results demonstrate capability methods used kernels satisfying normalization condition imposed cvms. comment detail results presented above. first note that time proposed algorithms appear competitive tendency favor training speed large datasets sometimes expense little accuracy. cvms appear faster three problems among single datasets studied subsection pendigits usps shuttle. noted however pendigits usps datasets correspond multi-category problems approached using decomposition method based solving several binary subproblems. shown tab. greatest binary subproblem datasets smaller problems collection adult collection. subproblem small iterations quite cheap overall cost running procedure reasonably low. cases training possibly constitutes convenient choice. advantage fw-based methods lies instead capability eﬀectively handle larger problems results adult collections show. recognition problems datasets ijcnn reuters oﬀers accurate classiﬁers employs larger running time compared mfw. said kdd-pc problem case speedoﬀered considerably larger orders magnitude. shuttle dataset returns mixed results probably high lack homogeneity size subproblems solved decomposition approach. finally methods clearly advantageous protein kdd-full datasets oﬀer accuracy cvms along considerably improved running time. results adult datasets particular interest deserve comment. consist series datasets increasing size study expect gain understanding performance algorithm gradually increases. fact documented datasets commonly used compare scalability algorithms. regard results appear encouraging. algorithms outperform every instance collection respect time observed speedup increases monotonically dataset size increases reaching peak orders magnitude method. algorithms also outperforms running times datasets adult collection similar testing accuracies. clear advantage method respect adult series probably explained considerable size support vector roughly amounts full dataset quite expensive slowing procedure. regards advantage interpret results follows. beginning training process algorithms start small approximating ball progressively expand including examples. intuitively ﬁrst iterations methods tend include large number points order increase radius ball examples belong optimal support vector algorithms remove model approach solution. support vector large case number spurious examples quite large hampering progress towards optimum. endowed possibility explicitly removing points current coreset approximation implying weights useless patterns vanish limit. large number iterations taken drop tolerance numerically considered zeros. contrast possesses ability remove undesired points directly thus enjoys considerable advantage number examples small. paper described application \u0001–coreset based methods computational geometry task eﬃciently training idea ﬁrst proposed introduced algorithms falling category based frank-wolfe optimization scheme. methods analytical formulas learn classiﬁer training thus require solution nested optimization subproblems. compared results presented explored variant algorithm compares favorably terms testing accuracy achieves training times similar original version. large experiments report paper conﬁrms considerably expands conclusions reached long minor loss accuracy acceptable frank-wolfe based methods able build classiﬁers considerably smaller time compared cvms turn proven faster traditional software. conclusions statistically assessed using non-parametric tests. second contribution work present preliminary evidence capability handle wider family kernels cvms thus allowing greater ﬂexibility building classiﬁer. variations procedure explored future work including learning tasks classiﬁcation.", "year": 2012}