{"title": "A Latent Variable Recurrent Neural Network for Discourse Relation  Language Models", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.", "text": "paper presents novel latent variable recurrent neural network architecture jointly modeling sequences words discourse relations adjacent sentences. recurrent neural network generates individual words thus reaping beneﬁts discriminatively-trained vector representations. discourse relations represented latent variable predicted marginalized depending task. resulting model therefore employ training objective includes discourse relation classiﬁcation also word prediction. result outperforms state-ofthe-art alternatives tasks implicit discourse relation classiﬁcation penn discourse treebank dialog classiﬁcation switchboard corpus. furthermore marginalizing latent discourse relations test time obtain discourse informed language model improves strong lstm baseline. natural language processing recently experienced neural network tsunami advantage neural architectures employ discriminatively-trained distributed representations capture meaning linguistic phenomena ranging individual words longer-range linguistic contexts sentence level beyond because discriminatively trained methhowever comparison probabilistic graphical models previously dominant machine learning approach neural architectures lack ﬂexibility. treating linguistic annotations random variables probabilistic graphical models marginalize annotations unavailable test training time elegantly modeling multiple linguistic phenomena joint framework graphical models represent uncertainty every element model adding many layers latent variables makes difﬁcult train. paper present hybrid architecture combines recurrent neural network language model latent variable model shallow discourse structure. model learns discriminatively-trained distributed representation local contextual features drive word choice intra-sentence level using techniques state-of-the-art language modeling however model treats shallow discourse structure speciﬁcally relationships pairs adjacent sentences latent variable. result model discourse relation classiﬁer language model. speciﬁcally trained maximize conditional likelihood discourse relations outperforms state-of-the-art methods implicit discourse relation classiﬁcation penn discourse treebank dialog classiﬁcation switchboard model learns discourse annotations well language modeling objective unlike previous recursive neural architectures learn annotated discourse relations model trained maximize joint likelihood discourse relations text possible marginalize discourse relations test time outperforming language models account discourse structure. contrast recent work continuous latent variables recurrent neural networks require complex variational autoencoders represent uncertainty latent variables model simple implement train requiring minimal modiﬁcations existing recurrent neural network architectures implemented commonly-used toolkits theano torch cnn. focus class shallow discourse relations hold pairs adjacent sentences relations describe adjacent sentences related example contrast latter sentence offer answer question posed previous sentence. shallow relations capture full range discourse phenomena account well-known problems implicit discourse relation classiﬁcation penn discourse treebank conll shared task dialog classiﬁcation characterizes structure interpersonal communication switchboard corpus component contemporary dialog systems model outperforms state-of-the-art alternatives implicit discourse relation classiﬁcation penn discourse treebank dialog classiﬁcation switchboard corpus. probability word conditioned entire preceding sequence words yt<n summary vector htn−. vector computed recurrently htn− embedding current word xytn− rk×v dimensionality word embeddings. language model summarized matrix deﬁnes output embeddings offset. function deterministic non-linear transition function. typically takes element-wise non-linear transformation vector resulting word embedding linear transformation previous hidden state. model described thus identical recurrent neural network language model mikolov paper replace simple hidden state units complex long short-term memory units consistently shown yield much stronger performance language modeling simplicity still term rnnlm referring model. document context language model drawback rnnlm cannot propagate longrange information sentences. even remove sentence boundaries long-range information attenuated repeated application non-linear transition function. propose document context language model address issue. core idea represent context vectors representing intra-sentence word-level context representing inter-sentence context. vectors figure fragment model latent variable illustrates discourse information sentence information sentence affects distribution words prediction within sentence generative probability sentence decomposes across tokens usual per-token probabilities shown equation figure discourse relations incorporated parameterizing output matrices depending discourse relation holds between matrices favor different parts embedding space. bias term also parametrized discourse relation relation favor speciﬁc words. discourse relations observed model form latent variable recurrent neural network connections recent work lvrnns discussed difference latent variables correspond linguistically meaningful elements wish predict marginalize depending situation. present probabilistic neural model sequences words shallow discourse relations. discourse relations treated latent variables linked recurrent neural network words latent variable recurrent neural network random variable capturing discourse relation sentences vector summary contextual information sentence dclm model maintains default context vector ﬁrst sentences documents treats parameter learned model parameters training. model trained ways maximize joint probability maximize conditional probability joint training objective suitable language modeling scenarios conditional objective better discourse relation prediction. describe objective detail. joint likelihood objective joint likelihood objective function directly adopted joint probability deﬁned equation objective function single document sentences utterances maximizing objective function jointly optimize model language language discourse relation prediction. such viewed form multi-task learning learn shared representation works well discourse relation prediction language modeling. however practice large vocabulary size number tokens means language modeling part objective function tends dominate. hidden dimension size vocabulary language modeling. size prediction matrix matriw possible discourse relation. reduce number parameters factoring matrices components inference discourse relations probability discourse relations given sentences decomposed product probabilities individual discourse relations condip. yt−) yt−) terms product given equations normalizing involves small ﬁnite number discourse relations. note inference easy case words observed probabilistic coupling discourse relations. possible values forces objective function attend speciﬁcally problem maximizing conditional likelihood discourse relations treat language modeling auxiliary task alogue corpus annotated collections phone conversations. corpora contain annotations discourse relations dialogue relations hold between adjacent spans text. discourse relation language model carefully designed decouple discourse relations other conditioning words. clear text documents spoken dialogues sequential discourse structures seems likely modeling structure could improve performance. traditional hidden markov model generative approach modeling sequential dependencies difﬁcult training reduces relative frequency eshybrid probabilistictimation. however neural architecture proposed here training already expensive large number parameters must estimated. adding probabilistic couplings adjacent discourse relations would require dynamic programming training inference increasing time complexity factor quadratic number discourse relations. attempt paper; compare conventional dialogue prediction task propose alternative form document context language model contextual information impacts hidden state rather going directly outputs yt+. obtain slightly better perplexity approach fewer trainable parameters. however model would couple subsequent sentences making prediction marginalization discourse relations considerably challenging. sequential monte carlo algorithms offer possible solution considered future work. penn discourse treebank provides low-level discourse annotation written texts. pdtb discourse relation annotated between argument spans arg. types relations explicit implicit. explicit relations signalled discourse markers span almost totally unconstrained range single clause entire paragraph need adjacent either discourse marker. however automatically classifying relations considered relatively easy constraints discourse marker itself addition explicit relations difﬁcult incorporate language models must generate word exactly once. contrary implicit discourse relations annotated adjacent sentences based semantic understanding discourse arguments. automatically classifying discourse relations challenging task therefore focus implicit discourse relations leaving future work question apply modeling framework explicit discourse relations. training collapse relation types implicit single dummy relation type holds adjacent sentence pairs share implicit relation. prior work ﬁrst-level discourse relation identiﬁcation sections pdtb training sections development parameter tuning sections testing. preprocessing lower-cased tokens substituted numbers special token num. build vocabulary kept frequent words training replaced lowfrequency words special token unk. prior work focuses detecting individual relations balanced training sets constructed equal number instances without relation type paper target challenging multiway classiﬁcation problem strategy applicable; case since method deals entire documents possible balance training way. switchboard dialog corpus annotated switchboard corpus humanhuman conversational telephone speech annotations label utterance possible speech acts agree hedge wh-question. speech acts form structure dialogue pertain preceding succeeding utterances swda corpus includes ﬁve-minute conversations. adopted standard split stolcke using conversations training nineteen conversations test. parameter tuning randomly select nineteen conversations training development set. parameter tuning train model full training selected conﬁguration. preprocessing techniques pdtb. single-layer lstm build recurrent architecture models implement package. implementation available https//github.com/ jiyfeng/drlm. additional details follow. rate avoid exploding gradient problem used norm clipping trick threshold addition used value dropout rate input context vector hidden state similar architecture proposed pham training procedure monitored performance development set. experiments epochs enough. hyper-parameters model includes tunable hyper-parameters dimension word representation hidden dimension lstm unit consider values corpus experiments best combination selected grid search development set. main evaluation discourse relation prediction using pdtb swda corpora. also evaluate language modeling determine whether incorporating discourse annotations training time marginalizing test time improve performance. ﬁrst evaluate model implicit discourse relation prediction pdtb dataset. prior work ﬁrst-level discourse relation prediction focuses one-versus-all binary classiﬁcation setting attack general fourway classiﬁcation problem performed rutherford compare following methods rutherford build featurerich classiﬁers pdtb augment classiﬁers additional automaticallylabeled training instances. compare published results state-of-the-art. eisenstein employ recursive neural network architecture. experimental setting different re-run system using setting described results shown table conditionallytrained discourse relation language models outperforms alternatives metrics. jointly-trained drlm level previous state-of-the-art conditional training model provides signiﬁcant additional advantage indicated binomial test. kalchbrenner blunsom employ complex neural architecture convolutional network utterance recurrent network length dialog. knowledge model attains state-of-the-art accuracy task outperforming prior work results shown table conditionallytrained discourse relation language model outperforms competitive systems task. binomial test shows result line signiﬁcantly better previous state-of-the-art comparisons published results macro-f scores available. accuracy discourse-aware language modeling joint model discourse language modeling drlm also function language model assigning probabilities sequences words marginalizing discourse relations. determine whether discourse-aware language modeling improve performance compare following systems rnnlm+lstm basic architecture rnnlm proposed shown outperform kneserney smoothed -gram model modeling wall street journal text. following pham replace sigmoid nonlinearity long short-term memory dclm compare document context language model context-to-output variant identical current modeling approach except parametrized discourse relations. model achieves strong results language modeling small medium-sized corpora outperforming rnnlm+lstm. results perplexities language modeling pdtb swda summarized table comparison line line shows beneﬁt considering multi-sentence context information language modeling. line shows adding discourse relation information yields improvements datasets. emphasize discourse relations test documents marginalized annotations required test set; improvements disambiguating power discourse relations training set. training procedure requires discourse annotations approach scale large datasets typically used language modeling. consequence results obtained somewhat academic perspective practical language modeling. nonetheless positive results motivate investigation training procedures also capable marginalizing discourse relations training time. discourse dialog modeling early work discourse relation classiﬁcation utilizes rich handcrafted feature sets recent representation learning approaches attempt learn good representations jointly discourse relation classiﬁers discourse parsers particular relevance applications neural architectures pdtb implicit discourse relation classiﬁcation approaches essentially classiﬁers take supervision annotated discourse relations pdtb training set. contrast approach probabilistic model entire text. probabilistic models frequently used dialog tagging hidden markov models dominant approach work emission distribution n-gram language model dialogue act; conditionally-trained recurrent neural network language model. alternative neural approach dialogue tagging combined convolutionalrecurrent architecture kalchbrenner blunsom modeling framework simpler relying latent variable parametrization purely recurrent architecture. language modeling increasing number attempts incorporate document-level context information language modeling. example mikolov zweig introduce ldastyle topics based language modeling. sordoni convolutional structure summarize context previous utterances context vector based language modeling. models paper provide uniﬁed framework model context current sentence. wang construct bag-of-words representations previous sentences used inform language model generates current sentence. relevant work document context language model describe connection model adding discourse information latent variable attain better perplexity held-out data. latent variable neural networks introducing latent variables neural network model increases representational capacity main goal prior efforts space perspective model discourse relations latent variables shares merit. unlike prior work approach latent variables carry linguistic interpretation least partially observed. also prior models employ continuous latent variables requiring complex inference techniques variational autoencoders contrast discrete latent variables model easy maximize over. presented probabilistic neural model sequences words shallow discourse relations adjacent sequences. model combines positive aspects neural network architectures probabilistic graphical models learn discriminatively-trained vector representations maintaining probabilistic representation targeted linguistic element case shallow discourse relations. method outperforms state-of-the-art systems discourse relation detection tasks also applied language model marginalizing discourse relations test data. future work investigate possibility learning partially-labeled training data would least potential advantages. first would enable model scale large datasets needed competitive language modeling. second training data resulting vector representations might support even accurate discourse relation prediction. thanks trevor cohn chris dyer lingpeng kong quoc helpful discussions anonymous reviewers feedback. work supported google faculty research award third author. partially performed during jelinek memorial summer workshop speech language technologies university washington seattle supported johns hopkins university grant darpa lorelei contract hr-- gifts google microsoft research amazon mitsubishi electric research laboratory.", "year": 2016}