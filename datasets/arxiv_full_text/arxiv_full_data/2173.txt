{"title": "Towards A Rigorous Science of Interpretable Machine Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.", "text": "autonomous cars adaptive email-ﬁlters predictive policing systems machine learning systems increasingly ubiquitous; outperform humans speciﬁc tasks often guide processes human understanding decisions deployment systems complex applications surge interest systems optimized expected task performance also important criteria safety nondiscrimination avoiding technical debt providing right explanation systems used safely satisfying auxiliary criteria critical. however unlike measures performance accuracy criteria often cannot completely quantiﬁed. example might able enumerate unit tests required safe operation semi-autonomous confounds might cause credit scoring system discriminatory. cases popular fallback criterion interpretability system explain reasoning verify whether reasoning sound respect auxiliary criteria. unfortunately little consensus interpretability machine learning evaluate benchmarking. current interpretability evaluation typically falls categories. ﬁrst evaluates interpretability context application system useful either practical application simpliﬁed version must somehow interpretable doshi-velez second evaluates interpretability quantiﬁable proxy researcher might ﬁrst claim model class—e.g. sparse linear models rule lists gradient boosted trees—are interpretable present algorithms optimize within class wang wang rudin large extent evaluation approaches rely notion you’ll know concerned lack rigor? notions interpretability appear reasonable reasonable meet ﬁrst test facevalidity correct test subjects human beings. however basic notion leaves many kinds questions unanswerable models deﬁned-to-be-interpretable model classes equally interpretable? quantiﬁable proxies sparsity seem allow comparison think comparing model sparse features model sparse prototypes? moreover applications interpretability needs? move ﬁeld forward—to compare methods understand methods generalize—we need formalize notions make evidence-based. objective review chart path toward deﬁnition rigorous evaluation interpretability. need urgent recent european union regulation require algorithms make decisions based user-level predictors signiﬁcantly aﬀect users provide explanation addition volume research interpretability rapidly growing. section discuss interpretability contrast criteria reliability fairness. section consider scenarios interpretability needed why. section propose taxonomy evaluation interpretability—application-grounded human-grounded functionallygrounded. conclude important open questions section speciﬁc suggestions researchers work interpretability section deﬁnition interpret means explain present understandable terms. context systems deﬁne interpretability ability explain present understandable terms human. formal deﬁnition explanation remains elusive; ﬁeld psychology lombrozo states explanations are... currency exchanged beliefs notes questions constitutes explanation makes explanations better others explanations generated explanations sought beginning addressed. researchers classiﬁed explanations deductive-nomological nature providing sense mechanism keil considered broader deﬁnition implicit explanatory understanding. work propose data-driven ways derive operational deﬁnitions evaluations explanations thus interpretability. interpretability used conﬁrm important desiderata systems exist many auxiliary criteria wish optimize. notions fairness unbiasedness imply protected groups somehow discriminated against. privacy means method protects sensitive information data. properties reliability robustness ascertain whether algorithms reach certain levels performance face parameter input variation. causality implies predicted change output perturbation occur real system. usable methods provide information assist users accomplish task—e.g. knob tweak image lighting—while trusted systems conﬁdence human users—e.g. aircraft collision avoidance systems. areas fairness privacy research communities formalized criteria formalizations allowed blossoming rigorous research ﬁelds however many cases formal deﬁnitions remain elusive. following psychology literature keil notes explanations highlight incompleteness argue interpretability assist qualitatively ascertaining whether desiderata—such fairness privacy reliability robustness causality usability trust—are met. example provide feasible explanation fails correspond causal structure exposing potential concern. systems require interpretability. servers postal code sorting craft collision avoidance systems—all compute output without human intervention. explanation necessary either signiﬁcant consequences unacceptable results problem suﬃciently well-studied validated real applications trust system’s decision even system perfect. explanation necessary appropriate? argue need interpretability stems incompleteness problem formalization creating fundamental barrier optimization evaluation. note incompleteness distinct uncertainty fused estimate missile location uncertain uncertainty rigorously quantiﬁed formally reasoned about. machine learning terms distinguish cases unknowns result quantiﬁed variance—e.g. trying learn small data limited sensors—and incompleteness produces kind unquantiﬁed bias—e.g. eﬀect including domain knowledge model selection process. illustrative scenarios cannot create complete list scenarios system fail. enumerating possible outputs given possible inputs computationally logistically infeasible unable undesirable outputs. proxy function ultimate goal. example clinical system optimized cholesterol control without considering likelihood adherence; automotive engineer interested engine data make predictions engine failures broadly build better car. other privacy prediction quality privacy nondiscrimination even objectives fully-speciﬁed exact dynamics trade-oﬀ fully known decision case-by-case. even standard settings exists taxonomy evaluation considered appropriate. particular evaluation match claimed contribution. evaluation applied work demonstrate success application game-playing agent might best human player classiﬁer correctly identify star types relevant astronomers. contrast core methods work demonstrate generalizability careful evaluation variety synthetic standard benchmarks. section analogous taxonomy evaluation approaches interpretability application-grounded human-grounded functionally-grounded. range taskrelevant general also acknowledge human evaluation essential assessing interpretability human-subject evaluation easy task. human experiment needs well-designed minimize confounding factors consumed time resources. discuss trade-oﬀs type evaluation would appropriate. application-grounded evaluation involves conducting human experiments within real application. researcher concrete application mind—such working doctors diagnosing patients particular disease—the best show model works evaluate respect task doctors performing diagnoses. reasoning aligns methods evaluation common human-computer interaction visualization communities exists strong ethos around making sure system delivers intended task example visualization correcting segmentations microscopy data would evaluated user studies segmentation target image task homework-hint system evaluated whether student achieves better post-test performance speciﬁcally evaluate quality explanation context end-task whether results better identiﬁcation errors facts less discrimination. examples experiments include cases important baseline well human-produced explanations assist humans trying complete task. make high impact real world applications essential community respect time eﬀort involved evaluations also demand high standards experimental design evaluations performed. community recognizes easy evaluation metric. nonetheless directly tests objective system built thus performance respect objective gives strong evidence success. human-grounded evaluation conducting simpler human-subject experiments maintain essence target application. evaluation appealing experiments target community challenging. evaluations completed humans allowing bigger subject pool less expenses since compensate highly trained domain experts. human-grounded evaluation appropriate wishes test general notions quality explanation. example study kinds explanations best understood severe time constraints might create abstract tasks factors—such overall task complexity—can controlled question course evaluate quality explanation without speciﬁc end-goal ideally evaluation approach depend quality explanation regardless whether explanation model post-hoc interpretation black-box model regardless correctness associated prediction. examples potential experiments include concrete example. common intrusion-detection test topic models form forward simulation/prediction task human diﬀerence model’s true output corrupted output determine whether human correctly understood model’s true output functionally-grounded evaluation requires human experiments; instead uses formal deﬁnition interpretability proxy explanation quality. experiments appealing even general human-subject experiments require time costs perform necessary approvals beyond resources machine learning researcher. functionally-grounded evaluations appropriate class models regularizers already validated e.g. human-grounded experiments. also appropriate method mature human subject experiments unethical. challenge course determine proxies use. example decision trees considered interpretable many situations section describe open problems determining proxies reasonable. proxy formalized challenge squarely optimization problem model class regularizer likely discrete non-convex often non-diﬀerentiable. examples experiments include essential three types evaluation previous section inform other factors capture essential needs real world tasks inform kinds simpliﬁed tasks perform performance methods respect functional proxies reﬂect performance real-world settings. section describe important open problems creating links three types evaluations imagine matrix rows speciﬁc real-world tasks columns speciﬁc methods entries performance method end-task. example could represent well decision tree depth less worked assisting doctors identifying pneumonia patients constructed methods machine learning could used identify latent dimensions represent factors important interpretability. approach similar eﬀorts characterize classiﬁcation clustering problems example might perform matrix factorization embed tasks methods respectively low-dimensional spaces shown figure embeddings could help predict methods would promising problem similarly collaborative ﬁltering. current features accessible clinician repository discrimination-in-loan cases system must provide outputs assist lawyer decision. ideally would linked domain experts agreed employed evaluate methods applied domain expertise. large open repositories problems classiﬁcation regression reinforcement learning advocate creation repositories contain problems corresponding real-world tasks human-input required. creating repositories challenging creating collections standard machine learning datasets must include system human assessment availablity crowdsourcing tools technical challenges surmounted. practice constructing matrix expensive since cell must evaluated context real application interpreting latent dimensions iterative eﬀort hypothesizing certain tasks methods share dimensions checking whether hypotheses true. next open problems hypotheses latent dimensions correspond hypotheses tested much less expensive humangrounded evaluations simulated tasks. disparate-seeming applications share common categories application involving preventing medical error bedside application involving support identifying inappropriate language social media might similar involve making decision speciﬁc case—a patient post—in relatively short period time. however comes time constraints needs scenarios might diﬀerent application involving understanding main characteristics large omics data goal—science—is much abstract scientist hours days inspect model outputs. local interpretability implies knowing reasons speciﬁc decision former important scientiﬁc understanding bias detection goal; latter needs justiﬁcation speciﬁc decision. domains internal model structure costs even need understand training algorithm. severity incompleteness also aﬀect explanation needs. example imagine spectrum questions safety self-driving cars. general curiosity autonomous cars make decisions. other wish check speciﬁc list scenarios between might want check general property—safe urban driving—without exhaustive list scenarios safety criteria. decision needs made bedside operation plant must understood quickly scientiﬁc anti-discrimination applications end-user willing spend hours trying fully understand explanation. aﬀect kind cognitive chunks have organize individual elements information collections example clinician notion autism adhd developmental diseases. nature user’s expertise also inﬂuence level sophistication expect explanations. example domain experts expect prefer somewhat larger sophisticated model—which conﬁrms facts know—over smaller opaque one. preferences quite diﬀerent hospital ethicist narrowly concerned whether decisions made ethical manner. broadly decison-makers scientists compliance safety engineers data scientists machine learning researchers come diﬀerent background knowledge communication styles. disparate applications share common categories disparate methods share common qualities correlate utility explanation. before provide factors correspond diﬀerent explanation needs here deﬁne cognitive chunks basic units explanation. hierarchies abstractions limit human needs process time. example part explanation involve deﬁning unit function units providing explanation terms unit. work laid groundwork process rigorously deﬁne evaluate interpretability. many open questions creating formal links applications science human understanding traditional machine learning regularizers. mean time encourage community consider general principles. claim research match type evaluation. would critical reliability-oriented paper cites accuracy statistics choice evaluation match speciﬁcity claim made. contribution focused particular application expected evaluated context application human experiment closely-related task contribution focused better optimizing model class deﬁnition interpretability expected evaluated functionally-grounded metrics. community must careful work interpretability recognizing need costs human-subject experiments. section hypothesized factors latent dimensions interpretability. creating shared language around factors essential evaluation also citation comparison related work. example work creating safe healthcare agent might framed focused need explanation unknown inputs local scale evaluated level application. contrast work learning sparse linear models might also framed focused need explanation unknown inputs time evaluated global scale. share work community service describing factors course adding reﬁning factors taxonomies evolve. considerations move away vague claims interpretability particular model toward classifying applications common terms. acknowledgments piece would possible without dozens deep conversations interpretability machine learning researchers domain experts. friends colleagues appreciate support. want particularity thank goodfellow kush varshney hanna wallach solon barocas stefan rping jesse johnson feedback.", "year": 2017}