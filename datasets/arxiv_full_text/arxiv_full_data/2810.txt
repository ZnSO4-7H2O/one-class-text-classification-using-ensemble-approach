{"title": "MaskGAN: Better Text Generation via Filling in the______", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.", "text": "neural text generation models often autoregressive language models seqseq models. models generate text sampling words sequentially word conditioned previous word state-of-the-art several machine translation summarization benchmarks. benchmarks often deﬁned validation perplexity even though direct measure quality generated text. additionally models typically trained maximum likelihood teacher forcing. methods well-suited optimizing perplexity result poor sample quality since generating text requires conditioning sequences words never observed training time. propose improve sample quality using generative adversarial networks explicitly train generator produce high quality samples shown success image generation. gans originally designed output differentiable values discrete language generation challenging them. claim validation perplexity alone indicative quality text generated model. introduce actor-critic conditional ﬁlls missing text conditioned surrounding context. show qualitatively quantitatively evidence produces realistic conditional unconditional text samples compared maximum likelihood trained model. recurrent neural networks common generative model sequences well sequence labeling tasks. shown impressive results language modeling machine translation text classiﬁcation text typically generated models sampling distribution conditioned previous word hidden state consists representation words generated far. typically trained maximum likelihood approach known teacher forcing ground-truth words back model conditioned generating following parts sentence. causes problems when sample generation model often forced condition sequences never conditioned training time. leads unpredictable dynamics hidden state rnn. methods professor forcing scheduled sampling proposed solve issue. approaches work indirectly either causing hidden state dynamics become predictable randomly conditioning sampled words training time however directly specify cost function output encourages high sample quality. proposed method generative adversarial networks framework training generative models adversarial setup generator generating images trying fool discriminator trained discriminate real synthetic images. gans success producing realistic images approaches seen limited text sequences. discrete nature text making infeasible propagate gradient discriminator back generator standard training. overcome using reinforcement learning train generator discriminator still trained maximum likelihood stochastic gradient descent. gans also commonly suffer issues training instability mode dropping exacerbated textual setting. mode dropping occurs certain modalities training rarely generated generator example leading generated images volcano multiple variants volcano. becomes signiﬁcant problem text generation since many complex modes data ranging bigrams short phrases longer idioms. training stability also issue since unlike image generation text generated autoregressively thus loss discriminator observed complete sentence generated. problem compounds generating longer longer sentences. reduce impact problems training model text ﬁll-in-the-blank in-ﬁlling task. similar task proposed bowman robust setup. task portions body text deleted redacted. goal model inﬁll missing portions text indistinguishable original data. in-ﬁlling text model operates autoregressively tokens thus ﬁlled standard language modeling conditioning true known context. entire body text redacted reduces language modeling. designing error attribution time step noted important prior natural language research text inﬁlling task naturally achieves consideration since discriminator evaluate token thus provide ﬁne-grained supervision signal generator. consider instance generator produces sequence perfectly matching data distribution ﬁrst time-steps produces outlier token despite entire sequence clearly synthetic result errant token discriminative model produces high loss signal outlier token others likely yield informative error signal generator. introduce text generation model trained in-ﬁlling consider actor-critic architecture extremely large action spaces. consider evaluation metrics generation synthetic training data. research reliably extending training discrete spaces discrete sequences highly active area. training continuous setting allows fully differentiable computations permitting gradients passed discriminator generator. discrete elements break differentiability leading researchers either avoid issue reformulate problem work continuous domain consider methods. seqgan trains language model using policy gradients train generator fool cnn-based discriminator discriminates real synthetic text. generator discriminator pretrained real fake data phase training policy gradients. training monte carlo rollouts order useful loss signal word. follow-up work demonstrated text generation without pretraining rnns additionally produced results generator matching highdimensional latent representations. professor forcing alternative training teacher forcing using discriminator discriminate hidden states generator conditioned real synthetic samples. since discriminator operates hidden states gradients passed generator hidden state dynamics inference time follow training time. gans applied dialogue generation showing improvements adversarial evaluation good results human evaluation compared maximum likelihood trained baseline. method applies reinforce monte carlo sampling generator. replacing non-differentiable sampling operations efﬁcient gradient approximators shown strong results discrete gans. recent unbiased variance gradient estimate techniques tucker prove effective. wgan-gp avoids issue dealing backpropagating discrete nodes generating text one-shot manner using convolutional network. hjelm proposes algorithmic solution uses boundary-seeking objective along importance sampling generate text. rajeswar discriminator operates directly continuous probabilistic output generator. however accomplish this recast traditional autoregressive sampling text since inputs predetermined. instead optimize lower-variance objective using discriminator’s output rather standard objective. reinforcement learning methods explored successfully natural language. using reinforce cross entropy hybrid mixer directly optimized bleu score demonstrated improvements baselines. recently actor-critic methods natural language explored bahdanau instead rewards supplied discriminator adversarial setting rewards task-speciﬁc scores bleu. work distinct employ actor-critic training procedure task designed provide rewards every time step believe in-ﬁlling mitigate problem severe mode-collapse. task also harder discriminator reduces risk generator contending near-perfect discriminator. critic method helps generator converge rapidly reducing high-variance gradient updates extremely high action-space environment operating word-level natural language. task imputing missing tokens requires maskgan architecture condition information past future. choose seqseq architecture. generator consists encoding module decoding module. discrete sequence binary mask generated length selects tokens remain. token time replaced special mask token mask remains unchanged mask standard language-modeling decoder ﬁlls missing tokens auto-regressively however conditioned masked text well ﬁlled-in point. generator decomposes distribution sequence ordered conditional sequence discriminator identical architecture generator except output scalar probability time point rather distribution vocabulary size. discriminator given ﬁlled-in sequence generator importantly given original figure seqseq generator architecture. blue boxes represent known tokens purple boxes imputed tokens. demonstrate sampling operation dotted line. encoder reads masked sequence masked tokens denoted underscore decoder imputes missing tokens using encoder hidden states. example generator alphabetical ordering real context give discriminator true context otherwise algorithm critical failure mode. instance without context discriminator given ﬁlled-in sequence director director guided series fail reliably identify director director bigram fake text despite bigram potentially never appearing training corpus reason ambiguous occurrences director fake; *associate* director guided series director *expertly* guided series potentially valid sequences. without context words real discriminator found assign equal probability words. result course inaccurate learning signal generator correctly penalized producing bigrams. prevent this discriminator computes probability token real given true context masked sequence model fully-differentiable sampling operations generator’s probability distribution produce next token. therefore train generator estimate gradient respect parameters policy gradients reinforcement learning ﬁrst employed gans language modeling analogously generator seeks optimize parameters generator performing gradient ascent using reinforce family algorithms unbiased estimator ∇θeg rt∇θ variance gradient estimator reduced using learned value function baseline produced critic. results generator gradient contribution single token nomenclature quantity interpreted estimate advantage here action token chosen generator maskgan source code available https//github.com/tensorflow/models/tree/ task design rewards time step single sequence order credit assignment result token generated time-step inﬂuence rewards received time step subsequent time steps. gradient generator include intuitively shows gradient generator associated producing depend discounted future rewards assigned discriminator. non-zero discount factor generator penalized greedily selecting token earns high reward time-step alone. full sequence generated words finally conventional training discriminator updated according gradient aside avenues explored highlight particular problems task plausible remedies. task becomes difﬁcult long sequences large vocabularies. address issue extended sequence length modify core algorithm dynamic task. apply algorithm maximum sequence length however upon satisfying convergence criterion increment maximum sequence length continue training. allows model build ability capture dependencies shorter sequences moving longer dependencies form curriculum learning. order alleviate issues variance reinforce methods large vocabulary size consider simple modiﬁcation. time-step instead generating reward sampled token instead seek full information generator distribution. sampling generator produces probability distribution tokens compute reward possible token conditioned generated before. incurs computational penalty since discriminator must used predict tokens performed efﬁciently potential reduction variance could beneﬁcial. prior training ﬁrst perform pretraining. first train language model using standard maximum likelihood training. pretrained language model weights seqseq encoder decoder modules. language models pretrain seqseq model in-ﬁlling task using maximum likelihood particular attention parameters described luong select model producing lowest validation perplexity masked task hyperparameter sweep runs. initial algorithms include critic found inclusion critic decreased variance gradient estimates order magnitude substantially improved training. evaluation generative models continues open-ended research question. seek heuristic metrics believe correlated human-evaluation. bleu score used extensively machine translation compare quality candidate translations reference. motivated metric compute number unique n-grams produced generator occur validation corpus small compute geometric average metrics uniﬁed view performance generator. maximum-likelihood trained benchmark able hyperparameter conﬁgurations small decreases validation perplexity o−point. however found models yield considerable improvements sample quality abandoned trying reduce validation perplexity. biggest advantages gan-trained models generator produce alternative realistic language samples unfairly penalized producing high likelihood single correct sequence. generator explores ‘off-manifold’ free-running mode alternative options valid maximize probability underlying sequence. therefore choose focus architectures hyperparameter conﬁgurations small reductions validation perplexity rather searched improved heuristic evaluation metrics. present conditional unconditional samples generated imdb data sets word-level. maskgan refers gan-trained variant maskmle refers maximumlikelihood trained variant. additional samples supplied appendix penn treebank dataset vocabulary unique words. training contains words validation contains words test contains words. experiments train training partition. ﬁrst pretrain commonly-used variational lstm language model parameter dimensions common maskgan following ghahramani validation perplexity loading weights language model maskgan generator pretrain masking rate validation perplexity finally pretrain discriminator samples produced current generator real training text. also maskgan unconditional mode entire context blanked thus making equivalent language model. present length- language model sample table additional samples included appendix. imdb dataset maas consists movie reviews taken imdb. review contain several sentences. dataset divided labeled training instances labeled test instances unlabeled training instances. label indicates sentiment review either positive negative. ﬁrst words review training train models leads dataset million words. identical training process pretrain language model validation perplexity loading weights language model maskgan generator pretrain masking rate validation perplexity finally pretrain discriminator samples produced current generator real training text. positive follow good earth movie linked vacation comedy credited modern yarns helpful something modern best interesting drama based story famed date training achieved state-of-the-art word level validation perplexity penn treebank dataset. rather performing models still maximum-likelihood trained table perplexity calculated using pre-trained language model equivalent decoder used maskmle maskgan models. language model used initialize models. models recent architectures found neural architecture search zoph extensive hyperparameter search maskgan supported training improve validation perplexity results state-of-the-art models. however instead seek understand quality sample generation. highlighted earlier fundamental problem generating free-running mode potentially leads ‘off-manifold‘ sequences result poor sample quality teacher-forced models. seek quantitatively evaluate dynamic present sampling. commonly done bleu shown bleu necessarily correlated sample quality. believe correlation even less in-ﬁlling task since many potential valid in-ﬁllings bleu would penalize valid ones. instead calculate perplexity generated samples maskgan maskmle using language model used initialize maskgan maskmle. maskgan maskmle produce samples autoregressively building upon previously sampled tokens produce distribution next. maskgan model produces samples likely initial model maskmle model. maskmle model generates improbable sentences assessed initial language model inference compounding sampling errors result recurrent hidden states never seen teacher forcing conversely maskgan model operates free-running mode training supports robust sampling perturbations. contrast image generation mode collapse measured directly calculating certain n-gram statistics. instance measure mode collapse percentage unique n-grams generated imdb movie reviews. unconditionally generate sample results almost total bi/tri/quad-grams. results table show maskgan show mode collapse evidenced reduced number unique quadgrams. however complete samples models still unique. also observed training initial small drop perplexity ground-truth validation steady increase perplexity training progressed. despite this sample quality remained relatively consistent. ﬁnal samples generated model perplexity ground-truth hypothesize mode dropping occurring near tail sequences since generated samples unlikely generate previous words correctly order properly model distribution words tail. theis also shows validation perplexity necessarily correlate sample quality. ultimately evaluation generative models still best measured unbiased human evaluation. therefore evaluate quality generated samples initial language model maskmle model maskgan model blind heads-up comparison using amazon mechanical turk. note models number parameters inference time. raters compare quality extracts along axes asked ﬁrst extract second extract neither higher quality. table mechanical turk blind heads-up evaluation pairs models trained imdb reviews. reviews model unconditionally sampled randomized. raters asked sample preferred pair. ratings obtained model pair comparison. table mechanical turk blind heads-up evaluation pairs models trained ptb. news snippets model unconditionally sampled randomized. raters asked sample preferred pair. ratings obtained model pair comparison. mechanical turk results show maskgan generates superior human-looking samples maskmle imdb dataset. however smaller dataset results closer. also show results seqgan maskgan show maskgan produces superior samples seqgan. work supports case matching training inference procedures order produce higher quality language samples. maskgan algorithm directly achieves gan-training improved generated samples assessed human evaluators. experiments generally found training contiguous blocks words masked produced better samples. conjecture allows generator opportunity explore longer sequences free-running mode; comparison random mask generally shorter sequences blanks gain gan-training substantial. found policy gradient methods effective conjunction learned critic highly active research training discrete nodes present even stable training procedures. also found attention important in-ﬁlled words sufﬁciently conditioned input context. without attention in-ﬁlling would reasonable subsequences became implausible context adjacent surrounding words. given this suspect another promising avenue would consider gan-training attention-only models vaswani general think proposed contiguous in-ﬁlling task good approach reduce mode collapse help training stability textual gans. show maskgan samples larger dataset signiﬁcantly better corresponding tuned maskmle model shown human evaluation. also show produce high-quality samples despite maskgan model much higher perplexity ground-truth test set. would like thank george tucker jascha sohl-dickstein shlens ryan sepassi jasmine collins irwan bello barret zoph gabe pereyra eric jang google brain team particularly ﬁrst year residents humored listening commenting almost every conceivable variation core idea. dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio. actor-critic algorithm sequence prediction. international conference learning representations samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems samuel bowman luke vilnis oriol vinyals andrew rafal jozefowicz samy bengio. generating sentences continuous space. signll conference computational natural language learning tong yanran ruixiang zhang devon hjelm wenjie yangqiu song yoshua bengio. maximum-likelihood augmented discrete generative adversarial networks. arxiv preprint arxiv. thomas degris patrick pilarski richard sutton. model-free reinforcement learning continuous action practice. american control conference ieee yarin zoubin ghahramani. theoretically grounded application dropout recurrent neural networks. advances neural information processing systems goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems hakan inan khashayar khosravi richard socher. tying word vectors word classiﬁers loss framework language modeling. international conference learning representations jiwei monroe tianlin alan ritter jurafsky. adversarial learning neural dialogue generation. conference empirical methods natural language processing minh-thang luong hieu pham christopher manning. effective approaches attention-based neural machine translation. conference empirical methods natural language processing andrew maas raymond daly peter pham huang andrew christopher potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies-volume association computational linguistics takeru miyato andrew goodfellow. virtual adversarial training semi-supervised text classiﬁcation. international conference learning representations volume kishore papineni salim roukos todd ward wei-jing zhu. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics association computational linguistics richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems george tucker andriy mnih chris maddison dieterich lawson jascha sohl-dickstein. rebar low-variance unbiased gradient estimates discrete latent variable models. conference neural information processing systems ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin. attention need. conference neural information processing systems yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. corr abs/. http//arxiv.org/abs/.. model trained adam method stochastic optimization default tensorﬂow exponential decay rates model uses -layers unit lstms generator discriminator dimensional word embeddings variational dropout. used bayesian hyperparameter tuning tune variational dropout rate learning rates generator discriminator critic. perform gradient descent steps discriminator every step generator critic. share embedding softmax weights generator proposed bengio press wolf inan furthermore improve convergence speed share embeddings generator discriminator. additionally noted architectural section critic shares discriminator parameters exception separate output head estimate value. generator discriminator variational recurrent dropout next show <eos> interactive telephone technology taken leap retail business <eos> next show <eos> interactive telephone technology long dominated <unk> nation largest economic next show <eos> interactive telephone technology exercised stake u.s. france next show <eos> interactive telephone technology taken leap complicate case next show <eos> interactive telephone technology <unk> number clients estimates mountain-bike next show <eos> interactive telephone technology instituted week <unk> <unk> <unk> wis. auto present additional language model samples here. modiﬁed seqgan train generate samples using size architecture generator maskgan generator present samples maskgan samples. <unk> basis despite huge after-tax interest income <unk> million <eos> west germany world corrupt organizations multibillion-dollar <unk> atmosphere metropolitan zone historic array removed <eos> another takeover target directors attempted october <unk> british airways allowed three funds cineplex odeon corp. shares made fresh group purchase part revised class <unk> british <unk> <unk> <unk> <unk> seed <eos> <unk> performance <eos> pitch black complete shock ﬁrst back really looking forward pitch black complete shock ﬁrst back promos well pitch black complete shock ﬁrst back days positive follow good earth movie linked vacation comedy credited modern yarns helpful something modern best interesting drama based story famed negative really understand movie falls like seeing sorry reason watched casting emperor expecting anything negative much time time persevered become cast good realize book made story manhattan allies widely witnessed gan-training also common failure mode collapse across various n-gram levels. mode collapse extreme collapse -gram level described gulrajani manifest grammatical albeit inanely repetitive phrases example course discriminator discern out-of-distribution sample however certain failure modes observed generator move common modes frequently present text. notice maskgan architecture often struggles produce syntactically correct sequences hard boundary must end. also relatively challenging task humans ﬁlled text must contextual also match syntactically boundary blank text present ﬁxed number words. similar failure modes present image generation produced samples often lose global coherence despite sensible locally. expect larger capacity model mitigate issues. absence global scalar objective optimize training monitor various n-gram language statistics assess performance. however crude proxies quality produced samples. instance maskgan models improvements particular n-gram metric extreme expense validation perplexity seen figure could devolve generator sample diversity. below produce several samples particular model which despite dramatically improved -gram metric lost diversity.", "year": 2018}