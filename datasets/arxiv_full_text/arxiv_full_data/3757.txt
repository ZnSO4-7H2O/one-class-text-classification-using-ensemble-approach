{"title": "On the Stability of Deep Networks", "tag": ["stat.ML", "cs.IT", "cs.LG", "cs.NE", "math.IT", "math.MG"], "abstract": "In this work we study the properties of deep neural networks (DNN) with random weights. We formally prove that these networks perform a distance-preserving embedding of the data. Based on this we then draw conclusions on the size of the training data and the networks' structure. A longer version of this paper with more results and details can be found in (Giryes et al., 2015). In particular, we formally prove in the longer version that DNN with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data.", "text": "work study properties deep neural networks random weights. formally prove networks perform distance-preserving embedding data. based draw conclusions size training data networks’ structure. longer version paper results details found particular formally prove random gaussian weights perform distance-preserving embedding data special treatment in-class out-of-class data. deep neural nets revolution areas machine learning audio analysis computer vision. many state-of-the-art results achieved using architectures. work study properties architectures random weights. prove preserve distances data along layers property allows stably recovering original data features calculated network. results provide insights outstanding empirically observed performance size training data. motivation studying networks random weights threefold. first differences networks used decades state-of-the-art training strategies usage random initialization weights. second series works empirically showed successful learning techniques based randomization. third recent works studied optimization aspect training deep networks also done randomization bruna show pooling stage causes shift invariance property. bruna interpret step removal phase complex signal show signal recovered pooling stage using phase retrieval methods. short note presentation purposes consider previously studied pooling step assuming data properly aligned. focus roles layers linear operation followed element-wise non-linear activation function. expectation taken normal i.i.d. elements. section illustrate concept exemplify results gaussian mixture models semi-truncated linear function linear interval constant outside popular rectiﬁed linear unit example function sigmoid functions satisfy property approximately. following theorem shows standard layer performs stable embedding data gromov-hausdorff sense. theorem linear operator applied i-th layer non-linear activation function manifold input data i-th layer. random matrix i.i.d normally distributed entries output dimension semi-truncated linear function high probability mahendran vedaldi demonstrate possible recover input output. next result provides theoretical justiﬁcation observation showing possible recover input layer output order show entire network produces stable embedding input need show gaussian mean width grow signiﬁcantly data propagate layers network. instead bounding variation gaussian mean width throughout network bound change covering number i.e. lowest number ℓ-balls radius cover bound covering number dudleys inequality plog bound gaussian mean width proof present sketch proof deferring full proof treats also gaussian mean width directly longer version paper. hard since non-linear activation function shrinks data increase size covering; therefore focus linear part. following distances ones factor. sufﬁcient complete proof. important question deep learning amount labeled training samples needed training. using sudakov minoration upper bound size ǫ-net demonstrated networks random gaussian weights realize stable embedding; consequently network trained using screening technique selecting best among many networks generated random weights suggested pinto saxe pinto number data points needed used order guarantee network represents data o/ǫ)). since proxy data dimension conclude number training points grows exponentially intrinsic dimension data. shown random gaussian weights perform distance-preserving embedding data. result provides relationship complexity input data size required training set. addition draws connection dimension features produced networkwhich still keep metric information original manifold complexity data. though focused case linear ﬁlters random gaussian entries possible extend analysis distributions sub-gaussian random convolutional ﬁlters using proof techniques extension learned presented extended version note. pinto beyond simple features large-scale feature search approach unconstrained face recognition. ieee international conference automatic face gesture recognition workshops march pinto doukhan dicarlo high-throughput screening approach discovering good forms biologically inspired visual representation. plos comput biol saligrama aperiodic sequences uniformly decaying correlations applications compressed sensing system identiﬁcation. ieee trans. inf. theory sept. saxe mcclelland ganguli exact solutions nonlinear dynamics learning deep linear neural network. international conference learning representations", "year": 2014}