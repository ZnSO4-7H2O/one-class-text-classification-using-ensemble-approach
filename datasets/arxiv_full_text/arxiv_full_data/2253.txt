{"title": "Rough extreme learning machine: a new classification method based on  uncertainty measure", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Extreme learning machine (ELM) is a new single hidden layer feedback neural network. The weights of the input layer and the biases of neurons in hidden layer are randomly generated, the weights of the output layer can be analytically determined. ELM has been achieved good results for a large number of classification tasks. In this paper, a new extreme learning machine called rough extreme learning machine (RELM) was proposed. RELM uses rough set to divide data into upper approximation set and lower approximation set, and the two approximation sets are utilized to train upper approximation neurons and lower approximation neurons. In addition, an attribute reduction is executed in this algorithm to remove redundant attributes. The experimental results showed, comparing with the comparison algorithms, RELM can get a better accuracy and repeatability in most cases, RELM can not only maintain the advantages of fast speed, but also effectively cope with the classification task for high-dimensional data.", "text": "extreme learning machine single hidden layer feedback neural network. weights input layer biases neurons hidden layer randomly generated; weights output layer analytically determined. achieved good results large number classiﬁcation tasks. paper extreme learning machine called rough extreme learning machine proposed. relm uses rough divide data upper approximation lower approximation approximation sets utilized train upper approximation neurons lower approximation neurons. addition attribute reduction executed algorithm remove redundant attributes. experimental results showed comparing comparison algorithms relm better accuracy repeatability cases; relm maintain advantages fast speed also eﬀectively cope classiﬁcation task high-dimensional data. development information society various industries produced great deal data eﬀectively analyze data becomes urgent problem classiﬁcation basic form data analysis; attracted scholars much attention extreme learning machine proposed huang classiﬁcation method. recent years extensively studied researchers zhang proposed privileged knowledge extreme learning machine called elm+ radar signal recognition practical applications many classiﬁcation tasks privileged knowledge traditional take advantage privileged knowledge; elm+ makes full privileged knowledge input data feature space correction space; uses traditional privileged knowledge corrected hidden layer output matrix output layer weights; solving corrected optimization problem solution elm+ obtained. aiming classiﬁcation blind domain adaptation uzair developed model named aelm order cope problem diﬀerence distribution training data target domain data a-elm uses multiple classiﬁers system; a-elm algorithm global classiﬁer trained whole training data classify classes data; data divided subsets local classiﬁers also trained based subsets. data coming algorithm utilizes classiﬁers classify data. local classier whose square error least global classiﬁer ﬁnal training classiﬁcation model. deng proposed fast accurate kernelbased supervised extreme learning machine referred rkelm rkelm introduces kernel function random selection mechanism improve performance algorithm. aiming reducing size output matrix hidden layer support vectors rkelm randomly selected training number support vectors limited less number neurons hidden layer. traditional easily aﬀected noise fast speed; sparse representation classiﬁcation good ability resist noise speed many advantages. designed extreme learning machine method based adaptive sparse representation image classiﬁcation called ea-src easrc combines employs regularization mechanism improve generalization performance. sake optimal regularization parameter selection adopts leave-onecross validation scheme. addition considering reducing computational complexity ea-src uses reduce dimensions proposed extreme learning machine method transfer learning mechanism called tl-elm diﬀerent traditional tl-elm asks diﬀerence domain knowledge domain knowledge must small possible. solving optimization problem output weights got. imbalanced data classiﬁcation problem wang proposed distributed weighted extreme learning machine referred dwelm handle data imbalanced data dwelm draws mapreduce framework sample weighting method. aiming data stream classiﬁcation containing concept drift mirza designed meta-cognitive online sequential extreme learning machine called mos-elm mos-elm development os-elm uses online sequential learning method learn data stream deal concept drift sliding window adaptively adjusted according accuracy classiﬁcation; sample correctly classiﬁed deleted sliding window; misclassiﬁed added sliding window smote algorithm executed retrain classiﬁer. rough mathematical tool analyze data proposed pawlak deal imprecise inconsistent incomplete information eliminate redundant attributes feature sets without preliminary prior information widely used pattern recognition image processing biological data analysis expert systems ﬁelds recent years researchers investigated methods combine rough neural network better classiﬁcation models kothari applied rough theory architecture unsupervised neural network proposed algorithm uses kohonen learning rule train neural network. azadeh proposed integrated data envelopment analysis-artiﬁcial neural network-rough algorithm assessment personnel eﬃciency ﬁrst uses rough determine many candidate reductions calculates performance neural network reduct; best reduct selected results though dea. proposed hybrid intelligent system combining rough artiﬁcial neural network predict failure ﬁrms reduction algorithm called reduction designed; reduction method rough utilized eliminate irrelevant redundant attributes scans samples delete samples inconsistent decisions; last association rules extracted data; ahn’s hybrid classiﬁcation model instance matches association rules instance classiﬁed association rules; otherwise algorithm data reduced reduction method train classiﬁer classify instance. introduced rough rule granular extreme learning machine called rrgelm; rrgelm uses rough extract association rules number neurons hidden layer decided number association rules; weights input layer randomly generated determined conditions whether instances covered association rules not. works promoted developments rough neural network. however models utilize rough reduce attributes cost training rough neural networks large. inspired models classiﬁcation method combined extreme learning machine rough referred relm proposed paper. relm input weights biases hidden layer randomly generated training divided parts upper approximation lower approximation train upper approximation neurons lower approximation neurons. addition attributes reduction introduced eliminate inﬂuence redundant attributes classiﬁcation results. input weights biases hidden layer randomly generated output weights analytically determined relm overcome disadvantages conventional neural networks fast training speed. relm utilizes rough divide data upper approximation lower approximation uses upper approximation lower approximation train upper boundary neurons lower boundary neurons correspondingly. every neuron relm cantains neurons upper boundary neuron lower boundary neuron. ﬁnal classiﬁcation result decided kind neurons. reduction preprocess data according data dose need prior knowledge. using rough relm remove redundant attributes without information loss improve performance proposed extreme learning machine algorithm. rough sets neural networks fuse rough sets well. diﬀerent existing algorithms rough reduce attributes determine number neurons hidden layer relm uses results data divided rough train diﬀerent kinds neurons; preferably combines rough extreme learning machine. rest paper organized follows. section basic concepts principles rough reviewed. section introduces proposed algorithm describes implementation process principle relm. section relm comparison algorithms evaluated data sets results analysed detail. finally conclusions stated section section give description necessary preparatory knowledge paper. firstly look essence extreme learning machine introduces steps elm. rough reviewed describes basic concepts principles rough detail. single-hidden layer feedback neural network; input weights biases hidden layer nodes randomly selected output weights analytically determined least square method. fast speed good generalization ability successfully applied attribute reduction rough claims objects attribute value belong decision class otherwise violates principle classiﬁcation consistency means basic concepts u/rb certainly assigned means basic concepts u/rb possibly assigned means sure whether basic concepts belong not. boundary region empty indicates crisp; attributes contain knowledge crisper concepts boundary region represents uncertainty degree knowledge; greater boundary greater uncertainty knowledge order measure uncertainty rough following deﬁnition introduced. section give introduction structure relm describes basic principles relm. train hidden nodes relm using rough theory also found section. detailed execution steps relm seen algorithm relm development diﬀerent conventional neurons relm rough neurons trained data divided rough set. rough divide universe distinct parts lower approximation upper approximation set. relm neuron contains neurons upper approximation neuron trained upper approximation lower approximation neuron trained lower approximation set. input weights biases upper approximation neurons lower approximation neurons randomly generated. output weights upper approximation neurons lower approximation neurons analytically determined method elm. classiﬁcation result relm decided outputs upper approximation neurons lower approximation neurons although training process kinds neurons relatively independent. known relm closely combines rough sets division result universe rough used guide learning process relm. relm kind based uncertainty measure eﬀectively analyze imprecise inconsistent incomplete information. structure relm showed fig.. algorithm relm important training rough neurons. fig. known kinds neurons lower approximation neuron upper approximation neuron. neuron actually contains lower approximation neuron upper approxilet input weights connecting input neurons upper approximation neurons biases upper approximation neurons. upper approximation neurons relm trained suppose test data ytest hlower hupper output matrices lower approximation neurons upper approximation neurons ytest correspondingly. ﬁnal output matrices weight balance output matrices lower approximation neurons upper approximation neurons. lower output target matrix lower approximation neurons upper number neurons hidden layer plays important role rlm. number neurons hidden layer relm large overﬁtting; small ﬁtting problem appear. relm uses dividing result data rough determine number neurons hidden layer. data larger positive region presents attributes good ability distinguish data better relm determine small larger boundary region indicates attributes divide data well trend choose large number neurons parameters predeﬁned user mean weights positive region boundary region correspondingly determining obvious decided according division data self words decreases dependence empirical knowledge; number neurons hidden layer determined reduce blindness selecting certain extent. train upper approximation neurons using xupper obtain output weights train lower approximation neurons using xlower obtain output weights calculate ﬁnal output matrices hidden layer testing data hupper hlower algorithm steps relm diﬀerent other. neurons relm rough neurons trained based division data rough relm classiﬁcation method based uncertainty measure; method uncertainty measure good advantage dealing inconsistent incomplete information number neurons hidden layer also decided information provided rough need much experience knowledge. utilizing output results upper approximation neurons lower approximation neurons relm make full information provided rough sets better classiﬁcation result. section demonstrate eﬀectiveness proposed method comparison algorithms data sets. verify capabilities algorithm choose celm cselm delm melm randomsampleelm selm comparison algorithms algorithms experiments real data sets data sets website manmade data sets generated platform descriptions real data sets seen data sets website give brief introduction man-made data sets. information data sets seen table rough analyse categorical data numerical data sets ﬁxed data sets discretized. discretization method equal interval discretization number intervals number labels data set. order test eﬃciency relm section choose celm cselm delm melm randomsampleelm selm comparison algorithms. relm c=.; activation function chosen sigmoid radbas tribas sine hardlim. parameters relm comparison algorithms tables test results time overheard also showed tables table obvious relm better celm cselm delm melm randsampleelm selm relm data sets; relm gets highest accuracy data sets gets second best biodeg data set; loses delm cselm melm selm horse glass biodeg lungcncer yeast data sets; results indicate approximation ability relm eﬀective classiﬁcation task. table time overheard relm comparison algorithms. analyzing data known proposed algorithm much advantage time overheard least time-consuming algorithm data sets words relm time-consuming algorithm. according theory fast speed time consumed rough method. combining data table shows rough improve performance proposed algorithm; therefore classiﬁcation task real-time requirement worth considering using relm. testing eﬀect activation function performance relm choose sigmoid radbas tribas sine hardlim activation functions. every data tested activation functions relm tested data sets. number neurons test results showed table table conclude accuracies relm diﬀerent diﬀerent activation functions. activation function sigmoid relm gets best accuracies; activation function tribas relm gets best accuracies; activation function hardlim relm gets best accuracies; test results proposed algorithm well experimental data sets choosing radbas sine activation function best accuracy radbas best accuracy sine. every data standard deviations accuracies also diﬀerent activation function. example activation function sigmoid standard deviation activation function radbas changes similar situations found data sets. test results obvious activation function great impact performance relm; activation functions best results diﬀerent data sets also diﬀerent select activation function depending experimental data set. users much empirical knowledge choosing activation functions sigmoid tribas hardlim seems good initial selection experimental data sets. table known number neurons hidden layer signiﬁcant impact performance relm. numbers neurons hidden layer diﬀerent test results also diﬀerent. sizes experimental data sets large data sets best results small analyzing test results conclude performance proposed algorithm increase increase number neurons hidden layer. number neurons hidden layer large small performance algorithm decreased. reason phenomenon large cause structure relm complex rlem over-ﬁtting training data; small cause target classiﬁcation model cannot eﬀectively approximated relm under-ﬁtting appear. combining tables found relm trend choose small condition guaranteeing performance; indicates method determining number neurons hidden layer eﬃcient. order test reduction mechanism relm execute proposed algorithm relm without reduction mechanism data sets; number neurons hidden layer activation function hardlim relm urelm tested times results showed tables fig.. fig. found performances relm urelm ﬂuctuation main reason input weights biases randomly generated number neurons hidden layer ﬁxed; randomness results ﬂuctuation relm’s performance. analysing curves fig. relm better urelm cases. results table average accuracies executing results. results table obvious performance relm signiﬁcantly better urelm experimental data sets results indicate reduction mechanism relm improve performance relm. data table reduction results relm. table relm remove redundant attributes without changing distinguish ability condition attributes; conclude reduction mechanism eﬀective relm. however reduction mechanism gets abnormal reduction result hepatitis data set. hepatitis data attributes condition attributes removed redundant attributes. reason reduction mechanism produces result dependence condition attributes decision attributes large. words relative positive region decrease removing condition attribute; reduction algorithm removed condition attributes. order test impact data dimensions time overheard choose horse biodeg lungcancer germany parkinsons vehicle segement hypelane waveform experimental data sets; activation function radbas; number neurons hidden layer algorithm rough reduction method comparison algorithm. time overheard showed fig.-. fig.-fig. time overhead relm rs+bp. subgraph graph local graph change trend relm’s time overhead. fig.-fig. seen time overhead relm rs+bp increases increase data dimensions. rs+bp time-consuming algorithm time overhead rs+bp much relm. relm utilized mechanism cost relm keeps lower level although time expenditure also showing signiﬁcant increasing trend. word time complexity relm less sensitive data dimension rs+bp. paper proposed extreme learning machine algorithm rough method called relm. relm utilizes data division result rough train upper approximation neurons lower approximation neurons; output weights analytically determined. ﬁnal classiﬁcation result decided kinds neurons. addition attribute reduction method introduced remove redundant attributes. experimental results showed relm eﬀective algorithm. however experiments found performance relm seems unstable; variances accuracies somewhat large data sets. fig.-fig. seen time cost relm almost increases exponentially. improve performance’s stability relm reduce time complexity research directions future work. work supported national natural science fund china open program state laboratory software architecture china postdoctoral science foundation foundation liaoning educational committee research center online education", "year": 2017}