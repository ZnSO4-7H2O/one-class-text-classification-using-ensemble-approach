{"title": "Learning Markov networks with context-specific independences", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Learning the Markov network structure from data is a problem that has received considerable attention in machine learning, and in many other application fields. This work focuses on a particular approach for this purpose called independence-based learning. Such approach guarantees the learning of the correct structure efficiently, whenever data is sufficient for representing the underlying distribution. However, an important issue of such approach is that the learned structures are encoded in an undirected graph. The problem with graphs is that they cannot encode some types of independence relations, such as the context-specific independences. They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set, in contrast to conditional independences that must hold for all its assignments. In this work we present CSPC, an independence-based algorithm for learning structures that encode context-specific independences, and encoding them in a log-linear model, instead of a graph. The central idea of CSPC is combining the theoretical guarantees provided by the independence-based approach with the benefits of representing complex structures by using features in a log-linear model. We present experiments in a synthetic case, showing that CSPC is more accurate than the state-of-the-art IB algorithms when the underlying distribution contains CSIs.", "text": "abstract—learning markov network structure data problem received considerable attention machine learning many application ﬁelds. work focuses particular approach purpose called independence-based learning. approach guarantees learning correct structure efﬁciently whenever data sufﬁcient representing underlying distribution. however important issue approach learned structures encoded undirected graph. problem graphs cannot encode types independence relations context-speciﬁc independences. particular case conditional independences true certain assignment conditioning contrast conditional independences must hold assignments. work present cspc independence-based algorithm learning structures encode context-speciﬁc independences encoding log-linear model instead graph. central idea cspc combining theoretical guarantees provided independence-based approach beneﬁts representing complex structures using features log-linear model. present experiments synthetic case showing cspc accurate state-of-the-art algorithms underlying distribution contains csis. nowadays powerful representation joint probability distributions markov networks. structure markov network encode complex probabilistic relationships among variables domain improving efﬁciency procedures probabilistic inference. important problem learning structure samples drawn unknown distribution. number alternative algorithms purpose developed recent years. approach independencebased approach algorithms follow approach proceed using statistical tests learn series conditional independences data encoding undirected graph. important advantage approach provides theoretical guarantees learning correct structure together efﬁciency gained using statistical tests. recent approaches proceed inducing features data instead undirected graph. features real-valued functions partial variable assignments using functions possible encode complex structures encoded graphs. algorithms follow approach encode structure features log-linear model. unfortunately current algorithms based learning features efﬁcient alternative multiple user deﬁned hyperparameters need performing parameters learning. parameters learning step often intractable requiring iterative optimization runs inference step model iteration. many practical cases underlying distribution problem present context-speciﬁc independences conditional independences hold certain assignment conditioning hold remaining assignments. case encoding structure undirected graph leads excessively dense graphs obscuring csis present distribution resulting computationally expensive computation inference algorithms reason encoding csis log-linear model obscure them achieving sparser models therefore signiﬁcant improvements time space sample complexities work presents cspc independence-based algorithm learning features instead graph order encode csis. algorithm designed adaptation purpose well-known algorithm cspc proceeds ﬁrst generating initial features dataset searches space possible contexts learning csis present underlying distribution. context algorithm elicits csis using statistical tests generalizes current features order encode elicited csis. central idea cspc combining theoretical guarantees provided independence-based approach beneﬁts representing complex structures using features. knowledge algorithm near cspc algorithm since uses statistical tests learn csis. however restricts attention learning distributions represented decomposable markov networks. latter omit competitor experiments. conducted empirical evaluation synthetic data generated known distributions contains csis. experiments prove cspc signiﬁcantly accurate state-of-the-art algorithms example figure shows graph example induced log-linear model. notice features example encode obscured graph. alternatively graphically represented graphs value this figure shows graph induced features encodes figure shows graph induced features encodes ﬁgures gray nodes correspond assignment variable. markov network domain random variables represented undirected graph nodes numerical parameters representation used factorize distribution hammersley-clifford theorem using completely connected sub-graphs potential functions cliques} lower dimension parameterized follows complete assignment domain projection assignment variables clique normalization constant. often used alternative representation log-linear model clique potential represented exponentiated weighted features assignment follows feature partial assignment subset domain given assignment feature said satisﬁed single variable also holds associate indicator function assignment associating value satisﬁed otherwise. markov network induced log-linear model adding edge graph every pair variables appear together subset feature then clique potentials constructed log-linear features obvious csis ﬁner-grained type independences. independences similar conditional independences hold speciﬁc assignment conditioning called context independence. formally deﬁne follows figure graph induced features example graph induced features example graph induced features example gray nodes correspond assignment variable. notice graph figure cannot encode occurs speciﬁc context absent others. edges connect pairs variables conditionally dependent even single choice values variables. since deﬁned speciﬁc context csis cannot encoded together single undirected graph nonetheless structures encoded features example. task algorithms learning graph encodes independences i.i.d. samples unknown underlying distribution that algorithms perform succession statistical independence tests determine truth value conditional independence pearson’s discarding graphs inconsistent test. decision test perform based independences learned varying speciﬁc algorithm. advantage algorithms guarantees learn correct underlying structure three assumptions underlying distribution graph-isomorph independences encoded graph; underlying distribution positive outcomes statistical independence tests correct independences learned subset independences present another advantage using algorithms computational efﬁciency polynomial running time also avoiding need performing parameters learning. efﬁciency gained computational cost test proportional number rows number variables involved tests. perhaps best known algorithm follows approach created learning structure bayesian networks. correct assumptions described above tests correct produce errors removing edges algorithm tests independence among variables conditioning subsets adjacencies variables. learning markov networks ﬁrst algorithm follows approach gsmn efﬁcient algorithm computes tests constructing structure learning adjacencies variable; using grow-shrink algorithm recent algorithm improves gsmn ibmap-hc learns structure performing hill-climbing search space graphs looking maximizes ib-score score posterior probabilities graphs hill-climbing search starts empty structure adding edges reaching local maxima ibmap-hc relaxes assumption correctness statistical tests improving gsmn sample complexity reducing cascade effect incorrect tests. encodes csis generalizing iteratively features. this cspc decomposes search space csis nested spaces space possible contexts context space possible csis. first cspc generates initial features searches spaces nested loops outer loop explores space contexts; inner loop elicits data csis using statistical tests generalizes features according elicited csis. initial features must generated starting point whole algorithm. alternative generate features correspond initial fully connected graph similar fashion adding feature possible complete assignment variable case size initial exponential respect number variables. reason cspc uses optimal initial features adding feature unique example often used alternative guarantees generalized features algorithm match least training example. discovering csis present data cspc explores complete contexts found dataset unique training example. given context features cspc decides csis elicit similar fashion variable markov network induced subset features satisﬁes context xx\\xa. then adjacent induced graph subset adjacencies taken order deﬁne conditional independence independence obtained contextualizing conditioning using context finally present data encoded current features. eliciting csis data propose straightforward adaptation traditional independence test. similar adaptation proposed central idea adaptation arbitrary seen conditional independence conditional distribution tested using non-contextualized test sample drawn conditional distribution practice generalization features used cspc encode csis present data. speciﬁc encoded features loglinear factorizing features correspond conditional distribution factorization done using recently proposed adaptation well known hammersley-clifford theorem csis called context-speciﬁc hammersley-clifford theorem features correspond subset features satisfy context denoted given features factorized sets features obtained removing variable obtained removing variable formally {xa} generalization consists factorizing features features satisﬁed context factorization features results sets features shown figure features obtained removing features obtained removing finally features correctly encodes shown figure notice features figure features shown example section presents explanation cspc puts pieces together. pseudocode shown algorithm input algorithm receives domain variables dataset algorithm starts generating initial features. then space contexts explored. context current features generalized using generalization algorithm subroutine. subroutine described algorithm consists elicitation csis features generalization steps. input subroutine receives current features context domain variables dataset features log-linear model returned. algorithm step elicitation csis follows strategy trying independences smallest number variables conditioning set. this conditioning variable consists subset size adjacencies terminating subsets smaller strategy avoiding effect incorrect tests practice quality statistical tests decreases exponentially number variables involved step features generalization made elicited. step input current features partitioned sets features satisfy features satisfy then satisﬁed features factorized according context-speciﬁc hammersleyclifford theorem. finally features deﬁned joining allow proper experimental design range well-understood conditions evaluated approach artiﬁcially generated datasets. testing effectiveness approach propose speciﬁc class deterministic models presents controlled number csis. evaluation consists parts. ﬁrst part show potential improvements obtained experiment. second part compare cspc state-of-the-art algorithms gsmn ibmap-hc additionally also compare order highlight improvements resulting contextualizing since originally designed learning bayesian networks omitting step edges orientation generated artiﬁcial data gibbs sampling class models similar example generalized distributions discrete binary variables. chose models since representative case distribution controlled number csis. demonstrate learning csis represents important improvement quality learned distributions compared alternative representation graphs. scenario algorithms lead excessively dense graphs obscures underlying csis. considered models variables maximum cliques size. since complexity structure learning grows exponentially size maximum clique literature algorithms typically tested models maximum cliques size underlying structure fully connected graph nodes plus node model pairs variables \\{xf context-speciﬁc independent given context instead variables remain dependent. variables underlying structure contains contextual independences form given structure deﬁned log-linear model contains sets features pairwise features encodes dependence rest variables \\{xf triplet features variables resulting features generated different models varying numerical parameters. parameters generated satisfy log-odds ratio order strong dependencies model parameters pairwise features forced satisfy following wφwφ ratio wφwφ symmetric respectively since ratio unknowns choose sampled solved. parameters triplet features forced satisfy parameters generated using procedure used pairwise features. parameters forced satisfy following factorization pairwise features already deﬁned. experiments datasets generated sampling log-linear models using rao-blackwellized gibbs sampler chains burn-in sampling. used synthetic datasets explained learn structure parameters algorithms. synthetic data together executable version cspc competitors publicly available. fair comparison pearson’s statistical independent test algorithms signiﬁcance level ibmap-hc algorithm alternatively works using bayesian test margaritis particular dataset evaluated algorithms training sizes varying order obtain number samples sufﬁcient satisfying csis underlying distribution proposed. lost learned distribution used approximate underlying distribution equal zero better learned models lower values measure. since algorithms learn structure markov network complete distribution obtained learning numerical parameters. case algorithms features induced maximum cliques graph learned. learning parameters computed pseudo-loglikelihood using available version libra toolkit. pseudologlikelihood without regularization avoid sparsity ﬁnal model interested measuring quality structure learning step. ﬁrst experiment shows potential improvement obtained generated datasets terms generated data. measure figure obtained learning parameters three proposed structures empty structure fully connected structure iii) underlying structure. distribution learned empty structure informs impact encoding incorrect independences measure. consequently fully connected structure shows impact measure obtained incorrect dependences obscuring real csis present data. underlying structure contains features exactly encodes csis proposed model described section iv-a. ﬁgure shows average standard deviation generated datasets training sizes varying different domain sizes order better show differences among show scale. results empirically distribution obtained learning parameters underlying structure always signiﬁcantly better obtained using empty fully structures. notice logarithmic difference representing differences orders magnitude non-logarithmic space. results differences orders magnitude cases orders magnitude case however trend zero varying size training data. gibbs sampler exact method generation training data. result also seen well increases fully structure better empty structure. second experiment compare obtained cspc gsmn ibmap-hc results shown figure datasets experiment shown figure cspc accurate algorithm cases lower near differences cspc competitors orders since clearly affected quality structure wanted determine whether actual structures correct. reporting average feature length learned models since known value underlying model experiment. statistical measure useful analyzing structural quality log-linear models shown several recent works figure reports values experiment shown above. horizontal line graphs shows exact feature length underlying structure cspc perform better cases showing always nearest number average feature length horizontal line. consistent results shown figure gsmn increases average number features length well number datapoints grows domain sizes. trend learning dense structures well number datapoints grows reaching fully structure. example case gsmn algorithms shows trend reach fully structure shown figure similar fully structure shown figure also well increases difference average feature length cspc competitors also increases. also consistent results shown figure surprising result shown ibmap-hc show trend gsmn seen lower number datapoints algorithm learns fully structures however higher number datapoints algorithm learns empty structure average feature length equal argue bayesian nature ibmap-hc works optimizing posterior probability structures hill-climbing search. using large amount data ibmap-hc seems prone getting stuck empty structure local minima almost structures except correct finally additional result show table average feature length features satisﬁed value variable result shown algorithms running increasing number variables ﬁxed number datapoints discriminating different columns value expected values cspc near near comparison rest competitors. summary results support theoretical claims demonstrate efﬁciency cspc learning distributions csis. figure potential improvements obtained learning parameters underlying structure fully empty structures. average standard deviation repetitions increasing number datapoints training domain sizes figure comparison obtained learning parameters cspc gsmn ibmap-hc average standard deviation repetitions increasing number datapoints training domain sizes figure comparison average feature length obtained cspc gsmn ibmap-hc average standard deviation repetitions increasing number datapoints training domain sizes average feature length solution underlying structure horizontal line. paper proposed cspc independence-based algorithm learning features instead graph. cspc overcomes inefﬁciency traditional algorithms learning csis data representing log-linear model. cspc proceeds generalizing iteratively initial features order represent csis present data exploring possible contexts eliciting data csis usings statistical tests generalizing features according elicited csis. experiments synthetic case show approach accurate state-of-the-art algorithms underlying distribution contains csis. directions future work include adapting efﬁcient algorithms learning csis; validation real world datasets; comparison state-of-the-art non-independence-based approaches adding moore lee’s ad-trees speeding execution statistical tests etc. bromberg schl¨uter edera independencebased markov networks structure discovery international conference tools artiﬁcial intelligence http//ai.frm.utn.edu.ar/fschluter/p/d.pdf. haaren davis markov network structure learning randomized feature generation approach proceedings twenty-sixth national conference artiﬁcial intelligence. aaai press boutilier friedman goldszmidt koller context-speciﬁc independence bayesian networks proceedings twelfth international conference uncertainty artiﬁcial intelligence. morgan kaufmann publishers inc. fierens context-speciﬁc independence directed relational probabilistic models inﬂuence efﬁciency gibbs sampling european conference artiﬁcial intelligence bromberg margaritis honavar efﬁcient markov network structure discovery using independence tests journal artiﬁcial intelligence research vol.", "year": 2013}