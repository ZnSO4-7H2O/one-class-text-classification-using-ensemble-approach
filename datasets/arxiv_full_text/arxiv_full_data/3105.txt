{"title": "Distributed Low-rank Subspace Segmentation", "tag": ["cs.CV", "cs.DC", "cs.LG", "stat.ML"], "abstract": "Vision problems ranging from image clustering to motion segmentation to semi-supervised learning can naturally be framed as subspace segmentation problems, in which one aims to recover multiple low-dimensional subspaces from noisy and corrupted input data. Low-Rank Representation (LRR), a convex formulation of the subspace segmentation problem, is provably and empirically accurate on small problems but does not scale to the massive sizes of modern vision datasets. Moreover, past work aimed at scaling up low-rank matrix factorization is not applicable to LRR given its non-decomposable constraints. In this work, we propose a novel divide-and-conquer algorithm for large-scale subspace segmentation that can cope with LRR's non-decomposable constraints and maintains LRR's strong recovery guarantees. This has immediate implications for the scalability of subspace segmentation, which we demonstrate on a benchmark face recognition dataset and in simulations. We then introduce novel applications of LRR-based subspace segmentation to large-scale semi-supervised learning for multimedia event detection, concept detection, and image tagging. In each case, we obtain state-of-the-art results and order-of-magnitude speed ups.", "text": "vision problems ranging image clustering motion segmentation semi-supervised learning naturally framed subspace segmentation problems aims recover multiple low-dimensional subspaces noisy corrupted input data. low-rank representation convex formulation subspace segmentation problem provably empirically accurate small problems scale massive sizes modern vision datasets. moreover past work aimed scaling low-rank matrix factorization applicable given nondecomposable constraints. work propose novel divide-and-conquer algorithm largescale subspace segmentation cope lrr’s non-decomposable constraints maintains lrr’s strong recovery guarantees. immediate implications scalability subspace segmentation demonstrate benchmark face recognition dataset simulations. introduce novel applications lrr-based subspace segmentation large-scale semisupervised learning multimedia event detection concept detection image tagging. case obtain state-of-the-art results order-of-magnitude speed ups. visual data though innately high dimensional often reside close union low-dimensional subspaces. subspaces might reﬂect physical constraints objects comprising images video trajectories rigid objects naturally occurring variations production subspace segmentation techniques model classes data recovering bases multiple underlying subspaces applications include image clustering segmentation images video motion afﬁnity graph construction semi-supervised learning here input matrix datapoints drawn multiple subspaces k·k∗ nuclear norm column norms parameter trades penalties. segments columns subspaces using solution along extensions nnlrs admits strong guarantees correctness strong empirical performance clustering graph construction applications. however standard algorithms solving unsuitable large-scale problems sequential nature reliance repeated computation costly truncated svds. much computational burden solving stems nuclear norm penalty known encourage low-rank solutions might hope leverage large body past work parallel distributed matrix factorization improve scalability lrr. unfortunately techniques tailored optimization problems losses constraints decouple across entries input matrix. decoupling requirement violated problem constraint non-decomposable constraint introduces algorithmic analytic challenges arise decomposable matrix factorization problems. address challenges develop analyze evaluate provably accurate divide-and-conquer approach large-scale subspace segmentation speciﬁcally accounts non-decomposable structure problem. contributions three-fold algorithm introduce parallel divide-and-conquer approximation algorithm suitable largescale subspace segmentation problems. scalability achieved dividing original problem computationally tractable communication-free subproblems solving subproblems parallel combining results using technique randomized matrix approximation. algorithm call dfc-lrr based principles divide-factor-combine framework decomposable matrix factorization cope non-decomposable constraints lrr. analysis characterize segmentation behavior algorithm showing dfc-lrr maintains segmentation guarantees original algorithm high probability even enjoying substantial speedups namesake. analysis features signiﬁcant broadening original theory treat richer class lrr-type subproblems arise dfc-lrr. moreover since ultimate goal subspace segmentation matrix recovery theory guarantees correctness substantial reduction problem complexity work applications ﬁrst present results face clustering synthetic subspace segmentation demonstrate dfc-lrr achieves accuracy comparable fraction time. propose validate novel application methodology large-scale graph-based semi-supervised learning. used construct afﬁnity graphs semi-supervised learning past prior attempts failed scale sizes real-world datasets. leveraging favorable computational properties dfc-lrr propose scalable strategy constructing subspace afﬁnity graphs. apply methodology variety computer vision tasks multimedia event detection concept detection image tagging demonstrating order magnitude improvement speed accuracy exceeds state art. remainder paper organized follows. section ﬁrst review low-rank representation approach subspace segmentation introduce novel dfc-lrr algorithm. next present theoretical analysis dfc-lrr section section highlights accuracy efﬁciency dfc-lrr variety computer vision tasks. present subspace segmentation results simulated real-world data section section present novel application dfc-lrr graph-based semi-supervised learning problems conclude section notation given matrix rm×n deﬁne compact singular value decomposition rank diagonal matrix non-zero singular values rm×r rn×r associated left right singular vectors denote orthogonal projection onto column space robust subspace segmentation problem observe matrix rm×n columns datapoints drawn multiple independent subspaces column-sparse outlier matrix. goal identify subspace associated column despite potentially gross corruption introduced important observation task projection matrix space sometimes termed shape iteration matrix block diagonal whenever columns multiple independent subspaces hence achieve accurate segmentation ﬁrst recovering space approach seeks recover space solving convex optimization problem presented importantly solution comes guarantee correctness column space exactly equal space whenever certain technical conditions moreover show work also well-suited construction afﬁnity graphs semisupervised learning. setting goal deﬁne afﬁnity graph nodes correspond data points edge weights exist nodes drawn subspace. thus used recover block-sparse structure graph’s afﬁnity matrix afﬁnities used semi-supervised label propagation. present scalable divide-and-conquer algorithm called dfc-lrr lrr-based subspace segmentation. dfc-lrr extends principles framework non-decomposable problem. dfc-lrr algorithm summarized algorithm next describe step detail. step divide input matrix submatrices dfc-lrr randomly partitions columns lcolumn submatrices ct}. simplicity assume divides evenly. step factor submatrices parallel dfc-lrr solves subproblems parallel. subproblem form input matrix used dictionary subset columns used observations. typical algorithm easily modiﬁed solve return low-rank estimate factored form. step combine submatrix estimates dfc-lrr generates ﬁnal approximation ˆzproj low-rank solution projecting onto column space column projection technique commonly used produce randomized low-rank matrix factorizations also employed dfc-proj algorithm runtime noted many state-of-the-art solvers nuclear-norm regularized problems like per-iteration time complexity rank-km truncated required iteration. dfc-lrr reduces per-iteration complexity signiﬁcantly requires time subproblem. performing subsequent column projection step relatively cheap computationally since solver return solution factored form. indeed deﬁne maxi column projection step dfc-lrr requires time. despite signiﬁcant reduction computational complexity dfc-lrr provably maintains strong theoretical guarantees algorithm. make statement precise ﬁrst review technical conditions accurate space recovery required lrr. alternative formulation involves replacing instances resulting low-rank estimate would dimensions step dfc-lrr would compute low-rank approximation block-diagonal matrix diag. analysis relies quantities rank clean data matrix coherence singular vectors combine properties single deﬁnition deﬁnition -coherence). matrix rm×n -coherent rank intuitively coherence small information well-distributed across rows matrix space easier recover outlier corruption. using properties established following recovery guarantee lrr. theorem suppose rm×n supported columns coherent independent column support range range solution returned lrr. exists constant column space exactly equals space whenever words exactly recover space even constant fraction columns corrupted outliers. rank coherence shrink grows allowing greater outlier tolerance. main theoretical result shows that high probability conditions guarantee accuracy dfc-lrr also exactly recovers space recall independent subspace setting accurate space recovery tantamount correct segmentation columns proof result generalizes analysis broader class optimization problems adapts analysis found appendix. theorem failure probability conditions ˆzproj solution returned dfc-lrr. exists constant column space ˆzproj exactly equals thm. establishes that like dfc-lrr tolerate constant fraction data points corrupted still recover correct subspace segmentation clean data points high probability. number although uses notion column coherence analyze work closely related notion -coherence ease notation proofs. moreover note rank-r matrix rm×n supported columns column coherence r)-coherent. datapoints large solving directly prohibitive dfc-lrr need solve collection small tractable subproblems. indeed thm. guarantees high probability recovery dfc-lrr even subproblem size logarithmic corresponding reduction computational complexity allows dfc-lrr scale large problems little sacriﬁce accuracy. notably column sampling complexity better established matrix factorization setting require columns sampled requires worst case columns matrix completion robust matrix factorization. explore empirical performance dfc-lrr variety simulated real-world datasets ﬁrst traditional task robust subspace segmentation next complex task graph-based semi-supervised learning. experiments designed show effectiveness dfc-lrr theory section holds violated. synthetic datasets satisfy theoretical assumptions rank incoherence small fraction corrupted columns real-world datasets violate criteria. experiments inexact augmented lagrange multiplier algorithm base algorithm. subspace segmentation experiments regularization parameter values sugsuggested prior work. experiments report parallel running times dfc-lrr i.e. time longest running subproblem plus time required combine submatrix estimates column projection. experiments implemented matlab. simulation studies architecture using single core main memory real data experiments performed architecture equipped .ghz -core main memory. ﬁrst verify dfc-lrr produces accuracy comparable signiﬁcantly less time synthetic real-world settings. focus standard robust subspace segmentation task identifying subspace associated input datapoint. construct synthetic robust subspace segmentation datasets ﬁrst generate datapoints independent r-dimensional subspaces manner similar subspace independently select basis uniformly matrices rm×r orthonormal columns matrix rr×ns independent entries distributed uniformly form matrix rm×ns samples subspace uiti rm×kns given outlier fraction next generate additional independent outlier samples denoted rm×no. outlier sample independent entries average absolute value entries original samples. create input matrix rm×n random permutation columns ﬁrst experiments regularizer vary fraction outliers. measure frequency dfc-lrr able recover space identify outlier columns using criterion deﬁned figure shows average performance trials. dfc-lrr performs quite well gaps phase transitions dfc-lrr small sampling columns virtually non-existent sampling columns figure shows corresponding timing results accuracy results presented figure timing results show substantial speedups dfc-lrr relative modest tradeoff accuracy denoted figure note report timing results values dfc-lrr successful trials i.e. success rate equaled figure moreover figure shows timing results using parameter values except ﬁxed fraction outliers variable number samples subspace i.e. ranges timing results also show speedups minimal loss accuracy figure trade-off computation segmentation accuracy face recognition experiments. results obtained averaging across independent runs. time dfc-lrr varying number subproblems. segmentation accuracy experiments. next demonstrate comparable quality increased performance dfc-lrr relative real data namely subset extended yale database standard face benchmarking dataset. following experimental setup frontal face images human subjects chosen resized pixels forms -dimensional feature vector. noted previous work low-dimensional subspace effectively used model face images person hence face clustering natural application subspace segmentation. moreover illustrated figure signiﬁcant portion faces dataset corrupted shadows hence collection images ideal benchmark robust subspace segmentation. feature vector representation images create dictionary matrix dfc-lrr parameter next resulting low-rank coefﬁcient contains left singular vectors afﬁnity matrix compute afﬁnity matrix matrix used cluster data clusters spectral embedding followed k-means. following comparison different clustering methods relies segmentation accuracy. clusters assigned label based majority vote ground truth labels points assigned cluster. evaluate clustering performance dfclrr computing segmentation accuracy i.e. cluster assigned label based majority vote ground truth labels points assigned cluster. segmentation accuracy computed averaging percentage correctly classiﬁed data classes. figures show computation time segmentation accuracy respectively dfclrr varying numbers subproblems relatively-small data requires minutes converge. dfc-lrr demonstrates roughly linear computational speedup function comparable accuracies smaller values quite gradual decrease accuracy larger graph representations samples vertices weighted edges express afﬁnity relationships samples crucial various computer vision tasks. classical graph construction methods separately calculate outgoing edges sample. local strategy makes graph vulnerable contaminated data outliers. recent work computer vision illustrated utility global graph construction strategies using graph laplacian matrix low-rank based regularizers. regularization also effectively used encourage sparse graph construction building upon success global construction methods noting connection subspace segmentation graph construction described section present novel application low-rank representation methodology relying dfc-lrr algorithm scalably yield sparse low-rank graph present variety results large-scale semi-supervised learning visual classiﬁcation tasks provide detailed comparison leading baseline algorithms. columbia consumer video content detection compiled stimulate research recognizing highlydiverse visual content unconstrained videos dataset consists youtube videos semantic categories three popular audio/visual features extracted. multimedia event detection video corpus consists multimedia videos average duration minutes used detecting speciﬁc semantic events. event videos provided positive examples remainder videos null videos correspond event. work keep positive examples sample null videos resulting dataset videos. extract features video ﬁrst sampled frames accumulated obtain video-level nus-wide-lite image tagging nus-wide among largest available image tagging benchmarks consisting crawled images flickr associated user-provided tags. ground-truth images manually provided selected concept tags. generate lite version sampling images. image wavelet texture block-wise lab-based color moments visual words extracted normalized ﬁnally concatenated form single feature representation image. three graph construction schemes evaluate described below. note exclude baselines graph l-graph either scalability concerns prior work already demonstrated inferior performance relative algorithm deﬁned knn-graph construct nearest neighbor graph connecting vertex nearest neighbors terms distance speciﬁed feature space. exponential weights associated edges i.e. cheng proposed noise-resistant l-graph encourages sparse vertex connectedness motivated work sparse representation subsequent work entitled sparse probability graph enforced positive graph weights. following approach implemented variant solving following optimization problem sample slr-graph novel graph construction method contains two-steps ﬁrst dfc-lrr performed entire data recover intrinsic low-rank clustering structure. treat resulting low-rank coefﬁcient matrix afﬁnity matrix sample samples largest afﬁnities selected form basis matrix used solve optimization described problem resulting non-negative coefﬁcients used deﬁne graph. benchmarking dataset ﬁrst construct graphs treating sample images/videos vertices using three algorithms outlined section create weighted edges vertices. fair comparison parameter settings namely slr-graph. moreover knn-graph tuning range given graph structure perform semi-supervised label propagation using efﬁcient label propagation algorithm enjoys closed-form solution often achieves state-of-the-art performance. perform separate label propagation category benchmark i.e. series binary classiﬁcation label propagation experiments ccv/med experiments nus-wide-lite. category randomly select half samples training points remaining half test set. repeat process times category different random splits. finally compute mean average precision based results test sets across runs label propagation. ﬁrst performed experiments using benchmark smallest datasets explore tradeoff computation accuracy using dfc-lrr part proposed slr-graph. figure presents time required slr-graph versus dfc-lrr three different numbers subproblems figure presents corresponding accuracy results. ﬁgures show dfc-lrr performs comparably smaller values performance gradually degrades larger moreover dfc-lrr orders magnitude faster achieves superlinear speedups relative lrr. given scalability issues modest-sized dataset along comparable accuracy dfc-lrr slr-graph exclusively dfc-lrr larger datasets. table summarizes results semi-supervised learning experiments using three graph construction techniques deﬁned section results show proposed slr-graph approach leads signiﬁcant performance gains terms across benchmarking datasets vast majority features. results demonstrate beneﬁt enforcing low-rankedness sparsity graph construction. moreover conventional low-rank oriented algorithms e.g. would computationally infeasible benchmarking datasets thus highlighting utility employing dfc’s divide-and-conquer approach generate scalable algorithm. primary goal work introduce provably accurate algorithm suitable large-scale low-rank subspace segmentation. contemporaneous work also aims scalable subspace segmentation method offers guarantee correctness. contrast dfc-lrr provably preserves theoretical recovery guarantees program. moreover divide-and-conquer approach achieves empirical accuracy comparable state-ofthe-art methods obtaining linear superlinear computational gains standard subspace segmentation tasks novel applications semi-supervised learning. dfc-lrr also lays groundwork scaling derivatives known offer improved performance e.g. latlrr setting standard subspace segmentation nnlrs graph-based semi-supervised learning setting. techniques prove useful developing scalable approximations convex formulations subspace segmentation e.g. cand`es wright. robust principal component analysis? journal cheng wang huang yan. multi-task low-rank afﬁnity pursuit image segmentation. iccv gear. multibody grouping motion images. int. comput. vision august gemulla nijkamp haas sismanis. large-scale matrix factorization distributed stochastic gradient hastie simard. metrics models handwritten character recognition. statistical science w.-s. zheng b.-g. x.-w. kong. nonnegative sparse coding discriminative semi-supervised learning. kumar mohri talwalkar. sampling-based approximate spectral decomposition. icml ganesh wright chen fast convex optimization algorithms exact recovery robust subspace segmentation low-rank representation. icml yan. exact subspace segmentation outlier detection low-rank representation. arxiv. yan. latent low-rank representation subspace segmentation feature extraction. iccv yan. active subspace toward scalable low-rank learning. neural computation mackey talwalkar jordan. divide-and-conquer matrix factorization. nips recht. simpler approach matrix completion. arxiv.v recht r´e. parallel stochastic gradient algorithms large-scale matrix completion. optimization online tomasi kanade. shape motion image streams orthography. international journal computer torresani szummer fitzgibbon. efﬁcient object category recognition using classemes. eccv vidal sastry. generalized principal component analysis ieee trans. pattern anal. mach. intell. wang zhang. label propagation linear neighborhoods. icml wang wang zhang shen quan. linear neighborhood propagation applications. ieee trans. proof thm. rests upon three results deterministic recovery guarantee lrr-type problems generalizes guarantee probabilistic estimation guarantee column projection established probabilistic guarantee showing uniformly chosen submatrix -coherent matrix nearly -coherent. results presented secs. respectively. proof thm. follows sec. follows unadorned norm represents spectral norm matrix. also make technical condition introduced ensure corrupted data matrix well-behaved used dictionary deﬁnition matrix β-rwd thm. analyzes recovery constraint observation matrix dictionary equal input matrix next theorem provides comparable analysis observation matrix column submatrix dictionary. theorem suppose rm×n β-rwd rank independent column support range range rm×l column submatrix supported columns suppose corresponding column submatrix shows that high probability column projection exactly recovers coherent matrix sampling number columns proportional corollary rm×n -coherent rm×l matrix columns sampled uniformly without replacement. following lemma shows that high probability captures full rank coherence much larger lemma rm×n -coherent rm×l matrix columns sampled uniformly without replacement. log/ǫ ﬁxed constant larger solution dfc-lrr subproblem. index deﬁne event r)-coherent event event column space matrix equal space choice thm. implies holds realized. hence hold indices column space precisely equals space median rank ˆzt} equals dfc-lrr partitions columns uniformly random variable hypergeometric distribution therefore satisﬁes hoeffding’s inequality hypergeometric distribution proof thm. parallel thm. begin introducing oracle constraints would guarantee desired outcome satisﬁed. lemma assumptions thm. suppose matrices additionally satisfy oracle constraints derive sufﬁcient conditions solving moreover show solution satisﬁes oracle constraints solutions require additional notation. matrix rn×l deﬁne rr×l rn×r} orthogonal projection onto orthogonal projection onto orthogonal complement matrix column support deﬁne column normalized version satisﬁes binary matrix selects columns feasible problem hence optimal solution must exist. explicitly constructing dual certiﬁcate show also solves subproblem", "year": 2013}