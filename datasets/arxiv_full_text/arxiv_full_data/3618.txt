{"title": "Wide Compression: Tensor Ring Nets", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep neural networks. Our results show that our TR-Nets approach {is able to compress LeNet-5 by $11\\times$ without losing accuracy}, and can compress the state-of-the-art Wide ResNet by $243\\times$ with only 2.3\\% degradation in {Cifar10 image classification}. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.", "text": "deep neural networks demonstrated state-of-theart performance variety real-world applications. order obtain performance gains networks grown larger deeper containing millions even billions parameters thousand layers. tradelarge architectures require enormous amount memory storage computation thus limiting usability. inspired recent tensor ring factorization introduce tensor ring networks signiﬁcantly compress fully connected layers convolutional layers deep neural networks. results show tr-nets approach able compress lenet- without losing accuracy compress state-of-the-art wide resnet degradation cifar image classiﬁcation. overall compression scheme shows promise scientiﬁc computing deep learning especially emerging resourceconstrained devices smartphones wearables devices. deep neural networks made signiﬁcant improvements variety applications including recommender systems time series classiﬁcation nature language processing image video recognition accuracy improvements require developing deeper deeper networks evolving alexnet googlenet -layer resnet -layer wideresnet densenets unfortunately evolution architecture comes signiﬁcant increase number model parameters. mobile phones wearables devices etc. applications storage memory test runtime complexity extremely limited resources compression areas thus essential. prior work observed redundancy trained neural networks useful area research compression network layer parameters vast majority research focused compression fully connected layer parameters latest deep learning architectures almost entirely dominated convolutional layers. example alexnet parameters convolutional layers wide resnet parameters convolutional layers. necessitates techniques factorize compress multi-dimensional tensor parameters convolutional layers. propose compressing deep neural networks using tensor ring factorizations viewed generalization single canonical polyadic decomposition extensions exact formulation described detail section note also generalization tensor train factorization includes ﬁrst extension. inspired previous results image processing demonstrate general factorization technique extremely expressive especially preserving spatial features. speciﬁcally introduce tensor ring nets layers deep neural network compressed using tensor ring factorization. fully connected layers compress weight matrix investigate different merge/reshape orders minimize real-time computation memory needs. convolutional layers carefully tucker factorization shown exhibit good performance data representation compressing fully connected layers deep neural networks tucker decomposition approach applied compress fully connected layers convolution layers. tensor train representation another example factorizes tensor boundary matrices order tensors demonstrated capability data representation deep learning model compared multi-dimensional data completion showing intermediate rank expressive motivating generalization. paper investigate deep neural network compression. compress ﬁlter weights distort spatial properties mask. since mask dimensions usually small compress along dimensions instead compress along input output channel dimensions. verify expressive power formulation train several compressed networks. first train lenet- lenet- mnist dataset compressing lenet- without degradation achiving accuracy compressing lenet-- degrading additionally examine state-ofthe-art -layer wide-resnet cifar used effectively compress wide-resnet decay performance obtaining accuracy. compression results demonstrates capability compress state-of-the-art deep learning models resources constrained applications. section discusses related work neural network compression. compression model introduced section discusses general tensor ring factorizations speciﬁc application fully connected convolutional layers. compression method convolutional layers novelty previous papers extend factorization-based compression methods beyond fully connected layers. finally show experimental results improve upon state-of-the-art compressibility withsigniﬁcant performance degradation section conclude future work section past deep neural network compression techniques largely applied fully connected layers previously dominated number parameters model. however since modern models like resnet wideresnet moving toward wider convolutional layers omitting fully connected layers altogether important consider compression schemes work fronts. many modern compression schemes focus postprocessing techniques hashing quantization strength methods applied addition compression scheme thus orthogonal methods. similar work novel representations like circulant projections truncated representations low-rank tensor approximation deep neural networks widely investigated literature effective model compression generative error fast prediction speed tensor networks recently drawn considerable attention multidimensional data representation deep learning figure tensor decompositions. tensor diagrams four popular tensor factorization methods decomposition tucker decomposition tensor train decomposition tensor ring decomposition used paper. shown viewed generalization section also compare tucker decomposition compression schemes. figure tensor diagrams. left graphical representation length vector matrix order i×i×i tensor right factorized forms product matrix product rows columns respectively tensor product along common axis. explicitly tensor product bottom right orders i-th element tensor diagrams figure introduces popular tensor diagram notation represents tensor objects nodes axes edges undirected graph. edge connecting nodes indicates multiplication along axis dangling edge shows axis remaining product dimension given edge weight. compact notation useful representing various factorization methods merge ordering computation complexity paper measured ﬂops number ﬂops construct depends sequence merging detailed analysis schemes given appendix resulting following conclusions. several interpretations made observations. first though different merge orderings give different counts worst choice expensive best choice. however since make kind choice note since every merge order combination hierarchical sequential merges striving toward hierarchical merging good heuristic minimize count. thus paper always strategy. summation equivalent feed-forward layer shape takes ﬂops. additionally summation equivalent anfeed-forward layer shape takes ﬂops. analysis demonstrates layer separation approach tensor ring equivalent low-rank matrix factorization fully-connected layer thus reducing computational complexity relatively smaller batch size testing samples. here compression beneﬁt computation; large converges io/) large small signiﬁcant. additionally though expensive reshaping step grows cubically grow batch size; conversely multiplication quadratic paper parameter selected picking small large achieve optimal since needs small enough computation saving. convolutional layer compression figure merge ordering. order tensor merged factored form either hierarchically sequentially note computational complexity forming generally expensive discussed detail appendix computational cost computational cost depends order merging note need fully construct tensor tensor representation sufﬁcient obtain reduce computational cost layer separation approach profigure convolutional layer. dashed lines show convolution operation here decompose decompose dashed line represent convolution operation expressed note i×i×i decompose number channels entering layer figure decompose feature dimension entering layer. note second scenario compression storage requirements lose gain computational complexity design tradeoff. experiments factorize higher order tensors order achieve gain model compression. initialization general nonconvex optimization choice initial variables dramatically effect quality model training. particular found initializing parameter randomly gaussian distribution effective carefully chosen variance. initialize tensor factors drawn i.i.d. merging factors merged tensor elements mean variance amount parameters uncompressed limit also gaussian. since latter distribution works well training uncompressed models choosing value initialization well-motivated observed necessary good convergence. cost tensor ring decomposition applied onto kernel tensor factorizes order tensor four tensors. purpose maintain spatial information kernel tensor factorize spatial dimension merging spatial dimension order tensor tensor multiplication along slice count convolution count tensor multiplication along slices count also equivalent three-layer convolutional networks without non-linear transformations convolutional layer feature maps feature maps patch contains convolutional layers feature maps feature maps patch convolutional layer feature maps feature maps patch. common sub-architecture choice deep cnns like inception module googlenets without nonlinearities convolution layers. complexity employ ratio complexity layer complexity tensor ring layer quantify capability reducing computation parameter costs small ﬁlter sizes often case deep neural networks image processing often direct multiplication compute convolution efﬁcient using problem order io)) ﬂops. therefore consider direct multiplication baseline. datasets lenet-- lenet- mnist resnet wideresnet cifar cifar. networks trained using tensorﬂow experiments lenet implemented nvidia gpus experiments resnet wideresnet implemented nvidia titan gpus. cases tensor ring rank used networks networks trained randomly initialization using proposed initialization method. overall show compression scheme give signiﬁcant compression gains small accuracy loss even negligible compression gains accuracy loss. goal compressing lenet-- network assess effectiveness compressing fully connected layers using trns; name suggests lenet-- contains hidden fully connected layers output dimension output layer dimension table gives parameter settings lenet-- original form tensor factored form. compression rate greater achieved reduction computational complexity typical choices. table shows performance results mnist classiﬁcation original model compressed models using matrix factorization trns. accuracy loss compress accuracy loss compress note also matrix factorization compression performs worse compression suggesting high order structure helpful. note also rank tucker approximation equivalent rank matrix approximation compressing fully connected layer. investigate compression convolutional layers small network. lenet- convolutional neural networks convolution layers followed fully connected layers achieves error rate mnist. dimensions compression given table wider network much greater potential compression positive compression rate whenever however reduction complexity limited occurs however performance experiment still positive. setting compress lenet- lower error rate original model well tucker factorization approach. also require reduction count incur error still quite reasonable many real applications. finally evaluate performance tensor ring nets cifar cifar image classiﬁcation tasks here input images colored size belonging object classes respectively. overall images training images testing. table gives dimensions resnet after compression. similar reshaping scheme used wideresnet. note resnet compression gain wideresnet bound closer suggesting high compression potential. results given table demonstrates trns able signiﬁcantly compress resnet wideresnet tasks. picking resnet gives compression ratio tucker compression method almost performance lift cifar almost lift cifar compared uncompressed model performance degradation datasets. compression wideresnet even successful suggesting trns well-suited extremely overparametrized models. compression trns give better performance cifar uncompressed resnet decay uncompressed wideresnet. cifar decay increases wideresnet achieves lower error uncompressed resnet overfewer parameters. compared tucker compression method compression rate trns incur performance degradation datasets table fully connected compression. dimensions three-fully-connected layers uncompressed trn-compressed models. computational complexity includes tensor product merging feed-froward multiplication table fully connected results. lenet-- mnist datase trained epochs using minibatch size trained random weight initialization. adam used optimization. testing time samples. compression ratio. learning rate. table small convolution compression. dimensions lenet- layers original form trn-compressed computational complexity includes tensor product merging convolution operation convolution table small convolution results. lenet- mnist dataset trained epochs using minibatch size adam used optimization. testing time samples. compression ratio. learning rate. tucker incurs performance degradation. compressibility even signiﬁcant wideresnet achieve performance tucker compression trns compress cifar cifar. tradeoff runtime; table large convolution compression. dimensions layer resnes cifar dataset. resblock includes sequence input batch normalization relu convolution layer batch normalization relu convolution layer. input length inserted beginning unit. details. table large convolution results. -layer resnet -layer wide-resnet cifar dataset cifar dataset trained epochs using minibatch size model trained using momentum decaying learning rate. compression ratio. evolution figure shows train test errors training compressed resnet cifar classiﬁcation task various choices also compared tucker tensor factorization. particular note generalization particularly high tucker tensor factorization method trns much smaller. generalization error ﬁnal train test errors improve upon tucker method suggesting trns easier train. introduced tensor ring factorization approach compress deep neural networks resource-limited devices. inspired previous work shown tensor rings high representative power image completion tasks. results show signiﬁcant compressibility area future work reduction computational complexity. repeated reshaping needs fully connected convolutional layers computational overhead especially moderately large. tradeoff reasonable considering considerable compressibility gains appropriate memorylimited applications especially training ofﬂoaded cloud. additionally believe actual wall-clocktime decrease tensor-speciﬁc hardware low-level routines continue develop–we observe example numpy’s function considerably optimized tensorﬂow’s tensordot. overall believe promising compression scheme open doors using deep learning much ubiquitous computing environment.", "year": 2018}