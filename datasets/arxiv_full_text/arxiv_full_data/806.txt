{"title": "Learning to Repeat: Fine Grained Action Repetition for Deep  Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.", "text": "sahil sharma aravind lakshminarayanan balaraman ravindran indian institute technology madras chennai india {sahil ravi}cse.iitm.ac.in aravindsrinivasgmail.com reinforcement learning algorithms learn complex behavioral patterns sequential decision making tasks wherein agent interacts environment acquires feedback form rewards sampled traditionally algorithms make decisions i.e. select actions execute every single time step agent-environment interactions. paper propose novel framework fine grained action repetition enables agent decide action well time scale repeating figar used improving deep reinforcement learning algorithm maintains explicit policy estimate enabling temporal abstractions action space. empirically demonstrate efﬁcacy framework showing performance improvements three policy search algorithms different domains asynchronous advantage actor critic atari domain trust region policy optimization mujoco domain deep deterministic policy gradients torcs racing domain. reinforcement learning used solve goal-directed sequential decision making problems wherein explicit supervision form correct decisions provided agent evaluative feedback form rewards sampled environment. algorithms model goal-directed sequential decision making problems markov decision processes however problems exponential continuous state space tabular algorithms maintain value policy estimates every state become infeasible. therefore need able generalize decision making unseen states. recent advances representation learning deep neural networks provide efﬁcient mechanism generalization combination representation learning deep neural networks reinforcement learning objectives shown promising results many sequential decision making domains atari domain mnih schaul mnih mujoco simulated physics tasks domain lillicrap robosoccer domain torcs domain mnih often settings consist agent interacting environment discrete time steps. common feature shared deep reinforcement learning algorithms repeatedly execute chosen action ﬁxed number time steps represents action taken time step said algorithms general aik+ aik+ action repetition allows algorithms compute action every time steps hence operate higher speeds thus achieving real-time performance. also offers advantages smooth action policies. importantly shown lakshminarayanan durugkar macro-actions constituting action repeated times could interpreted introducing temporal abstractions induced policies thereby enabling transitions temporally distant advantageous states. figure figar induces temporal abstractions learnt policies. arrows indicate action executed frames numbers depict number time steps action repeated. thunder bolt corresponds ﬁring action. arrow alongside thunderbolt corresponds action ﬁgure agent learns execute operation traveling passes executes temporally elongated actions complete task skillfully avoiding frame. ﬁgure agent catches glimpse pink opponent towards bottom right frame executes temporally elongated actions intercept kill time scale action repetition largely static algorithms schaul lakshminarayanan ﬁrst explore dynamic time scales action repetition setting show leads signiﬁcant improvement performance atari games. however choose time scales experiments limited representative games. moreover method limited tasks discrete action space. propose figar framework enables algorithm regardless whether action space continuous discrete learn temporal abstractions form temporally extended macro-actions. figar uses structured factored representation policy whereby policy choosing action decoupled action repetition selection. note deciding actions action repetitions independently enables temporal abstractions without blowing action space unlike vezhnevets lakshminarayanan contribution work twofold. first propose generic extension algorithms coming factored policy representation temporal abstractions second empirically demonstrate figar’s efﬁciency improving policy gradient algorithms improvements performance several domains atari games asynchronous advantage actor critic tasks mujoco simulated physics tasks domain trust region policy optimization torcs domain deep deterministic policy gradients framework centered general idea deciding necessary. similar ideas outside domains. instance satija pineau explore real time neural machine translation action every time step decide whether output token target language based current context. transition point dynamic programming algorithm modiﬁcation tabular dynamic programming paradigm reduce learning time memory required control continuous stochastic dynamic systems. done determining transition points underlying mdp. policy changes transition point states. algorithm learns optimal transition point states using variant q-learning evaluate whether add/delete particular state transition points. figar learns transition points underlying generalization across state space unlike tpdp tabular infeasible large problems. dynamic frameskip deep q-network proposes multiple time scales action repetition augmenting deep network separate streams primitive actions corresponding time scale. time scale action repetition dynamically learned. although framework leads signiﬁcant improvement performance atari games suffers able support multiple time scales potential explosion action space restricted discrete action spaces. durugkar also explore learning macro-actions composed using action repeated different time scales. however framework limited discrete action spaces performance improvements signiﬁcant. learning temporally extended actions abstractions interest long time. vezhnevets propose strategic attentive writer learning macro-actions building dynamic action-plans directly reinforcement learning signals. instead outputting single action observation straw maintains multi-step action plan. agent periodically updates plan based observations commits plan replanning steps. although straw framework represents general temporal abstraction figar figar seen framework compliment straw whereby decision repeat could hierarchical plan base action levels. figar framework structured policy representation time scale execution could thought parameterizing chosen action. work explores parameterized policies hausknecht stone discrete actions parameterized continuous values. case discrete/continuous actions parameterized discrete values. state spaces atari also sophisticated kind explored hausknecht figar also naturally connected semi-mdps framework. smdps mdps durative actions. assumption smdps actions take holding time complete mahadevan dietterich typically modeled distributions corresponding next state transition corresponding holding time denotes number time steps current action policy next action policy. rewards entire holding time action credit assigned picking action. framework naturally durative actions policy structure decision consists choice action time scale execution. therefore convert original smdp trivially. fact give structure smdp clear repeat chosen action holding time happens holding time speciﬁed smdp framework. think part policy outputs probability distribution time scales holding time distribution. therefore framework naturally smdp deﬁnition action repetition rate characterizing holding time. also rewards holding time appropriate discounting factor smdp framework. actor critic algorithms execute policy gradient updates maintaining parametric estimates policy value function value function estimates used reduce variance policy gradient updates. asynchronous advantage actor critic learns policies based asynchronous n-step returns. learner threads execute copies policy asynchronously parameter updates sent central parameter server regular intervals. ensures temporal correlations broken subsequent updates since different threads possibly explore different parts state space parallel. objective function policy improvement estimate return time step algorithm uses n-step returns estimating biased estimate hence think estimate represents advantage taking action state value function updated using n-step error estimate n-step return current state. j-step returns used ﬁxed hyper-parameter. simplicity assume deﬁnition trpo policy optimization algorithm. constrained optimization surrogate loss function proposed theoretical guarantees monotonic policy improvement. trpo surrogate loss function potential next policies according deterministic policy gradient theorem gradient performance objective deterministic policy continuous action spaces respect policy parameters given appropriately deﬁned performance objective model built according theorem consists actor outputs action vector continuous action space critic model evaluates action chosen state. ddpg algorithm extends algorithm introducing non-linear neural network based function approximators actor critic. figar provides algorithm ability model temporal abstractions augmenting ability predict number time steps action chosen execution repeated. prediction conditioned current state environment. state time action taken time action policy action network realizing action policy objective function improving construct action repetition policy figar-z. repetition network output size action repetition policy evaluated return policy components. policy choosing actions policy choosing action repetitions. algorithm describes generic framework deriving algorithm algorithm stand action repetitions would able perform. tradition algorithms constant. implies action repetition static ﬁxed. figar action repetitions choose w··· w|w|}. central idea behind figar objective function used update parameters θaof maintained used update parameters action repetition policy well ﬁrst subsection desribe operates. next sub-sections describe instantiations figar extensions policy gradient algorithms trpo ddpg. figar operates following procedure describes figar variant navigates solving ﬁrst state seen predicts tuple action execute number time steps execute decided based whereas decided based tuple known action decision. denote state agent action decisions made. similarly denote action repetition action chosen action decisions. note w··· w|w|} allowed action repetitions. figar-ac uses represent policy value function respectively. vector size equal action space underlying scalar. figar extends algorithm follows deﬁned previous sub-section addition figarac deﬁnes neural network fθx. neural network outputs |w|-dimensional vector representing probability distribution elements sampled time scale multinomial distribution decides long action decided repeated. actor composed deﬁne action repeated times state encountered. note return used target based decision steps steps potential change actions executed takes place. based time steps. note point implies action space extended dimension |w|. factored representation figar policy number parameters blow extend action space naive coupling actions action repetitions would suffering kind action-space blowseen vezhnevets wherein able control respect different action repetition levels would need model |a|×|w| actions action-values would blow ﬁnal layer size times. although generic enough output continuous discrete actions consider discrete action spaces. preserving notation previous subsection describe figar-trpo consider case output generated network dimensional dimension independent describing continuous valued action. stochastic policy hence modeled multi-variate gaussian diagonal co-variance matrix. parameters mean well co-variance matrix together represented concatenated mean-covariance vector represented function figar-trpo constructed follows parameters sub-network computes action repetition distribution. implies figar-trpo combination operator deﬁned algorithm sense scalar multiplication. controls relative learning rate core-policy parameters action repetition parameters. denotes gaussian distribution action executed denotes multinomial softmax-based action repetition probability distribution. controls relative divergence corresponding policies. appendix explanation loss function used. subsection present extension ddpg figar framework. ddpg consists denotes deterministic policy vector size equal action space underlying mdp; denotes critic network whose output single number estimated state-action value function figar framework extends ddpg algorithm follows introduced similar figar-ac. implies complete policy figarddpg computed tuple neural networks similar ddpg figar-ddpg loss function actor. actor receives gradients critic. actors proposed policy directly critic critic provides actor gradients proposed policy follows improvement. figar-ddpg total policy concatenation vectors hence gradients total policy also simply concatenation gradients policies ensure sufﬁcient exploration exploration policy action repetition \u0001-greedy version behavioral action repetition policy. action part policy continues temporally correlated noise exploration generated ornstein-uhlenbeck process details). different algorithms figar extensions learn dynamic action repetition? figar impact performance different algorithms various tasks? figar able learn control several different kinds action repetition sets next three sub-sections experiment simplest possible action repetition |w|}. fourth sub-section understand effects changing action repetition policies learnt. perhaps important hyper-parameter depicts conﬁdence ability agent predict future. choice depend domain agent operating. wanted demonstrate ability figar learn temporal abstractions hence instead tuning optimal chosen arbitrarily. speciﬁc time scales choose figar-ac well trained million decision steps. evaluated terms ﬁnal policy learnt. treating score obtained algorithm baseline calculated percentage improvement offered figarac figure plots metric versus game names. improvement enduro atlantis staggering respectively. figure y-axis clipped make presentable. appendix contains experimental details scores obtained methods. appendix contains experiments validating setup. answer ﬁrst question posed experiments conducted record percentage times particular action repetition chosen. figure presents action repetition distribution across selection games chosen arbitrarily. values rounded decimal places hence game. game played episodes using policy used calculate average scores figure tables together show figar-ac generally prefers lower action repetition come temporal abstractions policy space abstractions demonstrated figure temporal abstractions always help general gameplay however seen figure figar-ac outperforms games. could potentially think figar deep exploration framework using learnt policy predicting actions every time step completely discarding action-repetition policy evaluation time. appendix contains empirical argument usage figar demonstrates temporal abstractions encoded indeed important game play performance. full policy trained jointly. policies learnt trpo optimization step compared current best known policy arrive overall best policy. results sub-section best policy. table compares performance trpo figar-trpo. number brackets average action repetition chosen. seen table figar learns either policies much faster execute albeit cost slight loss optimality learns policies similar non-repetition case performance competitive baseline algorithm. best policy evaluated episodes arrive average scores contained table trpo difﬁcult baseline mujoco tasks domain. whole figar outperforms trpo domains although gains marginal tasks. appendix contains experimental details. video showing figar-trpo’s learned behavior policies found http//youtu.be/jiaotbth-k. figar-ddpg torcs figar-ddpg trained tested torcs domain. chosen arbitrarily. figar-ddpg manages complete race task ﬂawlessly manages ﬁnish laps circuit simulator stops. total reward obtained figar-ddpg obtained ddpg. also observed figar-ddpg learnt policies smoother learnt ddpg. video showing learned driving behavior figar-ddpg agent found https//youtu.be/dxj-sf-wx. appendix experimental architectural details. sub-section answers third question raised beginning section afﬁrmative. demonstrate nothing sacrosanct action repetitions figar-ac performed well good performance carries action repetition sets. demonstrate generality figar respect chose wide variety action repetition sets trained evaluated figar-ac variants learn repeat respect respective action repetition sets. table describes various figar-variants considered experiments terms action repetition note hyper-parameters various variants figar-ac tuned rather ones obtained tuning figar- used. table contains comparison scores obtained various figar-ac variants comparison baseline. clear figar able learn action repetition performance fall even hyper-parameters tuned figar- used variants. appendix propose light-weight framework improving current deep reinforcement learning algorithms policy optimization whereby temporal abstractions learned policy space. framework generic applicable algorithms concerned policy gradients continuous well discrete action spaces trpo ddpg. figar maintains structured policy wherein action probability distribution augmented probability distribution choosing time scale repeating chosen action. results demonstrate figar used signiﬁcantly improve current policy gradient actor-critic algorithms thereby learning better control policies across several domains discovering optimal sequences temporally elongated macro-actions. atari torcs mujoco represent environments largely deterministic minimal degree stochasticity environment dynamics. highly deterministic environments would expect figar agents build latent model environment dynamics hence able execute large action repetitions without dying. exactly highly deterministic environment like game freeway. figure demonstrates chicken able judge speed approaching cars appropriately cross road manner takes goal without colliding cars time avoiding narrowly. said that certainly ability stop action repetition general would important especially stochastic environments. setup consider ability stop executing macro-action agent committed however necessary skill event unexpected changes environment executing chosen macro-action. thus stop start actions stopping committing macro-actions added basic dynamic time scale setup robust policies. believe modiﬁcation could work general stochastic worlds like minecraft leave future work. used open source implementation https//github.com/miyosuda/ async_deep_reinforce. thank volodymr mnih giving valuable hyper-parameter information. thank aravind rajeswaran helpful discussions regarding feedback mujoco domain tasks. trpo implementation modiﬁcation https//github.com/aravindr/robustrl. ddpg implementation modiﬁcation https//github.com/yanpanlau/ddpg-keras-torcs. thank ilds compute resources used running experiments. references marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research june matthew hausknecht prannoy mupparaju sandeep subramanian shivaram kalyanakrishnan peter stone. half ﬁeld offense environment multiagent learning teamwork. aamas adaptive learning agents workshop kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature february volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning emanuel todorov erez yuval tassa. mujoco physics engine model-based control. ieee/rsj international conference intelligent robots systems ieee alexander vezhnevets volodymyr mnih simon osindero alex graves oriol vinyals john agapiou strategic attentive writer learning macro-actions. advances neural information processing systems used lstm-variant algorithm figar-ac experiments. async-rmsprop algorithm used updating parameters hyper-parameters mnih initial learning rate used linearly annealed million steps. used n-step returns entropy regularization used encourage exploration similar mnih entropy regularization found hyper-parameter tuning action-policy action repetition policy fθx. alien amidar assault asterix atlantis bank heist beam rider bowling breakout centipede chopper command crazy climber demon attack enduro freeway frostbite gopher james bond kangaroo koolaid krull kung master name game phoenix pong q-bert road runner quest space invaders star gunner time pilot tutankhamun wizard since atari games tend quite complex jointly learning factored policy random weight initializations proved less optimal compared stage-wise approach. approach followed training figar-ac ﬁrst train networks using regular ac-objective function. stage trains action part policy value function small number iterations ﬁxed action repetition rate next stage train entire architecture jointly. kind non-stationary training objective ensures good value function estimator good action policy estimator start training full policy jointly. every time figar decides execute action time steps step action selection made. since number time steps action repeated variable training time measured terms action selections carried out. ﬁrst stage training executed million action selections next stage executed million steps. comparison baseline million steps since large entropy regularization required explore components policy-space also ends meaning policies learnt diffused would like evaluation done every million steps followed strategy similar \u0001-greedy. probability action action repetition drawn output distribution probability action maximum probability selected. evaluation done episodes steps whichever smaller arrive average score. table contains scores obtained ﬁnal figar-ac policies atari games. numbers inside brackets depict conﬁdence interval conﬁdence threshold calculated averaging scores episodes. table contains scores competing method straw learns temporal abstractions maintaining action plans subset games figar straw trained tested. note scores obtained straw agents averages performing replicas. infer tables figar straw competitive other figar clearly out-performing straw breakout straw clearing outperforming figar frostbite. figure demonstrates evolution performance figar-ac versus training progress. also contains corresponding metrics facilitate comparisons. episode long evaluation phase also keep track best episodic score. also plot best episode’s score versus time idea learnt policy compared best could been. used level architecture mnih turn uses level architecture mnih except pre-lstm hidden layer size instead mnih similar mnih actor critic share layer. hence ﬁnal layer same. networks different ﬁnal layer softmax-non linearity output non-linearity model multinomial distribution output linear. note appendix state evaluating policy learnt agent simply chose sample output probability distributions probability chose optimal action/action repetition probability choice might seem rather arbitrary. hence conducted experiments understand well agent performs shift choosing maximal action towards sampling output distributions figure demonstrates performance figar-ac deteriorate signiﬁcantly comparison even always sample policy distributions games. cases signiﬁcant deterioration believe diffused nature policy distributions learnt. hence although choice evaluation scheme might seem arbitrary fact reasonable. figure average performance plotted probability sample ﬁnal policy distribution atari points toward left side sub-graph depict average performance greedy version policy towards right side depict performance stochastic version policy. previous discussion leads novel trade-off game-play performance versus speed. figure demonstrated although figar-ac learns temporally elongated macro-actions favor shorter actions many games. since action repetition distribution diffused sampling distribution help figar choose larger action repetition rates probably cost optimality game play. table demonstrates exactly figar does. generated playing episodes steps whichever lesser recording fraction times action repetition chosen. policy used populating table stochastic policy contrast table table expanded version figure alien amidar assault asterix atlantis bank heist beam rider bowling breakout centipede chpr crzy clmbr attk enduro pong freeway frostbite gopher james bond kangaroo koolaid krull kung phoenix pong q-bert road runner quest invdr star gunner time pilot tutankham figure table created using .-greedy policy described previous subsection. reason compare stochastic policy .-greedy version instead fully-greedy version policy would deterministic would good evaluations. hence seen figar learns trade-off optimality game-play speed choosing whether sample policy probability distributions probability thus behave stochastically behave .-greedily sample distributions small probability. table compared figure understand stochasticity ﬁnal policy affects action repetition chosen. clear trend seen games wherein stochastic variant ﬁnal policy learns longer longer actions albeit small cost loss optimality game-play expanded version figure presented table comparison table explained appendix policy used populating table picks greedy action probability stochastically samples output probability distributions probability alien amidar assault asterix atlantis bank heist beam rider bowling breakout centipede chpr crzy clmbr attk enduro freeway frostbite gopher james bond kangaroo koolaid krull kung phoenix pong q-bert road runner quest invdrs star gunner time pilot tutankham table contains average action repetition chosen games figarvariants. episodes used populate table used table seen games stochastic variant policy learns play higher speed although might result loss optimality game play demonstrated figure alien amidar assault asterix atlantis bank heist beam rider bowling breakout centipede chopper command crazy climber enduro demon attack freeway frostbite gopher james bond kangaroo koolaid krull kung master name game phoenix pong q-bert road runner quest space invaders star gunner time pilot tutankhamun wizard figar-trpo corresponding baseline algorithm operate dimensional feature vector observations. trpo algorithm operates phases. ﬁrst phase trajectories sampled according current behavioral policy create surrogate loss function. second phase policy improvement step performed carrying optimization step surrogate loss function subject kl-divergence constraint policy. experiments policy improvement steps performed. varies learning progress schedule value would take next iteration deﬁned linearly terms return last iteration hence return large previous iteration small number episodes used construct surrogate loss function current iteration. best policy found keeping track average returns seen training phase policy evaluated episodes obtain average score trpo policy learnt. important hyper-parameters figar-trpo βkl. using grid search found optimal hyper-parameters tuned tasks. tanh non-linearity used throughout. mean vector realized using -hidden layer neural network hidden layer sizes standard deviation realized using parameter layer parameterizes standard deviation depend input. hence concatenation output mean network std-dev layer forms action policy described section action repetition function realized using -hidden layer neural network similar mean network albeit smaller hidden layer sizes however output non-linearity softmax layer size dictated value action repetition network kept small ensure figar-trpo signiﬁcantly parameters trpo. mean network std-dev layer act-rep network share parameters layers ρθaθx πθxold fθxold kind splitting probability distributions happens action-policy action-repetition policy independent probability distributions. theoretically sound realize figar-trpo minimize loss lθaoldθxold however found practice optimizing relaxed version objective function lθaoldθxold lθaoldθxold ddpg algorithm also operates low-dimensional feature-vector observations. domain consists continuous actions acceleration break steering. hyper-parameter used main experiments chosen arbitrarily. unlike lillicrap useful batch normalization hence used. however replay memory used size target networks also used soft updates applied sine ddpg off-policy actor-critic method need ensure sufﬁcient exploration takes place. ornstein-uhlenbeck process details) ensured exporation carried action-policy space. ensure exploration action-repetition policy space adopted strategies. first \u0001-greedy version policy used train time. annealed training steps. algorithm training steps baselines well figar-ddpg. second probability instead picking greedy action-repetition sampled output distribution architecture hidden layer non-linearity used relu. hidden layer weights initialized using initialization actor network consisted -hidden layer neural network hidden sizes action vector critic hidden layer network size similar lillicrap actions included hidden layer fθc. ﬁnal output linear trained using td-error objective function similar lillicrap table contains ﬁnal evaluation scores attained various figar variants. figure contains bargraph visualization table demonstrate advantage figar variants relative baselines. could potentially figar evaluation stage actionrepetition rate picking every action according completely discarding learnt repetition policy πθx. figar variant denoted figar-wo-πθx. demonstrate figar-wo-πθx worse figar games hence temporal abstractions learnt encoded indeed non-trivial important gameplay performance. table contains comparison standard figar agent figar-wo-πθx. evaluation scheme appendix alien amidar assault asterix atlantis bank heist beam rider bowling breakout centipede chopper command crazy climber enduro demon attack freeway frostbite gopher james bond kangaroo koolaid krull kung master name game phoenix pong road runner quest space invaders star gunner time pilot tutankhamun wizard section contains results experiments figar-trpo. appendix contains experimental setup same. throughout experiments figar-trpo policy components share representations. appendix contains experimental results setting wherein share layers except ﬁnal one. agent/network denoted name figar-shared-trpo. hyper-parameters appendix except obtained grid-search similar appendix tuned tasks. values hyper-parameters found optimal training evaluation regime appendix used. performance best policy learnt tabulated table figar-shared-trpo whole perform much better figar-trpo. trpo experiments neural networks used rather shallow hidden layers deep. hence believe sharing layers thus leads small gains terms optimality policy learnt.", "year": 2017}