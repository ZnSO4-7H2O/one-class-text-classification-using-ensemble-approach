{"title": "Locally Private Bayesian Inference for Count Models", "tag": ["stat.ML", "cs.CL", "cs.CR", "cs.LG", "cs.SI"], "abstract": "As more aspects of social interaction are digitally recorded, there is a growing need to develop privacy-preserving data analysis methods. Social scientists will be more likely to adopt these methods if doing so entails minimal change to their current methodology. Toward that end, we present a general and modular method for privatizing Bayesian inference for Poisson factorization, a broad class of models that contains some of the most widely used models in the social sciences. Our method satisfies local differential privacy, which ensures that no single centralized server need ever store the non-privatized data. To formulate our local-privacy guarantees, we introduce and focus on limited-precision local privacy---the local privacy analog of limited-precision differential privacy (Flood et al., 2013). We present two case studies, one involving social networks and one involving text corpora, that test our method's ability to form the posterior distribution over latent variables under different levels of noise, and demonstrate our method's utility over a na\\\"{i}ve approach, wherein inference proceeds as usual, treating the privatized data as if it were not privatized.", "text": "aspects social interaction digitally recorded growing need develop privacy-preserving data analysis methods. social scientists likely adopt methods entails minimal change current methodology. toward present general modular method privatizing bayesian inference poisson factorization broad class models contains widely used models social sciences. method satisﬁes local differential privacy ensures single centralized server need ever store non-privatized data. formulate local-privacy guarantees introduce focus limited-precision local privacy—the local privacy analog limited-precision differential privacy present case studies involving social networks involving text corpora test method’s ability form posterior distribution latent variables different levels noise demonstrate method’s utility na¨ıve approach wherein inference proceeds usual treating privatized data privatized. data social processes often take form discrete observations observations often contain sensitive information people involved. aspects social interaction digitally recorded opportunities social scientiﬁc insights grow; however risk unacceptable privacy violations. result growing need develop privacy-preserving data analysis methods. microsoft research york university texas austin. correspondence aaron schein <ascheincs.umass.edu> zhiwei steven <stevenwoogmail.com> mingyuan zhou <mingyuan.zhoumccombs.utexas.edu> hanna wallach <hannadirichlet.net>. current methodology. toward framework differential privacy present method privatizing bayesian inference poisson factorization broad class models learning latent structure discrete data. class contains widely used models social sciences including topic models text corpora genetic population models stochastic block models social networks tensor factorization dyadic data includes deep hierarchical models dynamic models many others. method general modular allowing social scientists build existing derivations implementations non-private poisson factorization. derive method rely novel reinterpretation geometric mechanism well previously unknown general relationship skellam bessel poisson distributions; note results independent interest contexts. method satisﬁes strong variant differential privacy— i.e. local privacy—under sensitive data privatized randomized response method inference. ensures single centralized server need ever store non-privatized data—a condition non-negotiable many real-world settings. challenge introduced local privacy infer latent variables given privatized data. option na¨ıve approach wherein inference proceeds usual treating privatized data privatized. context maximum likelihood estimation na¨ıve approach shown exhibit pathologies observations discrete count-valued; researchers therefore advocated treating non-privatized observations latent variables inferred embrace approach extend bayesian inference form posterior distribution figure topic recovery method na¨ıve approach. generated non-privatized data synthetically true topics known. privatized data using noise level high noise level. heatmap subﬁgure visualizes data using denote positive counts blue denote negative counts. high noise level na¨ıve approach overﬁts noise therefore fails recover true topics. describe experiment detail section latent variables conditioned privatized data randomized response method; method asymptotically guaranteed draw samples posterior distribution. present case studies applying method overlapping community detection social networks topic modeling text corpora. order formulate local-privacy guarantees introduce focus limited-precision local privacy—the local privacy analog limited-precision differential privacy originally proposed flood case study report suite experiments test method’s ability form posterior distribution latent variables different levels noise. experiments also demonstrate utility method na¨ıve approach case studies; provide illustrative example ﬁgure background problem formulation differential privacy. differential privacy rigorous privacy criterion guarantees single observation data signiﬁcant inﬂuence information obtained analyzing data set. subsets range local differential privacy. focus local differential privacy refer local privacy. setting observations remain private even data analysis algorithm. algorithm sees privatized versions observations often constructed adding noise speciﬁc distributions. process adding noise known randomized response—a reference survey-sampling methods originally developed social sciences prior development differential privacy limited-precision local privacy. deﬁnition requires condition hold pairs observations practice notoriously difﬁcult achieve extremely large meaning pair observations arbitrarily different often case data social processes. therefore introduce focus limited-precision local privacy—the local privacy analog limited-precision differential privacy originally proposed flood subsequently used privatize analyses geographic location data ﬁnancial network data although limited-precision local privacy weaker local privacy still provide reasonably strong guarantees. deﬁnition positive integer randomized response method -private pairs observations subsets range data analysis algorithm sees observations’ -private responses data analysis satisﬁes -limited-precision local privacy. limited-precision local privacy implies \u0001-local privacy. geometric mechanism. several standard randomized response methods differential privacy toolbox many involve adding independently generated noise element observation. unfortunately commonly used noise mechanisms—the gaussian laplace mechanisms—are poor choices count data involve real-valued distributions. therefore focus geometric mechanism viewed discrete analog laplace mechanism. geometric mechanism adds noise drawn two-sided geometric distribution element observation. two-sided geometric therefore geometric mechanism parameter -private randomized response method data analysis algorithm sees obsern -private responses data analysis satisﬁes -limited precision local privacy. differentially private bayesian inference. bayesian statistics begin probabilistic model relates observable variables latent variables joint distribution goal inference compute posterior distribution latent variables conditioned observed values posterior almost always analytically intractable thus inference involves approximating common methods approximate bayesian inference variational inference wherein parameters approximating distribution markov chain monte carlo wherein approximate posterior ﬁnite samples generated markov chain whose stationary distribution exact posterior. conceptualize methods randomized algorithm returns approximation posterior distribution general satisfy \u0001-differential privacy. however mcmc algorithm returns single sample posterior guarantees privacy adding noise posterior samples also guarantee privacy though noised samples collectively approximate distribution depends different exact posterior speciﬁc models also noise transition kernel mcmc algorithm construct markov chain whose stationary distribution exact posterior something close guarantees privacy also take analogous approach privatize variational inference wherein noise sufﬁcient statistics computed iteration locally private bayesian inference. ﬁrst formalize general objective bayesian inference local privacy. given generative model non-privatized data latent variables joint distribution assume randomized response method generates privatized data sets bayesian inference form following posterior distribution correctly characterizes uncertainty latent variables conditioned observations assumptions—i.e. privatized data model randomized response method expansion equation shows posterior implicitly treats non-privatized data latent variable marginalizes using mixing distribution posterior characterizes uncertainty given randomized response method. observation generate samples approximate expectation equation assuming already method approximating non-private posterior context mcmc alternating sampling values non-privatized data complete conditional—i.e. )—and sampling values latent variables—i.e. pm)— constitutes markov chain whose stationary distribution pmr. scenarios already derivations implementations sampling need able sample efﬁciently order obtain locally private bayesian inference algorithm; whether depends heavily assumptions note objective bayesian inference local privacy deﬁned equation similar williams mcsherry identify barrier inference unable analytically form next sections show poisson factorization model geometric mechanism analytically form marginal likelihood derive efﬁcient mcmc algorithm asymptotically guaranteed generate samples posterior equation locally private poisson factorization poisson factorization. assume count-valued data set. assume count data independent poisson random variable pois count’s latent rate parameter function latent variables class models known poisson factorization described section includes many widely used models social science. example mixed-membership stochastic block model social networks corresponds case count matrix; non-negative real-valued πcd. similarly latent dirichlet allocation well-known topic model text corpora—corresponds case count matrix; non-negative real-valued matrices φkv. cases standard assume independent gamma priors elements latent matrices comprise facilitates efﬁcient bayesian inference matrices gamma–poisson conjugacy geometric mechanism. focus geometric mechanism natural choice count data. reinterpreting geometric mechanism involving skellam noise deriving general relationship skellam bessel poisson distributions able obtain analytic tractability efﬁcient bayesian inference also maintaining local privacy guarantees. particular show augmenting model auxiliary variables allows analytically form marginal likelihood sample efﬁciently desired. non-privatized count generated model m—i.e. yn∼pois—and privatized follows skellam distribution marginal distribution difference independent poisson random variables pois pois. theorem express generative process three equivalent ways shown ﬁgure provides unique necessary insight. ﬁrst useful showing mcmc algorithm guarantees privacy since two-sided geometric noise existing privacy mechanism. second represents two-sided geometric noise terms pair poisson random variables exponentially distributed rates; doing reveals auxiliary variables facilitate inference. third marginalizes three poisson random variables directly drawn skellam distribution also happens desired marginal likelihood geometric mechanism. derive second third ways theorem deﬁnition skellam distribution additive property poisson random variables. rely previously unknown general relationship skellam bessel poisson distributions derive efﬁcient draw samples pmr. explained section need obtain locally private mcmc algorithm drawing samples latent variables given privatized data provided already draw samples latent variables given non-private data. input mcmc algorithm privatized data theorem consider poisson random variables pois pois. minimum min{y difference deterministic functions however conditioned random variables marginally generated follows present case studies applying method overlapping community detection social networks topic modeling text corpora. case study formulate local-privacy guarantees ground illustrative examples. report suite experiments test method’s ability form posterior distribution latent variables different types data different levels noise. focus synthetic semisynthetic data control effects model mismatch although model mismatch important problem outside scope paper. using synthetic semi-synthetic data also allows vary high-level properties data reference methods. compare performance method references methods non-private poisson factorization non-privatized data non-private poisson factorization privatized data—i.e. na¨ıve approach wherein inference proceeds usual treating privatized data privatized. throughout experiments mcmc reference methods. performance measure. ideally would directly compare method’s posterior distribution na¨ıve approach’s posterior distribution non-private poisson factorization non-privatized data. unfortunately three posteriors analytically intractable. however mcmc approximate posterior ﬁnite samples latent variables instead form expected value respect one—e.g. furthmore focus synthetic semisynthetic data aggregate loss function compare expected values values used generate true value. data deﬁne divergence poisson distribution implied poisson distribution implied comparing value aggregate loss na¨ıve approach ﬁrst truncates negative counts zero thus uses truncated geometric mechanism minimum positive must minimum thus practice means observe difference poisson-distributed counts still recover counts drawing bessel random variable. assuming skel theorem represent explicitly difference latent non-negative counts deﬁne minimum latent counts min{˜y gn}. given randomly initialized latent variables sample value conditional posterior bessel distribution equations constitute draw samples pmr. given sampled draw samples latent variables conditional posteriors non-private poisson factorization. finally also sample equation follows gamma–poisson conjugacy fact exponential prior expressed gamma prior shape parameter equal equations along one—i.e. desired. figure block structure recovery method na¨ıve approach. generated non-privatized data synthetically. privatized data using three different levels noise. depicts data using denote positive counts blue denote negative counts. noise level increases na¨ıve approach overﬁts noise fails recover true values predicting high values even sparse parts matrix. contrast method recovers latent structure even high noise levels. function method value na¨ıve approach value non-private poisson factorization provides proxy measuring divergence posterior distributions non-private poisson factorization. organizations often want know whether employees interacting efﬁciently productively possible. example missing connections employees that present would signiﬁcantly reduce duplication effort? natural communities emerge digitally recorded employee interactions match formal organizational structure? answer questions many organizations want partner social scientists order gain actionable insights based employees’ interactions. however sharing interaction data increases risk privacy violations. moreover standard anonymization procedures reverse-engineered adversarially thus provide privacy guarantees contrast formal privacy guarantees provided differential privacy sufﬁcient employees consent sharing data. limited-precision local privacy. scenario data count matrix element matrix number interactions actor actor single observation data single element. theorem -private precision difference observations less privatized versions indistinguishable provided sufﬁciently small. furthermore privatized version indistinguishable privatized version example interacted three times adversary would unable tell whether interacted provided sufﬁciently small. note adversary would able tell interacted though exact number times. poisson factorization. explained section mixedmembership stochastic block model learning latent overlapping community structure social networks special case poisson factorization count matrix; non-negative real-valued matrices respectively; πcd. factors represent much actors participate communities respectively factor represents much actors community interact actors community standard assume independent gamma priors factors—i.e. gamma shape rate hyperparameters respectively. synthetic data. generated social networks actors communities. randomly generated true parameters θ∗ic π∗cd∼ setting encourage sparsity; exaggerates block structure network. sampled data pois noised three increasing values since magnitude counts figure mean divergence poisson distribution implied ˆµij poisson distribution implied lower better. error bars denote standard deviation across replications. subﬁgure reports results across nine values given setting controls sparsity magnitude count observations. counts grow larger denser performance private methods approaches performance non-private poisson factorization small values method almost always outperforms na¨ıve approach difference especially stark higher noise levels. varied across trials allowed three values vary setting precision empirical mean data setting three values model sampling iterations saving every sample ﬁrst using samples compute ˆµij given equation ﬁgure visually compare estimates method na¨ıve approach three different noise levels estimate non-private method true values µ∗ij. na¨ıve approach overﬁts noise predicting high rates sparse parts matrix. contrast method maintains precise representation data even high noise levels. enron. processed enron corpus obtain adjacency matrix number emails sent actor actor included actor sent least email sent received least hundred emails yielding actors. email included multiple recipients incremented corresponding counts one. non-private poisson factorization data obtained point estimates factors θ∗ic π∗cd. generated semi-synthetic data sets pois ﬁxed π∗cd re-scaled θ∗ic θ∗ic allowing vary. rescaling changes overall interaction rate actor without changing relative frequency participation across communities; allows vary scale sparsity network maintaining realistic community block structure. randomly generated varying parameterization gamma distribution mean allows increase scale density data. generated semi-synthetic data sets three values nine levels noise applied method na¨ıve approach noised data running sampling iterations saving every sample ﬁrst compute ˆµij. ﬁgure report mean topic models widely used social sciences learning latent topics text corpora often characterize high-level thematic structure many settings corpora contain sensitive information people involved result people unwilling consent sharing data without formal privacy guarantees provided differential privacy. limited-precision local privacy. scenario data count matrix element matrix number times word type occurred document similar communitydetection scenario single observation data might correspond single element. case adversary would unable tell whether occurred provided sufﬁciently small. however natural interpretation would assume single observation entire document—i.e. case norm difference documents less privatized versions indistinguishable provided sufﬁciently small. example privatized version email includes sentence hate boss indistinguishable email without sentence. note also natural consider heterogeneous document-speciﬁc precision levels—i.e. leading enable author document choose much noise document sharing example author wanted make sure adversary would unable tell wrote surprise party times email would ﬁrst then achieve would well-known topic model text corpora—is special case poisson factorization count matrix; non-negative real-valued matrices rek= φkv. factor represents much topic used document factor represents much word type used topic again standard assume independent gamma priors factors—i.e. gamma shape rate hyperparameters respectively. synthetic data. generated synthetic data documents topics word types. topics well separated putting majority mass different word types. also ensured documents well separated three equal groups thirty putting majority mass different topic. sampled data θ∗dkφ∗kv. generated heterogeneously-noised data sampling beta distribution mean concentration parameter sampling word type repeated small large value model sampling iterations saving every sample ﬁrst selected posterior sample highest joint probability. note that label-switching cannot average samples following newman aligned topic indices using hungarian bipartite matching algorithm. visualize results ﬁgure na¨ıve approach performs poorly recovering topics high noise case. enron. experiments created corpus treating sent email enron corpus single document. removing stopwords latent dirichlet allocation special case poisson factorization—using mallet default settings obtain point estimate parameters θ∗dk φ∗kv. frequently used topics sub-selected word types used topic documents topic. resultant data included documents word types. previous study used semi-synthetic experimental design allow vary length density documents control model mismatch. trial generated data pois rescaled φ∗kv φ∗kv θ∗dk θ∗dk allowing vary across trials. randomly generated varying generated semi-synthetic data sets three values nine levels noise applied method na¨ıve approach data running sampling iterations saving every sample ﬁrst using samples approximated posterior mean calculated mean divergence poisson distribution implied ˆµdv poisson distribution implied µ∗dv. include plot results appendix; tell similar story shown ﬁgure presented general modular method privatizing bayesian inference poisson factorization broad class models contains widely used models social sciences. method satisﬁes local differential privacy. formulate local-privacy guarantees introduced limited-precision local privacy—the local privacy analog limited-precision differential privacy. finally case studies demonstrated method’s utility na¨ıve approach wherein inference proceeds usual treating privatized data privatized. method able efﬁciently sample values non-privatized data. accomplish introducing auxiliary variables exploiting special relationships bessel skellam poisson distributions obtain sequence closed-form conditional distributions every variable. straightforward alternative approach sample unnormalized density using black-box techniques although conceptually simpler many beneﬁts closed-formedness. importantly algorithm easily built future develop stochastic inference algorithms massive data sets. complete conditionals closed-form exponential families simple recipes translating mcmc algorithms coordinate-ascent variational inference algorithms method complete conditionals well known exponential families except bessel distribution; however show theorem below. allows derive full cavi algorithm—we include derivation algorithm appendix leave future work development stochastic version scale massive data sets. andr´es miguel bordenabe nicol´as chatzikokolakis konstantinos palamidessi catuscia. geoindistinguishability differential privacy locationbased systems. proceedings sigsac conference computer communications security york acm. isbn ----. http//doi.acm.org/./ bernstein garrett mckenna ryan sheldon daniel michael miklau gerome. differentially private learning undirected graphical models using collective graphical models. arxiv preprint arxiv. dimitrakakis christos nelson blaine mitrokotsa aikaterini rubinstein benjamin robust private bayesian inference. international conference algorithmic learning theory dwork cynthia mcsherry frank nissim kobbi smith adam. calibrating noise sensitivity private data analysis. proceedings theory cryptography conference volume flood mark katz jonathan stephen smith adam. cryptography economics supervisory information balancing transparency conﬁdentiality. https//papers.ssrn.com/sol/ papers.cfm?abstract_id=. karwa vishesh slavkovi´c aleksandra krivitsky pavel. differentially private exponential random graphs. international conference privacy statistical databases springer paisley john blei david jordan michael bayesian nonnegative matrix factorization stochastic variational inference. airoldi edoardo blei david erosheva elena fienberg stephen handbook mixed membership models applications papadimitriou antonis narayan arjun haeberlen andreas. dstress efﬁcient differentially private comproceedings putations distributed data. twelfth european conference computer systems eurosys york acm. isbn ----. http//doi.acm.org/./ ramage daniel rosen evan chuang jason manning christopher mcfarland daniel topic modeling social sciences. nips workshop applications topic models ranganath rajesh tang linpeng charlin laurent proceedblei david. deep exponential families. ings international conference artiﬁcial intelligence statistics roberts margaret stewart brandon tingley dustin airoldi edoardo structural topic model applied social science. nips workshop topic models computation application evaluation schein aaron paisley john blei david wallach hanna. bayesian poisson tensor factorization inferring multilateral relations sparse dyadic event counts. proceedings sigkdd international conference knowledge discovery data mining schein aaron zhou mingyuan blei david wallach hanna. bayesian poisson tucker decomposition learning structure international relations. proceedings international conference machine learning wang yu-xiang fienberg stephen smola alex. privacy free posterior sampling stochastic gradient monte carlo. proceedings international conference machine learning yang xiaolin fienberg stephen rinaldo alessandro. differential privacy protecting multi-dimensional contingency table data extensions applications. journal privacy conﬁdentiality zhang zuhe rubinstein benjamin dimitrakakis christos. differential privacy bayesian inference. proceedings aaai conference artiﬁcial intelligence zhou mingyuan. inﬁnite edge partition models overlapping community detection link prediction. proceedings conference artiﬁcial intelligence statistics figure mean divergence poisson distribution implied ˆµdv poisson distribution implied lower better. error bars denote standard deviation across replications. subﬁgure reports results across nine values given setting controls sparsity magnitude count observations. counts grow larger denser performance private methods approaches performance non-private poisson factorization small values method almost always outperforms na¨ıve approach difference especially stark higher noise levels. figure two-sided geometric distribution obtained randomizing parameters skellam distribution ﬁxed paramters skellam distribution asymmetric centered value zero; however two-sided geometric distribution symmetric centered zero. also heavy tailed discrete analog laplace distribution. therefore geometric mechanism parameter -private randomized response method data analysis algorithm sees observations’ -private responses data analysis satisﬁes -limited precision local privacy. skellam distribution marginal distribution difference independent poisson random variables pois pois. proof. two-sided geometric random variable generated taking difference independent identically distributed geometric random variables geometric distribution special case negative binomial distribution shape parameter equal furthermore negative binomial distribution represented mixture poisson distributions gamma mixing distribution. therefore re-express equation follows finally gamma distribution shape parameter equal exponential distribution difference independent poisson random variables marginally skellam random variable relationship bessel skellam distributions theorem consider poisson random variables pois pois. minimum min{y difference deterministic functions however conditioned random variables marginally generated follows derive full coordinate-ascent variational inference algorithm locally private poisson factorization. θdkφkv. standard assume independent gamma priors factors—i.e. denote geometric expected value straightforward factorization joint ﬁrst generates poisson random variables computes remaining variables given deterministic relationships underlying poissons equivalently ﬁrst generate sums poissons thin using multinomial binomial draws. following equation delta functions implicitly present multinomial binomial pmfs. note write θdkφkv. however closely approximated using delta method previously used variational inference schemes approximate intractable expectations particular variable expectation approximately", "year": 2018}