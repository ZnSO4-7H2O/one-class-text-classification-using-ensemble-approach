{"title": "Hyperspectral Image Classification with Support Vector Machines on  Kernel Distribution Embeddings", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We propose a novel approach for pixel classification in hyperspectral images, leveraging on both the spatial and spectral information in the data. The introduced method relies on a recently proposed framework for learning on distributions -- by representing them with mean elements in reproducing kernel Hilbert spaces (RKHS) and formulating a classification algorithm therein. In particular, we associate each pixel to an empirical distribution of its neighbouring pixels, a judicious representation of which in an RKHS, in conjunction with the spectral information contained in the pixel itself, give a new explicit set of features that can be fed into a suite of standard classification techniques -- we opt for a well-established framework of support vector machines (SVM). Furthermore, the computational complexity is reduced via random Fourier features formalism. We study the consistency and the convergence rates of the proposed method and the experiments demonstrate strong performance on hyperspectral data with gains in comparison to the state-of-the-art results.", "text": "propose novel approach pixel classiﬁcation hyperspectral images leveraging spatial spectral information data. introduced method relies recently proposed framework learning distributions representing mean elements reproducing kernel hilbert spaces formulating classiﬁcation algorithm therein. particular associate pixel empirical distribution neighbouring pixels judicious representation rkhs conjunction spectral information contained pixel itself give explicit features suite standard classiﬁcation techniques well established framework support vector machines furthermore computational complexity reduced random fourier features formalism. study consistency convergence rates proposed method experiments demonstrate strong performance hyperspectral data gains comparison state-of-the-art results. keywords hyperspectral images pixelwise classiﬁcation kernel methods. hyperspectral images consist high-dimensional pixel observations allow reconstruction spectral proﬁles objects imaged thanks acquisition several hundred narrow spectral bands. supervised classiﬁcation pixels challenging task commonly arises remote sensing imaging structure hyperspectral imagery seldom studied comprehensive manner approaches focusing either spatial information building tools available normal imagery focus spectral information without principled make both. propose novel approach classiﬁcation based kernel embeddings distributions utilizes spatial spectral information data. aimed hyperspectral imaging method propose general applied types data. kernel methods support vector machines employed hyperspectral imaging pixel data lifted potentially inﬁnite-dimensional feature space called reproducing kernel hilbert space linear separating hyperplanes sought. however spectral information contained pixels often suﬃcient task including local spatial information available imagery obtain good classiﬁcation accuracy. approach encode spatial neighbourhood pixel random sample distribution associated pixel treat distribution additional feature classiﬁcation. order consistent spatial information also hadamard multiplication kernels. kernel kernel embeddings distributions linear kernel spatial information similarly section related work reviewed. section provides background kernel embeddings distributions random features fast approximations kernel methods mathematical morphology allow analyse understand geometrical structures images. section studies consistency convergence rate proposed method experiments given section many techniques include spatial information classiﬁcation process. particular interest combining feature space representations describing spatial information describing pixels. morphological feature spaces considered several publications impressive results hand kernel methods also studied extensively particularly compositions kernels allow building feature space representations. marry approaches framework instead usual feature sending data point feature space whole distribution represented rkhs. yields framework learning distributions representations rkhs. approach pixel associated distribution neighbours eﬀectively hyperspectral image treated distributions. similar approach regression applied multispectral imaging data. however authors partition multispectral image classify partitions goal obtain responses level groups neighbouring pixels suﬃces goal predict averaged quantity image area pixel-level classiﬁcation considered. another related line work used mean hyperspectral perform dimensionality reduction. positive deﬁnite kernel. moore-aronszajn theorem unique rkhs real-valued functions implying corresponds inner product features particular kih. means viewed feature many typical choices kernels rkhs inﬁnite-dimensional. denote random variable following distribution mean kernel embedding deﬁned expectation characteristic kernels include gaussian matern family many others embedding injective space probability distributions further given random variables following distribution following distribution inner product corresponding embeddings given computational storage requirements kernel methods large datasets prohibitive practice need compute store kernel matrix. consider dataset d-dimensional observations storage requirements calculation takes operations. remedy developed approximate translation-invariant kernels unbiased using random feature representation. namely translation-invariant positive deﬁnite kernel turn attention hyperspectral image around pixel location consider square patch size treat pixels random sample distribution speciﬁc location instead calculating kernel individual data points calculate kernel distributions. empirical mean kernel thus given simply noted outliers patch damage estimation mean. similarly work proposed weighted mean weights depend spatial information. kernels obtain called convolutional kernels also used contrast however random feature expansions explicitly represent feature space. consider data partitioned sets following distribution i.i.d. structure data given drawn joint meta distribution follow notation denote loss function. write following expected risk function data problem theorem given arbitrary probability distribution variance lipschitz continuous function constant arbitrary loss function lipschitz continuous second argument constant follows theorem denote loss class denote rademacher complexity. supg∈g bound variance functions trace kernel bounded loss function lipschitz continuous following bound holds probability least evaluate classiﬁcation accuracy proposed approach using standard datasets aviris indian pines rosis university pavia. ﬁrst data image dimension pixels spectral bands geometrical resolution training composed pixels image composed classes. dimensions second data pixels spectral bands geometrical resolution training composed pixels testing pixels image composed classes. commonly used testing pavia data report performance testing set. ﬁrst data testing generate monte-carlo simulations selecting randomly pixels class aggregate result classiﬁcation. used morphological proﬁle feature space commonly used pixel classiﬁation described supplementary material. also product kernels kernel kernels. kind techinique previously explored contrast previous work approximate product kernels thanks addition kernels real multiplication since work ﬁnite dimension hilbert spaces. results classiﬁcation reported table table classiﬁcation algorithm used c-svm parameter selected −fold cross-validation grid iwith results table show kernel mean perform well state results images. size scale seems important linked theorem increasing size scale increase size training set. explanation parameters evaluation found also supplementary materials. table overall accuracy kappa statistic average accuracy obtained diﬀerent kernels applied aviris indian pines hyperspectral data set. monte carlo simulations. selected training samples class. table overall accuracy kappa statistic average accuracy obtained diﬀerent kernels applied university pavia hyperspectral data set. used classical training set. article developed method pixel classiﬁcation hyperspectral imaging. method uses spatial information encoded distributions neighbourhood around pixel. even simple kernel choices obtained results comparable state-of-the-art. establish convergence rates prove two-stage consistent. improvements possible figure classiﬁcation maps indian pines hyperspectral image using diﬀerent approaches points class training set. ground truth linear estimated kernel kernel kernel using diﬀerent feature spaces employing suitable representations individual pixels. believe established interesting research direction local distributions treated additional features supervised learning task particular interest hyperspectral imaging diﬃcult combine spatial spectral information principled approach viewed step direction. figure classiﬁcation maps pavia hyperspectral image using diﬀerent approaches classical training set. ground truth linear estimated kernel kernel", "year": 2016}