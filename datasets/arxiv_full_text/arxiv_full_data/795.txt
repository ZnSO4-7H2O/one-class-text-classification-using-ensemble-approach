{"title": "A Novel Progressive Learning Technique for Multi-class Classification", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior.", "text": "lemma given standard slfn hidden nodes activation function infinitely differentiable interval arbitrary distinct samples randomly chosen intervals respectively according continuous probability distribution probability hidden layer output matrix slfn invertible ||hŒ≤ lemma given small positive value activation function infinitely differentiable interval exists arbitrary distinct samples randomly chosen intervals respectively according continuous probability distribution probability ||hnxp Œ≤pxm tnxm|| current matrix dimension classes introduced. therefore accommodat output weight matrix increased number output layer urons matrix transformed given equation. equation seen that error difference arget class predicted class ùíâùíå+ùú∑ùíå scaled learning factor added since classes introduced k+th time instant initial data samples target class label value corresponding class therefore k-learning step update classes written represents knowledge previously learnt. dimension increased m+c. opposed populating increased dimension identity matrix values entries calculated network recalibrated matrix represents learning class beginning training phase current data sample considering none previous data samples belong newly introduced class. i.e. computed equivalent k-learning step equivalent classes beginning training phase. algorithm identifies classes recalibrates time continues learning. results sudden rise learning curve network. first rise corresponding occurring sample corresponds learning class second rise occurring sample corresponds learning class ‚Äòd‚Äô. learning curve class shown black line. another class who‚Äôs testing accuracy shown introduced sample. fifth class introduced sample learning curve shown huang chen convex incremental extreme learning machine neurocomputing vol. huang chen enhanced random search based incremental extreme learning achine neurocomputing vol. wang yuan study effectiveness extreme learning machine\" neurocomputing vol. huang zhou ding zhang extreme learning machine regression multiclass classification ieee trans. approximation classification problems\" ieee transactions systems cybernetics part vol. huang liang rong saratchandran sundararajan \"on-line sequential extreme learning machine\" computational huang huang song rends extreme learning machines review neural networks vol. daliri hybrid automatic system diagnosis lung cancer based genetic lgorithm fuzzy extreme learning zhang fuzzy extreme learning machine classification electronics letters vol. avci coteli automatic target recognition system based wavelet extreme learning achine expert systems jarvis peter. towards comprehensive theory human learning vol. psychology press zhao chen chen wang wang class incremental extreme learning machine activity recognition cognitive", "year": 2016}