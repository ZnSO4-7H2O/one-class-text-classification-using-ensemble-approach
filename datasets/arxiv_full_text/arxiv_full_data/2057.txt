{"title": "Efficient Probabilistic Inference with Partial Ranking Queries", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Distributions over rankings are used to model data in various settings such as preference analysis and political elections. The factorial size of the space of rankings, however, typically forces one to make structural assumptions, such as smoothness, sparsity, or probabilistic independence about these underlying distributions. We approach the modeling problem from the computational principle that one should make structural assumptions which allow for efficient calculation of typical probabilistic queries. For ranking models, \"typical\" queries predominantly take the form of partial ranking queries (e.g., given a user's top-k favorite movies, what are his preferences over remaining movies?). In this paper, we argue that riffled independence factorizations proposed in recent literature [7, 8] are a natural structural assumption for ranking distributions, allowing for particularly efficient processing of partial ranking queries.", "text": "distributions rankings used model data various settings preference analysis political elections. factorial size space rankings however typically forces make structural assumptions smoothness sparsity probabilistic independence underlying distributions. approach modeling problem computational principle make structural assumptions allow cient calculation typical probabilistic queries. ranking models \u0010typical\u0011 queries predominantly take form partial ranking queries paper argue independence factorizations proposed recent literature natural structural assumption ranking distributions allowing particularly efcient processing partial ranking queries. rankings arise number machine learning application settings preference analysis movies books political election analysis applications central problems typically arise representation ciently build represent exible statistical model reasoning ciently statistical models draw probabilistic inferences observations. problems challenging fact that number items ranked increases number possible rankings increases factorially. cient representations reasoning identify exploitable problem structure number smart structural assumptions proposed scienti community. assumptions typically designed reduce number necessary parameters model believe problems approached view central challenges intertwined model structure chosen typical inference queries answered ciently. typical inference queries? paper assume ranking data useful typical inference queries take form partial rankings. example election data given voter's top-k favorite candidates election interested inferring preferences partial rankings ubiquitous come myriad forms top-k votes approval ballots rating data; probability models rankings cannot ciently handle partial ranking data therefore limited applicability. ranking datasets fact often predominantly composed partial rankings rather full rankings addition often heterogenous containing partial ranking data mixed types. paper contend structural assumption independence particularly well suited answering probabilistic queries partial rankings. independence introduced recent literature huang generalized notion probabilistic independence ranked data. like graphical model factorizations based conditional independence independence factorizations allow exible modeling distributions rankings learned sample complexity. show particular independence assumptions made prior distribution partial ranking observations decompose allows cient conditioning. main contributions work follows denoted using vertical notation σ−|σ−| .|σ−. ranks item item rank less rank example might {artichoke broccoli cherry date} ranking artichoke|broccoli|cherry|date encodes prefsince rankings items intractable estimate even explicitly represent arbitrary distributions without making structural assumptions underlying distribution. many possible simplifying assumptions make focus recent approach ranks items assumed satisfy intuitive generalized notion probabilistic independence known independence. paper argue rifed independence assumptions particularly e\u001bective settings would like make queries taking form partial rankings. remainder section review independence. independence assumption posits rankings item generated independently generating rankings smaller disjoint item subsets partition piecing together full ranking interleaving smaller rankings together. example rank item foods might rank vegetables fruits separately interleave subset rankings form full ranking. formally dene independence notions relative rankings interleavings. vegetables {artichoke broccoli} fruits {cherry date} ranking four items artichoke|date|broccoli|cherry. case relative ranking vegetables artichoke|broccoli relative ranking fruits date|cherry. interleaving vegetables fruits a|b|a|b. independence found approximately hold number real datasets relationships identi data instead exhaustively representing ranking probabilities represent factors distributions smaller sets. hierarchical independent models. relative ranking factors distributions rankings. reduce parameter space natural consider hierarchical decompositions itemsets nested collections partitions example figure shows hierarchical decomposition vegetables independent fruits among \u0010healthy\u0011 foods healthy foods insimplicity restrict consideration binary hierarchies tuples form either null case called leaf hierarchies item sets respectively. second case assumed nontrivial partition item set. nition distribution factors independently respect hierarchy item sets independent respect factor independently respect subhierarchies respectively. like bayesian networks hierarchies represent families distributions obeying certain independence constraints parameterized locally. draw model generates full rankings recursively starting drawing rankings leaf sets working tree sequentially interleaving rankings reaching root. parameters hierarchical models simply interleaving relative ranking distributions internal nodes leaves hierarchy respectively. decomposing distributions rankings small pieces hierarchical models allow better interpretability cient probabilistic representation sample complexity cient optimization show paper cient inference. cally computationally intractable since requires multiplying dimensional functions unless exploit structural decompositions problem. section describe decomposition certain class likelihood functions space rankings observations `factored' simpler factor subset observation involving items smaller subset observations involving interleavings independently. decompositions often exploited cient inference. ative ranking distribution vegetables zeroing rankings vegetables place artichoke place updates interleaving distribution zeroing interleavings place vegetable place normalizes resulting distributions. vegetables fruits) enough notice interleaving vegetables fruits independent relative ranking vegetaif example element interbles. leaves a|b|a|b since relφa broccoli|artichoke. since interleavsee cannot decomposable. subset observations prior decompose according hierarchy show posterior also decomposes. proposition hierarchy item set. given prior distribution observation hierarchy cient cient inference might glance seem restrictive render proposition useless practice. overcome limitation \u0010hierarchy speci decomposability explore special family observations property decomposability depend speci cally particular hierarchy implying particular observations cient inference always possible servation. notice however decomposability particular observation depend items partitioned hierarchy. specifinstead vegetables fruits sets ically given stringent conditions nition obvious nontrivial completely decomposable observations even exist. nonetheless exist nontrivial examples next section exhibit rich general class completely decomposable observations. partial rankings. begin discussion introducing partial rankings allow items tied respect ranking `dropping' verticals vertical representation space partial rankings captures rich natural class observations. particular partial rankings encompass number commonly occurring special cases traditionally modeled isolation work used setting. remaining items implicitly ranked behind. partial rankings type correspond subremaining subset items. example partial rankings type might arise note \u0010the term partial ranking used confused standard objects partial order namely exive transitive antisymmetric binary relation; ranking subset search engines example although explicit factorization respect hierarchy items. simplicity begin considering single layer case items partitioned leaf sets factorization depends following notions consistency relative rankings interleavings partial ranking. vegetable single fruit ranks single vegetable single fruit last ranks. alternatively partially speci interleaving ab|ab. full interleavings a|b|b|a b|a|b|a consistent a|a|b|b consistent partial rankings themselves algorithm conditioning partial ranking applied recursively subhierarchies theorem every partial ranking completely decomposable. algorithm details recursive conditioning algorithm. consequence theorem proposition conditioning partial ranking observations performed linear time respect number model parameters. interesting consider completely decomposable observations exist beyond partial rankings. main contributions show observations. theorem every completely decomposable observation takes form partial ranking. together theorems form signi cant insight nature rankings showing notions partial rankings complete decomposability exactly coincide. fact result shows even possible partial rankings complete decomposability practical matter results show algorithm based simple multiplicative updates parameters exactly condition observations take form partial rankings. interested conditioning observations theorem suggests slower approximate inference approach might necessary. section cient inference algorithm proposed section estimating independent model partially ranked data. estimating model using partially ranked data typically considered cult estimating using full rankings common practice simply ignore partial rankings dataset. ability method incorporate available data however lead signi cantly improved model accuracy well wider applicability method. section propose cient method estimating structure parameters hierarchical independent model heterogeneous datasets consisting arbitrary partial ranking types. central approach idea given someone's partial preferences algorithm pseudocode prcondition algorithm recursively conditioning hierarchical independent prior distribution partial ranking observations. nitions runtime prcondition linear number model parameters. cient algorithms developed previous section infer full preferences consequently apply previously proposed algorithms designed work full rankings. censoring interpretations partial rankings. model estimation problem full rankings stated follows. given i.i.d. training examples drawn hierarchical independent distribution recover structure parameters partial ranking setting assume i.i.d. draws training example undergoes censoring process producing partial ranking consistent example censoring might allow ranking top-k items observed. allow arbitrary types partial rankings arise censoring make common assumption partial ranking type resulting censoring depend itself. algorithm. treat model estimation partial rankings problem missing data problem. many problems could determine full ranking corresponding observation data could apply algorithms work completely observed data setting. since full rankings given utilize expectationmaximization approach inference compute posterior distribution full rankings given observed partial ranking. case apply algorithms designed estimate hierarchical structure model parameters dataset full rankings. pected log-likelihood training data respect model. hierarchical structure model provided known beforehand m-step performed using standard methods optimizing parameters. structure unknown structural approach analogous methods graphical models literature structure learning incomplete data unfortunately structure learning algorithm unable directly posterior distributions computed e-step. instead observing sampling independent models done ciently exactly simply sample full rankings posterior distributions computed e-step pass full rankings structure learning algorithm number samples necessary instead scaling factorially scales according number samples required detect independence related work. several recent works model partial rankings. busse learned nite mixtures mallows models top-k data lebanon developed nonparametric model based mallows models handle arbitrary types partial rankings. settings central problem marginalize mallows model full rankings consistent particular partial ranking. ciently papers rely fact marginalization step performed closed form. closed form equation however seen special case setting since mallows models always shown factor independently according chain structure. moreover instead resorting complicated mathematics based inversion combinatorics theory complete decomposability o\u001bers simple conceptual understand mallows models conditioned ciently partial ranking observations. number ballots meath data length particular note vast majority ballots dataset consist partial rather full rankings. inference top-k examples meath data seconds dual pentium machine unoptimized python implementation. using `brute force' inference estimate would require roughly hundred years. extracted second dataset database searchtrails collected browsing sessions roughly users logged many cases users unlikely read articles story twice often possible think order user reads collection articles top-k ranking articles concerning particular story/topic. ability model visit orderings would allow make long term predictions user browsing behavior even recommend `curriculums' articles users. algorithms roughly visit orderings eight popular posts www.huffingtonpost.com concerning `sarah palin' popular subject u.s. presidential election. since user visited every article full rankings data thus method `ignoring' partial rankings work. structure discovery experiments initialize distributions uniform random restarts. experiments several observations using learning partial rankings. first observe typical runs converge structure quickly three iterations. figure shows progress sarah palin data whose structure converges third iteration. expected log-likelihood increases iteration remark structure becomes interpretable secondly number iterations required reach convergence log-likelihood depends types partial rankings observed. algorithm subsets meath dataset time training rankings length larger figure shows number iterations required convergence function observe fastest convergence datasets consisting almost-full rankings slowest convergence consisting almostempty rankings almost iterations necessary trains using rankings types. value partial rankings. show using partial rankings addition full rankings allows achieve better density estimates. learned models synthetic data drawn hierarchy training using full rankings plus varying numbers partial ranking examples repeat setting bootstrap trials evaluation compute loglikelihood testset examples. speed learn structure learn parameters trial. figure plots test log-likelihood function number partial rankings made available training shows indeed able learn accurate distributions data made available. comparing nonparametric model. comparing performance independent models approaches previously possible since could handle partial rankings. using methods compare independent models state-of-the-art nonparametric lebanon-mao estimator data figure shows data drawn synthetically independent model method signi cantly outperforms estimator. meath data approximately independent trained subsets size subset evaluated algorithm learning independent model estimator using full ranking data using data. before methods better partial rankings made available. smaller training independent model performs well better estimator. larger training nonparametric method starts perform slightly better average advantage nonparametric model guaranteed consistent converging correct model given enough data. advantage independent models however simple interpretable highlight global structures hidden within data. conclusion probabilistic reasoning problems often case certain data types suggest certain distribution representations. example sparse dependency structure data often suggests markov random representation low-order permutation observations recent work shown fourier domain representation appropriate. work shows hand observed data takes form partial rankings hierarchical independent models natural representation. conjugate priors showed independent model guaranteed retain factorization structure conditioning partial ranking surprisingly work shows observations take form partial rankings amenable simple multiplicative update based conditioning algorithms. finally showed possible learn hierarchical independent models partially ranked data signi cantly extending applicability previous work. work supported part under muri muri wnf. guestrin funded part career iis-. thank horvitz white liebling discussions. much research conducted huang visiting microsoft research redmond.", "year": 2012}