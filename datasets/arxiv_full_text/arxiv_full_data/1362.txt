{"title": "Competitive Multi-scale Convolution", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.", "text": "figure proposed deep convnet modules depicted contains multi-scale convolutional ﬁlters within module contains max-pooling path resembles original inception module depicted comparison. generally produces superior classiﬁcation results paper introduce module deep convnets composed several multi-scale convolutional ﬁlters joined maxout activation unit promotes competition among ﬁlters. idea inspired recently proposed inception module currently produces state-of-the-art results ilsvrc paper introduce deep convolutional neural network module promotes competition among multi-scale convolutional ﬁlters. module inspired inception module replace original collaborative pooling stage competitive pooling represented maxout activation unit. extension following objectives selection maximum response among multi-scale ﬁlters prevents ﬁlter co-adaptation allows formation multiple sub-networks within model shown facilitate training complex learning problems; maxout unit reduces dimensionality outputs multi-scale ﬁlters. show proposed module typical deep convnets produces classiﬁcation results either better comparable state following benchmark datasets mnist cifar- cifar- svhn. competitive activation units deep convolutional neural networks generally understood building network combination multiple sub-networks capable solving simpler task compared complexity original problem involving whole dataset similar ideas explored past using multi-layer perceptron models resurgence competitive activation units deep convnets instance rectiﬁed linear unit promotes competition input ﬁxed value maxout local winner-take-all explore explicit competition among input units. shown srivastava competitive activation units allow formation sub-networks respond similarly similar input patterns facilitates training ∗this research supported australian research council cenclassiﬁcation detection challenges gist proposal depicted fig. data input layer ﬁltered parallel multi-scale convolutional ﬁlters output scale convolutional layer passes batch normalisation unit weights importance scale also pre-conditions model finally multi-scale ﬁlter outputs weighted joined maxout unit reduces dimensionality joint ﬁlter outputs promotes competition among multi-scale ﬁlters prevents ﬁlter co-adaptation allows formation multiple sub-networks. show introduction proposal module typical deep convnet produces best results ﬁeld benchmark datasets cifar- cifar- street view house number producing competitive results mnist main reasons behind outstanding performance deep convnets attributed competitive activation units form piece-wise linear functions relu maxout lwta general activation functions enable formation sub-networks respond consistently similar input patterns dividing input data points regions classiﬁers regressors learned effectively given sub-problems regions simpler involving whole training set. addition joint training subnetworks present deep convnets represents useful regularization method practice relu allows division input space regions maxout lwta divide space many regions number inputs reason latter functions estimate exponentially complex functions effectively larger number sub-networks jointly trained. important aspect deep convnets competitive activation units fact batch normalization units helps respect convergence rate also preconditioning model promoting even distribution input data points results maximization number regions produced piece-wise linear activation functions furthermore training convnets competitive activation units usually involves dropout consists regularization method prevents ﬁlter coadaptation particularly important issue models ﬁlter co-adaptation lead severe reduction number sub-networks formed training. figure competitive activation units gray nodes active ones errors backpropagation. relu active input bigger lwta activates node maximum value maxout output containing maximum value input. ﬁgure adapted fig. another aspect current research deep convnets idea making network deeper shown improve classiﬁcation results however main ideas studied ﬁeld increase depth convnet without necessarily increasing complexity model parameter space szegedy al.’s model achieved convolutional ﬁlters placed local ﬁlter present inception module order reduce input dimensionality ﬁlter. simonyan al.’s approach idea large number layers convolutional ﬁlters small size work restrict complexity deep convnet maxout activation units selects input nodes shown finally multi-scale ﬁlters deep convnets another important implementation increasingly explored several researchers essentially multiscale ﬁltering follows neuroscience model suggests input image data processed several scales pooled together deeper processing stages become robust scale changes explore idea proposal depicted fig. also argue multi-scale nature ﬁlters prevent co-adaptation training. assume image represented denotes image lattice image patch size centred position represented xi±. models proposed paper follow structure model general deﬁned follows denotes composition operator represents convnet parameters fout denotes averaging pooling unit followed softmax activation function network blocks represented block containing represents maxout activation function convolutional ﬁlters module represented weight matrices means module block different ﬁlter sizes ilters different ﬁlters represent batch normalization scaling shifting parameters represents pooling operator subset input data layer centred i.e. xi±. using convnet module deﬁned proposed models differ mainly presence absence node max-pooling operator within module βp). module contain node called competitive multi-scale convolution module max-pooling node call competitive inception similarity original inception module original inception module also implemented comparison purposes call model inception style similar following differences function denotes concatenation input parameters; convolution applied input second round convolutions ﬁlter sizes larger equal relu activation function present convolutional layer. overview models structural parameters displayed fig. note models inspired googlenet particular replace original convolutional layers multi-scale ﬁlters sizes inception style model ensure number output units module competitive inception competitive multi-scale convolution also max-pooling path module used original inception module another important point general designing inception style network follow suggestion szegedy include relatively larger number ﬁlters module compared ﬁlters sizes important distinction original googlenet inception style network fig. fact replace fully connected layer last layer single convolution node last module followed average pooling softmax unit similarly model propose modiﬁcation limit number training parameters avoid concatenation nodes different paths number channels equal number classes concatenation would imply paths would directly linked subset classes. main reason explored ﬁeld justify competitive activation units fact build network formed multiple underlying sub-networks clearly given activation units consist piece-wise linear functions shown composition several layers containing units divide input space number regions exponentially proportional number network layers sub-networks trained samples fall regions result become specialised problem particular region overﬁtting avoided sub-networks must share parameters another worth noting regions formed underlying convolutional ﬁlters coadapt otherwise input training samples fall region competitive unit degenerates simple linear transform preventing formation sub-networks. straightforward solution avoid co-adaptation achieved limiting number training samples mini-batch stochastic gradient descent. small batches allow generation noisy gradient directions training activate different maxout gates different linear pieces activation unit ﬁtted allowing formation exponentially large number regions. however drawback approach lies determination right number samples mini-batch. mini-batch size small leads poor convergence large allow formation many sub-networks. recently liao carneiro propose solution problem based distributes training samples evenly regions formed competitive unit allowing training different sets training points region competitive unit resulting formation exponential number sub-networks. however still potential problem approach underlying convolutional ﬁlters trained using feature spaces size induce ﬁlters co-adapt converge similar regions feature space also preventing formation subnetworks. figure proposed competitive multi-scale convolution competitive inception networks together reference inception style network three models ensure output layer number units. also note that inception style model uses relu convolutional layers number ﬁlters convolutional node represented number brackets models assume -class classiﬁcation problem. competitive multi-scale convolution module proposed paper represents issue introduced speciﬁcally different sizes convolutional ﬁlters within competitive unit force feature spaces ﬁlters different other reducing chances ﬁlters converge similar regions feature space. instance ﬁlters sizes joined competitive unit means former ﬁlter -dimensional space latter ﬁlter additional dimensions total dimensions dimensions allow training process ﬁlter signiﬁcantly larger feature space words different ﬁlter sizes within competitive unit imposes soft constraint ﬁlters must converge different values avoiding co-adaptation issue. sense idea similar dropconnect which training drops zero weights randomly picked network connections goal training regularization. nevertheless underlying ﬁlters size promotes coadaptation even random connections dropped zero. compared dropconnect stochastically drops ﬁlter connections training approach deterministically drops border connections ﬁlter show experiments approach effective dropconnect task preventing ﬁlter co-adaptation within competitive units. quantitatively measure performance proposed models competitive multi-scale convolution competitive inception four computer vision/machine learning benchmark datasets cifar- cifar mnist svhn ﬁrst describe experimental setup using cifar- mnist show quantitative analysis proposed models inception style model presented sec. additional versions proposed models justify multi-scale ﬁlters explained sec. finally compare performance proposed competitive multi-scale convolution competitive inception respect current state four benchmark datasets mentioned above. cifar- dataset contains images commonly seen object categories images used training rest testing categories equal volume training test images. images cifar- consist -pixel images objects well-centered middle image. cifar dataset extends cifar- increasing number categories whereas total number images remains same cifar- dataset considered harder classiﬁcation problem cifar- since contains times less images class times categories. well-known mnist dataset contains grayscale images comprising handwritten dataset divided images training testing note number images digit uniformly distributed. finally street view house number also digit classiﬁcation benchmark dataset contains images printed digits cropped pictures house number plates. cropped images centered digit interest nearby digits distractors kept image. svhn three sets training testing sets extra images less difﬁcult used helping training process. data augmentation experiments compare results methods data augmentation. benchmark datasets minimize softmax loss function present last layer model respective classiﬁcation dataset report results proportion misclassiﬁed test images standard comparing algorithms benchmark datasets. reported results generated models trained using initial learning rate following multi-step decay ﬁnal learning rate stopping criterion determined convergence observed error validation set. mini-batch size cifar cifar- mnist datasets svhn dataset. momentum weight decay standard values respectively. result reported compute mean standard deviation test error separately trained models model training parameters change random initialization ﬁlter weights randomly shufﬂe training samples. gpu-accelerated convnet library matconvnet perform experiments speciﬁed paper. experimental environment desktop equipped memory titan graphic card. using machine report mean training testing times models. section show results several experiments show design choices models provide comparisons terms test errors number parameters involved training process training testing times. tables show results cifar- mnist models competitive multi-scale convolution competitive inception inception style models addition models explained below. note models tables constrained numbers input channels output channels module networks contain three blocks three modules shown fig. argue multi-scale nature ﬁlters within competitive module important avoid coadaptation issue explained sec. assess importance comparing number parameters test error results proposed models model competitive single-scale convolution basically architecture competitive multiscale convolution model represented fig. following changes ﬁrst blocks contain four sets ﬁlters ﬁrst module second third modules sets ﬁlters; third block three ﬁlters size ﬁrst module followed modules ﬁlters. notice conﬁguration implies replace multi-scale ﬁlters ﬁlter largest size module node conﬁguration similar recently proposed model conﬁguration competitive single-scale convolution around times parameters competitive multi-scale convolution model takes longer train displayed tables idea behind largest size ﬁlters within module based results obtained training batch normalisation units competitive multi-scale convolution modules indicates highest weights placed largest size ﬁlters within module shown fig. classiﬁcation results competitive single-scale convolution shown tables demonstrate consistently inferior competitive multi-scale convolution model. another important point test section relevance dropping connections deterministic stochastic manner training competitive convolution modules. recall questions posed sec. deterministic masking provided proposed competitive multi-scale convolution module effective avoiding ﬁlter co-adaptation stochastic masking provided dropconnect quantitative analysis competitive dropconnect singlescale convolution take competitive singlescale convolution proposed randomly drop connections using rate computed average number parameters learn round training competitive multi-scale convolution notice competitive dropconnect singlescale convolution fact number parameters competitive single-scale convolution. using fig. dropconnect rate table results cifar- proposed models addition competitive single-scale convolution competitive dropconnect single-scale convolution test research questions posed sec. table results mnist proposed models addition competitive single-scale convolution competitive dropconnect single-scale convolution test research questions posed sec. figure mean standard deviation learned values batch normalisation unit competitive multi-scale convolution model cifar-. result provides estimate importance placed ﬁlter training procedure. module blocks speciﬁed fig. results tables show around times parameters takes longer train performs signiﬁcantly worse competitive multi-scale convolution model. finally reported training testing times tables show clear relation number model parameters times. comparison state show performances proposed competitive multi-scale competitive inception convolution models cifar- cifar- mnist svhn compare current state ﬁeld method competitive multi-scale convolution competitive inception rcnn- deeply-supervised nets network network maxout networks stochastic pooling method competitive multi-scale convolution competitive inception rcnn- deeply-supervised nets network network tree based priors maxout networks stochastic pooling method rcnn- competitive multi-scale convolution deeply-supervised nets competitive inception network network conv. maxout+dropout stochastic pooling method competitive multi-scale convolution rcnn- competitive inception convolution deeply-supervised nets drop-connect network network conv. maxout+dropout stochastic pooling terms model design choices sec. proposed competitive multi-scale convolution produces accurate classiﬁcation results proposed competitive inception. given main differfigure competitive multi-scale convolution module ﬁlters size equivalent four ﬁlters masks number deterministically masked weights using dropconnect rate possible randomly dropped weights shown note even though proportion number weights dropped same deterministic stochastic masking weights make difference performance explained paper. listed follows. stochastic pooling proposes regularization based replacement deterministic pooling stochastic procedure randomly selects activation within pooling region according multinomial distribution estimated activation pooling unit. maxout networks introduces piece-wise linear activation unit used together dropout training introduced fig. network network model consists introduction multilayer perceptrons activation functions placed convolution layers replacement ﬁnal fully connected layer average pooling number output channels represent ﬁnal number classes classiﬁcation problem. deeply-supervised nets introduce explicit training objectives hidden layers addition back-propagated errors last softmax layer. recurrent structure replaces purely feed-forward structure convnets explored model rcnn extension model based maxout activation function instead multilayer perceptron introduced model also shows batch normalization units crucial allowing effective training several single-scale ﬁlters joined maxout units. finally tree based priors model proposes training method classes samples using generative prior learned data shared related classes model learning. comparison cifar- dataset shown tab. results sorted based performance method results proposed methods highlighted. results cifar- dataset displayed tab.. table shows results mnist worth reporting best result produced competitive multi-scale convolution model test error betence models presence maxpooling path within module conclude path help classiﬁcation accuracy model. better performance models respect inception style model attributed maxout unit induces competition among underlying ﬁlters helps classiﬁcation results compared collaborative nature inception module. considering model complexity important notice relation number parameters training testing times linear even though inception style model fewer parameters trains tests faster proposed competitive multi-scale convolution competitive inception models. answering questions posed sec. assume classiﬁcation accuracy proxy measuring co-adaptation ﬁlters within single module intuition ﬁlters joined maxout activation unit co-adapt become similar other relatively small number large regions input space formed results sub-networks train sub-network becoming less specialized region argue main consequence potential lower classiﬁcation accuracy depending complexity original classiﬁcation problem. using assumption note tables multi-scale ﬁlters within competitive module fact important avoid co-adaptation ﬁlters shown accurate classiﬁcation results multi-scale compared single-scale model. furthermore deterministic opposed stochastic mapping also appears effective avoiding ﬁlter co-adaptation given accurate classiﬁcation results former mapping. nevertheless reason behind worse performance stochastic mapping fact dropconnect designed fully connected layers test comparison convolutional ﬁlters. speciﬁc think fully connected layer usually encapsulates hundreds thousands weights inputs similar scale dimensions thus random dropping subset weight elements hardly change distribution outputs pattern. however convolution ﬁlters small dimensions maxout unit controls ﬁlters most masking scheme small weights matrix could result catastrophic forgetting explains competitive dropconnect single-scale convolution performs even worse competitive singlescale convolution cifar-. also experiment assesses whether ﬁlters larger size within competitive module improve classiﬁcation accuracy expense larger number parameters train. test inclusion ﬁlters sizes module blocks ﬁlter sizes module block classiﬁcation result obtained cifar- number model parameters experiment shows increasing number ﬁlters larger sizes necessarily help improve classiﬁcation results. important modiﬁcation suggested proposed competitive multi-scale convolution model replacement maxout relu activation largest size ﬁlter module kept ﬁlters removed. argue model perhaps less complex probably accurate proposed model. however results obtained model cifar- show model parameters classiﬁcation test error signiﬁcantly larger proposed models. mnist model parameters produces classiﬁcation error also shows advantage proposed models. comparisons state tables sec. show proposed competitive multiscale convolution model produces best results ﬁeld three four considered datasets. however note comparison strictly fair ﬁve-model validation experiment provides robust performance assessment method. contrast methods ﬁeld show single result performance. consider best result results experiment competitive multi-scale convolution model best results four datasets analysis results also allows conclude main competitors approach rcnn models method quite related approach rcnn method follows quite different strategy. paper show effectiveness using competitive units modules contain multi-scale ﬁlters. argue main reason superior classiﬁcation results proposal compared current state several benchmark datasets lies following points deterministic masking implicitly used multi-scale ﬁlters avoids issue ﬁlter co-adaptation; competitive unit joins underlying ﬁlters batch normalization units promote formation large number sub-networks specialized classiﬁcation problem restricted small area input space regularized fact trained together within model; maxout unit allows reduction number parameters model. important note modules applied several types deep learning networks plan apply types models recurrent neural network serre wolf bileschi riesenhuber poggio. robust object recognition cortex-like mechanisms. pattern analysis machine intelligence ieee transactions szegedy sermanet reed anguelov erhan vanhoucke rabinovich. ieee conference going deeper convolutions. computer vision pattern recognition june", "year": 2015}