{"title": "Learning dynamic Boltzmann machines with spike-timing dependent  plasticity", "tag": ["cs.NE", "cs.AI", "cs.LG", "stat.ML"], "abstract": "We propose a particularly structured Boltzmann machine, which we refer to as a dynamic Boltzmann machine (DyBM), as a stochastic model of a multi-dimensional time-series. The DyBM can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. This proposed structure is motivated by postulates and observations, from biological neural networks, that the synaptic weight is strengthened or weakened, depending on the timing of spikes (i.e., spike-timing dependent plasticity or STDP). We show that the learning rule of updating the parameters of the DyBM in the direction of maximizing the likelihood of given time-series can be interpreted as STDP with long term potentiation and long term depression. The learning rule has a guarantee of convergence and can be performed in a distributed matter (i.e., local in space) with limited memory (i.e., local in time).", "text": "propose particularly structured boltzmann machine refer dynamic boltzmann machine stochastic model multidimensional time-series. dybm inﬁnitely many layers units allows exact efﬁcient inference learning parameters proposed structure. proposed structure motivated postulates observations biological neural networks synaptic weight strengthened weakened depending timing spikes show learning rule updating parameters dybm direction maximizing likelihood given time-series interpreted stdp long term potentiation long term depression. learning rule guarantee convergence performed distributed matter limited memory boltzmann machines seen successful applications recognition images tasks machine learning particularly recent development deep learning standard approaches training boltzmann machine iteratively apply hebbian rule either exactly approximately values parameters updated directions increasing likelihood given training data respect equilibrium distribution boltzmann machine hebbian rule boltzmann machine limited sense concept time missing. biological neural networks spike-timing dependent plasticity postulated supported empirically example synaptic weight strengthened post-synaptic neuron ﬁres shortly pre-synaptic neuron ﬁres weakened order ﬁring reversed paper study dynamics boltzmann machine dynamic boltzmann machine derive learning rule dybm interpreted stdp. conventional boltzmann machine trained collection static patterns dybm trained time-series patterns. particular dybm gives conditional probability next values time-series given historical values. conditional probability depend whole history time-series dybm thus used iteratively generative model time-series. speciﬁcally deﬁne dybm boltzmann machine multiple layers units layer represents recent values time-series remaining layers represent historical values time-series. assume recent values conditionally independent given historical values. dybm allows inﬁnite number layers recent values depend whole history time series. train dybm likelihood given time-series maximized respect conditional distribution next values given historical values. deﬁnition dybm general approach training dybm constitute ﬁrst contribution paper. show learning rule dybm signiﬁcantly simpliﬁed exhibits various characteristics stdp observed biological neural networks dybm inﬁnite number layers particularly structured parameters. speciﬁcally assume weight unit representing recent value unit representing value past geometric functions respect show updating parameters associated pair units requires information available units required information maintained keeping ﬁrst-in-ﬁrst-out queue last values unit convergence learning rule guaranteed sufﬁciently learning rate parameters always updated likelihood given training data increased. learning rule formally derived dybm interpretation stdp constitute second contribution paper. prior work extended boltzmann machine incorporate timing spikes various ways however existing learning rules extended boltzmann machines involve approximation contrastive divergence characteristics stdp show dybm. dybm considered recurrent neural network equipped memory units. dybm thus related long short term memory rnns distinguishes dybm existing rnns training dybm require backpropagation time chain rule derivatives. distinguishing feature dybm follows fact dybm equivalently interpreted non-recurrent boltzmann machine. learning rule derived interpretation non-recurrent boltzmann machine clearly involve backpropagation time proper learning rule equivalent rnn. result training dybm free vanishing gradient problem learning rule existing recurrent neural networks involves stdp limited form learning rule. example learning rule depends timing spikes whether post-synaptic neuron ﬁres immediately pre-synaptic neuron ﬁres. learning rule magnitude changes weight depend difference timings spikes observed biological neural networks extended version paper appeared paper however contains details perspectives omitted also note notations terminologies paper necessarily consistent boltzmann machine network units mutually connected weight number units. value i-th unit weight i-th unit j-th unit standard assume bias associated i-th unit. following notations column vectors matrices ij∈. denote parameters boltzmann machine. denotes expected value product values i-th unit j-th unit respect learning rate particularly interesting case restricted boltzmann machine units divided layers weight units layer case evaluated approximately contrastive divergence methods exact computation intractable property allows contrastive divergence following conditional independence. values units t-th layer corresponding bias. matrix whose element weight i-th unit ﬁrst layer j-th unit second layer. conditional probability given given propose dynamic boltzmann machine inﬁnitely many layers units similar dybm weight units right-most layer figure unlike layer dybm common number units bias weight dybm shared among different units particular manner. formally deﬁne dybm-t boltzmann machine layers positive integer inﬁnity. values units t-th layer consider values time bias units layer matrix whose denotes weight i-th unit time j-th unit time element unit s-th layer arbitrary bias arbitrary weight unit t-th layer bias weight effect dybm following. dybm inﬁnitely many layers deﬁned formal limit dybm-t consider pθ|x) conditional probability given interval denote )t∈i. units layer weight other conditional probability property conditional independence analogous note pθ|x) independent bias weight associated units layer. propose dybm model time-series following sense. speciﬁcally given history time-series dybm-t gives probability next values time-series pθ|x). dybm- interpreted markov model dybm-t order markov model next values conditionally independent history given values last steps. dybm-∞ next values depend whole history time-series. principle dybm-∞ thus model time-series possibly long-term dependency long values time-series moment conditionally independent given values preceding moment. using conditional probability given dybm-t probability sequence length given derive learning rule dybm-t likelihood given time-series maximized. learning rule particularly simpliﬁed limit parameters dybm-∞ particular structures. show learning rule exhibits various characteristics spike-timing dependent plasticity figure ﬁgure illustrates equation particular forms equation horizontal axis represents vertical axis represents value deﬁned discontinuous deﬁned discontinuous dij. hand −dji respectively. could example update direction every time observed using latest history approach stochastic gradient. practice however computation intractable large parameters learn number pairs connected units units densely connected). thus propose particular form weight sharing motivated observations biological neural networks leads particularly simple exact efﬁcient learning rule. biological neural networks stdp postulated supported experimentally. particular synaptic weight pre-synaptic neuron post-synaptic neuron strengthened post-synaptic neuron ﬁres shortly pre-synaptic neuron ﬁres weight weakened post-synaptic neuron ﬁres shortly pre-synaptic neuron ﬁres dependency timing spikes missing hebbian rule boltzmann machine figure value unit j-th unit. namely post-synaptic neuron likely immediately spike pre-synaptic unit arrives delay likelihood controlled magnitude value spike arrived i-th unit diminishes time value immediately spike i-th unit arrives. unlikelihood controlled magnitude learn. decreases magnitude represents weight spike pre-synaptic neuron generated spike post-synaptic neuron. assumption convenient computational purposes justiﬁed limit inﬁnitesimal time steps. speciﬁcally consider scaled dybm step size time probability ﬁring made /n-th original dybm. limit scaled dybm continuous time probability simultaneous spikes units tends zero. learn values uijk based training dataset. assume given analogy biological neural networks given parameters dij) determined based physical constraints chemical properties weight bias learned based neural spikes geometric functions varying decay rates motivated long-term memory figure ﬂexibility geometric functions. particular geometric functions well approximate hyperbolic function whose value decays slowly geometric functions. slow decay considered essential long-term memory. however results also hold simple cases speciﬁcations letting represent appearing follows typically single time-series available training dybm-∞. case form used recall arbitrarily zeros history made single time-series eligibility traces needed training computed recursively ones used particular training updated adding deleting remaining eligibility trace calculated non-recursively qij. fact experience suggests recursive calculation βijk amenable numerical instability βijk calculated non-recursively. homogeneous dynamic boltzmann machine here show dybm-∞ indeed understood generative model time series. purpose specify bias units s-th layer weight units s-th layer units t-th layer recall model single time step used iteratively deﬁne distribution time series. strictly speaking however iterative generative model deﬁned solely boltzmann machine. consider following homogeneous dybm special case dybm-∞. layer units common vector bias. units s-th layer bias matrix weight layers depend distance layers. pair i-th unit s-th layer j-th unit t-th layer connected weight learning rule derived dybm-∞ section section holds homogeneous dybm. property homogeneous dybm homogeneous dybm consisting layers t-th layer equivalent homogeneous dybm consisting layers layer. therefore iterative model single time step equivalent generative model time-series deﬁned single homogeneous dybm. speciﬁcally values time-series time generated based conditional probability pθ|x) given homogeneous dybm consisting layers layer. values generated time used part history homogeneous dybm consisting layers layer turn deﬁnes conditional probability values time homogeneous dybm also layers positive time steps homogeneous dybm interpreted recurrent neural network discuss following reference artiﬁcial neural network. figure illustrates learning rule derived section .-section point artiﬁcial neural networks. consider pre-synaptic neuron post-synaptic neuron fifo queue considered axon stores spikes traveling conduction delay axon spikes generated last steps stored. spikes axon determine value another eligibility trace records aggregated information spikes generated neuron spikes generated past discounted rate depends remaining eligibility trace αijk records aggregated information spikes reached spikes arrived past discounted rate depends dybm-∞ considered recurrent neural network taking binary values equipped memory units store eligibility traces fifo queue learning inference n-dimensional binary time-series arbitrary length recurrent neural network needs working space binary bits ﬂoating-point numbers number ordered pairs connected units dybm-∞) maximum delay speciﬁcally binary bits correspond bits fifo queues. ﬂoating-point numbers correspond eligibility traces coefﬁcients weight bias parameters dybm-∞ updated distributed manner learning rules observe distributed update performed constant time independent |l|. according neuron likely high uijk αijk high conditions met. learning rule suggests increases time neuron ﬁres often expected latest values parameters dybm-∞. learning rule suggests uijk increases time neuron ﬁres often expected magnitude changes uijk proportional magnitude αijk. implement long term potentiation. according neuron less likely high high conditions met. learning rule suggests vijk increases time neuron ﬁres less often expected magnitude changes vijk proportional magnitude βij. exchanged learning rule suggests vijk increases time neuron ﬁres less often expected magnitude changes vijk proportional magnitude implement long term depression. here terms involve expected values considered mechanism homeostatic plasticity keeps ﬁring probability relatively constant. particular mechanism homeostatic plasticity appear discussed stdp literature expect however formally derived mechanism homeostatic plasticity plays essential role stabilizing learning artiﬁcial neural networks. without homeostatic plasticity values parameters indeed diverge ﬂuctuate training. work provides theoretical underpinnings postulates stdp. recall hebb rule ﬁrst postulated middle last century seen limited success engineering applications years later hopﬁeld network boltzmann machine used provide theoretical underpinnings. particular hebbian rule shown increase likelihood data respect distribution associated boltzmann machine stdp postulated biological neural networks used artiﬁcial neural networks rather ways. work establishes relation stdp boltzmann machine ﬁrst time formal manner. speciﬁcally propose dybm stochastic model time-series. dybm gives conditional probability next values multi-dimensional time-series given historical values. conditional probability depend whole history arbitrary length dybm limitation markov model higher order markov model ﬁnite order. conditional probability given dybm-∞ thus applied recursively obtain probability generating particular time-series arbitrary length. dybm-∞ trained distributed manner limited memory parameters proposed structure. learning rule local space parameters associated pair units dybm-∞ updated using information available locally units. learning rule local time requires limited length history time-series. training guaranteed converge long learning rate sufﬁciently small. dybm-∞ proposed structure considered recurrent neural network taking binary values memory units speciﬁcally neuron stores eligibility traces updates values based spikes generates spikes received neurons. axon stores spikes travel pre-synaptic neuron post-synaptic neuron. synaptic weight updated every moment depending spikes generated moment values eligibility traces spikes stored axon. learning rule exhibits various characteristics stdp including long term potentiation long term depression postulated observed empirically biological neural networks. learning rule also exhibits form homeostatic plasticity similar studied bayesian spiking networks however bayesian spiking network mixture-of-expert model particular type directed graphical model study product-of-expert model particular type undirected graphical model. expect theoretical underpinnings stdp provided paper accelerate engineering applications stdp. particular prior work proposed various extensions boltzmann machine deal time-series data existing learning algorithms extended boltzmann machines involve approximations. hand homogeneous dybm considered recurrent boltzmann machine memory naturally extends boltzmann machine taking account dynamics incorporating memory. stdp dybm hebb rule boltzmann machine.", "year": 2015}