{"title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping", "tag": ["cs.CV", "cs.AI"], "abstract": "Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields.", "text": "mean field inference central statistical physics. attracted much interest computer vision community efﬁciently solve problems expressible terms large conditional random fields. however since models posterior probability distribution product marginal probabilities fail properly account important dependencies variables. mixture distributions similarly minimizes kl-divergence true posterior. introducing ideas namely conditioning groups variables instead single ones using parameter conditional random ﬁeld potentials identify temperature sense statistical physics select groups perform minimization efﬁciently. extension clamping method proposed previous works allows produce descriptive approximation true posterior inspired diverse paradigms mixture mean field approximations. demonstrate positively impacts real-world algorithms initially relied mean ﬁelds. mean field modeling technique central statistical physics century. ability handle stochastic models involving millions variables dense graphs attracted much attention community. routinely used tasks diverse detection segmentation denoising depth stereo pose-estimation approximates true probability distribution fully-factorized easy encode manipulate true distribution usually deﬁned practice conditional random field representable explicitly involves complex inter-dependencies variables. case approximation extremely useful tool. drastic approximation often conveys information interest true distribution concentrate conﬁgurations different equally likely cannot jointly encoded product law. section depicts case groups variables correlated take among many values equal probability. situation simply pick valid conﬁguration call mode ignore others. so-called structured mean field methods help overcome limitation. effective requires arbitrary choices design simpliﬁed sub-graph problem impractical especially initial densely connected. introduce novel automatically structure approximation show used return several potentially valid answers ambiguous situations. instead relying single fully factorized probability distribution introduce mixture distributions refer multi-modal mean field compute mmmf partitioning state space subsets standard approximation sufﬁces. similar spirit approach difference clamping acts simultaneously arbitrarily sized groups variables opposed time. show dealing large crfs strong correlations essential. efﬁciency mmmf choose groups. introduce temperature parameter controls much smooth original probability distribution approximation. several temperatures spot groups variables take different labels different modes distribution. force optimizer explore alternative solutions clamping them forcing take different values. temperature-based approach unlike require priori knowledge structure therefore compatible black models. conditional random fields often used represent correlations variables mean field inference means approximate computationally efﬁcient way. brieﬂy review techniques below. mean field inference possible conﬁgurations denote exponentially large makes explicit computation marginals maximum-a-posteriori intractable wide range variational methods proposed approximate among those mean field inference popular. involves introducing distribution written since fully factorized terms kl-divergence recombined expected energy containing many terms potentials convex negative entropy containing term variable. optimization performed using provably convergent gradient-descent scheme shown section simpliﬁcation sometimes comes cost downplaying dependencies variables. divmbest method addresses issue starting following observation looking assignment graphical model resulting necessarily best probabilistic model capture known problem. furthermore optimizers stuck local minima. proposed solution sequentially several local optima force different introducing diversity constraints objective function. recently shown provably effective solve diverse maps jointly constraints however none methods provide generic practical choose local constraints enforced variable sub-groups. furthermore return maps. contrast approach yields multi-modal approximation posterior distribution much richer description show useful. another approach improving approximation decompose mixture product laws clamping variables ﬁxed values ﬁnding values best factorized distribution resulting deterministic conditioning. summing resulting approximations partition function provably improve approximation true partition function procedure repeated iteratively clamping successive variables practical relatively small crfs. iteration variable clamped chosen basis graphical model weights requires intimate knowledge internals always available. finally divmbest approaches provide choose best solution without looking ground-truth except relies training classiﬁer purpose. contrast show multi modal bayesian nature output induces principled temporal consistency solve directly practical problems. motivate approach present example illustrates typical failure mode standard technique designed prevent. fig. depicts pixel represents binary variable connected neighbors attractive pairwise potentials. sake illustration split grid four zones follows. attractive terms weak left side strong right. similarly part unary terms favor value completely random bottom part. unary potentials depicted left fig. result standard approximation bottom terms probability pixels assigned label bottom right corner grid interaction potentials strong pixels assigned high probabilities could well assigned high probabilities zero. explain mmmf algorithm produce equally likely modes pixels zero high probability pixel high probability. given deﬁned respect graphical model probability states state space introduced section standard approximation models single mode discussed section therefore propose create richer representation accounts potential multiple modes replacing fully factorized distribution weighted mixture distributions better minimizes kl-divergence potential roadblock increased difﬁculty minimization problem. section present overview approach solving discuss aspects following two. formally assume partitioned disjoint subsets replace original mean field approximation form figure typical failure mode resolved mmmf. grey levels indicate marginal probabilities prior product laws approximation states individual probabilities variable take value labels probability state belongs values minimizing kl-divergence making computation tractable guarantee evaluate parameters subset separately performing standard approximation each. achieve constrain support distributions disjoint words approximation specialized subset state space computed minimize kl-divergence there. practice enrich approximation recursively splitting states among partition subsets reindexed initially represents whole state space. take newly created subset breadth-ﬁrst order preset number subsets reached. time algorithm proceeds following steps ﬁnds groups variables likely different values different modes yields binary tree whose leaves subsets forming desired statespace partition. given partition ﬁnally evaluate section introduce cardinality based criterion show makes minimization kl-divergence possible. section show entropy-based criterion selects iteration groups variables clauses depend. cardinality based clamping state space partition ≤k≤k introduced heart approximation quality tractability critically depend well chosen split obtained clamping zero value single binary variwords given states split broken subsets able. xk|xi index speciﬁc xk|xi variable. compute mean field approximation subspaces needs perform standard mean field approximation constraining probability assigned clamped variable either zero one. however limiting large dense crfs used practice clamping variable among many time little inﬂuence overall. pushing solution towards qualitatively different minimum corresponds distinct mode require simultaneously clamping many variables. remedy this retain clamping idea apply groups variables instead individual ones modes posterior keeping computationally tractable. specifestimation parameters ically given states split split cardinality-based denote groups variables chosen entropy-based criterion labels words splits variables assigned values less example semantic segmentation would segmentations cardinality clamping scheme introduced yields state space partition ≤k≤k. show given partition minimizing kl-divergence using multi-modal approximation disjointness constraint becomes tractable. small constant. makes optimization problem better behaved removes need tightly constrain individual variable retaining ability compute divergence maximized near-disjointness constraint proved formally supplementary material second equality valid constant neglecting term order appears near disjointness assumption supports. given terms constraints mixture probabilities must cardinality-based clamping scheme deﬁned performing maximization using standard lagrangian dual procedure requires evaluating constraint derivatives. despite potentially exponentially large number terms involved ways. cases lagrangian dual procedure reduces series unconstrained mean field minimizations well known additional potentials. substantially greater zero smaller treat large independent random variables under therefore gaussian approximation replace cardinality constraint simpler linear ﬁnally unary potentials problem. details provided supplementary material. recall section that general case arbitrary number modes. correspond leaves binary tree created succession cardinality-based splits. therefore consider mode branching points path leading near disjointness enforced constraints. list variables cardinality threshold sign inequality deﬁne constraint ﬁrst introduce temperature parameter model lets smooth probability distribution want approximate. well known parameter physicists used different context vision study inﬂuence corresponding approximation exploit resulting behavior select appropriate values variables. partition function normalizes integral one. reduces goes inﬁnity always yields maximum-aposteriori value becomes increasingly smooth. performing approximation high ﬁrst term kl-divergence convex negative entropy dominates makes problem convex. decreases second term kldivergence expected energy becomes dominant function stops convex local minima start appear. supplementary material introduce physics-inspired proof that case dense gaussian approximate upper-bound closed-form critical temperature divergence stops convex. validate experimentally prediction using directly densecrf code makes easy deﬁne temperature range within look generic computation possible range must determined empirically. entropy-based splitting describe approach splitting root node tree. subsequent splits done exactly way. variables clamped whose value change local minimum another force exploration minima. them start tmax temperature high enough divergence convex progressively reduce successive temperature perform approximation starting estimate previous speed computation. looking resulting approximations starting lowest temperature ones telltale sign increasing convexity assignment variables deﬁnite suddenly becomes uncertain. intuitively happens terms bind variables overcome entropy terms encourage uncertainty. physical terms viewed local phase-transition variables labels positive become candidates clamping. none increase temperature. several either pick random domain knowledge pick suitable subset values discussed results section. ﬁrst synthetic data demonstrate mmmf approximate multi-modal probability density function better standard recent approach also relies clamping explore multiple modes. demonstrate translates actual performance gain real-world algorithms—one people detection segmentation —both relying traditional mean field approach. make code test datasets publicly available. parameters control mmmf number modes cardinality threshold split value entropy thresholds hlow hhigh temperature tmax introduced section experiments hlow hhigh discussed section dense gaussian approximate upper bound critical temperature closed-form simply take tmax upper bound guarantee tmax otherwise choose tmax empirically small validation-set testing. demonstrate approach minimizes kl-divergence better standard clamping experimental protocol generate conditional random ﬁelds random weights task mmmf approximation lowest kl-divergence given number nodes. number reduces note authors look approximation log-partition function strictly minimizing kl-divergence demonstrated supplementary material. involves randomly chosen positive negative weights problem effectively mimics difﬁcult real-world ones repulsive terms uncontrolled loops strong correlations. fig. plot kl-divergence function number modes used approximate distribution standard benchmarks. modes obtained using either entropy-based criterion described section maxw refer baseline-maxw. involves sequentially clamping variable largest absolute values pairwise potentials edges linking neighbors. shown best methods among several others performed roughly similarly. experiments used phase-transition criterion section select candidate variables clamp. either randomly chose group variables clamp used maxw fig. plot resulting curves evaluated instances. ours-random performs better method baseline-maxw cases even though knowledge internals ours-maxw does performs even better. results grid demonstrate advantage clamping variables groups gets larger. figure kl-divergence using either clamping method averaged trials. vertical bars represent standard deviations. attractive means pairwise terms drawn uniformly whereas repulsive means drawn grid indicates grid topology whereas random indicates connections chosen randomly many grids. experiments variables crfs. probabilistic occupancy method relies mean field inference pedestrian detection. speciﬁcally given several cameras overlapping ﬁelds view discretized ground plane algorithm ﬁrst performs background subtraction. estimates probabilities occupancy every discrete location marginals product minimizing divergence true conditional posterior distribution formulated deﬁning energy function. value computed using generative model represents humans simple cylinders projecting rectangles various images. given probability presence absence people different locations known camera models produces synthetic images whose proximity corresponding background subtraction images measured used deﬁne energy. algorithm usually robust fail multiple interpretations background subtraction image possible. stems limited modeling power standard approximation illustrated supplementary material. show that cases replacing mmmf retaining rest fig. depicts happens replace mmmf approximate true posterior changing nothing else algorithm. generate branches binary tree section potential variables clamp described section among those clamp largest entropy gap—h using notations —and neighbors grid. evaluating cardinality constraint take meaning branch tree corresponds neighborhood selected location least person present neighborhood. since typically create locations discretizing ground plane grid cells forces newly instantiated modes signiﬁcantly different opposed featuring detection shifted centimeters. fig. plot results dotted curves representing moda scores functions distance threshold used compute cases used modes mmmf approximation followed divmbest evaluation metric produce score selecting among detection maps corresponding mode yielding highest moda score. produces dotted mmmf curves systematically blue dotted however turn improvement practical technique need choose among possible interpretations without using ground truth. temporal consistency jointly best sequence modes reconstruct trajectories sequence. original algorithm poms computed successive instants used produce consistent trajectories using k-shortest path algorithm involves building graph ground location time step corresponds node neighboring locations consecutive time steps connected. ﬁnds node-disjoint shortest paths graph cost going location proportional negative log-probability location since mmmf produces multiple poms solve multiple shortest-path problem graph additional constraint time step paths copies nodes corresponding mode described details supplementary material. solid blue lines fig. depict moda scores using ones multi-modal version label ksp∗. mmmf curves ones. makes sense ambiguous situations rarely persist frames. result enforcing temporal consistency eliminates them. crf-based semantic segmentation best known application inference computer vision many recent algorithms rely dense crf’s purpose. demonstrate mmmf approximation enhance inference component recent algorithms pascal segmentation dataset video segmentation figure replacing mmmf algorithm blue curves moda scores obtained using ones scores using mmmf. shown solid lines temporal consistency enforced dotted lines otherwise. note mmmf lines corresponding blue ones cases. frames dataset using single camera. frames terrace dataset using cameras. frames epfl-lab dataset using single camera. frames epfl-lab dataset using cameras. individual images write posterior terms approximate. create branch binary tree section ﬁrst potential variables clamp described section select ones sliding window largest entropy take evaluating cardinality constraint meaning seek dominant label among selected variables split state space half variables take value less half fig. illustrates results image dataset. evaluate results quantitatively ﬁrst divmbest metric section assume oracle select best mode multi-modal approximation looking ground truth. fig. depicts results validation pascal dataset terms average intersection union score function number modes. mode used result boils standard inference using yields improvement approximation. seem small considers modify figure qualitative semantic segmentation. original image. entropy gap. labels maximum posteriori probability approximation. labels maximum posteriori probability best mode mmmf approximation. algorithm’s inference engine leave unary terms unchanged. engine shown contribute approximately overall performance means almost double effectiveness. analysis purposes implemented baselines instead clamping groups variables clamp variable maximum entropy step. depicted curve fig. absolutely effect illustrates importance clamping groups variable instead single ones divmbest approach ﬁrst computes adds penalty term energy function another different ﬁrst. repeats process. adapted approach inference. green curve fig. depicts result mmmf outperforms semantic video segmentation. experiment images video segmentation dataset using case exploit temporal consistency avoid oracle nevertheless exploitable result section furthermore spite relatively frame-rate speciﬁcally ﬁrst deﬁne compatibility measure consecutive modes based label probabilities matching key-points compute using key-point matching algorithm compute shortest path sequence modes taking account individual mode probabilities given finally corresponding mode chosen shortest path algorithm produce segmentation. fig. report results terms score. time improvement around indicates imposing temporal consistency substantially improves quality inference. best knowledge state video semantic segmentation methods applicable image sequences. requires non-moving scenes super-pixel decomposition prevents using dense crf-based image segmentors. applied street scenes requires much higher frame rate provide accurate estimation. shown mmmf aproach makes possible structure standard approximation crfs increase performance algorithms depend effect algorithm creates several alternative approximations probabilities assigned them effectively models complex situations interpretation possible.", "year": 2016}