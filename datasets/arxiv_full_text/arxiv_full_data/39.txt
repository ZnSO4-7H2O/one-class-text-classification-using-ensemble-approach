{"title": "A Factorization Machine Framework for Testing Bigram Embeddings in  Knowledgebase Completion", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.", "text": "embedding-based knowledge base completion models mostly combined distributed representations individual entities relations compute truth scores missing links. facts however also represented using pairwise embeddings i.e. embeddings pairs entities relations. paper explore bigram embeddings ﬂexible factorization machine model several ablations investigate relevance various bigram types dataset relative improvements compared compositional model. present knowledge bases yago freebase google knowledge vault provide immense collections structured knowledge. relationships often exhibit regularities models capture used predict missing entries. common approach completion tensor factorization collection fact triplets represented sparse mode- tensor decomposed several low-rank sub-components. textual relations i.e. relations entity pairs extracted text imputation missing facts modelling together relations models rescal transe distmult models learn distributed representations entities relations infer truth value fact combining embeddings constituents appropriate composition function. factorization models however operate level embeddings single entities relations. implicit assumption facts compositional i.e. subject relation object fact atomic constituents. semantic aspects relevant imputing truth directly recovered constituents composing respective embeddings score. notation sets entities relations respectively. denote fact stating relation subject object goal learn embeddings larger sub-constituents want learn embeddings also entity pair bigram well relation-entity bigrams example consider freebase facts relation eating/practicer diet/diet object veganism. overall objects observed relation thus makes sense learn joint embedding bigrams together instead distinct embeddings atom alone learn compatibility. possible pairs entities relations. achieved using factorization machine framework modular feature components allowing selectively discard certain bigram embeddings compare relative importance. models empirically compared evaluated dataset toutanova adressing question generic bigram embeddings completion model ﬁrst time; adaption factorization machines matter; iii) experimental ﬁndings comparing different bigram embedding models fbk. brief recall factorization machines factorization machine quadratic regression model low-rank constraint quadratic interaction terms. given sparse input feature vector output prediction universal schema model riedel factorize entries together relations entity pairs extracted text embedding textual relations vector space relations. singh extend model include variety interactions entities relations using different relation vectors interact subject object both. jenatton also recognize need integrate rich higher-order interaction information score. like nickel however model speciﬁes relationships relation-speciﬁc bilinear forms entity embeddings. embedding methods completion include distmult trilinear score transe offers intriguing geometrical intuition. among aforementioned methods embeddings mostly learned individual subjects relations objects; merely model constitutes exception. methods rely expressive composition functions deal non-compositionality interaction effects neural tensor networks recently introduced holographic embeddings comparison otherwise used products composition functions models enable richer interactions unit constituent embeddings. however comes potential disadvantage presenting less wellbehaved optimisation problems slower model parameters denotes product. instead allowing individual quadratic interaction coefﬁcient pair assumes matrix quadratic interaction coefﬁcients rank thus interaction coefﬁcient feature pair represented inner product k-dimensional vectors rank constraint provides strong form regularisation otherwise over-parameterized model pools statistical strength estimating similarly proﬁled interaction coefﬁcients retains total number parameters linear summary efﬁciently harness large sparse features interactions retaining linear memory complexity. feature representation facts completion task unit bigram indicator features learn low-rank embeddings both. formalize this refer elements units fact elements bigrams fact r|e|+|r| one-hot indicator vector encodes particular unit furthermore deﬁne r|e||r| r|r||e| r|e| disregard general extension higher-order interactions described original paper consider quadratic case. also omit global model bias found helpful task empirically. subject object entity embedding used. parametrization learned. hyperparameter denoting ratio negative facts sampled positive fact contributions true false facts balanced even negative facts positives. loss differs standard negative log-likelihood objective logistic link found performs better practice. intuition comes fact instead penalizing badly classiﬁed positive facts emphasis positive facts correctly classiﬁed. since used regularization loss asymptotically linear resulting objective continuous bounded below guaranteeing well deﬁned local minimum. training details evaluation optimized loss using adam minibatches size using initial learning rate initialize model parameters furthermore hyperparameter like introduced discount importance textual mentions loss. sampling negative fact alter object given training fact random repeat times sampling negative facts every epoch anew. small implied risk sampling positive facts negative rare discounted loss weight negative samples mitigates issue further. hyperparameters selected grid search minimising mean reciprocal rank ﬁxed random subsample size validation set. reported results test set. competitive unit model scoring fact harnessing expressive beneﬁts sigmoid link function relation modelling deﬁne truth score fact sigmoid function given output model unit bigram features deﬁned score easily modiﬁed individual summands removed particular discarding summand model recovered i.e. hand alternatives model bigrams entity pairs tested removing summands single bigram remaining complementary unit distmult baseline employ ranking evaluation scheme computing ﬁltered hits scores whilst ranking true test facts among candidate facts altered object. particular bigrams observed training learned embedding; -embedding used these. nulliﬁes impact score models back-off using nonzero embeddings. results table gives overview general results different models. clearly bigram models obtain improvement unit distmult model. ﬁne-grained analysis model performances characterized whether entity pairs test facts textual mentions available training results exhibit similar pattern like models perform worse test facts model learn little without relations reversed behavior. side observation several models achieved highest overall i.e. using three light-weight bigram models performs better full even though types embeddings used. possible explanation applying embedding several interactions embeddings instead interaction makes harder learn since multiple functionalities competing. another interesting ﬁnding bigram types achieve much better results others particular model possible explanation becomes apparent closer inspection test given test fact usually contains least bigram never observed yet. cases bigram embedding design offset values used. proportions test facts happens respectively bigrams thus models already deﬁnite advantage model originates purely nature data. trivial somehow important lesson learn know relative prevalence different bigrams dataset incorporate exploit sub-tuples choose. finally initial example relation eating/practicer diet/diet object veganism indeed instances model embedding gives correct fact predictions purely compositional distmult model ranks outside generally cases single object co-appeared test fact relation training hits model distmult. supports intuition bigram embeddings fact better suited cases objects possible relation. demonstrated provide approach completion incorporate embeddings bigrams naturally. offers compact uniﬁed framework various tensor factorization models expressed including model extensive experiments demonstrated bigram models improve prediction performances substantially straightforward unigram models. surprising important result bigrams entity pairs particularly appealing. non-compositionality compositionality broader class knowledge bases involving higher order information time origin context tuples. deciding modes merged high order embedding without rely heavy cross-validation open question. thank th´eo trouillon rockt¨aschel pontus stenetorp thomas demeester discussions hints well reviewers comments. work supported epsrc studentship allen distinguished investigator award marie curie career integration award.", "year": 2016}