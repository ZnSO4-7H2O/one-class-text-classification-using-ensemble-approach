{"title": "Tensor Switching Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.", "text": "present novel neural network algorithm tensor switching network generalizes rectiﬁed linear unit nonlinearity tensor-valued hidden units. network copies entire input vector different locations expanded representation location determined hidden unit activity. even simple linear readout representation implement highly expressive deep-network-like function. network hence avoids vanishing gradient problem construction cost larger representation size. develop several methods train network including equivalent kernels inﬁnitely wide deep networks one-pass linear learning algorithm backpropagation-inspired representation learning algorithms. experimental results demonstrate network indeed expressive consistently learns faster standard relu networks. deep networks continue post impressive successes wide range tasks rectiﬁed linear unit arguably used simple nonlinearity. work develop novel deep learning algorithm tensor switching network generalizes relu hidden unit conveys tensor instead scalar yielding expressive model. like relu network network linear function input conditioned activation pattern hidden units. separating decision activate analysis performed active even linear classiﬁer reach back across layers input network implementing deep-network-like function avoiding vanishing gradient problem otherwise signiﬁcantly slow learning deep networks. trade-off representation size. exploit properties networks develop several methods suitable learning different scaling regimes including equivalent kernels svms small medium datasets one-pass linear learning algorithm visits data point large simpler datasets backpropagation-inspired representation learning algorithms generic use. experimental results show networks indeed expressive consistently learn faster standard relu networks. related work brieﬂy summarized follows. respect improving nonlinearities idea severing activation analysis weights hidden layer studied reordering activation analysis proposed tackling vanishing gradient problem tensor methods used train single-hidden-layer networks. convex learning inference various deep architectures found too. finally conditional linearity deep relu networks also used mainly analyze performance. comparison network simply reorder sever activation analysis within hidden layer. instead cross-layer generalization concepts applied recent deep learning architectures increase expressiveness also help avoiding vanishing gradient problem figure single-hidden-layer standard relu network. single-hidden-layer tensor switching relu network hidden unit conveys vector activities—inactive units convey vector zeros active units convey copy input. following ﬁrst construct deﬁnition shallow networks generalize deﬁnition deep networks ﬁnally describe qualitative properties. simplicity show fully-connected architectures using relu nonlinearity. however popular nonlinearities e.g. pooling maxout addition relu also supported fully-connected convolutional architectures. ts-relu network generalization standard relu networks permits hidden unit convey entire tensor activity describe build standard relu network. consider relu layer weight matrix rn×n responding input vector resulting hidden activity layer heaviside step function denotes elementwise product. rightmost equation splits apart hidden unit’s decision activate represented term information conveys active denoted step rewrite made following tensor operations vector-tensor cross product cijk... aibjk... tensor-matrix hadamard product c...ji a...jibji a...kji. input vector ﬁrst expanded matrix representation rn×n hidden unit. hidden unit active input vector copied corresponding row. otherwise ﬁlled zeros. finally expanded representation collapsed back projection onto central idea behind ts-relu network learn linear classiﬁer directly rich expanded representation rather collapsing back lower dimensional standard relu network hidden layer activity sent linear classiﬁer trained minimize loss function ts-relu network contrast expanded representation sent linear classiﬁer loss function ts-relu neuron thus transmits vector activities compared standard relu neuron transmits single scalar difference following call standard relu network scalar switching relu network. construction given generalizes readily deeper networks. deﬁne nonlinear expansion operation linear contraction operation becomes wl)× given layer obtain deep ts-relu network deﬁne ternary expansion operation decision activate based ss-relu variables entire tensor transmitted associated hidden unit active. l-th layer activity tensor network written ⊕xl− rnl×nl−×···×n. thus compared deep ss-relu network deep ts-relu network simply omits contraction stages contraction steps order rnl×nl−×···×n grows depth adding additional dimension layer. interpretation scheme that hidden unit layer active entire tensor copied appropriate position otherwise tensor zeros copied. another equivalent interpretation input vector copied given position hidden units layers respectively active. otherwise hence activity propagation deep ts-relu network preserves layered structure deep ss-relu network chain hidden units across layers must activate activity propagate input output. network decouples hidden unit’s decision activate analysis performed input unit active distinguishing feature leads following properties. cross-layer analysis. since representation preserves layered structure deep network offers direct access entire input simple linear readout effectively reach back across layers input thus implicitly learns analysis weights layers time therefore avoids vanishing gradient problem construction. error-correcting analysis. activation analysis severed careful selection analysis weights clean certain amount inexactitude choice activate e.g. noisy even random activation weights. network activation also implies analysis. fine-grained analysis. this consider single-hidden-layer networks hidden unit. unit active conveys entire input vector hence full-rank linear input output implemented. unit active conveys single scalar hence implement rank- linear input output. choosing right analysis weights network always implement network vice versa. such clearly greater modeling capacity ﬁxed number hidden units. although representation highly expressive comes cost exponential increase renders networks substantial width depth challenging show expressiveness permits networks perform fairly well without extremely wide deep often noticeably better networks sizes. also networks useful sizes still implemented reasonable computing resources especially combined techniques sec. section derive equivalent kernels ts-relu networks arbitrary depth inﬁnite number hidden units layer providing theoretical insight ts-relu analytically different ss-relu. kernels represent extreme inﬁnite features might used datasets small medium sizes. figure equivalent kernels function angle unit-length vectors deep ss-relu kernel converges everywhere deep tsrelu kernel converges origin everywhere else. consider single-hidden-layer ts-relu network hidden units element activation weight matrix rn×n i.i.d. zero mean gaussian arbitrary standard deviation inﬁnite-width random ts-relu kernel vectors figure compares linear kernel single-hidden-layer inﬁnite-width random ss-relu kernel important qualitative features. first discontinuous derivative hence much sharper peak kernels. intuitively means close match counts much moderately close match. second unlike ss-relu kernel non-negative everywhere ts-relu kernel still negative lobe though substantially reduced relative linear kernel. intuitively means dissimilar support vector provide evidence particular classiﬁcation negative evidence much weaker standard linear kernel. derive kernels deeper ts-relu networks need consider deeper ss-relu kernels well since activation analysis severed activation instead depends ss-relu counterpart. based upon recursive formulation ﬁrst deﬁne zeroth-layer kernel generalized angle denotes easily figure also plots deep ts-relu ss-relu kernels function depth. shape kernels reveals sharply divergent behavior networks. depth increases equivalent kernel network falls ever rapidly angle input vectors increases. means vectors must ever closer match retain high kernel value. argued earlier highlights ability network pick amplify small differences inputs resulting quasi-nearest-neighbor behavior. contrast equivalent kernel network limits depth increases. thus rather amplifying small differences collapses depth even dissimilar vectors receive high kernel values. this proof succinct using geometric view longer proof found supplementary material. kernel directly deﬁned product feature vectors naturally valid kernel. figure inverted backpropagation learning ﬂowchart denotes signal denotes pseudo gradient denotes equivalence. pathway. auxiliary pathways zl’s related nonlinear expansions al’s related linear contractions. resulting equivalent alternating expansion contraction pathway yields following present learning algorithms suitable different scenarios. one-pass ridge regression sec. learns linear readout leaving hiddenlayer representations random hence convex exactly solvable. inverted backpropagation sec. learns analysis activation weights. linear rotationcompression sec. also learns weights learns activation weights indirect way. scheme leverage intuition precision decision hidden unit activate less important carefully tuned analysis weights part compensate poorly tuned activation weights. randomly draw activation weights {wl} solve analysis weights using ridge regression done single pass dataset. first data point expanded tensor representation accumulated data points processed once analysis weights determined regularization parameter. unlike standard network setting would able select linear readout hidden layer ﬁnal classiﬁcation decision network offers direct access entire input vectors parcellated hidden units activate. even linear readout effectively reach back across layers input implementing complex function representable network random ﬁlters. however scheme requires high memory usage storing even higher computation cost solving makes deep architectures impractical. therefore scheme best suit online learning applications allow one-time access data require deep classiﬁer. ridge regression learning uses random activation weights learns analysis weights. provide gradient-based procedure learn weights. learning analysis weights simply requires generally easy compute. however since activation weights network appear inside heaviside step function zero derivative gradient also zero. bypass this introduce sequence auxiliary variables deﬁned recursion al−wl rnl×nl−×···×nl. derive pseudo gradient using proposed inverted backpropagation denotes moore–penrose pseudoinverse. al’s related linear contraction operator derivatives non-zero easy compute. works sufﬁciently well non-zero proxy motivation scheme recover learning behavior networks. this ﬁrst note reﬂects fact networks linear active hidden units known order expansion contraction steps effect ﬁnal output. hence linear contraction steps alternate expansion steps instead gathered expansion steps. gradient network replacing expanded representation inﬂuence inverted gradient recover compared one-pass ridge regression scheme controls memory makes training moderately-sized network modern computing resources feasible. ability train activation weights also relaxes assumption analysis weights clean inexact activations caused using even random weights. although inverted backpropagation learning controls memory time complexities better one-pass ridge regression exponential growth network’s representation still severely constrains potential toward applied recent deep learning architectures network width depth easily beyond e.g. thousand. addition success recent deep learning architectures also heavily depends acceleration provided highly-optimized gpu-enabled libraries operations previous learning schemes mostly unsupported. address concerns provide standard backpropagation-compatible learning algorithm longer keep separate variables. instead deﬁne ×nlnl−. directly ﬂattens expanded representation linearly projects scheme even though still lacks non-zero gradient previous layer learned using backpropagation properly rotate utilized nonlinearity. therefore representation learning becomes indirect. simultaneously control representation size easily becomes compressive. interestingly often works surprisingly well suggests linearly compressing expanded representation back size representation still retain advantage thus used default. scheme also combined inverted backpropagation learning still desired. understand linear compression remove representation power note equivalent linear contraction operation tensor-valued unit projected independently. linear compression introduces extra interaction tensor-valued units. another view linear compression’s role kernel analysis shown sec. —adding linear layer change shape given kernel. experiments focus comparing networks goal determining nonlinearities differ counterparts. svms using ss-relu ts-relu kernels implemented matlab based libsvm-compact networks learning algorithms sec. implemented python based numpy’s ndarray data structure. implementations utilize multicore acceleration. addition networks linear rotation-compression learning also implemented keras enjoys much faster acceleration. adopt datasets viz. mnist cifar svhn reserve last training images validation. also include svhn’s extra training training process zero-pad mnist images datasets spatial resolution—× figure comparison models. dot’s coordinate indicates differences one-pass asymptotic error rates pair models sharing hyperparameters. ﬁrst quadrant shows better errors. validation error rates v.s. training time cifar shallower intermediate deeper models. svms grid search kernels depth dimension reduction images reduction. networks fully-connected architectures grid search depth width based python implementation. networks convolutional architectures adopt vgg-style convolutional layers standard convolution-max pooling blocks block three convolutions plus fully-connected layers ﬁxed width experiments based keras implementation. mlps cnns universally learning rate momentum weight decay batch size reduce grid search complexity focusing architectural hyperparameters. networks trained epochs mnist cifar epochs svhn without data augmentation. source code scripts reproducing experiments available https//github.com/coxlab/tsnet. table summarizes experimental results including one-pass asymptotic error rates corresponding depths nonlinearities perform better almost categories conﬁrming theoretical insights sec. .—the cross-layer analysis error-correcting analysis ﬁne-grained analysis demonstrate using nonlinearities affects distribution performance across different architectures plot performance gains introduced using nonlinearities variants fig. fact dots ﬁrst quadrant suggests nonlinearities predominantly beneﬁcial. also ease concern networks’ higher complexity simply consume advantage actual time also provide examples learning progress time fig. results suggest even unoptimized network implementation still provide sizable gains learning speed. finally verify effectiveness inverted backpropagation learning useful activation ﬁlters even without actual gradient train single-hidden-layer mlps hidden units visualize learned ﬁlters fig. results suggest inverted backpropagation functions equally well. networks learn quickly? general network sidesteps vanishing gradient problem skips long chain linear contractions analysis weights linear readout direct access full input vector switched different parts highly expressive expanded representation. directly accelerates learning. also well-ﬂowing gradient confers beneﬁts beyond layers—e.g. layers placed layers also learn faster since layers self-organize rapidly permitting useful error signals lower layers faster. lastly using inverted backpropagation linear rotationl learn fast still quite compression learning although {wl} random ﬁrst epochs error-correcting nature still compensate learning progress. challenges toward deeper networks. shown fig. equivalent kernels deeper networks extremely sharp discriminative unavoidably hurts invariant recognition dissimilar examples. explain nonlinearities higher layers works better since lower layers form invariant representations higher layers classify. remedy this need consider types regularization smoothing techniques future work. main future direction improve network’s scalability require parallelism customization preferably memory storage/bandwidth improved scalability also plan verify nonlinearity’s efﬁciency state-of-the-art architectures still computationally prohibitive current implementation. would like thank james fitzgerald mien brabeeba wang scott linderman fruitful discussions. also thank anonymous reviewers valuable comments. work supported iarpa swartz foundation.", "year": 2016}