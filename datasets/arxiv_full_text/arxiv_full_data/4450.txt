{"title": "GANosaic: Mosaic Creation with Generative Texture Manifolds", "tag": ["cs.CV", "stat.ML"], "abstract": "This paper presents a novel framework for generating texture mosaics with convolutional neural networks. Our method is called GANosaic and performs optimization in the latent noise space of a generative texture model, which allows the transformation of a content image into a mosaic exhibiting the visual properties of the underlying texture manifold. To represent that manifold, we use a state-of-the-art generative adversarial method for texture synthesis, which can learn expressive texture representations from data and produce mosaic images with very high resolution. This fully convolutional model generates smooth (without any visible borders) mosaic images which morph and blend different textures locally. In addition, we develop a new type of differentiable statistical regularization appropriate for optimization over the prior noise space of the PSGAN model.", "text": "paper presents novel framework generating texture mosaics convolutional neural networks. method called ganosaic performs optimization latent noise space generative texture model allows transformation content image mosaic exhibiting visual properties underlying texture manifold. represent manifold state-of-the-art generative adversarial method texture synthesis learn expressive texture representations data produce mosaic images high resolution. fully convolutional model generates smooth mosaic images morph blend different textures locally. addition develop type differentiable statistical regularization appropriate optimization prior noise space psgan model. mosaics classical form. romans masters skillfully selecting small colored stones make beautiful mosaics large scenes. later paintings renaissance painter archimboldo composed various objects make amazing portraits people. general mosaics work properties human visual system average colors spatial regions looking distance large image emerges looking closely details single tiles emerge. modern times computer graphic algorithms enabled different forms digital image mosaics however methods distinct non-overlapping small tiles paint large image. seamless mosaic style like archimboldo’s whole image acts mosaic without tiles borders visually closer modern methods texture synthesis transfer. image quilting recombines patches original textures order smoothly reconstruct target image texture transfer\". however disadvantage high runtime complexity generating large images. addition since instance models merely copy original pixels cannot used generalize create novel textures multiple examples. work uses discriminatively trained deep neural network effective parametric image descriptors allowing texture synthesis novel form texture transfer called neural style transfer.\" however texture synthesis transfer performed single example image lacks ability represent morph textures deﬁned several different images. spatial versions generative adversarial networks well suited unsupervised learning textures periodic spatial allows high quality texture synthesis efﬁcient memory speed usage. also leverage information many input images learn texture manifold rich distribution many textures allowing morphing novel textures. generative models give widely varied outputs instance neural descriptor based approaches texture synthesis. variety proposed method. figure texture manifold learned psgan satellite images city sydney serving texture prior neural mosaic. ganosaic renders human portrait mosaic size pixels minimizing loss function paint target image stay close texture manifold w.r.t. texture generator input tensor spatial dimensions channels. perceptual distance loss represented layer conv- pre-trained network. mosaic best seen zoomed-in appreciate rich small details city forest. texture style images size pixels used training psgan. novel ganosaic method steps. first psgan trained example images details section second generator psgan used module optimization problem generate image close possible content\" image staying learned texture manifold style\". adapt input noise tensor s.t. output image generator close possible target image done deﬁning distance loss function optimizing w.r.t. however psgan training came prior noise distribution mosaic optimization push values away prior distribution lead degenerate looking textures. aesthetically pleasing mosaics want stay close possible texture manifold style\". psgan texture model mosaic style better represented texture loss term ensures input tensor stays close prior statistics used psgan training. concretely model loss ensure local channels keep statistical independence. total loss function therefore composed parts content loss texture loss denotes mean squared tensor elements. mapping correspondence map\" speciﬁes perceptual distance metric want w.r.t. content image. simple predeﬁned image transformation complex approach e.g. utilizing outputs pretrained convolutional ﬁlters using image downscaling operator split frequencies resultant image frequencies determined content image high frequencies come texture manifold. split improves mosaic quality section ablation studies regarding effects choice texture loss also required keep optimized noise tensor close manifold textures created prior noise distribution. regularizes local noise channels noise tensor section details loss ltex effect mosaic output. generation seamless mosaics unique texture visual aesthetics ﬂexible differentiable texture model learns morphs diverse texture images large scalability respect output mosaic size calls generator efﬁciently split small tensor chunks seamlessly forming large ﬁnal image section contains brief summary texture model psgan elaboration generative adversarial networks learn generator network distort noise vector sampled standard distribution distorted probability distribution close distribution observed training samples form rh×w×. achieved game theoretic idea letting generator network play additional network discriminator task discriminator classify sample generator training generator tries good possible producing samples classiﬁed discriminator real training data. extensions psgan beyond standard framework threefold. first spatial gans architecture chosen fully convolutional version dcgan noise vector extended spatial tensor rl×m×d. here spatial dimensions channel dimension. hence akin dcgan fractionally strided convolutions psgans upsample spatial dimensions output dimensions case typically hence total upsampling case convolution layers fractional stride discriminator output location advantage approach image patches used training minibatches different size image outputs used sampling model yielding arbitrary large output resolution. however receptive ﬁeld single location tensor spatially limited output away regions independently sampled. local statistics must therefore independent position words sampling single texture possible. overcome limitation second extension fraction channels spatially shared allow conditioning global structure. ﬁnal extension implement spatial basis parts used anchor image generation. shown plane wave parameterization spatial basis allows generation periodic textures also lead better quality non-periodic textures. wave numbers plane waves psgan given functions global channels multi-layered perceptron learned end-to-end alongside total tensor consists three parts local part global part periodic part concatenated channel dimension. learning spatially shared global channels deﬁne texture sampled independent samples local dimensions give rise local pattern variation. importantly global channels allowed change smoothly spatial dimensions yields spatial transitioning textures locally still plausible textures. hence speak texture manifold. figure illustration psgan model. fully convolutional generator network maps spatial tensor zλµi spatial indices input image every subvector spatial location e.g. blue green columns figure limited area usual training discriminator gets either generated image image patch real data. figure examples psgan texture models used mosaics. morph plots show ability texture manifolds smoothly change texture processes. plots created bi-linearly interpolating tensor random texture samples. figure shows texture manifolds learned psgan look. input texture images psgan training sydney satellite images google maps stone wall images wikimedia commons scaly\" generator psgan trained using batch normalization every deconvolutional layer. batch normalization calculates layer statistics capture distribution feature activations minibatch input however ganosaic method different psgan training found empirically ganosaic works better using ﬁxed statistics batch normalization operations trained concretely pre-calculate batch normalization statistics batch many instances sampled prior. afterwards statistics used constant rescaling batch normalization operation inside network also makes easier practical implementation splitting procedures large mosaics ability ganosaic. figure reference density ˆpprior pprior using uniform distribution prior pprior gauss kernel density estimation. samples drawn different distribution prior. grid regularly spaced points measure distance reference density kernel density estimate samples. minimizing distance yields samples consistent prior pprior initial samples optimization w.r.t. content loss introduce spatial correlations local dimensions. training texture model however local dimensions psgan model independently sampled prior every spatial position channel dimension. hence correlations imply move away learned texture manifold. remedy this introduce regularization term idea samples taken joint distribution neighboring local dimensions distributed according prior distribution training case prior independent uniform distribution means samples whole hypercube contrast local dimensions perfectly correlated samples would exclusively diagonal hypercube. implement idea assume independence channel dimension considered different channels samples. employing kernel density estimate joint distribution estimated compared prior distribution. practical reasons pairwise neighboring positions evaluated. restriction neighboring positions justiﬁed noting correlations natural images fall monotonously distance computational beneﬁt reduction quadratic linear time complexity. formally write texture loss term ltex measures distance probability distributions square brackets denote concatenation column vectors matrix. spatial offsets determines neighboring positions distribution regularized. took kernel density estimate given rdl× evaluated point given valid kernel function used employed gaussian kernel. form target probability distribution regularizer convolution prior probability distribution gaussian kernel i.e. ˆpprior pprior finally distance function probability distributions needs deﬁned. simply calculate distance square difference distributions evaluated makes regularizer differentiable function w.r.t. figure gives example behavior regularizer. note regularizer similar determinantal point processes particular resulting samples tend regular comparison samples prior. texture optimization procedure used gradient optimization constrained values compatible prior distribution. speed single gradient step seconds pixel image seconds pixels tesla usually less iterations enough nice looking mosaic images. timings apply simple perceptual distance standard psgan architecture using neural network cost time depending layers channels. general expect large content image rich psgan model make optimization complicated require iterations optimize inspect detail. small texture sets training images expressive psgan models could used good-looking mosaics. optimizer worked quite well starting single texture found iterations stochastic search using sampled mosaic lowest loss initialization help optimizer reach solutions slightly better trivial initialization random texture. addition randomly sampling initializations optimizer able explore multiple different solutions mosaic loss optimization another source aesthetic variety. correspondence used content loss encodes directly choice perceptual distance metric allows ﬂexibility transfer texture appearance mosaic. identity images mosaic output degenerate. figure shows drawback identity correspondence content image high frequencies difﬁcult texture manifold. adding downscaling emphasize lower frequencies content image lead better texture appearance figure. downscaling much make mosaic reproduce content image less accurately figure trade-off stone textures really well recognizable. another interesting choice instead exact colors luminance channel image deﬁned average channels. useful texture manifold different color content image. figure shows example mosaic luminance color variation downscaling color transformations examples manually speciﬁed correspondence maps alternative take ﬁlters pretrained network similar approach figure shows results different aesthetics choices perceptual distance ﬂexible w.r.t. color also ﬂexible w.r.t. spatial matching content. network architecture deep wide uses pooling operations. speciﬁc choice correspondence always outputs constant value equivalent disabling whole content loss term ganosaic loss. case introduce simpler alternative method create mosaic images using psgan texture generator. directly paint noise global dimensions values related pixels content image. e.g. random linear projection pixels channels global noise tensor followed nonlinearity keep values concretely given image size pixels calculate downscaled version size pixels spatial resolution latent noise space sample random matrix rdg× calculate sin. afterwards generate image projection spatial position local noise prior. simple approach useful exploration texture manifolds fast calculate. figure shows example random projection paints frequency segments content image exhibiting details texture manifold. useful figure downscaling average pooling correspondence allows tradeoff content loss accurate content image rendering strong texture aesthetics mosaics balanced. mosaics resolution pixels images stones wikimedia commons textures. creating smoothly morphing videos illustrating random walks space texture manifolds projecting speciﬁc content image usually preserving well frequencies content. please example video guevara rendered sydney textures youtu.be/ gafqwekls. drawback random projection mosaics completely ignore high frequencies content image. random projection output also unpredictable projections look better others fast generation speed enables exploring many images artistic selection process. optimizing local values together global lead lower content loss tuning alone ﬁxing random sample prior. however special care needs taken keep distribution close prior distribution avoid degeneracy texture manifold thus texture loss term ltex deﬁned section figure shows graphically mosaic quality obtained optimizing regularizing using texture loss. term acts regularization term helpful preventing degeneration. also allows ganosaic reach better content loss optimizing rather figure shows plots convergence emphasizes optimization behaviour different settings previous ﬁgure. plot shows values content loss optimizing different settings optimizing leads lower content loss. texture loss effectively lowered intuition tensors look display images channels tensor. figure correlated content image deviate prior. values lead degeneracy displayed figure effect term ltex de-correlates content image makes locally consistent uniform prior corresponds mosaic figure figure results using different correspondence maps. shows using luminance allows make mosaics experimental look away original color palette used layers network encode complex perceptual distances. used textures knitted\" scaly\" juan miro paintings please zoom-in details. figure random projections useful texture exploration sydney satellite views. content image size projected global noise channels size spatially local noise prior. resulting generates pixel output image. figure different mosaic results zoom-in right details. optimizes global dimensions worse content also optimize local dimensions content better. however lower content loss also combined degeneration texture manifold people texture become blurry. texture loss corrects degeneration still ﬁtting content better texture many small people crowd used emphasizes visual degeneracy correct random-like texture appearance figure show optimization convergence plots random projection steps optimizer decreases costs bfgs gradient steps. compare optimizing optimizing setting regularizing setting thin lines different runs dashed line mean. show images channels optimization values correlated content image different random noise prior lead degenerate looking textures output mosaic. effect make channels closer prior. section contains short discussion properties ganosaic mosaics. traditionally photomosaic algorithms utilized large image datasets texture rendering usually done single texture contrast ganosaic rich texture manifolds style representations allows generation convolutional neural networks large mosaics smoothly rendering content image. generation texture mosaics achieved optimization latent noise space psgan texture model. application ganosaic consumer-facing apps allow quick rendering user content using pre-trained texture models. another professional case graphical design artist psgan ganosaic tools creative process practical recommended user regard ganosaic process fully automatic semi-automatic artist explore results different initializations iterations optimizer select mosaics show textures right aesthetic look. fast runtime ability handle arbitrarily large output image resolutions comes spatial architecture used texture prior ganosaic. generation easily split calculating separate subtensors combining results. property applies also optimization framework since loss function terms calculated aggregated spatial chunks. thus mosaics high resolution practically unlimited memory made. computation time scales linearly number output pixels. order minimize mosaic loss equation different spatial regions input noise tensor converge different values thus image output generator mosaic different texture processes. works well frequencies content image limit high frequencies output mosaic receptive ﬁeld generator model determines highest possible frequencies obtain mosaics setting generator input. optimizing local global dimensions improves texture mosaic figure using style image human photography content neural style transfer method produces image shown detailed look output image reveals interesting artifacts. jacket segment good stone mosaic face segment issues rendered similar content pixels lacks stone texture. resolution allows paint ﬁner details better ﬁtting content always limit high mosaic image frequencies level pixel detail achievable. downscaling improve image quality output mosaic acts like averaging ﬁlter removes high frequencies content image. thus content loss depends lower frequencies high frequencies determined texture manifold. choice content image pixel size matter well. large content images lower frequencies easier mosaics. practice always upscale input image obtain larger mosaics lower frequencies. using smaller texture brush relative image leads lower content loss analogous large photomosaic small tiles. hand smaller content images imply larger textures relative challenging case mosaic optimization. approach ganosaic locally right\" texture different neural style approach figure shows example neural style transfer using style image many textures. style descriptor consists feature correlations marginalized spatial extent image style transfer transfer full distribution style image content thus mixed stone background. pixel space optimization preserve high frequency details content image regions looks like merely painting directly content rather trying represent textures. contrast ganosaic works modifying input noise tensor generator network network acts regularizer outputs images close underlying psgan texture manifold. ganosaic optimize directly pixels output mosaic image modiﬁes input constrained optimization problem since noise space much lower spatial size image e.g. layers image pixels generated tensor size spatial positions. related work explored blending multiple styles parametrically mixing statistical descriptors result style averaging globally properties input styles rather painting locally different styles. contrast ganosaic focus different modes texture distribution well approximates content image locally regularization ensures local region texture manifold. references bergmann nikolay jetchev roland vollgraf. learning texture manifolds periodic spatial gan. proceedings international conference machine learning alexei efros william freeman. image quilting texture synthesis transfer. proceedings annual conference computer graphics interactive techniques siggraph goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems", "year": 2017}