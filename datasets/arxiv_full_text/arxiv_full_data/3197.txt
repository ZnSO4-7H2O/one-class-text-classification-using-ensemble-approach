{"title": "Dictionary learning for fast classification based on soft-thresholding", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Classifiers based on sparse representations have recently been shown to provide excellent results in many visual recognition and classification tasks. However, the high cost of computing sparse representations at test time is a major obstacle that limits the applicability of these methods in large-scale problems, or in scenarios where computational power is restricted. We consider in this paper a simple yet efficient alternative to sparse coding for feature extraction. We study a classification scheme that applies the soft-thresholding nonlinear mapping in a dictionary, followed by a linear classifier. A novel supervised dictionary learning algorithm tailored for this low complexity classification architecture is proposed. The dictionary learning problem, which jointly learns the dictionary and linear classifier, is cast as a difference of convex (DC) program and solved efficiently with an iterative DC solver. We conduct experiments on several datasets, and show that our learning algorithm that leverages the structure of the classification problem outperforms generic learning procedures. Our simple classifier based on soft-thresholding also competes with the recent sparse coding classifiers, when the dictionary is learned appropriately. The adopted classification scheme further requires less computational time at the testing stage, compared to other classifiers. The proposed scheme shows the potential of the adequately trained soft-thresholding mapping for classification and paves the way towards the development of very efficient classification methods for vision problems.", "text": "classiﬁers based sparse representations recently shown provide excellent results many visual recognition classiﬁcation tasks. however high cost computing sparse representations test time major obstacle limits applicability methods large-scale problems scenarios computational power restricted. consider paper simple eﬃcient alternative sparse coding feature extraction. study classiﬁcation scheme applies soft-thresholding nonlinear mapping dictionary followed linear classiﬁer. novel supervised dictionary learning algorithm tailored complexity classiﬁcation architecture proposed. dictionary learning problem jointly learns dictionary linear classiﬁer cast diﬀerence convex program solved eﬃciently iterative solver. conduct experiments several datasets show learning algorithm leverages structure classiﬁcation problem outperforms generic learning procedures. simple classiﬁer based soft-thresholding also competes recent sparse coding classiﬁers dictionary learned appropriately. adopted classiﬁcation scheme requires less computational time testing stage compared classiﬁers. proposed scheme shows potential adequately trained soft-thresholding mapping classiﬁcation paves towards development eﬃcient classiﬁcation methods vision problems. recent decade witnessed emergence huge volumes high dimensional information produced sorts sensors. instance massive amount high-resolution images uploaded internet every minute. context challenges develop techniques process large amounts data computationally eﬃcient way. focus paper image classiﬁcation problem challenging tasks image analysis computer vision. given training examples multiple classes goal rule permits predict class test samples. linear classiﬁcation computationally eﬃcient categorize test samples. consists ﬁnding linear separator classes. linear classiﬁcation focus much research statistics machine learning decades resulting algorithms well understood. however many datasets cannot separated linearly require complex nonlinear classiﬁers. popular nonlinear scheme leverages eﬃcency simplicity linear classiﬁers embeds data high dimensional feature space linear classiﬁer eventually sought. feature space mapping chosen nonlinear order convert nonlinear relations linear relations. nonlinear classiﬁcation framework heart popular kernel-based methods make computational shortcut bypass explicit computation feature vectors. despite popularity kernel-based classiﬁcation computational complexity test time strongly depends number training samples limits applicability large scale settings. recent approach nonlinear classiﬁcation based sparse coding consists ﬁnding compact representation data overcomplete dictionary. sparse coding known beneﬁcial signal processing tasks denoising inpainting coding also recently emerged context classiﬁcation viewed nonlinear feature extraction mapping. usually followed linear classiﬁer also used conjunction classiﬁers classiﬁcation architectures based sparse coding shown work well practice even achieve state-of-the-art results particular tasks crucial drawback sparse coding classiﬁers however prohibitive cost computing sparse representation signal image sample test time. limits relevance techniques large-scale vision problems computational power scarce. consider one-sided version soft-thresholding function equal zero negative values naturally extended vectors applying scalar coordinate independently. given dictionary applied transformed signal represents coeﬃcients features signal outcome considers important features used classiﬁcation. details consider paper following simple two-step procedure classiﬁcation architecture illustrated fig. proposed classiﬁcation scheme advantage simple eﬃcient easy implement involves single matrix-vector multiplication operation. soft-thresholding successfully used well number deep learning architectures shows relevance eﬃcient feature extraction mapping. remarkable results coates show simple encoder coupled standard learning algorithm often achieve results comparable sparse coding provided number labeled samples dictionary size large enough. however case proper training classiﬁer parameters becomes crucial reaching good classiﬁcation performance. objective paper. propose novel supervised dictionary learning algorithm call last jointly learns dictionary linear classiﬁer tailored classiﬁcation architecture based soft-thresholding. pose learning problem optimization problem comprising loss term controls classiﬁcation accuracy regularizer prevents overﬁtting. problem shown diﬀerence-of-convex program solved eﬃciently iterative solver. perform extensive experiments textures digits natural images datasets show proposed classiﬁer coupled dictionary learning approach exhibits remarkable performance respect numerous competitor methods. particular show classiﬁer provides comparable better classiﬁcation accuracy sparse coding schemes. rest paper organized follows. next section highlight related work. section formulate dictionary learning problem classiﬁers based soft-thresholding. section presents novel learning algorithm last based optimization. section perform extensive experiments textures natural images digits datasets section ﬁnally gathers number important observations dictionary learning algorithm classiﬁcation scheme. ﬁrst highlight section diﬀerence proposed approach existing techniques sparse coding dictionary learning literature. then draw connection considered approach neural network models architecture optimization aspects. classiﬁcation scheme adopted paper shares similarities popular architectures sparse coding feature extraction stage. recall sparse coding mapping applied datapoint known that parameters sparse coding classiﬁer trained discriminative excellent classiﬁcation results obtained many vision tasks particular signiﬁcant gains standard reconstructive dictionary learning approaches obtained dictionary optimized classiﬁcation. several dictionary learning methods also consider additional structure dictionary order incorporate task-speciﬁc prior knowledge line research especially popular face recognition applications mixture subspace model known hold knowledge discriminative dictionary learning methods optimize dictionary regards sparse coding variant still requires solve trivial optimization problem. work however introduce discriminative dictionary learning method speciﬁc eﬃcient soft-thresholding map. interestingly softthresholding viewed coarse approximation non-negative sparse coding show appendix motivates soft-thresholding feature extraction merits sparse coding classiﬁcation well-established. closer work several approaches introduced approximate sparse coding eﬃcient feed-forward predictor whose parameters learned order minimize approximation error respect sparse codes. works however diﬀerent several aspects. first approach require result soft-thresholding mapping close sparse coding. rather require solely good classiﬁcation accuracy training samples. moreover dictionary learning approach purely supervised unlike kavukcuoglu finally methods often nonlinear maps multi-layer soft-thresholding gregor lecun diﬀerent considered paper. single soft-thresholding mapping considered advantage simple eﬃcient easy implement practice. also strongly tied sparse coding classiﬁcation architecture considered work also quite strongly related artiﬁcial neural network models neural network models multi-layer architectures layer consists neurons. neurons compute linear combination activation values preceding layer activation function used convert neurons’ weighted input activation value. popular choices activation functions logistic sigmoid hyperbolic tangent nonlinearities. classiﬁcation architecture seen neural network hidden layer hidden units’ activation function zero bias equivalently activation function constant bias across hidden units. dictionary deﬁnes connections input hidden layer represents weights connect hidden layer output. important recent contribution glorot showed using rectiﬁer activation function results better performance deep networks classical hyperbolic tangent function. that rectiﬁer nonlinearity biologically plausible leads sparse networks; property highly desirable representation learning architecture considered paper close glorot diﬀers several important aspects. first architecture sparsity features. justiﬁed fact approximant non-negative sparse coding sparsity penalty without imposing restriction neurons’ bias rectiﬁer networks representation might however sparse. potentially explains necessity additional sparsifying regularizer activation values glorot enforce sparsity network sparsity achieved implicitly scheme. second unlike work employs biological argument introduce rectiﬁer function choose soft-thresholding nonlinearity strong relation sparse coding. work therefore provides independent motivation considering rectiﬁer activation function biological motivation turn gives another motivation considering soft-thresholding. third rectiﬁed linear units often used context deep networks seldom used hidden layer. sense classiﬁcation scheme considered paper simpler description seen particular instance general neural network models. optimization perspective learning algorithm leverages simplicity classiﬁcation architecture diﬀerent generic techniques used train neural networks. particular neural networks generally trained stochastic gradient descent adopt optimization based framework directly exploits structure learning problem. problem formulation present learning problem estimates jointly dictionary rn×n linear classiﬁer fast classiﬁcation scheme described section consider binary classiﬁcation task rn×m denote respectively training points denotes convex loss function penalizes incorrect classiﬁcation training sample regularization parameter prevents overﬁtting. soft-thresholding deﬁned typical loss functions used hinge loss max) adopt paper smooth approximation logistic loss log). optimization problem attempts dictionary linear separator sign training leads correct classiﬁcation. time keeps small order prevent overﬁtting. note simplify exposition bias term linear classiﬁer dropped. however study extends straightforwardly include nonzero bias. problem formulation reminiscent popular support vector machine training instead embed nonlinearity directly procedure linear classiﬁer learned. problem formulation learn jointly dictionary linear classiﬁer signiﬁcantly broadens applicability learned classiﬁer important nonlinear classiﬁcation tasks. note however adding nonlinear mapping raises important optimization challenge learning problem convex. ν/α. therefore without loss generality sparsity parameter rest paper. contrast traditional dictionary learning approaches based minimization problems sparsity parameter needs manually beforehand. fixing unconstraining norms dictionary atoms essentially permits adapt sparsity problem hand. represents important advantage setting sparsity parameter general diﬃcult task. problem non-convex diﬃcult solve general. section propose relax original optimization problem cast diﬀerence-of-convex program. leveraging property equivalence problem formulations eqs. holds components linear classiﬁer restricted zero. however limiting assumption zero components normal vector optimal hyperplane removed equivalent using dictionary smaller size. variable sign components essentially encodes classes diﬀerent atoms. words atom likely active samples class conversely atoms likely active class samples. assume vector known priori. words means prior knowledge proportion class class atoms desired dictionary. example setting half entries vector equal half encodes prior knowledge searching dictionary balanced number class-speciﬁc atoms. note estimated distribution diﬀerent classes training assuming proportion class-speciﬁc atoms dictionary approximately follow training samples. vj)+ smooth function parameter controls accuracy approximation speciﬁcally increases quality approximation becomes better. function often referred soft-plus plays important role training objective many classiﬁcation schemes classiﬁcation restricted boltzmann machines note approximation used make optimization easier learning stage; test time original soft-thresholding applied feature extraction. problem still nonconvex optimization problem hard solve using traditional methods gradient descent newton-type methods. however show section problem written diﬀerence convex program leads eﬃcient solutions. proposition states problem give explicit decomposition objective function crucial optimization. following proposition exhibits decomposition hinge loss. problems well studied optimization problems eﬃcient optimization algorithms proposed good performance practice references therein sriperumbudur exists number popular approaches solve globally programs techniques often ineﬃcient limited small scale problems. robust eﬃcient diﬀerence convex algorithm proposed suited solving general large scale programs. iterative algorithm consists solving iteration convex optimization problem obtained proven theorem refer paper theoretical guarantees stability robustness algorithm. although guaranteed reach local minima authors state often converges global optimum. case using multiple restarts might used improve solution. note close concave-convex procedure introduced deﬁned proposition note that convexity problem convex solved using convex optimization algorithm method propose projected ﬁrst-order stochastic subgradient descent algorithm. stochastic gradient descent eﬃcient optimization algorithm handle large training sets make exposition clearer ﬁrst deﬁne function details iteration algorithm training sample drawn. updated performing step direction many diﬀerent stepsize rules used stochastic gradient descent methods. paper similarly strategy employed mairal chosen stepsize remains constant ﬁrst iterations takes value ρt/t. moreover accelerate convergence stochastic gradient descent algorithm consider small variation algorithm minibatch containing several training samples along labels drawn iteration instead single sample. classical heuristic stochastic gradient descent algorithms. note that size minibatch equal number training samples algorithm reduces traditional batch gradient descent. finally complete last learning algorithm based formally given algorithm starting feasible point last solves iteratively constrained convex problem given solution proposed algorithm recall problem corresponds original program except function replaced linear approximation around current solution iteration many criteria used terminate algorithm. choose terminate maximum number iterations reached terminate algorithm earlier following condition satisﬁed matrix concatenation small positive number. condition detects convergence learning algorithm veriﬁed whenever change small. termination criterion used example sriperumbudur section evaluate performance classiﬁcation algorithm textures digits natural images datasets compare diﬀerent competitor schemes. expose section choice parameters model algorithm. focus experimental assessment scheme. following methodology coates break feature extraction algorithms learning algorithm basis functions learned encoding function sparse coding) maps input point feature vector. ﬁrst step analysis therefore encoder soft-thresholding mapping compare last existing supervised unsupervised learning techniques. then following subsections compare complete classiﬁcation architecture several classiﬁers terms accuracy eﬃciency. particular show proposed approach able compete recent classiﬁers despite simplicity. ﬁrst discuss choice model parameters method. unless stated otherwise choose vector according distribution diﬀerent classes training set. value regularization parameter found empirically good choice experiments. worth mentioning setting cross-validation might give better results would also computationally expensive. moreover parameter soft-thresholding mapping approximation recall ﬁnally sparsity parameter always equal method therefore require manual setting cross-validation procedure. experiments moreover chosen initialize last setting equal random subsample training vector whose entries equal however noticed empirically choosing diﬀerent initialization strategy signiﬁcantly change testing accuracy. then maximum number iterations last moreover setting properly parameters algorithm quite crucial controlling convergence algorithm. experiments parameter denotes number iterations. furthermore ﬁrst iterations several values tested value leads smallest objective function chosen rest iterations. finally minibatch size algorithm depends size training data. particular size training data relatively small used batch gradient descent computation gradient tractable. case number iterations otherwise batch size perform iterations stochastic gradient descent algorithm ﬁrst experiments focus comparison learning algorithm learning techniques encoder soft-thresholding mapping methods. present comparative study textures natural images classiﬁcation tasks. supervised random samples atoms chosen randomly training supervised manner. denotes desired proportion class atoms dictionary dictionary built randomly picking training samples class samples class number atoms dictionary. supervised k-means build dictionary merging subdictionaries obtained applying k-means algorithm successively training samples class number clusters ﬁxed respectively log). corresponds original objective function replaced smooth approximant. smoothing procedure similar used relaxed formulation last initialization strategy. setting allows directly compare last generic stochastic gradient descent procedure widely used training neural networks. following glorot mini-batch linear classiﬁer trained feature space. random samples k-means approaches consider classiﬁcation tasks roughly equal number training samples class. finally last dictionary linear classiﬁer learned simultaneously. encoder used compute features. similarly taking patches texture. test data contain training patches. patches moreover normalized unit norm. fig. shows binary classiﬁcation accuracy soft-thresholding based classiﬁer function dictionary size dictionaries learned diﬀerent algorithms. algorithm becomes less crucial dictionaries large. second diﬃcult classiﬁcation task algorithm yields best classiﬁcation accuracy tested dictionary sizes dictionary sizes. using much larger dictionaries might result performance close obtained using algorithm comes price additional computational memory costs. fig. illustrates evolution objective function respect elapsed training time last dictionary size last quickly converges solution small objective function. hand reaches solution larger objective function last. learning algorithms restrict ﬁrst stage dataset classes deer horse. extend results multi-class scenario later section fig. illustrates training examples classes. classiﬁcation results reported fig. again soft-thresholding based classiﬁer dictionary linear classiﬁer learned last outperforms learning techniques. particular using last dictionary learning strategy results signiﬁcantly higher performance stochastic gradient descent dictionary sizes. note small dictionary last reaches accuracy whereas learning algorithms reach accuracy even dictionary contains many atoms. illustrate point show fig. testing features obtained dictionary atoms learned respectively k-means method last. despite low-dimensionality feature vectors classes separated reasonable accuracy using algorithm whereas features obtained k-means algorithm clearly cannot discriminated ﬁnally illustrate fig. dictionaries learned using k-means last atoms. observed that k-means dictionary consists smoothed images minimize reconstruction error algorithm learns discriminative dictionary whose goal underline diﬀerence images classes. summary supervised learning algorithm speciﬁcally tailored soft-thresholding encoder provides signiﬁcant improvements traditional dictionary learning schemes. classiﬁer reach high accuracy rates even small dictionaries possible learning schemes. test sample previously learned dictionary resulting feature vector. linear trained resulting feature vectors. classiﬁcation architecture denoted sparse coding below similar raina nearest neighbor classiﬁer last comparative scheme nearest neighbor classiﬁer dictionary learned using supervised k-means procedure described test time sample assigned label dictionary atom closest table ﬁrst shows accuracies diﬀerent classiﬁers binary textures classiﬁcation tasks described experiments linear classiﬁer results poor performance close random classiﬁer. suggests considered task nonlinear tackled nonlinear classiﬁer. kernel results signiﬁcant increase classiﬁcation accuracy. similarly sparse coding linear mapping also results much better performance compared linear classiﬁer nearest neighbor approach performs worse sparse coding. note that ﬁxed dictionary size classiﬁer outperforms sparse coding classiﬁers tasks. moreover provides comparable superior performance kernel tasks. turn binary experiment deer horse described previous subsection. show classiﬁcation accuracies diﬀerent classiﬁers table last outperforms sparse coding nearest neighbour classiﬁers tested dictionary sizes. kernel however slightly outperforms last experiment. note however kernel approach much slower test time makes impractical large-scale problems. overall proposed last classiﬁer compares favorably diﬀerent tested classiﬁers. particular last outperforms sparse coding technique ﬁxed dictionary size experiments. result notable sparse coding classiﬁers known provide good classiﬁcation performance vision tasks. note that used another standard learning approach k-means soft-thresholding based classiﬁer outperformed sparse coding shows importance learning scheme success classiﬁer. address multi-class classiﬁcation task using one-vs-all strategy often done classiﬁcation problems. speciﬁcally learn separate dictionary binary linear classiﬁer solving optimization problem one-vs-all problem. classiﬁcation done predicting using binary classiﬁer last one-vs-all task naturally choosing prediction highest score. dictionary roughly images training set. proposed approach used dictionaries size usps mnist latter dataset contains much training samples. compare last baseline classiﬁcation techniques described previous section well sparse coding based methods. addition building dictionary unsupervised consider sparse coding classiﬁers mairal huang aviyente ramirez construct dictionary supervised fashion. classiﬁcation results shown table last largely outperforms linear nearest neighbour classiﬁers. moreover method slightly better accuracy rbf-svm mnist slightly worse usps dataset. approach also outperforms soft-thresholding based classiﬁer optimized stochastic gradient descent tasks highlights beneﬁts optimization technique compared standard algorithm used training neural networks. also report glorot performance three hidden layer rectiﬁed network optimized stochastic gradient decent without unsupervised pre-training. seen last much simpler architecture slightly outperforms deep rectiﬁer network mnist task. furthermore last outperforms unsupervised sparse coding classiﬁer datasets. interestingly proposed scheme also competes with sometimes outperforms discriminative sparse coding techniques dictionary tuned classiﬁcation. providing comparable results last classiﬁer much faster test time sparse coding techniques rbf-svm classiﬁers. noteworthy mention best discriminative dictionary learning results aware datasets achieved mairal error rate mnist usps. note however paper authors explicitly incorporate translation invariance problem augmenting training shifted versions digits. focus goes instead methods augment training distorted transformed samples. similar glorot takes advantage fact dealing images sometimes referred permutation invariant columns data could shuﬄed without aﬀecting result. consider scenario focus comparison performance classiﬁers. relatively high dimensions problem limit classiﬁers feedforward architectures. fact using rbf-svm task would prohibitively slow training testing stage. one-vs-all task dictionary size last methods moreover unlike previous experiment last half entries sign vector half high variability intra-class images relatively small dictionary size number atoms required encode positive class might suﬃcient according distribution images training set. results reported table again experiment conﬁrms superiority learning algorithm linear svm. moreover last signiﬁcantly outperforms generic training algorithm challenging classiﬁcation example. surprising last signiﬁcantly surpasses rectiﬁer neural network hidden layers trained using generic stochastic gradient descent algorithm shows that despite simplicity architecture adequate training classiﬁcation scheme give better performance complicated structures potentially diﬃcult train. ﬁnally report results sparse coding classiﬁer dictionary trained using dictionary atoms error using much larger dictionary atoms error reduces computation test features however computationally expensive case. ﬁrst discuss section aspects related computational complexity last. then analyze sparsity obtained solutions. ﬁnally explain diﬀerences last generic stochastic gradient descent algorithm. compare computational complexity running times last classiﬁer ones diﬀerent classiﬁcation algorithms. table shows computational complexity classifying test sample using various classiﬁers time needed classify mnist test images. recall denote respectively signals dimension number training samples dictionary size. clearly linear classiﬁcation eﬃcient requires computation inner product vectors dimension nonlinear svms however test complexity linear number support vectors scales linearly training size solution therefore practical relatively large training sets like mnist cifar-. feature extraction sparse coding involves solving optimization problem matrix-vector multiplications controls precision typical value complexity becomes orders magnitude larger complexity proposed method. seen clearly computation times approach slightly expensive linear remains much faster methods. note moreover soft-thresholding classiﬁcation scheme simple implement practice test time direct involves linear operations. sparsity highly beneﬁcial property representation learning helps decomposing factors variations data high level features assess sparsity learned representation compute average sparsity representation data points mnist cifar- dataset. obtain average zeros mnist case cifar-. words representations sparse without adding explicit sparsity penalization interestingly reported average sparsity mnist cifar-. one-layer representation therefore exhibits interesting sparsity property providing good predictive performance. table computational complexity classifying test sample time needed predict labels test samples mnist dataset. reference experiments carried intel core machine ram. discussed earlier soft-thresholding classiﬁcation scheme belongs general neural network models. neural networks commonly optimized stochastic gradient descent algorithms opposed method proposed paper. proposed learning algorithm several advantages compared rithms) sensible diﬃcult choice stepsize. choosing large stepsize beneﬁcial helps escaping local minimas also lead oscillatory behaviour prevents convergence. interestingly optimization algorithm involve stepsize selection given convex optimization solver. fact algorithm solves sequence convex problems solved oﬀ-the-shelf convex solver. note even intermediate convex optimization problems solved gradient-descent based technique choice stepsize less challenging better understanding theoretical properties stepsize rules convex optimization problems. previously mentioned unlike algorithm assumes sign vector linear classiﬁer known. simple heuristic choice parameter shown however provide good results experiments compared sgd. course choosing parameter cross-validation might lead better results also implies slower training procedure. proposed supervised learning algorithm tailored soft thresholding based classiﬁer. learning problem jointly estimates discriminative dictionary classiﬁer hyperplane cast problem solved eﬃciently iterative algorithm. proposed algorithm leverages structure signiﬁcantly outperforms stochastic gradient descent experiments. furthermore resulting classiﬁer consistently leads better results unsupervised sparse coding classiﬁer. method moreover compares favorably standard techniques linear kernel nearest neighbour classiﬁers. proposed last classiﬁer also shown compete recent discriminative sparse coding techniques handwritten digits classiﬁcation experiments. mention that sparse coding encoder features form competition diﬀerent atoms dictionary encoder acts diﬀerent atoms independently. despite simple behavior scheme competitive dictionary classiﬁer parameters learned suitable manner. classiﬁcation scheme adopted paper seen hidden layer neural network soft-thresholding activation function. activation function recently gained signiﬁcant attention deep learning community believed make training procedure easier less prone local minima. work reveals interesting structure optimization problem one-hidden layer version network allows reach good minima. interesting question whether possible complexity reported fista algorithm beck teboulle denotes required precision. note another popular method solving sparse coding homotopy method eﬃcient practice however exponential theoretical complexity mairal similar structure networks many hidden layers. would help training deep networks oﬀer insights challenging problem usually tackled using stochastic gradient descent. show soft-thresholding viewed coarse approximation non-negative sparse coding mapping this consider proximal gradient algorithm solve sparse coding problem additional nonnegativity constraints coeﬃcients. speciﬁcally consider following mapping precisely corresponds soft-thresholding map. soft-thresholding corresponds approximation sparse coding iteration proximal gradient algorithm performed.", "year": 2014}