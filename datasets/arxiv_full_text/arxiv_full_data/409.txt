{"title": "Invariant Representations for Noisy Speech Recognition", "tag": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "stat.ML"], "abstract": "Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. We use ideas from recent research on image generation using Generative Adversarial Networks and domain adaptation ideas extending adversarial gradient-based training. A recent work from Ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. Our work focuses on investigating neural architectures which produce representations invariant to noise conditions for ASR. We evaluate the proposed architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.", "text": "modern automatic speech recognition systems need robust acoustic variability arising environmental speaker channel recording conditions. ensuring robustness variability challenge modern neural network-based systems especially types variability seen training. attempt address problem encouraging neural network acoustic model learn invariant feature representations. ideas recent research image generation using generative adversarial networks domain adaptation ideas extending adversarial gradient-based training. recent work ganin proposes adversarial training image domain adaptation using intermediate representation main target classiﬁcation network deteriorate domain classiﬁer performance separate neural network. work focuses investigating neural architectures produce representations invariant noise conditions asr. evaluate proposed architecture aurora- task popular benchmark noise robust asr. show method generalizes better standard multicondition training especially noise categories seen training. challenging aspects automatic speech recognition mismatch training testing acoustic conditions. testing system encounter recording conditions microphone types speakers accents types background noises. furthermore even test scenarios seen training signiﬁcant variability statistics. thus important develop systems invariant unseen acoustic conditions. several model feature based adaptation methods maximum likelihood linear regression feature-based mllr ivectors proposed handle speaker variability; noise adaptive training vector taylor series handle environment variability. increasing success deep neural network acoustic models end-to-end systems proposed modeling acoustic conditions within single network. allows take advantage network’s ability learn highly non-linear feature transformations greater ﬂexibility constructing training objective functions promote learning noise invariant representations. main idea work force acoustic model learn representation invariant noise conditions instead explicitly using noise robust acoustic features type noise-invariant training requires noise-condition labels training only. related idea generative adversarial networks gradient reverse method proposed goodfellow ganin lempitsky respectively present results aurora- speech recognition task section summarize ﬁndings section generative adversarial networks consist networks generator discriminator. generator network input randomly-generated feature vectors asked produce sample e.g. image similar images training set. discriminator network either receive generated image generator image training set. task distinguish fake generated image real image taken dataset. thus discriminator classiﬁer network sigmoid output layer trained gradient backpropagation. gradient propagated generator network. networks setup competing other generator trying deceive discriminator network discriminator tries best recognize deception similar adversarial game-theoretic settings. formally objective function training maximization discriminator forms usual cross-entropy objective gradients computed respect parameters parameters minimized using gradients propagated second term. minimization makes produce examples classiﬁers training ones. several practical guidelines proposed optimizing gans radford explored salimans prior work ganin lempitsky proposed method training network adapted domains. training data consists images labeled classes interest separate domain labels. network -like structure image ﬁrst network produces hidden representation representation input separate networks domain classiﬁer network target classiﬁer network goal training learn hidden representation invariant domain labels performs well target classiﬁcation task domain information doesn’t interfere target classiﬁer test time. similar objective forces generation distribution close data distribution gradient reverse method makes domain distributions similar other. network trained three goals hidden representation helpful target classiﬁer harmful domain classiﬁer domain classiﬁer good classiﬁcation accuracy. formally authors deﬁne loss function ground truth class domain label corresponding variables network predictions subsets parameters encoder recognizer domain classiﬁer networks respectively. hyper-parameters denote relative inﬂuence loss functions terms. inﬂuence representations produced neural network internal noise reduction discussed work sets baseline experiments aurora- dataset. recently shunohara multilayer sigmoidal network trained adversarial fashion in-house transcription task corrupted noise. model consists three neural networks. encoder produces intermediate representation used recognizer domain discriminator hidden representation trained improve recognition minimize domain discriminator accuracy. domain discriminator classiﬁer trained maximize accuracy noise type classiﬁcation task. average performance baseline multi-condition invariance model varying number noise conditions used training. bottom average performance seen versus unseen noise conditions. testing performed conditions systems dnn-hmm hybrid systems. context dependent states class labels interest. recording conditions speaker identity gender represent domains gans. task make hidden layer representations state classiﬁer network invariant respect domains. hypothesize adversarial method training helps state classiﬁer generalize better unseen domain conditions requires small additional amount supervision i.e. domain labels. figure depicts model model gradient reverse method. feed-forward neural network trained predict state branch predicts domain branch discarded testing phase. experiments used noise condition domain label merging noise types label clean label. training loss function stability training. term maximizes probability incorrect domain classiﬁcation contrast gradient reverse correct classiﬁcation minimized. terms regular cross-entropies minimized corresponding parameters simplicity single hyper-parameter weight third term. experimentally evaluate approach well-benchmarked aurora- noisy speech recognition task. aurora- based wall street journal corpus contains noises categories added clean data. every clean noisy utterance ﬁltered simulate frequency characteristics. training data contains clean utterances utterances noise condition i.e. total noisy utterances. test consists clean data data corrupted noise types data recorded different microphone clean noisy cases. table average word error rate aurora- dataset test conditions including seen unseen noise unseen microphone. first column number noise conditions used training. last preliminary experiment layer-wise pre-training close state-of-the-art model corresponding invariance training starting pretrained model. clean noisy data extract -dimensional mel-ﬁlterbank features deltas delta-deltas spliced frames resulting input features subsequently mean variance normalized. baseline acoustic model -layer rectiﬁed linear units every layer. trained using momentum-accelerated stochastic gradient descent epochs new-bob annealing order evaluate impact method generalization unseen noises performed experiments different seen noises. networks trained clean data noise condition added one-by-one following order airport babble restaurant street train. last training group includes noises therefore matches standard multi-condition training setup. every training group trained baseline invariance model branch layer binary classiﬁer predicting clean versus noisy data. imbalance amounts clean noisy utterances oversample noisy frames ensure every mini-batch contained equal number clean noisy speech frames. table summarizes results. figure visualizes word error rate baseline multicondition training invariance training number seen noise types varies. conclude best performance gain achieved small number noise types available training. seen invariance training able generalize better unseen noise types compared multi-condition training. note experiments layer-wise pre-training commonly used small datasets. baseline wers reported close state-of-the-art. preliminary experiments pre-trained network using noise types training show trend non-pretrained networks. paper presents application generative adversarial networks invariance training noise robust speech recognition. show invariance training helps system generalize better unseen noise conditions improves word error rate small number noise types seen training. experiments show contrast image recognition task speech recognition domain adaptation network suffers underﬁtting. therefore gradient term unreliable noisy. future research includes enhancements domain adaptation network exploring alternative network architectures invariance-promoting loss functions. would like thank yaroslav ganin david warde-farley insightful discussions developers theano theano development team blocks fuel merriënboer great toolkits. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. pages ghahramani welling cortes lawrence weinberger advances neural information processing systems curran associates inc. hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine kalinli ozlem seltzer michael droppo jasha acero alex. noise adaptive training robust automatic speech recognition. ieee transactions audio speech language processing miao yajie gowayyed mohammad metze florian. eesen end-to-end speech recognition using deep models wfst-based decoding. pages ieee workshop automatic speech recognition understanding ieee. sainath tara kingsbury brian ramabhadran bhuvana fousek petr novak petr mohamed abdel-rahman. making deep belief networks effective large vocabulary continuous speech recognition. pages automatic speech recognition understanding ieee workshop ieee. merriënboer bart bahdanau dzmitry dumoulin vincent serdyuk dmitriy warde-farley david chorowski bengio yoshua. blocks fuel frameworks deep learning. corr abs/..", "year": 2016}