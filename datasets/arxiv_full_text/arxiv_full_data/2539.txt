{"title": "Greedy Deep Dictionary Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.", "text": "abstract—in work propose deep learning tool deep dictionary learning. multi-level dictionaries learnt greedy fashion layer time. requires solving simple dictionary learning problem; solution well known. apply proposed technique benchmark deep learning datasets. compare results deep learning tools like stacked autoencoder deep belief network; state-of-the-art supervised dictionary learning tools like discriminative k-svd label consistent k-svd. method yields better results all. recent years interest dictionary learning. however concept dictionary learning around much longer. application vision information retrieval dates back late days term ‘dictionary learning’ coined; researchers using term ‘matrix factorization’. goal learn empirical basis data. basically required decomposing data matrix basis dictionary matrix feature matrix hence name ‘matrix factorization’. current popularity dictionary learning owes k-svd k-svd algorithm decompose matrix dense basis sparse coefficients. however concept dense-sparse decomposition predates k-svd since advent k-svd plethora work topic. dictionary learning used unsupervised problems well problems arising supervised feature extraction. dictionary learning used virtually inverse problems arising image processing starting simple image video denoising image inpainting complex problems like color image restoration inverse half toning even medical image reconstruction solving inverse problems goal work; interested dictionary learning perspective machine learning. briefly discussed sake completeness. transforms sparsifying step followed statistical feature extraction methods like feeding features classifier. dictionary learning replacing fixed transforms signal processing problems also replacing feature extraction scenarios. dictionary learning gives researchers opportunity design dictionaries yield sparse representation also discriminative information. initial techniques proposed naïve approaches learnt specific dictionaries class later approaches incorporated discriminative penalties dictionary learning framework. technique include softmax discriminative cost function discriminative penalties include fisher discrimination criterion linear predictive classification error hinge loss function discrimination introduced forcing learned features corresponding class labels. prior studies dictionary learning ‘shallow’ learning models like restricted boltzman machine autoencoder fall broader topic representation learning. cost function euclidean distance data representation given learned basis; boltzman energy; cost euclidean reconstruction error data decoded representation features. almost time dictionary learning started gaining popularity researchers machine learning observed better representation achieved going deeper. deep belief network formed stacking similarly stacked autoencoder created inside problems sparse representation objective learn basis represent samples sparse fashion i.e. needs sparse. ksvd well known technique solving problem. fundamentally solves problem form ksvd proceeds stages. first stage learns dictionary next stage uses learned dictionary sparsely represent data. solving l-norm minimization problem hard ksvd employs greedy orthogonal matching pursuit solve l-norm minimization problem approximately. dictionary learning stage ksvd proposes efficient technique estimate atoms time using rank update. major disadvantage ksvd relatively slow technique owing requirement computing every iteration. efficient optimization based approaches dictionary learning learn full dictionary instead updating atoms separately. dictionary learning formulation unsupervised. mentioned large volume work supervised dictionary learning problems. briefly discuss major ones here. first work sparse representation based classification much dictionary learning technique simple dictionary design problem training samples concatenated large dictionary. assumption training samples basis test sample belonging correct class. proposed model assumed since correct class represents vector going sparse. based assumption solved using sparse recovery technique. obtained problem classify achieved computing error test image representation class obtained xcac. denotes class. test sample simply assigned class lowest error. several improvements basic formulation proposed proposed since known class structure improve upon basic sparse classification approach incorporating group-sparsity. non-linear extension proposed. later works handled non-linear extension smarter fashion using kernel trick paradigm. however proposed simple extension instead using training samples basis learnt separate basis class used dictionaries classification. approach naïve; guarantee dictionaries different classes would similar. issue corrected. additional incoherency penalty dictionaries. penalty assures dictionaries different classes look different other. formulation given unfortunately formulation improve overall results much. learns dictionaries look different produce features distinctive; i.e. feature generated test sample dictionaries classes looked less same. aforesaid issue rectified combined concepts. first discrimination learned features second discrimination class specific dictionaries. second criteria demands features particular class reconstruct samples class accurately; however represent samples classes. idea formulated follows class specific dictionaries training samples class representaion dictionaries. according assumption portion pertaining correct class represent data well leads second term expression; dictionaries represent data well hence third term. discriminative dictionaries. mentioned before second term discriminates among learned features. term arises fisher discriminant analysis tries increase covariance classes decrease covariance within class. represented label consistent ksvd recent techniques learning discriminative sparse representation. simple understand implement; showed good results face recognition first technique called discriminative k-svd lc-ksvd proposes optimization problem following form computing exact gradient loss function almost intractable. however stochastic approximation approximate gradient termed contrastive divergence gradient. sequence gibbs sampling based reconstruction produces approximation expectation joint energy distribution using gradient computed. usually unsupervised studies trained discriminative rbms utilizing class labels also rbms sparse sparsity controlled firing hidden units threshold. supervision also achieved using sparse rbms extending similar sparsity structure within group class deep boltzmann machines extension stacking multiple hidden layers undirected learning model thus different stacked network architectures layer receives feedback top-down bottom-up layer signals. feedback mechanism helps managing uncertainty learning models. traditional model logistic units gaussian-bernoulli used well real valued visible units. ‘discriminative’ sparse code corresponding input signal sample nonzero values occur indices training sample dictionary item share label. basically formulation imposes labels sparse coefficient vectors zi’s also dictionary atoms. training lc-ksvd learns discriminative dictionary dictionary classification weights need normalized. test sample sparse coefficients learnt using normalized dictionary using l-minimization restricted boltzmann machines undirected models uses stochastic hidden units model distribution stochastic visible units. hidden layer symmetrically connected visible unit architecture restricted connections units layer. traditionally rbms used model distribution input data sigmoid function popular; non-linear activation functions used well. rectifier units large neural networks employ linear activation functions considerably speeds training. regularization simple tikhonov regularization however used practice. sparsity promoting term weight decay term used contractive autoencoder regularization term usually chosen differentiable hence minimizable using gradient descent techniques. section describe main contribution work. single shallow level dictionary learning yields latent representation data dictionary atoms. propose learn deeper latent representation data learning multi-level dictionaries. idea learning deeper levels dictionaries stems recent success deep learning various areas machine learning. schematic diagram dictionary learning shown fig. data dictionary feature representation dictionary learning follows synthesis framework i.e. dictionary learnt features synthesize data along dictionary. columns. problem clearly non-convex smooth hence solved gradient descent techniques; activation function needs smooth continuously differentiable. several extensions basic autoencoder architecture. stacked autoencoders multiple hidden layers inside corresponding cost function expressed follows solving complete problem computationally challenging. also learning many parameters lead over-fitting. address issues weights usually learned greedy fashion layer layer before solving simple. least square problem closed form solution. solution although analytic well known signal processing machine learning literature. solved using iterative soft thresholding algorithm every iteration steps ista work used dense dictionary learning layers till penultimate layer sparse dictionary learning final layer i.e. layer problem first layer would dense second layer would sparse. must noted dictionaries cannot collapsed single one. learning process nonlinear. example dimensionality sample first dictionary size second possible learn single dictionary size expect results two-stage dictionary. undirectional graph whereas dictionary learning unidirectional. evident figures cases task learn network weights atoms representation given data. differ cost functions used. boltzmann function. tries learn network weight output features similarity projected data features maximized. dictionary learning cost function different instead maximizing similarity minimize euclidean distance data synthesis stochastic formulation; dictionary learning deterministic. rbms formulated features values values outside range need normalized. many cases normalization affect performance scenarios suppresses important information. dictionary learning work real complex inputs. predominantly modeled synthesis problem i.e. dictionary features learnt synthesize data. expressed x=dsz data learnt synthesis dictionary sparse coefficients. learning dictionaries along deepest level features hard problem reasons dictionary learning bi-linear problem. learning multiple layers dictionaries along features makes problem even difficult solve. recently studies proven convergence guarantees single level dictionary learning proofs would hard replicate multiple layers. moreover number parameters required solved increases multiple layers dictionaries learnt simultaneously. limited training data could lead over-fitting. propose learn dictionaries greedy fashion. sync deep learning techniques moreover layer-wise learning guarantee convergence layer. diagram illustrating layer-wise learning shown fig. task find dictionary synthesize generate signals sparse features. alternate co-sparse analysis prior dictionary learning paradigm goal learn dictionary applied data first results show multi-level dictionaries cannot collapsed single expected perform same. carried experiments mnist variations. first case number basis multi-level dictionaries second case learn shallow dictionary atoms. results would same multi-level dictionaries would collapsible. want show representation learnt single level dictionary multi-level dictionary different. showcase this show classification results simple nearest neighbour classification accuracies shown table deterministic initialization dictionary learning. usually dictionary atoms initialized randomly choosing samples training leads variability results. work propose deterministic initialization based decomposition. orthogonal vectors used initialize dictionary. express autoencoder lingo dictionary learning autoencoder model learnt analysis synthesis dictionaries. best knowledge first work shows architectural similarity autoencoders dictionary learning. carried experiments several benchmarks datasets. first mnist dataset consists images handwritten digits ranging dataset images training images testing. compared technique state-of-the-art dictionary learning techniques like d-ksvd lcksvd tuned yield best possible results. comparison also done stacked denoising autoencoder deep belief network fine tuned softmax classifier. experiments; results copied discrepancy multi-level dictionary learning single level dictionary learning evident table learning linear would possible collapse multiple dictionaries one; dictionary learning inherently nonlinear. hence possible learn single layer dictionary place multiple levels expect output. learning techniques always yields better results shallow dictionary learning cases even achieve better accuracy highly tuned models like sdae. compare technique deep learning approaches terms speed algorithms convergence. convergence. machine used intel core running ghz; windows running matlab times smaller mnist variations approximately same. report results larger mnist dataset basic dataset. proposed deep dictionary learning algorithm orders magnitude faster deep belief network orders magnitude faster stacked autoencoder. huge saving training time. work propose idea deep dictionary learning instead learning shallow dictionary done learn multiple levels dictionaries. learning dictionaries makes problem highly non-convex. also learning many parameters always fraught problem over-fitting. account issues learn dictionaries greedy fashion compared results stacked autoencoder deep belief network implementation obtained respectively. three layer architecture. number nodes halved every subsequent layer. standard approach; tried configurations could improve upon this. want compare representation capability proposed technique vis-à-vis deep learning methods. results nearest neighbour support vector machine shown tables layer time. representation feature level used input learn following level. thus basic unit deep dictionary learning simple shallow dictionary learning algorithm; well known solved problem. compare deep learning tool existing ones like stacked autoencoder deep belief network. find method yields better results benchmark deeplearning datasets. main advantage method orders magnitude faster existing deep learning tools like stacked autoencoder deep belief network. preliminary work carry extensive experimentation future. plan test robustness dictionary learning presence missing data noise limited number training sample. future would also like apply technique practical problems arising biometrics vision speech processing etc. also work supervised dictionary unsupervised. future expect improve results even incorporating techniques supervised learning. aharon elad bruckstein \"k-svd algorithm designing overcomplete dictionaries sparse representation\" ieee transactions signal processing vol. eggert körner \"sparse coding nmf\" ieee international joint conference neural networks caballero price rueckert hajnal \"dictionary learning time sparsity dynamic data reconstruction\" ieee transactions medical imaging vol. majumdar ward learning space-time dictionaries blind compressed sensing dynamic reconstruction ieee international conference image processing dabbaghchian ghaemmaghami aghagolzadeh feature extraction using discrete cosine transform discrimination power analysis face recognition technology pattern recognition vol. mairal bach ponce sapiro zisserman. \"discriminative learned dictionaries local image analysis\". ieee conference computer vision pattern recognition yang sukthankar jurie. \"unifying discriminative visual codebook genearation classifier training object category recognition\". ieee conference computer vision pattern recognition mairal leordeanu bach hebert ponce. \"discriminative sparse image models class-specific edage detection image interpretation\". european conference computer vision mairal leordeanu bach hebert ponce. \"discriminative sparse image models class-specific edage detection image interpretation\". european conference computer vision jiang davis \"learning discriminative dictionary sparse coding label consistent k-svd\" ieee transactions pattern analysis machine intelligence vol. hinton salakhutdinov \"reducing dimensionality data neural networks\" science vol. vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion journal machine learning research vol. ramirez sprechmann sapiro. classification clustering dictionary learning structured incoherence shared features. ieee conference computer vision pattern recognition agarwal anandkumar jain netrapalli learning sparsely used overcomplete dictionaries alternating minimization international conference learning theory spielman wang wright exact recovery sparselyused dictionaries international conference learning theory arora bhaskara more algorithms provable daubechies defrise iterative thresholding algorithm linear inverse problems sparsity constraint\" communications pure applied mathematics vol.", "year": 2016}