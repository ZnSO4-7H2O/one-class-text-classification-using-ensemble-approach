{"title": "Concept Formation and Dynamics of Repeated Inference in Deep Generative  Models", "tag": ["stat.ML", "cs.LG", "cs.NE", "q-bio.NC"], "abstract": "Deep generative models are reported to be useful in broad applications including image generation. Repeated inference between data space and latent space in these models can denoise cluttered images and improve the quality of inferred results. However, previous studies only qualitatively evaluated image outputs in data space, and the mechanism behind the inference has not been investigated. The purpose of the current study is to numerically analyze changes in activity patterns of neurons in the latent space of a deep generative model called a \"variational auto-encoder\" (VAE). What kinds of inference dynamics the VAE demonstrates when noise is added to the input data are identified. The VAE embeds a dataset with clear cluster structures in the latent space and the center of each cluster of multiple correlated data points (memories) is referred as the concept. Our study demonstrated that transient dynamics of inference first approaches a concept, and then moves close to a memory. Moreover, the VAE revealed that the inference dynamics approaches a more abstract concept to the extent that the uncertainty of input data increases due to noise. It was demonstrated that by increasing the number of the latent variables, the trend of the inference dynamics to approach a concept can be enhanced, and the generalization ability of the VAE can be improved.", "text": "abstract deep generative models reported useful broad applications including image generation. repeated inference data space latent space models denoise cluttered images improve quality inferred results. however previous studies qualitatively evaluated image outputs data space mechanism behind inference investigated. purpose current study numerically analyze changes activity patterns neurons latent space deep generative model called variational auto-encoder kinds inference dynamics demonstrates noise added input data identiﬁed. embeds dataset clear cluster structures latent space center cluster multiple correlated data points referred concept. study demonstrated transient dynamics inference ﬁrst approaches concept moves close memory. moreover revealed inference dynamics approaches abstract concept extent uncertainty input data increases noise. demonstrated increasing number latent variables trend inference dynamics approach concept enhanced generalization ability improved. research deep generative models extract essential features unlabeled dataset currently active area. deep generative models reported useful broad range applications generating images movies text particular conventional bidirectional network structure recognition generation images made possible eliminate noise cluttered images smoothly interpolate diﬀerent images. detail recognition process mapping data point latent variable generation inverse process. several studies qualitatively highlighted importance repeated inferences data space latent space. present study repeated inferences deﬁned process deep generative model repeats recognition generation images. shown using noise-containing images initial values deep generative models eliminate noise repeating recognition generation several times moreover compared generating output image latent space smoothly morph image another repeating inferences several times improves quality output image however studies qualitatively evaluate output data. literature quantiﬁed dynamics repeated inferences latent space investigate repeating inferences eﬀective wide range applications. study focused dynamics repeated inferences variational auto-encoder typical type deep generative model. first noise-containing images presented initial inputs denoises images generates clean outputs using repeated inferences. various factors noise real environments cause data deviate original distribution. according manifold hypothesis data widely used typical applications likely concentrate vicinity non-linear sub-manifolds much lower dimensionality rather high dimensional space data actually presented therefore suggested inference begins outside sub-manifolds training data noise added initial inputs. however little works paid attention activity patterns neurons drawn original sub-manifolds initial point latent space. understand patterns examine expresses cluster structures latent space utilized mathematical notion known concept. first introduced study associative memory models concept referred centroid multiple correlated data points analytically demonstrated spontaneously evolve equilibrium state phenomenon called concept formation. furthermore dynamics neural activity patterns also studied terms associative memory model multiple correlated memories revealed dynamics neural activity patterns ﬁrst approach concept move toward memory pattern summary four major ﬁndings study. first consistent reports demonstrated dynamics repeated inferences drawn unique memory corresponding concept center cluster latent space. second averaging clusters latent space deﬁned abstract concept; deﬁnition memories concepts abstract concept hierarchically related ascending order. found inference dynamics approaches abstract concept extent uncertainty input data increases noise. result suggested model selects appropriate inference strategies accordance fraction noise added input data. third identiﬁed approximate necessary number latent variables memories latent space. number latent variable increases internal representations clusters tend become orthogonal makes dynamics repeated inferences approaches corresponding concept. finally checked generalization error vae. result demonstrated generalization performance model improved extent concept observed attract dynamics repeated inferences. variational auto-encoder generative model consisting neural networks namely encoder decoder encoder sends mapping data natural images latent variable space decoder gives inverse mapping. objective function obtained training following parameter maximizes log-likelihood data point considered. using latent variable conditional probability distribution taking variational lower bound log-likelihood gives following objective function equation prior distribution latent variable dklp) kullback-leibler divergence probability distributions ﬁrst term objective function corresponds regularization second term corresponds reconstruction error. models conditional distributions using respective neural networks. optimize parameters backpropagation samples generated method called reparameterization trick encoder qφ). latent variable modeled decompose random variable deterministic variables giving sample standard gaussian distribution eliminates need complicated integral training. conditions assumed expected reconstruction error approximated sample average rewritten outputs encoder parameter determining parameter decoder trained gradient ascent method maximize output decoder probability bernoulli distribution expectation conditional probability namely second term objective function approximated average samples. separate three-layer fully-connected neural network used encoder decoder mentioned above. number units middle layer number samples calculating reconstruction error activation function tanh. adam used parameter optimization algorithm learning rate reduced descending order number units latent variable unless otherwise noted. modiﬁed national institute standards technology database used dataset training. data considered cluster structures consisting types labels namely ‘’-‘’. study noisy mnist data inferred using trained network according following procedure time evolution latent variable obtained. first noise added image training dataset. pixels probability selected pixels image intensities selected pixels swapped image next data variable step taken finally generation recognition repeated times according following equations ﬁrst analysis demonstrate transient dynamics inference latent space ﬁrst attracted concept center memorized patterns moves memory. representation latent space captures cluster structures hidden behind minist data displays corresponding clusters latent space details expression latent space given appendix since cluster structure exists data latent variable deﬁnition concept based studies associative memory model relationship time development inference concept label represented mnist data numerically analyzed following sections. here numerically demonstrated transient dynamics activity pattern ﬁrst attracted concept center memorized patterns moves memory. figure expresses consecutive samples data space. time development activity pattern data space aligned left right other. upper left image corresponds initial value image noise applied used initial value. image generated concept shown right. ﬁgure removes noise contained image ﬁrst steps gradually shifts speciﬁc image qualitatively suggested result inference approaches concept once. gradual changes output images data space quantitatively evaluated latent space. time evolution euclidean distance namely neural activity patterns concepts every label mnist data latent space evaluated. distance concept diﬀerent initial images calculated. ﬁgure corresponds label used initial input vae. x-axis expresses time step repeated inference y-axis expresses euclidean distance clariﬁed trajectory vae’s inference ﬁrst rapidly approaches concept moves away result qualitatively consistent labels minist data. makes latent space activity patterns orthogonal relation another cluster correlated cluster lowdimensional space. relationships concepts label shown activity patterns latent variable space numerical concept shown heat expresses activity pattern neuron corresponds latent variable x-axis represents neuron’s index y-axis represents label. neurons contribute information representation many neurons pruned active. according observation neurons active. dimensions latent space examined using cumulative contribution ratio determined principal component analysis. cumulative contribution ratio principal component training images given shown variance latent space explained entirely figure activity pattern latent variable space concept. x-axis represents neuron index latent variable y-axis represents label heat shows activity pattern neuron. cumulative contribution ratio principal components. cosine similarity activity patterns label. deﬁnition cosine similarity concepts label hand cosine similarity concepts diﬀerent labels non-diagonal terms minimal namely near zero. ﬁgure shows concepts label orthogonal latent space. suggested activity patterns corresponding diﬀerent training data label correlated diﬀerent label orthogonal latent space vae. previously arbitrarily determined amount noise added initial input images. second analysis examine eﬀect noise dynamics repeated interferences modulated amount noise. since noise input images causes data deviate original distribution created another class abstract concept averaged labels’ concepts addition concept memory. respectively measuring distance trajectory neural activity pattern corresponding classes three classes hierarnum ¯ξnum chical relationship order ¯ξall. calculated minimum distances respective neural activity patterns corresponding classes figure shows minimum distances according noise fraction. ﬁgure x-axis represents noise fraction probability image intensities pixels swapped. every noise fraction minimum distances ﬁring pattern hierarchical concepts calculated changing initial image times. dots ﬁgure express mean minimum distance bars standard error mean parameter regions divided three stages correspond minimum distance ﬁring pattern hierarchical concepts ¯ξall respectively. stage ﬁring activity closest original pattern small amount noise. interestingly closest concept moderate noise stage activity came close concept ¯ξall stage iii. stages memory successfully retrieved inference path close cluster input data belongs; however stage model could determine original cluster recall failed. discovered noisy environment makes recognizing objects diﬃcult neural activity pattern wanders around center memories. accordingly model achieves inference dynamics depending input uncertainty. shown section above extracts cluster structures inherent mnist data infers images center cluster. dynamics inference shown schematic diagram considered adding noise image corresponds moving initial value direction orthogonal original data space. results ﬁrst analysis suggest inference starts position orthogonal space expressing mnist data activity patterns ﬁrst approach corresponding concepts high speed transitions corresponding memories. approaching concept repeated inferences indicates concept formation occurs latent space vae. neuroscience activities visual cortex macaque monkey human brain measured reported process global information detailed information. previous studies associative memory models explain behaviors spontaneous stabilization concept eﬀect memory retrieval also recalled concept memory pattern inference phase. results consistent ﬁndings studies associative memory models cerebral cortex. third analysis mechanism trajectory vae’s inference approaches concept clariﬁed follows. first following question must answered tree-like memory structure mentioned affect time evolution inference? words memory structure changed controlling hyperparameter veriﬁed trajectory inference circumstances examined respectively time evolution distance concept diﬀerent model hyperparameters compared ﬁgs. number neurons latent variable controlled order condition figure time development distance concept number elements latent variable written cosine similarity memory patterns concept corresponding model trajectory approached concept once whereas under condition approach concept. also trajectory gradually approached corresponding concept cosine similarity concepts label parameters shown ﬁgs. similarity oﬀ-diagonal term approximately zero. hand number latent variables decreases orthogonality concept decays. results third analysis suggest orthogonality concepts necessary trajectory inference drawn concept. since number latent variables decreases necessary express data fewer dimensions orthogonality concepts lost. reduction number latent variables considered cause instability memory patterns corresponding training data stabilize concepts. result trajectory inference goes straight stable point. also numerically assessed whether labels confused repeated inferences result assessment shown appendix fourth analysis engineering signiﬁcance attraction concept explained follows. figure generalization performance model according performance model evaluated using variational lower bound log-likelihood test mnist data. parameters minimize generalization error epoch total nine conditions selected learning rates mini-batch size generalization error minimum value vicinity change signiﬁcantly that. fourteen latent variable neurons express training data condition number neurons minimize generalization error consistent result. results suggest latent neurons required express mnist data network structure used study. moreover vicinity cluster structure appears representation latent variable space trajectory inference drawn concept. results suggest possible judge generalization performance model without computing generalization error found extracts cluster structures inherent mnist infers images center cluster. results ﬁrst analysis suggest inference starts point away original data distribution repeated inferences ﬁrst approach concept high speed slowly move toward memory pattern. learning inference multiple memory patterns widely studied using associative memory models associative memory model embedded multiple correlated patterns centroid correlated patterns spontaneously evolves ﬁxed point time evolution activity patterns approaches concept results ﬁrst second analyses qualitatively consistent ﬁndings suggesting mechanism underlying dynamics repeated inferences related traditional associative memory model. originally matsumoto proposed model explain dynamics neural activities macaque monkey’s visual cortex studied sugase although investigated study model proposed matsumoto experiment conducted sugase diﬀerent architectures nature results infer share universal working principle abstract level. behavior repeated inferences qualitatively consistent time evolution ﬁring patterns observed neuroscience literature current study used study simple hierarchical deep generative model. taking account exhibits patterns similar biological activities future work examine dynamics repeated inferences introducing biologically plausible mechanisms short-term plasticity common noise spontaneous ﬁring based log-normal weight future investigation useful neuroscience engineering applications. previously several studies demonstrated repeated inferences successfully denoise improve quality inferred images study suggests dynamics repeated inferences approaching center cluster inherent data leads denoising improving quality output images quantitatively observed data space. critical take suﬃcient number latent variables precisely represent concept inherent data; number latent variables suﬃcient cluster structures realized latent space concept hardly identiﬁed. results suggest stage appears number latent variables suﬃciently large number latent variables qualitatively changes dynamics repeated inferences. ¯ξnum ¯ξall reﬂect hierarchical structure mnist dataset. previous works discussed relationship hierarchy data deep neural networks. example deep neural networks claimed express abstract information deeper layers particular bengio stated deep layers speed mixing markov chains using ability manifest abstract information. hand saxe analytically showed deep neural networks learn data order large small modes internal representations branch accordingly hypotheses previous studies pointed representations learning dynamics deep neural networks reﬂect data’s hierarchy. study suggests inference process deep generative model also related hierarchy data. recently researchers actively working models capture features inherent data forms internal representations used study embeds data points simple isomorphic gaussian distribution. next step expand works using deep generative models investigate factors inﬂuence behavior repeated inferences approaching concept. likewise analyze dynamics repeated here show extract cluster structure hidden behind data. activity patterns latent variable using principal component analysis shown results principal component analysis using data shown result using three labels shown a.b. color point represents label data. mnist considered cluster structure consisting types labels i.e. ‘’-‘’. ﬁgures show latent variables extract cluster structure. figure two-dimensional embedding representations latent variable space. color point represents label data. embedding data corresponding three labels. possibility labels confused repeated inferences numerically tested. showed trajectory inference approaches concept orthogonality representation latent space. hand also conceivable escape concept caused attraction another cluster. eliminate possibility discriminative neural network constructed separately ﬁnal state inference classiﬁed. convolution-pooling-dropout-fullyconnected-dropout-softmax constructed discriminative neural network. kernel size convolution three size pooling probability dropout order input side. relu used activation function. model recorded discrimination ability test data included mnist dataset. result classifying ﬁnal state inference using above-mentioned discriminative neural network shown b.a. x-axis represents trial inference various initial images y-axis represents number label. heat indicates classiﬁcation probability number label. figure result classiﬁcation ﬁnal state inference image time evolution distance concept condition excluding trials activity pattern switched diﬀerent numbers expressed condition containing trials expressed gray. consider eﬀect labels cause neural activity patterns approaching mismatched concepts. taking example label ﬁrst measured distances neural activity pattern concept conditions. included neural activity patterns reminded inside cluster condition neural activity patterns latent space other. then averaged distances condition compared means. average trajectories compared b.b. shows average trial ﬁnal state identiﬁed gray shows average trials. shown ﬁgure above neural activity patterns conditions approached concept moving corresponding memories. result suggests presence labels doesn’t cause neural activity patterns move away concept.", "year": 2017}