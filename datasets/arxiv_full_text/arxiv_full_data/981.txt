{"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural  Networks using Tensor Methods", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Training neural networks is a challenging non-convex optimization problem, and backpropagation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for guaranteed training of two-layer neural networks. We provide risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for learnability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we propose a computationally efficient method with guaranteed risk bounds for training neural networks with one hidden layer.", "text": "training neural networks challenging non-convex optimization problem backpropagation gradient descent stuck spurious local optima. propose novel algorithm based tensor decomposition guaranteed training two-layer neural networks. provide risk bounds proposed method polynomial sample complexity relevant parameters input dimension number neurons. learning arbitrary target functions np-hard provide transparent conditions function input learnability. training method based tensor decomposition provably converges global optimum mild non-degeneracy conditions. consists simple embarrassingly parallel linear multi-linear operations competitive standard stochastic gradient descent terms computational complexity. thus propose computationally eﬃcient method guaranteed risk bounds training neural networks hidden layer. neural networks revolutionized performance across multiple domains computer vision speech recognition. ﬂexible models trained approximate arbitrary target function e.g. label function classiﬁcation tasks. composed multiple layers neurons activating functions applied recursively input data order predict output. neural networks extensively employed practice complete theoretical understanding currently lacking. training neural network framed optimization problem network parameters chosen minimize given loss function e.g. quadratic loss function error predicting output. performance training algorithms typically measured notion risk expected loss function unseen test data. natural question hardness training neural network bounded risk. ﬁndings mostly negative training even simple network np-hard e.g. network single neuron ∗university california irvine. email mjanzamiuci.edu †university california irvine. email sedghihuci.edu ‡university california irvine. email a.anandkumaruci.edu computational hardness training non-convexity loss function. general loss function many critical points include spurious local optima saddle points. addition face curse dimensionality number critical points grows exponentially input dimension general non-convex problems popular local search methods gradient descent backpropagation stuck local optima experience arbitrarily slow convergence. explicit examples failure presence local optima even simple separable settings documented section discussion. alternative methods training neural networks mostly limited speciﬁc activation functions speciﬁc target functions assume strong assumptions input related work details. thus uniﬁed framework training networks general input output activation functions provide guaranteed risk bound. paper ﬁrst time present guaranteed framework learning general target functions using neural networks simultaneously overcome computational statistical approximation challenges. words method computational sample complexity even dimension optimization grows addition also handle approximation errors target function generated given neural network. prove guaranteed risk bound proposed method. np-hardness refers computational complexity training worst-case instances. instead provide transparent conditions target functions inputs tractable learning. training method based method moments involves decomposing empirical cross moment output function input. pairwise moments represented using matrix higher order moments require tensors learning problem formulated tensor decomposition. decomposition tensor involves ﬁnding succinct rank-one components best input tensor. even though non-convex problem global optimum tensor decomposition achieved using computationally eﬃcient techniques mild non-degeneracy conditions methods recently employed learning wide range latent variable models incorporating tensor methods training neural networks requires addressing number nontrivial questions form moments informative network parameters? earlier works using tensor methods learning assume linear relationship hidden observed variables. however neural networks possess non-linear activation functions. adapt tensor methods setting? methods behave presence approximation sample perturbations? establish risk bounds? address questions shortly. main contributions propose eﬃcient algorithm training neural networks termed neural network-learning using feature tensors demonstrate method embarrassingly parallel competitive standard terms computational complexity main result establish bounded risk number training samples scales polynomially relevant parameters input dimension number neurons. analyze training two-layer feedforward neural network second layer linear activation function. classical neural network considered number works natural starting point analysis learning algorithm. note training even two-layer network non-convex ﬁnding computationally eﬃcient method guaranteed risk bound open problem now. high level nn-lift estimates weights ﬁrst layer using tensor decomposition. uses estimates learn bias parameter ﬁrst layer using simple fourier technique ﬁnally estimates parameters last layer using linear regresion. nn-lift consists simple linear multi-linear operations fourier analysis ridge regression analysis parallelizable large-scale data sets. computational complexity comparable standard sgd; fact parallel time complexity methods order method requires processors multiplicative factor scales linearly input dimension. work assume knowledge input density continuous diﬀerentiable function. unlike many theoretical works e.g. limit distributions product gaussian distributions input. unsupervised learning i.e. estimation density hard problem general models work investigate exploited make training neural networks tractable. knowledge naturally available experimental design framework person designing experiments ability choose input distribution. examples include conducting polling carrying drug trials collecting survey information utilizing generative models input score functions utilize knowledge input density obtain certain transformations input given class score functions. score functions normalized derivatives input pdf; input vector ﬁrst order score function vector second order score matrix higher order scores tensors. nn-lift method ﬁrst estimate cross-moments output input score functions decompose rank- components. risk bounds risk bound includes approximation estimation errors. approximation error error ﬁtting target function neural network given architecture estimation error error estimating weights neural network using given samples. weight matrix ﬁrst layer denoting number neurons denoting input dimension. suppose weight vectors non-degenerate i.e. weight matrix full column rank. assume continuous input distribution access score functions bounded non-zero measure. allow general sigmoidal activation functions non-zero third derivatives expectation thus eﬃciently learn neural network parameters polynomial sample complexity using nn-lift algorithm. addition method polynomial computational complexity fact parallel time complexity stochastic gradient descent backpropagation. theorem formal result. extend results non-realizable setting target function need generated neural network. method nn-lift succeed require approximation error suﬃciently small given network architecture. note practical interest consider functions large approximation errors since classiﬁcation performance case poor state informal version result follows. assume following target function continuous fourier spectrum suﬃciently smooth i.e. parameter deﬁnition) suﬃciently small speciﬁed implies approximation error target function controlled i.e. exists neural network given size target function bounded approximation bound require target function bounded ﬁrst order moment fourier spectrum; example show bound satisﬁed class scale location mixtures gaussian kernel function. observe kernel mixtures correspond smoother functions bound likely satisﬁed. intuitive since smoother functions lower amount high frequency content. also notice bound dependence variance gaussian input obtain relaxed bound middle values i.e. neither large small. appendix discussion proof corollary. intuitions behind conditions risk bound since exist worst-case instances learning hard natural expect nn-lift guarantees certain conditions met. assume input regular continuous probability density function details. reasonable assumption since boolean inputs reduces learning parity noise hard problem assume activating functions sufﬁciently non-linear since linear network collapsed single layer non-identiﬁable. precisely characterize estimation error depends non-linearity activating function third order derivative. another condition providing risk bound non-redundancy neurons. neurons redundant over-speciﬁed network. realizable setting target function generated neural network given number neurons require weights ﬁrst layer linearly independent. non-realizable setting require satisﬁed vectors randomly drawn fourier magnitude distribution target function precisely random frequencies drawn probability distribution kωk·|f |/cf fourier transform arbitrary function normalization factor; corresponding discussions details. mild condition holds distribution continuous domain. thus conditions achieving bounded risk mild encompass large class target functions input distributions. tensors required? employ cross-moment tensor encodes correlation third order score function output. decompose moment tensor rank- components yield weight vectors ﬁrst layer. require least third order tensor learn neural network weights following reasons matrix decomposition identiﬁable orthogonal components tensors identiﬁable non-orthogonal components. general realistic assume weight vectors orthogonal hence require tensors learn weight vectors. moreover tensors learn overcomplete networks number hidden neurons exceed input/output dimensions. note matrix factorization methods unable learn overcomplete models since rank matrix cannot exceed dimensions. thus critical incorporate tensors training neural networks. recent papers analyzed tensor methods detail established convergence perturbation guarantees despite non-convexity decomposition problem. strong theoretical guarantees essential deriving provable risk bounds nn-lift. extensions algorithm nn-lift extended layers recursively estimating weights layer layer. principle analysis extended controlling perturbation introduced layer-by-layer estimation. establishing precise guarantees exciting open problem. work assume knowledge generative model input. argued before many settings experimental design polling design input control learner. even known recent ﬂurry research activity shown wide class probabilistic models trained consistently using suite diﬀerent eﬃcient algorithms convex relaxation methods spectral tensor methods alternating minimization require polynomial sample computational complexity respect input hidden dimensions. methods learn rich class models also includes latent hidden variable models. another aspect addressed work issue regularization nn-lift algorithm. work assume number neurons chosen appropriately balance bias variance cross validation. designing implicit regularization methods dropout early stopping tensor factorization analyzing rigorously another exciting open research problem. analysis backpropagation loss surface optimization baldi hornik show activations linear backpropagation unique local optimum corresponds principal components covariance matrix training examples. however known exist networks non-linear activations backpropagation fails; instance brady construct simple cases linearly separable classes backpropagation fails. note simple perceptron algorithm succeed linear separability. gori tesi argue examples artiﬁcial backpropagation succeeds reaching global optimum linearly separable classes practical settings. however show non-linear separability backpropagation stuck local optima. detailed survey frasconi recently choromanska analyze loss surface multi-layer relu network relating spin glass system. make several assumptions variable independence input equally likely paths input output redundancy network parameterization uniform distribution unique weights realistic. assumptions network reduces random spin glass model known lowest critical values random loss function form layered structure number local minima outside band diminishes exponentially network size however imply computational eﬃciency guarantee good local optimal point using computationally cheap algorithms since still exponential number points. haeﬀele vidal provide general framework characterizing local optima become global deep learning scenarios. idea network suﬃciently overspeciﬁed exist local optima neurons zero contribution local optima fact global. provides simple uniﬁed characterization local optima global. however general clear design algorithms reach eﬃcient optimal points. previous theoretical works training neural networks analysis risk neural networks classical problem. approximation error layer neural network analyzed number works barron provides bound approximation error combines estimation error obtain risk bound computationally ineﬃcient method. sample complexity neural networks extensively analyzed barron bartlett assuming convergence globally optimal solution general intractable. anthony bartlett shalev-shwartz ben-david exposition classical results neural networks. andoni learn polynomial target functions using two-layer neural network gaussian/uniform input distribution. argue weights ﬁrst layer selected randomly second layer weights linear need ﬁtted optimally. however practice gaussian/uniform distributions never encountered classiﬁcation problems. general distributions random weights ﬁrst layer suﬃcient. framework impose mild non-degeneracy conditions weights. livni make observation networks quadratic activation functions trained computationally eﬃcient manner incremental manner. quadratic activations greedily adding neuron time solved eﬃciently eigen decomposition. however standard sigmoidal networks require large depth polynomial network practical. posted initial version paper zhang extended framework improper learning scenario output predictor need neural network. show norm incoming weights layer bounded learning eﬃcient. however usual neural networks sigmoidal activations norm weights scales dimension case algorithm longer polynomial time. arora provide bounds leaning class deep representations. layer-wise learning neural network learned layer-by-layer unsupervised manner. assume sparse edges random bounded weights threshold functions hidden nodes. diﬀerence here considering supervised setting input output allow general sigmoidal functions hidden neurons. recently posting initial version paper hardt provided analysis stochastic gradient descent generalization error convex non-convex problems training neural networks. show generalization error controlled mild conditions. however work address reaching solution small risk bound using general stuck spurious local optima. hand show addition small generalization error method yields neural network small risk bound. note method moment-based estimation methods come stability bounds guarantee good generalization error. closely related paper sedghi anandkumar consider learning neural networks sparse connectivity. employ cross-moment label score function input. show provably learn weights ﬁrst layer long weights sparse enough enough number input dimensions output classes paper remove restrictions allow output binary class number neurons exceed input/output dimensions moreover extend beyond realizable setting require target functions generated class neural networks consideration. denote euclidean norm vector denote inner product vectors matrix rd×k j-th column referred throughout paper matrices rd×k khatri-rao product rd×k deﬁned function notations throughout paper following convention distinguish diﬀerent types functions. denote arbitrary function exploit denote output realizable neural network. helps diﬀerentiate them. also notation denote estimated neural networks using ﬁnite number samples. introduce problem training neural network realizable non-realizable settings elaborate notion risk bound trained neural network approximates arbitrary function. known continuous functions compact domain arbitrarily well approximated feedforward neural networks hidden layer sigmoidal nonlinear functions input density estimated numerous methods score matching spectral methods. settings experimental design input density known learner since designs density function framework directly applicable there. addition need know normalization factor partition function input distribution estimation normalization factor suﬃces. paper propose algorithm training neural networks provide risk bound risk bounds respect arbitrary target function. risk expected loss joint probability density function input output here consider squared loss bound risk error arbitrary function want approximate denoting estimated neural network. notion risk also called mean integrated squared error. proposed risk error neural network consists parts approximation error estimation error. approximation error error ﬁtting target function neural network given architecture estimation error error training network ﬁnite number samples denoted thus risk error measures ability trained neural network generalize data generated function introduce realizable non-realizable settings elaborates sources error. matrices rd×k bias vectors involves estimation analysis label-function speciﬁed ﬁxed unknown parameters goal learn parameters ﬁnally bound overall given ﬁnite samples. task propose computationally eﬃcient algorithm requires polynomial number samples bounded estimation error. ﬁrst time result established neural network. non-realizable setting output arbitrary function necessarily neural network. want approximate denoting estimated neural network. setting additional approximation analysis also required. paper combine estimation result realizable setting approximation bounds barron leading risk bounds respect target function deﬁnition risk. detailed results provided section section introduce proposed method learning neural networks using tensor fourier regression techniques. method shown algorithm named nn-lift algorithm three main components. ﬁrst component involves estimating weight matrix ﬁrst layer denoted rd×k tensor decomposition method. second component involves estimating bias vector ﬁrst layer fourier method. ﬁnally estimate parameters last layer linear note unknown parameters estimated ﬁrst part thus main part algorithm. given fact also provide alternative method estimation parameters model given estimate tensor method. based incrementally adding neurons whose ﬁrst layer weights given remaining parameters updated using brute force search grid. since update involves updating corresponding bias term contribution ﬁnal output dimensional done eﬃciently; details section hard problem exist numerous instances becomes tractable. examples include spectral methods learning latent variable models gaussian mixtures topic admixture models independent component analysis moreover recent advances non-parametric score matching methods density estimation inﬁnite dimensional exponential families guaranteed convergence rates. methods used estimate input unsupervised manner. below discuss detail score function estimation methods. paper focus input generative information make training neural networks tractable. simplicity subsequent analysis assume quantities perfectly known; possible extend perturbation analysis take account errors estimating input pdf; remark estimation score function various eﬃcient methods estimating score function. framework score matching popular parameter estimation probabilistic models criterion parameters based matching data score function. swersky analyze score matching latent energy-based models. deep learning framework auto-encoders attempts encoding decoding functions minimize reconstruction error added noise; so-called denoising auto-encoders unsupervised framework involving unlabeled samples. alain bengio argue approximately learns ﬁrst order score function input noise variance goes zero. sriperumbudur propose non-parametric score matching methods density estimation inﬁnite dimensional exponential families guaranteed convergence rates. therefore methods observe columns weight vector linear coeﬃcients involved input variable taking derivative function chain rule linear coeﬃcients shows ﬁnal form. particular show lemma neural network equation notion tensor rank rank- components. clariﬁes score function acts linearization operator ﬁnal output nonlinear terms form also clariﬁes reason behind using tensor decomposition learning framework. tensor decomposition algorithm goal tensor decomposition algorithm recover rank- components tensor. step tensor decomposition algorithm proposed appendix details. main step tensor decomposition method tensor power iteration generalization matrix power iteration order tensors. tensor power iteration given convergence guarantees tensor power iteration orthogonal tensor decomposition developed literature note ﬁrst orthogonalize tensor whitening procedure apply tensor power iteration. thus original tensor decomposition need orthogonal. popular perform tensor decomposition stochastic computational complexity manner reduces computational complexity. done splitting data minibatches. starting ﬁrst mini-batch perform small number tensor power iterations result initialization next mini-batch mentioned earlier assume suﬃciently good approximation score function tensor given speciﬁc cases tensor factor form reduce computational complexity nn-lift computing whole tensor explicitly. factor form mean write score function tensor terms summation rank- components could summation samples existing structures model. state examples factor form provide computational complexity. example input follows gaussian distribution score function simple polynomial form computational complexity tensor decomposition number samples number initializations tensor decomposition. similar argument follows input distribution mixture gaussian distributions. processors. alternatively also exploit recent tensor sketching approaches computing tensor decompositions eﬃciently. wang build idea count sketches show running time linear input dimension number samples independent order tensor. thus tensor decompositions computed eﬃciently. diﬀerent previous step estimating based tensor decomposition methods. fourier-based method complex variables formed using labeled data random frequencies fourier domain; procedure prove lemma entries estimated phase complex numbers. also observe lemma magnitude complex numbers used estimate discussed appendix dimensional manifold denoted intersection sphere manifold actually surface spherical cap. order draw frequencies polynomial time consider d-dimensional spherical coordinate system angles deﬁned based cone axis. directly impose cone constraint limiting corresponding angle random draw. addition kothari meka propose method generating pseudo-random variables spherical logarithmic time. remark computation score function fourier method involve knowledge input however need know normalization factor also known partition function input pdf. score function immediately seen deﬁnition since normalization factor input labeled samples estimates regularization parameter denote estimation neurons. append neuron dummy variable represent bias thus rk+. ˆσˆh denote estimated parameters λ-regularized ridge regression canceled division thus estimation score function hard estimation input normalization factor. fourier method non-normalized estimation input leads normalization mismatch estimation corresponding complex number. problem since phase information complex numbers. neural network model given good estimation neurons estimate parameters last layer linear regression. provide procedure ridge regression algorithm estimate parameters last layer appendix details ridge regression corresponding analysis guarantees. section provide guarantees realizable setting function function recovery estimation done algorithm dercomplete regime provide results assuming full column rank. non-degeneracy weight vectors undercomplete setting weight matrix rd×k full column rank smin smin denotes minimum overcomplete setting khatri-rao product rd×k full column rank smin remark generalization. nonzero. here input nonlinear operator addition satisﬁes lipschitz property constant suppose nonlinear activating function satisﬁes property many popular activating functions step function elaborate conditions. non-degeneracy weight vectors required tensor decomposition analysis estimation undercomplete setting algorithm ﬁrst orthogonalizes tensor given decomposes tensor power iteration. note convergence power iteration orthogonal tensor decomposition guaranteed shown bhaskara condition satisﬁed smoothed analysis. step function {u>} used activating function lipschitz property hold non-continuity assume lipschitz property holds linear continuous part i.e. argue input step function {u>} w.h.p. linear interval orthogonalization procedure work need tensor components linearly independent. overcomplete setting algorithm performs steps additional tensorizing procedure; appendix details. case higher order tensor given algorithm ﬁrst tensorized performing steps addition non-degeneracy condition weight matrix coeﬃcients condition λj’s also required ensure corresponding rank- components vanish thus tensor decomposition algorithm recovers them. similarly coeﬃcients also nonzero enable using second order moment whitening step tensor decomposition algorithm. coeﬃcients vanishes option perform whitening; remark procedure details. note amount non-linearity derivative notation σ′′′ characterizing coeﬃcients ˜λj) need diﬀerentiability non-linear function points. particular input continuous variable dirac delta function derivative non-continuous points; instance derivative step function {x>} {x>} thus general need expectations exist type functions corresponding higher order derivatives. fourier methods. condition also assumed tackle sign issue recovering columns remark details. subgaussian noise bounded statistical leverage conditions standard conditions required ridge regression used estimating parameters second layer neural network. parameters σnoise aﬀect sample complexity ﬁnal guarantees. imposing additional bounds parameters neural network useful learning parameters computationally eﬃcient algorithms since limits searching space training particular fourier analysis assume following conditions. parameters. suppose columns weight matrix normalized i.e. entries ﬁrst layer bias vector bounded note normalization condition columns also needed identiﬁability parameters. instance nonlinear operator step function {z>} matrix identiﬁable norm thus normalization condition required identiﬁability. estimation entries bias vector obtained phase complex number fourier analysis; procedure details. since ambiguity phase complex number impose bounded assumption entries avoid ambiguity. main results. conditions relaxed more. suppose input bounded i.e. assume input probability density function regularity conditions might seem contradictory lower bound condition easy that. lower bound complete form sample complexity. here rd×d denotes matricization score function tensor rd×d×d; deﬁnition matricization. furthermore λmin minj∈ |λj| ˜λmin minj∈ |˜λj| ˜λmax maxj∈ |˜λj| minj∈ ˜ymax ˜ymax function estimate using estimated parameters stated tensorizing higher order tensors lower order ones estimate overcomplete models hidden layer dimension larger input dimension generalize idea higher order tensorizing modes higher order tensor tensorized single mode resulting lower order tensor. assume exactly know functions thus additional error introduced estimating them. straightforward incorporate corresponding errors estimating input density ﬁnal bound. order provide risk bound respect arbitrary target function also need argue approximation error addition estimation error. arbitrary function need neural network whose error approximating function bounded. combine estimation error training neural network. yields ﬁnal risk bound. approximation problem ﬁnding neural network approximates arbitrary function bounded error. thus diﬀerent realizable setting ﬁxed neural network analyze estimation. barron provides approximation bound two-layer neural network exploit here. result based fourier properties function recall deﬁnition fourier transform denoted called frequency variable. deﬁne ﬁrst absolute moment fourier magnitude distribution section detailed discussion connection columns weight matrix random frequency draws fourier magnitude distribution argued proof approximation bound. parameters need also found. shows following approximation bound condition section earlier explanations section. overcomplete regime linear independence property needs hold appropriate tensorizations frequency draws. requirements number samples parameter depend parameters neural network note also dependence parameters coeﬃcients ˜λj. since non-realizable setting neural network parameters correspond neural networks satisfy approximation bound proposed theorem generated random draws frequency spectrum function proposed bound stricter number hidden units increases. might seem counter-intuitive since approximation result theorem suggests increasing leads smaller approximation error. note approximation result theorem consider eﬃcient training neural network. result theorem also deals eﬃcient estimation neural network. imposes additional constraint parameter number neurons increases problem learning network weights challenging tensor method resolve. theorem mainly proved combining estimation bound guarantees theorem approximation bound results neural networks provided theorem note approximation bound provided theorem holds speciﬁc class neural networks generally recovered nn-lift algorithm. addition estimation guarantees theorem realizable setting observations outputs ﬁxed neural network theorem observe samples arbitrary function thus approximation analysis theorem directly applied theorem this need additional assumptions ensure nn-lift algorithm recovers neural network close neural networks satisfy approximation bound theorem therefore impose bound quantity full column rank assumption proposed theorem appendix complete proof theorem theorem formal statement. deﬁnition bound weaker fourier spectrum target energy higher frequencies. makes intuitive sense since easier approximate function smooth less ﬂuctuations. second term estimation error nn-lift algorithm analyzed theorem polynomial factors sample complexity estimation error slightly worse bound provided barron note provide estimation method computationally statistically eﬃcient method barron computationally eﬃcient. thus ﬁrst time computationally eﬃcient method guaranteed risk bounds training neural networks. discussion approximation bound approximation bound involves term constant shrink increasing neuron size recall measures distance unit step function {z>} scaled sigmoidal function provide following observations section provide additional discussions. ﬁrst propose example contrasting hardness optimization problems backpropagation tensor decomposition. discuss generalization learning guarantees higher dimensional output also continuous output case. discuss alternative approach estimating low-dimensional parameters model. discussed computational hardness training neural network nonconvexity loss function thus popular local search methods backpropagation stuck spurious local optima. provide example highlighting issue contrast tensor decomposition approach. consider simple binary classiﬁcation task shown figure blue magneta data points correspond diﬀerent classes. clear classes classiﬁed mixture linear classiﬁers drawn green solid lines ﬁgure. task consider two-layer neural network hidden neurons. loss surfaces backpropagation tensor decomposition shown figures respectively. shown terms figure classiﬁcation task colors correspond binary labels. two-layer neural network hidden neurons used. loss surface terms ﬁrst layer weights neuron plotted parameters ﬁxed. loss surface usual square loss objective spurious local optima. part spurious local optima drawn dashed lines global optimum drawn green solid lines. loss surface tensor factorization objective free spurious local optima. stark contrast optimization landscape tensor objective function usual square loss objective used backpropagation observed even simple classiﬁcation task backpropagation suﬀers spurious local optima case tensor methods least locally convex. comparison highlights algorithmic advantage tensor decomposition compared backpropagation terms optimization performing. results easily extended complicated cases higher dimensional output also continuous outputs rest section discuss necessary changes algorithm adapt cases. fourier method similar scalar case entries output estimate entries additional diﬀerence continuous case. suppose output generated noise vector independent input case ridge regression ridge regression method analysis immediately generalized non-scalar output applying method independently diﬀerent entries output vector recover diﬀerent columns matrix diﬀerent entries vector estimate ﬁrst layer weights greedily neurons weight vectors choose bias grid search learn contribution ﬁnal output. lines method proposed barron crucial diﬀerence case ﬁrst layer weights already estimated tensor method. barron proposes optimizing weight vector d-dimensional space whose computational complexity scale exponentially worst case. setup here since already estimated high-dimensional parameters need estimate dimensional parameters. hidden unit indexed parameters include bias input layer neuron weight neuron output makes approach computationally tractable even brute-force exhaustive search best parameters ﬁnite guarantees akin estimation bound proposed theorem complete proof provided appendix recall nn-lift algorithm includes tensor decomposition part estimating fourier technique estimating linear regression estimating application linear regression last layer immediately clear. section propose main lemmas clarify methods useful estimating unknown parameters exploit approximation bound argued barron provided theorem ﬁrst discuss main result arguing approximation bound function bounded parameter deﬁnition note corresponds ﬁrst term approximation error proposed theorem result barron consider bound parameters ﬁrst layer provides reﬁnement result also bounds parameters neural network also leads additional term involving approximation error seen theorem note bounding parameters neural network also useful learning parameters computationally eﬃcient algorithms since limits searching space training parameters. provide main ideas proving bounds follows. ﬁrst provide proof outline additional constraints parameters neural network; deﬁned compare form additional bounds. case barron argues approximation bound proved based main results. ﬁrst result says function closure random frequency draws fourier magnitude distribution recall section columns weight matrix normalized version random frequencies drawn fourier magnitude distribution equation connection along proof relation gcos recap here; proof lemma barron details. expanding fourier transform magnitude phase parts ejθ|f proposed novel algorithm based tensor decomposition training two-layer neural networks. computationally eﬃcient method guaranteed risk bounds respect target function polynomial sample complexity input neuron dimensions. tensor method embarrassingly parallel parallel time computational complexity logarithmic input dimension comparable parallel stochastic backpropagation. number open problems consider future. extending framework multi-layer network great interest. exploring score function framework train discriminative models also interesting. thank recht pointing barron’s work approximation bounds neural networks thank arthur gretton discussion estimation score function. also acknowledge fruitful discussion weiss presentation proof note condition bounded derivative rule cases step function sigmoidal function. similar analysis main case ﬁrst argue function closure functions gcos univariate functions bounded derivative. theorem combining estimation approximation bounds detailed editorial comments preliminary version draft. thank peter bartlett detailed discussions pointing several classical results neural networks. thankful andrew barron detailed discussion encouraging explore alternate incremental training methods estimating remaining parameters tensor decomposition step added discussion paper. thank daniel discussion random design analysis ridge regression. thank percy liang discussion score function. janzamin supported bigdata award sedghi supported career award anandkumar supported part microsoft faculty fellowship career award ccf- award n---. goal tensor decomposition algorithm recover rank- components tensor; refer equation notion tensor rank rank- components. exploit tensor decomposition algorithm proposed figure depicts ﬂowchart method corresponding algorithms procedures also speciﬁed. similarly algorithm states high-level steps tensor decomposition algorithm. main step tensor decomposition method tensor power iteration generalization matrix power iteration order tensors. tensor power iteration given tensor power iteration performed svd-based technique proposed procedure helps initialize non-convex tensor power iteration good initialization vectors. whitening preprocessing applied orthogonalize components input tensor. note convergence guarantees tensor power iteration orthogonal tensor decomposition developed literature tensorization tensorizing step applied want decompose overcomplete tensors rank larger dimension instance getting rank ﬁrst form order input tensor decomposition eﬃcient implementation tensor decomposition given samples main update steps tensor decomposition algorithm tensor power iteration multilinear operation performed tensor however tensor available beforehand needs estimated using samples computing storing tensor enormously expensive high-dimensional problems. essential note since form factor form tensor using samples parameters model manipulate samples directly perform power update multi-linear operations without explicitly forming tensor. leads eﬃcient computational complexity. details implicit update forms. proof theorem includes three main pieces arguing recovery guarantees three diﬀerent parts algorithm tensor decomposition fourier method linear regression. ﬁrst piece show tensor decomposition algorithm estimating weight matrix recovers desired error. second part analyze performance fourier technique estimating bias vector proving error recovery small. finally last step ridge regression analyzed ensure parameters last layer neural network well estimated leading estimation overall function provide analysis three parts. σ′′′ denotes third order derivative element-wise function concretely slightly abuse notation σ′′′ rk×k×k×k diagonal order tensor j-th diagonal entry equal properties used compute third order derivative r.h.s. equation follows. apply chain rule take derivatives generates factor derivative. since take order derivative factors linearity next layers leads derivatives vanished thus term derivative. expanding multilinear form ﬁnishes proof; deﬁnition multilinear form. lemma among conditions theorem consider rank constraint non-vanishing assumption coeﬃcients λj’s. whitening performed using empirical version second order score function speciﬁed assume coeﬃcients ˜λj’s vanish. suppose sample complexity remark observe addition permutation ambiguity recovery guarantees also sign ambiguity issue recovering columns matrix decomposition third order tensor sign coeﬃcient change overall tensor still ﬁxed. note coeﬃcient positive negative. according fourier method estimating mis-calculating sign also leads sign recovered opposite manner. words recovered sign bias consistent recovered sign many popular activating functions step function sigmoid function tanh function satisfy property. given property sign ambiguity parameters leads opposite sign input activating function compensated sign value recovered least squares. rank-one components columns matrix equation tensor decomposition form. apply tensor decomposition method nn-lift estimate columns employ noisy tensor decomposition guarantees anandkumar show perturbation tensor small tensor power iteration initialized svd-based procedure recovers rank- components small error. also analyze whitening step combine result leading lemma matricization rd×d×d; deﬁnition matricization. norm bounded matrix bernstein’s inequality. norm random variable inside summation bounded ˜ymax ˜ymax bound |˜y|. variance term also bounded remaining piece complete proof tensor decomposition part. analysis anandkumar involve whitening step thus need adapt perturbation analysis anandkumar additional whitening procedure. done lemma ﬁnal recovery bound lemma terms; involving involving kek. ﬁrst impose bound sample complexity bound involving dominates bound involving follows. considering bounds imposing lower bound number perturbation analysis proposed tensor decomposition method algorithm corresponding svd-based initialization procedure provided anandkumar consider eﬀect whitening proposed procedures thus need adapt perturbation analysis anandkumar whitening procedure incorporated. perform section. ﬁrst elaborate whitening step analyze proposed procedure works. analyze inversion whitening operator showing components whitened space translated back original space stated procedure ﬁnally provide perturbation analysis whitening step estimations moments given. vector input nonlinear operator proved similar lemma second option leads form coeﬃcient modiﬁed matrix rd×k denote whitening matrix noiseless case i.e. whitening matrix procedure constructed applying whitening matrix since square matrix also concluded therefore tensor whitened rank- components vj’s form orthonormal basis. discussion clariﬁes whitening procedure works. versions used whitening procedure. here rd×d noiseless case rd×k denotes whitening matrix constructed procedure ⊤cmcw thus orthogonalizes noisy matrix applying whitening matrix tensor analysis fourier method estimating parameter includes following lemmas. ﬁrst lemma argue mean random variable introduced algorithm realizable setting. clariﬁes phase related unknown parameter second lemma argue concentration around mean leading sample complexity result. note denoted realizable setting. fourier method also used estimate weight vector since appears magnitude complex number section also provide analysis estimating fourier method used alternative primarily estimate ridge regression analyzed appendix second equality uniform draws ωl;ν ωl;ν denotes indicator function here |ωl;ν| denotes volume dimensional subspace ωl;ν limit |ωl;ν| ·|ωl| |ωl| denotes surface area dimensional manifold survived integral thus note limν→+ rest proof computing integral. integral involves delta functions ﬁnal value expected computed single point speciﬁed intersection line based following integration property delta functions function sake simplifying mathematical notations substitute ωd’s ﬁrst equality note ωd’s implicitly function ﬁnally considered second equality delta integration property applied variable ˆh⊤β here noisy linear model additional bias denote ridge regression estimator regularization parameter deﬁned minimizer regularized empirical mean squared error i.e. empirical covariance denotes empirical mean ˆσˆh operator. analysis ridge regression leads following expected prediction error bound estimation output. satisﬁes lipschitz property following noise approximation statistical leverage conditions also hold. then choosing optimal λ-regularized ridge regression estimated output satisﬁes risk bound perturbation decomposition approximation estimation parts similar estimation part analysis need ensure perturbation exact means small enough apply analysis lemmas here addition empirical estimation quantities approximation error also contributes perturbation. realizable setting here observations arbitrary function address tensor decomposition fourier parts follows. recall notation denote output neural network. arbitrary function refer neural network satisfying approximation error provided theorem ultimate goal analysis show nn-lift recovers parameters speciﬁc neural network small error. precisely note class neural networks satisfying approximation bound theorem suﬃces output algorithm close enough them. theorem approximation error need analyze estimation perturbations characterized since neural network output directly observed goal argue norm perturbations small enough ensuring tensor power iteration recovers rank- impose bound term involving dominant bounding estimation part keest.k provides similar sample complexity estimation lemma ˜ymax substituted ymax. introduces additional approximation term linear regression forridge regression mulated given bounds approximation term contributes lower order terms. similar speciﬁc gaussian kernel function also provide results kernel functions general positive deﬁnite functions follows. said positive deﬁnite algorithm train neural network approximates class positive deﬁnite kernel functions similar bounds theorem corollary special case gaussian kernel function. recall columns randomly drawn fourier spectrum described given gaussian kernel fourier spectrum kωk·|f corresponds sub-gaussain distribution. thus singular values bounded", "year": 2015}