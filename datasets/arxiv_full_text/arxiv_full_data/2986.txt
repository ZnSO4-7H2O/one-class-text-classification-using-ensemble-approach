{"title": "A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic  Extraction of Semantics, Formation of Integrated Concepts and Re-selection  Features for Ambiguity", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Integration between biology and information science benefits both fields. Many related models have been proposed, such as computational visual cognition models, computational motor control models, integrations of both and so on. In general, the robustness and precision of recognition is one of the key problems for object recognition models.  In this paper, inspired by features of human recognition process and their biological mechanisms, a new integrated and dynamic framework is proposed to mimic the semantic extraction, concept formation and feature re-selection in human visual processing. The main contributions of the proposed model are as follows:  (1) Semantic feature extraction: Local semantic features are learnt from episodic features that are extracted from raw images through a deep neural network;  (2) Integrated concept formation: Concepts are formed with local semantic information and structural information learnt through network.  (3) Feature re-selection: When ambiguity is detected during recognition process, distinctive features according to the difference between ambiguous candidates are re-selected for recognition.  Experimental results on hand-written digits and facial shape dataset show that, compared with other methods, the new proposed model exhibits higher robustness and precision for visual recognition, especially in the condition when input samples are smantic ambiguous. Meanwhile, the introduced biological mechanisms further strengthen the interaction between neuroscience and information science.", "text": "vision interdisciplinary research directions neuroscience information science. mechanisms primate human visual processing cognition introduced computational cognition model. main difference hmax hierarchical architectures convolutional neural networks etc.) focused reproducing anatomical physiological psychophysical properties ventral pathway visual system consists inferior temporal cortical areas. ﬁrst publication well-known model developed improved different aspects example many researchers modiﬁed original hmax model adding feedback process improve recognition precision modiﬁcations include adding sparsity convolutional layer enhancing architecture adding speciﬁc layers model changing strategies feature selection ﬁltering properties frontier researchers introduce mechanisms attention visual model. itti proposed saliency-based model based saliency theory human visual system combined attention object recognition spatial information object introduced modeling dorsal pathway vision system. implement bayesian inference saliency model mechanisms middle high level cortices also topic area. based hmax deep neural network qiao developed series models introducing association attention model. introduced mechanisms show good performance object classiﬁcation identiﬁcation tasks. ability generalization objectives motivations visual cognition models. however recent ﬁndings point even state-of-art deep hierarchical networks suffer tiny disturbance transformation. tiny perturbation cause signiﬁcant difference output hierarchical network models abstract—integration biology information science beneﬁts ﬁelds. many related models proposed computational visual cognition models computational motor control models integrations general robustness precision recognition problems object recognition models. inspired features human recognition process biological mechanisms integrated dynamic framework proposed mimic semantic extraction concept formation feature re-selection human visual processing. main contributions proposed model follows semantic feature extraction local semantic features learnt episodic features extracted images deep neural network; feature re-selection ambiguity detected recognition process distinctive features according difference ambiguous candidates re-selected recognition. experimental results hand-written digits facial shape dataset show that compared methods proposed model exhibits higher robustness precision visual recognition especially condition input samples smantic ambiguous. meanwhile introduced biological mechanisms strengthen interaction neuroscience information science. science biological mechanisms applied computational models promotes development biologically inspired models. hand inspired recent ﬁndings biology models outperform classic algorithms performance efﬁciency. hand related neural mechanisms introduced qiao zhong state management control complex systemsinstitute automation chinese academy sciences beijing china zhang institute applied mathematics academy ﬁcult object recognition task various viewpoints scales deformation ambiguity. according biological ﬁndings objection recognition tasks involve multiple cortices many sophisticated mechanisms including preliminary cognition top-down attention semantic conceptual memory lake salakhutdinov tenenbaum recently employs semantics concepts explicitly achieves signiﬁcant improvement robustness one-shot character recognition. paper build biological mechanism based semantic neural network model extracts semantic information hierarchically forms concepts corresponding probabilities. model trained sequentially generate hierarchical information layer layer. model ﬁrstly trains neural network extract episodic features; integrates learnt episodic features semantic features. encode structure information model learns structural relationships semantics represents population vectors. population vectors concepts categories formed probabilistic way. proposed model also applies dynamic updating strategies feature re-selection adaption ambiguous condition online training concepts. mimicking implementing neural mechanisms visual processing model achieves robustness various ambiguous images small training samples. also efﬁcient diminish uncertainty semantics concepts ability generalization. rest paper organized follows. section introduces biological evidence proposed model. section explains framework methods bsnn. section presents experiments conducted shows experimental results. section summarizes current work points future direction. paper several biological mechanisms introduced framework mimic semantic extraction concept formation feature re-selection process human visual processing. here related biological evidence reviewed discussed validity later implementation. different types memory stored brain episodic memory semantic memory episodic memory stores events detailed contextual information semantic memory extracts regularities different spatial-temporal events forms perceptual categories complex concepts relations requires extraction regularities semantics carried episodes since hippocampus involved storage episodic memory prefrontal cortex contributes organization information extraction process could achieved hippocampus mpfc interaction extracted semantic information could used later tasks. proposed objects could described parts positional connectional relationships example neurons tuned contour fragment orientation speciﬁc object-relative position words neuron could respond convex curvature bottom right right thus area neurons respond individual contour fragments relationships encoded population responses neurons integrate information multiple fragments thus integrated explicit representations multi-part conﬁgurations could encoded attention required people carry various tasks since relevant environmental stimuli information selected processed brain several brain areas activated attention process frontal ﬁelds anterior cingulate frontal cortex etc. visual attention usually consists active exploration environment selection task-related information suppression distraction. visual stimuli clear task visual attention process could suppress distraction location previous attention focus positions search related information section present structure proposed framework. firstly outline framework described. secondly algorithms semantic feature extraction integrated concept formation feature re-selection given details. block primary episodic feature extraction block episodic features extracted original image directly. block includes four-layer convolutional deep belief network network trained layer layer without supervision. output layer activation states last layer learnt connection weights cluster-based method applied here provides better abstract description object feature maps. information learning block spatial positions semantic features learnt based output block block spatial information encoded based position-related population neurons. block structural concept formation block relationships semantic features formed spatial information block relationships encoded orientation-related population neurons. input sample relationship matrix generated represent global structure. block integrated recognition block combines episodic semantic features together recognition. block fig. learns weights episodic semantic features different pathways uses integrated classiﬁcation block fig. block feature re-selection block copes ambiguous situations dynamically. training procedure fig. correlation extracted features categories learnt. candidate recognition results features discriminative candidates selected classiﬁcation. paper episodic features extracted unsupervised deep neural network. unsupervised convolutional deep belief network ﬁrst introduced ng’s work feature extraction tasks. previous work cdbn used extract episodic information image. unsupervised model cdbn able extract good here representing -degree rotation matrix convolutional kernel denotes convolution operation shared basis units shared basis visible layer units. local features encode common components minimizing reconstruction error ensures good performance recognition. cdbn composed stacked convolutional restricted boltzmann machine crbm variant infer original input activation minimize reconstruction error. thus visual information could retrieved memory similar human. structure crbm showed shown fig. crbm includes three layers visible layer hidden layer pooling layer widths respectively. groups feature maps denoted connected shared local weights width width calculated nv−nw represent unit layer index column index stands unit layer layer pooling layer unit obtained pooling speciﬁc block denoted also groups feature maps width nh/c. mathematics crbm model special type energy-based models. given inputs hidden layer binary feature maps energy possible state rnv×nv bnh×nh×k deﬁned refers metric restricted frecon. model computational convenience kmeans metrics iteratively desired algorithm given initial k-means proceeds alternating steps context object recognition spatial structure information highly valuable difﬁcult proper representation. neuroscience researches reveal human brain processes kind information population neurons. taking population neurons related orientation example neuron preferred direction; closer preferred direction direction stimulus activate certain neuron. relationship preferred stimulus rate activation represented gaussian-like curve. many populations together input stimulus also uncertainty stimulus could encoded rate activations among population neurons. paper deﬁne kinds structural features position features relative position component object center relationship features relative structure consists spatial directions distances semantic components. former feature captures spatial positions different semantic features input sample. latter feature represent global concepts different features organized together. thus generalization ability limited. moreover focuses minimization reconstruction error could deeper semantic level extract structure information. overcome drawback paper introduces extraction semantic features enhance ability distinction features. generative model cdbn also ability achieve reconstruction activation. model recall original input image reconstruction augmentation training data. paper train two-layer cdbn apply extract features original images. also reconstruct input layer visualization data augmentation number training samples relatively small. visual reconstruction deﬁned semantics multiple deﬁnitions different ﬁelds linguistics cognitive science artiﬁcial intelligence etc. cognitive science semantic memory facts capture internal properties object human semantic memory store category abstract information object distinguish category objects others. binder desai proposed modality-speciﬁc semantic memory encoded corresponding cortex. convergence ﬁndings semantic information vision represented similar form visual episodic features abstract discriminative. inspired mentioned properties semantic memory neuroscience reasonable hypothesis semantic features visual task formed based learnt hierachical episodic features. semantic features likely activated diverse properties object. visual recognition property like stroke shape represents general cluster patches rather certain patch. formalized description semantic features given follows denote reconstruction function frecon space episodic features space input images. reconstructed episodic feature number episodic features) groups based similarities patches divide corresponding episodic features groups according reconstructions. group patches representative group minimizes loss population encodes relative relationship different components. paper populations neurons denoted position neurons pneurons relationship neurons rneurons. fig. fig. give examples pneurons. single neuron like gaussian ﬁlter consistent neuroscience researches. neuron preferred position position mostly activates neuron. multiple pneurons together population output spatial representations semantic features. fig. example representing direction population coding. activation visualization semantic features generate relative direction different semantic features. output relationship neurons encoding direction. features likely located image. paper computational consideration center position pneurons uniformly distributed gaussian functions discretization size semantic feature maps. input feature aggregated output pneurons forms matrix according center position semantic features positions population rneurons output relative relationship different features. every semantic features paper deﬁnes relationship matrix describe position relationships. shown fig. rneurons input direction output rneurons represented feature relationship matrix. center node matrix encode distance input position. activation process almost rneurons’ except rneurons prefer orientations rather positions. fig. shows detailed process rneurons activated. neuron preferred direction maximizes activation. certain neuron activation responding direction characterized gaussian depending difference input preferred direction. output semantic layer locate position semantic feature population pneurons. tuning function pneuron gaussian function centered certain position represents probability activation different input position. multiple pneurons semantic feature could encoded semantic fig. illustration relationship matrix rneurons. figure left example relationship neurons certain features. surrounding neuron preferred direction; center neuron encodes distance. right actual activation left neurons input features spatial relationship like middle one. output relationship neurons structural semantic information input sample. shown above structural information distributed encoded population neurons. neuron responds speciﬁc semantic features preferred direction. thus encoded structural features actually contain semantic structural information. inspired principles mentioned above feature reselection strategy applied cope ambiguous condition outputs classiﬁcation results high conﬁdence. recognition process backward block choose distinctive structural features. example model cannot decide whether handwritten digit back block choose \"horizon line\" \"half circle\" vertical relationship focus features distinguish relationships categories features learnt block recognition models based spatial positions structural relationships trained block signiﬁcance different features stored weights models. given potential model select features discriminative ability among candidates evaluated absolute differences weights categories. candidates model averages differences mean signiﬁcance features. paper better utilize features consider block feature matrix whole. corresponding weights features summed total weight block. re-selection process model automatically selects blocks larger weights average weights. distribution position relationship neurons category based experiences. sample distribution used approximated prior distribution. concepts utilized block judge possible candidates recognition results. integrated recognition bayesian learning prior work shown perception interpreted bayesian inference process different pathways. related model predict human movements well visual search tasks without assumptions parameter tuning paper object recognition considered bayesian inference process based models trained different kinds features. firstly recognition models like softmax classiﬁers built based different pathways different features including episodic semantic structural features. model outputs vector probabilities categories input sample. training process correlations features categories also learnt feature-selection block recognition results inferred output probabilities recognition models bayesian learning. computational convenience paper assumes recognition based different features independent. detailed computation process follows. category certain object output recognition models based different recognition features. object prior probabilities pathway initialized relative small number total number pathways. training prior distribution updated sample distribution. short mimicking population coding visual perception process proposed model integrated different information extracted original samples bayesian learning. recognition human brain static always adjusts adapts dynamically stimuli. paper especially focuses ambiguity images. ﬁndings visual systems suggest ambiguous image multiple competitive candidates human attention difference candidates. fig. structural concepts re-selected features digit relationship matrix built samples value cell represents average activation certain pneuron. re-selected features judgment achieved comparing structural information current sample learnt concepts. mentioned concepts distribution positions semantic features structural relationships. shown fig. structural concept digit represented relationship matrix generated averaging relationship matrices category reveals likely neurons activated input fig. shows selected signiﬁcant features distinguish input image ambiguous activations discriminative features would feed classiﬁcation models reselection. several experiments conducted verify effectiveness proposed biologically inspired model module tested analyzed details. experiments focused three aspects visualize episodic semantic features extracted proposed model; investigate structure information learned proposed model; evaluate classiﬁcation performance different datasets. trained without supervision likely attracted signiﬁcant features time. verify cdbn extract learn critical information image experiments reconstruction episodic features conducted. examples given fig. illustrates high similarity original image reconstruction. addition fig. illustrates learned weights encode episodic features. original images directly reconstructed high-level features indicates detailed information captured second layer. semantic features clustered extracted episodic features visualized fig. number clusters fig. different semantic features less similar other enhances variety features captures information less features. fig. illustration episodic features extracted cdbn model. learnt features ﬁrst layer second layer visualized respectively. upper visualized deconvolution feature maps input space layer layer bottom visualized averaging activations inputs. experiment visualize extracted episodic features verify features capture critical information original image. here mnist dataset used example. visualizations learnt weights cdbn given fig. corresponds episodic features model. here visualization techniques used including deconvolution method average activations used shown fig. proposed deconvolution method could achieve clearer edges parts method furthermore clear crbm model extract episodic features hierarchically original dataset. details features learnt ﬁrst layer crbm model mostly edges small details input digits whereas second layer cdbn model extracts sophisticated components like circles turning strokes. reasonable outputs second layer learn semantic features. fig. features highly similar. possible reason features extracting semantic features calculate activations pneurons applying position tuning functions features. here position tuning function gaussian function different mean covariance matrix. computational convenience discretized version illustrated fig. feature pneurons form position matrix. example position matrix shown fig. similar mixture several gaussian distributions. visualization structural outputs generated structural relationship matrix encodes distributions relative spatial relationships features. fig. illustrates example structural relationship matrix randomly selected training samples mnist dataset. small square includes eight direction neurons distance neuron illustration structural relationship matrix. square fig. matrix represents relationship pair semantic features. surrounding nodes encoded eight spatial relative directions semantic features. center nodes represent spatial distance pair semantic features. following strategy section iii-f signiﬁcance different features learnt training dataset. fig. illustrates learnt position matrix structural relationships digits shown fig. although activate similar semantic features position structural relationships features quite different. hence evaluating differences position structural relationship could distinctive features build classiﬁer speciﬁc separate fig. shows chosen features reselection distinguish illustrate process better fig. shows examples ambiguous images whereas input images misclassiﬁed convolutional neural network. ambiguous images generated method proposed so-called \"adversarial images\". optimizing modifying original image labeled misclassiﬁed convolutional neural network. applying back-propagation input space limiting martingale gradients able generate tiny perturbations original images could mislead model. images perturbation originally designed convolutional neural network could also affect ologically inspired models hmax etc.) mnist dataset. total mnist includes hand-writing digit images training testing. experiment small training randomly uniformly chosen mnist training data categories. code hmax model obtained author’s website. traditional cdbn model conﬁguration structure model without semantic features structural information. section two-layer cdbn feature maps ﬁrst layer feature maps second layer. pooling size layers. outputs second layer episodic features. episodic features semantic features extracted processed position tuning functions structural relationship neurons. types pneurons used size pair semantic features rneurons results shown table performance proposed bsnn better hmax traditional cdbn. main reason semantic features discriminative even small number integrated moreover performance improved introducing position neurons. ambiguous images mnist ambiguous data generated adding relative small perturbation original mnist data sets details method described iv-c. note networks classiﬁers trained original mnist data set. table classiﬁcation error rate different models ambiguous images. compared results table hmax traditional cdbn performances ambiguous images worse original images. feature re-selection performance bsnn increased selected features discriminative before. facial shape dataset third comparison experiments conducted facial shape dataset. artiﬁcial face composed components different shapes. examples shown fig. compared hand-written digits mnist facial shape dataset stable global structure also scaleshape-variant properties together local transformations. conducting experiments mentioned above compare performance hmax traditional cdbn model training data different sizes results shown table illustrate proposed model successfully learn discriminative features even small dataset. paper novel biologically inspired model proposed robust visual recognition mimicking visual processing system human brain. introducing semantics structural conceptual outputs traditional cdbn network model gains ability generalization especially small training dataset. procedure feature re-selection provides model robustness ambiguity. cognition process ambiguity detected recognition process features according difference ambiguous candidates re-selected online later cognition. future proposed model improved extracting spatiotemporal semantics concepts sequential analysis similar human neural system. another approach enhance model introduce biological mechanisms higher level perception inference. ﬂexible robust classiﬁer function prefrontal cortex human useful process different outputs integrated manner. riesenhuber poggio hierarchical models object recognition cortex nat. neuroscience vol. krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems dura-bernal wennekers denham top-down feedback hmax-like cortical model object perception based hierarchical bayesian networks belief propagation plos vol. serre riesenhuber realistic modeling simple complex cell tuning hmax model implications invariant object recognition cortex dtic document tech. rep. irshad jalali roux racoceanu hwee naour capron automated mitosis detection using texture sift features hmax biologically inspired approach pathology informatics vol. suppl mishra jenkins hierarchical model object recognition based natural-stimuli adapted ﬁlters acoustics speech signal processing ieee international conference ieee miau itti neural model combining attentional orienting object recognition preliminary explorations interplay what proc. annu. international conference ieee vol. patterson nestor rogers where know know? representation semantic knowledge human brain nature reviews neuroscience vol. lake salakhutdinov tenenbaum human-level concept learning probabilistic program induction science vol. moscovitch rosenbaum gilboa addis westmacott grady mcandrews levine black winocur functional neuroanatomy remote episodic semantic spatial memory uniﬁed account based multiple trace theory anatomy vol. benchenane peyrache khamassi tierney gioanni battaglia wiener coherent theta oscillations reorganization spike timing hippocampal-prefrontal network upon learning neuron vol. kesteren fernández norris hermans persistent schema-dependent hippocampal-neocortical connectivity memory encoding postencoding rest humans proc. natl. acad. sci. u.s.a. vol. petersen posner attention system human brain years after annu. review neuroscience vol. green munafò deyoung fossella gray using genetic data cognitive neuroscience growing pains genuine insights nature reviews neuroscience vol. grosse ranganath convolutional deep belief networks scalable unsupervised learning hierarchical representations proc. annu. int. conference mach. learning. eiter ianni lukasiewicz schindlauer tompits combining answer programming description logics semantic artiﬁcial intelli. vol. aksoy abramov dörr ning dellen wörgötter learning semantics object–action relations observation international robotics research szegedy zaremba sutskever bruna erhan goodfellow fergus intriguing properties neural networks learning representation iclr international conference", "year": 2016}