{"title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic  Modeling in Speech Recognition", "tag": ["cs.NE", "cs.CL", "cs.LG", "cs.SD"], "abstract": "We present a comprehensive study of deep bidirectional long short-term memory (LSTM) recurrent neural network (RNN) based acoustic models for automatic speech recognition (ASR). We study the effect of size and depth and train models of up to 8 layers. We investigate the training aspect and study different variants of optimization methods, batching, truncated backpropagation, different regularization techniques such as dropout and $L_2$ regularization, and different gradient clipping variants.  The major part of the experimental analysis was performed on the Quaero corpus. Additional experiments also were performed on the Switchboard corpus. Our best LSTM model has a relative improvement in word error rate of over 14\\% compared to our best feed-forward neural network (FFNN) baseline on the Quaero task. On this task, we get our best result with an 8 layer bidirectional LSTM and we show that a pretraining scheme with layer-wise construction helps for deep LSTMs.  Finally we compare the training calculation time of many of the presented experiments in relation with recognition performance.  All the experiments were done with RETURNN, the RWTH extensible training framework for universal recurrent neural networks in combination with RASR, the RWTH ASR toolkit.", "text": "recent experiments show deep bidirectional long shortterm memory recurrent neural network acoustic models outperform feedforward neural networks automatic speech recognition however training requires tuning experience. work provide comprehensive overview various blstm training aspects interplay within missing literature. investigate different variants optimization methods batching truncated backpropagation regularization techniques dropout study effect size depth training models layers. includes comparison computation times recognition performance. furthermore introduce pretraining scheme lstms layer-wise construction network showing good improvements especially deep networks. experimental analysis mainly performed quaero task additional results switchboard. best blstm model gave relative improvement word error rate compared best feed-forward baseline quaero task. experiments done using returnn rasr rwth’s extensible training framework universal recurrent neural networks toolkit. training conﬁguration ﬁles publicly available. deep neural networks yield state-of-the-art performance classiﬁcation many machine learning tasks class recurrent neural networks especially long short-term memory networks perform well dealing sequence data like speech. recently shown lstm based acoustic models outperform ffnns large vocabulary continuous speech recognition training procedure lstms esp. deep bidirectional lstms takes time effort tune arguably feedforward networks. many aspects considered training lstms exploring work network topology sequence chunking batch sizes optimization methods regularization experiments show huge variance recognition performance depending different aspects. missing overview effect interdependencies various approaches. best knowledge currently overview like exists literature presented work. comprehensive study various aspects training deep blstms provide conﬁguration ﬁles experiments framework returnn compared best ffnn baseline relative improvement word error rate train deep blstm networks layers acoustic modeling discovered pretraining scheme layer-wise construction improve performance deeper lstms. aware previous work applied pretraining lstms asr. hybrid rnn-hmm models developed early work bidirectional rnns timit presented early hybrid lstm-hmm presented timit. investigate various bidirectional unidirectional lstm topologies optional projection cases combined convolutional feed-forward layers acoustic modeling asr. variations lstm model studied although present standard lstm without peephole work. standard lstm model without peephole connections otherwise stated bidirectional lstms base tool rasr speech recognition toolkit rasr feature extraction pipeline decoding. extended rasr python bridge allow many kinds interactions external tools. python bridge introduced able returnn theano-based framework training forwarding recognition acoustic model. returnn multiple lstm implementations supports aspects discuss paper. particular lstm implementation supported custom cuda kernel gives great speed improvements. provide details software conﬁg ﬁles subset hours quaero broadcast conversational english speech database train. development eval evaluation eval sets consist hours speech each. recognition performed using -gram language model. details task found baseline common nn-hmm hybrid acoustic model acoustic models trained frame-wise cross entropy criterion based ﬁxed viterbi alignment. investigate discriminative sequence training study. input features -dimensional vtln-normalized gammatone don’t context window delta frames lstm expect lstm automatically learns context. classiﬁcation regression tree labels. also special residual phoneme types lexicon used transcription unknown unintelligible parts. remove frames aligned phonemes according ﬁxed viterbi alignment. means output class labels softmax layer recognition never hypothesize phonemes. minibatch construction similar described detail minibatch consists nchunks number chunks corpus segments. chunks frames long select every tstep frames corpus. common settings tstep nchunks minibatch size frames. nchunks update step stays every mini batch independent nchunks thus case total update scale epoch stays independent nchunks tstep impact total update scale. experiments train epochs. small separate cross validation measure frame error rate cross entropy model epochs epoch best epoch best evaluate eval eval. results tables select epoch best eval. also state epoch. give hint convergence speed whether overﬁt later. despite optimization method might already provide kind implicit learning rate scheduling always also another explicit learning rate scheduling method often called newbob start given initial learning rate relative improvement less epoch multiply learning rate next epoch. standard optimization method often adam initial learning rate gradient clipping default. number layers several experiments ﬁgure optimal number layers. theory layers hurt practice often optimization problem becomes harder. could overcome clever initializations skip connections highway network like structures deep residual learning initial experiments also direction successful far. existing work direction also mostly deep ffnns deep rnns except trains deep highway blstms layers. results seen table experiment optimum somewhere layers. earlier experiments optimum layers. seems improve hyperparameters deeper optimal network becomes. pretraining section also included best value train dataset dataset table gives hint amount overﬁtting. observe similar results deeper networks theory overﬁt even probably harder optimization problem. also seems optimum slightly deeper optimum. indicates sequence discriminative training improve results. table comparison number layers layer size ﬁxed forward backward direction. dropout adam nchunks eval reported best epoch. note values necessarily epoch minimum epochs. also train accumulated training i.e. dropout applied. layer size experiments hidden layer size table compare different layer sizes. note number parameters increases quadratically. optimum experiment however model size much smaller much worse used size experiments. topology bidirectional unidirectional original experiment showed quite huge degradation unidirectional lstm networks compared blstms relative unidirectional bidirectional although tune unidirectional network much. groups conﬁrm bidirectional networks perform better unidirectional ones huge degradation research investigated bidirectional rnns/lstms continuous input sequence online recognition. showed possible recognition delay investigated effect different numbers chunks nchunks window time steps tstep window maximum size resulting overall batch size nchunks. experiment done initial learning rate. many experiments varying nchunks best results nchunks experiments performance difference quite notable better nchunks compared nchunks might better variance thus stable gradient minibatch. note higher nchunks usually also faster certain point work parallel every chunk. usually many experiments ﬁxed tstep often slight degradation might problem harder train longer backpropagation time maybe need tune learning rate parameters longer chunks. varying tstep make much difference except smaller tstep training time epoch naturally becomes longer data often. compare many optimization methods variations between hyperparameters esp. also different initial learning rates table compare stochastic gradient descent momentum variant depends last minibatch another variant depends full history nesterov momentum mean-normalized adadelta adagrad adam adamax adam without learning rate decay term nadam adam gradient noise adam mnsgd combined rmsprop rmsprop inspired method called smorms also tried adasecant converge experiments task. also test effect newbob. note layers smaller nchunks leads worse results compared sections. notable variant also several model copies update independently merge together averaging minibatch updates vary amount model copies much batches merge. similar multi-gpu training behavior described method yielded best result experiments postpone research. overall adam always good choice. standard comes close experiments converges slower. newbob also important. note newbob also hyperparameters tuning likely yield improvements. also investigated effect various different gradient clipping variants settled clipping total gradient parameters value stabilized training cases although possible clipping yields best performance many cases. regularization methods regularization tried dropout standard regularization. optimal dropout factor depends hidden size many aspects although mostly optimal dropout drop activations multiply enlarge hidden layer size higher dropout values although experiments dropout worse dropout investigated pretraining scheme ffnn start layer layer epoch right output layer pretrain epoch either train layer full network full network training usually better. results seen table deeper networks scheme seems help more. indicates initialization might room improvement. overall best result pretraining scheme applied layer bidirectional lstm note esp. deeper networks improvements pretraining increases. able train layer blstm without pretraining diverged broke epochs. also training calculation time ﬁrst epochs shorter. experiments done geforce tesla times slower standard deviation geforce times slower standard deviation present pure train epoch calculation times geforce counting test epoch preparation. collected total times table summed train epoch time reach speciﬁc epoch. show model best speciﬁc time. cases combinations different hyperparameters methods yield best results. time downsampling simple method reduce calculation time performance trade-off. switchboard- release corpus training hub’ evaluation data used testing. -gram language model trained transcripts acoustic training data transcripts fisher english corpora running words. details found results table show improvement lstms also associative lstm table results switchboard. blstm models trained nadam gradient noise dropout additionally associative lstm layer top. section also experiments babel javanese full language pack keywordsearch task details). baseline ffnn layers parameters yields ce-training mpe-training. layer blstm parameters yields ce-training work studied effect various lstm hyperparameters. demonstrated train deeper lstm acoustic models layers. important achievement introduction pretraining lstms allows depth especially relevant deeper networks. showed reproduce good results ﬁndings several different corpora yield good overall results beats best ffnn quaero relatively. given current experience think adam always good choice optimization learning rate scheduling like newbob important pretraining helps esp. deeper models. dropout together regularization works best high gradient noise often helps. first experiments associative lstms promising. partially supported intelligence advanced research projects activity department defense u.s. army research laboratory contract wnf--c-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa dod/arl u.s. government. has¸im andrew senior franc¸oise beaufays long shortterm memory based recurrent neural network architectures large vocabulary speech recognition arxiv preprint arxiv. j¨urgen geiger zixing zhang felix weninger bj¨orn schuller gerhard rigoll robust speech recognition using long short-term memory recurrent neural networks hybrid acoustic modelling interspeech patrick doetsch albert zeyer paul voigtlaender ilya kulikov ralf schl¨uter hermann returnn rwth extensible training framework universal recurrent neural networks arxiv preprint arxiv. submitted ieee international conference acoustics speech signal processing alex graves navdeep jaitly abdel-rahman mohamed hybrid speech recognition deep bidirectional lstm automatic speech recognition understanding ieee workshop ieee xiangang xihong constructing long short-term memory based deep recurrent neural networks large vocabulary speech recognition acoustics speech signal processing ieee international conference ieee xiangang xihong improving long short-term memory networks using maxout units large vocabulary speech recognition acoustics speech signal processing ieee international conference ieee tara sainath oriol vinyals andrew senior hasim convolutional long short-term memory fully connected deep neural networks acoustics speech signal processing ieee international conference ieee andrew senior hasim izhak shafran context dependent phone models lstm acoustic modelling acoustics speech signal processing ieee international conference ieee zhang guoguo chen dong kaisheng sanjeev khudanpur james glass highway long short-term memory rnns distant speech recognition arxiv preprint arxiv. junyoung chung aglar g¨ulc¸ehre kyunghyun yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling corr vol. abs/. rafal jozefowicz wojciech zaremba ilya sutskever empirical exploration recurrent network architectures proceedings international conference machine learning david rybach stefan hahn patrick lehnen david nolden martin sundermeyer zoltan t¨uske simon wiesler ralf schl¨uter hermann rasr rwth aachen university open source speech recognition toolkit ieee automatic speech recognition understanding workshop waikoloa dec. simon wiesler alexander richard pavel golik ralf schl¨uter hermann rasr/nn rwth neural network toolkit speech recognition ieee international conference acoustics speech signal processing florence italy fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio theano features speed improvements deep learning unsupervised feature learning nips workshop markus nußbaum-thom simon wiesler martin sundermeyer christian plahl stefan hahn ralf schl¨uter hermann rwth quaero evaluation system english german interspeech makuhari japan sept. ralf schl¨uter bezrukov hannes wagner hermann gammatone features feature combination large vocabulary speech recognition acoustics speech signal processing icassp ieee international conference ieee vol. ilya sutskever james martens george dahl geoffrey hinton importance initialization momentum deep learning proceedings international conference machine learning simon wiesler alexander richard ralf schl¨uter hermann mean-normalized stochastic gradient large-scale deep learning ieee international conference acoustics speech signal processing florence italy john duchi elad hazan yoram singer adaptive subgradient methods online learning stochastic optimization journal machine learning research vol. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov improving neural networks preventing co-adaptation feature detectors arxiv preprint arxiv. xavier glorot yoshua bengio understanding difﬁculty training deep feedforward neural networks international conference artiﬁcial intelligence statistics frank seide gang chen dong feature engineering context-dependent deep neural networks conversational speech transcription automatic speech recognition understanding ieee workshop ieee zolt´an t¨uske pavel golik ralf schl¨uter hermann speaker adaptive joint training gaussian mixture models bottleneck features ieee automatic speech recognition understanding workshop scottsdale dec. pavel golik zolt´an t¨uske ralf schl¨uter hermann multilingual features based keyword search low-resource languages interspeech dresden germany sept.", "year": 2016}