{"title": "A quantitative assessment of the effect of different algorithmic schemes  to the task of learning the structure of Bayesian Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "One of the most challenging tasks when adopting Bayesian Networks (BNs) is the one of learning their structure from data. This task is complicated by the huge search space of possible solutions and turned out to be a well-known NP-hard problem and, hence, approximations are required. However, to the best of our knowledge, a quantitative analysis of the performance and characteristics of the different heuristics to solve this problem has never been done before.  For this reason, in this work, we provide a detailed study of the different state-of-the-arts methods for structural learning on simulated data considering both BNs with discrete and continuous variables, and with different rates of noise in the data. In particular, we investigate the characteristics of different widespread scores proposed for the inference and the statistical pitfalls within them.", "text": "challenging tasks adopting bayesian networks learning structure data. task complicated huge search space possible solutions turned wellknown -hard problem hence approximations required. however best knowledge quantitative analysis performance characteristics diﬀerent heuristics solve problem never done before. reason work provide detailed study diﬀerent state-of-the-arts methods structural learning simulated data considering discrete continuous variables diﬀerent rates noise data. particular investigate characteristics diﬀerent widespread scores proposed inference statistical pitfalls within them. bayesian networks applied several diﬀerent ﬁelds ranging water resources management discovery gene regulatory networks task learning divided subtasks structural learning i.e. identiﬁcation topology parametric learning i.e. estimation numerical parameters given network topology. particular challenging task learning structure diﬀerent methods proposed face problem classiﬁed categories methods based detecting conditional independencies also known constraint-based methods score+search methods also known score-based approaches. discussed input former algorithms conditional independence relations subsets variables used build represents large percentage relations however number conditional independence tests methods perform exponential thus approximation techniques required. although constraint-based learning interesting approach close semantic developed structure learning algorithms fall score-based method category given possibility formulating task terms optimization problem. name implies methods major components scoring metric measures quality every candidate respect dataset search procedure intelligently move space possible networks space enormous. detail shown searching space optimal structure -hard problem even maximum number parents node constrained. hence regardless strategy learn structure wishes pursue greedy local search techniques heuristic search approaches need adopted tackle problem inferring structure bns. however best knowledge quantitative analysis performance diﬀerent techniques never done. reason provide detailed assessment performance diﬀerent state-of-thearts methods structural learning simulated data considering discrete continuous variables diﬀerent rates noise data. precisely investigate characteristics diﬀerent scores proposed inference statistical pitfalls within constraint-based score based techniques. furthermore study impact various heuristic search approaches namely hill climbing tabu search genetic algorithms. work structured follows. next sections provide background bayesian networks heuristic search techniques section describe results study section conclude paper. graphical models representing joint distribution random variables direct acyclic graph nodes represent variables arcs encode statistical dependence among them. variables toward given node parents. structure known possible compute joint distribution variables product conditional distributions variable given parents. however even consider simplest case binary variables number parameters conditional probability table still exponential size. example case binary variables total number parameters needed compute full joint distribution literature families methods learn structure data. idea behind ﬁrst group methods learning conditional independence relations which turn network learned. methods often referred constraint-based approaches. second group methods so-called score-based approaches formulates task structure learning optimization problem scores aimed maximizing likelihood data given model. however approaches known lead hard formulations this heuristic methods need used good solution. class methods aims building graph structure reﬂect dependence relations data match empirical distribution. notwithstanding number conditional independence tests algorithms would perform among pair nodes test possible relations exponential this introduction approximations required. algorithm. algorithm starts fully connected graph basis pairwise independence tests iteratively removes extraneous edges. avoid exhaustive search separating sets edges ordered consider correct ones early search. separating found search pair ends. algorithm orders separating sets increasing values size starting restrict subset variables test independence. thus knowledge available advance need test conditioning possible variables. widely used eﬃcient algorithm markov blanket discovery iamb which variable keeps track hypothesis goal given obtain algorithm markov blanket equal iamb consists forward backward phase. forward phase adds possible variables could backward phase removes false positive variables hypotheses leaving true forward phase begins empty then iteratively variables strong association non-negative functions mutual information. grows large enough include variables network little association conditioned point forward phase complete. backward phase starts contains false positives small conditional associations true positives associate strongly. using test backward phase able iteratively remove false positives true negatives eliminated. practically however arbitrary data likely graph always fully connected since adding edge increase likelihood data i.e. approach overﬁts data. overcome limitation likelihood score almost always combined regularization term penalizes complexity model favor sparser solutions already mentioned optimization problem leads intractability enormous search space valid solutions. this optimization task often solved heuristic techniques. moving describe main heuristic methods employed face complexity give short description particularly relevant known score called bayesian information criterion example scoring function adopted several score-based methods. fewer edges speciﬁcally fewer parents node. term essentially weighs regularization term. eﬀect higher weight sparsity favored explaining data maximum likelihood. notice likelihood implicitly weighted number data points since point contributes score. sample size increases weight regularization term weight likelihood increase. however weight likelihood increases faster regularization term. means that data likelihood contribute score trust observations less need regularization. statistically speaking consistent score consequently contains independence relations implied true structure. describe main state-of-the-art search strategies took account work. particular stated section considered following search methods hill climbing tabu search genetic algorithms. hill climbing simplest iterative techniques proposed solving optimization problems. consists simple intuitive sequence steps good search scheme used baseline comparing performance advanced optimization techniques. hill climbing shares techniques tabu search concept neighborhood. search methods based latter concept iterative procedures neighborhood deﬁned feasible solution next solution searched among clear aforementioned algorithm hill climbing returns solution local minimum problem hand. local minimum generally correspond global minimum problem exam hill climbing guarantee return best possible solution given problem. counteract limitation advanced neighborhood search methods deﬁned. methods tabu search optimization technique uses concept memory. tabu search meta-heuristic guides local heuristic search procedure explore solution space beyond local optimality. main components method adaptive memory creates ﬂexible search behavior. memory-based strategies therefore main feature approaches founded quest integrating principles alternative forms memory appropriately combined eﬀective strategies exploiting them. tabus distinctive elements compared hill climbing local search methods. main idea considering tabus prevent cycling moving away local optima nonimproving moves. situation occurs something needs done prevent search tracing back steps came from. achieved declaring tabu moves reverse eﬀect recent moves. instance consider problem solutions binary strings preﬁxed length neighborhood solution consists solutions obtained ﬂipping bits. scenario solution obtained solution changing possible declare tabu avoid back certain number iterations tabus also useful help moving search away previously visited portions search space thus perform extensive exploration. reported tabus stored short-term memory search usually ﬁxed limited quantity information recorded. possible store complete solutions negative impact computational time required check whether move tabu moreover requires vast amount space. second option involves recording last transformations performed obtain current solution prohibiting reverse transformations. tabus represent main distinguish feature feature particular introduce issues search process. tabus prohibit attractive moves lead overall stagnation search process moves allowed. hence several methods revoking tabu deﬁned commonly known aspiration criteria. simplest commonly used aspiration criterion consists allowing move results solution objective value better current best-known solution commonly adopted conditions algorithm number iterations larger maximum number allowed iterations changes best solution performed last iterations speciﬁcally experiments modeled possible valid solutions search space binary adjacency matrix describing acyclic directed graphs. starting point search empty graph search stopped current best solution cannot improved move neighborhood. algorithms consider possible solutions navigate among means moves insertion edge removal edge currently topology. also recall literature many alternatives proposed navigate search space learning structure bayesian networks purpose work preferred stick classical ones. genetic algorithms class computational models mimic process natural evolution. often viewed function optimizers although range problems applied quite broad. peculiarity potential solutions undergo evolution represented ﬁxed length strings characters numbers. generation generation stochastically transform sets candidate solutions hopefully improved populations solutions goal ﬁnding solution suitably solves problem hand. quality candidate solution expressed using user-deﬁned objective function called ﬁtness. search process shown figure transform population candidate solutions make particular operators transform candidate solutions called genetic operators crossover mutation. crossover traditionally used combine genetic material solutions swapping part individual part other. hand mutations introduce random changes structures solutions. order able solve given optimization problem candidate solutions must encoded individuals often also genetic operators must adapted. widely used learn structure considering search space dags. large majority works encodes connectivity matrix structure individuals. approach used study. follows common structure unless stated otherwise following description valid variants initialization method variation operators used ensure every individual valid i.e. individual guaranteed acyclic graph. initialization method creates individuals exactly connections variables total number variables. connection added graph remains acyclic. nodes connected selected uniform probability variables. continuous variant value associated connection randomly generated uniform probability. crossover operates parents returns oﬀspring. oﬀspring initially copy ﬁrst parent. afterward node least connection selected second parent valid connections starting node added oﬀspring. three mutation operators considered implementation connection mutation remove connection mutation perturbation mutation. ﬁrst applied variants perturbation mutation applied continuous variant. oﬀspring resulting connection mutation operator diﬀers parent additional connection. newly connected nodes selected uniform probability. continuous variant value associated connection randomly generated uniform probability. similarly oﬀspring resulting remove connection mutation operator diﬀers parent less connection. nodes disconnected selected uniform probability. perturbation mutation operator applies gaussian perturbations values associated connections. total number perturbations less individual mutated less connections. value perturbed following gaussian distribution mean standard deviation resulting value bounded respectively. regardless variant mutation applied probability mutation applied speciﬁc mutation operator selected uniform probability available options terms parameters population size used evolutionary process able perform generations. parents selected applying tournament size survivor selection elitist sense ensures best individual survives next generation. generated data case discrete continuous random variables. them randomly generated structure related parameters entry conditional probability tables attached network structure) build simulated bns. also considered levels density networks namely complete graph. scenarios randomly sampled diﬀerent datasets sample sizes parametrized based number nodes. speciﬁcally networks variables generated datasets samples variables considered datasets samples. furthermore also considered noise samples random entries dataset. extent considered noise free dataset dataset error rate total total number random datasets. them considered constraint-based scorebased approaches structural learning. former category methods considered algorithm iamb recall methods return partially directed graph leaving undirected arcs unequivocally directable. order fair comparison score-based method returns dags randomly resolved ambiguities generating random solutions consistent statistical constraints iamb moreover among score-based approaches consider maximum likelihood scores namely log-likelihood them repeated inference conﬁguration using search strategies. diﬀerent methods compared total independent experiments. evaluate obtained results considered false positives false negatives also arcs generative model arcs generative model section comment results simulations. anticipated computed precision recall speciﬁcity well accuracy hamming distance assess performances underﬁt/overﬁt trade-oﬀ diﬀerent approaches. overall obtained results straightforward notice methods including edges inferred networks also subject errors terms accuracy also resemble bias metric tends penalize solutions false positive edges rather false negatives. hand since typical goal problems involving inference identiﬁcation novel relations underﬁtting approaches could eﬀective terms accuracy less useful practice. careful look ﬁrst evidence obtained simulations parameters highest impact inferred networks density number nodes reason ﬁrst focused attention parameters analyzed diﬀerent tested combinations methods scores behave. shown figure approaches seem perform better dealing low-density networks fact almost methods accuracy higher density equal lower density equal since edge network parameter learned reasonable think that edges present harder becomes problem solved learning moreover also observe results terms accuracy obtained discrete variables slightly better achieved continuous ones. extent outlier loglik score combined hill climbing tabu search datasets continuous variables trend opposite w.r.t. approaches. fact cases accuracy higher high-density datasets likely addition accuracy also computed hamming distance reconstructed solutions used generate data order quantify errors inference process. analysis showed that besides network density also number nodes inﬂuences results reported figure interesting observe that adopted experimental setup number samples proportional number nodes network nodes conﬁgurations lower number samples compared ones nodes. statistical point view alone would expect problem easier samples build since would lead statistical power intuitively compensate fact nodes parameters learn ones case nodes. case also observe constantly higher hamming distance nodes. fact dealing variables observed shift performance even density observe errors manifesting higher figure plots showing hamming distances solutions obtained tested approaches grouped type dataset number nodes network colors represent densities analysis performed devoted assessing impact overﬁt underﬁt. possible observe plots figure figure obtain opposite behaviors namely combinations scores search strategies show evident diﬀerent trends terms sensitivity speciﬁcity precision. details iamb tend underﬁt since produce networks consistently density. achieve similar results terms accuracy trend toward underﬁt make suited identiﬁcation novel edges rather indicated descriptive purposes. hand loglik score independently adopted search technique consistently overﬁts. behaviors observed violin plots figure figure density observe iamb/pc results recall values distribution results loglik score centered higher values. overﬁt underﬁt less aﬀected overﬁt reconstructing slightly denser networks. again trade-oﬀ regularizators well-known literature points suited predictions descriptive purposes. must noticed three regularizators genetic algorithms achieve similar results precision speciﬁcity compared hill climbing tabu search terms sensitivity presents reduced overﬁt. fact possible observe plots figure figure score sensitivity results genetic algorithms lower hill climbing tabu search highlighting reduced impact overﬁt. test support claims table claim greater less values i.e. accuracy performed one-tail test alternatives. tests performed conﬁgurations simulations. bayesian networks widespread technique model dependencies among random variables. challenging problem dealing learning structures i.e. statistical dependencies data sometime pose serious limit reliability results. despite extensive vast ﬁelds best knowledge quantitative assessment performance diﬀerent stateof-the-art methods learn structure never performed. work going direction ﬁlling presented study diﬀerent state-of-the-art approaches structural learning table summary major ﬁndings. results mannwhitney test support results simulations shown. tests performed settings comparisons described ﬁrst column table. also show considered metric adopted alternative one-tail extent investigated characteristics diﬀerent likelihood score combined commonly used search strategies schemes well constraintbased techniques inference analysis showed factors leading higher eﬀects performance i.e. density number variables variable type expected settings eﬀect number parameters learned hence complicating optimization task. furthermore also discussed overit/underﬁt trade-oﬀ diﬀerent tested techniques constraint-based approaches showing trends toward underﬁtting loglik score showing high overﬁt. interestingly conﬁgurations genetic algorithms showed evidence reducing overﬁt leading denser structures. overall place work starting eﬀort better characterize task learning structure bayesian networks data lead future eﬀective application approach.", "year": 2017}