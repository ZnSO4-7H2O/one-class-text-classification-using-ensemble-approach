{"title": "Fast Differentially Private Matrix Factorization", "tag": ["cs.LG", "cs.AI", "G.2; I.2.6; G.3; G.1.6"], "abstract": "Differentially private collaborative filtering is a challenging task, both in terms of accuracy and speed. We present a simple algorithm that is provably differentially private, while offering good performance, using a novel connection of differential privacy to Bayesian posterior sampling via Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm lends itself to efficient implementation. By careful systems design and by exploiting the power law behavior of the data to maximize CPU cache bandwidth we are able to generate 1024 dimensional models at a rate of 8.5 million recommendations per second on a single PC.", "text": "diﬀerential privacy oﬀers tools overcome problems. loosely speaking oﬀers participants plausible deniability terms estimate. provides guarantees recommendation would also issued suﬃciently high probability another speciﬁc participant taken action before. precisely type guarantee suitable allay concerns situation recent work e.g. mcsherry mironov focused designing custom built tools diﬀerential private recommendation. many design decisions context hand engineered nontrivial separate choices made obtain diﬀerentially private system made obtain system works well. furthermore none systems lead fast implementations. paper show large family recommender systems namely using matrix factorization well suited diﬀerential privacy. speciﬁcally exploit fact sampling posterior distribution bayesian model e.g. stochastic gradient langevin dynamics lead estimates suﬃciently diﬀerentially private time stochastic nature makes well amenable eﬃcient implementation. generality means need custom-design statistical model diﬀerential privacy rather possible retroﬁt existing model satisfy constraints. practical importance fact canoverstated means costly re-engineering deployed statistical models needed. instead simply reuse existing inference algorithm trivial modiﬁcation obtain diﬀerentially private model. leaves issue performance. best reported results using graphchi show state-of-the-art recommender systems built using single within matter hours rather requiring hundreds computers. paper show eﬃciently exploiting power properties inherent data obtain models achieve peak numerical performance recommendation. point times faster graphchi identical hardware. diﬀerentially private collaborative ﬁltering challenging task terms accuracy speed. present simple algorithm provably diﬀerentially private oﬀering good performance using novel connection diﬀerential privacy bayesian posterior sampling stochastic gradient langevin dynamics. simplicity algorithm lends eﬃcient implementation. careful systems design exploiting power behavior data maximize cache bandwidth able generate dimensional models rate million recommendations second single privacy protection recommender systems notoriously challenging problem. often competing goals stake similar users likely prefer similar products movies locations hence sharing preferences users desirable. time exacerbates type privacy sensitive queries simply since looking aggregate properties dataset properties behavior users ‘just like’ speciﬁc user. highly individualized behavioral patterns shown facilitate provably eﬀective user de-anonymization consider case couple using location recommendation service. since spouses share much location history likely receive similar recommendations based users’ preferences similar theirs. context sharing information desirable improves overall recommendation quality. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. copyright x-xxxxx-xx-x/xx/xx ..... experiments conﬁrm algorithm implemented high eﬃciency oﬀering favorable privacyaccuracy tradeoﬀ nearly matches systems without differential privacy meaningful privacy level. begin overview relevant ingredients namely collaborative ﬁltering using matrix factorization differential privacy primer computer architecture. three relevant understanding approach. particular basic understanding cache hierarchy microprocessors useful eﬃcient implementations. users rating items. observe small number entries rating matrix means user rated item popular tool deal inferring entries r|u|×|v| approximate rank popular interpretation given item elements measure extent item possesses attributes. given user elements measure extent interest user items score highly corresponding factors. conditions proposed netﬂix contest common minimize mean squared error deviations true ratings estimates. address overﬁtting norm penalty commonly imposed yields following optimization problem large number extensions proposed model. instance incorporating co-rating information neighborhoods temporal dynamics lead improved performance. since primarily interested demonstrating eﬃcacy diﬀerential privacy interaction eﬃcient systems design focus simple inner-product model bias. bayesian view. note optimization problem viewed instance maximum-a-posteriori estimation problem. minimizes penalized risk minimization explicitly designing diﬀerentially private algorithms. rationale deep connection samples posterior diﬀerentially private estimates. return aspect introducing stochastic gradient langevin dynamics. stochastic gradient descent. minimizing regularized collaborative ﬁltering objective typically achieved strategies alternating least squares stochastic gradient descent advantage former problem biconvex respechand typically faster converge also aﬀords much better cache locality properties. instead accessing e.g. reviews given user once need read appropriate tuples. time update randomly chosen rating record problem trivially parallelizing procedure requires memory locking synchronization rating could signiﬁcantly hamper performance. shows lock-free scheme achieve nearly optimal solution data access sparse. build statistical property obtain fast system suitable diﬀerential privacy. cryptographically protect personal information database allowing aggregate-level information accurately extracted. context means protect userspeciﬁc sensitive information using aggregate information beneﬁt users. deﬁnition states arbitrarily replace individual data point database output algorithm doesn’t change much. parameter deﬁnition controls maximum amount information gain individual person database given output algorithm. small prevents forms linkage attack individual data record refer readers detailed interpretations diﬀerential privacy statistical testing bayesian inference information theory. interesting side-eﬀect deﬁnition context collaborative ﬁltering also limits inﬂuence so-called whales i.e. users submit extremely large numbers reviews. inﬂuence also curtailed least assumption equal level diﬀerential privacy user. words diﬀerential privacy confers robustness collaborative ﬁltering. wang show posterior sampling bounded log-likelihood essentially exponential mechanism therefore protecting diﬀerential privacy free wang also suggests recent line works stochastic gradient descent hybrid monte carlo sampling essentially preserve diﬀerential privacy algorithmic procedure. consequence application interesting trust mcmc sampler converged i.e. sample approximately drawn posterior distribution sample private release. calibrate mcmc procedure provide diﬀerential privacy diﬀerence generic numerical linear algebra commonly used e.g. deep networks generalized linear models methods used recommender systems fact access properties regarding users items highly nonuniform. signiﬁcant advantage since allows exploit caching hierarchy modern cpus beneﬁt higher bandwidth disks main memory access would permit. typical computer architecture consists hard disk solid-state drive random-access memory cache. many factors aﬀect real available bandwidth read write patterns block sizes etc. measured desktop computer. table quick overview. good algorithm design pushing data cache level hide latency even amplify available bandwidth. subsystems eﬃcient caching. movie frequently reused desirable retain cache. neither suﬀer high latency random read memory comparably slower bandwidth relative cache. intuition conﬁrmed observed cache miss rates reported experiments section start describing ideas algorithmic framework diﬀerentially private matrix factorization. method involves preprocessing data sampling scaled posterior distribution provably differentially private profound statistical implications. describe speciﬁc monte carlo sampling algorithm stochastic gradient langevin dynamics justify setting. come novel personalize privacy protection individual users. finally discuss develop fast cacheeﬃcient solvers exploit bandwidth-limited hardware used general sgd-style algorithms. diﬀerential privacy mechanism relies recent observation posterior sampling preserves diﬀerential privacy provided log-likelihood user uniformly bounded simple remarkable result suggests sampling posterior distribution diﬀerentially private free extent. context claim that maxuvri method outputs sample trimming randomly delete ratings rated movies maximum number ratings single user much larger average number ratings. procedure underlying allows optspace work. reweighting alternatively weight user appropriately rated many movies smaller weight rating. mcsherry mironov used reweighting scheme controlling privacy loss. similar approach considered study non-uniform power-law matrix completion weighted trace norm eﬀect reweight loss-functions. addition procedures practical beneﬁts robustness recommendation system since prevent malicious user injecting much impact system e.g. wang mobasher another justiﬁcation procedures that fully observed matrix truly low-dimensional subspace neither procedures changes underlying subspace. therefore solutions similar non-preprocessed version. procedure diﬀerentially private matrix factorization summarized algorithm note conceptual sketch following theorem guarantees procedure indeed diﬀerentially private. complex models. strength approach namely large variety algorithms adapted quite easily diﬀerential privacy capable models. statistical properties. utility procedure? argue lose much accuracy sampling distribution instead exact optimization. deﬁne utility/accuracy well output predicts data. matrix factorization formulation treated maximum posteriori estimator bayesian probabilistic matrix factorization therefore distribution sampling actually scaledversion posterior distribution. wang shows single sample posterior distribution consistent whenever bayesian model gives rise consistent asymptotically factor away matching cram´er-rao lower bound whenever asymptotic normality posterior distribution holds. therefore argue taking sample posterior distribution results much worse estimating posterior mean estimator bpmf. moreover since results collapse point estimator output sampling procedure tend overﬁt start lose accuracy since still sampling scaled posterior distribution statistical property applies result remains asymptotically near optimal asymptotic relative eﬃanother interesting feature proposed procedure allows calibrate level privacy protection every user independently novel observation weights assigned diﬀerent users linear amount privacy guarantee particular user. sampling algorithm guarantees theorem still hold. idea customize system lower basic privacy protection users explained earlier level privacy less free. protection suﬃciently strong include even users database. adjusting weight parameter make privacy protection stronger particular users according much want privacy. procedure makes intuitive sense user wants perfect privacy weight eﬀectively database anymore. people care privacy ratings assigned default weight. formally deﬁne personalized diﬀerential privacy follows summary simply method protects b-diﬀerential privacy everybody little cost setting weight vector provide personalized service users demands stringent protection. best knowledge ﬁrst method kind protect diﬀerential privacy personalized fashion. tractable approach recent mcmc method named stochastic gradient langevin dynamics annealing stochastic gradient descent langevin dynamics samples posterior distribution basic update rule small number ratings. words updates almost identical used stochastic gradient descent. diﬀerence small amount gaussian noise added updates. allows solve extremely eﬃciently. describe eﬃcient implementation algorithm section nian motion. moreover gets small probability accepting proposal metropolis-hastings adjustment converges need adjustment algorithm proceeds designed above. seemingly heuristic procedure later shown consistent asymptotic in-law almost sure convergence sgld correct stationary distribution established. recently strengthens convergence guarantee include ﬁnite iterations. line work justiﬁes approach sgld large number iterations sampling distribution provides diﬀerential privacy. taking iterations make arbitrarily small. performance improvement existing libraries graphchi cache eﬃcient design prefetching pipelining fact exploit power property data judicious optimization random number generation. leads system comfortably surpasses even moderately optimized codes. primarily focus stochastic gradient descent solver subsequently provide details extend sgld. inference requires large number following operations data illustrate impact operations consider training dimensional model rating triples netﬂix. iteration requires read/write operations ram. main memory bandwidth gb/s latency million cache misses pass would take minutes. instead code accomplishes task approximately seconds using steps outlined below. deal dataﬂow disk pipelined design decomposing global local state akin means process users sequentially thus reducing retrieval cost user since operations amortized ratings. eﬀectively halves moreover since data cannot assumed pipeline reads disk. hides latency avoids stalling cpus. writer thread periodically snapshots model i.e. disk. note personalized recommender systems require considerable personalized hidden state topic models autoregressive processes want write snapshot user-speciﬁc data too. table lookup drawing samples gaussian quite costly easily dominating ﬂoating point operations combined. address pre-generating large table numbers performing random lookup within table. point lookup table random numbers statistically indistinguishable truth draw samples hence data suﬃce. finally cache eﬃciency read contiguous segments random oﬀset cache eﬃciency regard movies. point need exploit cache locality relative core rather simply avoiding cache misses. basic idea core exactly reads cache line time algorithm designers waste piece cache line fully utilized. exploit fact movie ratings follow power evident e.g. netﬂix figure means succeed keeping frequently rated movies cache substantial speedups. note traditional matrix blocking tricks widely used matrix multiplications operations useful sparsity rating matrix instead decompose movies tiers popularity. illustrate considering decomposition three blocks consisting next remaining long tail. within block process batch users simultaneously. preserve associated user vectors cache likely cache movie vectors also parallelizing updates multiple users require locks. movie parameters updated hogwild fashion design particularly eﬃcient low-dimensional models since block cache next ratings typically reside even extreme case dimensions ratings cache albeit cache. avoid penalty random requests perform latency hiding prefetching. actively request advance rating updated. dimensions less accurate prefetching leads dataﬂow cache. beyond that size latent variables could beneﬁt lowest level caching limited size caches modern computers. provide detailed caching analysis section illustrate eﬀect techniques. denote number rating data rated rated user respectively. parameters incur major cost diagonal matrices gamma distribution them. simply perform gibbs sampling round. however time-consuming part sample remaining vectors i.e. spark distributed system inferring recommendations factorization. recent comparison argument made someslower graphlab substantially faster mahout. consisting training ratings spanning customers ratings almost movie being rated scale stars. additionally released validation consists ratings validation purposes. secondly yahoo music recommender dataset consisting almost ratings music items users. also released validation consists ratings validation. re-scale rating scale compare performance datasets since sampling strategies somewhat incomparable moreover larger dataset poses challenges cache eﬃciency larger number items recommended. investigate eﬃciency accuracy fast solver stochastic gradient langevin dynamics solver compared state-of-the-art available recommenders. also explore diﬀerentially private accuracy using proposed method varying diﬀerent privacy budgets. compare performance solver sgld solver publicly available recommenders closed-source solver. particular compare solvers since latter tend excel massively parallel ﬂoating point operations. graphlab create closed source data analysis platform currently fastest recommender system available slightly faster graphchi. compared system graphlab create albeit without ﬁne-grained diagnostics possible graphchi. figure runtime comparisons c-sgd solver diﬀerentially private sgld solver non-private graphchi/graphlab identical hardware amazon c.xlarge instance. note regardless dimensionality factors c-sgd approximately times faster graphchi diﬀerentially private sgld also comparable graphchi high dimension eﬃcient computation graphchi ﬁrst needs preprocess data shards proposed parallel sliding windows data partitioned process graphs eﬃciently. comparison partition rating matrix netﬂix prize data yahoo music data blocks block contains ratings come around users. time algorithms read block disk. graphchi graphlab create default partition strategy. experiments amazon c.xlarge instance running ubuntu cpus ram. since nontrivial observe test rmse error epoch using graphlab create report timing graphlab create methods figure note unable obtain performance results bidmach yahoo dataset since scala encountered memory management issues. however reason believe results would favorable bidmach ﬁndings netﬂix dataset. reproducibility results carried illustrate convergence time. methods ﬁxed number epochs. epochs epochs respectively observe solver reach convergence time. figure shows timing results along convergence vary dimensions models. solvers i.e. c-sgd fast sgld beneﬁt caching algorithm. c-sgd around times faster graphchi graphlab simultaneously outperforming accuracy graphchi. primary reason discrepancy performance found order graphchi processes data partitions data random subsets optimizes subblock time. latter fast negatively aﬀects convergence seen figure note algorithm required fast sgld rather complex since performs sampling bayesian posterior. consequently slower plain sgd. nonetheless speed comparable graphchi terms throughput problem sgld complex models worse convergence becomes fact sampling large state space. possibly slow mixing sgld known problem sgld improving mixing rate considering advanced stochastic diﬀerential equation based sampler e.g. keeping cache eﬃciency updates important future work. best knowledge ﬁrst report convergence results sgld scale. described above convergence sgld based methods quite diﬀerent. illustrate convergence small dimension figure basically c-sgd estimate using several rounds begin overﬁtting. sgld ﬁrst needs burn-in start sampling procedure. note sgld converge fast case. higher dimensions sgld slower converge. careful tuning learning rate critical here. also investigated accuracy model function size gaussian lookup table. checked whether replacing explicit access samples normal distribution looking consecutive number precomputed parameters memory valid. seen figure smallest sets suﬃces. already numbers longer need gaussian random number generator results obtained essentially indistinguishable show cache eﬃciency c-sgd graphchi section. data access pattern accelerate hardward cache prefetching. meanwhile also software prefetching strategies prefetch movie factors advance. software prefetching usually dangerous practice implementing practice need know prefetching stride advance. prefetch movie factors. experiments prefetching stride empirically. experiments follows. gradient update step given parameters e.g. read stay cache ﬂushed away parameters. really care section ﬁrst time parameter read already staying cache not. cache cache miss push idle. succeeding updates weight user according min. moreover users ratings actually quite good approximation proﬁles using reasonable size random samples ratings. dataset ratings netﬂix ratings yahoo music data. study prediction accuracy i.e. utility private method varying diﬀerential privacy budget ﬁxed model dimensionality speciﬁcally model two-stage procedure ﬁrst takes diﬀerentially private item vectors latter obtain locally non-private user parameter estimates. perfectly admissible since users expectation privacy regard ratings. interpreting privacy guarantees subtle. privacy loss figure seem completely meaningless deﬁnition corresponding results mcsherry mironov appear much better. ﬁrst address comparison mcsherry mironov important point privacy loss stated terms user level privacy results mcsherry mironov stated terms rating level privacy oﬀers exponentially weaker protection. \u0001-user diﬀerential privacy translates -rating diﬀerential privacy. since case results suggest almost lose accuracy preserving rating differential privacy matches mcsherry mironov carefully engineered system. hand note plain privacy loss deceiving measure practical level protection. deﬁnition protects privacy arbitrary user malicious spammer rates every movie completely opposite fashion learned model would predict. truly paranoid requirement arguably right since probably table cache miss rates c-sgd graphchi. results obtained using cachegrind. cache miss rate graphchi considerably higher explains extent speed diﬀerence. cachegrind cache proﬁler analyze cache miss purpose. result table shows algorithm quite cache friendly compared graphchi dimensions. likely graphchi ingests data traverses data item block time. result less eﬃcient portfolio access frequency needs fetch data memory frequently. believe root cause decreased computational eﬃciency slower convergence code. investigate inﬂuence privacy loss accuracy. discussed previously small rescaling factor help nice bound loss function. private collaborative ﬁltering purposes ﬁrst trim training data setting user’s maximum allowable number ratings netﬂix competition dataset yahoo music data respectively. implementation approximately times fast graphchi next-fastest recommender system. strong endorsement stochastic gradient langevin dynamics obtain diﬀerentially private estimates recommender systems still preserving good utility. acknowledgments parts work supported grant adobe research. supported creative program ministry education foundation innovative research groups nnsf china china pillar program y.-x. wang supported award bcs- statistics singapore national research foundation international research centre singapore funding initiative administered programme oﬃce. lastly note retry fail procedure always sample correct distribution conditioned satisfying constraint bounded aﬀect relative probability ratio measurable event support conditional distribution. protect malicious users begin with. average user personalized privacy guarantee much stronger posterior distribution concentrates around models predict reasonably well users. result log-likelihood associated users bounded much smaller number high probability. example shown figure typical user’s personal privacy loss helps reduce essential privacy loss meaningful range. paper described algorithm eﬃcient collaborative ﬁltering compatible diﬀerential privacy. particular showed possible accomplish three goals accuracy speed privacy without signiﬁcant sacriﬁce either end. moreover introduced notion personalized diﬀerential privacy. deﬁned notion obtaining estimates respect diﬀerent degrees privacy required individual users. believe notion highly relevant today’s information economy expectation privacy tempered e.g. cost service quality hardware extent want incorporate opinions users. algorithm denote sampling equivalent distribution proportional taking posterior log-likelihood user bounded therefore algorithm obeys personalized diﬀerential privacy user take customized subset adjustied using", "year": 2015}