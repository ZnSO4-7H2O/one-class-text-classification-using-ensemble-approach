{"title": "Structured Embedding Models for Grouped Data", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S-EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.", "text": "word embeddings powerful approach analyzing language exponential family embeddings extend types data. develop structured exponential family embeddings method discovering embeddings vary across related groups data. study word usage u.s. congressional speeches varies across states party afﬁliation words used differently across sections arxiv copurchase patterns groceries vary across seasons. success method groups share statistical information. develop sharing strategies hierarchical modeling amortization. demonstrate beneﬁts approach empirical studies speeches abstracts shopping baskets. show s-efe enables group-speciﬁc interpretation word usage outperforms predicting held-out data. word embeddings unsupervised learning methods capturing latent semantic structure language. word embedding methods analyze text data learn distributed representations vocabulary capture co-occurrence statistics. representations useful reasoning word usage meaning word embeddings also extended data beyond text items grocery store neurons brain. exponential family embeddings probabilistic perspective embeddings encompasses many existing methods opens door bringing expressive probabilistic modeling problem learning distributed representations. develop structured exponential family embeddings extension studying embeddings vary across groups related data. study several examples u.s. congressional speeches word usage vary across states party afﬁliations; scientiﬁc literature usage patterns technical terms vary across ﬁelds; supermarket shopping data co-purchase patterns items vary across seasons year. s-efe discovers per-group embedding representation objects. naïve approach ﬁtting individual embedding model group would typically suffer lack data—especially groups fewer observations available—we develop methods share information across groups. figure illustrates kind variation capture. s-efe arxiv abstracts grouped different sections computer science quantitative ﬁnance nonlinear sciences s-efe results per-section embedding term vocabulary. using ﬁtted embeddings illustrate similar words word intelligence. intelligence used varies ﬁeld computer science similar words include artificial ﬁnance similar words include abilities consciousness. s-efe uncovers variations usage word intelligence. figure intelligence used differently across arxiv sections. words closest embedding query listed sections. method automatically orders sections along horizontal axis similarity usage intelligence. section additional details. graphical representation s-efe data categories. embedding vectors speciﬁc group context vectors shared across categories. detail embedding methods posit representation vectors term vocabulary; embedding vector context vector. idea conditional probability observed word depends interaction embedding vector context vectors surrounding words. s-efe posit separate embedding vectors group shared context vectors; ensures embedding vectors space. propose methods share statistical strength among embedding vectors. ﬁrst approach based hierarchical modeling assumes groupspeciﬁc embedding representations tied global embedding. second approach based amortization considers individual embeddings output deterministic function global embedding representation. stochastic optimization large data sets. work relates closely threads research embedding literature. embedding methods study language evolves time time thought type group though evolutionary structure consider. second thread multilingual embeddings approach different words appear groups interested variations embeddings across groups. contributions thus follows. introduce s-efe model extending grouped data. present techniques share statistical strength among embedding vectors based hierarchical modeling based amortization. carry thorough experimental study text databases arxiv papers section u.s. congressional speeches home state political party. using poisson embeddings study market basket data large grocery store grouped season. three data sets s-efe outperforms terms held-out log-likelihood. qualitatively demonstrate s-efe discovers words used differently across u.s. states political parties show word usage changes different arxiv disciplines. section develop structured exponential family embeddings model builds exponential family embeddings capture semantic variations across groups data. embedding models represent object using sets vectors embedding vector context vector. paper interested embeddings vary across groups data object want learn separate embedding vector group. separate embedding group allows study usage word like intelligence varies across categories arxiv words used differently u.s. senators depending state whether democrats republicans. s-efe model extends grouped data embedding vectors speciﬁc group sharing context vectors across groups. review model section formalize idea sharing context vectors section present approaches build hierarchical structure group-speciﬁc embeddings. exponential family embeddings collection objects goal learn vector representation objects based co-occurrence patterns. consider dataset represented matrix columns datapoints rows objects. example text column corresponds location text entry binary variable indicates whether word appears location represent object sets vectors embeddings vectors context vectors posit probability distribution data entries vectors interact. deﬁnition model requires three ingredients context conditional exponential family parameter sharing structure. next describe three components. exponential family embeddings learn vector representation objects based conditional probability observation conditioned observations context. context gives indices observations appear conditional probability distribution xvi. deﬁnition context varies across applications. text corresponds words ﬁxed-size window centered location given context corresponding observations xcvi indexed distribution exponential family sufﬁcient statistics natural parameter parameter vectors interact conditional probability distributions observation follows. embedding vectors context vectors combined form natural parameter link function. exponential family embeddings understood bank generalized linear models context vectors combined give covariates regression coefﬁcients embedding vectors. link function plays role glms modeling choice. identity link function. third ingredient model parameter sharing structure indicates embedding vectors shared across observations. standard model columns unique object shared representation across instances. objective function. maximize objective function given log-conditional likelihoods addition -regularization term embedding context vectors yielding note maximizing regularized conditional likelihood equivalent maximum posteriori. rather similar maximization pseudo-likelihood conditionally speciﬁed models here describe s-efe model grouped data. text examples grouped data congressional speeches grouped political parties scientiﬁc documents grouped discipline. goal learn group-speciﬁc embeddings data partitioned groups i.e. instance associated group s-efe model extends learn separate embedding vectors group. build s-efe model impose particular parameter sharing structure embedding context vectors. posit structured model context vectors shared across groups i.e. embedding vectors shared group level i.e. observation belonging group here denotes embedding vector corresponding group show graphical representation s-efe figure sharing context vectors advantages. first shared structure reduces number parameters resulting s-efe model still ﬂexible capture differently words used across different groups allowed vary. second important effect uniting embedding parameters space group-speciﬁc vectors need agree components could learn separate embedding model group done text grouped time slices approach would require ad-hoc postprocessing steps align embeddings. groups s-efe model times many embedding vectors standard embedding model. complicate inferences group-speciﬁc vectors especially groups less data. additionally object appear frequency particular group. thus naïve approach building s-efe model without additional structure detrimental quality embeddings especially small-sized groups. address problem propose different methods individual together sharing statistical strength among them. ﬁrst approach consists hierarchical embedding structure. second approach based amortization. methods introduce global embedding vectors impose particular structure generate hierarchical embedding structure. here impose hierarchical structure allows sharing statistical strength among per-group variables. that assume -regularization term applies global vectors fitting hierarchical model involves maximizing respect note reduced number parameters inferred; rather together common prior distribution. stochastic gradient ascent maximize amortization. idea amortization applied literature develop amortized inference algorithms main insight behind amortization reuse inferences past experiences presented task leveraging accumulated knowledge quickly solve problem. here amortization control number parameters s-efe model. particular per-group embeddings output deterministic function global embedding vectors different function group parameterize using neural networks similarly works amortized inference unlike standard uses amortized inference s-efe another potential advantage proposed parameter sharing structure that context vectors held ﬁxed resulting objective function convex convexity properties exponential families input functions unobserved must estimated together parameters functions depending architecture neural networks amortization signiﬁcantly reduce number parameters model still ﬂexibility model different embedding vectors group. number parameters s-efe model number groups dimensionality embedding vectors number objects amortization reduce number parameters number parameters neural network. since typically corresponds signiﬁcant reduction number parameters even scales linearly amortized s-efe model need introduce parameters group corresponding neural network parameters. given these group-speciﬁc embedding vectors compare architectures function fully connected feed-forward neural networks residual networks both consider hidden layer units. hence network parameters weight matrices i.e. parameters. neural network takes input global embedding vector outputs group-speciﬁc embedding vectors feed-forward neural network residual network respectively given considered hyperbolic tangent nonlinearity. main difference network architectures residual network focuses modeling group-speciﬁc embedding vectors weights feed-forward network would output residual network would output global vector groups. objective function amortization given maximize objective respect using stochastic gradient ascent. implement hierarchical amortized s-efe models tensorflow allows leverage automatic differentiation. example structured bernoulli embeddings grouped text data. here consider documents broken groups political afﬁliations scientiﬁc disciplines. represent data binary matrix group indicators since word appear certain position matrix contains non-zero element column. embedding models ignore one-hot constraint computational efﬁciency consider observations generated following conditional bernoulli distributions given entries zero embedding models typically downweigh contribution zeros objective function. mikolov negative sampling consists randomly choosing subset zero observations. corresponds biased estimate gradient bernoulli exponential family embedding model context given position surrounding words document according ﬁxed-size window. example structured poisson embeddings grouped shopping data. s-efe extend applications beyond text s-efe model supermarket purchases broken month. market basket access month shopping trip happened. rows data matrix index items columns index shopping trips. element denotes number units item purchased trip unlike text column contain non-zero element. context corresponds items purchased trip excluding case poisson conditional distribution appropriate count data. poisson s-efe also downweigh contribution zeros objective function provides better results allows inference focus positive signal actual purchases section describe experimental study. s-efe model three datasets compare quantitative results show sharing context vectors provides better results amortization hierarchical structure give improvements. data. apply s-efe three datasets arxiv papers u.s. senate speeches purchases supermarket grocery shopping data. describe datasets below provide summary datasets table arxiv papers dataset contains abstracts papers published arxiv different tags april june treat group s-efe goal uncovering words strongest shift usage. split abstracts training validation test sets proportions respectively. senate speeches dataset contains u.s. senate speeches contrast arxiv collection transcript spoken language. group data state origin speaker party afﬁliation. afﬁliations republican democratic party considered. result groups state/party combinations available data states senators party afﬁliation. split speeches training validation testing grocery shopping data dataset contains purchases customers. data covers period weeks. removing low-frequency items data contains unique items level. split data training test validation sets proportions respectively. training data contains shopping trips purchases total. text corpora vocabulary frequent terms remove words vocabulary. following mikolov additionally remove word frequent words speeds training. models. goal s-efe model datasets. text data bernoulli distribution conditional exponential family shopping data poisson distribution appropriate count data. global model cannot capture group structure. separate models ﬁtted independently group. s-efe without hierarchical structure amortization. s-efe hierarchical group structure. s-efe amortized feed-forward neural network s-efe amortized using residual network experimental setup hyperparameters. text dimension embeddings number hidden units experiment context sizes shopping data randomly truncate context baskets larger reduce size methods negative samples. methods subsample minibatches data manner. minibatch contains subsampled observations groups group subsampled proportionally size. text words subsampled within group consecutive shopping data observations sampled shopping trip level. sampling scheme reduces bias imbalanced group sizes. text minibatch size size corpus passes data; shopping data passes. default learning rate setting tensorflow adam standard initialization schemes neural network parameters. weights embeddings initialization schemes choose best based validation error. particular schemes embeddings drawn gaussian prior implied regularizer; embeddings initialized global embedding; context vectors initialized global embedding held constant embeddings vectors drawn randomly prior. finally method choose regularization variance also based validation error. runtime. implemented methods tensorﬂow. senate speeches runtime s-efe times slower runtime global hierarchical times slower runtime global amortized s-efe times slower runtime global efe. evaluation metric. evaluate held-out pseudo likelihood. model compute test pseudo log-likelihood according exponential family distribution used test entry better model assign higher probability observed word item lower probability negative samples. fair metric competing methods produce conditional likelihoods exponential family. make results comparable train evaluate methods number negative samples reported held likelihoods give equal weight positive negative samples. quantitative results. show test pseudo log-likelihood methods table report method outperforms baseline experiments. s-efe either hierarchical structure amortization outperforms competing methods based standard highlighted bold. global ignores per-group variations whereas separate cannot share information across groups. results global baseline better ﬁtting separate unlike methods global cannot used exploratory analysis variations across groups. results show using hierarchical s-efe always better using simple s-efe model ﬁtting separate group. hierarchical structure helps especially senate speeches data divided many save space report results context size only. context size shows relative performance. adam needs track history gradients parameter optimized. advantage reducing number parameters amortization results reduced computational overhead adam table test log-likelihood three considered datasets. s-efe consistently achieves highest held-out likelihood. competing methods global capture group variations separate cannot share information across groups. table list three different words different groups congressional speeches. s-efe uncovers words used differently republican senators democratic senators different states. complete table appendix. groups. among amortized s-efe models developed least amortization residual networks outperforms base s-efe. advantage residual networks feed-forward neural networks consistent results reported hierarchical s-efe amortized s-efe share information embedding particular word across groups amortization additionally ties embeddings words within group hypothesize senate speeches split many groups strong modeling constraint helps experiments. structured embeddings reveal spectrum word usage. motivated s-efe example usage intelligence varies arxiv category explain term per-group embeddings place groups spectrum. speciﬁc term groups project onto one-dimensional take embeddings vectors space using ﬁrst component principal component analysis one-dimensional summary close embeddings across groups. comparison posible s-efe shares context vectors grounds embedding vectors joint space. spectrum word intelligence along ﬁrst principal component horizontal axis figure dots projections group-speciﬁc embeddings word. unsupervised manner method placed together groups related physics spectrum computer science statistics math spectrum. give additional intuition usage intelligence different locations spectrum listed similar words groups computer science quantitative ﬁnance math statistics nonlinear sciences word similarities computed using cosine distance embedding space. eventhough embeddings relatively close spectrum model ﬂexibility capture high variabilty lists similar words. exploring group variations structured embeddings. result s-efe also allows investigate words highest deviation average usage group. example congressional speeches many terms expect senators differently might however want question like which words republicans texas differently senators? suggesting answer method guide exploratory data analysis. group compute words argsortv within words. table shows summary ﬁndings according s-efe republican senators texas border phrase country different contexts senators. variations probably inﬂuenced term frequency expect democrats washington talk washington frequently states. argue method provides insights frequency based analysis also sensitive context word appears. example washington might groups used often context president george others might appear context capital refer state. presented several structured extensions modeling grouped data. hierarchical s-efe capture variations word usage across groups sharing statistical strength hierarchical prior. amortization effective reduce number parameters hierarchical model. amortized s-efe model leverages expressive power neural networks reduce number parameters still ﬂexibility capture variations embeddings group. practical guidelines choosing s-efe. embeddings vary across groups data? capture variations across groups never separate embedding model group. recommend least sharing context vectors s-efe models ensures latent dimensions embeddings aligned across groups. addition sharing context vectors also recommend sharing statistical strength embedding vectors. paper presented ways hierarchical modeling amortization. hierarchical prior amortization? answer depends many groups data contain. experiments hierarchical s-efe works better many groups. less groups amortized s-efe works better. advantage amortized s-efe fewer parameters hierarchical model still ﬂexibility capture across-group variations. global embeddings amortized s-efe roles. capture semantic similarities words also serve input amortization networks. thus global embeddings words similar pattern across-group variation need regions embedding space lead similar modiﬁcations amortization network. number groups data increases roles become harder balance. hypothesize amortized s-efe stronger performance fewer groups. feed-forward residual networks? amortize s-efe recommend residual networks. perform better feed-forward networks experiments. feed-forward network output entire meaning word group-speciﬁc embedding residual network needs capacity model group-speciﬁc embedding differs global embedding. thank elliott suresh naidu helpful discussions sharing senate speeches. work supported iis- n--- darpa ppaml fa-- darpa simplex n--c- alfred sloan foundation john simon guggenheim foundation. francisco ruiz supported programme references abadi agarwal barham brevdo chen citro corrado davis dean devin tensorflow large-scale machine learning heterogeneous systems. software available tensorﬂow.org. table amortized s-efe highlights word embeddings change drastically speciﬁc group global mean. state list terms frequent vocabulary terms democrats state republican senators state differently.", "year": 2017}