{"title": "Recurrent computations for visual pattern completion", "tag": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when rendered <10% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared to whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information.", "text": "hanlin tang* bill lotter* martin schrimpf* paredes josue ortega caro walter hardesty david gabriel kreiman *these authors contributed equally correspondence addressed gabriel.kreimantch.harvard.edu program biophysics harvard university program software engineering augsburg university technische universität münchen ludwig-maximilians-universität münchen molecular cellular biology harvard university children’s hospital harvard medical school text statistics number words abstract number words significance statement number figures number tables number supplementary figures making inferences partial information constitutes critical aspect cognition. visual perception pattern completion enables recognition poorly visible occluded objects. combined psychophysics physiology computational models test hypothesis pattern completion implemented recurrent computations present three pieces evidence consistent hypothesis. first subjects robustly recognized objects even rendered visible recognition largely impaired processing interrupted backward masking. second invasive physiological responses along human ventral cortex exhibited visually selective responses partially visible objects delayed compared whole objects suggesting need additional computations. physiological delays correlated effects backward masking. third state-of-the-art feed-forward computational architectures robust partial visibility. however recognition performance recovered model augmented attractor-based recurrent connectivity. results provide strong argument plausibility role recurrent computations making visual inferences partial information. humans animals demonstrate remarkable ability make inferences partial data across cognitive domains ranging interpreting sensory information understanding social interactions. visual domain capacity ubiquitously illustrated pattern completion recognize objects rendered partially visible noise limited viewing angles poor illumination occlusion. significant progress describing neural machinery along ventral visual stream responsible recognizing whole objects computational models instantiating biologically plausible algorithms pattern recognition whole objects typically consist sequence linear filtering non-linear pooling operations. concatenation operations transforms pixel inputs feature representation amenable linear decoding object labels. family feed-forward algorithms performs quite well large scale computer vision experiments pattern recognition also provides first-order approximation describe activity neurons along ventral visual stream despite success feed-forward architectures describing initial steps visual recognition layer limited spatial integration inputs. additionally feed-forward algorithms generally lack mechanisms integrate temporal information take advantage rich temporal dynamics characteristic neural circuits allow comparing signals within across different levels visual hierarchy. spatial temporal integration likely play important role pattern completion mechanisms object occluded infinitely many possible contours could join object’s parts together. brain typically manages integrate parts correctly recognize occluded object. multiple studies highlighted importance temporal integration pattern completion demonstrating recognizing partially visible objects takes time recognizing fully visible ones behavioral level well physiological level conjectured recurrent computations including potentially within-layer horizontal connections well top-down signals along ventral visual stream involved implementing spatial temporal integrative mechanisms underlying pattern completion. recurrent connections link signals space within layer provide specific top-down modulation neurons larger receptive fields additionally recurrent signals temporally feed-forward counterparts therefore provide ideal incorporate neural dynamics temporal integration mechanisms. order examine plausible mechanisms involved pattern completion combined psychophysical experiments neurophysiological recordings computational modeling evaluate recognition performance using common objects rendered poor visibility. first show humans able robustly recognize objects despite limited amount information available task performance rapidly deteriorates computations interrupted introduction noise mask. furthermore image-byimage basis behavioral effect backward masking correlates increase latency neurophysiological signals along ventral visual stream. demonstrate modern feed-forward computer vision models robust occlusion breakdown representation individual images correlates image-by-image level increase physiological latency. finally extend previous notions attractor dynamics adding recurrence state-of-the-art model visual recognition provide proof-of-concept model captures essence human pattern completion behavior. esults twenty-one subjects performed visual recognition task involving categorization objects either partially visible presenting bubbles fully visible images followed either gray screen spatially-overlapping noise pattern image presentation time referred stimulus onset asynchrony ranged different trials presented random order. main experiment stimuli consisted objects belonging five categories object presented condition. parts revealed object chosen randomly. considered total images object comprising total images partial objects start describing psychophysics results absence mask. whole objects behavioral performance near ceiling expected subjects able robustly recognize partial objects across wide range visibility levels despite limited information provided although poor visibility degraded performance respect whole objects subjects still showed performance visibility even images visibility performance well chance levels small significant improvement performance longer soas partially visible objects separate experiment used explicit occluder generate images objects appeared behind uniform black surface consistent previous studies recognition also robust using heavily occluded images. presence occluder improved recognition performance respect recognition partial objects following experiments unless otherwise stated focused essential aspects pattern completion considering challenging condition partially visible objects without help cues occluders. twenty-one subjects performed forced-choice categorization task. fixation stimuli presented variable exposure times denoted stimulus onset asynchrony image followed either gray screen noise mask response choice screen shown. stimuli either presented unaltered rendered partially visible variant experiment stimuli occluded behavioral performance function percentage object visible unmasked masked trials. colors denote different soas. error bars denote s.e.m. horizontal dashed line indicates chance level size note discontinuity introduced x-axis report performance whole objects average recognition performance function stimulus onset asynchrony partial objects error bars denote s.e.m. performance significantly degraded masking compared unmasked trials multiple lines evidence behavioral physiological computational studies suggest recognition whole isolated objects approximately described rapid largely feedforward mechanisms. several investigators used psychophysical experiments involving backward masking attempt force visual recognition machinery operate fast regime minimal influences recurrent signals image rapidly followed spatially overlapping mask high-contrast mask stimulus interrupts additional presumably recurrent processing original image asked whether fast essentially feed-forward recognition regime imposed backward masking sufficient robust recognition partially visible objects randomly interleaving trials mask behavioral effects backward masking shown figure performance whole images affected mask shortest values partial objects followed backward mask performance significantly impaired two-way anova performance masking factors revealed significant interaction p<-). behavioral consequences shortening significantly stronger presence backward masking additionally backward masking disrupted performance across wide range visibility levels demonstrated figure effects backward masking significant similar effects backward masking observed using occluded objects interrupting processing backward masking large reduction ability pattern completion partially visible images occluded images across wide range values visibility levels. recent study recorded invasive physiological signals throughout ventral visual stream human patients epilepsy performed experiment similar figure experiment included objects images presented without masking. whole objects neural signals along ventral visual stream showed rapid selective responses different object categories shown example electrode left fusiform gyrus figure a-b. presenting partially visible objects neural signals remained visually selective visually selective signals elicited partial objects significantly delayed respect responses whole objects visible features varied trial trial different renderings object elicited wide distribution neural response latencies example peak voltage occurred post stimulus onset response first image right figure response last image right figure heterogeneity across different renderings object also evident range effects backward masking behavioral level psychophysics experiment discussed figure f-g. hypothesized images elicited longer neural delays would also susceptible backward masking. test hypothesis selected recording sites neurophysiological study showing strong visually selective signals considered images partially visible objects corresponding objects neurophysiology experiment. using images conducted separate psychophysics experiment evaluate effect backward masking individual image experiment allowed construct curve behavioral performance function backward masking selected images neurophysiology experiment example first image left figure less affected backward masking last image right particularly short values. quantify effect backward masking defined masking index -auc normalized area curve performance versus plot larger values correspond larger effects backward masking ranges values first last images figure respectively. invasive recordings neural activity human brain subject performing task described figure electrode left fusiform gyrus show intracranial field potential response averaged across five categories whole objects. electrode responded selectivity faces compared categories. gray rectangle indicates stimulus presentation time shaded area around curve indicates s.e.m. detail electrode fig. responses whole objects electrode showing single trial responses average response latency peak response marked x-axis. single-trial responses partial objects object. stimulus psychophysics experiments constructed images trials electrodes physiology experiments. example electrode subplot shows raster neural responses trial line partial trials selected psychophysics testing. trials elicited strong physiological responses wide distribution response latencies. trials sorted neural response latency. color indicates voltage inset zooms responses trials preferred category. conducted separate psychophysics experiment measure effect backward masking various soas partial exemplar images used physiology experiment computed masking index image defined area curve performance versus plots larger given image stronger effect masking. correlation effect backward masking neural response latency single partial object preferred category electrode significant correlation images preferred category electrode masking index showed weak significant correlation neural response latency even accounting image difficulty recording site differences effect stimulus selective masking index correlated neural response latency images non-preferred categories although physiology behavioral experiments conducted different subjects found images longer neural response latencies associated stronger effect interrupting computations backward masking. next investigate potential computational mechanisms involved pattern completion responsible behavioral physiological observations described figures began considering state-of-the-art implementations family purely feed-forward computational models visual recognition. computational models characterized hierarchical feed-forward processing progressive increases size receptive fields degree selectivity tolerance object transformations models successfully used describe rapid recognition whole objects behavioral level well neuronal firing rates areas inferior temporal cortex macaque monkey additionally deep convolutional network architectures achieve high performance computer vision competitions evaluating object recognition capabilities evaluated whether feed-forward models could lead successful recognition partially visible objects using objects figure representative family models considered alexnet eight-layer convolutional neural network trained back propagation imagenet large corpus natural images used features either activity last fully connected layer readout activity last retinotopic layer order measure effect low-level differences categories also considered pixels baseline performance figure standard feed-forward models robust occlusion performance various feed-forward computational models compared human performance model used feature representation trained classifier whole objects evaluated performance feature representation partial objects objects used train classifier appear partial objects test set. human performance shown images error bars denote s.e.m. applied dimensionality reduction using stochastic neighborhood embedding visualize representation alexnet model. whole objects partial objects different categories separable space boundaries learned whole objects generalize space partial objects. black arrow shows schematic example model distance definition image partial face average face centroid neural latency image correlated distance image category center models. correlation distance partial object whole category center neural response latency represents partial object responses recorded either electrode electrode results shown models correlation coefficients values permutation test shown subplot. sought measure robustness networks partial object visibility studies evaluated tolerance transformations size position changes i.e. training decision boundary condition testing conditions thus trained decision boundaries using support vector machine classifier linear kernel representation whole objects stimulus figure tested object categorization performance images partial objects. cross-validation performed objects objects used train decision boundary appear partial objects test set. performance pixels essentially chance level contrast models performed well chance alexnet layer demonstrated better performance pool layer. feed-forward models performed well chance significant respect human performance visibility levels results consistent reported experimental simulations occluded objects similar bottom-up neural networks observation bottom-up models perform humans recognition partially visible images depends strongly stimuli amount information available bottom-up models comparable humans visibility visualize effect partial visibility model representations used stochastic neighborhood embedding dimensionality reduction technique projects features layer alexnet onto dimensions expected representation whole objects showed clear separation among categories. however partial objects different categories similar whole objects corresponding categories. therefore decision boundaries trained whole objects generalize categorization partial objects demonstrated figure results consistent previous computational study using feed-forward models similar ones current work evaluating extensive image data thus despite success purely feed-forward models recognition whole objects models robust images severely reduced amounts visibility account human performance levels pattern completion visibility objects. next sought understand breakdown models’ representations objects partial visibility. illustrated t-sne plot removing large amounts pixels objects pushed model’s representation partially visible images large distance away whole counterparts distance representation partially visible object corresponding whole object category mean indicative magnitude impact partial visibility object representation. evaluated whether distortion feed-forward representation caused partial visibility correlated latencies neural recordings discussed figure reasoned images partial objects whose model representation distorted would lead longer neural response latencies. computed euclidean distance representation partial object category mean representation whole objects pool computational models. distance schematically illustrated image model black arrow figure note actual distances computed using features using graphical t-sne representation. found modest significant correlation object-by-object level computational distance category mean neural response latency pool features. statistical significance correlations assessed regressing distance category mean neural latency along several additional predictors account potential confounds. specifically included following factors percentage object visibility pixel distance regress variation explained low-level effects occlusion well difficulty; electrode number account interelectrode variability dataset masking index control overall recognition difficulty. model distance category mean pool layers correlated response latency beyond could explained additional factors state-ofthe-art feed-forward architectures robustly extrapolate whole partially visible objects failed reach human-level performance recognition partially visible objects. representation partially visible objects sufficiently distinct whole objects leading misclassification. difference representation whole partial objects increased time took selective neural response evolve response partial object also longer. behavioral neural modeling results presented suggest need additional computational steps beyond present feed-forward architectures build robust representation partially visible objects. several computational ideas originating models proposed hopfield shown attractor networks perform pattern completion. hopfield network units connected all-to-all fashion weights defining fixed attractor points dictated whole objects represented. framework images pushed farther away limited visibility would require processing time converge appropriate attractor consistent behavioral physiological observations. proof-of-principle augmented feed-forward models discussed previous section recurrent connections attempt generate robust representation attractorlike mechanism whole object specifically used alexnet architecture fixed feed-forward weights pre-training imagenet added recurrent connections layer. recurrent connectivity ubiquitous throughout visual neocortical areas biological systems. motivation include recurrent connectivity layer examine first simple minimal extension existing architectures denote activity layer time -dimensional feature vector time step activity determined combination activity previous time step constant input drive introduces non-linearity input previous layer kept constant time identical input purely feed-forward alexnet architecture. weight matrix governs temporal evolution layer. figure dynamic recurrent neural network showed improved performance time impaired backward masking top-level representation alexnet receives inputs governed weights added recurrent loop within top-level representation weight matrix governs temporal evolution representation. different ways choosing weights types recurrent models rnnh shown here trained partial objects discussed text shown figures s-s. performance recurrent neural networks rnnh function object visibility. rnnh approached human performance represented significant improvement original layer dashed line indicates chance. black curves ones depicted figure reproduced comparison purposes. error bars denote sem. temporal evolution feature representation rnnh visualized t-sne whole objects partial objects colored according category. visualization purposes partial object object shown. time representation partial objects approaches correct category clusters whole images. overall performance rnnh function recurrent time step compared human performance chance error bars denote s.e.m. correlation pattern responses recurrent model humans. dashed line indicates upper bound human-human similarity obtained computing well half subject pool correlates half. regressions computed separately category -avoid differences across categories dominate correlations followed averaging correlation coefficients across categories. time model becomes human-like. error bars denote s.d. across categories. effect backward masking. backward mask used psychophysics experiments rnnh model different values performance evaluated multiple time steps. error bars denote s.e.m. performance improved increasing values demonstrated humans figure parameters depended partial objects symmetric weight matrix dictated representation whole objects initial state network given activity previous layer followed binarization. state network evolved time according satlins saturating non-linearity first verified whole objects constituted attractor point network ensuring representation change time used inputs model. next evaluated responses rnnh images containing partial objects. model convergence based final time point evaluated performance model recognizing partially visible objects. rnnh demonstrated significant improvement alexnet layer dynamic trajectory representation whole partial objects layer rnnh model visualized figure recurrent computations taken place representations partial objects clustered together separated clusters whole objects category described figure time progressed cluster partial objects pulled apart moved towards respective categories. example representation partial chairs largely overlapped cluster whole chairs concomitant dynamic transformation representation partial objects overall performance rnnh model improved time addition average performance reported figure directly compared performance object-by-object level humans rnnh time rnnh behaved like humans object-byobject level time step model computed average correct rate partial objects object categories correlated vector pattern human performance upper bound represents human-human similarity defined correlation response patterns half subject pool half. time recurrent modelhuman correlation increased towards human-human upper bound. also examined correlations across objects irrespective category; correlations also improved time dominated acrosscategory differences performance. implementing recurrent connections attractor-like fashion feed-forward hierarchical model significantly improved model’s performance pattern completion additional computations consistent temporal delays described behavioral neural levels. reasoned backward mask introduced experiment discussed figure impaired recognition performance interrupting processing investigate whether could reproduce effect rnnh model. computed responses alexnet model mask features mask rnnh model certain number time steps. switching mask input earlier time points meant mimic shorter soa’s psychophysical experiments. read performance based resulting activity combining partial object mask different time points presenting mask reduced performance although cannot directly compare soas expressed milliseconds behavioral experiments time steps model results qualitatively consistent behavioral effects backward masking discussion natural viewing conditions routinely necessary recognize objects partially visible occlusion poor illumination. visual system capable completing patterns making inferences conditions even object visible investigated mechanisms underlying robust recognition partially visible objects behavioral physiological computational levels. demonstrated backward masking significantly impairs recognition partial images short stimulus onset asynchrony strength disruptive effect backward masking image-by-image basis correlated neural delays described previously invasive recordings along ventral visual stream state-of-the-art bottom-up computational architecture trained whole objects failed achieve robustness recognition partially visible objects introduction recurrent connections layer computational model significant improvement recognition objects partial information average level object-by-object level increase performance involved recurrent computations evolving time interrupted introduction mask experiments study rendered objects partially visible presenting bubbles attempt distill basic mechanisms required spatial integration pattern completion. quantitatively easier recognize objects behind real occluder results presented qualitatively similar using explicit occluders recognition objects shown explicit occluders also disrupted backward masking recognition partially visible objects requires longer reaction times additionally neural signals involved representing partially visible objects show significant delays respect corresponding responses representing continuous lines whole objects behavioral neural delays suggest need additional computational steps interpret partially visible images. notion supported observation interrupting additional computations introducing backward mask significantly impairs recognition several lines evidence suggest backward masking disproportionately affects recurrent computations accordingly conjectured disruptive effect backward masking pattern completion could ascribed impairment recurrent computations. several investigators argued rapid selective signals along ventral visual stream enable recognition whole objects within approximately stimulus onset reflect largely bottom-up processing among many others). long physiological delays recognition partial objects provide ample time recurrent connections exert effects pattern completion. indeed several physiological studies shown recurrent signals accompanied delays approximately throughout ventral visual cortex delays approximately duration could accounted recruitment lateral/horizontal recurrent connections within visual area top-down signals higher visual areas. current results allow disambiguate horizontal top-down effects could contribute pattern completion. related interpretation current findings challenging tasks recognizing objects minimal pixel information lead slower processing throughout ventral visual stream. according idea neuron would receive weaker inputs require longer time integration leading longer latencies observed experimentally behavioral physiological level. seems unlikely current observations could fully accounted longer integration times levels visual hierarchy. first images contrast normalized avoid overall intensity effects. second neural delays poor visibility images observed early visual areas third correlations effects backward masking neural delays persisted even accounting difficulty level fourth none state-of-the-art purely bottom-up computational models able account human level performance arguments rule slower processing throughout entire visual system intensity signals lower visibility conditions. however results presented still compatible notion inputs higher-level neurons case partial objects could weaker could require temporal integration. possibility consistent model proposed here. effects recurrent computations delayed respect bottom-up inputs expect slow integration would interact outputs recurrent signals. order model quantify notion recurrent connections could facilitate pattern completion presented proof-of-principle model extended bottom-up architectures adding recurrent connections level simple extension significantly improved performance showed correlation object-by-object level also qualitatively accounted effects backward masking rnnh model free parameters depended partial objects weights fully determined features representing whole objects last layer. potential challenge type architecture pervasive presence spurious attractor states particularly prominent network near capacity. furthermore simple instantiation recurrent architecture still performs humans particularly visibility conditions. conceivable complex architectures take account known lateral connections every layer well top-down connections visual cortex might improve performance even further. increasing complexity power model directly train network partially visible objects. extended recurrent computations hopfield model family models weights directly trained minimize time distance layer partial object corresponding whole object importantly avoid overfitting used subset objects training tested model’s classification performance objects. model denote recurrent weights trained subset objects five categories. model significantly outperformed rnnh models considering levels visibility model performed slightly human levels model extrapolate across objects categorize images partial objects seen before exploiting features similar different objects within categories experiment. model recurrent weights trained using solely objects categories performance evaluated using objects remaining categories perform better purely feed-forward architecture upon inspection representation observed several features sparsely represented across categories. therefore recurrent weights modified fraction possible features missing many important features distinguish objects. thus improvement built upon sufficiently rich dictionary features shared among objects within category. results show recurrent neural networks trained subsets partially visible objects achieve human level performance extrapolating across objects long trained sufficiently rich features. fundamental distinction rnnh latter model requires training partial objects. unclear whether humans require specific experience partial objects perform pattern completion. although specific photographs objects used psychophysics experiments presented subjects humans extensive experience recognizing objects partial information. extrapolating simplified models presented here inferred humans might able recognize novel objects partially visible based experience whole counterparts exclusively need direct experience partial objects cases extremely visibility. architectures showed robust extrapolating whole objects partial objects however exist infinitely many possible bottom-up models. hence even though examined state-of-the-art models quite successful object recognition failure account behavioral physiological results bottom-up models examined interpreted caution. imply impossible bottom-up architecture recognize partially visible objects. fact possible unfold recurrent network finite number time steps bottom-up model creating additional layer additional time step. however several advantages performing computations recurrent architecture including drastic reduction number units required well number weights need trained fact unfolding applicable know priori fixed number computational steps required contrast recurrent architectures allow arbitrary variable number computations. unclear directly time steps recurrent computation real time milliseconds versions involve dynamic evolution representation features partial objects dynamic changes bring representation partial objects closer whole objects. first approximation computational dynamics onto temporal lags observed behavioral physiological levels. furthermore dynamics interrupted presentation backward mask close temporal proximity image natural viewing conditions multiple cues recognition partially visible objects including understanding textures relative positions segmentation movement source illumination stereopsis among others. none mechanisms considered current instantiation models. interesting integrate additional sources information understand contribute mechanisms pattern completion real world conditions. convergence behavioral physiological theoretical evidence presented provides initial path biologically constrained hypothesis understand role recurrent computations pattern completion. psychophysics experiments total volunteers normal corrected normal vision participated psychophysics experiments reported study. subjects gave informed consent studies approved institutional review board. subjects positions recorded using infrared camera tracker performed main experiment variations main experiment. scheme experiment shown figure twenty-one subjects asked categorize images possible semantic groups pressing buttons gamepad. stimuli consisted contrast-normalized gray scale images objects belonging five categories trial initiated fixating cross least fixation subjects presented image object variable time referred stimulus onset asynchrony image followed either noise mask gray screen duration choice screen appeared requiring subject respond. noise mask generated scrambling phase images retaining spectral coefficients. images subtended approximately degrees visual field. approximately trials objects presented unaltered fashion objects rendered partially visible presenting visual features gaussian bubbles subject performed initial training session familiarize task stimuli. presented trials whole objects calibration trials occluded objects. calibration trials number bubbles titrated using staircase procedure achieve task difficulty correct rate. number bubbles kept constant rest experiment. results familiarization calibration phase included analyses. despite calibrating number bubbles wide range degrees occlusion positions bubbles randomized. image presented masked condition unmasked condition. variant experiment. variant experiment stimuli consisted images five categories previously recorded neural responses difference main experiment used exact images used physiological recordings order directly compare behavioral results physiological results images occlusion experiment. occlusion experiment generated occluded images revealed sets features partial objects contained explicit occlude activate amodal completion cues. stimulus consisted objects different categories. comparison also collected performance partial objects reduced stimulus set. neurophysiological data analyzed figures taken study reported reference refer details. briefly subjects patients pharmacologically intractable epilepsy intracranial electrodes implanted clinical purposes. studies approved hospital’s institutional review board carried subjects’ informed consent. images partial whole objects presented followed gray screen subjects performed five-alternative forced choice categorization task described figure construct stimulus variant experiment chose electrodes visually selective partial objects neurophysiology data identified images presented trials psychophysical testing. preferred category trials amplitude elicited neural response percentile included trials chosen represent distribution neural response latencies. constructing stimulus performed psychophysical experiments subjects evaluate effect backward masking exact images physiological data. select subset electrodes random bubble positions changed across experiments practical conduct separate psychophysics experiment electrodes original experiment. electrode selection procedure strictly dictated above-described physiological criteria performed even beginning psychophysics experiment. focused neural latency defined time peak physiological response shown figure extensive analysis physiological data presented masking index. quantify effect backward masking defined masking index %-pauc pauc percent area curve plotting performance function correlation masking index neural latency. determine correlation masking index neural response latency combined data recording sites first standardizing latency measurements used linear regression neural response latency masking index percent visibility recording site predictor factors avoid correlations dictated task difficulty differences recording sites. used trials preferred category recording site reported correlation statistical significance figure described text significant correlation masking index neural latency considering trials non-preferred category. correlation model distance neural response latency. described below simulated activity units several computational models response images used psychophysics physiology experiments. correlate model responses neural response latency computed euclidean distance model representation partial whole objects. computed distance partial object variant stimulus either whole counterpart centroid whole images category assessed significance using linear regression model distance versus neural response latency controlling masking index percent visibility recording site factors. considered ability recognize partially visible images state-ofthe-art feed-forward computational models vision first evaluated whether possible perform recognition purely based pixel intensities. next evaluated performance alexnet model alexnet eight-layer deep convolutional neural network consisting convolutional maxpooling fully-connected layers large number weights trained supervised fashion object recognition imagenet large collection labeled images used version alexnet trained using caffe popular deep learning library. layers within alexnet tested pool pool last convolutional layer architecture. last layer classification step fully connected every unit connected every unit previous layer. number dimensions used represent object pixels pool classification performance model evaluated stimulus consisting images partial objects partial objects used collect human performance main experiment used support vector machine linear kernel perform classification. used -fold cross-validation across objects. split contained objects training objects split validation testing object used exactly validation testing split equal number objects category split. decision boundaries training using parameter determined validation considering following possible values boundaries using images whole objects tested images partial objects. final performance numbers partial objects calculated full data images split classification performance evaluated partial objects corresponding objects test that splits partial object evaluated exactly once. graphically display representation images based units layer model plot used stochastic neighborhood embedding note done exclusively display purposes analyses including distances classification correlations based model representation units corresponding layer described above. model image computed euclidian distance model’s representation mean point across whole objects within corresponding category. distance-to-category corresponds y-axis figure c-d. schematically illustrate distance-to-category figure note schematic mathematically accurate t-sne embeddings directly reflective distances used calculations figure c-d. recurrent neural network constructed adding all-to-all recurrent connections feature layer alexnet consists state vector updated according input current time step value previous time step. denoting state vector time input network time general form update introduces non-linearity defined below. equation model represents feature vector time represents feature vector previous layer multiplied transition weight matrix simplicity first layers alexnet kept fixed original feed-forward versions. considered several different ways choosing weights first version involved hopfield network rnnh implemented matlab’s newhop function. since implementation based binary unit activity first converted scalar activities {-+} mapping values greater value weights rnnh symmetric represents activity unit response pattern model free parameters depend partial objects weights uniquely specified activity feed-forward network response whole objects. specifying activity rnnh updated according satlins represents saturating linear introduces constant bias transfer function term. activity rnnh simulated convergence defined first time point change sign features consecutive time points. also considered different versions models trained reconstruct feature representations whole objects feature representations corresponding partial objects state time computed activation weighted previous state input form previous layer loss function mean squared euclidean distance features partial objects features whole objects. specifically iterated fixed number time steps initial feed-forward pass keeping input constant. thus letting state last time step given image pattern feed-forward feature vector corresponding whole image pattern loss function form goes units goes images training set. trained cross validated fashion using cross validation scheme feed-forward models using algorithm rmsprop optimization weights trained objects fold. partial objects psychophysics experiment given objects well copy original images used train corresponding split. case input original image itself network change representation recurrent iterations. given high number weights learned compared number training examples rnns overfit fairly quickly. therefore early stopping implemented determined validation i.e. used weights time step validation error minimal. evaluate extent extrapolation across categories considered additional version rnn. recurring weights trained using objects category model tested using objects remaining categories. versions fixed classification performance assessed using linear feed-forward models. specifically boundaries trained using responses feed-forward model whole objects performance evaluated using representation different time steps recurrent computation. additional simulation models evaluate effects backward masking purpose simulated response feed-forward alexnet model masks used psychophysical experiments determine features mask image. next used mask fixed input recurrent network beginning different time points initial image input. code availability. code made available kreiman site also kreiman github repository. work supported fellowship fitweltweit programme german academic exchange service award ccf- award thank carlos ponce alan yuille siamak sorooshyari ben-yosef useful discussions comments. relevant data code made available upon publication lab’s publicly accessible github. conceptualization physiology experiment design physiological data collection analyses psychophysics experiment design psychophysics data collection computational models", "year": 2017}