{"title": "Asymmetric kernel in Gaussian Processes for learning target variance", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "This work incorporates the multi-modality of the data distribution into a Gaussian Process regression model. We approach the problem from a discriminative perspective by learning, jointly over the training data, the target space variance in the neighborhood of a certain sample through metric learning. We start by using data centers rather than all training samples. Subsequently, each center selects an individualized kernel metric. This enables each center to adjust the kernel space in its vicinity in correspondence with the topology of the targets --- a multi-modal approach. We additionally add descriptiveness by allowing each center to learn a precision matrix. We demonstrate empirically the reliability of the model.", "text": "computer vision delft university technology delft netherlands intelligent sensory information systems university amsterdam amsterdam netherlands work incorporates multi-modality data distribution gaussian process regression model. approach problem discriminative perspective learning jointly training data target space variance neighborhood certain sample metric learning. start using data centers rather training samples. subsequently center selects individualized kernel metric. enables center adjust kernel space vicinity correspondence topology targets multi-modal approach. additionally descriptiveness allowing center learn precision matrix. demonstrate empirically reliability model. elsevier ltd. rights reserved. departing standard gaussian process introduce regression approach incorporates multi-modality data distribution. gaussian process model global kernel metric shared samples rasmussen propose deﬁne training data centers considerably smaller number training samples. subsequently learn numerous training samples individualized kernel metric training data center. able smaller training kernel matrix computed training data centers retaining descriptive power model. highly efﬁcient testtime limits size kernel matrix. method introduce main changes standard gaussian process regressor deﬁne number centers training data clustering sampling; learn individual kernel metric parameters data center discriminatively metric learning giving rise multi-modal approach asymmetric kernel matrix. figure illustrates differences comparing standard gaussian process trained either data centers samples fewer samples kernel computation enhancing descriptiveness model learning individualized metrics center. additionally expand kernel parameters individual steps model speciﬁcally novel combination gives strength. clustering data gaussian processes previously proposed pintea snelson ghahramani asymmetric kernels studied works mackenzie tieu multivariate lengthscale parameter used kersting l´azaro-gredilla titsias pintea improved descriptiveness. here combine ideas approach suitable learning target data variance. consider training sample data center enforce samples share kernel metric assume univariate lengthscale kernel metric recover standard gaussian process deﬁnition. evaluate proposed approach gradually enabling changes center-based gaussian process univariate multi-modal asymmetric gaussian process multivariate multi-modal asymmetric gaussian process. experiments validate models regression datasets temp used titsias l´azaro-gredilla large scale airlines dataset hoang realistic image datasets ucsd chan vasconcelos voc- everingham pedestrian generic-object counting respectively. fig. intuitive illustration proposed asymmetric kernel metric optimization compared standard gaussian process data centers samples. given data center learns personalized size shape kernel obtain descriptive model standard gaussian process using limited number data centers kernel matrix computation. mixtures gaussian processes noteworthy work focusing mixtures gaussian processes l´azaro-gredilla meeds osindero nguyen bonilla rasmussen ghahramani tresp yuan neubauer tresp mixture gaussian processes proposed effectively deal large data. meeds osindero rasmussen ghahramani extend idea inﬁnite mixture gaussian processes. somewhat similarly splits problem subproblems divide-andconquer fashion solves problem gaussian process model. yuan neubauer proposes elegant variational bayesian algorithm training mixture gaussian process experts. effective model proposed l´azaro-gredilla authors simplify mixture experts gating function used assign samples components rather trajectory clustering employed. nguyen bonilla gracefully combines mixture gaussian process experts idea inducing points providing fast approximate gaussian process models. unlike works ﬁnal prediction entails combination predictions obtained within metric space individual components learn hyper-parameters associated training data centers single gaussian process. employing asymmetric kernel. therefore test-time input test sample rather computing kernel distances number components number training samples compute distances. case number data centers considerably smaller efﬁciency gaussian processes work qui˜nonero-candela rasmussen reviews sparse approximations gaussian process uniﬁed perspective analyzing implied prior different methods. csat´o opper proposes learning iteratively online sparse inducing points bayesian formalism minimizing divergence. inspired metric learning techniques lawrence uses forward selection obtain sparse time-efﬁcient model. snelson ghahramani proposes graceful solution learning sparse pseudo-inputs gradient based optimization. combination sparse methods based inducing points local regression based multitude experts describing locally target space proposed snelson ghahramani titsias titsias lawrence variational approaches used learn sparse representations. titsias jointly learns inducing points kernel hyper-parameters minimizing lower bound divergence. robust method hensman decomposes gaussian process model variationally factorized based global inducing variables. sminchisescu ranganathan focus iterative updates gaussian process. rodner proposes parameterized histogram intersection kernels bypass hyper-parameter estimation. proposes method speed hyper-parameter estimation inducing sparsity model. somewhat similar methods retain data centers informative training samples. unlike approaches subsequently extra information gaussian process model treating data centers differently. descriptiveness gaussian processes full matrices kernel deﬁnition proposed pintea vivarelli williams make model descriptive. here also learn precision matrices kernel metric deﬁnition. however work center individualized precision matrix. paciorek schervish proposes nonstationary covariance matrices gaussian process model tying kernel metrics input samples. however ﬁnal kernel matrix symmetric deﬁned using symmetric combinations per-sample covariances similar kersting l´azaro-gredilla titsias propose well-founded approaches adding descriptiveness extending gaussian process deﬁnition heteroscedastic approach modeling noise distribution dependent training data. kuss proposes effective manner train models. similarly also start assumption kernel metric data dependent learn individualized kernel metric. titsias l´azaro-gredilla proposes compelling method adjusting kernel distances assuming data mapped feature space based mahalanobis kernel distance estimated variational inference. unlike work learn shape scale kernels data center minimizing predictive loss. asymmetric kernels asymmetric kernel distances recent idea rather well-matured topic kulis mackenzie tieu tsuda tsuda asymmetric kernels proposed context classiﬁcation. shows similarity functions commonly used real-life applications related asymmetric kernels gives formal deﬁnition mathematical space described asymmetric kernels. work mackenzie tieu proposes asymmetric kernel regression context neural networks shows models better behaved around data boundaries. recent work kulis learns asymmetric distances visual domain adaptation context object recognition. similar methods also asymmetric kernel distances prove descriptive power limiting number samples training kernel computation. metric learning rahimi recht learns lower dimensional mapping data maintaining distances data samples kernel distances remain approximately equal ones original features. work learn kernel metric given targets rather feature representation. weinberger tesauro kernel metric minimizes leave-one-out regression error. jain combines kernel learning metric learning employing linear transformation. work ﬁxed kernel squared exponential kernel additionally expand parameters kernel full precision matrices. globerson roweis weinberger xing represent pioneering work ﬁeld metric learning. xing ﬁrst paper pose metric learning convex optimization problem learned similar/dissimilar pairs points. globerson roweis ﬁrst works propose mahalanobis distances metric learning. weinberger mahalanobis distance learned nearest neighbor classiﬁer induces large-margin separation classes. kedem kostinger weinberger recent works focusing metric learning classiﬁcation kernels huang focuses sparse kernel learning regression. work employ metric learning rather estimating optimal model hyper-parameters marginal likelihood rasmussen data center associated lengthscale proposed model thus marginal likelihood optimization straightforward case. redeﬁne gaussian process model allowing training data center learn individualized kernel metric. entails kernel matrix ceases symmetric case. however comes extra gain descriptive power despite using small samples training kernel matrix optimize individualized kernel metrics numerous available training samples. standard gaussian process revisited shortly revisit standard gaussian process formulation unify notations. mean predictive distribution rasmussen represents input test sample represents training samples used training kernel matrix computation represents training targets prediction input kernel metric used estimating sample distances noise hyper-parameter. center-based gaussian process equation indicates training procedure requires computation inverse training kernel-matrix prohibitive larger datasets. ﬁrst alteration gaussian process model investigate considering data centers rather individual training samples. either sampling data clustering centers despite simplicity effective getting fair overview variation training data samples training. principled manners deﬁning data centers effective sampling techniques possible. however focus center deﬁnition meant ﬁrst step towards reducing size training kernel matrix. strength model comes allowing centers learn individualized metrics. multi-modal asymmetric kernel given sparsiﬁed training data keeping training data centers lost information regarding smoothness variability target function different regions data space. therefore allow training center deﬁne individualized kernel metrics data neighborhood. lengthscale hyper-parameter deﬁning size kernel space thus propose individualized lengthscale hyper-parameters center entails second alteration standard gaussian process multivariate. denote training target vector composed values input training samples data centers number data centers denotes frobenius norm multivariate case predictive function following iteration perform gradient update step hyper-parameters therefore allowing jointly learned. denote data centers represents asymmetric training kernel matrix vector training tarn training samples data centers. gets lengthscale hyper-parameters estimated training data center rather globally. multivariate multi-modal kernel optimization multivariate case learn precision matrix rather scalar lengthscale data center therefore ensure precision matrix learned symmetric. this gradient computation apply derivations symmetric matrices. non-symmetric kernel whose size depends corresponding training center data centers. thus distance training center others computed within associated kernel space center. test-time training centers test sample. work restrain focus squared exponential kernel distance given individualized metrics data center kernel ceases symmetric. therefore longer employ standard cholesky decomposition estimating kernel-matrix inverse. compute kernel matrix inversion despite drawback individualized kernel metrics allow optimize scale kernel locally neighborhood data center. multivariate multi-modal asymmetric kernel allowing center deﬁne kernel metric change model locally resize kernel space better target space. highlighted chen buja size kernel space important also shape. therefore also consider multivariate extension model allows optimizing also kernel shape training center. kernel metric optimization standardly literature gaussian process hyperparameters learned gradient methods maximizing marginal likelihood weights. given optimize kernel metric following metric learning literature chen buja globerson roweis weinberger xing approach problem discriminatively optimize hyper-parameters respect squared loss. thus learn appropriate kernel metric data center training samples data centers regularization term squared loss weighted regularized squared loss targets function minimized employ estimating gradients fig. pixel intensity prediction input normalized pixel location center-gp standard trained training centers; univariate proposed asymmetric model using per-center univariate lengthscale kernel multivariate proposed asymmetric model using per-center multivariate legnthscale losses training validation losses data. model properties analyze properties considered bellet metric learning perspective learning paradigm. fully supervised learn training data best lengthscale hyper-parameters respect prediction loss. form metric. non-linear local respect predictive function. different metrics learned different regions target space. scalability. scales number data centers univariate case kernel matrix computed data centers. thus efﬁcient using training samples. multivariate case learn precision matrix method scales also number data dimensions making difﬁcult optimize. optimality solution. learning formulation given equations non-convex function respect hyper-parameters global optimum guaranteed. reason additional validation training select among local optima. figure illustrates need asymmetric model. samples share kernel metric ﬁgures would possible. restricting attention training samples lose information target function varies samples per-center kernel metrics help recover information. illustration training centers centers ellipses. normalized pixel coordinates input features pixel intensities targets. visualize three models center-gp standard data centers optimal lengthscale hyper-parameter found cross validations randomly sampled training pixels; univariate-agp using equation optimal lengthscale center estimated subsection multivariate-agp using equation ﬁgure univariate asymmetric model fig. numeric consistency euclidian distance estimated hyper-parameters method using data centers optimal hyper-parameter gaussian process sampled from. plot standard deviation repetitions. blue distance optimal hyper-parameter standard centers. data estimated lengthscales approach optimal value. estimates better sizes blobs compared center-based counterpart multivariate asymmetric model lowest error learns sizes shapes blobs. also show training validation losses ﬁgure additionally analyze consistency approach numerically. this sample uniform grid standard gaussian process ﬁxed lengthscale test lengthscales estimated method tend true lengthscale gaussian process sampled training data. data centers kernel computation. figure shows euclidian distance estimated hyper-parameters method optimal one. report standard deviation repetitions. blue show distance optimal hyper-parameter estimated standard grid search using data centers. training samples estimated lengthscale parameters approach optimal value. distance converge data centers training kernel matrix. table evaluation proposed models univariate-agp multivariate-agp large scale dataset used hoang proposed models trained data centers. compare results methods analyzed hoang fitc dtc. table indicates choice centers essential methods perform similar. given strength model learning individualized hyper-parameters less method used deﬁning centers subsequent experiments rely k-means clustering. multi-modal approach evaluation table depicts results approaches temp datasets compared titsias l´azaro-gredilla standard gaussian process model. gain brought multi-modal asymmetric methods center-based gaussian process standard gaussian process obvious temp dataset. explained larger number dimensions learn from multivariate case. datasets proposed models outperform titsias l´azaro-gredilla using data centers. performance decreases slightly increase number data centers multivariate asymmetric models. model trained number iterations univariate case learn larger number parameters precision matrix size increase data dimensionality multivariate model becomes considerably slower harder optimize. represents drawback proposed multivariate approach. variability target space also affects ease good solution found. state-of-the-art comparison large scale regression problem here consider challenging task number training samples markedly high. compare effective state-of-the-art methods focus problem representing data using informative subset retaining descriptiveness model hoang airlines dataset hoang containing training samples. models considered seeger fitc snelson ghahramani snelson ghahramani evaluating models repeat experiment three times report mean rmse together standard deviation. table shows proposed test following standard way. given non-convexity problem evaluate hyper-parameters small validation training epoch keep best. datasets validation obtained random samples taken trainval dataset. samples used deﬁning data centers training data statistics. different center selection approaches considered section standardize data zero mean unit variance dimension extracting statistics training data excluding validation set. parameter setting. initial learning rate looking plots validation training losses training. starting learning rate temp airlines datasets initial learning rate ucsd voc- data dimensionality considerably higher. batch-sgd mini-batches randomly selected training samples. given non-convexity solved problem make momentum advised sutskever regularization term loss function standard models well center-gp gaussian process trained data centers estimate model hyper-parameters performing grid-search evaluating validation samples. procedure initializing per-center lengthscales optimizing sgd. evaluation metric. comparison existing work report rmse nrmse deﬁned center selection analyze effect center selection method overall performance method. temp dataset dimensions sample. consider four center selection methods k-means random sampling spectral table nrmse temp datasets methods center-gp trained training centers only univariate-agp univariate asymmetric model using kernel metrics deﬁned multivariate-agp multivariate asymmetric model kernel metrics deﬁned results compared titsias titsias l´azaro-gredilla standard trained randomly sampled examples. show bold results outperforming baseline underline best result. table rmse results voc- general object counting dataset. ﬁrst glance models global image features learned deep learning framework similar last models local information dividing image grid extracting deep learning features cell. method outperforms models relying global image features. models perform well dealing prohibitive number training samples. approach outperforms existing methods seeger snelson ghahramani using limited number data centers. realistic data counting images test regression approach realistic image datasets ucsd chan vasconcelos voc- everingham people generic object counting respectively. given image data rely deep learning features. extract dimensional features output fullyconnected layer resnet- pretrained imagenet russakovsky computational efﬁciency univariate version approach centers since multivariate version requires optimizing precision matrix size pedestrian counting. figure shows results approach realistic problem pedestrian counting images ucsd dataset together training validation losses. compare dalal triggs uses level image features felzenszwalb relying person detection method speciﬁcally trained task chan vasconcelos employs motion segmentation masks. unlike methods either motion segmentation masks class speciﬁc detectors. global image features extracted pretrained deep generic object counting. table goal generic object counting voc- generic object dataset. compare models proposed recent deep learning method chattopadhyay features extracted pretrained deep learning model speciﬁcally ﬁne-tuned counting task. outperform glance models similar rely global image features. additionally obtain comparable performance aso-sub-ft-l-× seq-sub-ft-× rely local image information divide image grid fig. results ucsd pedestrians counting dataset compared three prior works dalal triggs felzenszwalb chan vasconcelos obtain comparable performance prior work though global deep learning features chan vasconcelos relies motion segmentation masks. training validation squared losses ucsd pedestrian counting dataset. extract features cell. worthwhile noting data centers computing kernel matrix achieve comparable performance methods relying stronger features. results support approach. work brings forth asymmetric kernel gaussian process model. encompasses three components training training centers only learning individualized kernel metrics center extending lengthscale hyperparameter precision matrix thus learning appropriate size also shape kernel metric. limitations imposed dependency percenter hyper-parameters discriminatively solve problem metric learning. individualized kernel metrics entail loss symmetry kernel matrix. however gain better describing target function neighborhood center points used kernel computation. acknowledgements research supported dutch national program commit. thanks mijung park marco loog insightful suggestions advice.", "year": 2018}