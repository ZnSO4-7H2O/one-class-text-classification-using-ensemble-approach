{"title": "Learning Domain-Invariant Subspace using Domain Features and  Independence Maximization", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Domain adaptation algorithms are useful when the distributions of the training and the test data are different. In this paper, we focus on the problem of instrumental variation and time-varying drift in the field of sensors and measurement, which can be viewed as discrete and continuous distributional change in the feature space. We propose maximum independence domain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns a subspace which has maximum independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is also designed to project samples according to their backgrounds so as to improve the adaptation. The proposed algorithms are flexible and fast. Their effectiveness is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision. They can greatly enhance the practicability of sensor systems, as well as extend the application scope of existing domain adaptation algorithms by uniformly handling different kinds of distributional change.", "text": "fabrication sensors devices responses signal source identical different instruments known instrumental variation. furthermore sensing characteristics sensors operating condition even signal source itself change time leads complex time-varying drift. result prediction model trained samples initial device earlier time period suitable devices latter time typical application plagued problem machine olfaction uses electronic noses pattern recognition algorithms predict type concentration odors applications machine olfaction range agriculture food environmental monitoring robotics biometrics disease analysis however owing nature chemical sensors many e-noses prone instrumental variation time-varying drift mentioned greatly hamper usage real-world applications. traditional methods dealing kinds drift require transfer samples predeﬁned samples needed collected device time period often used learn regression models features target domain source domain nevertheless collecting transfer samples repeatedly demanding especially nonprofessional e-nose users. cases domain adaptation techniques unlabeled target samples desirable. intuitive idea reduce inter-domain discrepancy feature level i.e. learn domain-invariant feature representation example proposed transfer component analysis ﬁnds latent feature space minimizes distributional difference domains sense maximum mean discrepancy. related methods introduced section ii-a. applied drift correction however existing domain adaptation algorithms faced difﬁculties. first designed handle discrete source target domains. time-varying drift however samples come stream change data distribution often continuous. solution split data several batches lose temporal order information. second variation sensitivity chemical sensors signal different conditions indicate different concepts. words conditional probability change samples different backgrounds background means device sample collected. methods like abstract—domain adaptation algorithms useful distributions training test data different. paper focus problem instrumental variation time-varying drift ﬁeld sensors measurement viewed discrete continuous distributional change feature space. propose maximum independence domain adaptation semi-supervised mida address problem. domain features ﬁrst deﬁned describe background information sample device label acquisition time. then mida learns subspace maximum independence domain features reduce inter-domain discrepancy distributions. feature augmentation strategy also designed project samples according backgrounds improve adaptation. proposed algorithms ﬂexible fast. effectiveness veriﬁed experiments synthetic datasets four realworld ones sensors measurement computer vision. greatly enhance practicability sensor systems well extend application scope existing domain adaptation algorithms uniformly handling different kinds distributional change. index terms—dimensionality reduction domain adaptation drift correction hilbert-schmidt independence criterion machine olfaction transfer learning training data source domain test ones target domain. samples domains collected different conditions thus different distributions. labeling samples target domain develop prediction models often labor-intensive time-consuming. therefore domain adaptation transfer learning needed improve performance target domain leveraging unlabeled target samples topic receiving increasing attention recent years broad applications computer vision text classiﬁcation also important ﬁeld sensors measurement. variations work partially supported fund hksar government central fund hong kong polytechnic university nsfc fund shenzhen fundamental research fund laboratory network oriented intelligent computation shenzhen china. zhang shenzhen graduate school harbin institute technology shenzhen china also department computing biometrics research centre hong kong polytechnic university kowloon hong kong paper present simple effective algorithm called maximum independence domain adaptation algorithm ﬁrst deﬁnes domain features sample describe background. then ﬁnds latent feature space samples domain features maximally independent sense hilbert-schmidt independence criterion thus discrete continuous change distribution handled uniformly. order project samples according backgrounds feature augmentation performed concatenating original feature vector domain features. also propose semi-supervised mida exploit label information hsic. mida smida ﬂexible. applied situations single multiple source target domains thanks domain features. fact notion domain extended background informative. although designed unsupervised domain adaptation problems proposed methods naturally allow unlabeled labeled samples domains thus applied semi-supervised supervised problems well. label information either discrete continuous illustrate effect algorithms ﬁrst evaluate several synthetic datasets. then drift correction experiments performed e-nose datasets spectroscopy dataset. note spectrometers suffer instrumental variation problem e-noses finally domain adaptation experiment conducted well-known object recognition benchmark ofﬁce+caltech results conﬁrm effectiveness proposed algorithms. rest paper organized follows. related work unsupervised domain adaptation hsic brieﬂy reviewed section section describes domain features mida smida detail. experimental conﬁgurations results presented section along discussions. section concludes paper. good surveys domain adaptation found section focus typical methods extract domain-invariant features. order reduce inter-domain discrepancy preserving useful information researchers developed many strategies. algorithms project samples common latent space transfer component analysis tries learn transfer components across domains reproducing kernel hilbert space using maximum mean discrepancy. extended semi-supervised encode label information preserve local geometry manifold. measured domain difference mutual information samples binary domain labels viewed primitive version domain features used paper. also minimized negated mutual information target samples cluster labels reduce expected classiﬁcation error. low-rank transfer subspace learning algorithm presented reconstruction guided knowledge transfer method. aligns source target data representing target sample local combination source samples projected subspace. label geometry information retained embedding different subspace learning methods ltsl. another class methods ﬁrst project source target data separate subspaces build connections fernando utilized transformation matrix source subspace target subspace represented eigenvectors pca. geodesic kernel method measures geometric distance different domains grassmann manifold constructing geodesic ﬂow. inﬁnite number subspaces combined along order model smooth change source target domain. adapted correct timevarying drift e-noses. sample stream ﬁrst split batches according acquisition time. ﬁrst latest batches connected every intermediate batch using gfk. another improvement domain adaptation shifting covariance observing modeling domain subspace sufﬁcient represent difference distributions dasc characterizes domains covariance matrices interpolates along geodesic bridge domains. hsic used convenient method measure dependence sample sets kernel functions associated rkhss respectively. joint distribution. hsic deﬁned square hilbert-schmidt norm cross-covariance operator exxyy expectation independent pairs drawn pxy. proved characteristic kernels hsic zero independent large hsic suggests strong dependence respect choice kernels. hsic biased empirical estimate. suppose rn×n kernel matrices respectively hsic researchers typically maximize dependence extracted/selected features label. however knowledge utilized domain adaptation reduce dependence extracted features domain features. reduce dependence extracted features background information. sample’s background information naturally exist thus easily obtained; different distributions training test samples; correlate distribution original features. domain label common domain adaptation problems example information. according characteristics information clearly interferes testing performance prediction model. thus minimizing aforementioned dependence desirable. first group features need designed describe background information. features called domain features. perspective drift correction main types background information device label acquisition time actually encode information place collection operation condition useful domain adaptation problems. formally consider instrumental variation following one-hot coding scheme used. suppose ndev devices result ndev different related domains. domain feature vector thus rndev sample device otherwise. time-varying drift also considered acquisition time added. sample collected device time rndev note traditional domain adaptation problems several discrete domains one-hot coding scheme applied construct domain features problems similar instrumental variation. data discrete domains cannot deal timevarying drift. propose general efﬁcient feature augmentation strategy concatenating original features domain features i.e. role strategy demonstrated linear dimensionality reduction example. suppose projection matrix learned augmented feature vector. dimension subspace. rm×h rmd×h. embedparts ding expressed means background-speciﬁc bias changes along background. instance sensitivity chemical sensors often decays time. signal indicates concentration earlier time actually suggests high concentration later time. cases feature augmentation important allows samples similar appearance different concepts treated differently background-speciﬁc bias. strategy also helps align domains better projected dimension. effect illustrated several synthetic datasets section iv-a analyzed complementary materials. section introduce formulation mida detail. suppose rm×n matrix samples. training test samples pooled together. importantly explicitly differentiate domain sample from. feature vectors augmented notations instead brevity. linear nonlinear mapping function used space. based kernel trick need know exact form inner product represented kernel matrix φtφ. then projection matrix applied project subspace dimension leading projected samples rh×n. similar kernel dimensionality reduction algorithms idea express projection direction linear combination samples space namely rn×h projection matrix actually learned. thus projected samples learn background-speciﬁc subspaces. author proposed feature augmentation strategy domain adaptation replicating original features. however strategy requires sample projected features suggesting interdomain discrepancy diminished subspace. therefore omitting scaling factor expression minimized kernel matrix goal minimizing difference distributions also preserving important properties data variance achieved maximizing trace covariance matrix project samples. covariance matrix trade-off hyper-parameter. using lagrangian multiplier method eigenvectors kxkx corresponding largest eigenvalues. note conventional constraint requiring orthonormal lead generalized eigenvector problem. however strategy inferior proposed adaptation accuracy training speed practice used. computing proper kernel function needs selected. common kernel functions include linear xty) polynomial gaussian radial basis function exp. according polynomial kernels original features higher inﬁnite dimensional space thus able detect types dependence. however choosing suitable kernel width parameter also important powerful kernels maximum mean discrepancy criterion used measure difference distributions. song showed hsic applied measure dependence features labels binary-class classiﬁcation problem identical constant factor label kernel matrix hsic properly designed. however feasible discrete domains. hand mida deal variety situations including multiple domains continuous distributional change. stationary subspace analysis algorithm able identify temporally stationary components multivariate time series. however ensures mean covariance components stationary suitable preserving important properties data. concept drift adaptation algorithms able correct continuous time-varying drift. however rely newly arrived labeled data update prediction models mida works unsupervisedly. mida aligns samples different backgrounds without considering label information. however labels samples known incorporated subspace learning process beneﬁcial prediction. therefore extend mida semi-supervised mida since explicitly differentiate domain labels samples unlabeled labeled samples exist domain. similar hsic adopted maximize dependence projected features labels. biggest advantage strategy types labels exploited discrete labels classiﬁcation continuous ones regression. label matrix deﬁned follows. c-class classiﬁcation problems one-hot coding scheme used i.e. rc×n labeled belongs class; otherwise. regression problems target values centered ﬁrst. then equals target value labeled; otherwise. linear kernel function chosen label kernel matrix i.e. trade-off hyper-parameter. solution eigenvectors kxkx corresponding largest eigenvalues. outline mida smida summarized algorithm iii.. statements brackets correspond specialized smida. besides variance label dependence another useful property data geometry structure preserved manifold regularization conveniently incorporated smida. experiments adding generally increases accuracy slightly cost three hyper-parameters. consequently adopted paper. section ﬁrst conduct experiments synthetic datasets verify effect proposed methods. then drift correction experiments performed enose datasets spectroscopy dataset. show universality proposed methods evaluate visual object recognition dataset. comparison made recent unsupervised domain adaptation algorithms learn domain-invariant features. fig. mida compared dataset discrete domains. domain labels used construct domain features mida according one-hot coding scheme introduced section iii-a. similar deﬁnition used synthetic datasets methods linear kernel used original features hyper-parameter order quantitatively assess effect domain adaptation logistic regression models trained labeled source data tested target data. accuracies displayed caption showing order performance mida original feature. aligns domains ﬁrst projected dimension. however classes large overlap dimension direction alignment different discrimination. incorporating label information source domain help. contrary mida align domains well domainspeciﬁc bias second dimension brought feature augmentation played role. explanation included supplementary materials. thus good accuracy obtained using dimensions classiﬁcation. fig. mida compared dataset continuous distributional change resembles timevarying drift machine olfaction. samples classes drift upper right. chronological order samples used construct domain features mida i.e. ﬁrst sample second sample etc. parameter setting mida fig. whereas number stationary components classiﬁcation accuracies obtained training logistic regression model ﬁrst halves data classes testing last halves. succeeds ﬁnding direction free time-varying drift. however classes cannot well separated direction. plot randomly scattered colors suggest time-varying drift totally removed subspace. mida ﬁrst mapped data space third dimension time projected plane orthogonal direction drift space. label information used last experiments. keeping label dependence subspace priority smida adopted instead mida. synthetic dataset fig. best direction align domains also mixes classes results output mida plot labels source domain used learning subspace. plot observe classes separated. fact class separation still found third dimension space learned mida. however purpose dimensionality reduction generally hope keep important information ﬁrst dimensions. nonlinear kernels often applied machine learning algorithms data linearly separable. besides also useful domain adaptation domains linearly alignable shown fig. plot inter-domain changes distributions different classes. hence difﬁcult linear projection direction align domains even domainspeciﬁc biases mida. actually domain-speciﬁc rotation matrices needed. since target labels available rotation matrices cannot obtained accurately. however nonlinear kernel used original features space higher dimensions domains linearly alignable. applied kernel width although domains perfectly aligned plot classiﬁcation model trained source domain better adapted target domain. comparison different kernel kernel parameters synthetic datasets included supplementary materials. sensor array drift dataset collected vergara dedicated research drift correction. total samples collected e-nose sensors course months. different kinds gases different concentrations. split batches authors according acquisition time. table supplementary material details dataset. classify type gases despite concentrations. similar took samples batch labeled training samples whereas batches unlabeled test ones. evaluation strategy resembles situation real-world applications. dataset sample represented features extracted sensors’ response curves feature ﬁrst normalized zero mean unit variance within batch. timevarying drift preprocessed features across batches visually inspected fig. obvious samples different batches different distributions. next labeled samples batch adopted source domain unlabeled ones batch target domain. proposed algorithms together several recent ones used learn domain-invariant features based samples. then logistic regression model trained source domain tested target one. multiclass classiﬁcation one-vs-all strategy utilized. fig. comparison mida synthetic dataset. plots show data original space projected spaces mida respectively. chronological order sample indicated color. classiﬁcation accuracies fig. comparison mida smida synthetic dataset. plots show data original space projected spaces mida smida respectively. classiﬁcation accuracies fig. comparison different kernels synthetic dataset. plots show data original space projected spaces mida linear kernels respectively. classiﬁcation accuracies fig. scatter ethanol acetone samples batches sensor array drift dataset. samples projected subspace using pca. different colors indicate different batches. geodesic kernel manifold regularization combination informationtheoretical structural correspondence learning marginalized stacked denoising autoencoder methods hyper-parameters tuned best accuracy. kpca sstca proposed mida smida polynomial kernel degree used. kpca learned subspace based union source target data. sstca mida smida eigenvalue decomposition needs done kernel matrices. order reduce computational burden randomly chose samples target domain using methods twice number samples source domain. used generate subspaces source target domains. subspace dimension determined according subspace disagreement measure results ml-comgfk copied pivot features binarized training pivot predictors using logistic regression. also compared several variants methods. table notation means discrete domains used mida smida similar compared methods. domain feature vector sample thus source domain target. however strategy cannot make samples intermediate batches. intuitive assumption distributions adjacent batches similar. adapting information batch taking samples batches consideration improve generalization ability learned subspace. concretely samples randomly selected batches instead batch alone. sample domain feature deﬁned batch index viewed proxy acquisition time. mida smida maximized independence table batch index increases accuracies methods generally degrade conﬁrms inﬂuence time-varying drift. continuous smida achieves best average domain adaptation accuracy. continuous versions mida smida outperform discrete versions proving proposed methods effectively exploit chronological information samples. also surpass ml-comgfk uses samples intermediate batches build connections source target batches. feature augmentation important dataset since removing continuous smida causes drop four percentage points average accuracy. fig. average classiﬁcation accuracies varying subspace dimension shown. mida smida better methods features extracted. noninvasive approach disease screening monitoring e-noses attracting attention concentration biomarkers breath proved related certain diseases makes possible analyze person’s health state e-nose conveniently. example concentration acetone diabetics’ breath often higher healthy people however instrumental variation time-varying drift e-noses hinder popularization technology real-world applications. unsupervised domain adaptation algorithms applied solve problem. collected breath analysis dataset years using e-noses model paper samples diseases selected experiments including diabetes chronical kidney disease cardiopathy lung cancer breast cancer. proved related certain breath biomarkers. performed binary-class classiﬁcation tasks distinguish samples disease healthy samples. sample represented steady state responses nine sensors e-nose. sensor used sense sample response reach steady state minutes. steady state response close relationship concentration measured gas. therefore feature vector contains information needed disease screening. show instrumental variation time-varying drift dataset draw steady state responses sensors samples fig. data point indicates breath sample. plot sensitivity sensor devices gradually decayed time elapsed. plot aging effect signiﬁcant replace sensors devices ones case signal suggest concentration high concentration addition responses different devices different numbers samples classes respectively. chose ﬁrst samples collected device class labeled training samples. among samples samples randomly selected class validation rest testing. hyper-parameters tuned validation sets. logistic regression adopted classiﬁer f-score accuracy criterion. results compared table kpca sstca mida smida kernel used. methods stationary subspace analysis mida smida capable handling chronological information simply regarded device discrete domain learned device-invariant features them. strategy used discrete mida smida. continuous mida smida fig. illustration instrumental variation time-varying drift breath analysis dataset. plots show steady state responses samples sensors respectively. domain features deﬁned according exact acquisition time converted years number devices ndev naturally considers chronological information treating sample stream multivariate time series identifying temporally stationary components. however cannot deal time series multiple sources multi-device case dataset. thus samples arranged chronological order despite device labels. table improvement made little possibly stationary criterion suitable preserving important properties data. example noise data also stationary mida smida achieved obviously better results methods. address instrumental variation time-varying drift. background-speciﬁc bias brought feature augmentation compensate change conditional probability dataset. smida better mida label information ﬁrst samples class better kept. similar e-noses data collected spectrometers one-dimensional signals indicating concentration analytes. instrumental variation also problem section test methods corn dataset. spectroscopy dataset collected three near-infrared spectrometers designated moisture protein starch contents corn samples measured device ranges measured values respectively. sample represented spectrum features. dataset resembles traditional domain adaptation datasets time-varying drift. three discrete domains deﬁned based three devices. adopt source domain target ones. domain samples assigned test rest training set. hyper-parameter tuning applied three-fold crossvalidation training sets three domains. best hyper-parameters determined algorithm regression model trained training source domain applied test target domains. regression algorithm ridge regression regularization parameter subspace dimension domains respect fig. applicable classiﬁcation problems. kpca sstca mida smida kernel used. semisupervised methods sstca smida target values normalized zero mean unit variance subspace learning. domain features deﬁned according device indices using one-hot coding scheme. domain adaptation done prediction error large. domain adaptation algorithms managed signiﬁcantly reduce error. kpca also good performance probably source target domains similar principal directions also contain discriminative information. therefore source regression models target samples well. dataset different domains identical data composition. result corresponding data aligned subspaces alignment explains small error however condition hold datasets. mida smida obtained lowest average errors target domains. aiming exploring prediction accuracy instrument variation trained regression models training target domains tested domain. results listed train target table iii. found smida outperforms results. could attributed three reasons inter-domain discrepancy dataset relatively easy correct; kernel smida improves accuracy; smida learned subspace basis training test samples. although test samples unlabeled provide information distribution samples make learned subspace generalize better viewed merit semi-supervised learning. testify assumption conducted another experiment multiple target domains. training samples source domain test ones target domains leveraged together subspace learning mida smida. average rmse target domains mida smida. compared results table single target domain results improved showing incorporating unlabeled samples target domains beneﬁcial. gong evaluated domain adaptation algorithms four visual object recognition datasets namely amazon caltech- dslr webcam common classes selected them samples class domain images total. image encoded -bin histogram using surf features. normalized histograms z-scored zero mean unit variance dimension. following experimental setting provided sample code authors experiments conducted random trials pair domains. unsupervised trail labeled samples class smida explained fact hsic criterion used mida used identical certain conditions source target domain besides feature augmentation strategy mida crucial dataset change conditional probability. hand sstca handle source target domains. sstca uses manifold regularization strategy preserve local geometry information hence introduces three hyper-parameters smida. moreover computing data adjacency graph sstca matrix inversion operation sstca make slower mida smida. compared speed domain adaptation experiment server intel xeon ram. parallel computing used. codes algorithms written matlab average running times trial mida smida sstca respectively. therefore mida smida practical sstca. besides initially designed drift correction. dataset used show universality. paper introduced maximum independence domain adaptation learn domain-invariant features. main idea mida reduce inter-domain discrepancy maximizing independence learned features domain features samples. domain features describe background information sample domain label traditional domain adaptation problems. ﬁeld sensors measurement device label acquisition time collected sample expressed domain features unsupervised drift correction achieved using mida. feature augmentation strategy proposed paper adds domainspeciﬁc biases learned features helps mida align domains. randomly chosen source domain training unlabeled samples target domain made test set. semi-supervised trails three labeled samples class target domain also assumed labeled. averaged accuracies pair domains well standard errors listed tables low-rank transfer subspace learning domain adaptation shifting covariance recent method called integration global local metrics domain adaptation copied best results reported original papers methods tested hyper-parameters tuned best accuracy. logistic regression adopted classiﬁer. polynomial kernel degree used kpca sstca mida smida. domain features deﬁned according domain labels using onehot coding scheme. mida smida achieve best average accuracies unsupervised semi-supervised visual object recognition experiments. observe sstca comparable performance mida mida smida ﬂexible algorithms. design domain features hsic criterion applied kinds domain adaptation problems including discrete continuous distributional change supervised/semi-supervised/unsupervised multiple domains classiﬁcation regression etc. also easy implement fast requiring solve eigenvalue decomposition problem. future directions include extending deﬁnition domain features applications. zhang tian kadri xiao zhou on-line sensor calibration transfer among electronic nose instruments monitoring volatile organic chemicals indoor quality sens. actuators chem. vol. fernando habrard sebban tuytelaars unsupervised visual domain adaptation using subspace alignment proceedings ieee international conference computer vision gong grauman learning kernels unsupervised domain adaptation applications visual object recognition international journal computer vision vol. barshan ghodsi azimifar jahromi supervised principal component analysis visualization classiﬁcation regression subspaces submanifolds pattern recogn. vol.", "year": 2016}