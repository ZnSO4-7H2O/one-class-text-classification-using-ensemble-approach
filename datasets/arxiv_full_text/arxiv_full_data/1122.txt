{"title": "Variational Probability Flow for Biologically Plausible Training of Deep  Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The quest for biologically plausible deep learning is driven, not just by the desire to explain experimentally-observed properties of biological neural networks, but also by the hope of discovering more efficient methods for training artificial networks. In this paper, we propose a new algorithm named Variational Probably Flow (VPF), an extension of minimum probability flow for training binary Deep Boltzmann Machines (DBMs). We show that weight updates in VPF are local, depending only on the states and firing rates of the adjacent neurons. Unlike contrastive divergence, there is no need for Gibbs confabulations; and unlike backpropagation, alternating feedforward and feedback phases are not required. Moreover, the learning algorithm is effective for training DBMs with intra-layer connections between the hidden nodes. Experiments with MNIST and Fashion MNIST demonstrate that VPF learns reasonable features quickly, reconstructs corrupted images more accurately, and generates samples with a high estimated log-likelihood. Lastly, we note that, interestingly, if an asymmetric version of VPF exists, the weight updates directly explain experimental results in Spike-Timing-Dependent Plasticity (STDP).", "text": "states ﬁring rates adjacent neurons. local rules also beneﬁcial machine computation. weight updates local case backpropagation necessary store weights activities massive neural network memory gpu. schemes bypass need store weights often price slower convergence optimal solution spike-timing-dependent plasticity another attribute biological networks rule states change weights increases time interval pre-synaptic post-synaptic spike decreases sign change depends temporal ordering spikes. pre-synaptic neuron spikes post-synaptic neuron weight synapse increases almost increase probability post-synaptic spike next pre-synaptic spike. order spikes switched synaptic weight decreases. bengio proposed novel weight update showed simulation gives rise stdp. update rule justiﬁed using learning framework energy-based objective function. main contribution paper proposal learning algorithm variational probability flow biologically plausible following ways. update rule given weight local. depends binary-valued states realvalued ﬁring rates adjacent neurons. asymmetric version update explains stdp. model neurons binary-valued real-valued. model generative output targets required. updates avoid feedback paths confabulations. updates allow intra-layer connections trained. fact asymmetric update discovered pre-synaptic state post-synaptic ﬁring rate. form different update proposed ∆wij depends rate post-synaptic transitions rate change integrated post-synaptic activity. quest biologically plausible deep learning driven desire explain experimentally-observed properties biological neural networks also hope discovering efﬁcient methods training artiﬁcial networks. paper propose algorithm named variational probably flow extension minimum probability training binary deep boltzmann machines show weight updates local depending states ﬁring rates adjacent neurons. unlike contrastive divergence need gibbs confabulations; unlike backpropagation alternating feedforward feedback phases required. moreover learning algorithm effective training dbms intra-layer connections hidden nodes. experiments mnist fashion mnist demonstrate learns reasonable features quickly reconstructs corrupted images accurately generates samples high estimated log-likelihood. lastly note that interestingly asymmetric version exists weight updates directly explain experimental results spiketiming-dependent plasticity immense success deep learning machine learning artiﬁcial intelligence signiﬁcant interest determining extent explains learning biological neural networks. example bengio pointed several challenges ﬁnding biologically plausible implementations backpropagation need precisely-clocked linear feedback paths exact knowledge weights derivatives involved feedforward paths. moreover backpropagation requires output targets passes real-valued signals rather binary-valued spikes neurons contrastive divergence hand trains binary generative model involves gibbs sampling generate data confabulations computation negative gradient unclear implemented biologically. minimally update rules biologically plausible learning algorithm local. precisely change weight synapse depend information directly accessible synapse fully-observed boltzmann machines ﬁrst apply training fully-observed boltzmann machines deﬁning connectivity states one-hop neighbors i.e. differ bit. given training data objective function becomes note transition rate one-hop neighbor differs j-th bit. words ﬁring rate poisson process determines ﬂipping state j-th vertex. please refer supplementary material detailed proofs. name suggests variational extension minimum probability derive variational objective function training boltzmann machines observed hidden variables prove upper bound probability observed variables. outline variational expectation-maximization algorithm minimizes objective experiments restricted boltzmann machines show able quickly learn features resemble strokes reconstruct corrupted digaccurately previous methods. deep boltzmann machines also propose strategy require greedy layer-wise pre-training moreover applies directly intra-layer connections signiﬁcantly improve model’s ability learn sparse representations. applied mnist fashion mnist data experiments show able generate realistic samples estimated log-likelihoods similar generative adversarial networks future work explore local learning rules exploited design model-parallel algorithms efﬁciently gpus. boltzmann machines energy-based probabilistic models given undirected graph without loops multiple edges associate binary random variable real bias vertex real weight edge probability vector exp) energy bixi temperature. simplicity paper. statistical learning useful designate variables observed others hidden. probability observed state vector obtained marginalizing hidden states. paper multilayer networks refer layered structure connections either within layers consecutive layers. instance special case layers ﬁrst layer observed others hidden. special cases dbms underlying graph edges consecutive layers hi+. dbms intra-layer connections studied well. minimum probability flow maximum likelihood estimation intractable many important classes statistical models. overcome problem sohl-dickstein designed learning objective known minimum probability flow ﬁnite state models sampled effectively using suitable continuous-time markov chain ctmc parameterized probability state time markov chain. denote empirical distribution objective function consider boltzmann machine observed hidden variables samples corresponding empirical distribution. associated ctmc. goal minimize marginal probability represents marginal probability objective function therefore hold constant minimize conditional distributions minimum occurs however possible compute latter distribution unless joint distribution time known. therefore approximate assuming ctmc close stationary distribution select approximation implies upper bound bound improves tends model biologically desirable perform optimizations constraining weights bounded space could adding weight decay term substituting latter integral average samples optimizing resulting stochastic objective parameters still converge limit sample conditional distribution boltzmann machine hidden states could initialized randomly applying gibbs sampling. discuss approximate sampling schemes next section. experiments observe enough generate sample achieve good convergence learning algorithm. multilayer networks variational algorithm allows train hidden variables. section discuss strategies improving performance hidden variables layered structure. rbm. rbms hidden variables conditionally independent given observed variables bernoulli probabilities computed sampled efﬁciently machine. pseudocode applying rbms given algorithm. stochastic gradient descent exploited training minor change. traditionally performed deep learning parameters updated minibatch. start epoch perform e-step entire data sampling hidden states data point using parameters attained last epoch. observed-hidden pairs parables longer conditionally independent given observed variables strategy training train pair layers starting generating data trained. here propose different strategy require greedy layer-wise training. approximate bernoulli probabilities using weighted inputs zeroing inputs hi+. sample probabilities proceeding sample hi+. generated hidden states used m-step weights every layer updated time. pseudocode training shown algorithm. generate confabulations given start layer binary vector picked uniformly priors. perform gibbs sampling steps alternating experiments seems enough ensure good mixing. ﬁnal state used initialize next gibbs sampling steps generating pseudocode process given algorithm. intra-layer connections. introduce employ train dbms intra-layer connections viewed stack rbms connections hidden nodes fig... train generalized dbms take additional step performing asynchronous gibbs updates hidden units given values previous update. specifically update neurons ﬁxed order sampling j-th neuron probability kjhk bj). denote intrainter-layer weights respectively. updates intra-layer weights also follow local rules confabulation-generating procedure additional step involving intra-layer gibbs updates. modiﬁcations indicated algorithm. traditional associative memories intra-layer connections help model learn rich energy landscapes sparse distributed representations otherwise impossible disconnected layers factorial distributions. embracing hope learn observed data hidden priors greater explanatory power. usually assumes absence connections within layer derive tractable training algorithms. overcomes limitation combination probability variational techniques. experiments explore effect intra-layer connections model’s performance generative tasks. stdp. first present simpliﬁed model learning biological neural networks. neuron model either excited state resting state time takes excited neuron transit resting state follows exponential distribution rate time resting neuron become excited occurs rate exp. synaptic weight updated post-synaptic neuron transits pre-synaptic neuron excited. change value proportional post-synaptic rate transition post-synaptic state transition affects sign update. update rule local sense depends states ﬁring rates adjacent neurons. weights symmetric combine updates ∆wij ∆wji gives update show update explains stdp. suppose pre-synaptic spike post-synaptic spike. speciﬁcally time cases consider either pre-synaptic neuron still excited pre-synaptic neuron returned resting state ﬁrst case occurs probability weight update ∆wij here expected time transition inverse transition rate. second case occurs probability weight update thus expected weight change suppose instead pre-synaptic spike happens post-synaptic spike again cases either post-synaptic neuron still excited post-synaptic neuron returned resting state ﬁrst case occurs probability exp. presynaptic spike immediate weight change ﬁrst post-synaptic state takes place. update ∆wij ﬁrst ﬂip. second case happens probability exp. however absence stimuli post-synaptic neuron become excited pre-synaptic neuron returns rest. hence update overall average weight change plotting average weight updates derived fig. good experimental measurements stdp. future hope statistical model that unlike require symmetry whose update rule precisely given image reconstruction rbms explored expressiveness model comparing rbms trained persistent contrastive divergence terms ability reconstruct corrupted parts mnist digits. reconstruct images markov chains starting corrupted images performed gibbs transitions make well. found gibbs transitions enough good results. codes derived fig. able reconstruct initially corrupted pixels different corruption patterns. reconstruction errors measured norm pixel differences original image reconstructed image i.e. using number hidden units errors various training methods averaged mnist test samples independent experiments reported table. performs much better pcd-. also better pcd- cases. although pcd- slightly better right case takes longer time gibbs transitions training reconstruction. deep boltzmann machines performance train dbms evaluated datasets mnist fashion mnist. dataset trained dbms architecture based algorithm. ﬁrst intra-layer connections second does. fig. illustration. fashion mnist scaled thresholding. adam optimizer used optimization default values parameters e.g. learning rate weight decay parameter rbms dbms. used mini-batch size code implemented theano details found supplementary material. restricted boltzmann machines ﬁrst experiment trained three rbms using algorithm. denote number hidden units. fig. shows randomly selected weight ﬁlters rbms. models learned reasonable ﬁlters sharper rbm. also noticed training objective converged quickly. shown fig.. curves ﬂattened less epochs. verify learns distributed representation plotted histogram mean activations hidden units averaged training examples. shown fig.. majority hidden units mean activations overall mean activation reasonably sparse mnist data. also tracked metrics weight sparsity squared weight given |x||h| dimensions weight visible unit hidden unit weight sparsity rough measure number nonzero weight parameters. decreased training shown fig.. suggests many weights near zero. remaining nonzero weights grew value training seen increasing squared weight fig.. high-contrast features fig. results demonstrate rbms trained operate compositional phase distributed representations table average reconstruction error corrupted mnist digits. rows columns randomly corrupted patterns e.g. means rows corrupted. tested gibbs updates pcd. domly states equal probability initialize bernoulli prior computed mean feedforward activations training dataset conjecture absence intra-layer connections bottom-up signals crucial initializing factorial priors good neighborhood. second strategy used well generative task. confabulations generated trained dbms using algorithm. displayed fig. fig. also estimated log-likelihood mnist test samples ﬁtting gaussian parzen density window samples generated learned dbms standard deviation parzen density estimator chosen using validation following procedure estimated log-likelihoods reported table. mnist. table. clearly dbm-p dbm-p able achieve log-likelihoods much higher using high layers. also perform better many previous generative models deep boltzmann machines deep belief networks stacked contractive auto-encoders log-likelihood dbm-p slightly worse deep generative stochastic nets dbm-p better thanks intralayer connections. fact log-likelihood dbm-p quite close generative adversarial nets away seen fig. trained dbms generate realistic digit samples. also note produced better samples dbm-r achieved log-likelihood dbm-r achieved outperforms dbm-p good dbm-p. consistent conjecture intra-layer connections enable learning sparse distributed representations highly-explanatory latent priors bottom-up signals required. meanwhile found often produced digits undesirable gaps fig.. lower log-likelihood. postulate gaps dbms exploring factorial prior phenomenon commonly observed drawback resolved intra-layer connections enable non-factorial priors. indeed obvious gaps digits dbm-r dbm-p. fashion mnist. trained dbms fashion mnist data contains kinds fashion images zalando fig. learned dbms generate meaningful samples t-shirts coats ankle boots without tricks learning procedure hyper-parameters. generated samples look good samples mnist. likely reason thresholding damages fashion mnist data signiﬁcantly. samples dbm-r dbm-r shown space constraints. work highly related able train deep generative models without backpropagation using variational ideas making sense stdp. however work differs aspects. first different weight update rules explain stdp stated introduction. second employ different strategy avoid backpropagation i.e. update rule instead target propagation. another similar work also explores stdp using energy-based models equilibrium propagation equilibrium propagation objective function data energy network loss model prediction output target trade-off parameter. generative rather discriminative model minimize instead minimizing network energy optimizes exp/ one-hop neighbors data point objective kind exponential transition energy rather form state energy. compare existing methods train rbms dbms. differences explicit objective function require gibbs sampling generate confabulations compute gradient gives rise stdp. training dbms instead employing two-stage process i.e. pre-training stack rbms separate recognition model initialize updating different dbms left traditional right intra-layer connections. digits generated dbms different initialization hidden layer cherry-picked. e.g. dbm-p refers intra-layer connections generating samples mean activation prior. greyscale images represent probabilities visible units. salakhutdinov larochelle learns parameters simultaneously without greedy layer-wise initialization weights require data confabulations. moreover trains neural nets intra-layer connections hard achieve conventional methods. recently many groups worked embedding stdp deep learning biologically plausible training neural nets e.g. found stdp-like behavior recurrent spiking networks proposed framework training energy-based models without explicit backpropagation. another stab direction. even advancements still done. instance question whether possible even necessary away symmetric weights energy-based models achieve biological plausibility. studies suggest methods batch-normalization could help ameliorate weight transport problem paper derived training objective learning algorithm known variational probability flow gives rise biologically plausible training deep neural networks. three main directions future work. firstly interested extending recurrent spiking networks believe closer biological networks. hope also uncover directed version whose weight updates given simply ∆wij −yiαjδj eliminate requirement symmetric weights. secondly neurons compute using binary values real-world data mostly represented real values. therefore hope design natural interfaces data types efﬁcient learning. finally explore local learning rules exploited design model-parallel algorithms efﬁciently gpus. algorithms enable train massive neural networks tens billions neurons distributed energy-efﬁcient manner. shaowei funded sutd grant sres sutd-zju grant zjurp. zuozhu supported sutd president’s graduate fellowship. would like thank nvidia computational support. also thank christopher hillar ganesh binghao dewen thiparat chotibut gary phua zhangsheng helpful work discussions.", "year": 2017}