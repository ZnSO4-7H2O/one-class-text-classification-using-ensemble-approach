{"title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural  Sequence Models", "tag": ["cs.LG", "cs.CL", "cs.NE", "I.2.7; I.2.6"], "abstract": "Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric (e.g. Hamming loss) evaluated on the output of beam search. While well-defined, this \"direct loss\" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines.", "text": "hypothesize under-performance beam search certain scenarios resolved using better designed training objective. beam search potentially offers accurate search compared greedy decoding hope appropriately trained models able leverage beam search improve performance. order train models effectively make beam search propose training procedure focuses ﬁnal loss metric evaluated output beam search. well-deﬁned valid training criterion direct loss objective discontinuous thus difﬁcult optimize. hence approach form sub-differentiable surrogate objective introducing novel continuous approximation beam search decoding procedure. experiments show optimizing training objective yields substantially better results sequence tasks compared cross-entropy trained greedy decoding cross-entropy trained beam decoding baselines. several related methods including reinforcement learning imitation learning discrete search based methods also proposed make training searchaware. methods include approaches forgo direct optimization global training objective instead incorporating credit assignment search errors using methods like early updates explicitly track reachability gold target sequence search procedure. addressing related problem credit assignment search errors training paper propose approach novel property directly optimize continuous global training objective using backpropagation. result approach credit assignment handled directly gradient optimization end-to-end computation graph. closely related work approach proposed goyal consider beam search develop continuous approximation greedy decodbeam search desirable choice test-time decoding algorithm neural sequence models potentially avoids search errors made simpler greedy methods. however typical cross entropy training procedures models directly consider behaviour ﬁnal decoding method. result cross-entropy trained models beam decoding sometimes yield reduced test performance compared greedy decoding. order train models effectively make beam search propose training procedure focuses ﬁnal loss metric evaluated output beam search. well-deﬁned direct loss objective discontinuous thus difﬁcult optimize. hence approach form sub-differentiable surrogate objective introducing novel continuous approximation beam search decoding procedure. experiments show optimizing training objective yields substantially better results sequence tasks compared cross entropy trained greedy decoding cross entropy trained beam decoding baselines. sequence-to-sequence models successfully used many sequential decision tasks machine translation parsing summarization dialog generation image captioning beam search desirable choice test-time decoding algorithm models because potentially avoids search errors made simpler greedy methods. however typical approach training neural sequence models locally normalized maximum likelihood objective objective directly reason behaviour ﬁnal decoding method. result cross-entropy trained models beam decoding sometimes yield reduced test performance compared greedy decoding negative results unexpected. training procedure search-aware able consider effect changing model’s scores might scheduled sampling objectives. related work involves training generator gumbel reparamterized sampling module reliably sequences decode-time constructing surrogate loss functions close task losses. denote seqseq model parameterized denote input sequence gold output sequence result beam search beam). ideally would like directly minimize ﬁnal evaluation loss evaluated result running beam search input model throughout paper assume evaluation loss det= refer idealized training objective directly evaluates prediction loss direct loss objective deﬁne unfortunately optimizing objective using gradient methods difﬁcult objective discontinuous. sources discontinuity describe later detail beam search decoding involves discrete argmax decisions thus represents discontinuous function. introduce surrogate training objective avoids problems result fully continuous. order accomplish this propose continuous relaxation composition ﬁnal loss metric decoder function beam speciﬁcally form continuous function softlb seeks approximate result running decoder input evaluating result using introducing module able construct surrogate training objective speciﬁed detail section surrogate objective equation additionally take hyperparameter trades approximation quality smoothness objective. certain conditions equation converges objective equation increased. ﬁrst describe standard discontinuous beam search procedure training approach involving continuous relaxation beam search. discontinuity beam search formally beam search procedure hyperparameter maintains beam elements time step expands elements k-best candidates next time step. procedure ﬁnds approximate argmax scoring function deﬁned output sequences. describe beam search context seqseq models algorithm speciﬁcally encoderdecoder model nonlinear auto-regressive decoder deﬁne global model score sequence length local output scores time step seqseq model neural models function implemented differentiable mapping r|h| yields scores vocabulary elements using recurrent hidden states corresponding time steps. notation hidden state decoder time step beam element embedding output symbol timestep beam element cumulative model score step beam element algorithm denote rk×|v cumulative candidate score matrix represents model score successor candidate vocabulary beam element. score obtained adding local output score using peaked-softmax operation propose iterative algorithm computing continuous relaxation top-k-argmax procedure algorithm takes input score matrix size returns peaked matrices size matrix represents index i-th max. example mass concentrated index matrix corresponds argmax mass concentrated index nd-highest scoring element. speciﬁcally obtain matrix computing squared difference i-highest score scores matrix using peaked-softmax operation negative squared differences. results scores closer i-highest score higher mass scores away i-highest score. hence continuous relaxation top-k-argmax operation simply implemented iteratively using operation continuous allows gradient backpropagation. vector converges hard index pairs representing hard backpointers successor candidates described algorithm ﬁnite introduce notion soft backpointer represented vector k-probability simplex represents contribution beam element previous time step beam element current time step. obtained row-wise values representing soft backpointers. describe approach detail algorithm illustrate soft beam recurrence step figure composing loss function beam search function optimization proposed equation make decomposability loss function across timesteps. thus sequence total loss experiments hamming loss easily computed time-step simply comparing gold exact computation vary according loss proposed procedure applicable long total loss decomposable across time-steps. decomposability loss strong assumption existing literature running total score candidate. function algorithms yields successive hidden states recurrent neural models like rnns lstms etc. embedding operation maps word vocabulary continuous embedding vector. finally backpointers time step beam elements previous time step also stored identifying best sequence conclusion search procedure. backpointer time step beam element denoted points elements previous beam. denote vector backpointers beam elements bt∗. follow-backpointer operation takes input backpointers candidates beam elements time step traverses sequence reverse following backpointers time step identifying candidate words associated backpointer results sequence length procedure described algorithm discontinuous top-k-argmax procedure returns pair vectors corresponding highest-scoring indices backpointers vocabulary items score matrix ˜st. index selection results hard backpointers time step restrict gradient backpropagation. next section describe continuous relaxation top-k-argmax procedure forms crux approach. continuous approximation top-k-argmax property approximation real valued vector argmax respect vector scores approximated temperature controlled softmax operation. argmax operation represented structured prediction made assumption often using decomposable losses surrogates nondecomposable ones. detail continuous relaxation beam search algorithm cumulative loss beam element time step embedding matrix target vocabulary size size embedding vector. algorithm discrete selection functions replaced soft continuous counterparts backpropagated through. results operations matrix vector operations ideal implementation. important aspect algorithm longer rely exactly identifying discrete search prediction since interested continuous approximation direct loss computation expressed soft beam search formulation eliminates sources discontinuities associated training objective equation computational complexity approach training scales linearly beam size hence roughly times slower standard training beam size since established pointwise convergence peaked-softmax argmax vectors unique maximum value establish pointwise convergence objective equation objective equation long ties among top-k scores beam expansion candidates time step. posit absolute ties unlikely random initialization weights domain scores empirically observe noticeable impact potential ties training procedure approach experimented different annealing schedules starting non-peaked softmax moving toward peakedsoftmax across epochs learning stable informative gradients. important cost functions like hamming distance high tend non-smooth generally regions away changepoints large gradient near changepoints makes optimization difﬁcult. decoding motivation behind approach make optimization aware beam search decoding maintaining continuity objective. however since approach doesn’t introduce model parameters optimization agnostic architecture seqseq model able experiment various decoding schemes like locally normalized greedy decoding hard beam search model trained. however reduce training procedure test procedure also experimented soft beam search decoding. decoding approach closely follows algorithm along soft back pointers also compute hard back pointers time step. computing relevant quantities like model score loss etc. follow hard backpointers obtain best sequence different hard beam decoding time step selection decisions made soft continuous relaxation inﬂuences scores lstm hidden states input embeddings subsequent time-steps. hard backpointers essentially estimate soft figure illustration approximate continuous beam search module obtain hidden states beam elements next time step starting hidden states corresponding beam elements current time step beam size ‘beam recurrence’ module expanded similar procedure carried ht+. comparison max-margin objectives max-margin based objectives typically motivated ankind surrogate training objective avoid discontinuities associated direct loss optimization. hinge loss structured prediction typically takes form input sequence gold target sequence output search space discontinuous cost function assume decomposable across time-steps sequence. finding cost augmented maximum score generally difﬁcult large structured models often involves searching output space computing approximate cost augmented maximal output sequence score associated beam search. procedure introduces discontinuities training procedure structured max-margin objectives renders amenable training backpropagation. related work incorporating beam search training neural sequence models involve costaugmented max-margin loss relies discontinuous beam search forward passes explicit mechanism ensure gold sequence stays beam training hence involve back propagation beam search procedure itself. also compute target score simply running forward pass lstm decoder gold target sequence. continuous approximation hinge loss optimized then ˜ghingeα max). empirically compare approach proposed approach optimize direct loss experiments. since goal investigate efﬁcacy approach training generic seqseq models perform experiments tagging tasks different characteristics output search spaces named entity recognition supertagging. seqseq models appropriate supertagging task long-range correlations sequential output elements large search space ideal considerably smaller search space weaker correlations predictions subsequent time steps. experiments observe improvements approach tasks. seqseq model bi-directional lstm encoder input sequence lstm decoder ﬁxed attention mechanism deterministically attends i-th input token decoding i-th output hence involve learning attention parameters. since computational complexity approach optimization scales linearly beam size instance impractical large beam sizes training. hence beam size beam search based experiments resulted improvements tasks discussed results. tasks direct loss function hamming distance cost aims maximize word level accuracy. named entity recognition named entity recognition conll shared task data german language provided data splits. perform preprocessing data. output vocabulary length peculiar characteristic problem training data naturally skewed toward default label sentences typically contain many named entities evaluation focuses performance recognizing entities. therefore modify hamming cost incorrect prediction doubly penalized compared incorrect predictions. hidden layers size label embeddings size mentioned earlier seqseq models ideal choice however wanted evaluate effectiveness approach different instantiations seqseq models. supertagging used standard splits bank training development testing. label space supertags much larger ner. distribution supertags training data exhibits long tail supertags encode speciﬁc syntactic information words’ usage. supertag labels correlated many tags encode similar information syntax. moreover task sensitive long range sequential decisions search effects holistically encodes syntax entire sentence. perform minor preprocessing data similar preprocessing task used hidden layers size supertag label embeddings also size standard evaluation metric task word level label accuracy directly corresponds hamming loss. hyperparameter tuning tuning hyperparameters related optimization trained models epochs picked models best performance development set. also multiple random restarts systems evaluated account performance variance across randomly started runs. pretrained models standard cross entropy training important stable optimization comparison report performance validation test sets tasks tables baseline model cross entropy trained seqseq model also used warm start proposed optimization procedures paper. baseline compared approximate direct loss training objective referred ˜gdlα tables approximate max-margin training objective referred ˜ghingeα tables. results reported models trained annealing also constant setting smooth inaccurate approximation original direct loss optimize. comparisons made basis performance models different decoding paradigms locally normalized decoding hard beam search decoding soft beam search decoding described section shown tables approach ˜gdlα shows significant improvements locally normalized baseline greedy decoding tasks improvement pronounced supertagging task surprising because evaluation metric tag-level accuracy congruent hamming loss ˜gdlα directly optimizes supertagging task sensitive search procedure tags across time-steps tend exhibit long range dependencies encode specialized syntactic information word usage sentence. another common trend observe annealing always results better performance training constant ˜gdlα ˜ghingeα shows stable training scheme smoothly approaches minimizing actual direct loss important proposed approach. additionally observe large difference soft approximation used decoding compared hard beam search decoding suggests approximation hard beam search effective discrete counterpart. supertagging observe baseline cross entropy trained model improves predictions beam search decoding compared greedy decoding accuracy points suggests beam search already helpful task even without search-aware training. optimization schemes proposed paper improve upon baseline soft direct loss optimization perner observe optimizing ˜gdlα outperforms approaches also observe interesting behaviour beam search decoding approximate maxmargin objective task. pretrained baseline model yields worse performance beam search done instead greedy locally normalized decoding. because training data heavily skewed toward label hence absolute score resolution different tags time-step decoding isn’t enough avoid leading beam search toward wrong hypothesis path. observed experiments hard beam search resulted predicting ‘o’s also hurt prediction tags future time steps hurt precision well recall. encouragingly ˜gdlα optimization even though warm started trained model performs worse beam search model becoming search aware resulted superior performance. however also observe approximate max-margin approach performs poorly here. attribute deﬁciency max-margin objective coupled approximate search methods like beam search provide guarantees ﬁnding supremum drive objective learn model scores search separately also experiments max-margin objective used hard beam search compute loss-augmented decodes. objective discontinuous evaluated performance gradient optimization nonetheless. included result tables found approach unstable considerably underperformed approximate max-margin direct loss objectives. best hypothesis difﬁcult value loss augmented decode gold sequence maintains higher model score. also warm started pre-trained model results worse performance beam search decode greedy decode observe adverse effect deﬁciency. result model scores gold hypothesis highly yields poor decoding outputs. observation indicates using max-margin based objectives beam search training actually achieve opposite original intent objective driven introducing search errors. observation optimization method improvements tasks–even hard beam search decoding trained model hurt performance–by making optimization search aware indicates effectiveness approach training seqseq models. beam search method choice performing search neural sequence models experiments conﬁrm necessarily guaranteed improve accuracy applied cross-entropy-trained models. paper propose novel method optimizing model parameters directly takes account process beam search continuous end-to-end sub-differentiable relaxation beam search composed ﬁnal evaluation loss. experiments demonstrate method able improve overall test-time results models using beam search test-time inference method leading substantial improvements accuracy.", "year": 2017}