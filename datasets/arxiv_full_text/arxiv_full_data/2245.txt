{"title": "Auditing Black-Box Models Using Transparent Model Distillation With Side  Information", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose a transparent model distillation approach to audit such models. Model distillation was first introduced to transfer knowledge from a large, complex teacher model to a faster, simpler student model without significant loss in prediction accuracy. To this we add a third criterion - transparency. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the teacher. Moreover, we use side information in the form of the actual outcomes the teacher scoring model was intended to predict in the first place. By training a second transparent model on the outcomes, we can compare the two models to each other. When comparing models trained on risk scores to models trained on outcomes, we show that it is necessary to calibrate the risk-scoring model's predictions to remove distortion that may have been added to the black-box risk-scoring model during or after its training process. We also show how to compute confidence intervals for the particular class of transparent student models we use - tree-based additive models with pairwise interactions (GA2Ms) - to support comparison of the two transparent models. We demonstrate the methods on four public datasets: COMPAS, Lending Club, Stop-and-Frisk, and Chicago Police.", "text": "features used create original model lack knowledge model type algorithm used train also original model available repeatedly query difficult determine close reverse engineered model original model. approaches proposed audit black-box models disparate impact removing permuting obscuring protected feature probing retraining black-box outcome model predictions change. usually focus protected features selected advance thus less likely detect biases known priori. assume access black model original training sample know features used inputs. gain insight black-box model treating teacher distilling output student model transparent somehow interpretable model distillation first introduced transfer knowledge large complex teacher model faster simpler student model transferring model function embedded data labeled function third requirement transparency. student model transparent understand making predictions match teacher’s outputs available side information help audit black-box risk scoring model. here side information actual outcome data point exactly true labels black-box risk scoring model intended predict first place. side information augments distillation several ways first reveals distortion present risk scores. consider risk score reliability diagrams figure compas stop-and-frisk risk scores well-calibrated empirical outcome chicago police risk score rather flat risk scores less exhibits sharp kink upwards. creators risk-scoring models distort scores achieve desired effects reduced sensitivity less important regions enhanced separation important parts scale. side information detect undo distortions risk scores distilling them. section shows this. second actual outcome side information train second transparent model predict actual outcomes compare transparent model transparent student model black-box teacher. student actual outcome models trained using transparent learning algorithm sample data using features differences using analogy active learning literature side information actual outcomes true label given oracle. being well-calibrated overall imply well-calibrated subgroups. abstract black-box risk scoring models permeate lives typically proprietary opaque. propose transparent model distillation approach audit models. model distillation first introduced transfer knowledge large complex teacher model faster simpler student model without significant loss prediction accuracy. third criterion transparency. gain insight black-box models treat teachers training transparent student models mimic risk scores assigned teacher. moreover side information form actual outcomes teacher scoring model intended predict first place. training second transparent model outcomes compare models other. comparing models trained risk scores models trained outcomes show necessary calibrate riskscoring model’s predictions remove distortion added black-box risk-scoring model training process. also show compute confidence intervals particular class transparent student models tree-based additive models pairwise interactions support comparison transparent models. demonstrate methods four public datasets compas lending club stop-and-frisk chicago police. introduction risk scoring models long history usage criminal justice finance hiring critical domains impact people’s lives designed predict future outcome example defaulting loan re-offending. worryingly risk scoring models increasingly used high-stakes decisions typically proprietary and/or opaque. approach detecting bias risk scoring models reverse engineer them i.e. train nearly identical model probed even re-trained varying conditions. however stymied lack access data ∗this work performed internship microsoft research. often available users individuals scored external parties assessment validation. legal scholars definitions bias disparate treatment disparate impact. example former explicitly using protected feature race gender deciding outcome. example latter different categories protected feature exhibiting disparate outcomes despite protected feature used explicitly decision. discussion concepts context machine learning models. since discovering disparate treatment black-box model requires access model least knowledge whether used protected feature research auditing black-box models centered uncovering disparate impact. figure reliability diagram empirical probability positive actual outcomes risk score probability scale logit probability scale lines best-fit straight lines. good suggests risk score logit probability actual outcomes linear relationship directly compared relationship linear risk score must calibrated prior comparison. blue monotonic curves learned nonlinear transformations. figure transformed risk score. bottom risk score histograms. third benefit side information look differences student outcome models models similar. example compas section observe student outcome models model important feature number priors almost identically. student outcome models strongly agree model effects features increases confidence student model faithful representation black-box teacher model differences observed features meaningful. approach comparing distilled student model anmodel type trained actual outcomes different training single transparent model actual outcomes seeing learned. similarities differences transparent student model black-box transparent model actual outcomes informative training model mimic black also informative training model data black-box’s original training data. suspect better perform distillation strong student model possible long model remains understandable. allows single global approximation used weaker models would require multiple local approximations. auditing black-box looking large number independent local approximations straightforward. approaches auditing black-box models complicated correlations among features particularly correlation protected non-protected features. approach immune this. disentangling correlations features difficult computationally expensive. studying models parameter values whereas model distillation extracts model’s learned knowledge data labeled model transparency interpretability definitions concepts still flux transparency interpretability highly correlated model complexity simpler models make easier humans understand model. calibration process matching predicted probabilities empirical rates data. standard calibration techniques include platt scaling isotonic regression whether risk score well calibrated across subgroups several fairness metrics. several papers shown impossible simultaneously achieve balance positive negative classes corbett-davies show example classifier well-calibrated intentionally designed hide racial disparities. calibration techniques undo distortions risk score scale. transparent model distillation black-box risk-scoring function. p-dimensional denote feature. data size data point features labels actual assigned black-box. outcome class transparent models. train student model class predict outputs teacher model want rich complex possible faithful student still simple enough remain understandable. experiment types transparent models paper gams linear models. generalized additive models pairwise interactions variant generalized additive models based shallow trees gams’ claim transparency stems additive form contribution feature prediction visualized graphs figure term ensemble shallow trees restricted operate feature ensemble shallow trees operating pairs features capture pairwise interactions. higher order interactions added increase accuracy less easy visualize thus difficult understand. classification logistic link. regression identity. model class trained data sample using features predict labels different intimately related hope models learn similar patterns make similar mistakes except differences risk scores true outcomes require models different. providing method calculating confidence intervals particular kind transparent model class experiment support comparing student models actual outcome models. related work place proposed approach context existing methods overview concepts used including model distillation transparency interpretability calibration. auditing black-box models several approaches proposed audit black-boxes disparate impact. datta propose feature importance measures account correlated features require access black-box model knowledge input data features used. others require query access black-box retraining aspects proposed approach hinted previous papers. main focus experiment adler trained model predict outcomes overfitted interpretable model predict first’s predictions. different distillation setup ours risk scores outcomes. adebayo kagal also learn risk scoring models black-box model cannot queried. like approach papers study models tandem risk scores outcomes time. wang train model predict outcomes another predict membership protected subgroup connection models identify features proxy protected features. chouldechova g’sell train different outcome models identify subgroups models differ terms fairness metric interest. compare classifiers whereas want uncover disparate impact risk-scoring model. papers work single model discovering subgroups miscalibrated predicting outcomes exhibiting significant associations protected features black-box outputs finally several fairness criteria interplay risk scores outcomes proposed subgroup wellcalibrated predictive parity error rate balance model distillation transfers learned mapping model inputs outputs. hinton point conceptual block hindered wider adoption approach association knowledge learned model learned calibration undo risk score distortions section figure example risk score distortions applied parts scale provide finer resolution. undo distortion make reasonable assumption risk scores monotonic well-calibrated specifically relationship risk score logit probability nonlinear case chicago police lending club risk scores learn blue line nonlinear monotonic transformation risk scores scale linearly related logit probability seen figure related common calibration method isotonic regression except transformation learned using monotonic spline instead monotonic stepped function giving additional smoothness. transformed risk scores instead risk scores input student models. risk scores already linear relationship logit probability calibration step needed risk scores used directly inputs student model. transparent distillation side information distilling black-box risk-scoring models side information actual outcomes help. risk score actual outcomes intimately related. train second transparent model class predict actual outcomes compare second transparent model transparent student model trained mimic black-box. student model black-box risk score teacher trained teacher’s outputs input label features output prediction model functional form model actual outcome input label features data points student model output prediction model functional form logit since binary. model student model actual outcomes labeled another model. transparent models assigns risk scores actual risk scores want compare make predictions. types transparent models experiment make predictions summing contributuons individual features pairs features. figure relationship logit empirical probability transformed risk score; similar middle figure x-axis risk score applying nonlinear transformation. lines best-fit straight lines. good suggests transformed risk score logit probability linear relationship compared. bottom transformed risk score histograms. model related additive function decomposition methods used example hyperparameter optimization examine importance interactions hyperparameters terms learned together using gradient boosting obtain additive formulation. however unlike classical gams features shaped using splines gams shape features using short trees. contribution paper devising variance estimate main feature contributions pairwise feature contributions models models used decision trees also suggested interpretable. used explicitly recently however transparency depends tree structure change dramatically fold fold feature contributions lack confidence intervals. full complexity models random forests gradient boosted trees neural networks accurate lack transparency feature contributions determined post-hoc using methods partial dependence feature importance measures etc. suffer bias extrapolation training manifold ignored correlations features contribution feature model given empirical results section describe results applying transparent model distillation four black-box models risk scores actual outcomes side information. compas recidivism risk outcome compas proprietary score developed predict recidivism risk subject scrutiny racial bias algorithm race input proponents suggest race-blind. propublica collected analyzed released data compas scores actual recidivism outcomes defendants broward county florida. candidates protected features data race sex. compas black-box model protected necessarily complex. know model type features data used train original compas model. figure shows four shape plots four features available recidivism prediction race number priors gender. shows learned transparent models trained predict compas risk score true recidivism outcome transparent model trained mimic compas model gives insight compas model works. transparent model trained true outcome shows learned data itself. point-wise confidence intervals shown models. bottom figure shows difference green terms along confidence intervals difference takes account covariace green terms. compas biased race groups. examining plots left figure mimic model green true outcome model similar ages confidence intervals plot overlap significantly confidence intervals difference plot usually include zero. greater number samples variance large evidence models disagree. difference compas mimic model true label model significant ages compas model apparently predicts risk young offenders evidence support model trained true labels risk appears highest young offenders. suggests interesting bias favoring young offenders compas model appear explained data. next graphs figure show risk function race. compas mimic model predicts african americans higher risk caucasians lower risk transparent model trained true labels suggests comparing transparent models comparing feature contributions models requires ensuring that feature contributions scale; differences random noise. scaling considerations section described nonlinear transformation transform risk scores linearly related logit probabilities. logit probability scale precisely scale outcome model represents effect contributions individual features since uses logit link. addition transforming scores rescale distilled models trained ensure provide predictions observed data standard deviation. multiply match standard deviation other. done account student models attenuated range predictions relative range scores trained predict consequence regression mean. cases score also trained using features available distillation leading greater smaller range predictions outcome model. feature contributions models scale compared. detecting differences want compare difference contribution feature score student model compared outcome model ohj. tell difference statistically significant section describe confidence interval constructed difference. controlling variability ensure differences models arising trained different data points train models data sample. however induces correlation feature contributions account estimating including confidence interval difference estimating sample variance contribution paper variance estimate models. employ bootstrap-of-little-bags approach originally developed bagged models obtain pointwise confidence intervals feature contributions difference models’ feature contributions. bootstrap-of-little-bags based two-level structured crossvalidation data points selected test set. remaining split training validation sets. repeat inner splitting times outer splitting times total training samples train student model outcome model xj’s feature contribution estimated specifically model inner outer fold. mean ensemble average training models estimate variance variance estimate conservative however given trying detect differences overestimating means less likely mistake random noisy differences real differences. large consistency estimate established confidence interval figure shaped feature contributions four features compas risk score student model actual recidivism outcome model categorical features race categories ordered decreasing importance score student model. blue line captures difference models plots mean-centered vertical axes. appendix additional features interactions pairs features. compas agrees data number priors. column compas mimic model true-labels model agree impact number priors risk error bars overlap range wide largest difference observed priors. gender opposite effects compas compared true outcome. column discrepancy compas mimic model true-labels model learned gender. compas model predicts females higher risk data suggests correct women males lower risk data suggests correct men. suspect difference arises training data males compas model good distinguishing male female could although space include figures here models identify number significant pairwise interactions gender features. part data predominantly male main effects correct males females interactions gender features allows model correct predominately male main effects. interactions make models accurate allowing model effects cannot represented main effects individual features. observe number interesting interactions gender features length stay number prior convictions. pairwise interactions compas included appendix. suspect compas black-box model complex enough properly model interactions might explain differences observe transparent student model train mimic compas transparent model trained true outcomes. example noted compas model appears biased favor young offenders evidence support true outcome model. strong interactions young variables gender charge degree length stay suspect compas able model explain compas needs predict risk young offenders lending club loan risk score defaults lending club online peer-to-peer lending company makes information public loans finances. subset five years loans matured lending club assigned risk score outcome whether loan defaulted. individual joint loans remove nonbaseline features loan payment information could leak information label. candidates protected features data include state code. know model type lending club used black-box risk-score model. believe lending club additional features available public dataset models features figure selected shaped main pairwise feature contributions lending club risk score student model actual loan default outcome model appendix additional main pairwise features. figure shows selected main pairwise features lending club data. remaining main pairwise features appendix. figure lines show learned transparent student model trained mimic lending model training risk scores assigned model green lines show transparent model learned trained true credit default labels. comparing green lines helps understand black-box lending model learned differs model could learned true labels. fico score models agree qualitatively fico score quantitatively student model suggests black-box gives weight fico model trained true outcomes. suspect true outcome model uses features black-box black-box model places greater emphasis important feature uses fico. year issued green bars graph demonstrate increase risk loan defaults exactly around time subprime crisis. bars risk-scoring model change dramatically years appearing behind actual risk. would make sense lending club black-box model updated conservatively instead rapidly updated economic conditions behavior change. pairwise interactions year loan issue home ownership status shows home mortgage increases lending club loan default risk home mortgage beyond. note difference ranges pairwise plots range goes outcome model whereas range much lower student risk scoring model supporting hypothesis lending club black-box model updated time. model. gives oppotunity test transparent model distillation side information accurately detect features used black-box model. comparing transparent student model trained mimic chicago police black-box second transparent model trained true outcomes models trained using features. transparent distillation providing highfidelity information black-box model able detect features used model. chicago police department released arrest data used create risk assessment score probability individual involved shooting incident victim offender. candidates protected features data include race age. know model type used black-box know features used model available. trained transparent student model mimic model intentionally included features student model. figure shows main effects learned student model eight features chicago police department used model figure shows main effects learned student model features department says used model. figures transparent student model learns trained mimic chicago police department model green transparent model learns trained true outcomes. striking difference plots figure figure little visible figure visible figure transparent model trained mimic black-box makes significant features used black-box little features used black-box. amount green figures similar suggesting signal available unused features police model could used chose use. confirms transparent student model provide insight inner workings black-box model demonstrates advantages using side information. comparing transparent student learns black-box transparent model learns true labels valuable train transparent model outcomes ways using available features could york police department’s stop-and-frisk data scrutinized racial bias goel proposed simple heuristic risk score model created data tested data probability individual possessing weapon denotes primary stop circumstance presence suspicious object denotes secondary stop circumstance sight sound criminal activity bulдe denotes bulge clothing apply risk scoring model label data following goel al.’s pre-processing steps. problem risk-scoring model black-box. know precise functional form using features train similar sample data. linear regression student models recover coefficients three features used miniscule variance. note easy risk-scoring model three features simple functional form. discussion including available features sometimes interested detecting potential bias features intentionally excluded black-box model. example credit risk scoring model probably allowed race input. unfortunately excluding race inputs prevent model learning biased. racial bias data likely outcomes labels used learning; using race feature remove bias labels. training transparent student model mimic black-box model intentionally include features originally used create black-box risk scores even protected features specifically interested examining models could learn them. indications missing features black-box used additional features access developed test assess impact missing features could analysis based following black-box risk scoring model access hidden features useful predicting outcome error student model risk score positively correlated error student model outcome model. table provides confidence intervals correlation score outcome residuals score student model based three correlation statistics. lending club stop-andfrisk cannot distinguish correlations zero. police compas appears evidence correlation indicating data sets student model access relevant features. however upper intervals never strong effect. http//www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page note risk score proposed academic research group nypd. since goel tested model data data train training testing samples similar. fidelity close student teacher assessed accurately student models predict teachers’ outputs test-sets. compas models roughly equal fidelity accuracy. fidelity models rmse lower scale. possible reasons compas score challenging predict include propublica data sample missing essential features. agrees findings test likelihood missing features proposed previous section. another possible explanation small size data advantage model distillation benefit additional unlabeled data black-box teacher queried label data found additional data points propublica data assigned risk scores actual outcomes. adding training sample student model retraining students find marginal improvement student model’s fidelity regardless opposite removing points training sample increments student’s fidelity decreases marginally analyses suggest compas missing salient features pressing lack data. terms cross-model class comparisons gam’s results generally comparable complex less intelligible models random forests risk score student models random forests competitive lending club chicago police data sets. linear logistic regression behind several data sets suggesting model functional form might simple. stopand-frisk data model functional form simple linear form unsurprisingly linear regression performs well whereas noisiness forests reduces accuracy. bias discovery transparency advantages using transparent models understand bias data bias black-box models trained data need know advance biases look for. examining black-box model often shows bias would anticipated advance. unexpected biases like discovered examining transparent model testing done study bias determine source. know look advance useful many kinds bias real data would known look design statistical tests test for. transparent models less powerful transparent models sufficiently expressive flexible pick nonlinear relationships. categorical features equivalent linear logistic regression. however continuous features advantage gams ability model nonlinear relationships. compare estimated effects linear logistic regression. however continuous features difference gams logistic regression significant. consider figure equivalent bottom figure using linear logistic regression transparent models instead gams. model able shape feature non-linear manner across entire range detected significant differences student risk score model outcome model young groups logistic regression ascribes number effect feature predicted outcome. resulted significant difference detected rich caruana johannes gehrke paul koch marc sturm noemie elhadad. intelligible models healthcare predicting pneumonia risk hospital -day readmission. kdd. decio coviello nicola persico. economic analysis black-white disparities york police department’s stop-and-frisk program. journal legal studies william dieterich christina mendoza brennan. compas risk scales demonstrating accuracy equity predictive parity. technical report. northpointe inc. andrew gelman jeffrey fagan alex kiss. analysis york city police department’s stop-and-frisk policy context claims racial bias. amer. statist. assoc. robert gibbons giles hooker matthew finkelman david weiss paul pilkonis ellen frank tara moore david kupfer. computerized adaptive diagnostic test major depressive disorder screening tool depression. journal clinical psychiatry andreas henelius puolamäki henrik boström lars asker panagiotis papapetrou. peek black exploring classifiers randomization. data mining knowledge discovery geoffrey hinton oriol vinyals jeff dean. distilling knowledge neural network. nips deep learning representation learning workshop. giles hooker. generalized functional anova diagnostics highdimensional functions dependent variables. journal computational graphical statistics francisco louzada anderson guilherme fernandes. classification methods applied credit scoring systematic review overall comparison. surveys operations research management science conclusion propose method audit black-box risk models potential bias using model distillation train transparent student model mimic black-box model comparing transparent mimic model transparent model trained using features true outcomes instead labels predicted black-box model. differences transparent mimic model truelabels model indicate differences black-box model makes predictions model trained true outcomes makes predictions highlighting potential biases black-box model. demonstrate method four public data sets. advantages approach transparent models accurate despite interpretable method generates reliable confidence intervals interpretation able undo distortions present black-box model’s predictions often detect features used black-box model need know advance biases look for. philip adler casey falk sorelle friedler gabriel rybeck carlos eduardo scheidegger brandon smith suresh venkatasubramanian. auditing black-box models indirect influence. icdm. julia angwin jeff larson surya mattu lauren kirchner. machine bias there’s software used across country predict future criminals. it’s biased blacks. https//www.propublica.org/article/machinebias-risk-assessments-in-criminal-sentencing accessed carolin strobl anne-laure boulesteix achim zeileis torsten hothorn. bias random forest variable importance measures illustrations sources solution. bioinformatics florian tramer vaggelis atlidakis roxana geambasu daniel jean-pierre hubaux mathias humbert juels huang lin. fairtest discovering unwarranted associations data-driven applications. ieee european symposium security privacy. sandra wachter brent mittelstadt chris russell. counterfactual explanations without opening black automated decisions gdpr. harvard journal technology wang berk ustun flavio calmon. direction discrimination information-theoretic analysis disparate impact machine learning. arxiv preprint arxiv. muhammad bilal zafar isabel valera manuel gomez rodriguez krishna gummadi. fairness beyond disparate treatment disparate impact learning classification without disparate mistreatment. icwww. figure shaped feature contributions remaining features compas risk score student model actual recidivism outcome model blue line captures difference models figure shaped pairwise feature contributions pairs compas risk score student model actual recidivism outcome model pairs rows consist continuous feature binary categorical feature. rest pairs capture interactions continuous features categorical features many levels. figure shaped pairwise feature contributions remaining pairs lending club risk score student model actual loan default outcome model pairs consist continuous feature binary categorical feature. rest pairs capture interactions continuous features categorical features many levels.", "year": 2017}