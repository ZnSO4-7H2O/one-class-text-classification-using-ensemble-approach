{"title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.", "text": "traditional topic models account semantic regularities language. recent distributional representations words exhibit semantic consistency directional metrics cosine similarity. however neither categorical gaussian observational distributions used existing topic models appropriate leverage correlations. paper propose mises-fisher distribution model density words unit sphere. representation well-suited directional data. hierarchical dirichlet process base topic model propose efﬁcient inference algorithm based stochastic variational inference. model enables naturally exploit semantic structures word embeddings ﬂexibly discovering number topics. experiments demonstrate method outperforms competitive approaches terms topic coherence different text corpora offering efﬁcient inference. prior work topic modeling mostly involved categorical likelihoods applications topic models textual domain treat words discrete observations ignoring semantics language. recent developments distributional representations words succeeded capturing certain semantic regularities explored extensively context topic modeling. paper propose probabilistic topic model novel observational distribution integrates well directional similarity metrics. employ semantic similarity euclidean distance word vectors reduces gaussian observational distribution topic modeling cosine distance word embeddings anpopular choice shown good measure semantic relatedness mises-fisher distribution well-suited model directional data previously applied topic models. work observational distribution. word viewed point unit sphere topics canonical directions. speciﬁcally hierarchical dirichlet process bayesian nonparametric variant latent dirichlet allocation automatically infer number topics. implement efﬁcient inference scheme based stochastic variational inference perform experiments different newsgroups english text corpora nips compare baselines gaussian lda. model spherical outperforms three systems measure topic coherence. instance shdp obtains gains gaussian nips dataset newsgroups dataset. qualitative inspection reveals consistent topics produced shdp. also empirically demonstrate employing leads efﬁcient topic inference. topic modeling word embeddings proposed topic model uses gaussian distribution word embeddings. performing inference vector representations words model encouraged group words semantically similar leading coherent topics. contrast propose utilize mises-fisher distributions rely cosine similarity word vectors instead euclidean distance. topic models distribution used model directional data placing points unit sphere reisinger propose admixture model uses model documents represented vector normalized word frequencies. account word level semantic similarities. unlike method word embeddings. addition model nonparametric. nonparametric topic models variants successfully applied topic modeling however models assume categorical likelihood words encoded one-hot representation. section describe generative process documents. rather one-hot representation words employ normalized word embeddings capture semantic meanings associated words. word document represented normalized mdimensional vector similarity words quantiﬁed cosine angle corresponding word vectors. model based hierarchical dirichlet process model assumes collection topics shared across documents corpus. topics represented topic centers since word vectors normalized viewed direction unit sphere. mises−fisher distribution commonly used model directional data. likelihood topic word figure graphical representation spherical model. symbol next random variable denotes parameter variational distribution. assume documents corpus document contains words countably inﬁnite topics represented normalization constant modiﬁed bessel function ﬁrst kind order interestingly log-likelihood proportional equal cosine distance vectors. distance metric also used mikolov measure semantic proximity. sampling document subset topics determine distribution words. denote topic selected word document hence drawn categorical distribution mult proportion topics document draw dirichlet process enables estimate number topics data. generative process generation document follows log-normal mult stick-breaking distribution concentration parameter dirichlet process concentration parameter stick proportions log-normal hyper-prior distributions concentrations centers topics respectively. figure provides graphical illustration model. setup perform experiments different text corpora documents newsgroups documents nips corpus. utilize -dimensional word embeddings trained text wikipedia using wordvec. vectors post-processed unit norm. evaluate model using measure topic coherence shown effectively correlate human judgement this compute pointwise mutual information using reference corpus documents wikipedia. calculated using cooccurence statistics pairs words -word sliding windows results table details topic coherence averaged topics produced model. observe shdp model outperforms glda points newsgroups points nips dataset. also individual topics inferred shdp stochastic variational inference rest paper bold symbols denote variables kind employ stochastic variational mean-ﬁeld inference estimate posterior distributions latent variables. enables sequentially process batches documents makes appropriate large-scale settings. approximate posterior distribution latent variables mean-ﬁeld approach ﬁnds optimal parameters fully factorizable qqqqq) maximizing evidence lower bound denotes dirichlet distribution parameters need optimize elbo. similar view parameter; hence δβ∗. prior distribution follow conjugate distribution; hence posterior closed-form. since dimensional variable importance sampling approximate posterior. batch size update equations parameters total number documents number word document total number words dictionary step size respectively. natural parameter function computing sufﬁcient statistics distribution topic table examples words coherent topics inferred nips dataset gaussian spherical hdp. last model topic coherence computed using wikipedia documents reference. lda. calculate normalized value loglikelihood subtracting minimum value dividing difference maximum minimum values. shdp converges faster g-lda requiring around iterations g-lda takes longer converge. classical topic models account semantic regularities language. recently distributional representations words emerged exhibit semantic consistency directional metrics like cosine similarity. neither categorical gaussian observational distributions used existing topic models appropriate leverage correlations. work demonstrate mises-fisher distribution model words points unit sphere. base topic model propose efﬁcient algorithm based stochastic variational inference. model naturally exploits semantic structures word embeddings ﬂexibly inferring number topics. show method outperforms three competitive approaches terms topic coherence different datasets. figure normalized log-likelihood training size documents nips corpus. since log-likelihood values comparable gaussian shdp normalize demonstrate convergence speed inference schemes models. make sense qualitatively higher coherence scores g-lda supports hypothesis using likelihood helps producing coherent topics. shdp produces topics newsgroups topics nips dataset. david newman karl grieser timothy baldwin. automatic evaluation topic coherence. human language technologies annual conference north american chapter association computational linguistics pages association computational linguistics. john paisley chingyue wang david blei michael jordan. nested hierarchical dirichlet processes. pattern analysis machine intelligence ieee transactions jeffrey pennington richard socher christopher manning. glove emglobal vectors word representation. pirical methods natural language processing pages joseph reisinger austin waters bryan silverthorn raymond mooney. proceedings spherical topic models. international conference machine learning pages thomas grifﬁths mark steyvers padhraic smyth. author-topic model authors documents. proceedings conference uncertainty artiﬁcial intelligence pages auai press.", "year": 2016}