{"title": "A Compositional Object-Based Approach to Learning Physical Dynamics", "tag": ["cs.AI", "cs.LG"], "abstract": "We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural network, it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that the NPE's compositional representation of the structure in physical interactions improves its ability to predict movement, generalize across variable object count and different scene configurations, and infer latent properties of objects such as mass.", "text": "present neural physics engine framework learning simulators intuitive physics naturally generalize across variable object count different scene conﬁgurations. propose factorization physical scene composable object-based representations neural network architecture whose compositional structure factorizes object dynamics pairwise interactions. like symbolic physics engine endowed generic notions objects interactions; realized neural network trained stochastic gradient descent adapt speciﬁc object properties dynamics different worlds. evaluate efﬁcacy approach simple rigid body dynamics two-dimensional worlds. comparing less structured architectures show npe’s compositional representation structure physical interactions improves ability predict movement generalize across variable object count different scene conﬁgurations infer latent properties objects mass. endowing agent program physical reasoning constrains agent’s representation environment establishing prior environment’s physics. agent leverage constraints rapidly learn tasks ﬂexibly adapt changes inputs goals naturally generalize reasoning novel scenes example foundational sense intuitive physics prior guides humans decompose scene objects carry expectations object boundaries motion across different scenarios humans perceive balls billiard table meaningless patches color rather impermeable objects. expect balls moving toward bounce certain collision rather pass other crumble pieces disperse smoke. replace billiard ball bowling ball expectations ball-to-ball interactions differ underlying sense inertia collisions remain. arrange immovable wooden obstacles table expectations ball’s surface interacts wood remain constant regardless obstacles arranged. ability plan trajectories space without relearn physics scratch time regardless whether three balls eight balls whether obstacles whether obstacles arranged another whether conﬁguration objects seen before suggests humans leverage prior physics reason level abstraction objects relations events primitive. paper explores question building prior agent program. view program simulator takes input provided physical scene past states objects outputs future states physical properties relevant objects goal design program naturally generalizes across variable object count different scene conﬁgurations without additional retraining. proposed framework neural physics engine outlines several ingredients useful realizing generalization capabilities. describe ingredients context speciﬁc instantiation applied two-dimensional worlds balls obstacles. general approaches emerged search program captures common-sense physical reasoning. top-down approach formulates problem inference parameters symbolic physics engine bottom-up approach learns directly observations motion prediction physical judgments. program top-down approach generalize across scenario supported entities operators description language. however brittle scenarios supported description language adapting scenarios requires modifying code generating code physics engine itself. contrast gradient-based bottom-up approaches apply model architecture learning algorithm speciﬁc scenarios without requiring physical dynamics scenario pre-speciﬁed. often comes cost reduced generality transferring knowledge scenes require extensive retraining even cases seem trivial human reasoning. takes step toward bridging expressivity adaptability combining strengths approaches. framework realized differentiable physics simulator combines rough symbolic structure gradient-based learning. exhibits several strong inductive biases explicitly present symbolic physics engines notion objects-speciﬁc properties object interactions. implemented neural network also ﬂexibly tailor speciﬁc object properties dynamics given world training. design extrapolate variable number objects different scene conﬁgurations spatially temporally local computation. framework proposes four ingredients useful generalization across variable object count different scene conﬁgurations without additional retraining. ﬁrst ingredient view objects primitives physical reasoning. second mechanism selecting context objects given particular object. together ingredients reﬂect natural assumptions physical environment exist objects objects interact factorized manner. third fourth ingredients factorization compositionality applied levels scene network architecture. level physical scene factorizes scene object-based representations composes smaller building blocks form larger objects. method representation adapts scene conﬁgurations variable complexity shape. level network architecture explicitly reﬂects causal structure object interactions factorizing object dynamics pairwise interactions. models future state single object function composition pairwise interactions context objects scene. structure serves guide learning towards objectbased reasoning designed physical knowledge transfer across variable number objects anywhere scene. previous bottom-up approaches coupled learning vision learning physical dynamics take different approach reasons. first disentangling visual properties object physical dynamics step toward achieving generality physics engine. vision dynamics necessary believe keeping functionalities separate important common-sense generalization robust cases visual appearance changes dynamics remain same. second optimistic components indeed decoupled vision model visual input intermediate state space dynamics model evolve objects state space time. example work object detection localization extracting position velocity well work extracting latent object properties therefore paper focuses learning dynamics state space taking small step toward emulating general-purpose physics engine eventual goal building system exhibits compofigure physics programs consider space physics programs object-based representations physical laws markovian translation-invariant. consider object turn predict future state conditioned past states context objects. sec. present speciﬁc instantiation uses neighborhood mask select context objects. sec. apply instantiation investigate variations two-dimensional worlds balls obstacles matter-js physics engine testbed exploring npe’s capabilities model simple rigid-body dynamics. worlds generated simpliﬁed physics engine believe learning model simple physics npe’s framework ﬁrst necessary step towards emulating full capacity general physics engine maintaining differentiability allow eventually learn complex real-world physical phenomena would challenging engineer conventional physics engines. paper establishes important step. consider detail speciﬁc instantiation uses neighborhood mask select context objects. section discusses four ingredients framework that combined comprise neural network-based physics simulator learns observation. object-based representations make observations factorization scene. ﬁrst regards spatially local computation. physics change across inertial frames sufﬁces separately predict future state object conditioned past states objects neighborhood similar fragkiadaki sec. shows large structures represented composition smaller objects spatially local attention window helps achieve invariance scene conﬁguration. second observation regards temporally local computation. physics markovian prediction need immediate next timestep show sec. enough predict physics effectively long timescales. given observations natural choose object-based state representation. state vector comprises extrinsic properties intrinsic properties global properties given time instance. pairwise factorization letting particular object focus object objects scene context objects models focus object’s velocity composition pairwise interactions neighboring context objects scene time input represented pairs object state vectors ...}. shown fig. composes encoder function decoder function. encoder function fenc summarizes interaction single object pair. encodings pairs concatenated focus object’s past state input decoder function. focus object necessary input decoder neighboring context objects summed encoder output would zero. decoder function predicts focus object’s velocity practice predicts change compute updates position using velocity ﬁrst-order approximation. predict velocity rather position help avoid memorizing environment; training network predict position conditions network worlds training domain making difﬁcult transfer knowledge across environments. include acceleration state representation position velocity fully parametrize object’s state. thus acceleration learned observing velocity consecutive timesteps hence choice input timesteps. explored longer input durations well found additional beneﬁt. context selection pair selected neighbors neighborhood masking function takes value euclidean distance positions focus context object respectively time less neighborhood threshold many physics engines collision detection scheme phases. broad phase used computational efﬁciency uses neighborhood threshold select objects might necessarily will collide object. narrow phase performs actual collision detection smaller subset objects also resolves collisions objects collide. analogously neighborhood mask implements broad phase implements narrow phase. mask constrains search space context objects network ﬁgures detect resolve collisions. mask speciﬁc case general attention mechanism select contextual elements scene. function composition symbolic physics engines evolve objects time based dynamics dictate independent behavior behavior objects notably particular object’s reference frame forces feels objects additive. architecture incorporates several inductive biases reﬂect recipe. composition fenc fdec induce causal structure pairs objects. provide loose interpretation encoder output effect object object require effects additive forces are. design allows scale naturally different numbers neighboring context objects. inductive biases effect strongly constraining space possible simulators learn focusing compositional programs reﬂect pairwise causal structure object interactions. purpose contrasting following baselines illustrate beneﬁt pairwise factorization function composition architectural features npe. architectures baselines shown work well similar tasks immediately clear whether npe’s assumptions useful necessary good baselines comparison. viewed another comparing baselines lesion study baseline lacks aspect structure. no-pairwise no-pairwise baseline summarized fig. similar compute pairwise interactions; otherwise encoder decoder npe’s. therefore directly highlights value npe’s pairwise factorization. also markovian variant social lstm sums encodings context objects encoding object independently similar social lstm’s social pooling. information modeling objects interact would present encoding step. possible mechanism predicting dynamics encoder’s object encoding consists abstract object representation force ﬁeld created object. therefore decoder could apply force ﬁelds context objects focus object’s abstract object representation predict focus object’s velocity. alahi demonstrated social lstm’s performance modeling human trajectories would interesting architectural assumptions perform physics moving objects. currently implemented also predicts angular velocity along velocity experiments paper always angular velocity well gravity friction pairwise forces zero. included parameters implementation future work planning test situations scenarios angular velocity important block towers magnetism. however current work vestigial zero appear evaluation. figure scenario models ﬁgure compares lstm architectures predicting velocity object example scenario heavy balls light balls objects object neighborhood object ignored. encoder consists pairwise layer feedforward network decoder also feedforward network. input decoder concatenation summed pairwise encodings input state object encoder encoder without pairwise layer. decoder decoder. input decoder concatenation summed context encodings encoding object shufﬂe context objects inputted lstm binary indicate whether object context focus object. lstm long short-term memory networks shown sequentially attend objects interesting test whether lstm well-suited modeling object interactions object states explicitly given input. cognitive science viewpoint lstm interpreted serial mechanism object tracking lstm architecture accepts state context object last step takes focus object’s state predicts velocity. lstm moves object space sequentially lack factorized compositional structure highlights value npe’s function composition independent interactions object neighbors. notion compositionality treats object pairwise interaction independently encapsulated separate computational entity reused rearranged; encoder function applied pair. function encapsulates computation repeatedly applied neighboring context objects equally composes repeated encoding function decoder function predict velocity. lstm exhibit notion compositionality designed take advantage factorized structure scene. unlike lstm’s structure differentiate focus context object state representation indicate whether object context focus object. shufﬂe order context objects account ordering bias. object-based representations necessary three ingredients explained motivation object-based representations sec. sec. analyze three ingredients context several experiments. prediction task ﬁrst test even capable predicting physics number objects held constant. generalization task test npe’s capability generalize across variable object count. inference task test inverted infer mass prediction generalization settings. experiments compare npe-nn modiﬁed without neighborhood mask analyze context selection mechanism analyze factorization lstm analyze compositionality sec. analyzes neighborhood mask depth. test npe’s capability generalize across different scene conﬁgurations sec. using matter-js physics engine evaluate worlds balls obstacles. worlds exhibit nonlinear dynamics support wide variety scenarios. bouncing balls interest cognitive science study causality counterfactual reasoning gerstenberg trained -timestep windows trajectories timesteps world objects generate trajectories. experiments train multiple worlds together shufﬂe examples across training worlds train without curriculum schedule. worlds vertical dimension pixels horizontal dimension pixels constrain maximum velocity object pixels/second. normalize positions dividing horizontal dimension normalize velocities dividing maximum velocity. like battaglia predictions effective long timescales even trained predict immediate next time step. randomly selected simulation videos found https//goo.gl/bwyuof. plots show results three independent runs averaged held-out test data different random seeds. shown graphs fig. fig. lstm’s predicted trajectories diverge ground truth different reasons videos illuminate. lstm fail predict plausible physical movement entirely npe’s predictions initially adhere closely ground truth slowly diverge accumulation subtle errors human perceptual system also accumulates errors however preserves general intuitive physical dynamics roughly consistent people’s intuitive expectations. consider simple worlds four balls uniform mass measure performance simulation visualize cosine similarity predicted velocity ground truth velocity well relative error magnitude predicted velocity ground truth velocity timesteps simulation. models take timesteps initial input previous predictions input future predictions. measure progress training also display mean squared error normalized velocity. test whether learned knowledge simple physics concepts transferred extrapolated worlds number objects previously unseen unseen worlds test data combinatorially complex varied observed worlds training data. objects equal mass. simulation npe’s predictions consistent whereas lstm’s prediction begin diverge wildly towards timesteps simulation consistently outperforms baselines order magnitude velocity prediction show infer latent properties mass. proposal motivated experiments battaglia uses probabilistic physics simulator infer various properties scene conﬁguration. whereas physical rules simulator manually pre-speciﬁed learns rules observation. train worlds used prediction generalization tasks uniformly sampled mass ball log-spaced chose discrete-valued masses simplify qualitative understanding model’s capacity infer. future work would like investigate continuously valued masses evaluate binary comparisons summarized fig. fig. select scenarios exhibiting collisions focus object masses objects score npe’s prediction possible mass hypotheses focus object. prediction scored ground-truth loss used training. hypothesis whose prediction yields lowest error npe’s figure quantitative evaluation prediction generalization tasks. rows cosine similarity relative error magnitude. bottom velocity test course training. worlds chaotic systems surprising predictions diverge ground truth time consistently outperforms baselines fronts especially testing objects generalization task. npe’s performance continues improve training npe-nn lstm quickly plateau. hypothesize npe’s structured factorization state space guides wasting time exploring suboptimal programs. npe’s accuracy signiﬁcantly greater baseline models’ mass inference. notably achieves similar inference performance whether prediction generalization settings showcasing strong generalization capabilities. lstm performs poorest reaching random guessing analyze effectiveness different neighborhood thresholds constant-mass prediction task. neighborhood threshold quite robust ball radii. predicts outputs given inputs infers inputs given outputs. though adopted particular parametrization object limited semantic meaning elements input expect latent object properties inferred way. differentiable expect also infer object properties backpropagating prediction error randomly sampled input. would useful inferring non-categorical values positions invisible objects whose effects felt whose positions unknown. fig. vary npe’s neighborhood threshold evaluate performance constant-mass prediction task. units ball radii means context object detected exactly touching focus object. ball radii pixels maximum velocity pixels timestep maximum distance balls initially touching next timestep ball radii. given velocities sampled uniformly makes sense performs well robust range performance drops smaller larger important note different work better different domains object geometries. figure visualizations scales complex dynamics world conﬁgurations lstm cannot. masses visualized cyan yellow-green consider collision balls world ground truth collision happens balls correctly predicts this. predicts slower movement ball ball overlaps ball lstm predicts slower movement incorrect angle world boundary ball overlaps ball ﬁrst glance models seem handle collisions well world internal obstacles successfully resolve collisions. suggests pairwise factorization handles object interactions well letting generalize different world conﬁgurations whereas lstm memorized geometry world. include analysis prediction generalization tasks without neighborhood mask npe-nn neighborhood mask gives order magnitude improvement velocity prediction loss loss continues improve training npe-nn loss quickly plateaus. interesting npe-nn performs better lstm predictive error outperforms lstm mass inference. observations suggest computing interactions focus object shares context object effective inferring property focus object disregarding factorized effects. also suggest additional spatial structure constraining context space neighborhood mask prevents naively ﬁnding associations objects cannot inﬂuence focus object. experiments neighborhood mask additional practical beneﬁt reducing computational complexity number objects scene number context-focus object pairs considers bounded neighborhood mask constant number. though beyond scope work extend functionality context selection mechanism include worlds contain forces distance future instantiations investigate general context selection mechanism learned jointly model parameters. demonstrate representing large structures composition smaller objects building blocks. important testing npe’s invariance scene conﬁguration; scene conﬁguration matter underlying physical laws remain same. worlds contain balls bouncing around variations different wall geometries. geometries internal obstacles shape rectangle respectively. internal obstacles. obstacles linearly attached wall like protrusion obstacles figure quantitative evalution compositional state representation simpliﬁes physical prediction problem local arrangements context balls obstacles even wall geometries complex varied macroscopic scale. therefore surprising models perform consistently across wall geometries. note consistently outperforms models performance increases varied internal obstacles cosine similarity velocity angle. prominent geometries relative error magnitude. train conceptually simpler worlds test complex worlds. variations wall geometries adds difﬁculty extrapolation task. context objects present focus object’s neighborhood time. geometries objects scene wall geometries. shown fig. robust scenes internal obstacles even observed scenes training. explain npe’s superior performance generalization perspective context selection factorization compositionality. design three ingredients transform testing data distribution similar training data distribution generalization across variable object count different scene conﬁgurations happens naturally. consider generalizing across variable object count. neighborhood mask selects context objects need focus bounded subset objects regardless total number objects. factorizing scene pairwise interactions induces causal structure context object focus object matter object count causal structure remains consistent input merely object pairs. composing pairwise interactions together summation encourages encoder output additive decoder receives appropriate effect context objects regardless many are. consider generalizing across different scene conﬁgurations. state representation composes larger structures smaller objects many real-world objects composed smaller components. therefore even wall geometries complex varied macroscopic scale input distribution remains roughly same prediction problem still remains objects local glimpse entire scene. top-down bottom-up approaches recent top-down approaches investigate probabilistic game physics engines computational models physical simulation humans however models require full speciﬁcation physical laws object geometries. given speciﬁcation inferring physical laws compose apply given scenario strength automatically inferring visual data physical laws object properties present requires work inverse graphics physics-based visual understanding builds structural assumptions top-down approaches differentiable architecture opens possible path joint training vision model automatically adapt speciﬁc physical properties scene. bottom-up approaches attempt bypass intermediate step ﬁnding physics representations directly visual observations physical judgments passive action-conditioned motion prediction. work historically compositional nature limited ﬂexibility transfer knowledge conceptually similar worlds physics remain same number objects complexity object conﬁgurations varies. moreover approaches infer latent properties does. work taken similar hybrid approaches neuroanimator ﬁrst work train neural network emulate physics simulator interaction network learns simulate physics graph objects relations. sketching combines symbolic structure assumes generic objects interactions differentiability allows speciﬁc nature interactions learned training. approach starting general sketch program ﬁlling speciﬁcs inspired ideas program synthesis community examples work combine symbolic neural approaches sketching include graph-based neural networks transforming autoencoders composing functions reuse repeatedly applies encoder object pair iteratively applies object scene focus object recursively predicts future timesteps using predictions previous timesteps employing function reuse achieve generalization also featured work abelson andreas lake reed freitas socher work assemble small subprograms form larger programs. also dynamically composes internal modules based number objects arrangement context objects. object-based approaches fragkiadaki battaglia notably similar work sense work take object-based approach model bouncing balls environment. work inspired fragkiadaki iterative approach predicting motion object turn conditioned context. contrast model assumes relational structure objects beyond visual attention window centered around focus object whereas explicitly processes interaction focus context object. compare simulation videos ours speciﬁc signiﬁcant improvements evident approach. example work balls appear attracted walls; balls appear bounce along walls even attractive force present. balls rarely touch collisions magnetically repel short distance. exhibit behaviors tends preserve intuitive physical dynamics colliding balls. addition differences show strong predictive performance generalizing eight balls balls videos. also crucially show performance stronger generalization conditions variable mass complex scene conﬁgurations. recently battaglia independently parallel developed architecture call interaction network learning model physical systems. show architecture apply several different kinds physical systems including n-body gravitational interactions string falling gravity. like work model simulate many timesteps effectively trained next-timestep prediction generalize different world conﬁgurations different numbers objects. compared interaction network main difference architecture take object relations explicit input instead learns nature relations constraining attention neighborhood objects. another difference function reuse demonstrated trained automatically infer properties input mass without retraining. contrast train additional classiﬁer model inference. work also exhibits four ingredients framework view similarities work converging evidence utility object-based representations compositional model architectures learning emulate general-purpose physics engines. paper ﬁrst explore learning physics simulator take opportunity highlight value paper’s contributions. hope contributions seed research builds framework paper proposes. showed object-based representations context selection mechanism factorization compositionality useful ingredients learning physics simulator generalizes across variable object count different scene conﬁgurations spatially temporally local computation. generalization possible ingredients transform testing data distribution similar training data distribution. makes strong assumptions nature objects physical environment. assumptions inductive biases give enough structure help constrain model physical phenomena terms objects also general enough learn physical dynamics almost exclusively observation. applied simple two-dimensional worlds bouncing balls ranging complexity. showed achieves prediction error extrapolates learned physical knowledge previously unseen number objects world conﬁgurations infer latent properties mass. compared several baselines designed test ingredients framework found superior performance ingredients combined npe. though demonstrated balls environment nonlinear dynamics complex scene conﬁgurations state representation architecture propose quite general-purpose assume little speciﬁc dynamics scene. paper works toward emulating general purpose physics engine framework visual physical aspects scene disentangled. next steps include linking perceptual models extract properties position mass visual input. learning simulate unsupervised learning structure environment. simulator like incorporated agent context model-based planning model-based reinforcement learning becomes prior environment guides learning reasoning. combining expressiveness physics engines adaptability neural networks compositional architecture supports generalization fundamental aspects physical reasoning neural physics engine important step towards lifting agent’s ability think level abstraction concept physics primitive. thank tejas kulkarni insightful discussions guidance. thank ilker yildirim erin reynolds feras saad andreas stuhlm¨uller adam lerer chelsea finn jiajun anonymous reviewers valuable feedback. thank liam brummit kevin kwok guillermo webster help matter-js. work supported mit’s superurop urop programs center minds brains machines award ccf- grant n---. chen duan houthooft schulman sutskever abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems pages jain zamir savarese saxena. structural-rnn deep learning spatio-temporal graphs. proceedings ieee conference computer vision pattern recognition pages kulkarni kohli tenenbaum mansinghka. picture probabilistic programming language scene perception. proceedings ieee conference computer vision pattern recognition pages yildirim freeman tenenbaum. galileo perceiving physical object properties integrating physics engine deep learning. advances neural information processing systems pages trained models using rmsprop backpropagation algorithm euclidean loss iterations learning rate learning rate decay every training iterations beginning iteration used minibatches size used split training validation test data. models implemented using neural network libraries built collobert l´eonard encoder consists pairwise layer hidden units -layer feedforward network hidden units layer rectiﬁed linear activations. binary mask zero non-neighboring objects implement encoder layers without bias non-neighboring objects contribute encoder activations. encoding parameters shared across object pairs. decoder ﬁve-layer network hidden units layer rectiﬁed linear activations last layer. encoder architecture encoder without pairwise layer. decoder architecture decoder. lstm three layers hidden units linear layer last layer. rectiﬁed linear activations layer. informally explored several hyperparameters varying number layers hidden dimension learning rates though exhaustive search found hyperparameter settings work well. figure error analysis velocity position summarize error velocity position train-test variant experiment. normalized velocity shown gray columns white columns show error euclidean distance predicted position ground truth position ball. normalized radius ball multiplying values would give actual euclidean distance pixels. consistently outperforms baselines order magnitude also reﬂected bottom fig. notice experiments variable mass exhibit slightly higher error constant-mass variants even variable mass experiments contain masses differ factor experiments different scene conﬁgurations report error npe-nn; unnecessary computational complexity operating objects degradation performance without mask evident experiments make need neighborhood mask clear.", "year": 2016}