{"title": "An Introduction to Convolutional Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.  This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.", "text": "abstract. ﬁeld machine learning taken dramatic twist recent times rise artiﬁcial neural network biologically inspired computational models able exceed performance previous forms artiﬁcial intelligence common machine learning tasks. impressive forms architecture convolutional neural network cnns primarily used solve difﬁcult image-driven pattern recognition tasks precise simple architecture offers simpliﬁed method getting started anns. document provides brief introduction cnns discussing recently published papers newly formed techniques developing brilliantly fantastic image recognition models. introduction assumes familiar fundamentals anns machine learning. artiﬁcial neural networks computational processing systems heavily inspired biological nervous systems operate. anns mainly comprised high number interconnected computational nodes work entwine distributed fashion collectively learn input order optimise ﬁnal output. basic structure modelled shown figure would load input usually form multidimensional vector input layer distribute hidden layers. hidden layers make decisions previous layer weigh stochastic change within detriments improves ﬁnal output referred process learning. multiple hidden layers stacked upon each-other commonly called deep learning. fig. simple three layered feedforward neural network comprised input layer hidden layer output layer. structure basis number common architectures included limited feedforward neural networks restricted boltzmann machines recurrent neural networks learning paradigms image processing tasks supervised unsupervised learning. supervised learning learning pre-labelled inputs targets. training example input values associated designated output values. goal form training reduce models overall classiﬁcation error correct calculation output value training example training. unsupervised learning differs training include labels. success usually determined whether network able reduce increase associated cost function. however important note image-focused pattern-recognition tasks usually depend classiﬁcation using supervised learning. convolutional neural networks analogous traditional anns comprised neurons self-optimise learning. neuron still receive input perform operation basis countless anns. input image vectors ﬁnal output class score entire network still express single perceptive score function last layer contain loss functions associated classes regular tips tricks developed traditional anns still apply. notable difference cnns traditional anns cnns primarily used ﬁeld pattern recognition within images. allows encode image-speciﬁc features architecture making network largest limitations traditional forms tend struggle computational complexity required compute image data. common machine learning benchmarking datasets mnist database handwritten digits suitable forms relatively small image dimensionality dataset single neuron ﬁrst hidden layer contain weights manageable forms ann. consider substantial coloured image input number weights single neuron ﬁrst layer increases substantially also take account deal scale input network also need larger used classify colour-normalised mnist digits understand drawbacks using models. matter? surely could increase number hidden layers network perhaps increase number neurons within them? simple answer question reasons being simple problem unlimited computational power time train huge anns. second reason stopping reducing effects overﬁtting. overﬁtting basically network unable learn effectively number reasons. important concept most machine learning algorithms important every precaution taken reduce effects. models exhibit signs overﬁtting reduced ability pinpoint generalised features training dataset also test prediction sets. main reason behind reducing complexity anns. less parameters required train less likely network overﬁt course improve predictive performance model. differences neurons layers within comprised neurons organised three dimensions spatial dimensionality input depth. depth refer total number layers within third dimension activation volume. unlike standard anns neurons within given layer connect small region layer preceding practice would mean example given earlier input ’volume’ dimensionality leading ﬁnal output layer comprised dimensionality would condensed full input dimensionality smaller volume class scores ﬁled across depth dimension. cnns comprised three types layers. convolutional layers pooling layers fully-connected layers. layers stacked architecture formed. simpliﬁed architecture mnist classiﬁcation illustrated figure convolutional layer determine output neurons connected local regions input calculation scalar product weights region connected input volume. rectiﬁed linear unit aims apply fully-connected layers perform duties found standard anns attempt produce class scores activations used classiﬁcation. also suggested relu used layers improve performance. simple method transformation cnns able transform original input layer layer using convolutional downsampling techniques produce class scores classiﬁcation regression purposes. fig. activations taken ﬁrst convolutional layer simplistic deep training mnist database handwritten digits. look carefully network successfully picked characteristics unique speciﬁc numeric digits. however important note simply understanding overall architecture architecture sufﬁce. creation optimisation models take quite time quite confusing. explore detail individual layers detailing hyperparameters connectivities. kernels usually small spatial dimensionality spreads along entirety depth input. data hits convolutional layer layer convolves ﬁlter across spatial dimensionality input produce activation map. activation maps visualised seen figure glide input scalar product calculated value kernel. network learn kernels ’ﬁre’ speciﬁc feature given spatial position input. commonly known activations. alluded earlier training anns inputs images results models train effectively. comes fullyconnected manner standard neurons mitigate every neuron convolutional layer connected small region input volume. dimensionality region commonly referred receptive ﬁeld size neuron. magnitude connectivity depth nearly always equal depth input. example input network image size receptive ﬁeld size would total weights neuron within convolutional layer. perspective standard neuron seen forms would contain weights each. convolutional layers also able signiﬁcantly reduce complexity model optimisation output. optimised three hyperparameters depth stride setting zero-padding. depth output volume produced convolutional layers manually number neurons within layer region input. seen forms anns neurons hidden layer directly connected every single neuron beforehand. reducing hyperparameter signiﬁcantly minimise total number neurons network also signiﬁcantly reduce pattern recognition capabilities model. also able deﬁne stride depth around spatial dimensionality input order place receptive ﬁeld. example stride would heavily overlapped receptive ﬁeld producing extremely large activations. alternatively setting stride greater number reduce amount overlapping produce output lower spatial dimensions. represents input volume size represents receptive ﬁeld size amount zero padding referring stride. calculated result equation equal whole integer stride incorrectly neurons unable neatly across given input. despite best efforts still models still enormous image input real dimensionality. however methods developed greatly curtail overall number parameters within convolutional layer. parameter sharing works assumption region feature useful compute spatial region likely useful another region. constrain individual activation within output volume weights bias massive reduction number parameters produced convolutional layer. result backpropagation stage occurs neuron output represent overall gradient totalled across depth thus updating single weights opposed every single one. pooling layer operates activation input scales dimensionality using function. cnns come form max-pooling layers kernels dimensionality applied stride along spatial dimensions input. scales activation original size whilst maintaining depth volume standard size. destructive nature pooling layer generally observed methods max-pooling. usually stride ﬁlters pooling layers allow layer extend entirety spatial dimensionality input. furthermore overlapping pooling utilised stride kernel size destructive nature pooling kernel size usually greatly decrease performance model. also important understand beyond max-pooling architectures contain general-pooling. general pooling layers comprised pooling neurons able perform multitude common operations including l/l-normalisation average pooling. however tutorial primarily focus max-pooling. fully-connected layer contains neurons directly connected neurons adjacent layers without connected layers within them. analogous neurons arranged traditional forms ann. despite relatively small number layers required form formulating architecture. said would idiotic simply throw layers together expect work. reading related literature obvious much like forms anns cnns tend follow common architecture. common architecture illustrated figure convolutional layers stacked followed pooling layers repeated manner feeding forward fully-connected layers. another common architecture stack convolutional layers pooling layer illustrated figure strongly recommended stacking multiple convolutional layers allows complex features input vector selected. also advised split large convolutional layers many smaller sized convolutional layers. reduce amount computational complexity within given convolutional layer. example stack three convolutional layers receptive ﬁeld neuron ﬁrst convolutional layer view input vector. neuron second convolutional layer view input vector. neuron third convolutional layer view input vector. stacks feature non-linearities turn allows express stronger features input fewer parameters. however important understand come distinct memory allocation problem especially making backpropagation algorithm. input layer recursively divisible two. common numbers include whilst using small ﬁlters stride make zero-padding ensure convolutional layers reconﬁgure dimensionality input. amount zero-padding used calculated taking away receptive ﬁeld size dividing two.activation cnns extremely powerful machine learning algorithms however horrendously resource-heavy. example problem could ﬁltering large image input we’re ﬁltering kernels zero padding result three activation vectors size calculates roughly million activations enormous megabytes memory image. case options. firstly reduce spatial dimensionality input images addition rules-of-thumb outlined above also important acknowledge ’tricks’ generalised training techniques. authors suggest read geoffrey hinton’s excellent practical guide training restricted boltzmann machines. convolutional neural networks differ forms artiﬁcal neural network instead focusing entirety problem domain knowledge speciﬁc type input exploited. turn allows much simpler network architecture paper outlined basic concepts convolutional neural networks explaining layers required build detailing best structure network image analysis tasks. research ﬁeld image analysis using neural networks somewhat slowed recent times. partly incorrect belief surrounding level complexity knowledge required begin modelling superbly powerful machine learning algorithms. authors hope paper reduced confusion made ﬁeld accessible beginners.", "year": 2015}