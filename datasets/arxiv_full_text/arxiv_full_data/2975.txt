{"title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic  Interpretation", "tag": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "abstract": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.", "text": "examine form smooth approximation zero loss learning performed using reformulation widely used logistic function. approach based using posterior mean novel generalized betabernoulli formulation. leads generalized logistic function approximates zero loss retains probabilistic formulation conferring number useful properties. approach easily generalized kernel logistic regression easily integrated methods structured prediction. present experiments learn models using optimization method consisting combination gradient descent coordinate descent using localized grid search escape local minima. experiments indicate optimization quality improved learning meta-parameters optimized using validation set. experiments show improved performance relative widely used logistic hinge loss methods wide variety problems ranging standard irvine libsvm evaluation datasets product review predictions visual information extraction task. observe approach robust outliers compared logistic hinge losses; outperforms comparable logistic margin models larger scale benchmark problems; combined gaussianlaplacian mixture prior parameters kernelized version formulation yields sparser solutions support vector machine classiﬁers; integrated probabilistic structured prediction technique approach provides accurate probabilities yielding improved inference increasing information extraction performance. loss function minimization standard solving many important learning problems. classical statistical literature known empirical risk minimization learning performed minimizing average risk loss where model input feature vector label pairs features labels loss model output focus moment standard binary linear classiﬁcation task encode target class label model parameter vector letting tiwt deﬁne logistic hinge loss indicator function takes value argument true argument false. course loss functions complex example deﬁned learned linear combination simpler basis loss functions focus widely used losses now. different loss functions characterize classiﬁcation problem differently. logistic loss hinge loss similar shape veriﬁed figure logistic regression models involve optimizing logistic loss optimizing hinge loss heart support vector machines seemingly sensible objective classiﬁcation problem empirical risk minimization loss function known np-hard problem hinge loss penalize model heavily data points classiﬁed incorrectly away decision boundary. seen figure penalties much signiﬁcant zero loss. zero-one loss captures intuitive goal simply minimizing classiﬁcation errors recent research directed learning models using smoothed zero-one loss approximation previous work shown hinge loss recently loss efﬁciently effectively optimized directly using smooth approximations. work also underscored robustness advantages loss outliers. loss convex current ﬂurry activity area deep neural networks well award winning work loss approximations highlighted numerous advantages non-convex loss functions. work here interested constructing probabilistically formulated smooth approximation loss. ﬁrst compare widely used logistic loss hinge loss loss little detail. logistic loss well known logistic regression model arises form negative likelihood deﬁned model. speciﬁcally logistic loss arises sigmoid function parametrizing probabilities easily recovered re-arranging obtain probability model form work here take familiar logistic function shall transform create functional form. sequence curves starting blue curve figure give intuitive visualization alter traditional logistic loss. call loss function generalized beta-bernoulli logistic loss acronym referring give name arises combined beta-bernoulli distribution generalized logistic parametrization. give bayesian motivations beta-bernoulli construction section gain additional intuitions effect construction practical perspective consider following analysis. viewing negative likelihood traditional logistic regression parametrization loss function might pose following question alternative functional form underlying probability would lead loss function exhibiting plateau similar loss incorrectly classiﬁed examples? might also pose second question possible construct simple parametrization single parameter controls sharpness smooth approximation loss? intuition answer ﬁrst question traditional logistic parametrization converges zero probability small values argument. turn leads loss function increases linear behaviour small values shown figure contrast loss function deﬁned small values function converge non-zero probability. effect manifests desired plateau seen clearly loss functions deﬁned model figure answer second question indeed yes; speciﬁcally control sharpness approximation factor reminiscent technique used previous work created smooth approximations hinge loss well smooth approximations loss show intuitive effect construction different increasing values gamma figure deﬁne formally below. figure probability corresponding negative probability function logistic loss compared generalized beta-bernoulli model different values used parameters corresponds here llog denotes logistic loss lbbγ denotes beta-bernoulli loss denotes logistic regression model µbbγ denotes generalized beta-bernoulli model show section constants well deﬁned interpretations terms standard parameters beta distribution. impact proposed generalized beta-bernoulli loss arise applying fuller bayesian analysis formulation logistic function. visualization proposed loss figure corresponds weak non-informative prior figure show probability given model function right negative probability loss left varied integer powers interval logistic function transition becomes abrupt increases. loss function behaves like usual logistic loss close provides increasingly accurate smooth approximation zero loss larger values intuitively location plateau smooth logistic loss approximation y-axis controlled choice effect weak uniform prior small minimum probability model imperceptible terms impact sigmoid function space leads plateau negative loss function. contrast strong prior losses figure leads minimum maximum probabilities much zero one. work makes number contributions enumerate here primary contribution work probabilistically formulated approximation loss based generalized logistic function beta-bernoulli distribution. result generalized sigmoid function probability negative probability space. second contribution work present explore adapted version optimization algorithm proposed optimize meta parameters learning using validation sets. present series experiments optimize loss using basic algorithm modiﬁed version. linear models show complete approach outperforms widely used techniques logistic regression linear support vector machines. expected experiments indicate relative performance approach increases noisy outliers present data. present number experiments larger scale data sets demonstrating method also outperforms widely used logistic regression techniques despite fact underlying models involved linear. apply model structured prediction task formulated mining faces wikipedia biography pages. proposed method well adapted setting improved probabilistic modeling capabilities approach yields improved results visual information extraction improved probabilistic structured prediction. also show approach also easily adapted create novel form kernel logistic regression based generalized beta-bernoulli logistic regression framework. kernelized version method kernel bblr outperforms non-linear support vector machines. expected regularized kbblr yield sparse solutions; however since developed robust method optimizing non-convex loss propose explore novel non-convex sparsity encouraging prior based mixture gaussian laplacian. sparse kbblr typically yields sparser solutions svms comparable prediction performance degree sparsity scales much favorably compared svms remainder paper structured follows. section present review relevant recent work area loss approximation. section present underlying bayesian motivations proposed loss function. section provide details optimization algorithms. section present experimental results using protocols facilitate comparisons prior work well evaluate method large scale structured prediction problems. provide ﬁnal discussion conclusions section limγ→∞ lglog lhinge. achieved approximation using factor shifted version usual logistic loss. illustrate construction used approximate hinge loss figure maximum margin bayesian network formulation also employs smooth differentiable hinge loss inspired huber loss similar shape min. sparse probabilistic classiﬁer approach truncates logistic loss leading sparse kernel logistic regression models. proposed technique learning support vector classiﬁers based arbitrary loss functions composed using combination hyperbolic tangent loss function polynomial loss function. figure generalized logistic loss lglog proposed approximate hinge loss lhinge translating logistic loss llog increasing factor. show curves sigmoid function used directly approximate loss approach also uses similar factor show another important aspect compared variety algorithms directly optimizing loss novel algorithm optimizing sigmoid loss lsig. call algorithm smooth loss approximation smooth loss approximation. compared direct loss optimization algorithms branch bound technique prioritized combinatorial search technique algorithm referred combinatorial search approximation presented detail compared methods algorithm optimize sigmoidal approximation loss. evaluate compare quality non-convex optimization results produced algorithm sigmoid loss also presents training errors number standard evaluation datasets. provide excerpt results table perform similar comparisons experimental work. results indicated algorithm consistently yielded superior performance ﬁnding good minima underlying non-convex problem. furthermore also provide analysis run-time performance algorithms. experiments indicated technique signiﬁcantly faster alternative algorithms non-convex optimization. based results build upon approach work here. award winning work produced approximation loss creating ramp loss lramp obtained combining traditional hinge loss shifted inverted hinge loss illustrated figure showed optimize ramp loss using concave-convex procedure yields faster training times compared traditional svms. recent work proposed alternative online learning algorithm ramp loss explored table excerpt total loss variety algorithms standard datasets. loss logistic regression linear support vector machine also provided reference. similar ramp loss refer robust truncated hinge loss. recent work explored similar ramp like construction refer slant loss. interestingly ramp loss formulation also generalized structured predictions figure shifted hinge losses combined produce ramp loss lramp. usual hinge loss lhinge combined negative shifted hinge loss lhinge produce lramp although smoothed zero-one loss captured much attention recently older references similar research. activity using zero-one loss like functional losses machine learning specially boosting neural network communities. vincent analyzes loss deﬁned functional hyperbolic tangent tanh robust doesn’t penalize outliers excessively compared logistic loss hinge loss squared loss loss functions. loss interesting properties continuous zero-one loss like properties. variant loss used boosting algorithms work also shown hyperbolic tangent parametrized squared error loss transforms squared error loss behave like tanh hyperbolic tangent loss. shall also possible integrate novel smooth loss formulation models structured prediction. work similar explored ramp loss context structured prediction machine translation. derive novel form logistic regression based formulating generalized sigmoid function arising underlying bernoulli model beta prior. also scaling factor increase sharpness approximation. consider ﬁrst traditional widely used formulation logistic regression derived probabilistic model based bernoulli distribution. bernoulli probabilistic model form intuitive interpretation equivalent pseudo counts observations classes model beta function. beta distribution prior parameters bernoulli distribution posterior mean beta-bernoulli model easily computed fact posterior also beta distribution. property also leads intuitive form posterior mean expected value beta-bernoulli model consists simple weighted average prior mean traditional maximum likelihood estimate easy show mean expected value posterior predictive distribution equivalent plugging posterior mean parameters beta distribution bernoulli distribution i.e. given observations thus propose replace traditional sigmoidal function used logistic regression function given posterior mean beta-bernoulli model further increase model’s ability approximate zero loss shall also generalized form beta-bernoulli model natural parameter leads complete model based generalized beta-bernoulli formulation useful remind reader point used beta-bernoulli construction deﬁne function deﬁne prior parameter random variable frequently done beta distribution. furthermore traditional bayesian approaches logistic regression prior placed parameters used parameter estimation fully bayesian methods integrates uncertainty parameters. formulation here placed prior function commonly done gaussian processes. approach might seen pragmatic alternative working fully bayesian posterior distributions functions given data fully bayesian procedure would posterior predictive distribution make predictions using formulation corresponding loss given earlier equations figure showed setting scalar parameter larger values allows generalized beta-bernoulli model closely approximate zero loss. show loss figure corresponds stronger beta prior leads approximation range values even closer loss. might imagine little analysis form asymptotics function also given setting corresponding scaling factor linear translation found transform range loss interval limγ→∞ however shown figure loss function asymmetric limit large gamma corresponds different losses true positives false positives true negatives false negatives. reasons believe formulation many attractive useful properties. figure loss negative probability function generalized beta-bernoulli model different values used parameters corresponds loss also permits asymmetric loss functions. show negative probability function loss corresponds also give logistic loss point reference. here llog denotes logistic loss lbbγ denotes beta-bernoulli loss. turn problem estimating parameters given data form using model. deﬁned probabilistic model usual shall simply write probability deﬁned model optimize parameters maximizing probability minimizing negative probability. shall discuss detail section modiﬁed form optimization algorithm slowly increase interleave gradient descent steps coordinate descent implemented grid search. gradient stated beginning discussion parameter estimation optimization model large sufﬁciently large predictions given maximum minimum probabilities possible model. deﬁning class positive class maximum probability model equal true positive rate maximum probability negative class equal true negative rate provided asymptotic analysis expected values previous section. experiment section provide bblr results using asymptotic values parameters along cross-validated values hyperparameters regularization parameter described section however also possible learn hyper-parameters using training validation both. below provide partial-derivatives likelihood function hyper-parameters. discussed relevant recent work section above work shown algorithm applied lsig outperformed number techniques terms true loss minimization performance time. generalized beta-bernoulli loss lbbγ another type smooth approximation loss therefore variation algorithm optimize loss. recall compares generalized beta-bernoulli logistic loss directly deﬁned sigmoidal loss used work becomes apparent bblr formulation three additional hyper-parameters additional parameters control locations plateaus function plateaus well deﬁned interpretations terms probabilities. contrast plateaus sigmoidal loss located zero one. additionally practise interested optimizing regularized loss form prior regularization used parameters. experiments here follow widely used practice using gaussian prior parameters. corresponding regularized loss arising negative likelihood additional regularization term gives complete objective function parameter controls strength regularization. additional hyper-parameters original algorithm directly applicable formulation. however hold hyper-parameters ﬁxed able general idea approach perform modiﬁed optimization given algorithms experiments below strategy bblr series experiments. deal issue jointly learn weights well hyper-parameters bblr series experiments learn hyper-parameters gradient descent training set. precisely learn permit parameters easily re-parametrized within importantly initial experiments indicated basic formulation required considerable hand tuning learning parameters data set. case even using simplest smooth loss function without additional degrees freedom afforded formulation. develop metaoptimization procedure learning algorithm parameters. bblr series experiments learning meta-parameter optimization procedure. initial formal experiments indicate meta-optimization learning parameters fact essential practice. therefore present detail below. present meta-optimization extension various modiﬁcations approach algorithm proposed decomposed different parts; outer loop initializes model enters loop slowly increases factor sigmoidal loss repeatedly calling algorithm refer range optimization gradient descent range. range optimization part consists stages. stage standard gradient descent optimization decreasing learning rate stage probes parameter radius using dimensional grid search determine loss reduced thus implementing coordinate descent grid points. provide slightly modiﬁed form outer loop algorithm algorithm expressed initial parameters given model explicit parameters given algorithm. approach hard code initial parameter estimates result data. provide compressed version inner range optimization technique algorithm ﬁrst minor difference optimization algorithm extension selection initial algorithm starts optimizing. original algorithm uses solution initial solution modiﬁed algorithm uses obtained experiments using validation deﬁned within training data initialize gradient based optimization technique start idea search best produces reasonable solution algorithm start with weight associated gaussian prior leading penalty added parameter chosen grid search chosen bracket search algorithm. experience model parameters change problem problem hence must ﬁne-tuned best results. below present results three different groups benchmark problems selection university california irvine repository larger higher dimensionality text processing tasks libsvm evaluation archive product review sentiment prediction datasets used present results structured prediction problem formulated task visual information extraction wikipedia biography pages. finally explore kernelized version classiﬁer. experiments unless otherwise stated gaussian prior parameters leading penalty term. explore four experimental conﬁgurations bblr approach bblr modiﬁed algorithm following bblr parameters held ﬁxed corresponds minor modiﬁcation traditional negative logistic loss yields probabilistically well deﬁned smooth sigmoid shaped loss bblr values corresponding empirical counts positives negatives total number examples training corresponds simplistic heuristic partially justiﬁed bayesian reasoning; bblr outer meta-optimization learning parameters performed slam bblr outer meta-optimization learning parameters performed hyper-parameters optimized gradient descent using training initialized using values given asymptotic analysis using hard threshold classiﬁcations. iteration optimization step parameters updated complementary slam hyper-parameters adjusted/redeﬁned using metaoptimization procedure using subset training data validation set. consequently models produced bblr series experiments explore ability improved learning parameter meta-optimization method effectively minimize smooth approximation zero loss. bblr series experiments delve deepest ability bblr formulation slam optimization accurately make probabilistic predictions. evaluate technique following datasets university california irvine machine learning repository breast heart liver pima. datasets part compare directly results understand behaviour novel logistic function formulation explore behavior learning parameter optimization procedure. table shows brief details databases. facilitate comparisons previous results presented summarized table literature review section provide small initial experiments following experimental protocols. experiments compare bblrs following models logistic regression implementation linear using implementation used optimization sigmoid loss lsig using algorithm code distributed site associated despite fact used code distributed website associated found algorithm applied sigmoid loss lsig gave errors slightly higher given term table subsequent tables denote experiments performed using sigmoidal loss explored algorithm minimizing applying algorithm loss yielded slightly superior results sigmoidal loss empirical counts training used slightly worse results used analyzing ability different loss formulations algorithms minimize loss different datasets using common model class reveal differences optimization performance across different models algorithms. however certainly interested evaluating ability different loss functions optimization techniques learn models generalized data. therefore provide next experiments using traditional training validation testing splits following protocols used however shall soon experiments underscored importance extending original algorithm automate adjustment learning parameters. tables create random splits data perform traditional fold evaluation using cross validation within training tune hyper-parameters. table present loss splits well total loss across experiments algorithm. analysis allows make intuitive comparisons results table represents empirically derived lower bound loss. table present traditional mean accuracy across experiments. examining columns bblr table re-formulated logistic loss able outperform sigmoidal loss addition additional tuning parameters optimization column bblr able improve upon overall zero-one loss yielded logistic regression baseline methods. however important remember methods based underlying linear model comparatively small datasets consisting relatively dimensional input feature vectors. such necessarily expect statistically signiﬁcant differences test performance zero-one loss minimization performance. observation made motivated exploration learning noisy feature vectors. follow similar path below explore datasets much larger much higher dimensions subsequent experimental work. table present mean loss repetitions fold leave experiment noise added data following protocol given again bblr achieved moderate gain algorithm whereas gain bblr models noticeable. table also show percentage improvement best model linear svm. table show average errors noise added experiments. advantages directly approximating zero loss pronounced. however fact approach failed outperform baselines experiments here; whereas similar experiment algorithm sigmoidal loss outperform methods leads believe table mean loss repetitions fold leave experiment. performance using logistic regression linear sigmoid loss algorithm bblr model optimization using optimization algorithm bblr models additional tuning modiﬁed algorithm. table errors averaged across test splits fold leave experiment. performance using logistic regression linear sigmoid loss algorithm bblr model optimization using optimization algorithm bblr model additional tuning modiﬁed algorithm. issue per-dataset learning algorithm parameter tuning signiﬁcant issue. however observe bblr experiment used original optimization algorithm outperformed sigmoidal loss function optimized using algorithm. results support notion proposed beta-bernoulli logistic loss superior approach approximate zero-one loss empirical table mean loss repetitions fold leave experiment noise added data following protocol given performance using logistic regression linear sigmoid loss algorithm bblr model optimization using optimization algorithm bblr model additional tuning modiﬁed algorithm. give relative improvement error bblr technique right column. table errors averaged repetitions fold leave experiment noise added data. performance using logistic regression linear sigmoid loss algorithm bblr model optimization using optimization algorithm bblr model additional tuning modiﬁed algorithm. perspective. however results column bblr indicate combined novel logistic loss learning parameter optimization yield substantial improvements zero-one loss minimization correspondingly improvements accuracy. performed mcnemar tests four benchmarks comparing bblr linear svms. signiﬁcant number test instances benchmarks became difﬁcult statistically justify compare results. therefore performed pooled mcnemar tests considering split fold leave experiments independent tests collectively performing signiﬁcance tests whole. results pooled mcnemar test given table interestingly noisy dataset experiments bblr found statistically signiﬁcant models section present classiﬁcation results using much larger datasets webspam-unigrams. datasets predeﬁned training testing splits distributed site accompanying benchmarks also distributed libsvm binary data collection. webspam unigrams data originally came study table compiles details thsese databases. experiments additional noise feature vectors. table present classiﬁcation results cases bblr approach shows improved performance linear baselines. earlier small scale experiments used implementation liblinear large scale experiments. table errors larger scale experiments data sets libsvm evaluation archive. bblr compared model using mcnemer’s test bblr statistically signiﬁcant value performed mcnemar’s statistical tests comparing bblr linear models datasets. results found statistically signiﬁcant value cases. given noise added http//users.cecs.anu.edu.au/ xzhang/data/ http//www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/binary.html http//www.cc.gatech.edu/projects/doi/webbspamcorpus.html widely used benchmark problems method compared fundamentally based linear model fact experiments show statistically signiﬁcant improvements bblr widely used methods quite interesting. goal tasks predict whether product review either positive negative. experiments used count based unigram features four databases website associated database positive negative product reviews. table compiles feature dimension size sparse databases. four databases bblr bblr models outperformed linear svm. analyze results also performed mcnemer’s test. books dvds database results bblr bblr models found statistically signiﬁcant linear value bblr tended outperform bblr statistically signiﬁcant way. however since primary advantage bblr conﬁguration yields accurate probabilities necessarily expect dramatically superior performance compared bblr classiﬁcation. reason explore problem using models context structured prediction next experiments. bblr models used make structured predictions advantages beta-bernoulli logistic loss allows model produce accurate probabilistic estimates. intuitively controllable nature plateaus probability view formulation allow probabilistic predictions take values representative appropriate conﬁdence level classiﬁcation. simple terms predictions feature vectors decision boundary need take values near probablity zero probability beta-bernoulli logistic model used. models used components larger systems uses probabilistic inference complex reasoning tasks additional ﬂexibility could signiﬁcant advantage traditional logistic function formulation. following experiments explore hypothesis. performed face mining experiments wikipedia biography pages using technique relies probabilistic inference joint probability model. given identity mining technique dynamically creates probabilistic models disambiguate faces correspond identity interest. models integrate uncertain information extracted throughout document arising three different modalities text meta data images. information text metadata integrated larger model using multiple logistic regression based components. images face detection results bounding boxes text meta information extracted wikipedia identity richard parks shown panel figure bottom panel show instance mining model give summary variables used technique. model dynamically instantiated bayesian network. using bayesian network illustrated figure processing information intuitive. text meta-data features taken input bottom layer random variables inﬂuence binary indicator variables detected face logistic regression based sub-components. result visual comparisons faces detected different images encoded variables {d}. text meta data transformed feature vectors associated detected instance face. text analysis information image names image captions. location image page example refer meta-data. also treat information image directly involved facial comparisons meta-data relative size face faces detected image. bottom layer random variables figure used encode features discuss precise nature deﬁnition features detail therefore local feature vector face feature face index image index features used input part model responsible producing probability given instance face belongs identity interest encoded random variables figure {{ymn}nm therefore binary target target indicator variables figure images face detection results bounding boxes corresponding text meta information wikipedia biography page richard parks. instance facial co-reference model variable descriptions. corresponding face xmn. inferring variables jointly corresponds goal mining model. joint conditional distribution deﬁned general case model given joint model uses predictive scores face local binary classiﬁers mentioned discussed detail used maximum entropy models logistic regression models local binary predictions working multimedia features previous work. here compare result replacing logistic regression components model discussed bblr formulation. examine impact change terms making predictions based solely independent models taking text meta-data features input well impact difference bblr models used sub-components joint structured prediction model. hypothesis bblr method might improve results robustness outliers method potentially able make accurate probabilistic predictions could turn lead precise joint inference. particular experiment biographies faces. table shows results comparing maxent model bblr model. results ﬁve-fold leave wikipedia dataset. indeed obtain superior performance independent bblr models maximum entropy models. also improvement performance bblr models used coupled model joint inference used predictions. labelled bblr optimized addition model parameters using technique explained section produced statistically signiﬁcant results compared maximum entropy model signiﬁcance test used mcnemar test like earlier sets experiments. table compare beta-bernoulli logistic regression kernel beta-bernoulli logistic regression proposed approach compare favorably result widely considered state strong baseline. shown advantages using ramp loss kernel based classiﬁcation yield models even sparser traditional svms based hinge loss. well known based regularization typically yield sparse solutions used traditional kernel logistic regression. analysis previous experiments reveals regularized smooth zero loss approximation approach proposed general lead sparse models well. well known lasso regularization method yield sparse solutions often cost prediction performance. recently called elastic regularization approach based weighted combination regularization shown effective encouraging sparsity less negative impact performance. elastic approach course viewed prior consisting product gaussian laplacian distribution. however part motivation methods yield convex optimization problems combined logistic loss. since developed robust approach optimizing non-convex objective function above opens door non-convex sparsity encouraging regularizers. correspondingly propose explore prior parameters equivalently novel regularization approach based mixture gaussian laplacian distribution. formulation behave like smooth approximation counting norm prior parameters limit laplacian scale parameter goes zero gaussian variance goes inﬁnity. µkβb kernel beta-bernoulli model deﬁned section equation prior modeled mixture zero mean gausg laplacian distribution located sian algorithm found appendix table compare sparse kbblr using radial basis function kernel. free parameters tuned cross validation training data. sparse kbblr solution used mixture gaussian laplacian prior kernel weight parameters presented above. table compares sparse kernel bblr svms standard datasets. figure shows trends sparsity curves increase number training instances comparing kbblr svms product review databases. kbblr scales well compared solution training data size increases. support vectors svms increase almost linearly increase database size effect conﬁrmed number studies comparison kbblr gaussian-laplacian mixture prior produces logarithmic curve increase database size. right panel ﬁgure also shows weight distribution kbblr optimization gaussian-laplacian mixture prior yields observed sparse solution. figure comparing sparse kbblr svms vertical axis shows increase number support vectors increase number training instances image shows weight distribution regularized kbblr ﬁnal gauss-laplacian mixture solution optimization. presented novel formulation learning approximation zero loss. generalized beta-bernoulli formulation provided smooth loss approximation method class probabilistic classiﬁers. experimental results indicate generalized beta-bernoulli formulation capable yielding superior performance traditional logistic regression maximum margin linear svms binary classiﬁcation. like ramp like loss functions principal advantages approach robust dealing outliers compared traditional convex loss functions. modiﬁed algorithm adds learning hyper-parameter optimization step shows improved performance original optimization algorithm also presented explored kernelized version approach yields performance competitive non-linear svms binary classiﬁcation. furthermore gaussian-laplacian mixture prior parameters kernel betabernoulli model able yield sparser solutions svms retaining competitive classiﬁcation performance. interestingly increase training database size approach exhibited logarithmic scaling properties compares favourably linear scaling properties svms. best knowledge ﬁrst exploration gauss-laplace mixture prior parameters certainly combination novel smooth zero-one loss formulation. ability prior behave like smooth approximation counting prior similar approach known bridge regression statistics. however mixture formulation ﬂexibility compared simpler functional form bridge regression. interestingly combination generalized beta-bernoulli loss gaussian-laplacian parameter prior though smooth relaxation learning zero loss counting prior regularization formulation classiﬁcation intuitively attractive remained elusive practice now. also tested generalized beta-bernoulli models structured prediction task arising problem face mining wikipedia biographies. also model showed better performance traditional logistic regression based approaches tested independent models compared sub-parts bayesian network based structured prediction framework. experiment shows signs model optimization approach proposed potential used complex structured prediction tasks.", "year": 2015}