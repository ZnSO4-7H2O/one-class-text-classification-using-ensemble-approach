{"title": "Unsupervised, Efficient and Semantic Expertise Retrieval", "tag": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "abstract": "We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching.", "text": "expertise retrieval task gained popularity research community trec enterprise track remained relevant ever since broadening social media tracking dynamics expertise existing methods fail address challenges queries expert documents different representations describe concepts term mismatches queries experts occur inability widely used maximum-likelihood language models make semantic similarities words amount available data increases need powerful approaches greater learning capabilities smoothed maximum-likelihood language models obvious supervised methods expertise retrieval introduced turn last decade. however acceleration data availability major disadvantage that case supervised methods manual annotation efforts need sustain similar order growth. calls development unsupervised methods. expertise retrieval methods language model constructed every document collection. methods lack efﬁcient query capabilities large document collections query term needs matched every document proposed solution strong emphasis unsupervised model construction efﬁcient query capabilities semantic matching query terms candidate experts. speciﬁcally propose unsupervised log-linear model efﬁcient inference capabilities expertise retrieval task. show approach improves retrieval performance compared vector space-based generative language models mainly ability perform semantic matching method require supervised relevance judgments able learn textual evidence document-candidate associations alone. purpose work provide insight discriminative language models improve performance core retrieval tasks compared maximum-likelihood language models. therefore avoid explicit feature engineering incorporation external evidence paper. terms performance current best-performing formal language model exhibits worst-case time complexity linear size document collection. contrast inference time complexity approach asymptotically bounded number candidate experts. research questions follows discriminative log-linear model compare vector space-based methods generative language models expert retrieval task terms retrieval performance? learn regarding different types errors made generative discriminative language models? complexity inference log-linear model compare vector-space based generative abstract introduce unsupervised discriminative model task retrieving experts online document collections. exclusively employ textual evidence avoid explicit feature engineering learning distributed word representations unsupervised way. compare model state-of-the-art unsupervised statistical vector space probabilistic generative approaches. proposed log-linear model achieves retrieval performance levels state-of-the-art document-centric methods inference cost so-called proﬁle-centric approaches. yields statistically signiﬁcant improved ranking vector space generative models cases matching performance supervised methods various benchmarks. using solely text well methods work external evidence and/or relevance feedback. contrastive analysis rankings produced discriminative generative approaches shows complementary strengths ability unsupervised discriminative model perform semantic matching. transition knowledge information economy introduces great reliance cognitive capabilities crucial employers facilitate information exchange stimulate collaboration past organizations would set-up special-purpose database systems members maintain proﬁle however systems required employees proactive. addition self-assessments known diverge reality document collections quickly become practically infeasible manage manually. therefore active interest automated approaches constructing expertise proﬁles retrieving experts organization’s heterogeneous document repository expert ﬁnding addresses task ﬁnding right person appropriate skills knowledge attempts provide answer question electronic version article published international world wide conference. international world wide conference committee. april montréal québec canada. ----//. http//dx.doi.org/./.. contribute unsupervised log-linear model efﬁcient inference capabilities expertise retrieval task together open-source implementation. comparison retrieval performance log-linear model traditional vector space-based models language model methods well-known benchmarks. insights certainty predictions log-linear model correlates performance. in-depth analysis inference complexity log-linear model. comparative error analysis semantic log-linear model traditional generative language models perform exact matching. insight relative strengths semantic matching exact matching expert retrieval task. remainder paper organized follows. section brieﬂy discuss related work. section introduces log-linear model expert retrieval problem. section state research questions detail experimental set-up implementations. provide overview experimental results followed discussion analysis section section concludes paper discusses ideas future work. ﬁrst discuss prior work expert retrieval relation document retrieval. then review semantic search methods. ideas present paper inspired neural language models used automated speech recognition natural language processing consequently also review work ﬁelds. expert retrieval early expert retrieval systems often referred expert locator expertise management systems database systems often relied people self-assess expertise predeﬁned topics known generate unreliable results introduction pnoptic system later trec enterprise track active research interest automated expertise proﬁling methods. useful distinguish proﬁle-based methods create textual representation candidate’s knowledge document-based methods represent candidates weighted combination documents. latter generally performs better ranking former efﬁcient avoids retrieving documents relevant query much research generative probabilistic models expert retrieval models categorized candidate generation models topic generation models proximity-based variants special relevance unsupervised proﬁle-centric document-centric models balog focus textual evidence without incorporating collection-speciﬁc information supervised discriminative models preferred query-candidate relevance pairs available training. unlike generative counterparts models issue combining complex heterogeneous features resemble learning rank methods document retrieval however lack training data greatly hinder applicability beyond unsupervised generative supervised discriminative approaches graph-based approaches based random walks voting-based approaches based data fusion demartini propose vector space-based method entity ranking task; framework extends vector spaces operating documents entities. survey topic. note query document mismatch poses critical challenge search. semantic matching important attempt remedy problem. much work bridging semantic various different tasks expertise document retrieval closely related performance latter greatly impact former latent semantic models ﬁrst became popular introduction latent semantic indexing followed probabilistic lsms based neural networks emerged last decade. salakhutdinov hinton employ unsupervised deep auto-encoders documents patterns using semantic addressing. huang perform semantic matching documents queries leveraging clickdata optimizing document ranking. further deep models proposed learn rank large-vocabulary neural probabilistic language models modeling word sequence distributions become popular recently models learn continuous-valued distributed representations words also known embeddings order ﬁght curse dimensionality increase generalization introducing expectation similar word vectors signify semantically syntactically similar words. recurrent neural language models shown perform well collobert propose uniﬁed neural network architecture various tasks. even recently surge multimodal neural language models lend task automated image captioning. related work described following. work model conditional probability expertise candidate given single query term process learn distributed vector representation words candidates nearby representations indicate semantically similar concepts. propose log-linear model similar neural language models. important difference predict candidate expert instead next word. best knowledge ﬁrst propose solution. employ embedding layer shallow model reasons mentioned above learn continuous word representations incorporate semantic syntactic similarity tailored expert’s domain. denotes unnormalized score normalizing term. transformation log-space well-known trick prevent ﬂoating point underﬂow given inference straight-forward. given query compute rank candidate experts descending order probability. deﬁnes neural network single hidden layer. additional layers. preliminary experiments however show shallow log-linear model performs well-enough cases. larger data sets notice marginal gain adding additional layer projection matrix softmax layer expense longer training times loss transparency. matrices vector constitute parameters model. estimate using error back propagation follows. every document construct ideal distribution candidates based document-candidate associations continue extracting n-grams remains ﬁxed training. every n-gram generated document compute using model constructing optimize cross-entropy using batched gradient descent. loss function single batch instances associated targets follows refers document n-gram extracted dmax maxd∈d indicates longest document collection weight regularization parameter. update rule particular parameter given single batch size setting paper document collection predeﬁned candidate experts documents represented sequence words w|d| originating vocabulary operator denotes document length tokens. every document write denote candidates assod∈d cd). document-candidate associations obtained explicitly document meta-data implicitly mining references candidates document text. notice documents might associated candidate. presented query constituent terms t|q| expert retrieval task return list candidates ordered according topical expertise. generate ranking using relatively shallow neural network directly models employ vector-based distributed representations words candidate experts motivates unsupervised construction features express regularities expertise ﬁnding domain. representations capture similarity concepts closeness representations vector space. concepts similar feature activations interpreted model similar even interchangeable. address expert search task model rank candidates accordingly given propose unsupervised discriminative approach obtain probabilities. construct model solely textual evidence require query-candidate relevance assessments training consider external evidence corpus document link-based features. denote size vector-based distributed representations words candidate experts representations learned model using gradient descent notational convenience write probability distribution candidates result vector arithmetic. deﬁne probability candidate given single word log-linear model projection matrix maps one-hot representation word e-dimensional distributed representation |c|-dimensional bias vector matrix maps word embedding unnormalized distribution candidates normalj= bc)]j. consider bayes’ theorem transformation matrix bias vector interpreted term log-likelihood candidate log-prior respectively. projection matrix attempts soften curse dimensionality introduced large vocabularies maps words word feature vectors support large vocabularies crucial retrieval tasks assume conditional independence candidate’s expertise given observation data given sequence denote per-parameter learning rate parameter time respectively. learning rate consists number elements parameters; case global learning rate elements equal other. derivatives loss function given appendix. next section discuss experimental setup followed overview experimental results analysis section research questions discriminative log-linear model compare vector space-based methods generative language models expert retrieval task terms retrieval performance? particular model perform compared vector space-based generative approaches learn regarding different types errors best-performing generative model simply perform slightly better topics models perform decent well make different errors? latter holds ensemble rankings produced model types might exceed performance individual rankings. complexity inference log-linear model worst-case inference cost document-centric models makes unattractive online settings topics deﬁned beforehand document collection large. proﬁlecentric methods preferred settings infer language model candidate expert every topic vector space-based methods similar problems curse dimensionality consequently inferential time complexity likewise asymptotically bounded number experts. log-linear model handle incremental indexing proposed method applicable setting expert search task trec enterprise track therefore evaluate cerc benchmarks released track. dataset crawl wc’s sites june csiro enterprise research collection dump intranet australia’s national science agency. additionally evaluate method smaller recent benchmark based employee database tilburg university consists bi-lingual heterogeneous documents. table mining document-candidate associations inﬂuence performance extensively covered previous work beyond scope work. associations part benchmark. list possible candidates given extract associations performing case-insensitive match full name e-mail address cerc make publicly released associations compare approach existing unsupervised methods expert retrieval solely rely textual evidence static document-candidate associations. demartini propose generic framework adapt vector spaces operating documents entities. compare method tf-idf variants vector space model entity ranking terms language modeling balog propose models expert ﬁnding based generative language models. ﬁrst takes proﬁle-centric approach comparing language model every expert query second document-centric. consider models different smoothing conﬁgurations jelinek-mercer smoothing dirichlet smoothing equal average document length signiﬁcance results produced baselines determined using two-tailed paired randomization test implementation details vocabulary constructed corpus ignoring punctuation stop words case; numbers replaced numerical placeholder token. experiments prune retaining most-frequent words word encoded -bit unsigned integer. incomplete n-gram instances padded special-purpose token. matrix initialization scheme improves model training convergence take bias vector null. projection layer initialized pre-trained word representations trained google news data number word features similar pre-trained representations. used adadelta batched gradient descent weight decay training nvidia nvidia tesla gpus. iterate entire training experiment. start giving high-level overview experimental results address issues scalability provide error analysis discuss issue incremental indexing. overview experimental results evaluate log-linear model cerc benchmarks training extract non-overlapping n-grams cerc benchmarks overlapping n-grams benchmark. benchmark considerably smaller opted overlapping n-grams counter data sparsity. architecture benchmark model inherently speciﬁed benchmarks however choice n-gram size during training remains open. errors input propagated back projection matrix reached; single word causes large prediction error inﬂuence neighboring words well. larger window size negative impact batch throughput training. thus presented classic trade-off model performance construction efﬁciency. notice however number n-grams decreases window size increases extract non-overlapping instances. therefore larger values lead faster wall-clock time model construction cerc benchmarks experiments. sweep window width three benchmarks corresponding relevance assessments. report every conﬁguration observe signiﬁcant performance increase benchmarks underlines importance window size parameter. increase implies performance achieved solely initialization pre-trained representations model efﬁciently learns word representations tailored problem domain. highest scores attained relatively increases beyond gradual decrease observed benchmarks. remaining experiments choose regardless benchmark. words mainly occur documents associated particular expert learned produce distributions less uncertainty words associated many experts product aggregates expertise evidence generated query terms. hence queries strong evidence particular expert predictable generic queries. quantify uncertainty measure normalized entropy equation interpreted similarity measure given distribution uniform distribution. importantly figure shows statistically signiﬁcant negative correlation query-wise normalized entropy average precision benchmarks. table presents comparison log-linear model various baselines unsupervised method signiﬁcantly outperforms lsi-based method consistently. case tf-idf method proﬁle-centric generative language models always perform better statistical signiﬁcance achieved majority cases. document-centric language models perform slightly better method benchmarks terms ndcg cerc assessment match performance document-centric generative model jelinek-mercer smoothing. generative counterpart seems outperform method. notice assessments log-linear model consistently outperforms proﬁle-centric approaches challenged smoothed document-centric approach. addition precision-based measures log-linear model consistently outperforms methods compare next turn topic-wise comparative analysis discriminative generative models. that analyze scalability efﬁciency log-linear model compare generative counterparts address incremental indexing. log-linear model achieve superior performance established generative models? figure depicts per-topic differences average precision log-linear model model benchmarks. plot vertical bars positive difference correspond test topics log-linear model outperforms model vice versa bars negative difference. beneﬁt gained projection matrix two-fold. first avoids curse dimensionality introduced large vocabularies. second term similarity respect expertise domain encoded latent word features. examining words nearby query terms embedding space found words related query term. example word vector representations nonterminal similar benchmark observed figure log-linear models trained single words perform signiﬁcantly worse compared able learn neighboring words. take closer look topics log-linear model outperforms model vice versa. speciﬁcally investigate textual evidence related topic whether considered relevant benchmark. log-linear model examine terms nearby topic terms terms considered semantically similar model provide means semantic matching. every benchmark ﬁrst consider toptable evaluation results models trained cerc benchmarks. sufﬁxes denote dirichlet jelinek-mercer smoothing respectively signiﬁcance results determined using two-tailed paired randomization test respect log-linear model topics speciﬁcation return candidates associated deﬁnition standards. log-linear model however considers close related technologies utf- semantic matching works topics semantic coordination annotea server protocol former associated libraries rdf-related jargon names researchers ﬁeld latter associated implementations protocol maintainer project. cerc csiro topic nanohouse mentioned many irrelevant contexts therefore semantic matching fails. term quickly associated different kinds oils organizations related marines ﬁsheries. hand observe sensor networks associated sensor/networking jargon sensor platforms. topic forensic science workshop expands syntactically-similar terms names science laboratories references benchmark contains english dutch textual evidence. topics sustainable tourism interpolation beneﬁt semantic matching semantic interpolation associated polynomial kind relevance assessments focus stochastic methods. interestingly topic informatization/computerization dutch translation closely related. similar terms informatization according log-linear model dutch words related cryptography. similar dynamics work legal-political space translated terms semantic-syntactic relations performance. order quantify effect embedding matrix artiﬁcially expand benchmark topic terms nearby terms. examine performance proﬁle-centric generative language model evolves different values purpose analysis provide insight differences maximum-likelihood language models log-linear model. figure shows that benchmarks increases goes interestingly enough benchmarks exhibit decrease larger likewise generative language models outperform log-linear model table suggests cerc benchmarks require exper-topic differences suggest model log-linear model make different errors model excels retrieving exact query matches log-linear model performs semantic matching. based observations hypothesize combination approaches raise retrieval performance even further. test hypothesis propose simple ensemble rankings generated model log-linear model re-ranking candidates according multiplicatively-combined reciprocal rank rankensemble rankm denotes position candidate ranking generated model answering query equation equivalent performing data fusion using combsum scores given logarithm reciprocal ranks experts. table compares result ensemble constituents. compared supervised methods fang conclude fully unsupervised ensemble matches performance method cerc benchmark outperforms method benchmark. superior performance ensemble suggests viability hybrid methods combine semantic exact matching. inference log-linear model expressed linear algebra operations operations efﬁciently performed highly optimized software libraries special-purpose hardware baseline methods compare beneﬁt speed-ups. furthermore many implementation-speciﬁc details choice parameter values log-linear model generates ranking candidate experts straight-forward matrix operations. look-up operation projection matrix occurs constant time complexity multiplication one-hot vector comes selecting i-th column multiplication matrix e-dimensional word feature vector exhibits runtime complexity. consider addition bias term division normalizing function time complexity becomes notice however analysis considers sequential execution. modern computing hardware ability parallelize common matrix operations number candidate experts term impacts performance log-linear model figure scatter plot normalized entropy distribution returned log-linear model per-query average precision cerc benchmarks. pearson’s associated p-value parentheses. depicted linear obtained using ordinary least squares regression. table comparison model log-linear model ensemble former cerc benchmarks. signiﬁcance results determined using two-tailed paired randomization test respect ensemble ranking terms space complexity parameters addition intermediate results require memory space proportional size. considering sequence words batches instances require ﬂoating point numbers every forward-pass in-memory. upper bound seems reasonable modern computing standards severely limiting factor considering large-scale communities utilizing limited-memory gpus fast computation. inferential complexity vector space-based models entity retrieval depends mainly dimensionality vectors number candidate experts. dimensionality latent entity representations high efﬁcient nearest neighbor retrieval curse dimensionality. therefore time complexity lsitf-idf-based vector space models respectively denotes number latent topics lsi-based method. hyperparameters indicate dimensionality latent entity representations time complexity lsi-based method comparable log-linear model. note benchmarks consider paper therefore conclude tf-idf method loses log-linear model terms efﬁciency. compared unsupervised generative models balog proﬁle-centric model document-centric model inference time complexity respectively |c|. previous section showed log-linear model always performs better model nearly always outperforms model hence log-linear model generally achieves expertise retrieval performance model complexity cost model inference. existing unsupervised methods well-understood maximumlikelihood language models support incremental indexing. brieﬂy discuss incremental indexing capabilities proposed method. extending candidate experts requires log-linear model re-trained scratch changes topology network. moreover every document associated candidate expert considered negative example candidates. possible reiterate past documents learn additional matrix ﬁnal outcome unpredictable. consider stream documents instead predeﬁned log-linear model learned online fashion. however stochastic gradient descent requires training examples picked random batched update rule behaves like empirical expectation full training might able justify assumption documents arrive randomly n-grams extracted documents clearly violate requirement. considering stream documents leads model forgetting expertise evidence shift underlying distribution training data occurs. behavior undesirable task considered paper might well-suited temporal expert ﬁnding expertise drift time considered. however temporal expertise ﬁnding beyond scope paper left future work. introduced unsupervised discriminative log-linear model expert retrieval task. approach exclusively employs textual evidence. future work focus improving performance feature engineering incorporation external evidence. furthermore relevance feedback required training. renders model suitable broad range applications domains. evaluated model cerc benchmarks compared state-of-the-art vector space-based entity ranking language modeling approaches. log-linear model combines ranking performance best maximumlikelihood language modeling approach inference time complexity linear number candidate experts. observed notable increase precision existing methods. analysis model’s output reveals negative correlation per-query performance ranking uncertainty higher conﬁdence rankings produced approach often occurs together higher rank quality. error analysis log-linear model traditional language models shows make different errors. errors mainly semantic query intent textual evidence. benchmarks expect exact query matches others helped semantic matching. ensemble methods employing exact semantic matching generally outperforms individual methods. observation calls research area combining exact semantic matching. current limitation work scalability respect number candidate experts. started investigating trade-offs model performance time/space complexity. future hope apply scalable variants method large-scale social media communities purpose determining topic ownership. work focus expertise retrieval ideas proposed easily transferred general entity retrieval task. moreover approach likely applicable authorship attribution various entity retrieval prediction tasks. research supported amsterdam data science dutch national program commit elsevier european community’s seventh framework programme grant agreement research network program elias royal dutch academy sciences elite network shifts project microsoft research ph.d. program netherlands escience center under project number netherlands institute sound vision netherlands organisation scientiﬁc research project hor-- ci-- sh-- yahoo faculty research engagement program yandex. content represents opinion authors necessarily shared endorsed respective employers and/or sponsors. computing resources provided netherlands organisation scientiﬁc research allocation sh-- cartesius system advanced school computing imaging allocation distributed ascii supercomputer system. becerra-fernandez. role artiﬁcial intelligence technologies implementation people-finder knowledge management systems. knowledge-based systems", "year": 2016}