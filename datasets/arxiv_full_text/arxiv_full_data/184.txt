{"title": "Return of Frustratingly Easy Domain Adaptation", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being \"frustratingly easy\" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.", "text": "figure domain shift scenarios object recognition across visual domains sentiment prediction across text domains data distributions differ across domains applying classiﬁers trained domain directly another domain likely cause signiﬁcant performance drop. developed assume labeled examples target domain provided learn proper model adaptation. daume proposed supervised domain adaptation approach notable extreme simplicity merely changes features making domainspeciﬁc common copies trains supervised classiﬁer features domains. method performs well frustratingly easy implement. however cannot applied situations target domain unlabeled unfortunately quite common practice. work present frustratingly easy unsupervised domain adaptation method called correlation alignment coral aligns input feature distributions source target domains exploring second-order statistics. concretely coral aligns distributions re-coloring whitened source features covariance target distribution. coral simple efﬁcient computations needs computing covariance statistics domain applying whitening re-coloring linear transformation source features. then supervised learning proceeds usual–training classiﬁer transformed source features. despite frustratingly easy coral offers surprisingly good performance standard adaptation tasks. apply tasks object recognition sentiment prediction show outperforms unlike human learning machine learning often fails handle changes training test input distributions. domain shifts common practical scenarios severely damage performance conventional machine learning methods. supervised domain adaptation methods proposed case target data labels including perform well despite frustratingly easy implement. however practice target domain often unlabeled requiring unsupervised adaptation. propose simple effective efﬁcient method unsupervised domain adaptation called correlation alignment coral minimizes domain shift aligning second-order statistics source target distributions without requiring target labels. even though extraordinarily simple–it implemented four lines matlab code–coral performs remarkably well extensive evaluations standard benchmark datasets. machine learning different human learning. humans able learn labeled examples apply learned knowledge examples novel conditions. contrast supervised machine learning methods perform well given extensive labeled data distribution test distribution. theoretical practical results shown test error supervised methods generally increases proportion difference distributions training test examples. example donahue showed even state-of-the-art deep convolutional neural network features learned dataset images susceptible domain shift. addressing domain shift undoubtedly critical successfully applying machine learning methods real world applications. many existing methods. object recognition demonstrate works well standard bag-ofwords features state-of-the-art deep features outperforming existing methods including recent deep adaptation approaches latter approaches quite complex expensive requiring re-training network tuning many hyperparameters structure hidden adaptation layers. contrast coral needs compute covariance source target features. domain shift fundamental problem machine learning also attracted attention speech natural language vision communities supervised adaptation variety techniques proposed. consider source domain prior regularizes learning problem sparsely labeled target domain e.g. others minimize distance target source domains either re-weighting domains changing feature representation according explicit distribution distance metric learn transformation features using contrastive loss arguably simplest prominent supervised approach frustratingly easy feature replication given feature vector deﬁnes augmented feature vector data points source data points target. classiﬁer trained augmented features. approach simple however requires labeled target examples often available real world applications. early techniques unsupervised adaptation consisted re-weighting training point losses closely reﬂect test distribution dictionary learning methods learn dictionary difference source target domain minimized representation. recent state-of-the-art unsupervised approaches pursued adaptation projecting source target distributions lower-dimensional manifold ﬁnding transformation brings subspaces closer together. geodesic methods path along subspace manifold either project source target onto points along path closedform linear projects source points target alternatively subspaces aligned computing linear minimizes frobenius norm difference however approaches align bases subspaces distribution projected points. also require expensive subspace projection hyperparameter selection. figure illustration correlation alignment domain adaptation original source target domains different distribution covariances despite features normalized zero mean unit standard deviation. presents problem transferring classiﬁers trained source target. domains source decorrelation i.e. removing feature correlations source domain. target re-correlation adding correlation target domain source features. step source target distributions well aligned classiﬁer trained adjusted source domain expected work well target domain. might instead attempt align distributions whitening source target. however fail since source target data likely different subspaces domain shift. adaptive deep neural networks recently explored unsupervised adaptation. dlid trains joint source target architecture limited adaptation layers. reversegrad directly optimize deep representation domain invariance using additional loss layers designed purpose. training additional loss costly sensitive initialization network structure optimization settings. approach applied deep features achieves better comparable performance complex methods incorporated directly network structure. present extremely simple domain adaptation method– correlation alignment –which works aligning distributions source target features unsupervised manner. propose match distributions aligning second-order statistics namely covariance. algorithm think transformation intuitively whitens source data secﬁrst part usς+ re-colors target part covariance. illustrated figure figure respectively. traditional whitening adding small regularization parameter diagonal elements covariance matrix explicitly make full rank multiply original feature inverse square root whitening re-coloring slightly different since data likely lower dimensional space covariance matrices could rank. practice sake efﬁciency stability perform classical whitening coloring. advantageous because faster stable original covariance matrices might stable might slow converge; illustrated figure performance similar analytical solution equation stable respect paper ﬁnal algorithm written four lines matlab code illustrated algorithm might instead attempt align distributions whitening source target. shown figure fail source target data likely different subspaces domain shift. alternative approach would whitening target re-coloring source covariance. however demonstrated formulation derivation describe method taking multi-class classiﬁcation problem running example. suppose given source-domain training examples {xi} labels {yi} target data {ui} ddimensional feature representations input suppose feature vector means covariance matrices. illustrated figure feature normalization minimize distance second-order statistics source target features apply linear transformation original source features frobenius norm matrix distance metric covariance transformed source features rank rank analytical solution obtained choosing however data typically lower dimensional manifold covariance matrices likely rank derive solution general case using following lemma. lemma real matrix rank real matrix rank largest singular values corresponding left right singular vectors optispectively. then solution problem theorem moore-penrose pseudoinverse denote rank respec tively. then usς+ optimal solution problem equation min. proof. since linear transformation acsa since increase rank thus symmetric matrices conducting gives usσsus respectively. ﬁrst optimal value considering following cases case optimal solution thus optimal solution equation gether coral avoids subspace projection costly requires selecting hyper-parameter controls dimensionality subspace. note subspace-mapping approaches align eigenvectors source target covariance matrices. contrary coral aligns covariance matrices re-constructed using eigenvectors eigenvalues. even though eigenvectors aligned well distributions still differ difference eigenvalues corresponding eigenvectors source target data. coral general much simpler method takes account eigenvectors eigenvalues covariance matrix without burden subspace dimensionality selection. relationship methods maximum mean discrepancy based methods domain adaptation interpreted moment matching express arbitrary statistics data. minimizing polynomial kernel similar coral objective however previous work used kernel domain adaptation proposed closed form solution best knowledge. difference based approaches usually apply transformation source target domain. demonstrated asymmetric transformations ﬂexible often yield better performance domain adaptation tasks. intuitively symmetric transformations space ignores differences source target domain asymmetric transformations bridge domains. suppose computed multilayer neural network inputs layer suffer covariate shift well. batch normalization tries compensate internal covariate shift normalizing mini-batch zero-mean unitvariance. however illustrated figure normalization might enough. even used full whitening batch normalization compensate external covariate shift layer activations decorrelated source point target point. what’s more mentioned section whitening domains still work. method easily integrated deep architecture treating layers features although experiment coral applied hidden layer time multilayer coral could used implementing transformations extra layers follow original layer experiments transforming data source target space gives better performance. might fact transforming source target space classiﬁer trained using label information source unlabelled structure target. coral transforms source features target space classiﬁer parametrized trained adjusted source features directly applied target features. linear classiﬁer apply equivalent transformation parameter vector instead features results added efﬁciency number classiﬁers small number dimensionality target examples high. since correlation alignment changes features only applied base classiﬁer. efﬁciency also especially advantageous target domains changing rapidly e.g. scene changes course long video stream. relationship existing methods relationship feature normalization long known input feature normalization improves many machine learning methods e.g. however coral simply perform feature normalization rather aligns different distributions. standard feature normalization address issue illustrated figure example although features normalized zero mean unit variance dimension differences correlations present source target domains cause distributions different. relationship manifold methods recent state-of-theart unsupervised approaches project source target distributions lower-dimensional manifold transformation brings subspaces closer toevaluate method object recognition sentiment analysis shallow deep features using standard benchmarks protocols. experiments assume target domain unlabeled. follow standard procedure linear base classiﬁer. model selection approach used parameter cross-validation source domain. since hyperparameters required method results paper easily reproduced. compare published methods accuracies reported authors conduct experiments using source code provided authors. object recognition experiments domain adaptation used improve accuracy object classiﬁer novel image domains. standard ofﬁce extended ofﬁce-caltech datasets used benchmarks paper. ofﬁce-caltech contains object categories ofﬁce environment image domains ebcam dslr amazon caltech. standard ofﬁce dataset contains object categories domains ebcam dslr amazon. later also conduct larger scale evaluation ofﬁce-caltech cross-dataset testbed dataset. object recognition shallow features follow standard protocol conduct experiments ofﬁce-caltech dataset shallow features surf features encoded -bin bag-of-words histograms normalized zero mean unit standard deviation dimension. since four domains experiment settings namely mazon test altech) mazon test slr) follow standard protocol conduct experiments randomized trials domain shift average accuracy trials. trial standard setting randomly sample number labelled images source domain training unlabelled data target domain test set. well adaptation baseline manifold based methods project source target distributions lower-dimensional manifold. integrates inﬁnite number subspaces along subspace manifold using kernel trick. aligns source target subspaces computing linear minimizes frobenius norm difference. performs domain adaptation parametric kernel using feature extraction methods projecting data onto learned transfer components. introduces smoothness assumption enforce target classiﬁer share similar decision values source classiﬁers. even though methods complicated require tuning hyperparameters method achieves best average performance across domain shifts. method also improves adaptation baseline cases increasing accuracy signiﬁcantly object recognition deep features follow standard protocol conduct experiments standard ofﬁce dataset deep features. dlid trains joint source target architecture interpolating path source target domain. dann incorporates maximum mean discrepancy measure regularization reduce distribution mismatch. da-nbnn presents nbnn-based domain adaptation algorithm iteratively learns class metric inducing large margin separation among classes. decaf uses alexnet pretrained imagenet extracts layers source domains features train classiﬁer. applies classiﬁer target domain directly. adds domain confusion loss alexnet ﬁne-tunes source target domain. reversegrad recent domain adaptation approaches based deep architectures. similar utilizes multi-kernel selection method better mean embedding matching adapts multiple layers. reversegrad introduces gradient reversal layer allow direct optimization back-propagation. reversegrad binary classiﬁcation task treating source target domain classes. maximize binary classiﬁcation loss obtain invariant features. fair comparison apply coral pre-trained alexnet alexnet ﬁne-tuned source however ﬁne-tuning procedures reversegrad complicated loss hyper-parameters needed combine them. also require adding layers na-fc na-fc na-ft na-ft sa-fc sa-fc sa-ft sa-ft gfk-fc gfk-fc gfk-ft gfk-ft tca-fc tca-fc tca-ft tca-ft dlid dann da-nbnn decaf-fc decaf-fc reversegrad coral-fc coral-fc coral-ft coral-ft data source target domains. standard ﬁne-tuning source domain baseline results since three domains experiment settings. follow protocol conduct experiments random training/test splits mean accuracy domain shift. table compare method baseline methods discussed before. again method outperforms techniques almost cases sometimes large margin. note deep structures based methods report results settings. higher level fc/ft features lead better performance fc/ft. what’s more baselines also achieve good performance even better manifold methods deep methods. however coral outperforms consistently method achieves better performance across shifts. also achieves better peformance latest deep interesting ﬁnding that although ﬁne-tuning source domain achieve better performance target domain compared pre-trained network applying coral ﬁne-tuned network achieves much better performance applying coral pre-trained network possible explanation pretrained network might underﬁtting ﬁne-tuned network overﬁtting. since coral aligns source feature distribution target distribution overﬁtting becomes less problem. larger scale evaluation section repeat evaluation larger scale. conduct sets experiments investigate dataset size number classes affect performance domain adaptation methods. sets experiments full training protocol source data used training compared standard subsampling protocol previous sections. since target data used previous sections difference settings training dataset size source domain. direct comparison table conduct ﬁrst experiments ofﬁce-caltech dataset surf features. investigate effect number classes conduct second experiments cross-dataset testbed dataset images caltech images imagenet images classes using publicly available deep features tables compare coral available source code well baseline. table shows result ofﬁce-caltech dataset table shows result cross-dataset testbed dataset. experiments coral outperforms baseline methods margin deep features much larger shallow features. comparing table table performance difference between methods smaller source data used. fact training data used classiﬁer stronger generalize better domains. sentiment analysis also evaluate method sentiment analysis using standard amazon review dataset processed data dimensionality bag-of-words features reduced keep words without losing performance. dataset contains amazon reviews domains kitchen appliances books electronics. domain positive negative reviews. follow standard protocol conduct experiments random training/test splits report mean accuracy domain shift. table compare method published methods well adaptation baseline precursor interpolates features using ﬁnite number subspaces. introduces structural correspondence learning automatically induce correspondences among features different domains. presents nonparametric method directly produce re-sampling weights without distribution estimation. interesting observation that sentiment analysis task three stateof-the-art methods actually perform worse adaptation baseline despite difﬁculty task coral still performs well achieves interesting result margin coral published methods much larger deep features bag-of-words features. could deep features strongly correlated bag-of-words features similarly improvement images much larger text possibly bag-of-words text features extremely sparse less correlated image features. demonstrated high level deep features parts objects’. intuitively parts objects strongly correlated edges article proposed simple efﬁcient effective method domain adaptation. method frustratingly easy implement computation involved recoloring whitened source features covariance target domain. extensive experiments standard benchmarks demonstrate superiority method many existing state-of-the-art methods. results conﬁrm coral applicable multiple features types including highlyperforming deep features different tasks including computer vision natural language processing. authors would like thank mingsheng long judy hoffman trevor darrell helpful discussions suggestions; reviewers valuable comments. tesla used research donated nvidia corporation. research supported awards iis- iis-.", "year": 2015}