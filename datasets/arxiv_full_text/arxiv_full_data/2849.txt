{"title": "Addressing Function Approximation Error in Actor-Critic Methods", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and critic. Our algorithm takes the minimum value between a pair of critics to restrict overestimation and delays policy updates to reduce per-update error. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.", "text": "value-based reinforcement learning methods deep q-learning function approximation errors known lead overestimated value estimates suboptimal policies. show problem persists actor-critic setting propose novel mechanisms minimize effects actor critic. algorithm takes minimum value pair critics restrict overestimation delays policy updates reduce per-update error. evaluate method suite openai tasks outperforming state every environment tested. reinforcement learning problems discrete action spaces problem value overestimation result function approximation errors well-studied problem. however similar problems actor-critic methods continuous control domains largely left unstudied. paper show overestimation bias accumulation error temporal difference methods present actor-critic setting. proposed method works consideration challenges greatly outperforms current state art. temporal difference learning estimate value function updated using estimate subsequent state. subsequent estimate true value update might poor. furthermore update leaves residual error repeatedly propagated throughout state space nature temporal difference update. accumulated error cause arbitrarily states estimated high value resulting sub-optimal policy updates divergent behavior. discrete action setting property exaggerated updates greedy policy selects actions maximum approximate value causes consistent overestimation demonstrate overestimation property also present deterministic policy gradients furthermore na¨ıve generalization successful solution double actor-critic setting effective actor-critic. deal concern adapt q-learning variant double q-learning actor-critic format using pair critics unbiased value estimation. however unbiased estimate high variance still lead overestimations local regions state space turn negatively affect global policy. address concern propose clipped double q-learning variant favors underestimations bounding situations double q-learning poorly. situation preferable underestimations tend propagated learning states value estimate avoided policy. given role variance overestimation bias paper contains number components address variance reduction. first show target networks common approach deep q-learning methods critical variance reduction. second address coupling value policy propose delaying policy updates target converged. finally introduce novel regularization strategy sarsa-style update bootstraps similar action estimates reduce variance. modiﬁcations applied state actorcritic method continuous control deep deterministic policy gradients form twin delayed deep deterministic policy gradients actor-critic algorithm considers interplay function approximation error policy value updates. evaluate algorithm seven continuous control domains openai outperform state wide margin. given recent concerns reproducibility experiments across large number seeds fair evaluation metrics perform detailed ablation across contribution open source code learning curves. videos algorithm found github. function approximation error effect bias variance reinforcement learning algorithms studied prior works work focuses outcomes occur estimation error namely overestimation bias high variance build-up. several approaches exist reduce effects overestimation bias function approximation policy optimization q-learning. example double q-learning uses independent estimators make unbiased value estimates approaches focused directly reducing variance minimizing over-ﬁtting early high variance estimates corrective terms further variance value estimate considered directly estimate uncertainty agent exploration method. accumulation error temporal difference learning largely dealt either minimizing size errors time step mixing off-policy montecarlo returns. methods multi-step returns tradeenvironment policy variance accumulated estimation error bias shown effective approach importance sampling distributed methods approximate bounds however rather provide direct solution accumulating error methods circumvent problem considering longer horizon. another approach reduction discount factor reducing contribution error. method builds deterministic policy gradients actor-critic method uses learned value estimate train deterministic policy. extension deep reinforcement learning ddpg shown produce state results efﬁcient number iterations. recent improvements ddpg include distributed methods along multi-step returns prioritized experience replay distributional methods improvements orthogonal approach. experiments focus single-step return case improvements propose compatible multi-step returns. contrast method work done adapting deep q-learning methods continuous actions replace selection action highest estimated value various approximations reinforcement learning considers paradigm agent interacting environment learning reward-maximizing behavior. discrete time step given state agent selects actions respect policy receiving reward state environment return deﬁned discounted rewards γi−tr discount factor determining priority short-term rewards. reinforcement learning objective optimal policy parameters maximizes expected return policies learned maximizing learned estimate expected cumulative reward known value function. value function policy deﬁned expected return performing action state following after discrete action space greedy policy selects highest valued action often used. however continuous action space value-maximizing action cannot usually found analytically. instead actor-critic methods used policy known actor trained maximize estimated expected return deﬁned value estimate known critic. policy updated deterministic policy gradient theorem large state space value function estimated differentiable function approximator parameters deep q-learning requires target networks secondary frozen network qθtarget used maintain ﬁxed objective multiple updates although overestimation minimal update raises concerns. firstly overestimation develop signiﬁcant bias many updates left unchecked. secondly inaccurate value estimate lead repeated poor policy updates. particularly problematic feedback loop created sub-optimal actions might highly rated suboptimal critic reinforcing sub-optimal action next policy update. theoretical overestimation occur practice state-of-the-art methods? answer question plotting value estimate ddpg time learns mujoco hopper-v walkerd-v environments figure graph average value estimate states compare estimate true value. true value estimated using average discounted return evaluations following current policy starting states sampled buffer replay. clear overestimation bias occurs learning procedure contrasts novel method describe later greatly reduces overestimation critic. following section learning procedure actor-critic algorithms induce overestimation value actions taken sub-optimal policy. several approaches reducing overestimation bias proposed ineffective actor-critic setting. section introduces novel clipped variant double q-learning effectively critic. πφtarget additional target actor network. target network either updated periodically exactly match weights current network proportion time step θtarget θtarget. update applied off-policy fashion sampling random mini-batches experiences following current previous policies experience replay buffer q-learning estimation error lead overly optimistic value estimates i.e. overestimation bias. discrete action setting value estimate updated greedy target argmaxa target susceptible error maxa result initially random function approximation error transformed consistent overestimation bias propagated bellman equation. discrete action setting overestimation bias obvious artifact analytical maximization presence effects overestimation bias less clear actor-critic setting. begin proving value estimate deterministic policy gradients overestimation basic assumptions section propose clipped variant double q-learning actor-critic setting reduce overestimation bias section actor-critic methods policy updated respect value estimates approximate critic. section assume policy updated using deterministic policy gradient theorem show update induces overestimation value estimate. given current policy parameters φapprox deﬁne parameters actor update induced maximization approximate critic φtrue parameters hypothetical actor update respect true value function assume chosen normalize gradient i.e. z−||e|| without normalized gradients overestimation bias still guaranteed occur slightly stricter conditions examine case supplementary material intuoverestimation introduced value estimate. itively acts unbiased estimate acts approximate upper-bound value estimate. update rule induce underestimation bias preferable overestimation bias unlike overestimated actions value underestimated actions explicitly propagated policy update. note overestimation function approximation good measure uncertainty thus using over-estimating critic effective implement optimism face uncertainty lead suboptimal policies implementation computational costs reduced using single actor optimized respect target update identical standard update induces additional bias. suggests overestimation occurred value reduced similar double qlearning. proof convergence ﬁnite setting follows intuition. provide formal details justiﬁcation supplementary material treating function approximation error random variable minimum operator result higher value states lower variance estimation error expected minimum random variable decreases variance random variables increases. effect means minimization equation lead preference states low-variance value estimates leading safer policy updates stable learning targets. section emphasize importance minimizing error update build connection target networks estimation error propose modiﬁcations learning procedure actor-critic variance reduction. temporal difference update estimate value function built estimate subsequent state build function approximation error. reasonable expect small td-errors given bellman update estimation errors accumulate resulting potential large overestimation bias suboptimal policy updates. following equation denoting shown other. result target values unbiased estimates value actions selected policy. double authors propose using target network value estimates obtain policy greedy maximization current value network rather target network. actor-critic setting analogous update uses current policy rather target policy learning target γqθtarget πφ)). practice however found slow-changing policy actor-critic current target networks similar make independent estimation offered little improvement original learning target. show effect empirically section instead original double-q formulation used pair actors critics optimized respect respect optimizes respect using target update avoid bias introduced policy update. unbiased estimate occasionally problematic because meaning generally overestimate certain areas state space overestimation exaggerated. follows variance estimate proportional variance future rewards td-errors. given large discount factor variance grow rapidly update error update tamed. furthermore gradient update reduces errors respect small mini-batch gives little guarantee size errors value estimates outside mini-batch. high variance estimates problematic introduce overestimation bias provide noisy gradient policy update. greatly increases likelihood poor local minima primary concern methods using gradient-based policies. section examine relationship target networks function approximation error show stable target reduces growth error noted above. insight allows consider interplay high variance estimates policy performance suggest delaying policy updates effective strategy improve convergence. target networks critical tool achieve stability deep reinforcement learning value estimates quickly diverge without them. deep function approximators require multiple gradient updates converge target networks provide stable objective learning procedure allow greater coverage training data. without ﬁxed target update leave residual error begin accumulate. provide intuition examine learning behavior without target networks figure graph value similar manner figure mujoco hopper-v environment compare behavior ﬁxed policy examine value estimates policy continues learn trained current value estimate. target network uses slow moving update rate parametrized figure average estimated value randomly selected state hopper-v without target network slow updating target network ﬁxed learned policy. uses logarithmic scale true value denoted circles corresponding color. similar convergent behaviors considering ﬁxed policy. however policy trained current value estimate quick updating target networks result highly divergent behavior. worst case behavior updates high enough rate allow error build-up results slower corrections results suggest divergence occurs without target network product policy update high variance value estimate. actor-critic methods divergent? results suggest divergence occurs without network product policy update high variance value estimate. figure well section suggest failures occur interplay actor critic updates. value estimates diverge overestimation policy poor policy become poor value estimate inaccurate. target networks used reduce error multiple updates policy updates high error states cause divergence policy network updated lower frequency value network ﬁrst minimize error introducing policy update. using slowmoving target network helps regard propose delay policy updates value error small possible. proposed modiﬁcation freeze target network ﬁxed number updates update policy error minimized ﬁnally update target network slowly θtarget θtarget td-error remains small. sufﬁciently delaying policy updates limit likelihood repeated updates respect similar critics. even value estimate high variance average estimated gradient respect critic closer true gradient. less frequent policy updates occur lower variance critic principle result higher quality policy updates. captured empirical results presented section evaluate algorithm measure performance suite mujoco continuous control tasks interfaced openai allow reproducible comparison brockman al.’s original tasks modiﬁcations environment reward. implementation ddpg layer feed forward neural network hidden nodes respectively rectiﬁed linear units layer actor critic ﬁnal tanh unit following output actor. unlike original ddpg networks receive state action input ﬁrst layer. network parameters updated using adam learning rate without regularization. time step networks trained mini-batch variance function approximation error reduced regularization. introduce regularization strategy deep value learning target policy noise enforce notion similar actions similar value. function approximation implicitly relationship forced explicitly modifying training procedure. ideally rather ﬁtting exact target value ﬁtting value small area around target action would beneﬁt smoothing value estimate bootstrapping similar state-action value estimates. reduces variance value estimate thus function approximation error cost introducing negative bias. practice approximate expectation actions adding small amount random noise target policy averaging mini-batches. makes modiﬁed target update added noise clipped keep target small range. results algorithm reminiscent sarsa expected-sarsa value estimate instead learned offpolicy target policy noise chosen independently exploration policy. value estimate learned respect noisy policy deﬁned parameter intuitively known policies derived sarsa value estimates tend safer provide higher value actions resistant perturbations. thus style update additionally lead improvement stochastic domains failure cases. present twin delayed deep deterministic policy gradients builds deep deterministic policy gradients applying modiﬁcations described sections increase stability performance consideration function approximation error. maintains pair critics along single actor. time step update pair critics towards minimum target value actions selected target policy figure learning curves openai continuous control tasks. shaded region represents half standard deviation average evaluation trials. graphs cropped display interesting regions. transitions sampled uniformly experience replay buffer containing entire history agent. target policy noise implemented adding actions chosen target actor network clipped delayed policy updates consists updating actor target critic network every iterations fair comparison ddpg critics trained time step. larger would result larger beneﬁt training actor iterations would cripple learning. target networks updated remove dependency initial parameters policy purely exploratory policy ﬁrst time steps stable length environments ﬁrst time steps remaining environments. afterwards off-policy exploration strategy adding gaussian noise action. unlike original implementation ddpg used uncorrelated noise exploration found noise drawn ornstein-uhlenbeck task million time steps evaluations every time steps evaluation reports average reward episodes exploration noise. results reported random seeds simulator network initialization. compare algorithm ddpg well state policy gradient algorithms acktr trpo implemented openai’s baselines repository additionally compare method re-tuned version ddpg includes architecture hyper-parameter modiﬁcations ddpg without adjustments target policy updates. full comparison implementation algorithm baselines ddpg provided supplementary material perform ablation studies understand contribution individual component clipped double q-learning delayed policy updates target policy noise present results table compare performance removing component along modiﬁcations architecture hyperparameters actor-critic variants double-q learning described section ddpg denoted double- single critic variant double- double critic variant. fairness comparison methods also beneﬁted delayed policy updates target policy noise architecture hyper-parameters. full learning curves found supplementary material table average return last evaluations trials million time steps comparing ablation delayed policy target policy noise clipped double q-learning architecture hyper-parameters exploration maximum value task bolded. signiﬁcance component varies task task. halfcheetah-v environment architecture contributes largest improvement ant-v improvements almost entirely target. although actor trained half number iterations inclusion delayed policy update causes loss performance across environments except halfcheetah-v. full algorithm outperforms every method tasks ablation proposed components performed worst every task. addition single component causes insigniﬁcant improvement cases addition combinations performs much higher level. compare effectiveness double q-learning variants double- double figure table slow-changing nature actor-critic target networks double- remain close current networks capture independence. result double generally causes insigniﬁcant change results clipped double q-learning double- shows notable improvement walkerd-v ant-v. problems alleviated double- however still greatly outperformed full tasks. suggests subduing overestimations unbiased estimator effective measure improve performance. shown overestimation bias occurs actorcritic methods presence worsens empirical performance. consideration develop novel clipped variant double q-learning actor-critic methods. justify target network deep value learning role function approximation error reduction. finally introduce sarsa-style target regularizer bootstrap similar state-action pairs. show ﬁnal method improves overestimation bias learning speed performance number challenging tasks continuous settings exceeding performance numerous state algorithms. modiﬁcations simple implement maintain off-policy nature q-learning easily added deep q-learning alongside numerous improvements references anschel oron baram shimkin nahum. averaged-dqn variance reduction stabilization deep reinforcement learning. international conference machine learning barth-maron gabriel hoffman matthew budden david dabney will horgan dhruva muldal alistair heess nicolas lillicrap timothy. distributional policy gradients. international conference learning representations donghun defourny boris powell warren bias-corrected q-learning control max-operator bias q-learning. adaptive dynamic programming reinforcement learning ieee symposium ieee lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. arxiv preprint arxiv. dhariwal prafulla hesse christopher plappert matthias radford alec schulman john sidor szymon yuhuai. openai baselines. https//github.com/ openai/baselines pakman tishby naftali. taming noise reinforcement learning soft updates. proceedings thirty-second conference uncertainty artiﬁcial intelligence auai press shixiang lillicrap timothy sutskever ilya levine sergey. continuous deep q-learning modelbased acceleration. international conference machine learning henderson peter islam riashat bachman philip pineau joelle precup doina meger david. deep rearxiv preprint inforcement learning matters. arxiv. horgan quan john budden david barth-maron gabriel hessel matteo hasselt hado silver david. distributed prioritized experience replay. international conference learning representations mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous internamethods deep reinforcement learning. tional conference machine learning munos r´emi stepleton harutyunyan anna bellemare marc. safe efﬁcient off-policy reinforcement learning. advances neural information processing systems todorov emanuel erez tassa yuval. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference ieee seijen harm hasselt hado whiteson shimon wiering marco. theoretical empirical analysis expected sarsa. adaptive dynamic programming reinforcement learning adprl’. ieee symposium ieee yuhuai mansimov elman grosse roger liao shun jimmy. scalable trust-region method deep reinforcement learning using kronecker-factored approximation. pendrith mark ryan malcolm estimator variance reinforcement learning theoretical problems practical solutions. university south wales school computer science engineering popov ivaylo heess nicolas lillicrap timothy hafner roland barth-maron gabriel vecerik matej lampe thomas tassa yuval erez riedmiller martin. data-efﬁcient deep reinforcement learning dexterous manipulation. arxiv preprint arxiv. schaul quan john antonoglou ioannis silver david. prioritized experience replay. international conference learning representations puerto rico schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. international conference machine learning silver david lever heess nicolas degris thomas wierstra daan riedmiller martin. deterministic policy gradient algorithms. international conference machine learning singh satinder jaakkola tommi littman michael szepesv´ari csaba. convergence results single-step on-policy reinforcement-learning algorithms. machine learning ﬁnite setting version clipped double q-learning maintain tabular value estimates time step select actions argmaxa perform update setting target ﬁnite setting double q-learning often used deal noise induced random rewards state transitions either updated randomly. however function approximation setting interest towards approximation error thus update iteration. proof extends naturally updating either randomly. proof borrows heavily proof convergence sarsa well double q-learning proof lemma found singh building proposition bertsekas lemma consider stochastic process satisfy equation clipped double q-learning converge optimal value function deﬁned bellman optimality equation probability proof theorem apply lemma first note condition lemma holds conditions theorem respectively. lemma condition holds theorem condition along selection deﬁning argmaxa gradients deterministic policy gradient update unnormalized overestimation still guaranteed occur slightly stronger condition expectation value estimate. assume approximate value function equal true value function expectation steady-state distribution respect policy parameters original policy direction true policy update noting true maximizes rate change true value ∆qπ) given condition maximal rate change approximate value must least great ∆qθ) ∆qπ). summing change condition shows overestimation value function. critic learning rate critic regularization actor learning rate actor regularization optimizer target update rate batch size iterations time step discount factor reward scaling normalized observations gradient clipping exploration policy", "year": 2018}