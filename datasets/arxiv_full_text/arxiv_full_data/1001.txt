{"title": "Stochastic Neural Networks with Monotonic Activation Functions", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We propose a Laplace approximation that creates a stochastic unit from any smooth monotonic activation function, using only Gaussian noise. This paper investigates the application of this stochastic approximation in training a family of Restricted Boltzmann Machines (RBM) that are closely linked to Bregman divergences. This family, that we call exponential family RBM (Exp-RBM), is a subset of the exponential family Harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron. Using contrastive divergence along with our Gaussian approximation, we show that Exp-RBM can learn useful representations using novel stochastic units.", "text": "propose laplace approximation creates stochastic unit smooth monotonic activation function using gaussian noise. paper investigates application stochastic approximation training family restricted boltzmann machines closely linked bregman divergences. family call exponential family subset exponential family harmoniums expresses family members choice smooth monotonic non-linearity neuron. using contrastive divergence along gaussian approximation show exp-rbm learn useful representations using novel stochastic units. deep neural networks produced best results complex pattern recognition tasks training data abundant. here interested deep learning generative modeling. recent years witnessed surge interest directed generative models trained using back-propagation models distinct deep energy-based models including deep boltzmann machine deep belief network rely bipartite graphical model called restricted boltzmann machine layer. although gaussian noise stochastic units introduce paper potentially used stochastic back-propagation paper limited applications rbm. choice stochastic units constrained well-known members exponential family; past rbms used units bernoulli gaussian categorical gamma poisson conditional distributions. exception specialization rectiﬁed linear unit introduced sampling procedure limitation well-known exponential family members despite fact welling introduced generalization rbms called exponential family harmoniums covering large subset exponential family bipartite structure. architecture suggest procedure connecting arbitrary non-linearities importantly general sampling procedure missing. introduce useful subset call exponential family rbms approximate sampling procedure addressing shortcomings. basic idea exp-rbm simple restrict suﬃcient statistics identity function. allows deﬁnition unit using mean stochastic activation non-linearity neuron. restriction gain interpretability also trainability; show possible eﬃciently sample activation stochas concluding remarks welling suggest capability indeed desirablea future challenge therefore start modelling process desired non-linearity subsequently introduce auxiliary variables facilitate inference learning. neurons train resulting model using contrastive divergence. interestingly restriction also closely relates generative training exp-rbm discriminative training using matching loss regularization noise injection. following section introduces exp-rbm family section investigates learning exprbms eﬃcient approximate sampling procedure. here also establish connections discriminative training produce interpretation stochastic units exp-rbms inﬁnite collection bernoulli units diﬀerent activation biases. section demonstrates eﬀectiveness proposed sampling procedure combined contrastive divergence training data representation. choices normalization constants base measures produce diverse subsets exponential family. exp-rbm family suﬃcient statistics identity functions. exp-rbm restricts suﬃcient statistics single identity functions means single weight matrix ri×j before hidden unit receives wijvi similarly visible unit table stochastic units conditional distribution gaussian approximation distribution. polylogarithmic function equal condition satisﬁed zero otherwise. problem relating local conditionals joint form graphical models goes back work besag easy check that using general treatment yang joint form corresponding conditional learn generative model need able sample distributions deﬁne expectations sampling joint model also reduced alternating conditional sampling visible hidden variables many methods including contrastive divergence stochastic maximum likelihood variations require alternating sampling order optimize approximation gradient represents probability neuron ﬁring times unit time given average rate deﬁne poisson units within exp-rbm using gives properly normalized since non-negative integer gives tribution means available sampling routine poisson distribution learn parameters exponential family units poisson. section modiﬁed version knuth’s method poisson sampling. making simplifying assumption following laplace approximation demonstrates gaussian noise sample general conditionals exprbm smooth monotonic non-linearity. loss convex model parameters common matching loss given transfer function simply bregman divergence minimizing matching loss corresponds maximizing log-likelihood surprising however note generative training simply equal sampled exponential family distribution mean noise. extends previous observations linking discriminative generative training gaussian noise injection noise members exponential family turn relates regularizing role generative pretraining neural networks figure conditional probability diﬀerent stochastic units gaussian approximation wijvi vertical axis stochastic activation intensity table details stochastic units. section gives interpretation exp-rbm terms bernoulli inﬁnite collection bernoulli units. nair hinton introduce softplus unit approximation rectiﬁed linear unit max. probabilistic interpretation nonlinearity authors represent inﬁnite series bernoulli units shifted bias figure histogram hidden variable activities mnist test data diﬀerent types units. units heavier tails produce longer strokes figure note linear decay activities log-domain correspond exponential decay diﬀerent exponential coeﬃcients. evaluates ﬁlters learned various units and; section evaluates exp-rbms smaller dataset indirect sampling likelihood quantify generative quality models diﬀerent activation functions. objective demonstrate combination sampling scheme contrastive divergence training indeed produce generative models diverse choice activation function. figure instances mnist dataset. dataset momentum train model epochs. figure shows ﬁlters different stochastic units; table details diﬀerent stochastic units. here units ordered based asymptotic behavior activation function figure shows integrals diﬀerent values showing base measure constant relu. spite this experimental results pretraining relu units using gaussian noise suggests usefulness type approximation. evaluate representation capabilities exprbm diﬀerent stochastic units following sections. initial attempt adapt annealed importance sampling exp-rbms. however estimation importance sampling ratio general exprbm proved challenging. consider alternatives large datasets section qualitatively following series sigmoid function need adjusted depending limits. example case antisymmetric unbounded sinh sinh− |ηj|}) need change domain bernoulli units +.}. corresponds changing sigmoid hyperbolic tangent ηj). case also need change bounds series figure samples mnist dataset ﬁlters highest variance diﬀerent exprbm stochastic units bottom non-linearities grow rapidly also producing features represent longer strokes. right margin ﬁgure. asymptotic change activation function also evident hidden unit activation histogram figure activation produced test using trained model. ﬁgures suggest transfer functions faster asymptotic growth heavy-tailed distributions activations longer strokes mnist dataset also hinting preferable learning representation however comes cost trainability. particular exponential units occasionally large gradients reduce learning rate sigmoid/tanh unit remains stable learning rate factors aﬀect instability training exponential quadratic exp-rbms large momentum small number hidden units. initialization weights could also play important role sparse initialization regularization schemes could potentially improve training models. experiments used uniformly random values test set. point estimate likelihood test data hidden unit type every iteration updates. likelihood test data using density estimate produced directly training data gives upper-bound models. figure presents quantities hidden unit type present results learning rate achieves highest isl. ﬁgure shows estimated log-likelihood well function number epochs. number iterations increases models produce samples representative also consistent values getting closer training optimal parameter training set. general found stochastic units deﬁned using relu sigmoid/tanh numerically stable. however problem requ learns best model even increasing steps also increasing epochs factor could produce similar results using tanh units. shows non-linearities outside circle well-known commonly used exponential family sometimes produce powerful generative models even using approximate sampling procedure. paper studies subset exponential family harmoniums single suﬃcient statistics purpose learning generative models. resulting family distributions exp-rbm gives freedom choice activation function individual units paralleling freedom discriminative training neural networks. moreover possible eﬃciently train arbitrary members family. this introduced principled eﬃcient approximate sampling procedure demonstrated various exprbms learn useful generative models ﬁlters.", "year": 2016}