{"title": "Cached Long Short-Term Memory Neural Networks for Document-Level  Sentiment Classification", "tag": ["cs.CL", "cs.NE"], "abstract": "Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets.", "text": "recently neural networks achieved great success sentiment classiﬁcation ability alleviate feature engineering. however remaining challenges model long texts document-level sentiment classiﬁcation recurrent architecture deﬁciency memory unit. address problem present cached long short-term memory neural networks capture overall semantic information long texts. clstm introduces cache mechanism divides memory several groups different forgetting rates thus enables network keep sentiment information better within recurrent unit. proposed clstm outperforms state-of-the-art models three publicly available document-level sentiment analysis datasets. sentiment classiﬁcation widely used natural language processing techniques many areas e-commerce websites online social networks political orientation analyses etc. recently deep learning approaches gained encouraging results sentiment classiﬁcation frees researchers handcrafted feature engineering. among methods recurrent neural networks sentenceparagraph-level sentiment analysis expects model extract features limited source information document-level sentiment analysis demands selecting storing global sentiment message long texts noises redundant local pattern. simple rnns powerful enough handle overﬂow pick sentiment messages relatively time-steps efforts made solve scalability problem long texts extracting semantic information hierarchically ﬁrst obtain sentence representations combine generate high-level document embeddings. however solutions either rely explicit priori structural assumptions discard order information within sentence vulnerable sudden change twists texts especially long-range recurrent models match people’s intuition reading word word capable model intrinsic relations sentences. keeping word order rnns could extract sentence representation implicitly meanwhile analyze semantic meaning whole document without explicit boundary. partially inspired neural structure human brain computer system architecture present cached long short-term memory neural networks capture long-range sentiment information. dual store memory model proposed atkinson shiffrin memories reside short-term buffer limited time simultaneously strengthening associations long-term memory. accordingly clstm equips standard lstm similar cache mechanism whose internal memory divided several groups different forgetting rates. group high forgetting rate plays role cache model bridging transiting information groups relatively lower forgetting rates. different forgetting rates clstm learns capture remember forget semantics information long distance. main contributions follows introduce cache mechanism diversify internal memory several distinct groups different memory cycles squashing forgetting rates. result model capture local global emotional information thereby better summarizing analyzing sentiment long texts fashion. beneﬁting long-term memory unit forgetting rate could keep gradient stable long back-propagation process. hence model could converge faster standard lstm. model outperforms state-of-the-art methods large margin three document-level datasets worth noticing previous methods utilized extra user product information. section brieﬂy introduce related work areas first discuss existing documentlevel sentiment classiﬁcation approaches; second discuss variants lstm address problem storing long-term information. document-level sentiment classiﬁcation document-level sentiment classiﬁcation sticky task sentiment analysis infer sentiment polarity intensity whole document. challenging part every part document equally informative inferring sentiment whole document various methods investigated explored years methods depend traditional machine learning algorithms need effective handcrafted features. recently neural network based methods prevalent ability learning discriminative features data integrate tree-structured model lstm better semantic composition; bhatia enhances document-level sentiment analysis using extra discourse paring results. models work well sentence-level paragraph-level sentiment classiﬁcation. comes document-level sentiment classiﬁcation bottom-up hierarchical strategy often adopted alleviate model complexity memory augmented recurrent models although widely accepted lstm long-lasting memory units rnns still suffers forgetting information away current point scalability problem lstms crucial extend previous sentence-level work document-level sentiment analysis. various models proposed increase ability lstms store long-range information kinds approaches gain attraction. augment lstm external memory poor performance time huge external memory matrix. unlike methods fully exploit potential internal memory lstm adjusting forgetting rates. tries multiple time-scales distinguish different states partition hidden states several groups group activated updated different frequencies methhowever memory slower groups updated every step lead sentiment information loss semantic inconsistency. proposed clstm assign different forgetting rates memory groups. novel strategy enable memory group updated every time-step every long-term shortterm memories previous time-step taken account updating. long short-term memory network typical recurrent neural network alleviates problem gradient diffusion explosion. lstm capture long dependencies sequence introducing memory unit gate mechanism aims decide utilize update information kept memory cell. logistic sigmoid function. operator element-wise multiplication operation. input gate forget gate output gate memory cell activation vector time-step respectively size hidden vector rh×d rh×h trainable parameters. here dimensionality hidden layer input respectively. dealing considerably long texts lstm also fails capturing understanding signiﬁcant sentiment message speciﬁcally error signal would nevertheless suffer gradient vanishing modeling long texts hundreds words thus network difﬁcult train. since standard lstm inevitably loses valuable features propose cached long short-term memory neural networks capture information longer steps introducing cache mechanism. moreover order better control balance historical message incoming information adopt particular variant lstm proposed greff coupled input forget gate lstm coupled input forget gate lstm previous studies show merged version gives performance comparable standard lstm language modeling classiﬁcation tasks using input gate forget gate simultaneously incurs redundant information cached lstm cached long short-term memory neural networks aims capturing long-range information cache mechanism divides memory several groups diffigure overview proposed architecture. different styles arrows indicate different forgetting rates. groups stars fully connected layers softmax classiﬁcation. instance b-clstm text length equal number memory groups bidirectional clstm graves schmidhuber proposed bidirectional lstm model utilizes additional backward information thus enhances memory capability. also employ bi-directional mechanism clstm words text receive information sides context. formally outputs forward lstm k-th group outputs backward hence encode word given text indicates concatenation operation. task-speciﬁc output layer document-level sentiment classiﬁcation capability modeling long text proposed model analyze sentiment document. figure gives overview architecture. since ﬁrst group slowest group supposed keep long-term information better represent whole document utilize ﬁnal state group represent document. b-clstm concatenate state ﬁrst group forward lstm time-step ﬁrst group backward lstm ﬁrst time-step. different groups capture different-scale dependencies squashing scales forgetting rates. groups high forgetting rates short-term memories groups forgetting rates long-term memories. specially divide memory cells groups {g··· gk}. group includes internal memory output gate forgetting rate forgetting rate different groups squashed distinct ranges. represents forgetting rate k-th memory group step squash function constrains value forgetting rate within range. better distinguish different role group forgetting rate squashed distinct area. squash function could formalized intuitively forgetting rate approaches group tends long-term memory; approaches group tends shortterm memory. therefore group slowest group fastest one. faster groups supposed play role cache transiting information faster groups slower groups. table statistics three datasets used paper. rating scale yelp yelp range imdb ranges words/doc average length sample sents/doc average number sentences document. yelp yelp review datasets derived yelp dataset challenge year respectively. sentiment polarity review star stars reveals consumers’ attitude opinion towards restaurants. evaluation metrics accuracy evaluation metrics sentiment classiﬁcation. accuracy standard metric measure overall classiﬁcation result mean squared error used ﬁgure divergences predicted sentiment labels ground truth ones. then fully connected layer followed softmax function used predict probability distribution classes given input. formally probability distribution objective model minimize crossentropy error predicted true distributions. besides objective includes regularization term parameters. formally suppose train sentence label pairs section study empirical result model three datasets document-level sentiment classiﬁcation. results show proposed model outperforms competitor models several aspects modelling long texts. datasets existing datasets sentiment classiﬁcation stanford sentiment treebank composed short paragraphs several sentences cannot evaluate effectiveness model circumstance encoding long texts. evaluate model three popular real-world datasets yelp yelp table sentiment classiﬁcation results model competitor models imdb yelp yelp evaluation metrics classiﬁcation accuracy mse. models user product information additional features. best results group bold. cifg-lstm cifg-blstm coupled input forget gate lstm blstm denoted cifg-lstm cifg-blstm respectively combine input forget gate lstm require smaller number parameters comparison standard lstm. parameter conﬁguration choose parameters validation mainly according classiﬁcation accuracy convenience always strong correlation accuracy. dimension pre-trained word vectors dimension hidden units choose weight decay among adagrad optimizer initial learning rate batch size chosen among efﬁciency. clstm number memory groups chosen upon dataset discussed later. remain total number hidden units unchanged. given neurons instance four memory groups neurons. makes model comparable lstm. table shows optimal hyper-parameter conﬁgurations dataset. model initialization initialize recurrent matrices randomly sampling uniform distribution besides glove pre-trained word vectors. word embeddings ﬁne-tuned training. hyper-parameters achieving best results validation chosen ﬁnal evaluation test set. classiﬁcation accuracy mean square error models compared competitive models shown table comparing models neural network models several meaningful ﬁndings. among unidirectional sequential models fails capture store semantic features vanilla lstm preserves sentimental messages much longer rnn. shows internal memory plays role text modeling. cifg-lstm gives performance comparable vanilla lstm. help bidirectional architecture models could look backward forward capture features long-range global perspective. sentiment analysis users show opinion beginning review single directional models possibly forget hints. proposed clstm beats cifg-lstm vanilla lstm even surpasses bidirectional models. yelp clstm achieves accuracy percent worse b-clstm reveals cache mechanism successfully effectively stored valuable information without support bidirectional structure. model achieved state-of-the-art results large margin documentlevel datasets terms classiﬁcation accuracy. moreover b-clstm even surpassed jmars methods utilized extra user product information. terms time complexity numbers parameters model keeps almost counterpart models models hierarchically composition require computational resources time. rate convergence compare convergence rates models including cifg-lstm cifg-blstm bclstm baseline models conﬁgure hyper-parameter make sure every competing model approximately numbers parameters various models shown different convergence rates figure terms convergence rate b-clstm beats competing models. reason bclstm converges faster splitting memory groups seen better initialization constraints training process. also investigate performance model imdb encodes documents different lengths. test samples divided groups regard length. figure draw several thoughtful conclusions. overall performance b-clstm better cifg-blstm. means model adaptive short texts long documents. besides model shows power dealing long texts comparison cifg-blstm. paper address problem effectively analyzing sentiment document-level texts architecture. similar memory structure human memory forgetting rate captures global semantic features memory high forgetting rate captures local semantic features. empirical results three real-world document-level review datasets show model outperforms state-of-the-art models large margin. figure study model sensitivity document length imdb. test samples sorted length divided parts. left means classiﬁcation accuracy shortest samples. x-axis length ranking different number memory groups datasets. diagram model outperforms baseline method. yelp split memory groups achieves best result among tested memory group numbers. observe dropping trends choose groups. appreciate constructive work xinchi chen. besides would like thank anonymous reviewers valuable comments. work partially funded national natural science foundation china national high technology research development program china", "year": 2016}