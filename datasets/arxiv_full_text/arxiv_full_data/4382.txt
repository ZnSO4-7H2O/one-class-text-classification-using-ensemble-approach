{"title": "Universal representations:The missing link between faces, text,  planktons, and cat breeds", "tag": ["cs.CV", "stat.ML"], "abstract": "With the advent of large labelled datasets and high-capacity models, the performance of machine vision systems has been improving rapidly. However, the technology has still major limitations, starting from the fact that different vision problems are still solved by different models, trained from scratch or fine-tuned on the target data. The human visual system, in stark contrast, learns a universal representation for vision in the early life of an individual. This representation works well for an enormous variety of vision problems, with little or no change, with the major advantage of requiring little training data to solve any of them.", "text": "figure humans posses internal visual representation that works well number visual domains objects faces planktons characters. paper investigate universal representations constructing neural networks work simultaneously many domains learning share common visual structure obvious commonality exists. goal contrast capacity model total size combined vision problems. differently machines humans develop powerful internal representation images early years development representation subject slight reﬁnements even later life changes little. possible representation universal valence works equally well number problems reading text recognizing people contemplating art. existence non-trivial general-purpose representations means signiﬁcant part vision essentially learned all. however nature scope universal representations remains unclear. paper shed light question investigating extent deep neural networks shared extremely diverse visual domains advent large labelled datasets highcapacity models performance machine vision systems improving rapidly. however technology still major limitations starting fact different vision problems still solved different models trained scratch ﬁne-tuned target data. human visual system stark contrast learns universal representation vision early life individual. representation works well enormous variety vision problems little change major advantage requiring little training data solve them. paper investigate whether neural networks work universal representations studying capacity relation size large combination vision problems. showing single neural network learn simultaneously several different visual domains well better than number specialized networks. however also show requires carefully normalize information network using domainspeciﬁc scaling factors generically using instance normalization layer. performance machine vision systems nowadays believed comparable even superior human vision certain tasks narrow scope systems remains major limitation. fact vision human works well enormous variety different problems different neural networks required order recognize faces classify detect segment high-level object categories read text recognize bird ﬂower species interpret radiography echography image different parts human anatomy shown neural networks transfer knowledge tasks process adaptation called tuning. encouraging look much challenging problem learning single network works well problems simultaneously. several authors considered multi-task scenarios before task extract multiple labels visual domain tasks expected partially overlap since look object types. goal instead check whether extreme visual diversity still allows sharing information. order labelling task image classiﬁcation look combining numerous diverse domains setup simple allows investigate important question capacity models relation size combination multiple vision problems. problems completely independent total size grow proportionally matched equally unbounded increase model capacity. hand problems overlap complexity growth gradually slows down allowing model complexity catch that limit universal representations become possible. ﬁrst contribution show careful experimentation capacity neural networks large even contrasted complexity generated combining numerous diverse visual domains. example possible share layers including classiﬁcation ones datasets diverse cifar- mnist svhn without loss performance general extensive sharing parameters works well combination diverse domains. second contribution show that sharing possible notelessly requires normalize information carefully order compensate different dataset statistics test various schemes including domain-oriented batch instance normalization best method uses domain-speciﬁc scaling parameters learned compensate statistical differences datasets. however also show instance normalization used construct representation works well domains using single parameters without domain-speciﬁc tuning all. long propose deep adaptation network multi-task multi-branch network matches hidden representations task-speciﬁc layers reproducing kernel hilbert space reduce domain discrepancy. misra propose cross-stitch units combine activations multiple networks trained end-to-end. ganin lempitsky tzeng propose deep neural networks simultaneously optimized obtain domain invariant hidden representations maximising confusion domain classiﬁers. yosinski study transferability features deep neural networks between different tasks single domain. authors investigate layers pre-trained deep networks adapted tasks sequential manner. previous work explore various methods transfer different networks look learning universal representations diverse domains single neural network. work also related methods transfer information networks. hinton propose knowledge distillation method transfers information ensemble models single enforcing generate similar predictions existing ones. romero extend strategy encouraging similarity predictions also intermediate hidden representations different networks. chen address slow process sequential training teacher student networks scratch. authors accelerate learning process simultaneously training teacher student networks. line work focuses learning compact accurate networks task transferring knowledge different networks work aims learn single network perform well multiple domains. multi-task learning. multi-task learning extensively studied decades machine learning community. based idea tasks share common representation jointly learnt along task speciﬁc parameters. multi-task learning applied various computer vision problems reported achieve performance gains object tracking faciallandmark detection surface normals edge labels object detection segmentation object part detection contrast work multi-task learning typically focuses different tasks datasets. life-long learning. never ending learning lifelong learning learning many tasks sequentially retaining previously learnt knowledge. terekhov propose deep block-modular neural networks allow previously trained network learn task adding nodes freezing original network parameters. hoiem recently proposed term encodes regularization terms well hard constraints deﬁning structure learning problem. sharing. baseline separate neural networks learned domain. obtained regularizer decomposes additively case sharing common subset networks. example following common intuition early layers neural networks less specialized hence less domainspeciﬁc contain early layers depth different networks branch off. call ordinary feature sharing. adapted feature sharing. paper propose study alternatives ordinary feature sharing. abstractly interested minimizing difference between individual representations universal representation used common blueprint. example domains differ substantially statistics differences signiﬁcant impact response neurons possible compensate differences slightly adjusting representation parameters. another intuition features universal representation useful domains could deactivated depending problem. order explore ideas consider case representations small decompose number domain-dependent parameters universal representation blueprint. call adapted feature sharing. adapted feature sharing consider particular extremely simple form parametrization order able adjust different mean responses neurons domain well potentially select subset features consider adding learning without forgetting method learn task retaining responses original network task. main focus line research preserve information tasks tasks learned work aimed exploring capacity models multiple tasks learned jointly. call representation vectorial function mapping image rh×w× cdimensional code vector representations consider deep convolutional neural networks dcnn decomposed sequence linear non-linear functions called layers sophisticated cases feed-forward computational graph functions used nodes. concept important later data batches. neural networks almost invariably learned considering batches example images together. follow standard practice representing batch images adding fourth index data tensors rh×w××t information propagates network intermediate tensors also batches data points. consider problem learning neural networks multiple domains simplicity limit image classiﬁcation problems. hence domain consists input space rhd×wd×cd discrete label space joint probability distribution inputs labels loss function measuring quality label prediction ground truth value usual quality predictor measured terms expected risk e)]. domain furthermore also training training pairs results empirical risk goal learn predictors task order minimize overall risk. balancing different tasks interesting problem right simply choose minimize average risk figure left right three example modules instance normalization batch normalization batch normalization domainspeciﬁc scaling building modules. shaded blocks indicate learnable parameters. variants tested shown compactness. practice batch normalization instance normalization layers always immediately followed scaling layer discussed earlier section paper consider either ﬁxing scaling bias parameters across domains make domain speciﬁc. batch purity. model trained tested batches always pure i.e. composed data points single domain. simpliﬁes implementation importantly important effect layer. pure batch fact aggressively normalize datasetspeciﬁc biases would possible mixed batches. instead operates image-by-image basis affected choice pure mixed batches. important detail blocks used testing network learned. upon deploying architecture testing layers usually removed ﬁxing means variances ﬁxed averages accumulated several training batches unless done cannot evaluated individual images test time; furthermore removing usually slightly improves test performance also slightly faster. dropping requires care architecture difference pure batches different domains experiments test computing domainspeciﬁc means variances selected muxer collections share single means variances domains. also consider alternative setting applied training test times unchanged. disadvantage operate pure batches single images; advantage moments estimated onthe-ﬂy test batch instead pre-computed. together scale bias parameters form collections since domains trained jointly introduce also muxer namely layer extracts corresponding parameter given index current domain networks include batch instance normalization layers scaling layer already follows occurrence blocks. case simply adapt corresponding parameters rather introducing scaling layers. recently noted sometimes advantageous simplify batch normalization further consider instead instance normalization. instance normalization layer particular almost exactly expression mean covariance instance rather batch averages table top- error rate three datasets. individually trained networks dataset. deep sharing corresponds sharing convolutional last classiﬁer layer. full sharing corresponds sharing parameters including ﬁnal classiﬁer parameters domain-speciﬁc scale bias parameters. note three datasets classes allows share classiﬁer parameters. noted above training always consider pure batches. detail models learned means alternating batches domain roundrobin fashion. automatically balances datasets different sizes learning visits equal number training samples domain regardless different training sizes. corresponds weighing domain-speciﬁc loss functions equally. design also practical advantages. implementation different domains assigned different gpus. case computes model parameter gradients respect pure batch extracted particular dataset. gradients accumulated descent step. experiments focus image classiﬁcation problems scenarios. ﬁrst different architectures learning strategies evaluated portfolio diverse image classiﬁcation datasets planktons street numbers. computational reasons experiments consider relatively small pixels images tens thousands training images domain. second scenario test similar ideas larger datasets including imagenet less extensive manner computational cost small datasets data. choose image classiﬁcation tasks diverse domains including objects hand-written digit characters pedestrians sketches trafﬁc signs planktons house numbers. dataset statistics summarized table example images given table provides dataset introduced zero-shot learning class labels image. caltech- standard object classiﬁcation benchmark consists object categories additional background class. cifar consists colour object classes classes. daimler mono pedestrian classiﬁcation benchmark contains collection pedestrian non-pedestrian images. pedestrians cropped resized pixels. german trafﬁc sign recognition benchmark contains cropped images trafﬁc signs. sizes trafﬁc signs vary pixels. mnist contains handwritten digits centred images. omniglot consists different handwritten characters different alphabets. dataset originally designed shot learning. instead include character categories train test time. plankton imagery data classiﬁcation benchmark contains images various organisms ranging smallest single-celled protists copepods larval larger jellies. human sketch dataset contains human sketches every objects book house sun. street view house numbers real-world digit recognition dataset around images centred around single character resized pixels. majority datasets differ terms image resolutions characteristics images resized pixels greyscale ones converted setting three channels value. though would possible maintain images original resolution using single scale simpliﬁes network design. dataset also whitened subtracting mean dividing standard deviation channel. datasets ﬁxed train test splits ratio train test data respectively. architectures. choose state-of-the-art residual networks remarkable capacity performance. speciﬁcally experiment select resnet- model. network stack residual units convolutions feature size number ﬁlters respectively. network ends global average pooling layer fully connected layer followed softmax classiﬁcation. majority datasets different number classes dataset-speciﬁc fully connected layer experiments unless otherwise stated. explained section datasets balanced sampling batches different ones round-robin fashion training. follow data augmentation strategy size whitened image padded pixels sides patch randomly sampled padded image horizontal ﬂip. note mnist omniglot svhn contain digits characters augment ﬂipped images datasets. networks trained using stochastic gradient descent momentum. learning range gradually reduced short warm-up training learning rate weight decay momentum respectively. test time central crop images report percentages top- error rate. next experiment sharing features different depths architectures. resnet- model task stemming common trunk baseline sharing. baseline different resnet model trained scratch dataset convergence using batch size images although focus obtaining state-of-the-art results demonstrating effectiveness sharing representation among different domains chosen provides good speed performance trade-off achieves comparable results state-of-the-art methods much deeper resnet- obtains error rate cifar- dropconnect heavier multi-column network obtains mnist report svhn using sophisticated pooling mechanisms. network yields relatively error rates majority datasets absolute performance less good caltech. however inline results reported literature good performance datasets shown require pre-training large dataset imagenet short validates resnet- baseine good representative architecture. full sharing. next consider opposite sharing share parameters network common belief relatively shallow layers shareable whereas deeper ones domain-speciﬁc full sharing challenges notion. experiment resnet- conﬁgured domain-speciﬁc scaling parameters moments single trained three domains cifar mnist svhn domains happens contain exactly classes each. although cifar objects mnist/svhn digits nothing common randomly pair digits objects. allows share ﬁlter parameters including ﬁnal classiﬁcation layer realising full sharing. shown table evaluated different datasets performance network nearly learning three independent models. surprising result means model sufﬁcient capacity learn classiﬁers respond strongly either digit mnist svhn object cifar essentially learning operator. question whether combining problems together eventually exceed capacity network. deep sharing. next experiment sharing layers except last performs classiﬁcation. case therefore single convolutional layer contains rest network including last fully connected layer. setup similar full sharing allows combine classiﬁcation tasks even equal number classes. results table table show shared performs better training domain-speciﬁc networks. remarkably true tasks reduces average error rate remarkably improvement obtained reducing overall number parameters factor table top- error rate various tasks. table shows results case feature sharing different domains deep feature sharing convolutional weights partial sharing selected convolutional weights block block deep sharing convolutional ﬁlters partial sharing. here investigate whether beneﬁt specializing least part shared model individual tasks. test settings. ﬁrst setting network dataset-speciﬁc parameters shallower block beneﬁcial compensate different low-level statistics domains. second setting instead network specializes last block— beneﬁcial order capture different higher-level concepts different tasks. interestingly results table show deep sharing choice datasets model best conﬁguration. specializing last block marginally worse better specializing ﬁrst block. indicate high-level specialization preferable. network capacity. experiments suggested model sufﬁcient capacity accommodate tasks despite signiﬁcant diversity. fact individual networks perform worse single shared one. next increase capacity model keep sharing parameters tasks. order increase number convolutional ﬁlters twice four times increases number parameters times. differently learning independent networks setup allows model better added capacity accommodate different tasks reducing mean error rate points respectively. fact joint training exploit added capacity better suggests different domains overlap synergistically despite apparent differences. normalization strategies. shown learning single domains possible fact preferable learining individual models. however used speciﬁc normalization strategy well domain-speciﬁc scaling parameters moments choice scaling factors moments corresponds applying test time well pre-computed moments. inability compensate large variance among different domains. applied test time moments computed domain-agnostic scaling still used results better still poor domain-agnostic scaling works well long least moments domain-speciﬁc however best combination domain-speciﬁc moments scalings contrast normalizes images individually works well domain-speciﬁc domainagnostic scaling. price drop performance compared domain-speciﬁc parameters. however strategy signiﬁcant practical advantage domain-agnostic scaling effectively uses single parameters including ﬁlter weights scaling factors domains. suggest representation applicable novel domain without requiring domain-speciﬁc tuning normalization parameters all. data. part consider three large scale computer vision tasks object classiﬁcation imagenet face identiﬁcation vgg-face word classiﬁcation synthk dataset imagenet contains object categories million images. vgg-face dataset consists million face images different people centered resized ﬁxed height table top- error rate ﬁrst second rows show results alexnet without sharing parameters imagenet vgg-face datasets respectively. third fourth depict results vgg-m- without parameter sharing different capacities. indicates number parameters reduced thir individual network. last show results network sharing parameters imagenet vgg-face mjsynth datasets respectively. ated words different word classes. experiment higher-capacity model vgg-m- model ﬁlters second last fully connected layer instead synthk dataset contains classes small -dimensional bottleneck necessary order maintain size classes classiﬁer matrix reasonable. since synthk images much smaller datasets last downsampling layer used domain. without parameter sharing network performs similarly alexnet before convolutional layer parameters shared fully-connected layers parameters not. results show capacity model pushed limit. performance imagenet vgg-face still good minor larger drop synthk note total number parameters joint network third individual network parameters. order fair comparison evaluate performance three independent models third parameters show jointly trained model performs dramatically better individual models despite fact total number parameters same. machine vision consolidates challenge developing universal models that similarly human vision trained solve large variety problems come focus. component systems universal representations i.e. feature extractors work well visual domains despite signiﬁcant diversity latter. pixels variable width. synthk dataset contains approximately million synthetic images word lexicon generated ﬁxed height pixels variable width. show example images datasets table implementation details. imagenet images resized pixels shortest side maintaining original aspect ratio. training random patches cropped imagenet vggface synthk respectively. different input sizes lead different feature sizes last convolutional layer share convolutional feature maps among three tasks domain-speciﬁc fully connected layers. training augment data randomly cropping ﬂipping varying aspect ratio exception images synthk contain words. test time single center crop report top-error rates. best normalization strategy identiﬁed used. results. conduct experiments. ﬁrst network trained simultaneously imagenet vgg-face datasets signiﬁcant difference content well resolution domains. alexnet model adapt dimensionality ﬁrst fully connected layer vgg-face dataset train networks scratch. setting baseline obtained training individual networks without parameter sharing obtaining top- error rates imagenet vgg-face respectively sharing convolutional weights tasks achieve comparable performance illustrating high degree shareability representations. visual domains together high degree information sharing. however also shown successful sharing requires tuning certain normalization parameters networks preferably using domain-speciﬁc scaling factors order compensate inter-domain statistical shifts. alternatively techniques instance normalization compensate difference domain-agnostic manner. overall ﬁndings encouraging. universal representations seem within grasp current technology least wide array real-world problems. fact convincing results obtained smaller datasets believe larger problems addressed successfully moderate increase model capacity reﬁnements technology.", "year": 2017}