{"title": "Hierarchical Imitation and Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study the problem of learning policies over long time horizons. We present a framework that leverages and integrates two key concepts. First, we utilize hierarchical policy classes that enable planning over different time scales, i.e., the high level planner proposes a sequence of subgoals for the low level planner to achieve. Second, we utilize expert demonstrations within the hierarchical action space to dramatically reduce cost of exploration. Our framework is flexible and can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels of the hierarchy. Using long-horizon benchmarks, including Montezuma's Revenge, we empirically demonstrate that our approach can learn significantly faster compared to hierarchical RL, and can be significantly more label- and sample-efficient compared to flat IL. We also provide theoretical analysis of the labeling cost for certain instantiations of our framework.", "text": "study problem learning policies long time horizons. present framework leverages integrates concepts. first utilize hierarchical policy classes enable planning different time scales i.e. high level planner proposes sequence subgoals level planner achieve. second utilize expert demonstrations within hierarchical action space dramatically reduce cost exploration. framework ﬂexible incorporate different combinations imitation learning reinforcement learning different levels hierarchy. using long-horizon benchmarks including montezuma’s revenge empirically demonstrate approach learn signiﬁcantly faster compared hierarchical signiﬁcantly labelsample-efﬁcient compared also provide theoretical analysis labeling cost certain instantiations framework. learning good agent behavior reward signals alone— goal reinforcement learning—is particularly difﬁcult planning horizon long rewards sparse. successful method dealing long horizons imitation learning agent learns watching possibly querying expert. limitation existing imitation learning approaches require large amount demonstration data long-horizon problems. central question address paper experts available effectively leverage feedback? common strategy improve sample efﬁciency reinforcement learning long time horizons leverage hierarchical structure approach leverages hierarchical structure imitation learning. study case underlying problem domain hierarchical subtasks easily elicited expert. interested incorporating expert feedback learning process order speed learning time minimizing teaching effort expert begin formalizing problem hierarchical imitation learning carefully teases apart different cost structures naturally arise expert provides feedback multiple levels abstraction. present hierarchical imitation learning algorithm extends dagger algorithm two-level hierarchical setting design principle interactions expert minimized agent successfully learned subtasks; also provide theoretical analysis beneﬁts hierarchy next generalize approach horizon subtasks sufﬁciently short lower level learned reinforcement learning alone without expert feedback. leads novel algorithm combining imitation learning reinforcement learning finally show efﬁcacy approaches simple extremely challenging maze domain montezuma’s revenge case expert feedback available training framework reduces standard form hierarchical reinforcement learning show experiments incorporating modest amount expert feedback lead dramatic improvements performance compared pure hierarchical related work imitation learning. broadly dichotomize imitation learning passive collection demonstrations versus active collection demonstrations. former setting assumes demonstrations collected batch priori goal imitation learning policy mimic pre-collected demonstrations. latter setting assumes interactive expert provides demonstrations response actions taken current policy. explore extension approaches hierarchical settings. hierarchical reinforcement learning. several reinforcement learning approaches learning hierarchical policies explored foremost among options framework standard options framework often assumes useful options fully deﬁned priori planning learning occurs higher level. comparison agent direct access policies accomplish subgoals learn expert reinforcement feedback. closest hierarchical work approach kulkarni assumes similar hierarchical structure. mentioned introduction approach viewed special case framework expert feedback available training. combining reinforcement imitation learning. idea combining imitation learning reinforcement learning however previous work focuses policy classes imitation learning pre-training step contrast consider feedback multiple levels hierarchical policy class different levels potentially receiving different types feedback somewhat related hierarchical expert supervision approach andreas assumes access symbolic descriptions subgoals without knowing symbols mean execute them. sample complexity comparison imitation learning reinforcement learning studied much literature perhaps exception work simplicity consider environments natural two-level hierarchy; level corresponds choosing subtasks level corresponds executing subtasks. instance agent’s overall goal leave building. level agent ﬁrst choose subtask elevator take elevator down ﬁnally walk out. subtasks needs executed level actually navigatsubtasks also call subgoals denoted primitive actions denoted agent acts iteratively choosing subgoal carrying executing sequence actions completion picking subgoal. agent’s choices depend observed state assume horizon level i.e. trajectory uses subgoals horizon level i.e. primitive actions agent either accomplishes subgoal needs decide subgoal. total number primitive actions trajectory thus hfull hhihlo. hierarchical learning problem simultaneously learn hi-level policy called metacontroller well subgoal policies called subpolicies. learner achieve high reward meta-controller subpolicies together. subgoal also termination function {true false} terminates execution hierarchical agent behaves follows execution subpolicy generates lo-level trajectory hlo. overall behavior results hierarchical trajectory last state lo-level trajectory coincides next state ﬁrst state next lo-level trajectory τh+. subsequence excludes lo-level trajectories called hi-level trajectory denoted finally full trajectory τfull concatenation lo-level trajectories. labello lo-level labeling. expert provides good next primitive action towards given subgoal state given lo-level trajectory yielding labeled data agent learns subpolicies also termination functions labello also returns good termination values {true false} state yielding data although hierdemo label generated differ expert’s hierarchical policy mode expert interaction. hierdemo returns hierarchical trajectory executed expert required passive imitation learning thus enables hierarchical extension behavioral cloning hand label operations provide labels respect learning agent’s trajectories required interactive imitation learning. labelfull standard query used prior work learning policies labelhi labello hierarchical extensions. inspect operations newly introduced paper form cornerstone hierarchical interactive protocol enables substantial savings label efﬁciency. viewed lazy versions corresponding label operations requiring less effort. underlying assumption given hierarchical trajectory agrees expert level i.e. lo-level trajectories pass inspection i.e. inspectlo pass resulting full trajectory must also pass full inspection inspectfull pass. means hierarchical policy need always agree expert’s execution level succeed overall task. besides algorithmic reasons motivation separating types feedback different expert queries typically require different amount effort refer cost. assume costs label operations full costs inspect operation full. many settings lo-level inspection require signiﬁcantly less effort lo-level labeling i.e. instance identifying robot successfully navigated elevator presumably much easier labeling entire path elevator. reasonable cost model natural environments experiments assume inspect operations take time work checking ﬁnal state trajectory whereas label operations take time proportional trajectory length three label operations. begin section introducing hierarchical behavioral cloning needs passive access expert demonstrations. introduce hierarchical dagger best-performing algorithm provide theoretical analysis cost efﬁciency compared approach. algorithm uses hierarchical behavioral cloning warm start switches interactive mode expert labeling. consider natural extension behavioral cloning hierarchical setting expert provides hierarchical demonstrations consisting lo-level trajectories well h)}hhi hi-level trajectory train subpolicies best predict meta-controller best predicts respectively. train generally pervised learning subroutine stochastic optimization neural networks batch training procedure. termination functions need learned part hierarchical policy labels provided algorithm leverages expert hierarchical feedback well-known distribution mismatch problem learning execution still occur reduce sequential decision making supervised learning interactive imitation learning algorithms dagger address issue expert actively provide feedback respect agent’s trajectories. however exisiting interactive algorithms cannot leverage hierarchical feedback typically invoke labelfull learner’s trajectory start ﬁnish. labeling full trajectory wasteful horizon long exists hierarchical structure. example problem decomposes hierarchically subgoals typically easier learn others querying expert well-learned subgoals redundant. motivates algorithm refer hierarchical dagger aggregates datasets level across different learning rounds similarly counterpart. addition learning lo-level feedback algorithm incorporates additional feedback inspectlo labelhi. episode learner executes hierarchical policy including choosing subgoal executing lo-level trajectories i.e. rolling subpolicy chosen subgoal terminating execution according expert provides feedback agent fails execute entire task veriﬁed inspectfull inspectfull fails expert ﬁrst labels correct subgoals labelhi performs lo-level labeling long learner’s meta-controller chooses correct subgoal subpolicy fails intuitively hierarchical dagger allows agent learn subpolicies along good trajectories saves expert’s labeling effort agent enters irrelevant parts state space. require additional operations inspectlo inspectfull veriﬁcation steps often less costly full demonstrations labeling. next formalize intuition analyze labeling cost hierarchical dagger compare approach. theoretical analysis assume learner aims learn meta-controller policy policy class subpolicies class πlo. simplicity assume ﬁnite also assume realizability; expert’s policies found corg responding classes allows halving algorithm online learner levels. halving algorithm maintains version space policies acts majority decision makes mistake removes erring policies version space. hierarchical setting therefore makes mistakes level |πlo| mistakes learning mistake bounds used upper bound total cost expert feedback hierarchical dagger dagger. dagger consider agent endowed policy class πfull πlo} order enable apples-to-apples comparison oblivious hierarchical structure problem. bounds depend cost performing different types operations deﬁned section dagger consider modiﬁed version ﬁrst calls inspectfull requests labels inspection fails. proofs deferred appendix theorem given ﬁnite classes realizable expert policies total cost incurred expert hierarchical approach round bounded gopt subgoals actually used expert gopt theorem given full policy class πfull πlo} realizable expert policy total cost incurred expert approach round bounded bounds leading term full cost full inspection incurred every round viewed cost monitoring. contrast remaining terms viewed cost learning settings include terms coming respective mistake bounds. ratio cost hierarchical learning learning bounded applied upper bound |gopt| |g|. savings thanks hierarchy depend speciﬁc costs. typical setting expect inspection costs sufﬁces check ﬁnal state whereas labeling costs scale linearly length trajectory. cost ratio hhi+hlo thus realize signiﬁcant savings horizons individual level substantially shorter overall horizon. particular hfull hierarchical approach reduces overall labeling cost factor hfull. generally whenever hfull large reduce costs learning least constant factor—a signiﬁcant gain saving effort domain expert. hierarchical dagger motivated idea generally easier expert teach learning agent level instead supervising level. carry idea reinforcement learning setting agent learn subpolicies reinforcement signal alone. approach allows imitation learning level reinforcement learning level concreteness present variant dagger q-learning algorithm algorithm agent proceeds rolling-in learner’s meta-controller selected subgoal subpolicy selects executes primitive actions usual \u0001-greedy rule termination condition met. agent receives pseudo-reward also known intrinsic reward upon termination subgoal agent’s meta-controller chooses another subgoal process continues episode algorithm hierarchical imitation learning–q-learning input function pseudo providing pseudo-reward input predicate terminal indicating termination input annealed exploration probabilities initialize data buffers initialize subgoal q-functions environment instance start state initialize repeat involvement expert begins. similar hierarchical dagger expert inspects overall execution learner inspectfull returns fail expert provides hi-level feedback labelhi hilevel data aggregated training meta-controller. q-functions subpolicy updated stochastically experience-replay data buffers. difference hybrid il-rl approach hierarchical reinforcement learning buffers accumulate experience. long meta-controller’s subgoal agrees expert’s agent’s experience executing subgoal added buffer hand meta-controller selects subgoal accumulation experience current episode terminated. unlike hierarchical dagger algorithm assumes access real-valued function pseudo predicate terminal pseudo provides pseudo-reward state executing terminal indicates termination subgoal setup similar prior work hierarchical concretely assume access termination predicate terminal well predicate success indicating successful completion subgoal success always implies terminal. natural choice pseudo-reward function follows small penalty encourage short trajectories. predicates success terminal could provided expert learnt supervised reinforcement feedback. experiments explicitly provide predicates il-rl hybrid well hierarchical giving advantage hierarchical dagger needs learn terminate subpolicies. task overview. figure displays snapshot maze navigation domain. episode agent encounters instance maze large collection different layouts. maze consists rooms arranged -by- grid openings rooms vary instance instance initial position agent target. agent needs navigate corner maze target marked yellow. cells obstacles agent needs avoid survival. contextual information agent receives pixel-based representation displaying bird’s-eye view environment including partial trail indicating locations agent visited. large number random environment instances domain solvable tabular algorithms. note rooms always connected locations hallways always middle wall. primitive actions include going step down left right. addition instance environment designed ensure path initial location target shortest path takes least steps agent penalized reward runs lava wall also terminates episode. agent receives positive reward upon stepping yellow block. sponds four possible subgoals going room immediately north south west east ﬁfth possible subgoal target setup steps steps. episode terminated primitive steps agent unsuccessful. subpolicies meta-controller similar neural network architectures differ number action outputs. include neural network policy descriptions hyperparameters appendix. hierarchical imitation learning. ﬁrst compare performance hierarchical imitation learning algorithms imitation learning. maze domain success rate deﬁned average rate successful task completion previous test episodes random environment instances used training. labeling cost measured number expert labels label either subgoal primitive action generated expert. thus cost label operation equal length labeled trajectory. hierarchical imitation learning algorithms outperform imitation learners hierarchical dagger particular achieves consistently highest success rate approaches less episodes. figure displays median well interval maximum minimum success rate observed random executions algorithms. number expert labels required however varies signiﬁcantly hierarchical imitation learning variants. figure displays average success rate function total number expert labels. hierarchical dagger achieves signiﬁcant savings expert labels compared imitation learning algorithms. savings efﬁcient querying expert level particular lo-level labels always needed especially learner ﬁnds irrelevant part state space poor subgoal selection level well certain subgoals reliably learned. figure shows hierarchical dagger requires lo-level labels early training requests primarily hi-level labels subgoals mastered. result hierarchical dagger requires fraction lo-level labels compared dagger hybrid imitation–reinforcement learning. hybrid experiments deep double q-learning prioritized experience replay underlying procedure. addition evaluating performance algorithm empirically compare sample complexity standard hierarchical policy classes network architectures learn meta-controller subpolifigure maze navigation. sampled environment instance agent needs navigate bottom left bottom right. composition expert feedback time hierarchical dagger; number labels refers number subgoals actions provided expert label operations number inspect queries. success rate hybrid il-rl algorithm number hi-level labels requested function number lo-level samples. figure maze navigation hierarchical versus imitation learning. episode followed round training round testing. success rate measured previous test episodes labeling effort measured number expert labels i.e. number subgoals primitive actions generated expert. success rate episode. success rate versus number expert labels. success rate versus number lo-level expert labels. cies hierarchical reinforcement learning baseline q-learning procedure h-dqn also enhanced double learning prioritized experience replay. note q-learning learn anything meaningful either experimental setting long planning horizon sparse rewards maze domain subpolicy learner receives pseudo-reward successful execution corresponding stepping correct door. example subgoal next room north pseudo-reward associated stepping northern door door assigned negative pseudo-reward. dagger learn meta-controller level. figure shows learning progression hybrid algorithm implying main observations number hi-level labels rapidly increases initially ﬂattens learner becomes successful thanks availability inspectfull operation. hybrid algorithm makes progress learning agent passes inspectfull operation increasingly often algorithm starts saving signiﬁcantly expert feedback. compared hierarchical hybrid algorithm requires signiﬁcantly fewer samples level. include additional comparison appendix. compared hierarchical dagger number expert labels required reach certain accuracy higher meaning mode makes sense level expert labels expensive level ones completely infeasible show next domain. figure montezuma’s revenge hybrid il-rl versus hierarchical screenshot montezuma’s revenge black-and-white color-coded subgoals. learning progression algorithm solving entire ﬁrst room montezuma’s revenge; colors match subgoal depictions left pane. success ratio fraction times level learner achieves subgoal results shown typical learner. learning performance algorithm versus hierarchical q-learning. task overview. montezuma’s revenge among atari games difﬁcult existing deep reinforcement learning algorithms. montezuma’s revenge natural candidate hierarchical approach natural sequential order subtasks. figure displays environment annotated sequence subgoals. designated subgoals bottom right stair reverse path back right stair open door agent given pseudo-reward subgoal completion. enforce agent single life episode thereby preventing agent taking shortcut collecting note setting actual game environment equipped positive external rewards corresponding picking using open door optimal execution sequence subgoals requires primitive actions. surprisingly reinforcement learning algorithms frequently achieve score domain hybrid il-rl versus h-dqn. similar maze domain combination dagger level ddqn prioritized experience replay reinforcement learner level. figure shows learning progression hybrid algorithm montezuma’s revenge. setting horizon level learning meta-controller requires relatively samples. episode roughly corresponds labelhi query. subpolicies learnt order subgoal execution prescribed expert. ecution subgoals upon loss life. introduce simple modiﬁcation q-learning level make learning efﬁcient accumulation experience replay buffer begin ﬁrst time agents encounter positive pseudo-reward. modiﬁcation wellsuited considered interaction mode expert giving advice level mechanism explains example temporal mastering subgoal commencement learning subgoal figure period effectively training meta-controller takes place. modiﬁcation ensures reinforcement learner encounters least positive pseudo-rewards boosts learning long horizon settings naturally work off-policy learning scheme although h-dqn rely expert feedback give advantage h-dqn learners fair comparison. neural network architecture used kulkarni note hdqn fails achieve reward without enhancement. terminate training subpolicies success rate exceeds point subgoal considered learned. subgoal success rate deﬁned percentage successful subgoal completions previous attemps. termination subgoal training practical cope inherent instability figure compares average best runs hybrid il-rl versus modiﬁed h-dqn together min–max performance range among included runs. level sample sizes ﬁgure directly comparable middle panel learning progression displayed typical rather aggregate multiple runs. experiments performance imitation learning component stable consistent across many different trials. however performance reinforcement learning component varies substantially across trials. subgoal difﬁcult learn long horizon whereas reinforcement learning component tends master ﬁrst subgoals quickly especially compared h-dqn. advantage algorithm ability accumulate experience subgoal within relevant part state space subgoal part optimal trajectory. contrast h-dqn pick subgoals resulting lolevel samples corrupt subgoal experience replay buffers substantially slow convergence. presented hierarchical imitation learning framework exploits levels hierarchy effectively learn long time horizons. approach ﬂexible instantiated incorporate mixture imitation reinforcement feedback different levels hierarchy. compared imitation learning approach enjoys signiﬁcantly improved sample complexity theoretically empirically. compared hierarchical reinforcement learning approach achieves signiﬁcantly faster convergence practice. approach extended several ways. instance consider weaker feedback preference gradient-style feedback weaker form imitation feedback saying whether agent action correct incorrect corresponding bandit variant imitation learning hybrid il-rl approach relied availability subgoal termination predicate indicating subgoal achieved. many settings termination predicate relatively easy specify settings predicate needs learned. leave question learning termination predicate learning reinforcement feedback open future research. f¨urnkranz johannes h¨ullermeier eyke cheng weiwei park sang-hyeun. preference-based reinforcement learning formal framework policy iteration algorithm. machine learning hester todd vecerik matej pietquin olivier lanctot marc schaul piot bilal sendonaris andrew dulac-arnold gabriel osband agapiou john deep q-learning demonstrations. aaai kulkarni tejas narasimhan karthik saeedi ardavan tenenbaum josh. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. nips james littman michael taylor matthew huang jeff roberts david learning behaviors human-delivered discrete feedback modeling implicit feedback strategies speed learning. aamas mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. icml venkatraman arun gordon geoffrey boots byron bagnell andrew. deeply aggrevated differentiable imitation learning sequential prediction. arxiv preprint arxiv. vezhnevets alexander sasha osindero simon schaul heess nicolas jaderberg silver david kavukcuoglu koray. feudal networks hierarchical reinforcement learning. arxiv preprint arxiv. proof theorem ﬁrst term full obvious expert inspects agent’s overall behavior episode. whenever something goes wrong episode expert labels whole trajectory incurring full time. remaining work bound number episodes agent makes mistakes. quantity bounded number total mistakes made halving algorithm logarithm number candidate functions proof theorem similar proof theorem ﬁrst term full obvious. second term corresponds situation inspectfull ﬁnds issues. according algorithm expert labels subgoals also inspects whether subgoal accomplished successfully incurs cost time. number times situation happens bounded number times wrong subgoal chosen plus number times subgoals good least subpolicies fails accomplish subgoal. situation occurs times. situation subgoals chosen episode must come gopt subgoals halving algorithm makes |πlo| mistakes. last term corresponds cost labello operations. occurs meta-controller chooses correct subgoal corresponding subpolicy fails. similar previous analysis situation occurs |πlo| good subgoal completes proof. convolutional layer convolutional layer pooling layer convolutional layer convolutional layer pooling layer fully connected layer output layer ﬁlters kernel size stride ﬁlters kernel size stride pool size ﬁlters kernel size stride ﬁlters kernel size stride pool size nodes relu activation softmax activation although imitation learning component tends stable consistent samples required reinforcement learners vary experiments identical hyperparameters. section report additional results hybrid algorithm montezuma’s revenge domain. implementation hybrid algorithm game montezuma’s revenge decided limit computation million frames lo-level reinforcement learners experiments successfully learn ﬁrst subpolicies successfully learn ﬁrst subpolicies. last subgoal proved difﬁcult almost half experiments manage ﬁnish learning fourth subpolicy within million frame limit reason mainly longer horizon subgoal compared three subgoals. course function design subgoals always shorten horizon introducing intermediate subgoals. h-dqn baseline subgoals h-dqn baseline generally tends underperform proposed hybrid algorithm large margin. even given advantage confer implementation hdqn h-dqn experiments failed successfully master second subgoal instructive also examine sample complexity associated getting horizon sufﬁciently short appreciate difference between expert feedback level versus relying reinforcement learning train meta-controller. stark difference learning performance comes fact hi-level expert advice effectively prevents lo-level reinforcement learners accumulating experience frequently case h-dqn. potential corruption experience replay buffer also implies considered setting learning hierarchical easier compared learning. hierarchical thus susceptible collapsing learning version. additional related work imitation learning. another dichotomy imitation learning well reinforcement learning value-function learning versus policy learning. former setting assumes optimal behavior induced maximizing unknown value function. goal learn value function imposes certain structure onto policy class. latter setting makes structural assumptions aims directly policy whose decisions well imitate demonstrations. latter setting typically general often suffers higher sample complexity. approach agnostic dichotomy accommodate styles learning. instantiations framework allow deriving theoretical guarantees rely policy learning setting. sample complexity comparison imitation learning reinforcement learning studied much literature perhaps exception recent analysis aggrevated hierarchical reinforcement learning. feudal anhierarchical framework similar decompose task hierarchically particular feudal system manager multiple submanagers submanagers given pseudo-rewards deﬁne subgoals. prior work feudal reinforcement learning levels; require large amount data levels long planning horizon demonstrate experiments. contrast propose general framework imitation learners used substitute reinforcement learners substantially speed learning whenever right level expert feedback available. hierarchical policy classes additional studied hausknecht stone zheng andreas learning weaker feedback. work motivated efﬁcient learning weak expert feedback. receive demonstration data high level must utilize reinforcement learning level setting viewed instance learning weak demonstration feedback. primary elicit weaker demonstration feedback preference-based gradient-based learning studied f¨urnkranz loftin christiano similar montezuma’s revenge domain hierarchical deep reinforcement learning work well maze domain. level planning horizon possible subgoals step prohibitively difﬁcult hi-level reinforcement learner able achieve non-zero rewards within experiments. make comparison attempted provide additional advantage h-dqn algorithm giving head-start h-dqn reduction horizon giving hierarchical learner optimal execution ﬁrst half trajectory. resulting success rate figure note hybrid il-rl advantage still quickly outperforms h-dqn ﬂattens success rate.", "year": 2018}