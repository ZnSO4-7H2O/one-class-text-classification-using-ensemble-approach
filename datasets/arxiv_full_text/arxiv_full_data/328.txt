{"title": "Scatter Component Analysis: A Unified Framework for Domain Adaptation  and Domain Generalization", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.4"], "abstract": "This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.", "text": "abstract—this paper addresses classiﬁcation tasks particular target domain labeled training data available source domains different target. closely related frameworks domain adaptation domain generalization concerned tasks difference frameworks availability unlabeled target data domain adaptation leverage unlabeled target information domain generalization cannot. propose scatter component analyis fast representation learning algorithm applied domain adaptation domain generalization. based simple geometrical measure i.e. scatter operates reproducing kernel hilbert space. ﬁnds representation trades maximizing separability classes minimizing mismatch domains maximizing separability data; quantiﬁed scatter. optimization problem reduced generalized eigenvalue problem results fast exact solution. comprehensive experiments benchmark cross-domain object recognition datasets verify performs much faster several state-of-the-art algorithms also provides state-of-the-art classiﬁcation accuracy domain adaptation domain generalization. also show scatter used establish theoretical generalization bound case domain adaptation. supervised learning perhaps popular task machine learning recently achieved dramatic successes many applications object recognition object detection speech recognition machine translation successes derive large part availability massive labeled datasets pascal imagenet unfortunately obtaining labels often time-consuming costly process requires human experts. furthermore process collecting samples prone dataset bias i.e. learning algorithm trained particular dataset generalizes poorly across datasets. object recognition example training images collected speciﬁc conditions involving camera viewpoints backgrounds lighting conditions object transformations. situations classiﬁers obtained learning algorithms operating samples dataset cannot directly applied related datasets. developing learning algorithms robust label scarcity dataset bias therefore important compelling problem. domain adaptation domain generalization proposed overcome fore-mentioned issues. context domain represents probability distribution samples drawn often equated dataset. domain usually divided different types source domain target domain distinguish domain labeled samples domain without labeled samples. domains related different limits applicability standard supervised learning models target domain. particular basic assumption standard supervised learning training test data come distribution violated. goal domain adaptation produce good models target domain training labels source domain leveraging unlabeled samples target domain supplementary information training. domain adaptation demonstrated signiﬁcant successes various applications sentiment finally problem domain generalization arises situations unlabeled target samples available samples multiple source domains accessed. examples domain generalization applications automatic gating cytometry visual object recognition main practical issue several state-of-the-art domain adaptation domain generalization algorithms object recognition result optimization problems inefﬁcient solve therefore suitable situations require real-time learning stage. furthermore although domain adaptation domain generalization closely related problems domain adaptation algorithms cannot general applied directly domain generalization since rely availability samples target domain. highly desirable develop algorithms computed efﬁciently compatible domain adaptation domain generalization provides state-of-the-art performance. goals objectives address fore-mentioned issues propose fast uniﬁed algorithm reducing dataset bias used domain adaptation domain generalization. basic idea algorithm learn representations inputs classiﬁer invariant dataset bias. intuitively learnt representations incorporate four requirements separate points different labels separate data whole whilst separating points sharing label reducing mismatch domains. main contributions paper follows ﬁrst contribution scatter simple geometric function quantiﬁes mean squared distance distribution centroid. show four requirements encoded scatter establish relationship linear second contribution fast scatter-based feature learning algorithm applied domain adaptation domain generalization problems scatter component analysis algorithm best knowledge ﬁrst multi-purpose algorithm applicable across range domain adaptation generalization tasks. optimization reduces generalized eigenproblem admits fast exact solution kernel terms time complexity. third contribution derivation theoretical bound case domain adaptation. theoretical analysis shows domain scatter controls generalization performance sca. demonstrate domain scatter controls discrepancy distance certain conditions. discrepancy distance previously shown control generalization performance domain adaptation algorithms performed extensive experiments evaluate performance large suite alternatives domain adaptation domain generalization settings. found performs considerably faster prior state-of-the-art across range visual object cross-domain recognition competitive better performance terms accuracy. organization paper paper organized follows. section describes problem deﬁnitions reviews existing work domain adaptation domain generalization. sections describes proposed tool also corresponding feature learning algorithm scatter component analysis theoretical domain adaptation bound presented section comprehensive evaluation results analyses provided sections finally section concludes paper. background literature review section establishes basic deﬁnitions domains domain adaptation domain generalization. reviews existing work domain adaptation domain generalization particularly area computer vision object recognition. domain probability distribution input label spaces respectively. sake simplicity equate terms domain distribution used interchangeably throughout paper. yi}n i.i.d. sample domain. convenient notation corresponding empirical distribution dirac delta. deﬁne domain adaptation domain generalization follows. deﬁnition source target domain respectively denote samples drawn domains. task domain adaptation learn good labeling function given training examples. deﬁnition source domains target domain. denote samples drawn source domains. task domain generalization learn labeling function given sd∀d training examples. availability unlabeled target samples. goal learning labeling function performs well target domain. practice domain generalization requires work well although might violate deﬁnition note domain generalization exactly reduced domain adaptation domain adaptation domain generalization recently attracted great interest machine learning. present review recent literature organized parts domain adaptation domain generalization. domain adaptation earlier studies domain adaptation focused natural language processing e.g. references therein. domain adaptation gained increasing attention computer vision solving dataset bias object recognition object detection reader encouraged consult recent survey visual domain adaptation comprehensive review. classify domain adaptation algorithms three categories classiﬁer adaptation approach selection/reweighting approach iii) feature transformation-based approach. classiﬁer adaptation approach aims learn good adaptive classiﬁer target domain leveraging knowledge source auxiliary domains adaptive support vector machines utilize auxiliary classiﬁers adapt primary classiﬁer performs well target domain optimization criterion similar standard svms. domain adaptation machine employs domain-dependent regularizer based smoothness assumption sparsity regularizer least-squares svms recently multi-instance learning based classiﬁer action event recognition trained weakly labeled data proposed reweighting/selection approach reduces sample bias reweighting selecting source instances ‘close’ target instances selection considered ‘hard’ version reweighting. basic idea studied name covariate shift gong applied convex optimization strategy select source images maximally similar target images according maximum mean discrepancy referred landmarks. landmarks used construct multiple auxiliary tasks basis composing domain-invariant features. transfer joint matching uses reweighting strategy regularizer based -norm structured sparsity source subspace bases. feature transformation-based approach perhaps popular approach domain adaptation. daume proposed simple feature augmentation method replicating source target data well zero-padding resulting features extended method case heterogeneous features i.e. source target features different dimensionality introducing common subspace learnt standard formulation. subspace learning-based algorithm transfer component analysis semi-supervised version sstca utilizes maximum mean discrepancy minimize dataset bias wifi localization text classiﬁcation applications. metric learning-based domain adaptation approaches proposed early studies object recognition ofﬁce dataset. idea extracting ‘intermediate features’ minimize dataset bias projecting data onto multiple intermediate subspaces also considered. sampling geodesic flow geodesic flow kernel generate multiple subspaces interpolation source target subspace grassmann manifold point scatter work feature space reproducing kernel hilbert space main motivation transform original inputs onto high possibly inﬁnite dimensional space hope features linearly separable. important property rkhs perhaps allow computationally feasible transformation onto virtue kernel trick. deﬁnition arbitrary hilbert space functions deﬁne evaluation functional reproducing kernel hilbert space functional always bounded i.e. exists expression weakest condition ensures existence inner product also ability evaluate function every point domain provides useful notion practice. domains points rkhs using mean deﬁnition suppose equipped kernel corresponding rkhs feature denote probability distributions mean takes distributions points provide theorem shows difference true scatter ﬁnite sample estimate decreases sample size. theorem suppose true distribution samples size empirical distribution. suppose then probability manifold subspace. subspace alignment transforms source subspace subspace well-aligned target subspace without requiring intermediate subspaces. recent method called correlation alignment facilitates adaptive features aligning source target covariance matrices subspace learning-based methods transfer sparse coding domain invariant projection make following match source target distributions feature space. methods proposed follows similar intuition using hellinger distance alternative mmd. algorithms based hierarchical non-linear features deep learning also capable producing powerful domain adaptive features several works addressed probably approximately correct theoretical bounds domain adaptation. ben-david presented ﬁrst theoretical analysis domain adaptation adaptation bound classiﬁcation tasks based da-distance mansour extended work several ways built rademacher complexity discrepancy distance alternative da-distance. paper provide domain adaptation bound algorithm based latter analysis. domain generalization newer line research domain adaptation. blanchard ﬁrst studied issue proposed augmented encodes empirical marginal distributions kernel solving automatic gating cytometry. feature projection-based algorithm domain-invariant component analysis introduced solve problem. dica extends kernel incorporating distributional variance reduce dissimilarity across domains central subspace capture functional relationship features corresponding labels. domain generalization algorithms also used object recognition. khosla proposed multi-task max-margin classiﬁer refer undo-bias explicitly encodes dataset-speciﬁc biases feature space. biases used push dataset-speciﬁc weights similar global weights. fang developed unbiased metric learning based learning-to-rank framework. validated weakly-labeled images produces less biased distance metric provides good object recognition performance. extended exemplarsvm domain generalization adding nuclear norm-based regularizer captures likelihoods positive samples. proposed model referred lre-svm provides state-ofthe-art performance. recently autoencoder based algorithm extract domain-invariant features multi-task learning proposed although domain adaptation domain generalization goal approaches generally compatible domain adaptation methods cannot directly applied domain generalization vice versa. best knowledge lre-svm applied domain adaptation domain generalization. domain generalization algorithm formulation dica undo-bias typically allow take account unlabeled data target domain. furthermore several state-of-the-art domain adaptation domain generalization algorithms lre-svm require solution computationally complex optimization induces high complexity time. work establish fast algorithm overcomes issues. domain scatter think suppose given domains sample latent distribution domains. equipping sample empirical distribution computing scatter relative identity yields domain scatter µpi. note domain scatter coincides distributional variance introduced domain scatter also closely related maximum mean discrepancy used domain adaptation algorithms deﬁnition functions maximum mean discrepancy domains measures extent domains resemble another perspective function class following theorem relates domain scatter given domains case interest bounded linear functions feature space lemma scatter domains maximum mean discrepancy proof. note theorem involves levels probability distributions domains empirical distribution assigns probability points everything else. deﬁnition lemma also tells domain scatter valid metric kernel characteristic important example characteristic kernel gaussian kernel kernel used theoretical results experiments below. also remark estimated observed data bound provided analogous theorem utilize scatter formulate feature learning algorithm referred scatter component analysis speciﬁcally scatter quantiﬁes requirements needed develop effective solution domain adaptation generalization described next section. scatter component analysis aims efﬁciently learn representation improves domain adaptation domain generalization. strategy convert observations conﬁguration points feature space domain mismatch reduced. ﬁnds representation problem source target domains similar elements label similar; whereas elements different labels well separated variance whole data maximized. requirement quantiﬁed scatter leads four consequences domain scatter between-class scatter within-class scatter total scatter. remainder subsection deﬁnes four scatter quantities detail describes sca’s learning algorithm. also easily switched either domain adaptation domain generalization modifying conﬁguration input domains. worth emphasizing deﬁnition general sense covers domain adaptation domain generalization total scatter estimated data follows. rn×p matrix unlabeled samples number examples d-th domain). given feature corresponding kernel deﬁne functions arranged column vector centering {φ}n subtracting mean covariance matrix lemma algorithm formulate sca’s learning algorithm incorporating four quantities. objective seek representation solving optimization problem form following expression maximizing numerator encourages preserve total variability data separability classes. minimizing denominator encourages representation source target domains similar source samples sharing label similar. objective function. reformulate three ways. first express terms linear algebra. second insert hyperparameters control trade-off scatters scatter quantity could important others particular case. third impose constraint small control scale solution. xj∈k denote nk-tuple source samples class xi∈k furthermore centroid denote n-tuple class centroids centroid appears times centroid centroid source domain nkµk. follows within-class scatter maximizing fisher’s linear discriminant increases separation data points respect class clusters. given linear transformation follows lemma class scatters projected feature space relation methods closely related number feature learning domain adaptation methods. this observe lagrangian setting hyper-parameters recovers kpca. setting recovers kernel fisher discriminant method linear kernel equivalent fisher’s linear discriminant basis domain adaptation method object detection proposed setting yields algorithm unsupervised scatter component analysis closely related tca. difference algorithms constrains total variance regularizes transform whereas usca trades-off total variance constrains transform small) motivated theorem turns usca consistently outperforms case domain adaptation section eliminating term denominator usca yields semi-supervised extension sstca differs markedly sca. instead incorporating withinbetweenclass scatter objective function sstca incorporates term derived hilbert-schmidt independence criterion maximizes dependence embedding labels. usca essentially equivalent unsupervised domain invariant component analysis case domains however sstca supervised dica incorporates label-information differently notion central subspace. particular supervised dica requires data points labeled cannot applied experiments. computational complexity analyze computation complexity algorithm. suppose domains number samples domain denote total number samples number leading eigenvectors computing matrices takes hence total complexity solving eigendecomposition comparison transfer joint matching prior state-of-the-art domain adaptation algorithm object recognition uses alternating eigendecomposition procedure iterations needed. using notation complexity i.e. times slower sca. hyper-parameter settings reporting detailed evaluation results important explain hyper-parameters tuned. formulation described section four hyper-parameters choice kernel number subspace bases betweenclass total scatters trade-off domain scatter tuning hyper-parameters using standard strategy e.g. grid-search might impractical reasons. ﬁrst computational complexity. second crucial cross-validating large number hyper-parameters worsen generalization target domain since labeled samples target domain available. domain adaptation ﬁxed thus hyperparameters remain tunable domain generalization i.e. total scatter eliminated allowed tuned number tunable hyper-parameters remains unchanged. conﬁguration based empirical observation setting better terms cross-validation test performance domain generalization cases. evaluations used -fold cross validation using source labeled data optimal found strategy sufﬁcient produce good models domain adaptation generalization cases. analysis adaptation performance derive bound domain adapation shows controls generalization performance case squared loss despite widespread domain adaptation best knowledge ﬁrst generalization bound. main idea incorporate adaptation bound proven discrepancy distance generalization bound domain generalization terms domain scatter given remark denote hypothesis class functions compact set. given loss function deﬁned pairs labels distribution hypotheses hyp. consider case hypothesis subset rkhs supplementary material discusses associate family functions loss function provides useful rademacher bound. ingredients derive domain adaptation bounds terms domain scatter. true labeling functions domain respectively argminh∈hyp argminh∈hyp minimizers. successful domain adaptation shall assume small. following theorem provides domain adaptation bound terms scatter theorem family functions mapping source target sample respectively. rest assumptions lemma theorem supplementary material. hypothesis probability least following adaptation bound holds instructive compare theorem theorem analog expand discl empirical measure. also straightforward rewrite bound term empirical scatter applying theorem signiﬁcance theorem twofold. first highlights scatter controls generalization performance domain adaptation. second bound shows direct connection scatter domain adaptation theory proposed note bound might useful practical purposes since loose pessimistic hold hypotheses possible data distributions. remark theorem shows domain scatter terms arising generalization bound setting domain generalization. experiment domain adaptation ﬁrst experiments evaluated domain adaptation performance synthetic data real-world object recognition tasks. synthetic data designed understand behavior learned features compared algorithms whereas real-world images utilized verify performance sca. discrepancy symmetric satisﬁes triangle inequality deﬁne distance general dischyp assume universal kernel i.e. topological spaces loss squared loss discrepancy metric. important example universal kernel gaussian kernel kernel used experiments below. main step proof relationship domain scatter discrepancy distance. able special case kernel universal loss meansquare error. main technical challenge discrepancy distance quadratic hypotheses whereas linear. therefore need bound effects multiplication operator deﬁnition space continuous functions compact equipped supremum norm given deﬁne multiplication operator bounded linear operator given note general rkhs closed multiplication operator however since kernel universal follows closed multiplication since space continuous functions closed multiplication. moreover deﬁne sup-norm using identiﬁcation operator useful prove main theorem. lemma given equipped universal kernel holds proof. straightforward calculation. lemma requires universal kernel since deﬁned lemma allows relate domain scatter generalization bounds domain adaptation proven stating bounds introduce rademacher complexity measures degree class functions random noise. measure basis bounding empirical loss expected loss. experiments divided parts. section visualizes performance synthetic data. section evaluates performance range cross-domain object recognition tasks standard realistic hyper-parameter tuning. additional results tuning protocol established literature also reported supplementary material completeness. synthetic data figure depicts synthetic data consists dimensional data points three classes clusters. data points cluster generated gaussian distribution mean standard deviation cluster. kernel expa−b used algorithms. tunable hyper-parameters selected according -nearest neighbor’s test accuracy. compare features extracted kernel principal component analysis semisupervised transfer component analysis transfer joint matching sca. figure illustrates features extracted mmd-based algorithms reduce domain mismatch. blue colors indicate source target domains respectively. good features domain adaptation conﬁguration blue colors mixed. effect seen features extracted sstca indicates domain mismatch successfully reduced feature space. classiﬁcation domain adaptive features also certain level class separability. bottom highlights major difference algorithms terms class separability features clustered respect classes prominent gaps among clusters. suggests would easier simple function correctly classify features. real world object recognition summarize complete domain adaptation results range cross-domain object recognition tasks. several real-world image datasets utilized handwritten digits usps general objects caltech ofﬁce three cross-domain pairs constructed datasets usps+mnist msrc+voc ofﬁce+caltech. data setup usps+mnist pair consists images subsampled datasets handwritten digits. mnist contains training images test images size usps training images test images size pair constructed randomly sampling images usps images mnist. images uniformly rescaled size encoded feature vectors representing grayscale pixel values. source target classiﬁcation tasks constructed usps mnist mnist usps. msrc+voc pair consist -dimensional images share object categories aeroplane bicyclebird sheep taken msrc datasets. pair constructed selecting images msrc images voc. features extracted pixels follows. first images uniformly rescaled pixels length. second -dimensional dense sift features extracted using vlfeat open source package finally -dimensional codebook created using k-means clustering obtain codewords. ofﬁce+caltech consists images categories forms four domains amazon dslr webcam caltech. amazon images acquired controlled environment studio lighting. dslr consists high resolution images captured digital camera home environment natural lighting. webcam images acquired similar environment dslr low-resolution webcam. finally caltech images collected google images taking possible source-target combinations yields cross-domain datasets denoted used types extracted features datasets publicly available surf-bow features surf-bow decaf extracted using surf quantized -bin histograms codebooks computed k-means subset amazon images. ﬁnal histograms standardized zero mean unit standard deviation dimension. deep convolutional activation features constructed using deep convolutional neural network architecture model inputs mean-centered pixel values forward propagated convolutional layers fully-connected layers. used outputs layer features leading dimensional decaf features. baselines protocol compared classiﬁcation performance following algorithms classiﬁer features kpca transfer component analysis sstca geodesic flow kernel transfer sparse coding subspace alignment unsupervised scatter component analysis sca. realistic setting tunable hyper-parameters selected -fold cross validation according labels source domains only. feature learning algorithms evaluated three different classiﬁers -nearest neighbor support vector machines linear kernel domain adaptation machines l-svm standard off-theshelf classiﬁers speciﬁcally designed domain adaptation. extension incorporates domaindependent regularization encourage target classiﬁer sharing similar prediction values source classiﬁers. also utilize linear kernel dam. classiﬁcation accuracy -nearest neighbor ﬁrst report classiﬁcation accuracy competing algorithms according classiﬁer. goal clearly highlight adaptation impact induced purely representations since basically measures distance features. classiﬁcation accuracy usps+mnist msrc+voc pairs. best model average prior state-of-the-art second best. domain adaptation algorithms perform well even worse without adaptation strategy kpca. surprisingly unsupervised version algorithm usca highest accuracy msrc+voc cases. indicates label incorporation help improve domain adaptation msrc+voc clearly usps+mnist. furthermore usca always provide improvement features algorithms including fail mnist usps case. fig. visualization. projections synthetic data onto ﬁrst leading eigenvectors. numbers brackets indicate classiﬁcation accuracy target using -nearest neighbor bottom rows show domains classes respectively. surprisingly sstca also incorporates label information training perform competitively. ﬁrst possible explanation directly improves class separability whereas sstca maximizes dependence criterion relates indirectly separability. second sstca incorporates manifold regularization requires similarity graph i.e. afﬁnity matrix. results ofﬁce+caltech pair summarized table table general decaf induces stronger discriminative performance surf-bow features since decaf already provided signiﬁcantly better performance. consistently best average performance features slightly better prior state-of-the-art tjm. surf-bow best model cases second best cases. trend decaf better best performance cases comes second cases. although closest competitor highest number individual best cross-domain performance requires higher computational complexity section below. recall algorithms’ hyper-parameters used produce results tuned using labels source domain only. valid tuning protocol unsupervised domain adaptation setting. nevertheless best results established literature obtained using hyper-parameter tuning target labels. completeness also report results tuning-on-target protocol supplementary material. classiﬁcation accuracy l-svm next report results l-svm base classiﬁers feature learning algorithms. succinctness compare performance algorithms kpca sstca presented figure chart shows average accuracies relative performance features numbers alongside bars indicate absolute accuracies. table summarizes absolute accuracies features. general feature learning algorithms rectify domain adaptation performances features except cases ofﬁce+caltech surf-bow features kpca ofﬁce+caltech decaf features. considering absolute accuracies best average performances dataset still provided similar trend results. conﬁrms effectiveness regardless classiﬁer choice least among l-svm dam. compare absolute average performance lsvm performance l-svm evidently provide considerable performance improvement ofﬁce+caltech dataset. performances less powerful features features extracted mnist+usps msrc+voc even worse useful lesson ﬁnding make better features take real beneﬁt advanced classiﬁers context domain adaptation. finally seek investigate performance impact induced comparison l-svm. expected provide better performance since speciﬁcally designed domain adaptation. table outperforms l-svm operating features. surprisingly always case feature learning algorithm applied. moreover l-svm always produces higher performance gain relative features.than dam. could attributed overﬁtting considering hyper-parameters l-svm. combining feature learning algorithm complicates whole processs recall hyper-parameter selection based validation source data. mnist+usps msrc+voc ofﬁce+caltech algorithms executed matlab machine intel core arch linux -bit ram. note kpca basically utilizes optimization procedure single iteration eigenvalue decomposition. requires several iterations eigenvalue decomposition additional gradient update iteration solves dictionary learning sparse coding iterative procedure. general signiﬁcantly faster closest competitor accuracy tsc. speciﬁcally faster faster tsc. runs speed mnist+usps msrc+voc speed ofﬁce+caltech. several cases performs slower kpca note kpca less competitive accuracy compared runtime less interesting concerned about. experiment domain generalization second experiments show proposed algorithm also applicable domain generalization achieves state-of-the-art performance object action recognition datasets. evaluated algorithms three cross-domain datasets vlcs ofﬁce+caltech ixmas data setup ﬁrst cross-domain dataset refer vlcs consists images pascal labelme caltech- datasets represents domain. datasets share object categories bird chair person. domain vlcs dataset divided training test random selection overall dataset. detailed training-test conﬁguration domain summarized supplementary material. employed decaf features dimensionality inputs algorithms. features publicly available. second cross-domain dataset ofﬁce+caltech dataset section detailed explanation dataset. also used decaf features extracted dataset. third dataset ixmas dataset contains videos actions recorded different actors cameras viewpoints. dataset used benchmark evaluating human action recognition models. simulate domain generalization problem followed setup proposed frames actions utilized domains represented represented camera viewpoints task learn actions particular camera viewpoints classify actions unseen viewpoints. experiment used dense trajectories features extracted frames applied k-means clustering build codebook clusters descriptors trajectory mbhx mbhy. bag-ofwords features concatenated forming dimensional features frame. fig. l-svm average performance accuracy relative performance features. numbers bottom bars show absolute accuracy. line indicates baseline performance table exact numbers. compared algorithms following baselines -nearest neighbor classiﬁer. l-svm classiﬁer linear kernel. kpca kernel principal component analysis. undo-bias multi-task svm-based algorithm undoing dataset bias. three hyper-parameters require tuning. since original formulation designed binary classiﬁcation performed following setup multi-class classiﬁcation purposes. trained individual undo-bias classiﬁers number classes. prediction stage given test instance finally computed {k|∀k veriﬁed whether initial tuning proposal method using weaklylabeled data retrieved querying class labels search engine. however tuned hyper-parameters using kfold cross-validation strategy others fair comparison. lre-svm non-linear exemplar-svms model nuclear norm regularization impose low-rank likelihood matrix. lre-svm four hyper-parameters require tuning. undo-bias lre-svm prior state-of-the-art domain generalization algorithms object recognition tasks. note undo-bias dica cannot applied domain adaptation setting used -nearest neighbor base classiﬁer feature learning-based algorithms kpca dica usca/udica sca. tunable hyper-parameters selected according labels source domains. kernel-based methods kernel function kernel expa−b kernel bandwidth computed median heuristic. note unsupervised dica almost identical usca case. difference usca control parameter domain scatter/distributional variance term. results vlcs dataset dataset ﬁrst conducted standard training-test evaluation using -nearest neighbor i.e. learning model training domain testing test another domain check groundtruth performance also identify existence dataset bias. groundtruth evaluation results summarized table general dataset bias indeed exists despite state-of-the-art deep convolution neural network features decaf. example average crossdomain performance i.e. mean others drop corresponding in-domain performance particular caltech- highest bias labelme least biased dataset indicated largest smallest performance drop respectively. evaluated domain generalization performance seven cross-domain recognition tasks. complete results summarized table best model tasks outperforms prior state-of-the-art lre-svm. almost always better performance ‘raw’ baseline except caltech- target domain. average better closest competitor dataset undo-bias. vlcs cross-domain recognition hard task general since best model provides average improvement baseline. furthermore three algorithms domain generalization-based methods cannot achieve even better performance baseline. results ofﬁce+caltech dataset evaluated algorithms several cross-domain cases constructed ofﬁce+caltech dataset. detailed evaluation results four cases decaf reported table report cross-domain cases possibly constructed dataset since features already provided high accuracies closest competitor lre-svm. although lresvm performs best average best performance three four cross-domain cases comes second average. case underperforms lre-svm unsupervised version algorithm usca udica domain generalization case cannot compete state-of-the-art models. slightly better kpca average. suggests incorporating labeled information source domains feature learning improve domain generalization ofﬁce+caltech cases. results ixmas dataset table summarizes classiﬁcation accuracies ixmas dataset three cross-domain cases. standard baselines cannot match algorithms domain generalization strategies. dataset best performance three cases average. particular signiﬁcantly better others case. lresvm remains closest competitor second best average performance best cross-domain case. runtime performance next report average runtime performance cross-domain recognition tasks dataset. algorithms executed using software machine described section table runtime kpca dica expected since utilize optimization procedure single generalized eigenvalue decomposition. signiﬁcantly faster prior state-of-the-art domain generalization methods example vlcs dataset undo-bias lresvm require minutes needs minutes average training time. analogous trend also seen case ofﬁce+caltech ixmas datasets. outcome indicates better suited domain generalization tasks competing algorithms training stage real time required. conclusions scatter-based objective function straightforward encode relevant structure domain adaptation domain generalization problems. uses variances subsets data construct linear transformation dampens unimportant distinctions ampliﬁes useful distinctions extensive experiments several cross-domain image datasets show much faster competing algorithms provides state-of-the-art performance domain adaptation domain generalization. theoretical analysis shows scatter input domains i.e. domain scatter provides generalization bounds domain adaptation setting domain generalization recall remark prior work shown distributional variance arises terms controlling generalization performance scatter thus unifying quantity controls generalization performance domain adaptation generalization. natural extension kernel kernel fisher discriminant tca. contrast many domain adaptation methods objective functions combine total variance quantities fundamentally different kind graph laplacian sparsity constraints hilbertschmidt independence criterion central subspace easily extended semi-supervised domain adaptation incorporating target labels class scatters. finally remark possible speed large-scale problems using random features general dataset bias remains solved. existing algorithms perform satisfactorily several crossdomain tasks even using powerful feature extraction methods decaf dense trajectory-based features using less powerful features clearly unsatisfactory. thus crucial develop fundamental feature learning algorithms signiﬁcantly reduce dataset bias wide range situations. acknowledgments authors would like thank zheng sharing extracted dense trajectories features ixmas dataset also chen fang sharing unbiased metric learning code useful discussions. references krizhevsky sutskever hinton classiﬁcation deep krizhevsky learning multiple layers features tiny images master’s thesis department computer science university toronto apr. ponce berg everingham forsyth hebert lazebnik marszalek schmid russell torralba williams zhang zisserman dataset issues object recognition toward category-level object recognition vol. borgwardt gretton rasch h.-p. kriegel sch¨olkopf smola integrating structured biological data kernel maximum mean discrepancy bioinformatics vol. jiang literature survey domain adaptation statistical classiﬁers available http//sifaka.cs.uiuc.edu/jiang/domain adaptation/ survey uiuc tech. rep. duan w.-h. tsang domain adaptation multiples sources domain-dependent regularization approach ieee transactions neural networks learning systems exploiting privileged information data action event recognition ijcv vol. gestel suykens johan viaene vanthienen dedene moor vandewalle benchmarking least squares support vector machine classiﬁers machine learning vol. gretton borgwardt rasch sch¨olkopf smola kernel method two-sample-problem nips kulis saenko darrell what domain adaptation using asymmetric kernel transforms cvpr muhammad ghifary received beng meng degrees institut teknologi bandung indonesia awarded degree victoria university wellington zealand. currently working weta digital research intern. previously worked catholic parahyangan university lecturer. student member ieee aaai. published work conferences icassp iccv icml. main research interests include domain adaptation transfer learning representation learning deep learning applications computer vision. david balduzzi received degree mathematics university chicago subsequently held positions planck institute intelligent systems zurich. currently senior lecturer school mathematics statistics victoria university wellington. research interests machine learning computational neuroscience. published work conferences nips icml aaai aamas iccv journals annals statistics network science plos bastiaan kleijn professor victoria university wellington since also professor delft university technology professor headed sound image processing laboratory moved zealand. joining worked bell laboratories speech processing. founder global solutions developed voice video processing engines among others google skype yahoo sold google holds ph.d. electrical engineering delft university technology m.s.e.e. stanford. also earned ph.d. soil science m.s. physic university california riverside. fellow ieee. mengjie zhang received b.e. m.e. degrees artiﬁcial intelligence research center agricultural university hebei hebei china ph.d. degree computer science rmit university melbourne australia respectively. since victoria university wellington wellington zealand currently professor computer science head evolutionary computation research group associate dean faculty engineering. current research interests include evolutionary computation particularly genetic programming particle swarm optimization learning classiﬁer systems application areas image analysis multiobjective optimization classiﬁcation unbalanced data feature selection reduction shop scheduling. published academic papers refereed international journals conferences.", "year": 2015}