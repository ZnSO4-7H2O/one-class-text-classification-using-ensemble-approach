{"title": "NAG: Network for Adversary Generation", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.", "text": "adversarial perturbations pose serious threat deploying machine learning systems. recent works shown existence image-agnostic perturbations fool classiﬁers natural images. existing methods present optimization approaches solve fooling objective imperceptibility constraint craft perturbations. however given classiﬁer generate perturbation time single instance manifold adversarial perturbations. also order build robust models essential explore manifold adversarial perturbations. paper propose ﬁrst time generative approach model distribution adversarial perturbations. architecture proposed model inspired gans trained using fooling diversity objectives. trained generator network attempts capture distribution adversarial perturbations given classiﬁer readily generates wide variety perturbations. experimental evaluation demonstrates perturbations crafted model achieve state-of-the-art fooling rates exhibit wide variety deliver excellent cross model generalizability. work deemed important step process inferring complex manifolds adversarial perturbations. machine learning systems shown vulnerable adversarial noise small structured perturbation added input affects model’s prediction drastically. recently successful deep neural network based object classiﬁers also observed susceptible adversarial attacks almost imperceptible perturbations. researchers attempted explain intriguing aspect hypothesizing linear behaviour models ﬁnite training data etc. importantly adversarial perturbations exhibit cross model generalizability. perturbations learned model fool another model even second model different architecture trained disjoint subset training images recent startling ﬁndings moosavi-dezfooli mopuri shown possible mislead multiple state-of-the-art deep neural networks images adding single perturbation. perturbations image-agnostic fool multiple diverse networks trained target dataset. perturbations named universal adversarial perturbations single adversarial noise perturb images multiple classes. side adversarial noise poses severe threat deploying machine learning based systems real world. particularly applications involve safety privacy essential develop robust models adversarial attacks. side also poses challenge understanding models along current learning practices. thus adversarial behaviour deep learning models small structured noise demands rigorous study ever. existing methods weather image speciﬁc agnostic craft single perturbation makes target classiﬁer susceptible. specifically methods typically learn single perturbation possibly bigger perturbations fool target classiﬁer. observed given technique perturbations learned across multiple runs different. spite optimizing different data ordering initialization objectives learning close perturbations space essence approaches prove uaps exist given classiﬁer crafting perturbation limited information underlying distribution perturbations turn target classiﬁer itself. therefore figure overview proposed approach models distribution universal adversarial perturbations given classiﬁer. illustration shows batch random vectors {z}b transforming perturbations {δ}b added batch data samples {x}b. portion shows adversarial batch bottom portion shows shufﬂed adversarial batch middle portion shows benign batch fooling objective diversity objective constitute loss. note target trained classiﬁer parameters updated proposed training. hand parameters generator randomly initialized learned backpropagating loss. relevant task hand model distribution adversarial perturbations. help better analyze susceptibility models adversarial perturbations. furthermore modelling distributions would provide insights regarding transferability adversarial examples help prevent black-box attacks also helps efﬁciently generate large number adversarial examples learning robust models adversarial training empirical evidence shown perturbations exist large contiguous regions rather scattered multiple small discontinuous pockets. paper attempt model regions given classiﬁer generative modelling. introduce like generative model capture distribution unknown adversarial perturbations. freedom parametric assumptions distribution target distribution being unknown make framework suitable choice task. major contributions work novel objective craft universal adversarial perturbations given classiﬁer achieves state-of-the fooling performance multiple ﬁrst time show possible model distribution perturbations given classiﬁer generative model. this present easily trainable framework modelling unknown distribution perturbations. demonstrate empirically learned model capture distribution perturbations generates perturbations exhibit diversity high fooling capacity excellent cross model generalizability. rest paper organized follows section details proposed method section presents comprehensive experimentation validate utility proposed method section brieﬂy discusses existing related works ﬁnally section concludes paper. generative models images seen renaissance lately especially large datasets emergence deep neural networks. particularly generative adversarial networks variational auto-encoders shown signiﬁcant promise direction. work utilize like framework model distribution adversarial perturbations. typical framework consists parts generator discriminator generator transforms random vector meaningful image i.e. usually sampled simple distribution trained produce images indistinguishable real images true data distribution pdata. discriminator accepts image outputs probability real image sample pdata. typically trained output probability fake image presented. trained adversarially compete improve other. properly trained generator training expected produce images indistinguishable real images. modelling adversaries broad overview method illustrated fig. ﬁrst formalize notations used subsequent sections paper. note paper considered cnns trained object recognition data distribution classiﬁers trained denoted particular sample represented target denoted therefore output given layer denoted predicted label given data sample denoted output softmax layer denoted vector predicted probabilities target categories image-agnostic additive perturbation fools target denoted denotes limit perturbation terms norm. objective model distribution perturbations given classiﬁer. formally seek model since objective model unknown distribution image-agnostic perturbations given trained classiﬁer make suitable changes framework. modiﬁcations make discriminator replaced target already trained whose weights frozen novel loss instead adversarial loss train generator thus objective work train model fool target cnn. architecture also similar typical transforms random sample image dense layer series deconv layers. details exact architecture discussed section also appendix. proceed discuss fooling objective enables model adversarial perturbations given classiﬁer. order fool target generator driven suitable objective. typical gans adversarial loss train however work attempt model distribution whose samples unavailable. know single attribute samples able fool target classiﬁer. incorporate attribute fooling objective train models unknown distribution perturbations. denote label predicted target clean sample benign prediction predicted corresponding perturbed sample adversarial prediction. similarly denote output vector softmax layer without adding respectively. ideally perturbation confuse classiﬁer benign prediction different adversarial prediction. happen adding conﬁdence benign prediction reduced another category made higher. thus formulate fooling loss minimize conﬁdence benign prediction perturbed sample fig. gives graphical explanation objective fooling objective shown blue colored block. note fooling loss essentially encourages generate perturbations decrease conﬁdence benign predictions thus eventually label. fooling loss encourages learn guarantee high fooling capability generated perturbations objective might lead local minima learns limited effective perturbations however objective model distribution covers varieties perturbations. therefore introduce additional component loss encourages explore space perturbations generate diverse perturbations. term objective diversity objective. within mini-batch generated perturbations objective indirectly encourages different separating feature embeddings projected target classiﬁer. words given pair generated perturbations objective increases distance random index batch size data sample perturbation mini-batch respectively. note batch contains perturbations generated data samples output layer distance metric pair features. orange colored block fig. illustrates diversity objective. since important learn diverse perturbations exhibit maximum fooling give equal importance ﬁnal loss learn architecture implementation details present experimental details describe implementation working details proposed architecture. generator part network maps latent space distribution perturbations given target classiﬁer. architecture generator consists deconv layers. ﬁnal deconv layer followed tanh non-linearity scaling restricts value chosen order quasiimperceptible adversarial noise. generator network adapted details regarding architecture found appendix. performed experiments variety architectures trained perform object recognition task ilsvrc- dataset. kept architecture generator unchanged different target architectures separately learned corresponding adversarial distributions. turn transformed batch perturbations {δ}b size equal image also sample images {x}b available training data form mini-batch training data denoted benign batch perturbations training data one-to-one manner i.e. perturbation gets added corresponding image batch gives adversarial batch δb}. shown portion fig. also randomly shufﬂe perturbations ensuring perturbation remains origi} index batch i.e. this form shufﬂed adversarial batch shown bottom portion fig. note order prepare perturbations shufﬂed data samples thus training iterations consists three quasi-batches namely benign images batch adversarial batch shufﬂed adversarial batch three portions shown fig. feed target compute loss. obtain benign predictions clean batch samples {x}b. labels used compute conﬁdences corresponding adversarial batch samples. forms fooling objective shown similarly obtain feature representations softmax layer adversarial shufﬂed adversarial batches compute diversity component loss shown essentially diversity objective pushes apart ﬁnal layer representations corresponding maximizing codifferent perturbations training images randomly chosen ilsvrc train images ilsvrc validation testing images. latent space dimension experimented spaces different dimensions observed fooling rates obtained close. however observe generated perturbations demonstrate larger visual variety cases. thus keep experiments. batch size shallow networks vgg-f googlenet rest. models implemented tensorflow adam optimizer titan-x card. table average fooling rates perturbations modelled generative network rows indicate target perturbations modelled columns indicate attack. note that entry target matches network attack represents white-box attack rest represent black-box attacks. method along average fooling rates corresponding standard deviations also mentioned. best result case shown bold best cases shown blue. mean avg. rate achieved generator target cnns shown rightmost column. fooling rates achieved perturbations crafted learned generative model presented table results shown seven different network architectures trained ilsvrc- dataset computed test images. also investigate transfer rates perturbations attacking unknown models along target cnn. rows denote particular target modelled distribution perturbations columns represent classiﬁers attack. note target matches system attack fooling indicates white-box attack scenario entries represent black-box attack scenario. since network models perturbation space easily generate perturbation sampling feeding table report mean fooling rates generating multiple perturbations given classiﬁer. particularly white-box fooling rates computed averaging perturbations black-box rates averaged standard deviations mentioned next fooling rates. also mean average fooling rate achieved learned model target cnns shown rightmost column. clearly proposed generative model captures perturbations higher fooling rates note entries provided fooling rates cases perform better mean fooling rate obtained achieved figure shows perturbations generated proposed generative model different target cnns. note random sample corresponding distributions perturbations. fig. shows benign sample corresponding perturbed samples adding perturbations multiple cnns. note perturbations sampled corresponding distributions learned method. subsection examine diversity generated perturbations model. present different measures understand diversity perturbations. correlation measure measure presented show diversity crafted perturbations. trained generator googlenet detailed earlier. sampled random perturbations computed normalized inner product pairs. mean value product maximum value product doesn’t exceed doesn’t exceed respectively. result clearly shows perturbations generated model demonstrate large diversity compared spite perturbations high dimensional product value shows perturbations lack variety fail model manifold perturbations comprehensively. perturbations analyzed subsection found fig. figure sample universal adversarial perturbations different networks. target mentioned perturbations. note sample corresponding distributions across different samplings generative model perturbations vary visually. figure sample perturbations generated proposed approach googlenet maximum value normalized product pair perturbations computed visually look similar. hand maximum product value method perturbations visually different. results show proposed method faithfully models distribution perturbations effectively fool target cnn. interesting examine predicted label distribution adding perturbations. reveal labels images confused whether confusions diverse. subsection analyze labels predicted target adding perturbations modelled corresponding generative models considered vgg-f architecture images validation ilsvrc-. compute mean histogram predicted labels perturbations generated top- categories {jigsaw puzzle maypole otter dome electric fan}. though exists slight domination categories extent domination less compared case categories account predicted labels categories. relative higher diversity compared attributed effectiveness proposed diversity loss encourages model explore various regions adversarial manifold. subsection perform experiments undercase stand landscape latent space gans traversing learned manifold generally tells signs memorization walking latent space image generations result semantic changes considered model learned relevant interesting representations. however generative model attempts learn unknown distribution adversarial perturbations samples target distribution. therefore relevant investigate smooth semantic changes generations look smooth visual changes retaining ability fool target classiﬁer. figure shows results interpolation experiments resnet- classiﬁer. randomly taken pair points latent space considered intermediate points line joining generated perturbations corresponding intermediate points feeding learned generative model figure shows generated perturbations intermediate points along foolfigure interpolation pair points space shows distribution learned generator smooth transitions. ﬁgure shows perturbations corresponding points line joining pair points latent space. note perturbations learned fool resnet- architecture. perturbation corresponding fooling rate obtained images ilsvrc validation images mentioned. shows fooling capability intermediate perturbations also high remains different locations learned distribution perturbations. rates. clearly observe perturbations change smoothly pair consecutive points sequence gives morphing like effect large number intermediate points. intermediate perturbations fooling rate computed images ilsvrc- validation set. fig. perturbations corresponding fooling rates mentioned. high consistent fooling rates along path demonstrate modelling adversarial distribution faithful. proposed approach generates perturbations smoothly underlying manifold. attribute ability learned generative model effectiveness proposed objectives loss. results interpolation experiments found appendix. modelling adversaries multiple target clasmany recent adversarial perturbations works moosavi-dezfooli attempted explain cross-model generalizability correlation among different regions classiﬁcation boundaries learned them. subsection investigate learn model single distribution adversaries fool multiple target cnns simultaneously. consider target cnns presented table model single adversarial manifold fool simultaneously. keep part proposed architecture unchanged replace single target classiﬁer target networks. memory constraint models train smaller batch size. loss train generator summation individual losses computed target cnns separately. thus objective driving optimization becomes craft perturbations fool target cnns simultaneously. similar single target case diversity objective encourages explore multiple table mean fooling rates perturbations sampled distribution adversaries modelled multiple target cnns. perturbations result average fooling rate across target cnns higher best mean fooling rate achieved generator learned vgg-. table presents mean fooling rates obtained samples distribution perturbations learned fool target cnns. fooling rates slightly lesser obtained dedicated optimization however given complexity modelling learned perturbations achieve remarkable average fooling rate note around higher best mean fooling rate obtained individual network vgg-. emphasizes effectiveness proposed framework objectives simultaneously model perturbations classiﬁers signiﬁcant architectural differences. adversarial perturbations tantalizing revelation machine learning systems. speciﬁcally deep neural network based learning systems also shown vulnerable structured perturbations. ability generalize unknown models enables simple ways launch black-box attacks fool deployed systems. further existence image-agnostic perturbations along cross model generalizability exposes weakness current deep learning models. differing previous works work proposes novel simple effective objective enables learn image-agnostic perturbations. although existing objectives successfully craft perturbations attempt capture space perturbations. unlike existing works proposed method learns generative model capture space image-agnostic perturbations given classiﬁer. best knowledge work aims learn neural network generating adversarial perturbations simple feed-forwarding presented baluja fischer present neural network transforms image corresponding adversarial sample. note generates image speciﬁc perturbations doesn’t model distribution unlike generative adversarial network goodfellow radford shown gans trained learn data distribution generate samples image-to-image conditional gans improved generation quality inspired framework proposed neural network architecture model distribution universal adversarial perturbations given classiﬁer. discriminator part typical replaced trained classiﬁer fooled. generator part learned generate perturbations fool discriminator. also don’t train samples target data presented training. pair effective objectives proposed framework models perturbations fool given classiﬁer. paper presented ﬁrst ever generative approach model distribution adversarial perturbations given classiﬁer. propose inspired framework wherein successfully train generator network captures unknown target distribution without training samples proposed objectives naturally exploit attributes samples order model distribution. however unlike typical training deals pair conﬂicting objectives approach single well behaved optimization ability method generate perturbations state-of-the-art fooling rates surprising cross-model generalizability highlights severe susceptibilities current deep learning models. however proposed framework model distribution perturbations also enables conduct formal studies towards building robust systems. example goodfellow introduced adversarial training means learn robust models tramer extended ensemble adversarial training require large number adversarial samples. addition defence becomes robust samples exhibit diversity allow model fully explore space adversarial examples. existing methods limited generation speed instance diversity method modelling almost instantly produces adversarial perturbations lots variety. also shown approach efﬁciently model perturbations simultaneously fool multiple deep models. section provide additional results traversing manifold adversarial perturbations. line joining random points latent space consider intermediate points. transform points corresponding perturbations forwarding learned googlenet fig. shows generated perturbations. similar results presented resnet- classiﬁer shown main draft perturbations change smoothly pair consecutive points. circular patterns ﬁrst image slowly disappear image curtain like patterns emerge image clearly visible row. proceeding path generates wave like patterns shown middle bottom transform pattern. also fooling rates high consistent along path. process emphasizes proposed model learns relevant representations. figure interpolation pair points space show distribution learned generator smooth transitions. ﬁgure shows perturbations corresponding points line joining pair random points latent space. note perturbations learned fool googlenet architecture. perturbation corresponding fooling rate obtained images ilsvrc validation images mentioned. shows fooling capability intermediate perturbations also high similar different locations learned distribution perturbations. section explain architecture generator network proposed framework learns manifold adversarial perturbations given target classiﬁer. table presents details generator network basically includes initial fully connected layer followed several deconvolutional layers latent space vectors image size perturbations. architecture adapted presents improved techniques training virtual batch normalization better stabilize training generator network. note experiments generator architecture ﬁxed independent target attack. section present sample perturbations obtained multi-target case i.e. generator learned model adversaries simultaneously fool multiple target classiﬁers. fig. shows sample perturbations captured learned distribution. note perturbations exhibit signiﬁcant visual diversity. also section appendix show perturbations exhibit diversity terms predicted labels adding clean images. figure sample perturbations obtained distribution perturbations learned multiple target classiﬁers. learned craft perturbations simultaneously fool cnns mentioned main draft. note perturbations wide variety. proposed loss diversity component order generative model able capture variations distribution perturbations. diversity objective indirectly increases distance pair perturbations generated mini-batch objective projects feature representations belonging adversarial shufﬂed adversarial samples apart. transformations learned layers target ﬁxed objective helps generate diverse perturbations fool target. subsection investigate effectiveness multiple feature representations encourage diversity modelling perturbations. worked vgg-f model analysis considered conv conv softmax layers. separating features conv conv layers employed euclidean distance features obtained softmax layer cosine distance maximized. table shows fooling rates obtained proposed objective. optimization times report mean fooling rates along standard deviations. note fooling performance almost three representations. also standard deviations less indicating effectiveness fooling objective faithful modelling distribution. however upon visual investigation observe variations captured separating softmax representations relatively high. thus chose work representations softmax layer learn generative model perturbations experiments. table ablation results different representations encourage capturing diversity distribution perturbations vgg-f model. note numbers fooling rates obtained held-out across runs optimization. right column reports mean fooling rate along standard deviation. fig. shows benign corresponding adversarial images caffenet labels provided ground truth bottom predicted ones caffenet. note different perturbations added images result different predicted labels. figure benign corresponding perturbed images caffenet shows benign samples ground truth labels below. bottom shows predicted label corresponding perturbed images. section perform experiments understand weighting loss components i.e. diversity objective fooling objective affects learning. target architecture consider ablations resnet- architecture trained ilsvrc- data. vary value steps separately learn three generators. table presents corresponding fooling rate diversity measure learned generators. note diversity measure number labels account predicted labels adding perturbations also numbers computed validation images ilsvrc- data. results demonstrate measures clearly reﬂect relative weights given corresponding loss components. fooling rate increases weight given increases time number dominant labels decreases observation clearly demonstrates weights given individual loss components correlate resulting performance. treat weight adjustable hyper parameter system control generated perturbations high fooling limited variety less fooling wide variety. section present example perturbed images crafted adding different perturbations learned generator given benign image. note learned model perturbations simultaneously fool target cnns mentioned main draft predictions obtained googlenet fig. shows benign corresponding perturbed images crafted adding random perturbations generated leftmost image benign sample following four images perturbed samples. predicted labels mentioned corresponding images. note four predictions different demonstrates perturbations learned generator exhibit diversity terms predicted labels. importantly observe similar diversity generators trained different target cnns. figure sample benign image four perturbed images obtained adding different perturbations learned proposed generator note learned fool target cnns predictions obtained googlenet. ground truth benign image mentioned green predicted labels mentioned corresponding images different.", "year": 2017}