{"title": "A Generalization of Convolutional Neural Networks to Graph-Structured  Data", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "This paper introduces a generalization of Convolutional Neural Networks (CNNs) from low-dimensional grid data, such as images, to graph-structured data. We propose a novel spatial convolution utilizing a random walk to uncover the relations within the input, analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid. The convolution has an intuitive interpretation, is efficient and scalable and can also be used on data with varying graph structure. Furthermore, this generalization can be applied to many standard regression or classification problems, by learning the the underlying graph. We empirically demonstrate the performance of the proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular activity data set.", "text": "low-dimensional grid data images graph-structured data. propose novel spatial convolution utilizing random walk uncover relations within input analogous standard convolution uses spatial neighborhood pixel grid. convolution intuitive interpretation efﬁcient scalable also used data varying graph structure. furthermore generalization applied many standard regression classiﬁcation problems learning underlying graph. empirically demonstrate performance proposed mnist challenge state-of-the-art merck molecular activity data set. convolutional neural networks leading tool used address large machine learning problems lecun successfully provided signiﬁcant improvements numerous ﬁelds image processing speech recognition computer vision pattern recognition language processing even game boards hinton silver respectively). major success cnns justly credited convolution. successful application cnns implicitly capitalizes underlying attributes input. speciﬁcally standard convolution layer applied grid-structured input since learns localized rectangular ﬁlters repeatedly convolving multiple patches input. furthermore convolution effective input needs locally connective means signal highly correlated local regions mostly uncorrelated global regions. also requires input stationary order make convolution ﬁlters shift-invariant select local features independent spatial location. figure visualization graph convolution size given node convolution applied node closest neighbors selected random walk. right ﬁgure demonstrates random walk expand graph higher degree neighbors. convolution weights shared according neighbors’ closeness nodes applied globally nodes. therefore cnns inherently restricted subset datasets. nevertheless impressive improvements made applying cnns encourage generalize cnns non-grid structured data local connectivity stationarity properties. main contribution work generalization cnns general graph-structured data directed undirected offering supervised algorithm incorporates structural information present graph. moreover algorithm applied wide range regression classiﬁcation problems ﬁrst estimating graph structure data applying proposed active research learning graph structure data makes feasible demonstrated experiments paper. fundamental hurdle generalizing cnns graph-structured data corresponding generalized convolution operator. recall standard convolution operator picks neighboring pixels given pixel computes inner product weights neighbors. propose spatial convolution performs random walk graph order select closest neighbors every node figure shows. nodes convolution computed inner product weights selected closest neighbors ordered according relative position node. allows weights convolution every node reﬂects dependency node closest neighbors. image considered undirected graph edges neighboring pixels convolution operation standard convolution. proposed convolution possesses many desired advantages natural intuitive. proposed similar standard convolves every node closest spatial neighbors providing intuitive generalization. example learn graph structure using correla transferable. since criterion relevant variables selected relative position node convolution invariant spatial location node graph. enables application ﬁlter globally across data nodes varying graph structures. even transfered different data domains overcoming known limitation many generalizations cnns graphs. scalable. forward call graph convolution requires ﬂops number nodes graph variables. also amount memory required convolution run. since provides scalable fast operation efﬁciently implemented gpu. effective. experimental results merck molecular activity challenge mnist data sets demonstrates learning graph structure standard regression classiﬁcation problems simple application graph convolutional neural network gives results comparable state-of-the-art models. graph theory differential geometry heavily studied last decades mathematical statistical computational perspectives large body algorithms developed variety problems. laid foundations required recent surge research generalizing deep learning methods geometrical structures. bronstein provide extensive review newly emerging ﬁeld. currently main approaches generalizing cnns graph structured data spectral spatial approaches spectral approach generalizes convolution operator using eigenvectors derived spectral decomposition graph laplacian. motivation create convolution operator commutes graph laplacian similar regular convolution operator commutes laplacian operator. approach studied bruna henaff used eigenvectors graph laplacian convolution weighting distance induced similarity matrix. major drawback spectral approach graph dependent learns ﬁlters function particular graph laplacian. constrains operation ﬁxed graph structure restricts transfer knowledge different different domains. defferrard introduce chebnet spectral approach spatial properties. uses order chebyshev polynomials laplacian learn ﬁlters k-hop neighborhoods graph giving spatial interpretation. approach later simpliﬁed extended semi-supervised settings kipf welling although spirit spatial property similar suggested paper since builds upon laplacian method also restricted ﬁxed graph structure. spatial approach generalizes convolution using graph’s spatial structure capturing essence convolution inner product parameters spatially close neighbors. main challenge spatial approach difﬁcult shift-invariance convolution non-grid data. spatial convolutions usually position dependent lack meaningful global interpretation. convolution proposed paper spatial utilizes relative distance nodes overcome difﬁculty. diffusion convolutional neural network proposed atwood towsley similar convolution follows spatial approach. convolution also performs random walk graph order select spatially close neighbors convolution maintaining shared weights. dcnn’s convolution associates parameter power transition matrix transition matrix steps random walk. therefore inner product considered parameters weighted average nodes visited steps. practice dense graphs number nodes visited steps quite large might over-smooth signal dense graphs. furthermore atwood towsley note implementation dcnn requires power series full transition matrix requiring complexity limits scalability method. another example spatial generalization provided bruna uses multi-scale clustering deﬁne network architecture convolutions deﬁned cluster without weight sharing property. duvenaud hand propose neural network extract features molecular ﬁngerprints molecules arbitrary size shape designing layers local ﬁlters applied nodes neighbors. addition research generalizing convolution graph active research application different types neural networks graph structured data. earliest work ﬁeld graph neural network scarselli others starting gori fully presented scarselli model connect node graph ﬁrst order neighbors edges design architecture recursive inspired recursive neural networks. recently extended output sequences many models inspired original work graph neural networks. example battaglia introduce interaction networks studying spatial binary relations learn objects relations physics. problem selecting nodes graph convolution analogous problem selecting local receptive ﬁelds general neural network. work coates suggests selecting local receptive ﬁelds feed-forward neural network using closest neighbors induced similarity matrix contrast previous research suggest novel scalable convolution operator captures local connectivity within graph demonstrates weight sharing property helps transferring different domains. achieve considering closest neighbors found using random walk graph intuitively extends spatial nature standard convolution. step differentiates cnns images regular neural networks selection neighbors grid window combined shared weight assumption. order select local neighbors given node graph transition matrix calculate expected number visits random walk starting given node. convolution node applied nodes highest expected number visits section discuss application convolution single layer single graph. immediate extend deﬁnition complex structures explicitly explained section introduce notation order proceed discussion. notation graph features edges denote transition matrix random walk graph probability move node similarity matrix correlation matrix graph given respectively. deﬁne work assumes existence graph transition matrix graph structure data already known i.e. similarity matrix already known transition matrix obtained explained lov´asz graph structure unknown learned using several unsupervised supervised graph learning algorithms. learning data graph structure active research topic scope paper. interested reader start belkin niyogi henaff discussing similarity matrix estimation. absolute value correlation matrix similarity matrix following roux showed correlation features usually enough capture geometrical structure images. assume figure visualization graph generated grid node near center connecting node adjacent neighbors. weight node smaller weights ﬁrst order neighbors. corresponds standard convolution. increases number active neighbors also increases providing greater weight neighbors farther away still keeping local information. note also expected number visits node starting steps. provides measure similarity node neighbors considering random walk graph. increases incorporate neighbors away node summation gives appropriate weights node closest neighbors. figure provides visualization matrix grid. discussed earlier used obtain closest neighbors node. hence seems natural deﬁne convolution graph node using order denote permutation order descending order. every every notion ordered position nodes global feature graphs nodes. therefore take advantage satisfy desired shared weights assumption enabling meaningful transferable ﬁlters. deﬁne conv size convolution graph nodes weights therefore weights decided according distance induced transition matrix. convolved variable largest value matrix example always correspond node correspond node’s closest neighbor. higher values order determined graph’s unique structure. noted conv doesn’t take account actual distance nodes might susceptible effects negative correlation features. reason also experimented conv deﬁned selection value data dependent main components affecting value. firstly necessary large enough detect neighbors every node. transition matrix sparse might require higher values secondly properties stochastic processes know denote markov chain stationary distribution implies large values local information smoothed convolution repeatedly applied features maximum connections. reason suggest keeping relatively furthermore similar standard convolution implementation possible represent graph convolution tensor product transferring computational burden using highly optimized matrix multiplication libraries. every graph convolution layer input tensor observations features depth ﬁrst extend input additional dimension includes neighbors feature selected transforming input dimension tensor apply graph convolution layer dnew ﬁlters convolution weights tensor size therefore application graph convolution tensor product input weights along axes results output size development team libraries python inheriting tools provided libraries train neural networks dropout regularization advanced optimizers efﬁcient initialization methods. source code publicly available github major computational effort algorithm computation performed graph structure pre-processing step. usually onetime computation signiﬁcant constraint. however large graphs done naively might challenging. alternative achieved recalling needed order calculate expected number visits given node steps random walk. applications graph large also usually sparse. facilitates efﬁcient implementation breadth first search algorithm hence selection neighbors parallelized would require memory every unique graph structure making method scalable large graphs number different graphs manageable. problem many different large graphs inherently computationally hard. graph reduces memory required preprocessing graph. information required graph nearest neighbors every node. order test feasibility proposed graphs conducted experiments well known data sets functioning benchmarks merck molecular activity challenge mnist. data sets popular well-studied challenges computational biology computer vision respectively. implementations order enable better comparisons models reduce chance over-ﬁtting model selection process consider shallow simple architectures instead deep complex ones. hyperparameters chosen arbitrarily possible rather tuned optimized. nevertheless still report state-of-the-art competitive results data sets. merck molecular activity challenge merck molecular activity kaggle challenge based molecular activity data sets. target predicting activity levels different molecules based structure different atoms molecule. helps identifying molecules medicines intended target cause side effects. following henaff apply algorithm dataset. contains training test molecules. features molecules sparse active molecules. features correlation estimation accurate. therefore features active least molecules resulting features. seen figure signiﬁcant correlation structure different features. implies strong connectivity among features important application proposed method. training experiments performed using adam optimization procedure gradients derived back-propagation algorithm using root mean-squared error loss used learning rate ﬁxed number epochs implemented dropout regularization every layer optimization procedure. absolute values correlation matrix used learn graph structure. found small number nearest neighbors works best used models. convergence plot given figure demonstrates convergence selected architectures. contribution suggested convolution explained view alternatives figure left visualization correlation matrix ﬁrst molecular descriptors merck molecular activity challenge training set. proposed method utilizes correlation structure features. right convergence different methods test set. graph convolution converges steadily uses fewer parameters. fully connected neural network models ﬁrst applying convolution followed fully connected hidden layer converge better complex fully connected models. furthermore convergence former methods stable comparison fully connected methods parameter reduction. linear regression optimizing convolutions often considered automation feature extraction process. perspective simple application layer convolution followed linear regression signiﬁcantly outperforms results standalone linear regression. table provides thorough results different architectures explored compares winners kaggle challenge namely deep neural network random forest perform better winners kaggle contest. models henaff bruna spectral approach currently state-of-the-art. comparison them perform better spectral networks unsupervised graph structure equivalent done using correlation matrix similarity matrix. using spectral networks supervised graph structure holds state-of-the-art learning graph structure. direction explored graph learning beyond scope paper although straightforward apply proposed graph similar learned graph. mnist data often functions benchmark data test machine learning methods. experimented different graph structures images. ﬁrst experiment considered images observations undirected graph grid pixel connected adjoining neighbor pixels. used convolutions grid structure presented figure using number nearest neighbors. symmetry graph regions image multiple pixels equidistant pixel convolved. order solve this ties broken consistent manner convolution would reduced regular convolution window. exceptions would pixels close boundary. make example compelling broke ties arbitrarily making training process harder compared regular cnn. imitating lenet lecun considered architecture ooling ooling followed linear classiﬁer resulted error rate. comparable regular architecture achieves error rate .%-.%. outperform fully connected neural network achieves error rate around expected differences complexities models. second experiment used correlation matrix estimate graph structure directly pixels. since mnist pixels constant restricted data active pixels constant. used number neighbors. done order ensure spatial structure image longer effected results. neighbors partial subset pixels consideration relative location correlated pixels necessarily varies pixel pixel. result regular cnns longer applicable data whereas convolution proposed paper compared performance fully connected neural networks. table presents experimental results. graph performs fully connected neural networks fewer parameters. single layer graph convolution followed logistic regression greatly improves performance logistic regression demonstrating potential graph convolution feature extraction purposes. regular convolutions required million parameters convolution uses small amount parameters generate different maps input. suggests graph convolution made even effective development efﬁcient spatial pooling method graphs known unsolved problem. propose generalization convolutional neural networks grid-structured data graph-structured data problem actively researched community. novel contribution convolution graph handle different graph structures input. proposed convolution contains many sought-after attributes; natural intuitive interpretation transferred within different domains knowledge computationally efﬁcient effective. furthermore convolution applied standard regression classiﬁcation problems learning graph structure data using correlation matrix methods. compared fully connected layer suggested convolution signiﬁcantly fewer parameters providing stable convergence comparable performance. experimental results merck molecular activity data mnist data demonstrate potential approach. convolutional neural networks already revolutionized ﬁelds computer vision speech recognition language processing. think important step forward extend problems inherent graph structure.", "year": 2017}