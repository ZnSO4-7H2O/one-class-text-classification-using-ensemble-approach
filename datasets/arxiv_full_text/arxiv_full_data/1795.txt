{"title": "Identity-sensitive Word Embedding through Heterogeneous Networks", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Most existing word embedding approaches do not distinguish the same words in different contexts, therefore ignoring their contextual meanings. As a result, the learned embeddings of these words are usually a mixture of multiple meanings. In this paper, we acknowledge multiple identities of the same word in different contexts and learn the \\textbf{identity-sensitive} word embeddings. Based on an identity-labeled text corpora, a heterogeneous network of words and word identities is constructed to model different-levels of word co-occurrences. The heterogeneous network is further embedded into a low-dimensional space through a principled network embedding approach, through which we are able to obtain the embeddings of words and the embeddings of word identities. We study three different types of word identities including topics, sentiments and categories. Experimental results on real-world data sets show that the identity-sensitive word embeddings learned by our approach indeed capture different meanings of words and outperforms competitive methods on tasks including text classification and word similarity computation.", "text": "space suﬀers serious problems data sparseness. recent approaches represent words real vectors low-dimensional space much better capture syntactic semantic relationships between words. word embeddings proved eﬀective many tasks word analogy named entity recognition tagging text classiﬁcation basic intuition learning word embeddings comes distributional hypothesis shall know word company keeps intuition motivated many models including ﬁrst word embedding approach based neural networks senna glove skipgram etc. among models skipgram model widely adopted simplicity eﬀectiveness eﬃciency. basic idea skipgram embedding target word predict embeddings surrounding context words local context windows. although existing approaches proved already quite eﬀective tasks common problem distinguish meanings words diﬀerent contexts. words every word represented single embedding vector even multiple clearly diﬀerent meanings words multiple senses corresponds diﬀerent meaning words sense meanings vary subtly diﬀerent contexts representations words learned existing word embedding approaches mixture multiple meanings thus compromising accuracy semantic relations. address problem contextual meanings words must taken consideration. paper acknowledge word take diﬀerent identities used diﬀerent contexts every identify carries meaning represented embedding. words learn identity-sensitive word embeddings. intuition inspired theories social identity self categorization social psychology economics. theories suggest every person sense multiple identities herself usually perceived membership diﬀerent social groups identity dominate identities particular situations example ﬁrst author perceive himself microsoft employee data mining researcher existing word embedding approaches distinguish words diﬀerent contexts therefore ignoring contextual meanings. result learned embeddings words usually mixture multiple meanings. paper acknowledge multiple identities word diﬀerent contexts learn identitysensitive word embeddings. based identity-labeled text corpora heterogeneous network words word identities constructed model diﬀerent-levels word co-occurrences. heterogeneous network embedded low-dimensional space principled network embedding approach able obtain embeddings words embeddings word identities. study three diﬀerent types word identities including topics sentiments categories. experimental results real-world data sets show identitysensitive word embeddings learned approach indeed capture diﬀerent meanings words outperforms competitive methods tasks including text classiﬁcation word similarity computation. learning meaningful representation words critical many text mining tasks. traditional vector space models represent word one-hot vector high-dimensional permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. xxxxx xxxxxxx. copyright x-xxxxx-xx-x/xx/xx ..... figure illustration converting identity-labeled text corpora heterogeneous network. diﬀerent colors represent diﬀerent word identities. heterogeneous network encodes local context-level word co-occurrences wordcontext bipartite network. constructing word-context network identities words ignored words treated contexts. heterogeneous network also encodes identity-level word co-occurrences word-identity bipartite network. university michigan alumni third identity becomes dominant conversation ohio states. following simple analogy deﬁne identity word membership belonging group words derived taxonomy topic modeling process meaningful partitions words. besides topic identity many types word identities categories sentiments tags ideologies. meanings word diﬀerent identities usually different. therefore sound word embedding allow word diﬀerent embedding every identity belongs i.e. identity-sensitive embeddings. learn identity-sensitive word embeddings propose approach based heterogeneous network embedding. given text collection ﬁrst recognize identities every word token corpus therefore word diﬀerent contexts labeled diﬀerent identities. identity recognition process take reasonable existing word labeling approaches vary different types identities. example topic identities word tokens recognized gibbs sampling algorithm latent dirichlet allocation part-ofspeech words obtained though sequence labeling method conditional random ﬁelds sentiment polarity word tokens obtained sentiment dictionary word-sense disambiguation process. based identity-labeled corpus build heterogeneous network consisting words word identities encodes diﬀerent levels essential word co-occurrence information embedding process. heterogeneous network contains word-context network encodes local context level word co-occurrences mainly utilized existing word embedding approaches skipgram; also contains word-identity bipartite network encodes identity-level word co-occurrences. intuition words identity likely take similar meanings. network constructing process illustrated fig. extend previous work homogeneous network embedding learn embedding heterogeneous network eﬃcient scalable. egories. eﬀectiveness identity-sensitive word embeddings carefully evaluated using tasks text classiﬁcation word similarity computation. experimental results show identity-sensitive word embeddings indeed able capture diﬀerent meanings words outperforming competitive approaches including state-ofthe-art word embedding approaches methods consider contextual meanings words. inspired social identity self categorization theories. acknowledge words take diﬀerent identities present diﬀerent meanings diﬀerent contexts. levels co-occurrence information words word identities. principled heterogeneous network embedding approach proposed learn embeddings words word identities. rest paper organized follows. section discusses related work. section formally deﬁnes problem identity-sensitive word embedding heterogeneous networks. section describes approach heterogeneous network embedding. section presents experimental results conclude section word embedding attracting increasing attention recently word represented low-dimensional vector used features variety tasks. ﬁrst word embedding approach proposed bengio language modeling purpose neural network built predicting next word given previous words sentence. mnih simpliﬁed neural language model log-bilinear model. however previous approaches suﬀer serious problem high computational complexity large model output space mnih addressed problem hierarchical softmax techniques. mikolov proposed simple model skipgram using embedding target word predict embeddings surrounding context words local context windows essentially using local context-level word co-occurrences. compared skipgram previous work showed better results obtained constructing word co-occurrence network ﬁrst embedding co-occurrence network. existing embedding approaches represent word single embedding vector ignoring variation word meanings diﬀerent contexts. sound solution practice represent word multiple embeddings captures diﬀerent meaning word. indeed recent work learns multiple embeddings word huang used post-processing approach cluster context vectors word context cluster treated embedding word. neelakatan proposed multi-sense skipgram model estimate multiple embedding vectors word learning process. sense word associated context cluster. learning process average context vectors compared context cluster centers determine sense target word. experiments compare mssg model. chen used external knowledge wordnet estimate embedding sense word. compared work approach require external knowledge. similar work topic identity studied diﬀerentiate meanings words diﬀerent contexts. however work topic identity studied studied multiple types word identities besides topic identity. besides previous approaches learned embeddings free text approach learned embeddings embedding heterogeneous networks able capture diﬀerent levels word co-occurrences. meanings words usually ambiguous depending contexts. existing word embedding approaches ignore variations word meanings diﬀerent contexts. result word represented single embedding vector. paper take initiative study context-aware word embedding. acknowledge word diﬀerent identities diﬀerent contexts. deﬁne identity word follows definition identity word membership belonging group words. example word identity animal belongs animal; word ﬂower identity plant belongs plant. many types word identities e.g. topics sentiments ideologies categories tags. type identity word take multiple identities. take topic identity example word belong multiple topics; another example sentiment identity words positive negative sentiment orientation. word takes diﬀerent identities meanings word usually diﬀerent. therefore word diﬀerent identity represented diﬀerent embedding capturing diﬀerent meaning word i.e. identity-sensitive word embedding. speciﬁcally deﬁne identity-sensitive word embedding follows note although assume word different embedding identity. practice word usually identities. example word usually belongs topics; many words take either positive negative sentiment orientation. therefore compared word embeddings without considering word identities number parameters identity-sensitive word embeddings increase signiﬁcantly. learn identity-sensitive word embeddings identities tokens corpus must recognized ﬁrst. recognizing process depends types identities. example topic identities recognized gibbs sampling latent dirichlet allocation model; sentiment identities determined sentiment identities sentences. identities tokens corpora recognized leverage different levels word co-occurrences essential information word embedding. intuition based distributional hypothesis assumes words cooccur similar words likely similar meanings. therefore local context-level word co-occurrences leveraged also used skipgram model. encode local context-level word co-occurrences introduce word-context bipartite network network. words identities vocabulary context words without considering word identities. edges words identities context words. weight edge word context word number times co-occur local context windows. note constructing word-context network identities words ignored words treated contexts. reason this number unique context words reduced decreasing data sparsity. besides local context-level word cooccurrences another intuition words identity likely take similar meanings. example words assigned topic likely take similar meanings. therefore identity-level word co-occurrences leveraged. encode identity-level word cooccurrences introduce word-identity network deﬁned below distributions adopt widely used distance kl-divergence. empirical probability deﬁned degree vertex prestige vertex network i.e. degree vertex removing constants ﬁnal objective function simpliﬁed below mentioned above identity-sensitive word embeddings obtained embedding heterogeneous network constructed identity-labeled text corpora. heterogeneous network includes bipartite networks word-context network word-identity network. therefore intuitive approach minimize following objective function note objective function better approach weighting networks diﬀerently. however found equally weighting networks consistently perform well diﬀerent data sets able save users’ eﬀort choosing optimum parameter controlling importance networks. ditional probability requires calculating similarities vertex vertices computationally expensive. negative sampling technique approaches drawing negative edges positive edge. speciﬁcally aims minimize following objective function word-context network word-identity network encodes diﬀerent levels word co-occurrences. leverage levels word co-occurrences combine networks heterogeneous network. therefore learn word embeddings embed heterogeneous network constructed identity-labeled corpus. formally deﬁne problem follows labeled text corpora problem identity-sensitive word embedding aims learn multiple low-dimensional embedding vectors word embedding heterogeneous network constructed identity-labeled text copora. section introduce approach embedding heterogeneous networks obtain identity-sensitive word embeddings embeddings word identities. ﬁrst review previous model line large-scale information network embedding types networks including directed undirected weighted. mainly designed homogeneous information network embedding type nodes network. learn low-dimensional representation vertices network intuition preserve relationship proximity vertices lowdimensional spaces. essential idea line preserve ﬁrst-order second-order proximities vertices assumes vertices links vertices similar neighbors similar other. based distributional hypothesis linguistics words similar company neighbors likely similar other. corresponds assumption secondorder proximity word-context network. therefore utilize second-order proximity here. ally deﬁnes conditional distribution vertices practice second-order proximity pair vertex actually determined corresponding empirical conditional distribution gradient descent. edge sampled weight edge multiplied gradients. problematic values weights diverge i.e. weights large small. case hard choose appropriate learning rate diﬀerent edges sampled model updating. edge sampling approach eﬀectively addresses sampling edges according probabilities treating sampled edges binary edges. using edge sampling optimizing objective function concern weights edges networks comparable. resolved alternatively sampling edges networks. overall learning process summarized alg. section move forward evaluate eﬀectiveness proposed approach real-world data sets. three types word identities studied including topics sentiments categories. identity-sensitive word embeddings evaluated tasks text classiﬁcation contextual word similarity measuring. ﬁrst introduce experiment setup. dblp abstracts papers computer science bibliography. data diverse research ﬁelds used including data mining database artiﬁcial intelligence computational linguistics hardward system programming language theory. articles used seven diverse categories dbpedia ontology selected including arts history human mathematics nature technology sports. category articles randomly selected training sets. data topic category identities recognized. learn identity-sensitive word embeddings identities word tokens diﬀerent contexts must recognized advance. part introduce identity recognizing approaches diﬀerent types identities. topic. topic identities tokens corpus estimated gibbs sampling algorithm latent dirichlet allocation. sentiment. sentiment identities tokens training document determined identity document. speciﬁcally document positive sentiment tokens document assigned positive identity. however words sentiment orientations. therefore feature selection methods ﬁrst used select words likely sentiment orientation. recognizing identities tokens document words selected feature selection methods assigned identity document. available wikipediadatabase_download availableathttp//thinknook.com/ twitter-sentiment-analysis-training-corpus-dataset-\\ available http//nlp.stanford.edu/sentiment/index. html multiple word embeddings associated diﬀerent identities meanwhile context embedding word treated context. given test document recognize identity token document follows embedding word identity average context embedding words except document category. category identity identities tokens training document assigned identity document. identities tokens test documents recognized sentiment identity according eqn. network embedding line model applied learning word embeddings building word co-occurrence network ﬁrst embedding word co-occurrence network. words diﬀerent contexts also distinguished. mssg model dynamically learns multiple embeddings word training process captures diﬀerent sense word. mssg model number senses default according evaluate diﬀerent word embeddings tasks text classiﬁcation contextual word similarity measuring. text classiﬁcation. models trained training data sets learn word embeddings evaluated test data sets. embeddings documents training test data sets simple approach used averaging word vectors documents. note goal obtain optimum document embeddings compare word embeddings learned diﬀerent approaches. advanced document embedding techniques refer recursive neural networks convolutional neural networks embeddings documents training test data sets obtained one-vs-rest logistic regression model liblinear package used classiﬁcation. evaluate classiﬁcation performance metrics micro-f macro-f. contextual word similarity. also evaluate quality diﬀerent word embeddings task contextual word similarity measuring introduced given sentences task aims measure similarity pair words sentences. evaluation data includes word pairs sentential contexts collected wikipedia. contextual similarity scores word pairs collected amazon mechanical turk. wikifull data topic identities tokens recognized gibbs sampling algorithm. training wikifull data sets identity-sensitive word embeddings obtained. given pair words sentential contexts ﬁrst recognize identities words sentential contexts according eqn. able obtain corresponding embeddings words. similarity words calculated cosine similarity word embeddings. similarities pairs words calculated spearman correlation calculated similarities calculated based embeddings similarity scores given humans. topic used identities words. model also used types identities recognized. second variant allows word diﬀerent embeddings diﬀerent identities. dimension word embeddings diﬀerent models default without explicitly speciﬁed. window size skipgram mssg constructing word-context network line start looking results text classiﬁcation. table reports results text classiﬁcation dblp wikisample data sets. data sets topic category identities recognized. ﬁrst compare results approaches without considering word meanings diﬀerent contexts including skipgram line. performance line consistently outperforms skipgram three data sets consistent results reported reason line model learns word embedding ﬁrst constructing word co-occurrence network embedding network captures global structures word co-occurrences compared free text. mssg models considers variations word meanings diﬀerent contexts. word assigned diﬀerent topics diﬀerent documents; mssg word take diﬀerent senses diﬀerent contexts; meanings words determined identities diﬀerent depending contexts. though able diﬀerentiate meanings words diﬀerent contexts inferring topic identities performance good. because meanings words assigned topic assumed totally same reasonable. approach eﬀectively complements identity-sensitive word embedding diﬀerentiates meanings words topics/identities. performance mssg even inferior skipgram. reason mssg dynamically infer multiple senses words hard accurately estimate senses words diﬀerent contexts during learning process. models topic identity considered performance outperforms skipgram line. model also consistently outperforms model. reasons twofold. first approach leverages identity-level word co-occurrences utilize information; second model learns word embeddings embedding heterogeneous networks capture global structures word co-occurrences compared free text. also notice performance becomes using category identity even inferior skipgram. reason category identities tokens test documents hard recognize model. however model eﬀectively recognize identities tokens test document context embeddings words available model. also notice results category identity better topic identity. reason category identity relevant text classiﬁcation task compared topic identity. model also outperforms method dimension much larger. sults observed compared dblp wikisample data sets. performance still good. line model consistently outperforms skipgram. performance mssg still inferior skipgram. performance sentiment identity also good even inferior line model. model performs best considering sentiment identity also interested number identities would aﬀect performance. fig. reports performance text classiﬁcation w.r.t number identities dblp data sets. compared performance model quite stable sensitive number topics saves overhead choosing appropriate number topics using model. performance gain especially signiﬁcantly number topics speciﬁed less optimum one. reason diﬀerentiate meanings words topic/identity serious number topics small model meanings words topic/identity diﬀerentiated embedding. next look results task contextual word similarity. table reports results wikifull data set. compare word embedding approaches consider contextual word meanings including mssg ise. models topic identity used estimated unsupervised topics estimated. performance signiﬁcantly outperforms mssg model consistent previous results text classiﬁcation. still outperforms table results text classiﬁcation dblp wikisample data sets. types identities topics categories considered. topic identity numbers topics three data sets respectively. intuitively compare embeddings learned diﬀerent approaches part look nearest neighbors words based embeddings. similarity words calculated cosine similarity word embeddings. table presents examples nearest neighbors based embeddings learned diﬀerent approaches wikifull data set. overall word embeddings learned approach indeed able capture diﬀerent meanings word. take word language example. three different meanings language recognized model linguistic programming language human language. mssg model discovers diﬀerent meanings. result skipgram even worse mixture combination multiple meanings. fig. shows visualization identity-sensitive word embeddings dblp data topic identity. words mapped -dimensional space tsne fig. gives overview visualization. diﬀerent colors represent diﬀerent topics/identities ranked words topic visualized. right bottom zoom part visualization present topics. overall identity-sensitive word embeddings clear semantic structure. words belonging identities/topics clustered together diﬀerent identities/topics well separated. bottom topic examples fig. word probability taking diﬀerent identities/topics close other. explained linguistic theory meanings words diﬀerent senses still close fig. show examples words appear multiple topics including word networks learning language semantic. example word networks refer bayesian networks wireless networks; word language refer programming language natural language; word learning refer machine learning human learning. paper studied problem contextual word embeddings. acknowledged words take diﬀerent identities diﬀerent contexts proposed learn identity-sensitive word embeddings. proposed represent diﬀerent levels word co-occurrences heterogeneous network. principled heterogeneous network embedding approach based previous work proposed able obtain identity-sensitive word embeddings embeddings word identities. experiments show identity-sensitive word embeddings learned approach indeed able capture diﬀerent meanings word. experiments text classiﬁcation contextual word similarity proved effectiveness proposed approach. future plan study types word identities tags ideologies. besides interesting problem investigate whether identitysensitive word embeddings useful improving performance recognizing identities word tokens diﬀerent contexts. example whether identitysensitive word embeddings useful topic modeling process. potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies-volume pages association computational linguistics relationships sentiment categorization respect rating scales. proceedings annual meeting association computational linguistics pages association computational linguistics manning potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing volume page citeseer figure visualization identity-sensitive word embeddings dblp data using tsne topic identity used. diﬀerent colors represent diﬀerent identities", "year": 2016}