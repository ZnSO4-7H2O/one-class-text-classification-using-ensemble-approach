{"title": "Task-driven Visual Saliency and Attention-based Visual Question  Answering", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.NE"], "abstract": "Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.", "text": "challenge proposed far. four commonly used datasets cocoqa coco-vqa visualw feature different aspects. common practice tackle problem translate words word embeddings encode questions using bag-of-word long short term memory network encode images using deep convolutional neural networks following important step combine image question representations kind fusing methods answer generation concatenation elementwise multiplication parameter prediction layer episode memory attention mechanism etc. current works focus fusion features cares where focus questions image. common practice treat problem either generation classiﬁcation scoring task classiﬁcation gains popularity simplicity easiness comparison. works treat discriminative model learning conditional probability answer given image question. generative view emulate behavior people questions given image ﬁrst glance interesting regions. visual question answering witnessed great progress since classic problem unifying visual textual data system. many enlightening works explore deep image question encodings fusing methods attention effective infusive mechanism. current attention based methods focus adequate fusion visual textual features lack attention people focus questions image. traditional attention based methods attach single value feature spatial location losses many useful information. remedy problems propose general method perform saliency-like pre-selection overlapped region features interrelation bidirectional lstm novel element-wise multiplication based attention method capture competent correlation information visual textual features. conduct experiments large-scale coco-vqa dataset analyze effectiveness model demonstrated strong empirical results. visual question answering comes classic task combines visual textual modal data uniﬁed system. taking image natural language question input system supposed output corresponding natural language answer. problem requires image text understanding common sense knowledge inference. solution problem great progress approaching goal visual turing test also conducive tasks multi-modal retrieval image captioning accessibility facilities. terms single person unique taste choosing image regions interest him. large amount people statistical region-of-interest distributions. region patterns task-driven e.g. picture figure. task people focus mostly beds chairs laptop notebook regions captured weighted image image captioning task attention areas including striped ﬂoor. valuable intensify interesting region features suppress others image preprocessing step provides accurate visual features follow-up steps missing current works. analogy visual saliency captures standing regions objects image propose region pre-selection mechanism named task-driven visual saliency attaches interesting regions higher weights. taking advantage bidirectional lstm output arbitrary time step complete sequential information time steps after compute weight interest region feature relative them. best knowledge ﬁrst work employs analyzes bilstm models task-driven saliency detection ﬁrst contribution work. simple effective baseline method shows question feature always contributes predict answer image feature. image equally important question answer generation. necessary explore ﬁner-grained image features achieve better performance e.g. attention mechanism current attention based models generally correlation scores question image representations weights perform weighted region features resulting visual vector concatenated question vector ﬁnal answer generation. recent multi-step attention models deeper image understanding help achieve better performance regular attention models. however correlation score obtained inner product visual textual features essentially correlation vector obtained elementwise multiplication features. besides shows element-wise multiplication features achieves accurate results concatenation baseline model. hence propose employ element-wise multiplication attention mechanism fused features directly feed forward pooling layer ﬁnal fused feature. together saliency-like region pre-selection operation novel attention method effectively improves performance second contribution work. ﬁrst brieﬂy review saliency attention mechanism. then elaborate proposed method. present experiments baseline models compare stateof-the-art models visualize pre-selection saliency attention maps. finally summarize work. saliency generally comes contrasts pixel object surroundings describing outstanding could facilitate learning focusing pertinent regions. saliency detection methods mimic human attention psychology including bottomtop-down manners typical saliency methods pixelobject-oriented appropriate center bias difﬁculty collecting large scale tracking data. think task-driven saliency image features could conductive solving problem. inspires bilstm used saliency detection achieved good results text video tasks. sentiment classiﬁcation tasks assigns saliency scores words related sentiment visualizing understanding effects bilstm textual sentence. video highlight detection uses recurrent auto-encoder conﬁgured bilstm cells extracts video highlight segments effectively. bilstm demonstrated effectiveness saliency detection best knowledge used visual saliency task. visual attention mechanism drawn great interest gained performance improvement traditional methods using holistic image features. attention mechanism typically weighted image region features spatial location weights describe correlation implemented inner products question image features. explores ﬁner-grained visual features mimics behavior people attend different areas according questions. focusing knowing look multiplechoice tasks uses detected object regions plus holistic image feature make correlation question encoding uses correlation scores weights fuse features. uses last pooling layer features vgg- image region partitions adopts two-layer attention obtain effective fused features complex questions. proposes ingenious idea assembled network modules according parsed questions achieves multi-step transforming attention speciﬁc rules. location correlation vector representation besides concatenation image question features less accurate element-wise multiplication vector shown baseline model moreover many answers derived non-object background regions e.g. questions scenes hence object detection based attention methods. compared image captioning generates general descriptions image focuses speciﬁc image regions depending question. hand regions include non-object background contents hard object detection based methods. hand although people questions areas given image always region patterns attract questions. whole statistical region-of-interest patterns represent human-interested areas important later task. propose saliency-like region pre-selection attention-based framework illustrated figure. regarded classiﬁcation task simple easy transform generating scoring model. section elaborate model consisting four parts image feature pre-selection part models tendency people focus questions question encoding part encodes question words condensed semantic embedding attention-based feature fusion part performs second selection image features answer generation part gives answer output. described above current object detection based methods qualiﬁed answers derived speciﬁc object regions images example asked where bird/cat? answers fence/sink contained ilsvrc pascal detection classes. thus general pattern detector. addition generative perspective attention people focus questions. general visual saliency provides analogous useful information noticeable objects areas outstand surroundings case task. current attention mechanism relates question focusing location. samples available could yield region patterns attract questions statistics. statistical behavior large amounts workers amazon propose perform saliency-like pre-selection operation alleviate problems model patterns. image ﬁrst divided grids illustrated figure. taking grids region grids feed regions pre-trained resnet deep convolutional neural network produce n×n×di-dimensional region features dimension feature layer last fully-connected layer. since neighboring overlapped regions share visual contents corresponding features related focusing different semantic information. regard sequence regions result movement glancing image regions selectively allocated different degrees interest. speciﬁcally lstm special kind recurrent neural network capable learning long-term dependencies memory cell update gates endows ability retain information previous time-steps update rules lstm time step follows denote input forget output gates input region feature memory cell hidden unit output parameters trained. activate gates sigmoid nonlinearity cell contents hyperbolic tangent tanh gates control information memory cell retained forgotten element-wise multiplication inspired information completeness high performance bilstm encode region features directions using bilstm obtain scalar output region. output bilstm summation forward backward lstm outputs region loca+ tion n−t+ number regions n−t+ computed using hence output location inﬂuenced region features embodies correlation among regions. note that although dmn+ work uses similar bi-directional gated recurrent units visual input module purpose produce input facts contain global information. besides bigru takes features embedded textual space inputs. contrast bilstm used model takes directly visual features input main purpose output weights region feature selection. output values bilstm normalized softmax layer resulting weights multiplied region features. treat weights degree interest trained error back-propagation ﬁnal class cross entropy losses higher weights embody corresponding region patterns attract questions words region patterns higher attention values latter interaction question embeddings statistical way. question encoded using various kinds natural language processing methods lstm gated recurrent units skipthought vectors parsed stanford parser etc. since question encodings already dominate contribution answer generation compared image features simply encode question word wordvec embedding lstm encode questions match pre-selected region features. encode abstract higher-level information achieve better performance deeper lstm question encoding adopted model. according statistic image-question-answer training triples image feature pre-selection attached regions different prior weights generating meaningful region features. different questions focus different aspects visual content. necessary attention mechanism second select regions question effective features. propose novel attention method takes element-wise multiplication vector correlation image question features spatial location. specifically given pre-selected region features question embedding visual textual features common space dimension perform element-wise multiplication them. dc-dimensional fused features contain visual textual information higher responses indicate correlative features. traditional attention models correlation score achieved inner product mapped visual textual features region essentially elements fused feature. novel attention method noticeable advantages traditional attention i.e. information richer correlation vector versus correlation scalar effective element-wise multiplication vector versus concatenated vector visual textual features. correlative visual textual features question focus regions. choose apply pooling operation intermediate fused features pick maximum responses. produced dc-dimensional fused feature ﬁnal answer generation part. compared sum/average operation traditional attention models operation highlights responses ﬁnal fused feature every spatial location. taking problem classiﬁcation task simple implemented evaluated easy extended generation multiple choice tasks network surgery using fused feature previous step. linear layer softmax layer fused feature answer candidates entries top- answers training data. considering multiple choice problems e.g. visualw telling questions coco-vqa multiple choice tasks model adaptive extended concatenating question answer vectors fusion visual features using bilinear model ﬁnal fused feature answer feature possible future work. meanwhile view generation problem train lstm taking fused feature input obtain answer word lists phrases sentences training framework trained end-to-end using backpropagation feature extraction part using resnet kept ﬁxed speed training avoid noisy gradients back-propagated lstm elaborated rmsprop algorithm employed initial learning rate proved important prevent softmax spiking early prevent visual features dominating early simplicity proved similar performance pre-trained word embedding parameters initialize parameters network random numbers. randomly sample triples iteration. section describe implementation details evaluate model large-scale cocovqa dataset. besides visualize analyze role pre-selection novel attention method. implementation details obtain regions employing grids region stride grid. extract feature region layer last fully-connected layer resnet. dimension word embedding weights embedding initialized randomly uniform distribution similar performance pre-trained one. pre-selection bilstm region features layer size lstm question uses layers hidden units layer. common space visual textual features -dimensional. dropout convolutional linear layers. non-linear function hyperbolic tangent. training procedure early stopped accuracy increase validation iterations evaluate every iterations. takes around hours train model single nvidia tesla iterations. evaluation sample needs less millisecond. coco-vqa dataset largest among commonly used datasets contains tasks image datasets abstract scene dataset). follow common practice evaluate models tasks real image dataset includes training questions validation questions testing questions. many types questions require image question understanding commonsense knowledge knowledge inference even external knowledge. answers roughly divided types i.e. yes/no number other. evaluate results answer compared human-labeled answers accuracy computed metric i.e. accuracy predicted answer consistent least human-labeled answers. cocovqa dataset provide human-labeled answers training validation sets results testing tested evaluation server. whole testing named test-standard evaluated times total smaller development named testdev tested times times total. short coco-vqa dataset large hard enough evaluating models hence choose evaluate model conatt convolutional region pre-selection attention model replaces bilstm salatt model weight-sharing linear mapping implemented convolutional layer. besides also compare salatt model popular baseline models i.e. ibowimg state-of-the-art attention-based models i.e. d-nmn dmn+ tasks coco-vqa. results analysis effectiveness proposed functions train function-disabled models coco-vqa training show accuracies validation table. columns that holistic better traatt proving effectiveness elementwise multiplication feature fusion compared concatenation features. regatt better holistic indicating novel attention method indeed enriches visual features improves performance. salatt better regatt demonstrating strength region pre-selection mechanism. conatt worse salatt showing bilstm important region pre-selection part. consistent improvement resnet features showing importance good features vqa. summarize accuracies test-dev table. test-standard results table. results comparative higher attention based methods especially multiple-choice tasks. results answer type other includes object scene type questions demonstrate competence model detection. note that apply proposed region preselection mechanism basic model embedded attention-based models improve performance. computation training time regions compared attention-based methods observation many small objects could split regions adverse counting questions could improved possible future work. illustrate three groups samples produced model figure. group contains four ﬁgures left right bottom respectively original image pre-selection weights image attention maps different questions corresponding questions ground truth answers predicted answers shown them. number parentheses means amount human-labeled answer entry. weights normalized minimum maximum visualization enhancement i.e. weight dark region necessarily take ﬁrst sample example pre-selection operation gives high weight boy’s head region interesting people attract questions question dressed weather? attention focuses clothes surrounding regions positive answer. question what doing? attends snowboard thus giving answer snowboarding. third sample gives inaccurate explainable answers i.e. birds live park/zoo come food provided tourist classiﬁed pets left hand woman holds indeed phone human-labeled answers focus right hand. work propose general solution integrates region pre-selection novel attention method capture generic class region richer fused feature representation. procedures independent meancontribute better performance. although model simple achieves comparative higher empirical results state-of-the-art models. possible future works include adopting ﬁner-grained grids capture precise regions employing stacked attention layers multi-step reasoning accurate answer location applying general prefigure samples generated model. weights normalized visualization enhancement i.e. weight dark region necessarily represent question ground truth answer predicted answer respectively. green denotes true prediction false.", "year": 2017}