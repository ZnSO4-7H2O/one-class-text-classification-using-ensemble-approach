{"title": "Lifted Relational Neural Networks", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.", "text": "vojtˇech aschenbrenner faculty mathematics physics charles university prague czech republic filip ˇzelezn´y faculty electrical engineering czech technical university prague prague czech republic propose method combining relational-logic representations neural network learning. general lifted architecture possibly reﬂecting background domain knowledge described relational rules handcrafted learned. relational rule-set serves template unfolding possibly deep neural networks whose structures also reﬂect structures given training testing relational examples. diﬀerent networks corresponding diﬀerent examples share weights co-evolve training stochastic gradient descent algorithm. framework allows hierarchical relational modeling constructs learning latent relational concepts shared hidden layers weights corresponding rules. discovery notable relational concepts experiments relational learning benchmarks demonstrate favorable performance method. keywords relational learning lifted models neural networks lifted models also known templated models attracted signiﬁcant attention recently areas statistical relational learning. lifted models deﬁne patterns speciﬁc models unfolded. example lifted markov network model express friends smokers tend smokers pattern constrains probabilistic relationships sets vertices corresponding particular friends-smokers derived ground markov network. lifted patterns typically encoded relational logic-based languages. contribute method lifted feed-forward neural network learning ground network structure unfolded weighted rules relational logic. relational rules instantly interpretable handcrafted domain expert learned e.g. techniques inductive logic programming weights ground neural networks determined weighted relational rules learned stochastic gradient descent algorithm. means weights diﬀerent ground neurons constructed relational rule tied framework similarly weights shared lifted graphical models statistical relational learning weights tied together application ﬁlters convolutional neural networks deep learning. salient property approach distinguishing previous studies adapting neural networks relational learning ground network structure depends relational rule also particular example i.e. diﬀerent networks constructed diﬀerent examples exploit particular relational properties. however diﬀerent networks share weights bound relational rules weight-updates performed training example reﬂected networks produced examples allows model learn directly relational data. main advantage presented approach eﬀectively learn weights latent relational structures. diﬃcult task existing lifted systems based probabilistic inference typically needs expensive expectation maximization algorithms order learn parameters latent structures present. hand deep neural networks exploit work shown eﬀectively learn latent structures although obviously ground non-relational settings. combining relational logic deep neural networks obtain framework ﬂexible enough learn weights latent relational structures also verify experimentally. several works combining propositional relational logic neural networks none existing methods able learn weights latent non-ground relational structures. rest paper organized follows. next section brieﬂy summarizes preliminaries regarding relational logic assumed neural network paradigm. section explains principles proposed lifted relational neural networks method. section describes useful modeling constructs. section show weight-learning implemented section places presented methods context existing works. section subject method comparative experimental evaluation relational learning benchmarks conclude paper. ﬁrst-order logic theory formulas formed constants variables functions predicates constant symbols represent objects domain interest written lower-case. variables range objects domain written capitalized ﬁrst letter. function symbols used paper. predicate symbols represent relations among objects domain attributes. term constant variable atom predicate symbol applied tuple terms formulas constructed atoms using logical connectives quantiﬁers. ground term term containing variables. ground atom atom ground terms arguments literal atom negation atom clause universally quantiﬁed disjunction literals. risk confusion write universal quantiﬁers explicitly. clause exactly positive literal deﬁnite clause. deﬁnite clause negative literals called fact. given ﬁrst-order logic theory ground atoms constructed using constants function symbols predicates present theory herbrand base. herbrand interpretation also called possible world assigns truth value possible ground atom given herbrand base. formulas satisﬁable exists least world formulas true; world herbrand model. satisﬁable deﬁnite clauses least herbrand model model unique. least herbrand model function-free deﬁnite clauses constructed ﬁnite number steps using immediate-consequence operator immediate consequence operator maps space activation function predeﬁned family diﬀerentiable functions. neural network deﬁnes mapping input space target space vectors following pattern neural interconnections parameterized weights mapping seen composition activation functions feed forward neural networks typically hierarchical compound non-linear weighted sums gk)) conveniently depicted weighted directed model acyclic graph neurons adapting weights learned approximate target function typically performed sort gradient descent minimization given cost function cost {wd} capturing discrepancy upon training samples lifted relational neural networks lifted relational neural network weighted deﬁnite clauses i.e. pairs function-free deﬁnite clause real number. weighted deﬁnite clauses denote corresponding deﬁnite clauses without weights i.e. must satisfy following nonrecursiveness requirement must exist strict ordering predicates rule predicate head predicate body given lrnn least herbrand model deﬁne grounding lrnn bkθ} deﬁned ground deﬁnite clauses herbrand model already outlined notice contain predicates male/ father/ ground atoms based least herbrand model deﬁnition lrnn grounding. families ground neural network feedforward neural network constructed follows. reason allow recursion clearer explain weight learning next section. here note whereas rule sets without recursion lead optimization problems solvable algorithm basically modiﬁed back-propagation algorithm rule sets recursion would lead complicated optimization problems would directly allow exploit existing results training feedforward neural networks. example instance describe general rules explosiveness molecules sets facts describing particular molecules. lrnn predicting whether explosive simply construct ground compute output respective atom neurons explosive explosive distinctive feature lifted models ground lrnns examm merged lifted lrnn composed general ground networks displayed fig. rules provide adaptive means create latent groups atom types that bond predicate connecting couples atoms form relational features behave similarly if-then rules prefer outputs rule neurons high inputs atom neurons corresponding literals body rule high outputs. similarly prefer output atom neurons intuitively behave similarly disjunction high least rule neurons fact neurons inputs given atom neuron high output. logical operators various fuzzy logics serve inspiration selecting suitable activation functions. example goedel fuzzy logic conjunction ∧···∧ fuzzy logic literals given mini disjunction given maxi emulate reasoning goedel logic could simply mini maxi maxi here output rule neuron rh←b∧···∧bk minimum value makes fuzzy truth value implication equal goedel fuzzy logic. likewise output aggregation neuron minimum value makes fuzzy truth value respective ground implications equal simultaneously. lrnns emulate fuzzy logic programming. weights tied template denoted colors. sake space display ground rule sets instead complete ground networks fig. illustrates correspondence full ground neural network. rationale family activation functions follows. already mentioned activation function high output inputs high. achieve this crudely approximate lukasiewicz fuzzy conjunction given max{ function sigm plot function sigm shown left panel fig. activation function outputs value equal highest inputs. example illustrates seen ﬁnding best match pattern activation function high output least inputs high inputs somewhat high. satisfy this crudely approximate lukasiewicz fuzzy disjunction given min{ b+···+bk} function sigm plot function sigm shown right panel fig. example illustrates intuition activation function would also contain edge connecting yellow vertices. thus instance considered physicochemical property atoms instead brightness colors molecules instead colored graphs corresponding networks could detect presence molecular substructure similar prescribed pattern. weighted facts outputs aggregation neurons corresponding rules predicate head combined using activation functions intuitively rules facts predicate head seen forming logistic regression values given aggregation neurons lower layers. lrnn layer example achieve eﬀect using techniques propositionalization treating bodies rules features feeding attributes logistic regression classiﬁer. however soon lrnn max-sigmoid activation function obviously possible. useful interested detecting patterns less useful situations similar depicted next example. max-sigmoid family prediction whether individual would entirely based existence least person already diagnosed. would obviously meaningful base predictions fraction one’s friends diagnosed. another advantage avg-sigmoid family activation functions maxsigmoid family also functions avg-sigmoid family everywhere diﬀerentiable note activation function families based combinations diﬀerent aggregation functions might also exploited lrnn learning. section describe several constructs easy lrnns would diﬃcult impossible implement existing frameworks combining logic neural networks solely because unlike lrnns frameworks allow simultaneous learning target auxiliary predicates. moreover somewhat similar constructs could principle used probabilistic logic programming systems problog learning would require running costly algorithms repeatedly need perform computationally expensive probabilistic inference. many domains needs create clusters certain objects order achieve good generalization. case e.g. prediction adverse eﬀects drugs signiﬁcant improvements predictive accuracy gained methods able create auxiliary clusters similar drugs however existing methods still rather ad-hoc relying greedy discrete clustering. lrnns easy deﬁne predicates representing clusters train weights automatically prediction target predicates illustrated following example. example suppose that similarly temporal data patients drugs patients took time instants changes health occurred. also assume general rules like using max-sigmoid family aggregation functions weight learning lrnn implicitly create clusters drugs interact adversely clusters drugs clusters adverse eﬀects corresponding combinations drugs well appropriate deﬁnition predicate shortperiod. able perform experiments domain described example data available privacy reasons perform simpler experiments organic chemistry domains implicitly created soft clusters correspond groups atom types atomic bond types. describe experiments detail section show useful clusters indeed created automatically weight learning lrnns. reasons discussing example adverse eﬀects drugs indicate machinery lrnns promising existing problems rather ad-hoc solutions exist currently. however probably necessary mutually friends order rule make sense. rule still valid maybe lower certainty four people actually friends maybe even pairs more. easily expressible lrnns suitably deﬁning predicate clique automatically learning respective weights here predicate friends assumed part description examples soft matching cliques facilitated deﬁnition predicate based using activation functions max-sigmoid family predicates hasflu obtain desired behavior suitable weights. soft clustering soft matching probably modeling concepts would used often practice modeling concepts easily implementable lrnns. concept dimensional approximation sets graph patterns share structure labels exempliﬁed below. example consider problem predicting property e.g. toxicity organic molecules depends presence substructures certain rather large set. patterns structure e.g. aromatic six-rings substitutions positions could principle probabilistic modeling approximate probability distribution substitutions diﬀerent places substitutions jointly occurring patterns would high probability substitutions small probability. probabilistic modeling approach possible requires explicitly patterns. patterns correspond latent concept would resort hand similar approximations latent patterns modeled lrnns quite easily. instance want capture pair-wise dependencies substitutions neighboring atoms ﬁrst deﬁne auxiliary binary predicates basic aromatic six-ring benzene ring ring carbon atoms connected hydrogen atom connected aromatic bonds. carbon atoms replaced another atom speak substitution. exploiting process grounding lifted template facilitating weight sharing ground networks lrnns also emulate principal structures convolutional neural networks next example shows. example consider structure popular convolutional neural network architecture composed sparse convolutional layers alternated max-pooling. within sparse layer weights corresponding single convolution ﬁlter eﬀectively bound value ﬁlter repeated across. within selected subregions resulting feature-map values aggregated application max-pooling i.e. maximal values feature-map region propagated further. structural idea eﬃciently encoded lrnn generalized feature maps varying size choice max-sigmoid function family simple lifted template deﬁned follows corresponds convolution ﬁlter bound arbitrary number relational patterns case simple linear segments three neighboring features input feature-map deﬁned linearly ordered weighted facts feature family ensures max-aggregation applied convolutional layer. visualization grounding template particular feature-map consecutive values provided figure demonstration part standard convolutional neural network structure sparse convolutional layer composed application ﬁlter creating feature-map layer followed max-pooling calculation structure presented ground lrnn eﬃciently encoded template example generalizing feature-vectors unrestricted size. atoms denote output ground neural network goal learning atom neuron process weights rules minimizing cost cost predeﬁned cost function measures discrepancy output atom neurons training query atoms desired target values. similarly conventional weight adaptation performed gradient descent steps example pass output output atom neurons target values sigmoid. useful learning avg-sigmoid activation function family. alternative would cross-entropy error function. ground network corresponding ground body literals boθm respective ground rule carried recursive manner fact neurons reached ﬁxed constant values deﬁned note whole evaluation composed diﬀerentiable functions gradient since recursion allowed weight appear simple path fact neuron atom neuron. therefore possible learn weights using conventional online stochastic gradient descent algorithm except increments shared weights must accumulated simple consequence linearity partial diﬀerentiation. principle exploited e.g. learning convolutional neural networks remark consider ground regular feed forward neural network weights network shared i.e. bound value restriction particular weight appears simple path input output activation functions layers tiation operator therefore gradient computed ground neural networks created given lrnn standard components corresponding particular weight accumulated. weight could update respective weights template step iteration). iterates ground networks random order computes gradient error function current particular example given current weights template updates weights accordingly continues iterating steps order reduce risk getting stuck poor quality local optima also employ restart strategy algorithm. main inspiration work presented paper lifted graphical models markov logic networks bayesian logic programs however none existing lifted graphical models particularly well suited learning parameters latent relational structures. approach also generally related prior combining logical rules neural networks also known neural-symbolic integration kbann system. kbann also constructs network structure given rules rules propositional rather relational serve lifted template. therefore impossible learn relational latent structures soft clustering ﬁrst-order-logic constants. recent system cilp++ utilizes relational representation however converted propositional form propositionalization technique means latent relational structures exempliﬁed section cannot learned cilp++ either. somewhat closely related paper fonn also designs technique forming network relational rule however rule producing -layer networks relational patterns hierarchically aggregated. many approaches neural-symbolic integration aiming relational representations e.g. based core method typically search uniform model logic program scope thus principally diﬀer presented lifted modeling approach. standard feed-forward neural networks seen special case lrnns since ﬁxed neural architecture encoded corresponding ground rule respective activation functions salient aspect method allows learning structured examples rather attribute vectors. previous work adapting neural networks cope certain facets relational representations. example extension multi-instance learning presented similarly directed work facilitated aggregative reasoning process sets related tuples relational database sequence recurrent neural network structure also presented general structures approaches principally diﬀerent presented method follow lifted modeling strategy cope variations structure relational samples. loosely related works arise also neural networks community various recursive auto-encoders based idea reduced descriptions trained encode structured data. another line work convolutional neural networks techniques indirect encoding exploiting patterns regularities neural connections create compressed representations large neural networks. however approaches still geared towards learning ﬁxed-length propositional rather relational data. section describe experiments performed datasets organic molecules mutagenesis dataset four datasets predictive toxicollogy challenge nci-gi datasets mutagenesis dataset contains molecules labels denoting mutagenicity. number results published mutagenesis dataset extended features providing additional expert knowledge relational properties molecules degrading role learning capabilities relational models. extra features utilize atom-bond information. predictive toxicology challenge dataset composed four datasets molecules labeled toxicity female rats mouse male mouse nci-gi datasets contains several thousands molecules labeled ability inhibit growth diﬀerent types tumors. compare performance lrnns state-of-the-art relational learners kfoil nfoil kfoil combines relational rule learninng support vector machines nfoil combines relational rule learning naive bayes learning. lrnns simple hand-crafted template based idea implicit soft clustering described section principally identical template discussed figure template deﬁnes predicates clusters atom types predicates clusters bond types. three predicates representing atom-type clusters composed exhaustive lists atom types occurring datasets e.g. composed exhaustive lists bond types occurring datasets. predicates used deﬁnitions predicates diﬀerent types small chains atoms toxic. using generic template datasets make sure additional expert knowledge involved idea process learning useful latent relational concepts created within neural network means weight adaptation rather explicit enumeration contrast propositional approaches indeed none rules used template useful prediction hard logic rule without weight adaptation. parameters lrnns empirical risk minimization principle training cross-validation folds select parameters step size restarts number iterations etc. obtain unbiased estimates performance methods since test data never involved parameter selection. time training lrnn order hours larger nci-gi datasets. results experiments summarized figure lrnns perform clearly best algorithms terms accuracy lower prediction error kfoil nfoil signiﬁcant majority datasets. also tried compare lrnns another recent algorithm combining logic neural networks called cilp++ didn’t perform well relational datasets able obtain using cilp++ accuracy signiﬁcantly higher simple majority class error datasets. demonstration provide visualization latent grouping lrnn layers mutagenesis ptc-mr datasets fig. apparent learned weights ﬁgures hidden layers indeed learning useful latent groupings atom types. interesting note mutagenesis dataset learned groupings atom types gives atoms almost weight actually makes sense corresponds wild-card atom type. hand similar behavior relatively reasonable results mutagenesis reported expertknowledge attributes used experiments reported therein might explain discrepancy results. figure visualization latent concepts demonstrated lrnn’s weights rules deﬁning particular groups atoms learned mutagenesis dataset ptc-mr datasets. lighter colors denote lower darker colors higher weights respectively. order test modeling concept described section performed additional experiment mutagenesis dataset. used almost exactly template example instead ring structures used chains varying lengths trained resulting lrnn optimize template’s weights however interested extracting learned patterns. determined chains atoms gave highest output learned latent predicates. obtained following atom chain structures c-c-f c-cl c-br c-c-o o-n-c. least structures appear directly relevant mutagenicity contain organic structures containing halogen atoms structures relevant mutagenicity combination structures. paper introduced method combining relational-logic representations feedforward neural networks. introduced method close spirit lifted graphical models viewed providing lifted model construction ground neural networks. performed experiments indicate possible achieve state-of-the-art predictive accuracies weight learning generic templates able induce notable auxiliary concepts. many directions future work including structure learning transfer learning studying diﬀerent collections activation functions. important future direction also question extending lrnns support recursion. supported cisco sponsored research project modelling network trafﬁc relational features. supported czech science foundation project grant leverhulme trust", "year": 2015}