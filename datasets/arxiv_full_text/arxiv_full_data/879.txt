{"title": "Alternating optimization method based on nonnegative matrix  factorizations for deep neural networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks (DNNs). This method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative matrix factorizations (semi-NMFs). In this method, optimization processes are performed by calculating weight matrices alternately, and backpropagation (BP) is not used. We also present a method to calculate stacked autoencoder using a NMF. The output results of the autoencoder are used as pre-training data for DNNs. The experimental results show that our method using three types of NMFs attains similar error rates to the conventional DNNs with BP.", "text": "abstract. backpropagation algorithm calculating gradients widely used computation weights deep neural networks method requires derivatives objective functions diﬃculties ﬁnding appropriate parameters learning rate. paper propose novel approach computing weight matrices fully-connected dnns using types semi-nonnegative matrix factorizations method optimization processes performed calculating weight matrices alternately backpropagation used. also present method calculate stacked autoencoder using nmf. output results autoencoder used pre-training data dnns. experimental results show method using three types nmfs attains similar error rates conventional dnns deep neural networks attracted great deal attention high eﬃciency various ﬁelds speech recognition image recognition object detection materials discovery. using backpropagation technique proposed rumelhart computational performance improved training multilayer neural networks. however learning often takes long time converge fall local minimum. bengio proposed method improve general performance pre-training autoencoder. moreover selection appropriate learning rates restriction weights dropout also used minimize expected error. hinton discussed initialization weights neural networks variations fully-connected networks convolutional networks recurrent networks. lecun showed convolutional neural networks attain high eﬃciency image recognition. dnns activation functions used attain nonlinear properties. recently rectiﬁed linear function often used. feedforward neural networks computed multiplying weight matrices input matrices. thus main computations matrix–matrix multiplications accelerators gpus employed obtain high paper propose novel computing method fully-connected dnns uses types semi-nonnegative matrix factorizations method optimization processes performed calculating weight matrices alternately used. also present method calculate stacked autoencoder using output results autoencoder used pre-training data dnns. presented method computations represented matrix–matrix computations accelerators gpus mics employed like computations. computations mini-batches used avoid stagnations optimization precess. small mini-batch sizes decreases matrix sizes gains reductions computations. presented method also uses partitioned matrices; however matrix size larger conventional expect high performance. paper organized follows. section review conventional method computing dnns. section present method computing weights dnns using types semi-nmfs. also present method calculate stacked autoencoder using nmf. section show experimental results proposed approach. section presents conclusions. nout sizes input output units training data respectively. moreover rnin×m rnout×m input output data. using weight matrix rnout×nin bias vector rnout objective function layer neural networks written divergence function activation function regularization term. several activation functions sigmoid functions like logistic function hyperbolic tangent function. recently rectiﬁed linear unit widely used. basic concept algorithm solve alternating optimization optimizes weight matrix one. initial guesses respectively. autoencoder initial guesses discussed section iteration also deﬁne objective functions minimization problem nonnegative constraint minimization problem like however nonlinear activation function. paper call problem nonlinear semi-nmf. one. here note nonnegative constraint also note that require solution nonlinear least squares problems solved stationary iterationlike methods shown algorithms pseudo-inverse practice pseudo-inverse approximated using low-rank approximation using semi-nmf nonlinear semi-nmf algorithm proposed method summarized algorithm practice input data approximated using low-rank approximation based singular value decomposition proposed method also mini-batch technique. submatrix input data corresponding mini-batch index mini-batch. then order mini-batch technique proposed method need compute low-rank approximation uℓσℓv iteration. reduce required computational cost reusing results low-rank approximation follows section evaluate performance proposed method using stacked autoencoder fully-connected dnns mnist cifar several techniques improving performance aﬃne/elastic distortions denoising autoencoder. techniques also expected improve performance algorithm. therefore section make comparison simple proposed method number iterations autoencoder lsqs threshold low-rank approximation input data mnist cifar respectively. size mini-batches autoencoder computed using random samples. optimizing parameters used adam optimizer adam optimizer initial learning rates stacked autoencoder tuning mnist cifar respectively. parameters adam default parameters tensorflow. used normalized initialization initial guesses stacked autoencoder. size mini-batches autoencoder computed using random samples. fig. shows convergence history proposed method hidden units mnist cifar. table shows conﬁdence interval error rate computation time epoch methods several hidden units mnist. experimental results show method attains similar error rate several hidden units conventional dnns speciﬁcally proposed method achieves better error rates deeper hidden units. moreover proposed method needs smaller computation time stacked autoencoder almost computation time tuning. paper proposed alternating optimization algorithm computing weight matrices fully-connected dnns using semi-nmf nonlinear semi-nmf. also presented method calculate stacked autoencoder using nmf. experimental results showed method using attains similar error rate similar computation time conventional dnns almost computations proposed method represented matrix–matrix computations accelerators gpus mics employed like computations. proposed method also uses mini-batch technique; however matrix size larger conventional therefore expect proposed method achieves high performance recent computational environments. future work consider bias vector sparse regularizations activation functions. moreover extend algorithm convolutional neural networks. also consider parallel computation implementation evaluate performance recent parallel environments.", "year": 2016}