{"title": "A Unified Perspective on Multi-Domain and Multi-Task Learning", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework unifies MDL and MTL as well as encompassing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation provides an alternative pipeline for zero-shot learning (ZSL), where a model for a novel class can be constructed without training data. Moreover, it leads to a new and practically relevant problem setting of zero-shot domain adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model for an unseen domain can be generated by its semantic descriptor. Experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives.", "text": "paper provide neural-network based perspective multi-task learning multi-domain learning introducing concept semantic descriptor framework uniﬁes well encompassing various classic recent mtl/mdl algorithms interpreting different ways constructing semantic descriptors. interpretation provides alternative pipeline zero-shot learning model novel class constructed without training data. moreover leads practically relevant problem setting zero-shot domain adaptation analogous novel domains model unseen domain generated semantic descriptor. experiments across range problems demonstrate framework outperforms variety alternatives. multi-task multi-domain learning established strategies improve learning sharing knowledge across different related tasks domains. multi-domain learning refers sharing information problem across different contextual domains multi-task learning addresses sharing information different problems domain. domain/task distinction sometimes subtle methods proposed also address vice-versa settings sometimes loosely used interchangeably. however useful distinguish clearly domain relates covariate bias implicitly captured particular dataset speciﬁc data capture device. example ofﬁce dataset contains three domains related image source amazon webcam dslr. multi-domain learning problem posed training particular object recogniser across three domains contrast multi-task problem would share information across recognisers individual object categories issue simultaneously addressing multiple tasks multiple domains seems un-addressed literature knowledge. paper propose neural network framework addresses multi-domain multitask learning perform simultaneous multi-domain multi-task learning. concept framework idea multivariate semantic descriptor tasks domains. descriptor often available metadata exploited improve information sharing mdl. show various classic recent mtl/mdl methods special cases framework make particular assumptions descriptor existing algorithms typically implicitly assume categorical domains/tasks less effective information sharing detailed task/domain metadata available. example classic school dataset poses task predicting students’ grades typically interpreted containing domain corresponding school. however since school three year groups representing domains semantic descriptor tuple better information sharing. framework exploits multi-variate semantic descriptors effectively existing mtl/mdl algorithms would struggle implicitly consider tasks/domains atomic. setting addresses automatically constructing test-time classiﬁer categories unseen training time. neural-network framework provides alternative pipeline zsl. interestingly leads novel problem setting zero-shot domain adaptation synthesising model appropriate unseen domain given semantic descriptor. example suppose audio recogniser trained variety acoustic playback environments variety microphone types synthesise recogniser arbitrary environmentmicrophone combination? knowledge ﬁrst time zero-shot domain adaptation addressed speciﬁcally. multi-task learning aims jointly learn tasks discovering exploiting task similarity. various assumptions made achieve this. early study assumed linear model task written considered shared knowledge beneﬁts tasks task-speciﬁc knowledge. another common assumption predictors dimensional subspace imposing -norm predictor matrix column task results low-rank implicitly encourages parameter sharing. however assumes tasks related likely violated practice. forcing predictors shared across unrelated tasks signiﬁcantly degrade performance phenomenon called negative transfer task grouping framework thus proposed kang partitions tasks disjoint groups group shares dimensional structure. partially alleviates unrelated task problem misses fundamental information shared tasks overlap subspaces group. middle ground go-mtl algorithm allows information shared different groups representing model task linear combination latent predictors. thus concept grouping longer explicit determined coefﬁcients linear combination. intuitively model construction thought matrix column latent predictor coefﬁcient vector cues construct model task worth noting kind predictor matrix factorisation approach explain several models kumar daum´e regularised decomposition passos linear gaussian model prior earlier study assumes unit vectors generated dirichlet process methods literature assume task atomic entity indexed single categorical variable. recent studies noticed drawback strategy represent task structured metadata e.g. school dataset. thus replace predictor matrix tensor linear models across categorical variable placed tensor. follow line argyriou impose variety regularisations mentioned tensor ranks matriciations tensors scaled latent trace norm however suffers strong assumption tasks related. domain adaptation extensive work domain adaptation variety studies proposed supervised unsupervised methods. mentioned typical assumption domains indexed single categorical variable example data source amazon/dslr/webcam benchmark dataset pascal/imagenet/caltech modality image/video viewing angle paper take alternative approach generalising conventional categorical formulation domains instead investigate information sharing domains described vector discrete parameters. multi-domain learning multi-domain learning shares properties domain adaptation multi-task learning. conventional domain adaptation explicit pair source target domain knowledge transfer source→target. contrast encourages knowledge sharing directions. although existing algorithms reviewed previous section tackle well distinguish difference testing time makes prediction problem across multiple domains handles different problems zero-shot learning aims eliminate need training data particular task. widely studied different areas character object recognition typically label space training test data disjoint data seen test-time categories. instead test-time classiﬁers constructed given mid-level information. although diverse ways existing methods follow pipeline palatucci semantic descriptor refers attributes semantic word vectors work considered alternative pipeline similar larochelle frome light following illustration going beyond conventional generalise notion zero-shot learning tasks zero-shot learning domains. context zero-shot means training data seen target domain prior testing. challenge construct good model novel test domain based solely semantic descriptor. closest work zero-shot domain adaptation setting ding addresses issue missing modality help partially overlapped modalities previously seen. however single ﬁxed modality pair rather synergistically exploiting arbitrary number auxiliary domains multidomain framework. note despite title blitzer actually considers unsupervised domain adaptation without target domain labels target data. assume domains domain instances. denote feature vector instance domain associated semantic descriptor pair }j=··· ni}i=··· note that multi-domain multi-task learning instances effectively associated semantic descriptor indicating domain without loss generality propose objective function minimises empirical risk domains model understood two-sided neural network illustrated figure contains learning processes left-hand side representation learning starting original feature vector right-hand side model construction starting associated semantic descriptor weights train side. train standard back propagation performed loss calculated ground truth prediction neural network interpretation sides arbitrarily complex inner product layer sufﬁcient unify existing mdl/mtl algorithms demonstrate efﬁcacy approach. case d-by-k matrix b-by-k matrix number units middle layer; length feature vector semantic descriptor respectively. prediction based next demonstrate variety existing algorithms special cases general framework. clarity show mdl/mtl setting domains/tasks. observe rmtl feda mtfl go-mtl assume speciﬁc settings table notion used kept original paper e.g. analogous kumar daum´e matrices second column corresponding domain’s semantic descriptor different methods. methods implicitly assuming single categorical domain/task index -of-n encoding semantic descriptor argue structured domain/task-metadata often available framework directly exploited improve information sharing compared simple categorical indices. example suppose categorical variables describe domain states four distinct domains encoded distributed fashion contrast -of-n form used traditional multi-task learning methods ability exploit structured domain/task descriptors available improves information sharing compared existing mtl/mdl methods. experiments demonstrate examples problems multivariate domain/task metadata efﬁcacy improve learning. multi-domain multi-task existing frameworks focused either settings considered together. interpretation provides simple means exploit simultaneously better information sharing multiple tasks multiple domains available. domain task descriptors respectively mdmt learning performed simply concatenating descriptors corresponding domain task individual instance learning. zero-shot learning mentioned dominant zero-shot learning pipeline train time mapping learned classiﬁer/regressor task descriptor binary attribute vector continuous word-vector describing task name testing time prototype semantic vector novel class presented zero-shot recognition performed matching estimate prototype e.g. nearest neighbour framework achieved presenting novel semantic vector turn along novel category instances zero-shot recognition given maxj zero-shot domain adaptation zero-shot domain adaptation task also addressed framework. distributed rather -of-n encoded domain descriptor subset domains necessary effectively learn thus model suitable data novel held-out domain constructed applying semantic descriptor along data demonstrate framework experimental settings zsda mdmt. implementation implement model help caffe framework though don’t place regularisation terms non-linear function placed encourage sparse models σq). choice loss function regression classiﬁcation euclidean loss hinge loss respectively. preliminary experiments show mtl/mdl baselines compare proposed method single task learning baseline linear logistic regression regularisation four multi-task learning methods rmtl feda mtfl go-mtl note methods re-implemented within proposed framework. veriﬁed implementations original ones found performance difference signiﬁcant. baseline methods traditional -of-n encoding distributed descriptor encoding based metadata problem. zero-shot domain adaptation follow setting learn except domain held time. construct test-time models held domains using semantic descriptor. evaluate baselines blind-transfer learning single linear/logistic regression model aggregated data seen domains. ensure fair comparison distributed semantic descriptors concatenated feature vectors baselines i.e. included plain feature. tensor-completion tensor rdpp··· store linear models trained number categorical variables number states categorical variable zsda formalised setting model parameters held-out domain missing values recovering low-rank tensor completion algorithm low-rank strategy corresponds implementation romera-paredes data classic dataset collects exam grades students schools. given features regression problem predict student’s exam grade. schools three year groups. school year groups naturally form multivariate domains. note schools data students three year groups also choose school year group students domain sufﬁcient training data. finally distinct domains given categorical variables. settings results learn domains together zsda leave-onedomain-out strategy constructing test-time model based held-out domain’s descriptor learned training domains. case training/test split %/%. note test sets zsda same. results table averages test domains averages held-out domain performance holding domains turn method outperforms alternatives case. audio analysis tasks affected variety covariates notably playback device environment listening device directly applying model trained condition/domain anresult poor performance. moreover covariates/domains combinatorial models cannot trained situations even applying conventional domain adaptation scalable. zero-shot domain adaptation potential address this model could calibrated given environment. data investigate recognition complex noise domains covering acoustic environment microphone type. consider music-speech discrimination task introduced tzanetakis cook includes music speech tracks. categorical variables smartphone microphone live concert hall environment states off. four domains generated original live recording smartphone recording smartphone live hall noises synthesised audio degradation toolbox settings results mfcc extract audio features k-means build bag-of-words representation. split data training test keep test sets zsda. results table break results domain overall domain held-out case method best joint-best better exploiting semantic descriptor exception least practical case zsda recognition noise free environment given prior training noisy environments. zsda result generally demonstrates models synthesised deal effectively multivariate domains covariate combinations without needing exhaustively data explicitly train models would conventionally required. animal attributes includes images animal categories -dimensional binary attribute vector. attributes black furry stripes describe animal semantically provide unique mapping combination attributes animal. original setting split animals training hold testing. evaluate condition investigate multi-task learning attributes classes improves approaches typically taken analysing helps attributes semantic task descriptor traditional setting semantic descriptor -of-n unit vector indexing tasks. training decompose multi-class problem categories one-vs-rest binary classiﬁcation tasks. note case semantic descriptor reveals label given testing. one-vs-rest classiﬁers instance rank scores produce label. multi-task learning recently released decaf feature awa. pick animals training moderately overlapped attributes ﬁrst half images training test rest. results table show limited improvement existing approaches standard stl. however attributedescriptor approach encoding tasks improves accuracy stl. zero-shot learning adopt training/testing split lampert blind-transfer baseline meaningful different binary classiﬁcation problems aggregating lead anything. also tensor-completion practical exponential space observations. method achieves multi-class accuracy compared direct-attribute prediction approach using decaf features. recent result using decaf feature deng uses additional higher order attribute correlation information. given design solution speciﬁcally exploit higher order correlation result encouraging. restaurant consumer dataset introduced vargas-govea contains customer-to-restaurant scoring records record features three scores food service overall. build multi-domain multi-task problem follows domain refers restaurant task regression problem predict three scores given instance instance -dimensional feature vector based customer’s restaurant’s proﬁle. records cover restaurants scores pick frequently scored ones split training test sets equally. semantic descriptor constructed concatenating -bit domain -bit task indicator. conventional interpretations dataset consider atomic tasks. thus task overlap across domain domain overlap across task ignored. results table shows approach outperforms traditional setting better representing distributed mdmt problem. paper proposed uniﬁed framework multi-domain multi-task learning. core concept semantic descriptor tasks domains. used unify improve variety existing multi-task learning algorithms. moreover naturally extends single categorical variable index domains/tasks multivariate case enables better information sharing additional metadata available. beyond multi task/domain learning enables novel task zero-shot domain adaptation provides alternative pipeline zero-shot learning. neural networks also used address mtl/mdl learning shared invariant features contribution complementary approaches straightforward combine placing complex structure left-hand side future directions current semantic descriptor formed discrete variables. want extend continuous periodic variable like pose brightness time. assume semantic descriptor always observed improvement dealing missing descriptor also interest. acknowledgements gratefully acknowledge support nvidia corporation donation gpus used research.", "year": 2014}