{"title": "Learning the Experts for Online Sequence Prediction", "tag": ["cs.LG", "cs.AI"], "abstract": "Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of $r$ experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of $\\log(r)$. But, without firm prior knowledge on the problem, it is not clear how to choose a small set of {\\em good} experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web.", "text": "elad eban eladecs.huji.ac.il aharon birnbaum aharobcs.huji.ac.il shai shalev-shwartz shaiscs.huji.ac.il amir globerson gamircs.huji.ac.il selim rachel benin school computer science engineering. hebrew university jerusalem. online sequence prediction problem predicting next element sequence given previous elements. problem extensively studied context individual sequence prediction prior assumptions made origin sequence. individual sequence prediction algorithms work quite well long sequences algorithm enough time learn temporal structure sequence. however might give poor predictions short sequences. possible remedy rely general model prediction expert advice learner access experts makes predictions sequence. well known possible predict almost well best expert sequence length order log. without prior knowledge problem clear choose small good experts. paper describe analyze algorithm learns good experts using training previously observed sequences. demonstrate merits approach applying task click prediction web. sequence prediction task machine learning statistics. involves predicting next element sequence given previous elements. typical applications include stock market prediction click prediction browsing consumption prediction smart grids. although sequence prediction problem well studied current solutions either work long sequences require strong prior knowledge. work provide method uses training data learn predict novel sequence. shall show training sequences obtain prior knowledge needed predicting novel sequences. sequence prediction naturally cast online prediction problem every step predict next element receive true value element suffering loss made prediction error. allowed improve model predict next step. online formulation natural applications since element’s true value unfolds real time interested minimizing prediction loss process. classical approach problem called universal sequence prediction class methods methods guarantee asymptotically model achieve optimal prediction error. however price universality good performance reached seeing long sequences. intuitively reason prior knowledge sequence used take good model alternative approach introduce prior knowledge predicting expert advice experts expert sequence predictor. weighted-majority algorithm uses experts online prediction guaranteed perform almost well best expert. formally sequence length average number prediction mistakes weighted-majority bounded average number prediction mistakes made best therefore choosing experts face classical bias-complexity tradeoﬀ hand want small enough regret term log/t small. hand diﬀerent experts work well diﬀerent sequences since know type sequence going would like increase experts rich enough explain many types sequences. paper propose learn good experts based sample sequences. formally hypothesis class experts. convenient allow experts output predictions convert element prediction example r|σ| interpret prediction score symbols mapping score vector actual prediction maxσ∈σ therefore function loss prediction measured loss function loss given above it’s clear learning experts work experts sequences want predict. thus question address here choose good experts. propose learn experts training sequences. spirit empirical risk minimization shall seek experts perform well training set. highly non-trivial task case several reasons. first performance pool experts measured performance online algorithm whose parameters experts it’s clear optimize function. shall hindsight loss simpler function optimize results comparable theoretical guarantees. second would like experts arbitrarily long histories making predictions without ﬁtting. shall show done using variant context trees finally it’s clear generalization guarantees expected scheme. perform detailed generalization analysis providing theoretical bounds sample complexity learning good experts. learning task thus follows given training sequences learn experts work well online sequence prediction. sense viewed collaborative version sequence prediction. provide objective corresponds discriminative setting analyze generalization error minimizer. theoretical analysis provides generalization bounds show ﬁtting longer histories quantify advantage learning collaborative setting. ﬁnite alphabet. sequence symbols member denoted online sequence prediction takes place consecutive rounds. round forecaster observes preﬁx predicts then next symbol revealed forecaster pays pays otherwise. problem might diﬃcult optimize since objective function involves activation algorithm simple mathematical formulation. overcome diﬃculty show simpler objective used. light theorem know sequence second summand depends size. therefore ﬁxed size follow standard bound minimization approach minimizing ex∼d] instead ex∼d]. words minimize hindsight loss instead online loss. approach minimization yields following minimization problem training sequences assuming encoded convex convex function obtain objective convex individually jointly. suggests alternating optimization scheme alternates this case class loss function problem consider paper formalized follows given sample sequences assumed unknown distribution sampled i.i.d. goal learning experts size parameter learning problem wish learn running sequence small number mistakes. describe learn ﬁrst consider extreme situations. first i.e. prediction time simply follow predictions single expert exactly standard traditional setting statistical batch learning would like learn model hypothesis class whose expected loss randomly chosen example small possible. problem approach might case sequences diﬀerent types single expert able accurately predict sequences. extreme i.e. revert problem online learning hypothesis class problem approach complex sequence length required order guarantee good performance online learning might large. mentioned before function described context tree therefore without additional constraints learning class context trees ﬁnite sample lead over-ﬁtting. overcome this constrain depth tree. alternatively allow depth carefully discount long histories described next. following balance between long histories short histories done deﬁning norm matrices corresponding context trees longer histories penalized more. formally column context tree matrix depth corresponding node tree. sequence then deﬁne norm vectors ∥uj∥ norm matrices j’th another squared norm weighted squared euclidean norms columns weight column thus assign higher penalty columns corresponding deep nodes trees. ready describe algorithm call goal minimize loss respect vectors parameters experts. described section parameterize expert context tree matrix |σ∗|. mentioned earlier minimize alternating optimization minimizing done closed form optimizing f’s. scheme especially attractive since minimizing ﬁxed straightforward sequence best expert expert otherwise. optimizing ﬁxed done gradient descent using smooth loss thus given general scheme described particular experts use. follows specify those. function described using multiclass context tree. experts using generalization multiclass context trees following dekel described below. simplify notation denote multiclass context-tree k-ary rooted tree node tree associated vector prediction tree sequence determined follows. initially start vector current node root tree. vector associated current node traverse child becomes current node. vector associated current node traverse child. process repeated arrive either leaf tree. ﬁnal value gives score value elements actual prediction maxi convenient represent context tree matrix rows follows. order nodes full k-ary tree breadth ﬁrst manner. simplicity restrict trees bounded depth represent context tree matrix column matrix vector associated i’th node tree similarly sequence vector follows. suppose traverse root full k-ary tree according symbols described before. then coordinates corresponding nodes visited path rest coordinates zero. proof theorem given long version article. main ideas proof follows. first construct cover loss class h|f| then bound rademacher complexity class using generalization dudley’s chaining technique similar technique recently proposed srebro next turn attention speciﬁc class context trees bounded norm. following lemma bounds complexity. lemma class multiclass context trees maps r|σ| dened section r|σ| loss function r|σ| then proof lemma given long version article. main idea nice trick showing bound cover linear class based known bounds convergence rate sub-gradient mirror descent algorithms similar method zhang although bound slightly better. multiclass log-loss function satisﬁes conditions lemma hence corollary class multiclass context trees multiclass log-loss. probability exists constant then probability least minimizing done gradient descent. calculating gradient w.r.t. easy loss. implementation stochastic gradient descent update performed training sequence processed. light order bound suﬃces bound section derive bounds bounds depend following measures number experts complexity measure hypothesis class number training examples training loss ﬁrst deﬁne complexity measure hypothesis class respect loss function denition class functions target loss function. complexity sequence exists size |h′| exists satises reader familiar covering number bounds easily recognize determining size cover also easy verify class binary classiﬁers upper bounded dimension later show class bounded norm context trees bounded well. theorem probability exists constant assume also then probability information theorists machine learning researchers useful tools context trees store informative histories probability next symbol given these. however works consider predicting sequence single source. indeed work extends single sequence predictions collaborative setting model different sequences constrain predictors share common structure another related line work multitask prediction considers several diﬀerent multiclass prediction problems seeks common feature space those. setting diﬀerent several ways. first multitask setting receives training instances task known sample belongs class. case receive individual sequences. furthermore multitask setting test data comes known tasks whereas receive novel sequence unknown source. recent approach sequence modeling sequence memoizer based nonparametric bayesian models applied single type model multiple distinct models here. conceivable fully bayesian model collaborative sequence prediction built using models would interesting contrast approach. another possible approach problem probabilistic latent variable models discriminative counterparts sequence mapped latent variable corresponding best expert. next given class previous history probabilistic suﬃx tree used generate next action. however model handle long histories appropriately formulation state space unstructured. cases interest structure. example correspond items online shopping basket. prediction setting recently addressed rendle unlike case access multiple training sequences particular users prediction done users. furthermore temporal model ﬁrst order thus diﬀerent ours. note easily extend approach structured state spaces using structured prediction instead multiclass here. consider three diﬀerent baselines models. ﬁrst algorithm fact batch trained approach training sequences modeled single corresponding expert. thus directly model multiple temporal behaviors sequences data. second baseline online model evaluated test sequence individually. training done using algorithm being online algorithm training data. however given long enough sequences able model deterministic temporal behavior optimally. words algorithm beneﬁt adaptation performance crucially depends length sequence. denote baseline online pst. finally consider generative latent variable model mixture markov chains. order markov chain basic powerful tool modeling sequences. generalize markov chains allowing sequence generated regular markov models. think models diﬀerent chain types similarly experts lex. speciﬁcally sequence r-lmm model order deﬁned used generated sequences length note construction maximal possible generalization accuracy data evaluate accuracy online prediction test sequences. fig. show accuracy three baselines. notice approaches accuracy using sequences -lex require substantially samples order approach performance words agreement theoretical analysis sample complexity smaller -lex lmm. accuracy online much lower consider challenging task predicting browsing pattern users. speciﬁcally browsing logs users intra-net site. session sequence urls visited every user recorded server. dataset contains sequences length domain prediction problem distinct urls magnitude data split train validation test sets sizes training sets vary validation test sizes ﬁxed sequences respectively. applied three baseline models compared performance lex. experiment experts learned combined additional expert obtained training -lex algorithm resulting pool learned experts. addition smoothes performance short sequences algorithm might enough time decide experts follow. results shown fig. seen outperforms methods. considering diﬀerence accuracy -lex notice added accuracy multiple experts shrinks training size increases. trend agrees theory since data available -lex longer histories eventually able model temporal behavior. however show synthetic experiments small data sizes considerable. def= latent variable assigns chain type sequence. note standard markov chain simply -lmm. learn parameters training data using prediction using model done maximum a-posteriori assignment point time. since discount long histories expected perform well large enough training data available. parameters algorithms tuned using cross validation. described analyzed method learning experts online sequence prediction. particular speciﬁed class prediction suﬃx trees. thus experts capture dependencies arbitrarily long histories. achieved mapping context trees vector space designing norm space discounts long histories. generalization results show complexity model penalized maximal possible length histories rather eﬀective needed context based history empirical results show temporal user speciﬁc structure indeed used improve prediction accuracy. proposed approach extended several ways. first consider diﬀerent prediction goals instead predicting next symbol sequence corresponding next binary classiﬁer returns user likely take given action zero otherwise. alternatively consider ranking task want sort actions according interest user. objectives need replace multiclass loss corresponding loss.", "year": 2012}