{"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network  Training", "tag": ["cs.CV", "cs.DC", "cs.LG", "cs.NE"], "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "text": "ability train large-scale neural networks resulted state-of-the-art performance many areas computer vision. results largely come computational break throughs forms model parallelism e.g. accelerated training seen quick adoption computer vision circles data parallelism e.g. a-sgd whose large scale used mostly industry. report early experiments system makes model parallelism data parallelism call a-sgd. show using a-sgd possible speed training large convolutional neural networks useful computer vision. believe a-sgd make possible train larger networks larger training sets reasonable amount time. recently large convolutional neural networks achieved state-of-the-art results across many areas computer vision including character recognition object recognition object detection partly result larger datasets e.g. imagenet large scale visual recognition challenge accelerated training algorithms make data. approaches accelerated using many cpus gpus even many gpus believe accelerating training result break throughs computer vision. present experiments using system accelerating neural network training using asynchronous stochastic gradient descent many gpus call a-sgd. show system used speed training several times explore best a-sgd speed training. benchmark speed pipeline found train convolutional neural network ilsvrc dataset classes million images. like work network uses dropout relu neurons trained data augmentation. neural network seen large parameterized function. parameters function learned gradient descent style algorithms. traditional gradient descent gradient objective function needs calculated entire dataset. parameters updated gradient. repeated convergence. main issues approach dataset large memory gradient take long compute. dataset large stochastic gradient descent used. gradient objective function calculated small random partition dataset called minibatch. parameters updated minibatch gradient minibatch chosen. process repeated convergence. algorithm accelerated ways speeding calculation minibatch gradient parallelization stochastic gradient descent steps many approaches structure neural network computations exploited speed calculation minibatch gradient. called model parallelism. achieved using gpus distributed approaches distributed approaches distributed approaches added beneﬁt train models memory single device. many cases models ignore parallelization notable exception. it’s distbelief technique makes model parallelism data parallelism talk below. work similar experiment many gpus distributed framework accelerate computation large models. work differs primarily focus model parallelism train models single device especially unsupervised pre-training locally-connected neural networks. able train billion parameter model using signiﬁcantly smaller number nodes leveraging consumer off-the-shelf gpus high-speed interconnect. line research promising locally-connected unsupervised models currently performing models common computer vision benchmarks like ilsvrc. believe approach complementary theirs. another method speeding training neural networks using distributed versions stochastic gradient decent methods called data parallel speed rate entire dataset contributes optimization. data parallel part distbelief model especially interesting essentially many neural network models training independently occasionally communicating central parameter server synchronize overall effect many distributed gradient updates. makes straight-forward apply various model parallel approaches. model also proved useful computer vision problems achieving state-of-the-art performance computer vision benchmark million images methods outperform single based methods leveraging many parameters operate large scale work also exploits model parallelism data parallelism. gpus model parallelism a-sgd data parallelism. a-sgd subset distbelief system described technique ignores distributed approach model parallelism instead used gpus accelerate gradient computation. multiple replicas model used optimize single objective. model replica trained using gpu. achieved extending publicly available cuda-convnet code used allow several clients communicate server. communication. model requests updated parameters every etch steps sends updated gradient values every npush steps. distbelief paper etch npush regime would work well gpus gradients usually communicated every minibatch. typically parameters updated nsync steps copied nsync large e.g. additional overhead cost transferring parameters cpu. overhead reduce beneﬁt accelerate gradient calculations. experiments etch npush nsync. experiment different values nsync. performed experiments blue water supercomputer. nvidia tesla nodes gemini high-speed interconnect. make highperformance machine notes gpus high speed interconnect available off-theshelf. experiments performed less nodes. ﬁrst experiment test whether achieve similar performance a-sgd. used settings used single cases nsync experiment clients. resulting learning curves shown near state performance epoch takes days overﬁtting. speed experience minibatch test performance usually higher overall test performance averaging crops true here checkpoint over-ﬁtting gets test error next experiments want compare speed using varying numbers clients varying values nsync. since hard interpret many learning curves single plot smooth plot using sliding window mini batches. also plot training error sliding window doesn’t need adjusted different values nsync. since second experiment examined effect cold start learning number clients increases a-sgd instance hours. observe number gpus increase initial learning becomes much slower. also observe later training a-sgd instances clients learn rapidly. hypothesize early training many gradient directions decrease error. since client calculates different gradients averaging slow progress. later training gradients become consistent averaging increases speed learning. result suggests warm start beneﬁcial suggested also improved methods explicitly deal variance gradients adagrad adadelta third experiment explore nsync effects learning many clients. nsync values clients begin experiments warm start obtained training network single hours. warm start effect many clients clearer. nsync error decreases single gpus. note nsync increases error curve jagged artifacts. believe stale updates. also note nsync signiﬁcantly fewer minibatches processed hours error rate still lower. suggests cost associated increased update frequency still win. emphasize observations plot learning curves clients nsync values plan explore adagrad adadelta boost performance. believe a-sgd promising direction. recently showed larger models improve performance computer vision tasks larger models begin figure training error warm start. increasing number client shows signiﬁcant speed across values nsync. note nsync experiment cients= failed time publication included. research part blue waters sustained-petascale computing project supported national science foundation state illinois. blue waters joint effort university illinois urbana-champaign national center supercomputing applications.", "year": 2013}