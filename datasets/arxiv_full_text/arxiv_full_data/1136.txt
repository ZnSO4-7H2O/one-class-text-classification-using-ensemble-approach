{"title": "Imitation networks: Few-shot learning of neural networks from scratch", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In this paper, we propose imitation networks, a simple but effective method for training neural networks with a limited amount of training data. Our approach inherits the idea of knowledge distillation that transfers knowledge from a deep or wide reference model to a shallow or narrow target model. The proposed method employs this idea to mimic predictions of reference estimators that are much more robust against overfitting than the network we want to train. Different from almost all the previous work for knowledge distillation that requires a large amount of labeled training data, the proposed method requires only a small amount of training data. Instead, we introduce pseudo training examples that are optimized as a part of model parameters. Experimental results for several benchmark datasets demonstrate that the proposed method outperformed all the other baselines, such as naive training of the target model and standard knowledge distillation.", "text": "orem guarantees inﬁnitely wide neural network least hidden layer represent lipschitz continuous function arbitrary degree accuracy. theorem implies exists neural network well imitates behavior estimators keeping great representation power neural networks. paper propose novel method training neural networks small amount supervised training data. figure shows basic idea proposed method named imitation networks. proposed method ﬁrst trains reference model given small amount supervised training data transfers knowledge reference model target neural network model similar manner knowledge distillation although types black-box estimators applied reference model method principle particularly select reference model expected robust overﬁtting. different almost previous work knowledge distillation employs large number supervised training examples proposed method requires supervised training examples knowledge transfer. augment training examples introduce inducing points pseudo training examples helping make model training tractable much easier. original inducing point methods used scalpaper propose imitation networks simple effective method training neural networks limited amount training data. approach inherits idea knowledge distillation transfers knowledge deep wide reference model shallow narrow target model. proposed method employs idea mimic predictions reference estimators much robust overﬁtting network want train. different almost previous work knowledge distillation requires large amount labeled training data proposed method requires small amount training data. instead introduce pseudo training examples optimized part model parameters. experimental results several benchmark datasets demonstrate proposed method outperformed baselines naive training target model standard knowledge distillation. introduction current state-of-the-art machine learning heavily relies supervised learning deep neural networks large-scale datasets. however constructing large-scale datasets generally requires painstaking effort many real-world applications limited number training examples obtained. deep neural networks easily overﬁt training data especially small amount training data available. several techniques alleviating overﬁtting deep learning proposed e.g. semi-supervised learning transfer learning few-shot learning however approaches require either large number unsupervised training examples pretrained model trained large amount supervised training data thus learning neural networks examples remains challenge. meanwhile several estimators support vector machines gaussian processes ease adverse effect overﬁtting making bayesian principle maximum margin. universal approximator theable inference inducing points model parameters updated increase objective function actually lower bound marginalized likelihood. proposed method however parameters target model updated decrease training loss pseudo training examples updated increase training loss. this move pseudo training examples toward areas current target model well trained. also introduce ﬁdelity weighting controlling contribution pseudo training example based uncertainty predictions obtained reference models. enables eliminate unreliable pseudo examples model training. related work techniques avoiding overﬁtting several techniques already proposed alleviating overﬁtting neural network training. widely used techniques include data augmentation model regularization early stopping dropout weight decay implemented almost deep learning libraries. semi-supervised learning also major approach large amount unsupervised training examples available. recently generative models generative adversarial networks become popular approaches semi-supervised learning disentangle supervised information many latent factors variation principled transfer learning also widely used neural network training base model trained large amount training data related tasks available. one-shot few-shot learning problem learning classiﬁer supervised examples class. transfer learning few-shot learning relies reference models pre-trained large amount labeled training examples whose class labels different target class labels captures characteristics target classes relationships among reference classes single target example. compared approaches method requires additional training examples unlike semi-supervised learning achieves few-shot learning scratch meaning reference model trained training examples neither additional examples reference models trained large amount supervised training examples required. knowledge distillation knowledge distillation class techniques training shallow and/or narrow network also called model distillation model compression. speciﬁcally knowledge distillation learns small student network transferring knowledge large teacher networks mainly implementing network onto devices limited computational power. pioneered approach extended generalidea deep learning. ized previous methods introducing metric between output distribution teacher student predictions. previous researches demonstrates knowledge transferred either intermediate layers teacher model multiple teacher models mismatched unsupervised stimuli examples reconstructed meta-data recent papers showed knowledge distillation framework applicable learn student network teacher network completely structure student network. meanwhile method inherits idea knowledge distillation exploits arbitrary black-box estimator teacher contrast standard knowledge distillation require large amount real training data instead employs pseudo training data optimized model training. inducing point methods inducing point methods originally developed scalable inference regression classiﬁcation. inducing points pseudo training examples help model training tractable much easier. implies initial inducing points suggests variational approach provides objective function optimizing inducing points. method inducing points covariate function parameters updated increase objective function actually evidence lower bound marginalized likelihood. introduced stochastic variational inference improving scalability inducing points regarded latent variables marginalized model inference. meanwhile method ﬁrst introduces inducing points supervised neural network training updates pseudo training examples increase training loss updates model parameters decrease loss adversarial manner. model optimization knowledge distillation introducing framework proposed method ﬁrst describe knowledge distillation source main idea. knowledge distillation family methods transfer generalization ability pre-trained reference model target model input example model parameters reference target models respectively. following omit model parameters simplicity except explicitly state. recent knowledge distillation methods mainly focused training shallow narrow fidelity weighting note pseudo training examples always useful knowledge transfer. examples yielding unreliable predictions reference model likely harmful discarded model training. purpose introduce ﬁdelity weighting adaptively weighs training examples based uncertainty predictions obtained reference model introduction ﬁdelity weighting imitation loss replaced following equation. fidelity weighting valid various kinds reference estimators unless bayesian treatments cannot applied reference estimators principle. example original work utilized classiﬁers popular bayesian estimators. bayesian variants svms bayesian neural networks also employed purpose. pseudo example optimization described section inducing point method originally developed scalable inference regression classiﬁcation method inducing points model parameters updated increase objective function. hand proposed method updates model parameters pseudo training examples going completely different directions. namely model parameters updated decrease imitation loss meanpseudo training examples updated increase loss. this move pseudo training examples towards areas current target model well trained. proposed technique updating pseudo examples inspired adversarial training increases robustness neural networks adversarial examples formed applying small intentionally worst-case perturbations examples dataset. neural network target model help reference models much deeper wider neural networks minimizing following distillation loss ldis respect target supervised training examples corresponding supervisors supervised loss compares supervisor prediction unsupervised loss computes loss different predictions knowledge distillation usually deals classiﬁcation problems supervisor represented one-hot vector however also directly applied regression machine learning problems replacing supervisors shapes appropriate problem solved. proposed loss function shown almost formulations previous knowledge distillation work employ supervised training examples train target model however notice minimizing distillation loss limited number supervised training examples causes overﬁtting real supervisions required computing unsupervised loss. based observation proposed method called imitation networks newly introduces train pseudo training examples reference model minimizing following loss function limi call imitation loss knowledge distillation reference model ﬁrst pre-trained supervised training examples however note number supervised examples available problem setting. reference model single estimator ensemble multiple estimators. build multiple estimators single model changing hyper-parameters variance length scale kernels gps. introducing ensemble multiple estimators choose average predictions similar manner previous work formulation indicates target model tries imitate predictions reference model pseudo examples time tries return predictions supervised examples accurately possible. therefore target model inherit different properties coming reference model neural networks. adversarial example seen stochastic update original example increase loss small constant seen learning rate gradient sign part sign{∇xd)} replaced standard stochastic gradient ∇xd). also ground-truth supervisor removed update since every pseudo training example soft supervision instead. based discussion update pseudo training example obtained instead direct stochastic gradient update introduce recent advances stochastic optimization adam nesterov accelerated adam faster optimization. addition stochastic gradient part replaced types adversarial examples natural spatially transformed adversary build adversarial training procedure alternatively updates parameters target model decrease imitation loss pseudo examples increase loss. however adversarial training procedure well known unstable instead adopt another approach augments pseudo examples. employ ﬁxed pseudo examples t-th step model training update another pseudo examples originally carbon copy current convergence t-th training step current merged next locking pseudo examples make model training stable augmenting pseudo examples whose predictions distant reference model makes target model closer reference model. experiments qualitative analysis check behavior proposed method qualitatively ﬁrst turned banana dataset binary classiﬁcation two-dimensional examples. embedded two-dimensional examples dimensional vector space linear transformation transformation matrix randomly determined advance selected supervised training examples class randomly. selected supervised examples unused examples shown figure crosses supervised training examples dots unused examples dataset color corresponds class label. employed classiﬁer kernel reference models built ensemble multiple reference models trained different initial kernel parameters. used gpﬂow variational gaussian approximation l-bfgsb optimization inference utilized mean predictions pseudo supervisions. target model selected -layer fully-connected neural network intermediate layers units. used nadam model parameter optimization adam pseudo example optimization initial learning rates respectively. batch size number training epochs total. first examined extreme case pseudo examples densely distributed onto whole feature space. although realistic especially high-dimensional spaces useful check whether proposed method imitate behavior reference model precisely. figure shows distribution green dots pseudo examples. case optimize pseudo examples case employ ﬁdelity weighting shown means utilized original imitation loss shown utilized kullback-leibler divergence soft losses ignored hard loss figure show decision boundaries reference target models respectively. result indicates method imitated predictions reference model almost completely expected. figure visualizations predictions reference target models orange blue crosses supervised training examples orange blue dots unused examples dataset green dots pseudo training examples black lines classiﬁcation boundaries. left bottom right supervised unused examples pseudo training examples densely distributed onto whole feature space extreme case reference model trained supervised training examples target model trained densely distributed pseudo examples pseudo training examples generated supervised examples seeds target model trained generated pseudo examples target model trained supervised pseudo examples. target model trained supervised pseudo examples plus ﬁdelity weighting. next considered realistic setup initial pseudo examples generated mixture interpolation gaussian augmentation supervised training examples augmented technique shown section number pseudo training examples much smaller ﬁrst setup. thus training steps training epochs employed pseudo examples respectively. utilized hinge loss hard loss kullback-leibler divergence soft losses weights {varλ figure shows initial distribution pseudo training examples show decision boundaries optimized pseudo examples target models trained soft supervisions soft hard supervisions supervisions plus ﬁdelity weighting respectively. results indicate target model well imitated reference model even sparsely distributed pseudo examples. also figure indicate introduction hard supervisions yielded better classiﬁcation performance supervised examples preserving decision boundary much possible ﬁdelity weighting affect boundary much. distribution optimized pseudo examples partly describes properties proposed method. optimized pseudo examples distributed whole feature space decrease difference between target reference models. table classiﬁcation performances mnist fashion mnist datasets imitation ﬁdelity stand proposed method pseudo example optimization ﬁdelity weighting respectively next quantitatively evaluated classiﬁcation performances proposed method several benchmark datasets. used mnist fashion mnist datasets experiment. employed classiﬁer reference model setups related reference models section target model -layer datasets convolution layer group convolution layer channels l-th layer group) batch normalization prelu dropout fully-connected layers placed afterward. used nadam model parameter optimization adam pseudo example optimization initial learning rates prepared initial pseudo examples interpolating different supervised training examples augmented employing technique shown section thus whole training process contained training steps training step training epochs. weight hard loss mean weight soft loss decreased number supervised training examples increased. trained target model supervised training examples class randomly selected training examples datasets. then tested trained model test examples compared classiﬁcation accuracy averaged different selection supervised training examples. experimental conditions second case section compared performance following training strategies namely reference estimators naive training target neural network method imitation loss method imitation loss pseudo example optimization method imitation loss pseudo example optimization ﬁdelity weighting. table shows experimental result. result indicates proposed method outperformed naive training target neural network model. result also indicates naive training target neural network outperformed reference models mnist dataset. even undesirable setting proposed method transferring knowledge rather weak reference model large target model surprisingly superior comparable naive training. addition proposed method surprisingly outperformed reference models since supervised loss improved classiﬁcation performance. proposed method reasonably worked well even without pseudo example optimization however introduction pseudo example optimization improved classiﬁcation performance especially mnist dataset. although ﬁdelity weighting also improved performance cases contribution minor compared pseudo example optimization main contribution paper. proposed method directly applied standard knowledge distillation setups without modiﬁcations reference model deep wide network target model shallow narrow network. sub-task also tested proposed method knowledge distillation setup. selected -layer reference model -layer target model datasets detailed model conﬁgurations ﬁrst experiment. employ bayesian dropout method computing prediction uncertainty reference neural network. table shows experimental result. result indicates proposed method greatly improved classiﬁcation performance shallow target model expected. result also demonstrates proposed method surprisingly outperformed reference deep model mainly pseudo supervisors implied previous work meanwhile pseudo example optimization ﬁdelity weighting might necessarily effective standard knowledge distillation however large negative impacts classiﬁcation performance. conclusion paper proposed imitation networks simple effective method training neural networks limited number training examples. proposed method employed reference model robust overﬁtting built target neural network imitate behavior reference model trained limited training examples. different previous work knowledge distillation employs large amount supervised training examples method required reference model trained supervised examples. instead introduced pseudo examples optimized process target model training. qualitative quantitative experimental results demonstrated proposed method effectively reduces risk target model overﬁtting caused amount training data. since proposed framework shown section generic directly applied combinations reference target models. example enables train deep target network pre-trained shallow reference network trained small amount data inverse knowledge distillation. provide pre-training deep neural networks. hand method optimizing pseudo training examples rather speciﬁc sophisticated management pseudo examples remains largely investigated. tarvainen valpola. mean teachers better role models weight-averaged consistency tarproc. gets improve semi-supervised deep learning results. nips.", "year": 2018}